<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---819">PR - 819</h2>
<ul>
<li><details>
<summary>
(2024). Heterogeneous domain adaptation via incremental
discriminative knowledge consistency. <em>PR</em>, <em>156</em>, 110857.
(<a href="https://doi.org/10.1016/j.patcog.2024.110857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation is a challenging problem in transfer learning since samples from the source and target domains reside in different feature spaces with different feature dimensions. The key problem is how to minimize some gaps (e.g., data distribution mismatch) presented in two heterogeneous domains and produce highly discriminative representations for the target domain. In this paper, we attempt to address these challenges with the proposed incremental discriminative knowledge consistency (IDKC) method, which integrates cross-domain mapping, distribution matching, discriminative knowledge preservation, and domain-specific geometry structure consistency into a unified learning model. Specifically, we attempt to learn a domain-specific projection to project original samples into a common subspace in which the marginal distribution is well aligned and the discriminative knowledge consistency is preserved by leveraging the labeled samples from both domains. Moreover, domain-specific structure consistency is enforced to preserve the data manifold from the original space to the common feature space in each domain. Meanwhile, we further apply pseudo labeling to unlabeled target samples based on the feature correlation and retain pseudo labels with high feature correlation coefficients for the next iterative learning. Our pseudo-labeling strategy expands the number of labeled target samples in each category and thus enforces class-discriminative knowledge consistency to produce more discriminative feature representations for the target domain. Extensive experiments on several standard benchmarks for object recognition, cross-language text classification, and digit classification tasks verify the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Yuwu Lu and Dewei Lin and Jiajun Wen and Linlin Shen and Xuelong Li and Zhenkun Wen},
  doi          = {10.1016/j.patcog.2024.110857},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110857},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous domain adaptation via incremental discriminative knowledge consistency},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ReViT: Enhancing vision transformers feature diversity with
attention residual connections. <em>PR</em>, <em>156</em>, 110853. (<a
href="https://doi.org/10.1016/j.patcog.2024.110853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformer (ViT) self-attention mechanism is characterized by feature collapse in deeper layers, resulting in the vanishing of low-level visual features. However, such features can be helpful to accurately represent and identify elements within an image and increase the accuracy and robustness of vision-based recognition systems. Following this rationale, we propose a novel residual attention learning method for improving ViT-based architectures, increasing their visual feature diversity and model robustness. In this way, the proposed network can capture and preserve significant low-level features, providing more details about the elements within the scene being analyzed. The effectiveness and robustness of the presented method are evaluated on five image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100, Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances. Additionally, experiments on the COCO2017 dataset show that the devised approach discovers and incorporates semantic and spatial relationships for object detection and instance segmentation when implemented into spatial-aware transformer models.},
  archive      = {J_PR},
  author       = {Anxhelo Diko and Danilo Avola and Marco Cascio and Luigi Cinque},
  doi          = {10.1016/j.patcog.2024.110853},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110853},
  shortjournal = {Pattern Recognition},
  title        = {ReViT: Enhancing vision transformers feature diversity with attention residual connections},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained recognition via submodular optimization
regulated progressive training. <em>PR</em>, <em>156</em>, 110849. (<a
href="https://doi.org/10.1016/j.patcog.2024.110849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Progressive training has unfolded its superiority on a wide range of downstream tasks. However, it may fail in fine-grained recognition (FGR) due to special challenges with high intra-class and low inter-class variances. In this paper, we propose an active self-pace learning method to exploit the full potential of progressive training strategy in FGR. The key innovation of our design is to integrate submodular optimization and self-pace learning into a maximum–minimum optimization framework. The submodular optimization is regarded as a dynamic regularization to select active sample groups in each training round for restricting the search space of self-pace optimization. This can overcome the limitation of traditional self-pace learning that is easily trapped into local minimums when facing challenging samples. Extensive experiments on three public FGR datasets show that the proposed method can win at least 1.5% performance gain in various kinds of network backbones including swin-transformer.},
  archive      = {J_PR},
  author       = {Bin Kang and Songlin Du and Dong Liang and Fan Wu and Xin Li},
  doi          = {10.1016/j.patcog.2024.110849},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110849},
  shortjournal = {Pattern Recognition},
  title        = {Fine-grained recognition via submodular optimization regulated progressive training},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep multi-sphere support vector data description based on
disentangled representation learning. <em>PR</em>, <em>156</em>, 110842.
(<a href="https://doi.org/10.1016/j.patcog.2024.110842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep support vector data description (Deep SVDD) combines deep mapping network and support vector data description (SVDD) to jointly optimize network connection weights and hypersphere volume. However, when the parameters of deep mapping network are set improperly, Deep SVDD may face the problem of hypersphere collapse, where all input data are mapped as the hypersphere center. To overcome the hypersphere collapse problem of Deep SVDD and improve the feature learning ability of deep mapping network, deep multi-sphere SVDD based on disentangled representation learning (DMSVDD-DRL) is proposed. DMSVDD-DRL consists of a variational autoencoder (VAE) and multiple hyperspheres. The feature representations obtained by VAE are disentangled into discriminative representations and generative representations that obey mixture t t -distribution and Gaussian distribution, respectively. In the pre-training phase of DMSVDD-DRL, the network parameters and the hypersphere centers are initialized. In the training phase, the augmented data are added into the training set. The discriminative representations of both the input and augmented data are generated through the mapping network. Furthermore, multiple hyperspheres are constructed by the obtained discriminative representations in the feature space. Finally, the VAE loss of the input data, the reconstruction error of the augmented data, the augmentation loss between the input and augmented data, the average radius of the multiple hyperspheres, and the average distance from discriminative representations to their corresponding hypersphere centers are jointly minimized to obtain the optimal network connection weights and the multiple minimum volume hyperspheres. The effectiveness of the proposed DMSVDD-DRL is validated through the comparative and ablation experiments on the benchmark data sets. In addition, it is verified that DMSVDD-DRL is more robust against outliers in comparison with its related methods.},
  archive      = {J_PR},
  author       = {Hong-Jie Xing and Hui-Nan Wu and Ping-Ping Zhang},
  doi          = {10.1016/j.patcog.2024.110842},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110842},
  shortjournal = {Pattern Recognition},
  title        = {Deep multi-sphere support vector data description based on disentangled representation learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). BMPCN: A bigraph mutual prototype calibration net for
few-shot classification. <em>PR</em>, <em>156</em>, 110841. (<a
href="https://doi.org/10.1016/j.patcog.2024.110841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent studies on few-shot classification, most of the existing methods utilized word embeddings as prior knowledge to adjust the distribution of visual prototypes. However, this straightforward fusion of visual and semantic features profoundly alters the feature distribution in the original feature space, rendering it unable to effectively calibrate feature distribution through mutual guidance of cross-modal information. To address this problem, we propose a novel Bigraph Mutual Prototype Calibration Network (BMPCN) for few-shot learning in this paper, in which we not only update the distribution of class features based on prototype-level similarity in both visual and semantic spaces but also facilitate the mutual guidance of visual and semantic feature updates through instance-level similarity. In the BMPCN, a bigraph mutual promotion structure is proposed, wherein a visual graph is constructed with visual features as nodes and the similarity between visual features as edges. Simultaneously, the semantic feature nodes are automatically generated from images, and the class-level prior knowledge is leveraged to correct these automatically generated semantic nodes. To better update the bigraph mutual promotion structure, we propose a Bigraph Interactive Augmentation Module (BIAM), a Nearest Neighbor Proto-level Similarity Promotion Module (NN-PSP), and a Proto-level Similarity Promotion Module (PK-PSP) based on original knowledge augmentation to perform the bigraph update. For inter-graph updating, we use the prototype-level similarity obtained from the NN-PSP and PK-PSP modules to fully learn task-level information, thus enabling task-specific prototype updates. For intra-graph updating, our visual and semantic graphs use instance-level similarity analysis to extract potential correlations between different feature domains and implement mutual guidance in the BIAM module to correct the feature distribution of visual and semantic features. Experiments on three widely used benchmarks illustrated that our proposed method obtains excellent performance based on the backbone Conv-4, and the results outperform state-of-the-art methods by about 8% on miniImageNet, tieredImageNet, and CUB-200-2011. Code has been available at https://github.com/cmzHome/BMPCN-MASTER .},
  archive      = {J_PR},
  author       = {Jing Zhang and Mingzhe Chen and Yunzuo Hu and Xinzhou Zhang and Zhe Wang},
  doi          = {10.1016/j.patcog.2024.110841},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110841},
  shortjournal = {Pattern Recognition},
  title        = {BMPCN: A bigraph mutual prototype calibration net for few-shot classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Mask-DerainGAN: Learning to remove rain streaks by learning
to generate rainy images. <em>PR</em>, <em>156</em>, 110840. (<a
href="https://doi.org/10.1016/j.patcog.2024.110840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image deraining with unpaired data has been a challenging problem. Previous methods suffer from either the color distortion artifacts, due to the pixel-level cycle consistency loss, or the time-consuming training process. To address these problems, in this paper, we propose a novel method for rain removal based on using unpaired data. First, we obtain a rain streak mask from the derained result, which serves as a guidance for generating rainy images. Both the mask and the rain-free image are then fed into the proposed generator to obtain a high-quality rainy image, which implicitly helps improve the rain removal performance. In this way, the proposed learning framework simultaneously learns rain removal and rain generation in order to produce high-quality rain-free images and rainy images. Second, we propose a contrastive learning generator to preserve background texture details and ensure semantic consistency between the generated rain-free image and the original input. Experimental results demonstrate that our method surpasses most state-of-the-art unsupervised methods on multiple benchmark synthetic and real datasets.},
  archive      = {J_PR},
  author       = {Pengjie Wang and Pei Wang and Miaomiao Chen and Rynson W.H. Lau},
  doi          = {10.1016/j.patcog.2024.110840},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110840},
  shortjournal = {Pattern Recognition},
  title        = {Mask-DerainGAN: Learning to remove rain streaks by learning to generate rainy images},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Learning latent disentangled embeddings and graphs for
multi-view clustering. <em>PR</em>, <em>156</em>, 110839. (<a
href="https://doi.org/10.1016/j.patcog.2024.110839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph based methods have recently attracted much attention for multi-view clustering. Most existing methods seek the latent shared embeddings to learn a unified similarity graph or fuse multiple view-specific graphs to a consensus one for clustering, which may not sufficiently explore the common and complementary information among views. Besides, the high-order inter-view correlations are not fully investigated. To address these issues, this paper proposes a latent Disentangled Embeddings and GRaphs based multi-viEw clustEring (DEGREE) method, which considers the common and view-specific information in a latent subspace by explicit embedding disentanglement and multiple graphs learning. We assume that each view can be generated from a shared latent embedding and a corresponding view-specific embedding, which model the common information and exclusive complementary information among views, respectively. The intra-view and inter-view exclusivities among embeddings are encouraged by an orthogonality regularizer. To fully use the underlying information, we excavate the pairwise instance relations in both shared embedding and diverse view-specific embeddings by learning multiple graphs. Besides, a tensor singular value decomposition (t-SVD) based tensor nuclear norm regularizer is imposed on view-specific graphs, which helps to explore the high-order inter-view correlations. An alternative optimization algorithm is designed to solve the proposed model. Experimental evaluations on several popular datasets demonstrate that our DEGREE method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chao Zhang and Haoxing Chen and Huaxiong Li and Chunlin Chen},
  doi          = {10.1016/j.patcog.2024.110839},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110839},
  shortjournal = {Pattern Recognition},
  title        = {Learning latent disentangled embeddings and graphs for multi-view clustering},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering an inference recipe for weakly-supervised object
localization. <em>PR</em>, <em>156</em>, 110838. (<a
href="https://doi.org/10.1016/j.patcog.2024.110838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing weakly-supervised object localization (WSOL) methods have improved training procedures for better localization performance. However, the inference procedure has been overlooked. We observe that the useful information for localization is missed by the current inference practice of WSOL. To address this limitation, we propose a new test-time ingredient for WSOL: binarizing the penultimate feature map and their corresponding weights of the last linear layer. With this simple remedy, the proposed method consistently improves the localization performance of the existing training methods for WSOL. Extensive evaluation including with three different backbone networks on three different WSOL benchmarks validates its effectiveness. In addition, we demonstrate our method is also able to improve weakly-supervised semantic segmentation performances on PASCAL VOC dataset. Lastly, since our method is only applied during the testing phase, our performance gain comes with negligible computational overheads.},
  archive      = {J_PR},
  author       = {Sanghuk Lee and Cheolhyun Mun and Youngjung Uh and Junsuk Choe and Hyeran Byun},
  doi          = {10.1016/j.patcog.2024.110838},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110838},
  shortjournal = {Pattern Recognition},
  title        = {Discovering an inference recipe for weakly-supervised object localization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selections based on two-type overlap degrees and
three-view granulation measures for k-nearest-neighbor rough sets.
<em>PR</em>, <em>156</em>, 110837. (<a
href="https://doi.org/10.1016/j.patcog.2024.110837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selections facilitate data learning, and they effectively develop by combining the overlap degree and dependency degree related to k k -nearest-neighbor (KNN) rough sets. However, the overlap degree neglects the fusion centrality while the dependency degree adopts only the algebraic perspective, so the corresponding feature selection has advance space. In this paper, the overlap degree is improved by fusion priority while the dependency degree is enriched by informational and dual viewpoints, so systematic feature selections are two-dimensionally established to generate multiple improved algorithms. At first, an improved type of overlap degrees is proposed by operationally exchanging the integration summation and fusion division, thus better motivating feature sorting. Then based on KNN granulation, informational, joint, conditional entropies are constructed, and they derive relative entropies by combining the dependency degree; corresponding size relationships, system equations, and granulation monotonicity are acquired. Furthermore, three-view granulation measures (i.e., the dependency degree, conditional entropy, relative conditional entropy) determine three-view attribute reducts based on feature significances; after pre-sorting deletion features based on two-type overlap degrees, 2 × 3 = 6 2×3=6 heuristic reduction algorithms are systematically established to extend recent algorithm OD&amp;KNN. Finally, relevant uncertainty measures and feature selections are validated through data experiments, and five improved selection algorithms achieve better classification performance.},
  archive      = {J_PR},
  author       = {Jiang Chen and Xianyong Zhang and Zhong Yuan},
  doi          = {10.1016/j.patcog.2024.110837},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110837},
  shortjournal = {Pattern Recognition},
  title        = {Feature selections based on two-type overlap degrees and three-view granulation measures for k-nearest-neighbor rough sets},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detect-order-construct: A tree construction based approach
for hierarchical document structure analysis. <em>PR</em>, <em>156</em>,
110836. (<a href="https://doi.org/10.1016/j.patcog.2024.110836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document structure analysis (aka document layout analysis) is crucial for understanding the physical layout and logical structure of documents, with applications in information retrieval, document summarization, knowledge extraction, etc. In this paper, we concentrate on Hierarchical Document Structure Analysis (HDSA) to explore hierarchical relationships within structured documents created using authoring software employing hierarchical schemas, such as LaTeX, Microsoft Word, and HTML. To comprehensively analyze hierarchical document structures, we propose a tree construction based approach that addresses multiple subtasks concurrently, including page object detection (Detect), reading order prediction of identified objects (Order), and the construction of intended hierarchical structure (Construct). We present an effective end-to-end solution based on this framework to demonstrate its performance. To assess our approach, we develop a comprehensive benchmark called Comp-HRDoc, which evaluates the above subtasks simultaneously. Our end-to-end system achieves state-of-the-art performance on two large-scale document layout analysis datasets (PubLayNet and DocLayNet), a high-quality hierarchical document structure reconstruction dataset (HRDoc), and our Comp-HRDoc benchmark. The Comp-HRDoc benchmark is publicly available at .},
  archive      = {J_PR},
  author       = {Jiawei Wang and Kai Hu and Zhuoyao Zhong and Lei Sun and Qiang Huo},
  doi          = {10.1016/j.patcog.2024.110836},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110836},
  shortjournal = {Pattern Recognition},
  title        = {Detect-order-construct: A tree construction based approach for hierarchical document structure analysis},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Precise facial landmark detection by dynamic semantic
aggregation transformer. <em>PR</em>, <em>156</em>, 110827. (<a
href="https://doi.org/10.1016/j.patcog.2024.110827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, deep neural network methods have played a dominant role in face alignment field. However, they generally use predefined network structures to predict landmarks, which tends to learn general features and leads to mediocre performance, e.g., they perform well on neutral samples but struggle with faces exhibiting large poses or occlusions. Moreover, they cannot effectively deal with semantic gaps and ambiguities among features at different scales, which may hinder them from learning efficient features. To address the above issues, in this paper, we propose a Dynamic Semantic-Aggregation Transformer (DSAT) for more discriminative and representative feature (i.e., specialized feature) learning. Specifically, a Dynamic Semantic-Aware (DSA) model is first proposed to partition samples into subsets and activate the specific pathways for them by estimating the semantic correlations of feature channels, making it possible to learn specialized features from each subset. Then, a novel Dynamic Semantic Specialization (DSS) model is designed to mine the homogeneous information from features at different scales for eliminating the semantic gap and ambiguities and enhancing the representation ability. Finally, by integrating the DSA model and DSS model into our proposed DSAT in both dynamic architecture and dynamic parameter manners, more specialized features can be learned for achieving more precise face alignment. It is interesting to show that harder samples can be handled by activating more feature channels. Extensive experiments on popular face alignment datasets demonstrate that our proposed DSAT outperforms state-of-the-art models in the literature. Our code is available at https://github.com/GERMINO-LiuHe/DSAT .},
  archive      = {J_PR},
  author       = {Jun Wan and He Liu and Yujia Wu and Zhihui Lai and Wenwen Min and Jun Liu},
  doi          = {10.1016/j.patcog.2024.110827},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110827},
  shortjournal = {Pattern Recognition},
  title        = {Precise facial landmark detection by dynamic semantic aggregation transformer},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable time series anomaly detection using masked
latent generative modeling. <em>PR</em>, <em>156</em>, 110826. (<a
href="https://doi.org/10.1016/j.patcog.2024.110826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel time series anomaly detection method that achieves excellent detection accuracy while offering a superior level of explainability. Our proposed method, TimeVQVAE-AD , leverages masked generative modeling adapted from the cutting-edge time series generation method known as TimeVQVAE. The prior model is trained on the discrete latent space of a time–frequency domain. Notably, the dimensional semantics of the time–frequency domain are preserved in the latent space, enabling us to compute anomaly scores across different frequency bands, which provides a better insight into the detected anomalies. Additionally, the generative nature of the prior model allows for sampling likely normal states for detected anomalies, enhancing the explainability of the detected anomalies through counterfactuals . Our experimental evaluation on the UCR Time Series Anomaly archive demonstrates that TimeVQVAE-AD significantly surpasses the existing methods in terms of detection accuracy and explainability. We provide our implementation on GitHub: https://github.com/ML4ITS/TimeVQVAE-AnomalyDetection .},
  archive      = {J_PR},
  author       = {Daesoo Lee and Sara Malacarne and Erlend Aune},
  doi          = {10.1016/j.patcog.2024.110826},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110826},
  shortjournal = {Pattern Recognition},
  title        = {Explainable time series anomaly detection using masked latent generative modeling},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal explainability via latent shift applied to
COVID-19 stratification. <em>PR</em>, <em>156</em>, 110825. (<a
href="https://doi.org/10.1016/j.patcog.2024.110825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are witnessing a widespread adoption of artificial intelligence in healthcare. However, most of the advancements in deep learning in this area consider only unimodal data, neglecting other modalities. Their multimodal interpretation necessary for supporting diagnosis, prognosis and treatment decisions. In this work we present a deep architecture, which jointly learns modality reconstructions and sample classifications using tabular and imaging data. The explanation of the decision taken is computed by applying a latent shift that, simulates a counterfactual prediction revealing the features of each modality that contribute the most to the decision and a quantitative score indicating the modality importance. We validate our approach in the context of COVID-19 pandemic using the AIforCOVID dataset, which contains multimodal data for the early identification of patients at risk of severe outcome. The results show that the proposed method provides meaningful explanations without degrading the classification performance.},
  archive      = {J_PR},
  author       = {Valerio Guarrasi and Lorenzo Tronchin and Domenico Albano and Eliodoro Faiella and Deborah Fazzini and Domiziana Santucci and Paolo Soda},
  doi          = {10.1016/j.patcog.2024.110825},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110825},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal explainability via latent shift applied to COVID-19 stratification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated zero-shot learning with mid-level semantic
knowledge transfer. <em>PR</em>, <em>156</em>, 110824. (<a
href="https://doi.org/10.1016/j.patcog.2024.110824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional centralized deep learning paradigms are not feasible when data from different sources cannot be shared due to data privacy or transmission limitation. To resolve this problem, federated learning has been introduced to transfer knowledge across multiple sources (clients) with non-shared data while optimizing a globally generalized central model (server). Existing federated learning paradigms mostly focus on transmitting image encoders that take instance-sensitive images as input, making them less generalizable and vulnerable to privacy inference attacks. In contrast, in this work, we consider transferring mid-level semantic knowledge (such as attribute) which is not sensitive to specific objects of interest and therefore is more privacy-preserving and general. To this end, we formulate a new Federated Zero-Shot Learning (FZSL) paradigm to learn mid-level semantic knowledge at multiple local clients with non-shared local data and cumulatively aggregate a globally generalized central model for deployment. To improve model discriminative ability, we explore semantic knowledge available from either a language or a vision-language foundation model in order to enrich the mid-level semantic space in FZSL. Extensive experiments on five zero-shot learning benchmark datasets validate the effectiveness of our approach for optimizing a generalizable federated learning model with mid-level semantic knowledge transfer.},
  archive      = {J_PR},
  author       = {Shitong Sun and Chenyang Si and Guile Wu and Shaogang Gong},
  doi          = {10.1016/j.patcog.2024.110824},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110824},
  shortjournal = {Pattern Recognition},
  title        = {Federated zero-shot learning with mid-level semantic knowledge transfer},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Category-agnostic semantic edge detection by measuring
neural representation randomness. <em>PR</em>, <em>156</em>, 110820. (<a
href="https://doi.org/10.1016/j.patcog.2024.110820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge detection plays a fundamental role in computer vision tasks and gains wide applications. In particular, semantic edge detection recently draws more attention due to the high demand for a fine-grained understanding of visual scenes. However, detecting high-level semantic edges hidden in visual scenes is quite challenging. Existing semantic edge detection methods focus on category-aware semantic edges and require elaborate category annotations. Instead, we first propose the category-agnostic semantic edge detection task without additional semantic category annotations. To achieve this goal, we propose to utilize only edge position annotations and leverage the information randomness of semantic edges. Specifically, we align semantic edge positions to the ground truth by maximizing randomness on edge regions and minimizing randomness on non-edge regions in the training process. In the inference process, we first obtain neural representations by the trained network, and then generate semantic edges by measuring neural randomness. We evaluate our method by comparisons with alternative methods on two well-known datasets: Cityscapes (Cordts et al., 2016) and SBD (Hariharan et al., 2014). The results demonstrate our superiority over the alternatives, which is more significant under weak annotations. We also provide comprehensive mechanism studies to verify the generalizability, rationality, and validity of our working mechanism.},
  archive      = {J_PR},
  author       = {Zhiyi Pan and Peng Jiang and Qiong Zeng and Ge Li and Changhe Tu},
  doi          = {10.1016/j.patcog.2024.110820},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110820},
  shortjournal = {Pattern Recognition},
  title        = {Category-agnostic semantic edge detection by measuring neural representation randomness},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triadic temporal-semantic alignment for weakly-supervised
video moment retrieval. <em>PR</em>, <em>156</em>, 110819. (<a
href="https://doi.org/10.1016/j.patcog.2024.110819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Moment Retrieval (VMR) aims to identify specific event moments within untrimmed videos based on natural language queries. Existing VMR methods have been criticized for relying heavily on moment annotation bias rather than true multi-modal alignment reasoning. Weakly supervised VMR approaches inherently overcome this issue by training without precise temporal location information. However, they struggle with fine-grained semantic alignment and often yield multiple speculative predictions with prolonged video spans. In this paper, we take a step forward in the context of weakly supervised VMR by proposing a triadic temporal-semantic alignment model. Our proposed approach augments weak supervision by comprehensively addressing the multi-modal semantic alignment between query sentences and videos from both fine-grained and coarse-grained perspectives. To capture fine-grained cross-modal semantic correlations, we introduce a concept-aspect alignment strategy that leverages nouns to select relevant video clips. Additionally, an action-aspect alignment strategy with verbs is employed to capture temporal information. Furthermore, we propose an event-aspect alignment strategy that focuses on event information within coarse-grained video clips, thus mitigating the tendency towards long video span predictions during coarse-grained cross-modal semantic alignment. Extensive experiments conducted on the Charades-CD and ActivityNet-CD datasets demonstrate the superior performance of our proposed method.},
  archive      = {J_PR},
  author       = {Jin Liu and JiaLong Xie and Fengyu Zhou and Shengfeng He},
  doi          = {10.1016/j.patcog.2024.110819},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110819},
  shortjournal = {Pattern Recognition},
  title        = {Triadic temporal-semantic alignment for weakly-supervised video moment retrieval},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contracting skeletal kinematics for human-related video
anomaly detection. <em>PR</em>, <em>156</em>, 110817. (<a
href="https://doi.org/10.1016/j.patcog.2024.110817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting the anomaly of human behavior is paramount to timely recognizing endangering situations, such as street fights or elderly falls. However, anomaly detection is complex since anomalous events are rare and because it is an open set recognition task, i.e., what is anomalous at inference has not been observed at training. We propose COSKAD, a novel model that encodes skeletal human motion by a graph convolutional network and learns to COntract SKeletal kinematic embeddings onto a latent hypersphere of minimum volume for Video Anomaly Detection. We propose three latent spaces: the commonly-adopted Euclidean and the novel spherical and hyperbolic. All variants outperform the state-of-the-art on the most recent UBnormal dataset, for which we contribute a human-related version with annotated skeletons. COSKAD sets a new state-of-the-art on the human-related versions of ShanghaiTech Campus and CUHK Avenue , with performance comparable to video-based methods. Source code and dataset will be released upon acceptance.},
  archive      = {J_PR},
  author       = {Alessandro Flaborea and Guido Maria D’Amely di Melendugno and Stefano D’Arrigo and Marco Aurelio Sterpa and Alessio Sampieri and Fabio Galasso},
  doi          = {10.1016/j.patcog.2024.110817},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110817},
  shortjournal = {Pattern Recognition},
  title        = {Contracting skeletal kinematics for human-related video anomaly detection},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GR-GAN: A unified adversarial framework for single image
glare removal and denoising. <em>PR</em>, <em>156</em>, 110815. (<a
href="https://doi.org/10.1016/j.patcog.2024.110815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce the single image glare removal (SIGR) task. SIGR aims to eliminate glare or light caused by external environment in lighting scene. To efficiently improve natural image quality and usability, many research tasks, such as image deraining and shadow removal, have been investigated a lot. However, SIGR is still underexplored. Therefore, we propose to construct a dataset and explore deep learning-based models for SIGR. Our contributions can be summarized as follows: (1) We establish a new benchmark dataset for SIGR, termed De-Glare, aiming to propel research of SIGR. This dataset comprises pairs of {glare, glare-free} images sourced from both real-world and synthetic data, utilized for training and evaluating models. (2) We conduct a comprehensive benchmarking of extensive state-of-the-art (SoTA) methods on the constructed De-Glare dataset and provide insightful analyses based on the results. (3) An innovative approach for single image glare removal employing a multi-scale generative adversarial network (GR-GAN) model is proposed. Glare images typically exhibit irregular glare shapes and cluttered backgrounds. To address these irregular glare patterns, we introduce a deformable convolution-based glare attention detector (GAD) designed to generate an attention map which specifics glare spots or rays in the input image. In pursuit of enhancing the perceptual quality of output image, GR-GAN adaptively filters out irrelevant noises and enhances salient features through a generator with cascaded pyramid neck (CPN) network. This work can provide useful insights for developing better SIGR models. Without specific tuning, our method achieves the SoTA results on multiple computer vision tasks, including the image deraining and image shadow removal.},
  archive      = {J_PR},
  author       = {Cong Niu and Ke Li and Di Wang and Wenxuan Zhu and Haojie Xu and Jinhui Dong},
  doi          = {10.1016/j.patcog.2024.110815},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110815},
  shortjournal = {Pattern Recognition},
  title        = {GR-GAN: A unified adversarial framework for single image glare removal and denoising},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDSSI: Image deturbulence with semantic and spatial–temporal
information. <em>PR</em>, <em>156</em>, 110813. (<a
href="https://doi.org/10.1016/j.patcog.2024.110813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The propagation of light through turbulent media results in refractive index fluctuations, causing distortion and blurring of images captured by imaging equipment. Consequently, it is of paramount importance to mitigate turbulence effects. Deep convolutional neural networks incorporating attention mechanisms have demonstrated remarkable success in dynamic video restoration. However, in most networks, the attention mechanism is limited to capturing simple contextual features, failing to adequately exploit multi-level image features. This paper introduces IDSSI, an effective model that utilizes semantic and spatio–temporal information for turbulence mitigation. A novel two-branch feature extraction structure is proposed, capable of extracting multi-scale enhanced features with semantic information under limited supervision. These perceived semantic features are subsequently fused into global features, enhancing multi-scale perception for turbulence repair. Furthermore, a new Spatial–Temporal feature learning strategy is proposed. This approach facilitates the extraction and modulation of temporal information by obtaining edge cues, effectively concatenating and merging with spatial features. This strategy serves as an efficient alternative to 3D convolution. Experimental results on relevant datasets demonstrate the superiority of the proposed model over current state-of-the-art turbulence repair methods.},
  archive      = {J_PR},
  author       = {Xiangqing Liu and Li Tang and Gang Li and Zijun Zhang and Shaoan Yan and Yongguang Xiao and Jianbin Xie and Minghua Tang},
  doi          = {10.1016/j.patcog.2024.110813},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110813},
  shortjournal = {Pattern Recognition},
  title        = {IDSSI: Image deturbulence with semantic and Spatial–Temporal information},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided-attention and gated-aggregation network for medical
image segmentation. <em>PR</em>, <em>156</em>, 110812. (<a
href="https://doi.org/10.1016/j.patcog.2024.110812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transformers have been widely used in medical image segmentation to capture long-range and global dependencies using self-attention. However, they often struggle to learn the local details which limit their ability to capture irregular shapes and sizes of the tissues and indistinct boundaries between the tissues, which are critical for accurate segmentation. To alleviate this issue, we propose a network named GA2Net, which comprises an encoder, a bottleneck, and a decoder. The encoder computes multi-scale features. In the bottleneck, we propose a hierarchical-gated features aggregation (HGFA) which introduces a novel spatial gating mechanism to enrich the multi-scale features. To effectively learn the shapes and sizes of the tissues, we apply deep supervision in the bottleneck. GA2Net proposes to use adaptive aggregation (AA) within the decoder, to adjust the receptive fields for each location in the feature map, by replacing the traditional concatenation/summation operations in skip connections in U-Net like architecture. Furthermore, we propose mask-guided feature attention (MGFA) modules within the decoder which strives to learn the salient features using foreground priors to adequately grasp the intricate structural and contour information of the tissues. We also apply intermediate supervision for each stage of the decoder, which further improves the capability of the model to better locate the boundaries of the tissues. Our extensive experimental results illustrate that our GA2-Net significantly outperforms the existing state-of-the-art methods over eight medical image segmentation datasets i.e., five polyps, a skin lesion, a multiple myeloma cell segmentation, and a cardiac MRI scan datasets. We then perform an extensive ablation study to validate the capabilities of our method. Code is available at https://github.com/mustansarfiaz/ga2net .},
  archive      = {J_PR},
  author       = {Mustansar Fiaz and Mubashir Noman and Hisham Cholakkal and Rao Muhammad Anwer and Jacob Hanna and Fahad Shahbaz Khan},
  doi          = {10.1016/j.patcog.2024.110812},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110812},
  shortjournal = {Pattern Recognition},
  title        = {Guided-attention and gated-aggregation network for medical image segmentation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep generative domain adaptation with temporal relation
attention mechanism for cross-user activity recognition. <em>PR</em>,
<em>156</em>, 110811. (<a
href="https://doi.org/10.1016/j.patcog.2024.110811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sensor-based Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ( i . i . d . i.i.d. ). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal dependency relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognizes and integrates temporal dependency relations during the domain adaptation process. By synergizing the capabilities of generative models with the Temporal Relation Attention mechanism, our method improves the classification performance in cross-user HAR. The evaluation has been conducted on three public sensor-based HAR datasets targeting daily living activity and sports fitness activity scenarios to demonstrate the efficacy of the proposed DGDATA method.},
  archive      = {J_PR},
  author       = {Xiaozhou Ye and Kevin I-Kai Wang},
  doi          = {10.1016/j.patcog.2024.110811},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110811},
  shortjournal = {Pattern Recognition},
  title        = {Deep generative domain adaptation with temporal relation attention mechanism for cross-user activity recognition},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DM-GAN: CNN hybrid vits for training GANs under limited
data. <em>PR</em>, <em>156</em>, 110810. (<a
href="https://doi.org/10.1016/j.patcog.2024.110810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) training demands substantial data and computational resources. This paper aims to explore an economical approach for generating novel images with limited image data, addressing the challenge of data scarcity. Our contributions involve resolving the few-shot image generation challenge through the development of an unsupervised hybrid generative adversarial network named DM-GAN. We introduce a lightweight hybrid module (DC-Vit) comprising convolution and visual transformation, merging local and global features to enhance image perception, expressiveness, and ensure stable image generation. Additionally, a multi-scale adaptive skip connection module is incorporated to effectively mitigate the feature loss problem arising from inter-layer jumps, thereby producing more complete and regular images. To enhance the texture learning process and improve the quality and realism of synthesized images, we integrate the gray conjugate matrix into the loss function. Empirical evaluations are conducted on small sample datasets at various resolutions, including publicly accessible collections of art paintings, real-life photographs, and proprietary artifact image datasets. The experimental results unequivocally demonstrate the qualitative and quantitative superiority of our model over existing methods, underscoring its efficacy and robustness.},
  archive      = {J_PR},
  author       = {Longquan Yan and Ruixiang Yan and Bosong Chai and Guohua Geng and Pengbo Zhou and Jian Gao},
  doi          = {10.1016/j.patcog.2024.110810},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110810},
  shortjournal = {Pattern Recognition},
  title        = {DM-GAN: CNN hybrid vits for training GANs under limited data},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-language pre-training via modal interaction.
<em>PR</em>, <em>156</em>, 110809. (<a
href="https://doi.org/10.1016/j.patcog.2024.110809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing vision-language pre-training models typically extract region features and conduct fine-grained local alignment based on masked image/text completion or object detection methods. However, these models often design independent subtasks for different modalities, which may not adequately leverage interactions between modalities, requiring large datasets to achieve optimal performance. To address these limitations, this paper introduces a novel pre-training approach that facilitates fine-grained vision-language interaction. We propose two new subtasks — image filling and text filling — that utilize data from one modality to complete missing parts in another, enhancing the model’s ability to integrate multi-modal information. A selector mechanism is also developed to minimize semantic overlap between modalities, thereby improving the efficiency and effectiveness of the pre-trained model. Our comprehensive experimental results demonstrate that our approach not only fosters better semantic associations among different modalities but also achieves state-of-the-art performance on downstream vision-language tasks with significantly smaller datasets.},
  archive      = {J_PR},
  author       = {Hang Cheng and Hehui Ye and Xiaofei Zhou and Ximeng Liu and Fei Chen and Meiqing Wang},
  doi          = {10.1016/j.patcog.2024.110809},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110809},
  shortjournal = {Pattern Recognition},
  title        = {Vision-language pre-training via modal interaction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Auxiliary audio–textual modalities for better action
recognition on vision-specific annotated videos. <em>PR</em>,
<em>156</em>, 110808. (<a
href="https://doi.org/10.1016/j.patcog.2024.110808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current audio–visual datasets are class-relevant, where audio and visual modalities are annotated. Thus, current audio–visual recognition methods apply cross-modality attention or modality fusion. However, leveraging the audio modality effectively in vision-specific videos for human activity recognition is of particular challenge. We address this challenge by proposing a novel audio–visual recognition framework that effectively leverages audio modality in any vision-specific annotated dataset. The proposed framework employs language models ( e.g ., GPT-3, CPT-text, BERT) for building a semantic audio–video label dictionary (SAVLD) that serves as a bridge between audio and video datasets by mapping each video label to its most K-relevant audio labels. Then, SAVLD along with a pre-trained audio multi-label model are used to estimate the audio–visual modality relevance. Accordingly, we propose a novel learnable irrelevant modality dropout (IMD) to completely drop the irrelevant audio modality and fuse only the relevant modalities. Finally, for the efficiency of the proposed multimodal framework, we present an efficient two-stream video Transformer to process the visual modalities ( i.e ., RGB frames and optical flow). The final predictions are re-ranked with GPT-3 recommendations of the human activity classes. GPT-3 provides high-level recommendations using the labels of the detected visual objects and the audio predictions of the input video. Our framework demonstrated a remarkable performance on the vision-specific annotated datasets Kinetics400 and UCF-101 by outperforming most relevant human activity recognition methods.},
  archive      = {J_PR},
  author       = {Saghir Alfasly and Jian Lu and Chen Xu and Yu Li and Yuru Zou},
  doi          = {10.1016/j.patcog.2024.110808},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110808},
  shortjournal = {Pattern Recognition},
  title        = {Auxiliary audio–textual modalities for better action recognition on vision-specific annotated videos},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PPM: A boolean optimizer for data association in multi-view
pedestrian detection. <em>PR</em>, <em>156</em>, 110807. (<a
href="https://doi.org/10.1016/j.patcog.2024.110807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately localize occluded people in a crowd is a challenging problem in video surveillance. Existing end-to-end deep multi-camera detectors rely heavily on pre-training with the same multiview datasets used for testing, which compromises their real-world applications. An alternative approach presented here is to project the torso lines of the instance segmentation masks from multiple views to the ground plane and propose pedestrian candidates at the intersection points. The candidate selection process is, for the first time, formulated as a logic minimization problem in Boolean algebra. A probabilistic Petrick’s method (PPM) is proposed to seek the minimum number of candidates to account for all the foreground masks while maximizing the joint occupancy likelihoods in multiple views. Experiments on benchmark video datasets have demonstrated the much improved performance of this approach in comparison with the benchmark deep or non-deep algorithms for multiview pedestrian detection.},
  archive      = {J_PR},
  author       = {Rui Qiu and Ming Xu and Yuyao Yan and Jeremy S. Smith and Yuchen Ling},
  doi          = {10.1016/j.patcog.2024.110807},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110807},
  shortjournal = {Pattern Recognition},
  title        = {PPM: A boolean optimizer for data association in multi-view pedestrian detection},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Few-shot learning with long-tailed labels. <em>PR</em>,
<em>156</em>, 110806. (<a
href="https://doi.org/10.1016/j.patcog.2024.110806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Learning (FSL) is a challenging classification task in machine learning, and it aims to recognize unseen examples of new classes with only a few labeled reference examples (i.e., the support set). The training phase of FSL typically requires a large amount of labeled examples (i.e., the base set) to effectively learn transferable knowledge, but it is usually difficult to obtain sufficient data annotation in practical applications. Existing semi-supervised FSL approaches can learn generalizable representations from partly labeled data, yet they do not sufficiently consider the real distribution of those labeled data. In this paper, we propose a new problem setting termed Few-Shot Learning with Long-Tailed Labels (FSL-LTL) to further consider a more practical semi-supervised scenario where the labeled examples are long-tailed. To effectively address this new problem, we build a novel two-stage training framework dubbed Reweighted Contrastive Embedding (RCE). In the first stage of RCE, we adopt the popular contrastive learning framework to pre-train a reliable network in a self-supervised manner. In the second stage, we integrate the semi-supervised empirical risk into a Weighted Random Sampling (WRS) strategy to fine-tune the pre-trained backbone with the aid of a consistency regularization. Experimental results demonstrate the feasibility of the proposed FSL-LTL problem setting and the superiority of our new RCE method over existing FSL approaches and semi-supervised learning methods. These results also suggest that the RCE approach is a promising solution for addressing the new FSL-LTL problem.},
  archive      = {J_PR},
  author       = {Hongliang Zhang and Shuo Chen and Lei Luo and Jiang Yang},
  doi          = {10.1016/j.patcog.2024.110806},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110806},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot learning with long-tailed labels},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot classification with fork attention adapter.
<em>PR</em>, <em>156</em>, 110805. (<a
href="https://doi.org/10.1016/j.patcog.2024.110805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to transfer the knowledge learned from seen categories to unseen categories with a few references. It is also an essential challenge to bridge the gap between humans and deep learning models in real-world applications. Despite extensive previous efforts to tackle this problem by finding an appropriate similarity function, we emphasize that most existing methods have merely considered a single low-resolution representation pair utilized in similarity calculations between support and query samples. Such representational limitations could induce the instability of category predictions. To better achieve metric learning stabilities, we present a novel method dubbed Fork Attention Adapter (FA-adapter), which can seamlessly establish the dense feature similarity with the newly generated nuanced features. The utility of the proposed method is more performant and efficient via the two-stage training phase. Extensive experiments demonstrate consistent and substantial accuracy gains on the fine-grained CUB, Aircraft, non-fine-grained mini -ImageNet, and tiered -ImageNet benchmarks. By comprehensively studying and visualizing the learned knowledge from different source domains, we further present an extension version termed FA-adapter++ to boost the performance in fine-grained scenarios.},
  archive      = {J_PR},
  author       = {Jieqi Sun and Jian Li},
  doi          = {10.1016/j.patcog.2024.110805},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110805},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot classification with fork attention adapter},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Repeat and learn: Self-supervised visual representations
learning by repeated scene localization. <em>PR</em>, <em>156</em>,
110804. (<a href="https://doi.org/10.1016/j.patcog.2024.110804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large labeled datasets are crucial for video understanding progress. However, the labeling process is time-consuming, expensive, and tiresome. To overcome this impediment, various pretexts use the temporal coherence in videos to learn visual representations in a self-supervised manner. However, these pretexts (order verification and sequence sorting) struggle when encountering cyclic actions due to the label ambiguity problem. To overcome these limitations, we present a novel temporal pretext task to address self-supervised learning of visual representations from unlabeled videos. Repeated Scene Localization (RSL) is a multi-class classification pretext that involves changing the temporal order of the frames in a video by repeating a scene. Then, the network is trained to identify the modified video, localize the location of the repeated scene, and identify the unmodified original videos that do not have repeated scenes. We evaluated the proposed pretext on two benchmark datasets, UCF-101 and HMDB-51. The experimental results show that the proposed pretext achieves state-of-the-art results in action recognition and video retrieval tasks. In action recognition, our S3D model achieves 88.15% and 56.86% on UCF-101 and HMDB-51, respectively. It outperforms the current state-of-the-art by 1.05% and 3.26%. Our R(2+1)D-Adjacent model achieves 83.52% and 54.50% on UCF-101 and HMDB-51, respectively. It outperforms the single pretext tasks by 8.7% and 13.9%. In video retrieval, our R(2+1)D-Offset model outperforms the single pretext tasks by 4.68% and 1.1% Top 1 accuracies on UCF-101 and HMDB-51, respectively. The source code and the trained models are publicly available at https://github.com/Hussein-A-Hassan/RSL-Pretext .},
  archive      = {J_PR},
  author       = {Hussein Altabrawee and Mohd Halim Mohd Noor},
  doi          = {10.1016/j.patcog.2024.110804},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110804},
  shortjournal = {Pattern Recognition},
  title        = {Repeat and learn: Self-supervised visual representations learning by repeated scene localization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Dynamic selection for reconstructing instance-dependent
noisy labels. <em>PR</em>, <em>156</em>, 110803. (<a
href="https://doi.org/10.1016/j.patcog.2024.110803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an inevitable issue in annotating large-scale datasets, instance-dependent label noise (IDN) can cause serious overfitting in neural networks. To combat IDN, label reconstruction methods have been developed with noise transition matrices or DNNs to simulate the transition from clean labels to noisy labels. Nevertheless, the absence of correct supervisions will lead to learning wrong noise transitions. This motivates us to select samples with clean labels to fetch the correct supervisions. However, the difficulty in obtaining prior knowledge of the noise rate prohibits the use of existing sample selection methods. To this end, we propose a dynamic sample selection method, namely Identity Mapping (IdMap), to overcome this limitation. Inspired by the feature-dependent characteristic of IDN, we first introduce the extracted instance features and pseudo-ground-truth labels to reconstruct noisy labels. A partial identity mapping between two labels is then established and samples with consistent identity mapping output are selected as clean data to update the classifier. Extensive experiments on both artificial and real-world noisy datasets demonstrate the superiority of IdMap compared with other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jie Yang and Xiaoguang Niu and Yuanzhuo Xu and Zejun Zhang and Guangyi Guo and Steve Drew and Ruizhi Chen},
  doi          = {10.1016/j.patcog.2024.110803},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110803},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic selection for reconstructing instance-dependent noisy labels},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty estimation in HDR imaging with bayesian neural
networks. <em>PR</em>, <em>156</em>, 110802. (<a
href="https://doi.org/10.1016/j.patcog.2024.110802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of high dynamic range (HDR) imaging is to estimate potential high-quality images from multi-exposed low dynamic range (LDR) inputs. Intuitively, there exist various possible HDR images corresponding to given LDR inputs, which results in uncertainty in the estimated results. However, most existing HDR imaging methods employ l 1 l1 or l 2 l2 loss only to provide one possible estimation from various possible solutions, which fails to model the uncertainty, and thus lacks high-frequency details. In this work, we design Bayesian neural networks to capture the uncertainty, which can model one-to-many relations and provide various possible solutions. Concretely, we propose a Variational Bayesian Layer by leveraging a hierarchical prior on the network weights and inferring a new joint posterior, which is utilized to model uncertainty in high-frequency details ( e.g. , textures), and model uncertainty in semantic information ( e.g. , ghost areas), respectively. By leveraging Bayesian framework, the proposed method can provide various potential high-quality estimations, especially in high-frequency details. Experiments on different datasets show that the proposed method enables sampling possible HDR imaging, and the consensus estimate achieves state-of-the-art quantitative and qualitative results.},
  archive      = {J_PR},
  author       = {Qingsen Yan and Haishen Wang and Yifan Ma and Yuhang Liu and Wei Dong and Marcin Woźniak and Yanning Zhang},
  doi          = {10.1016/j.patcog.2024.110802},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110802},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty estimation in HDR imaging with bayesian neural networks},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HFGN: High-frequency residual feature guided network for
fast MRI reconstruction. <em>PR</em>, <em>156</em>, 110801. (<a
href="https://doi.org/10.1016/j.patcog.2024.110801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is a valuable medical imaging technology, while it suffers from a long acquisition time. Various methods have been proposed to reconstruct sharp images from undersampled k-space data to reduce imaging time. However, these methods hardly reconstruct high-quality aliasing-free Magnetic Resonance (MR) images with clear structures, especially in high-frequency components. To address this problem, we propose a High-Frequency residual feature Guided Network (HFGN) for fast MRI reconstruction. HFGN uses a sub-network, High-Frequency Extraction Network (HFEN), to learn the difference between the U-Net reconstruction result and the ground truth, then uses the learned features to guide the reconstruction of the network. In the reconstruction network, we propose Residual Channel and Spatial Attention block (RCSA), which uses frequency domain and image domain convolution branching to learn the global and local features of the image simultaneously. The experiment results under different acceleration rates on different datasets demonstrate that our proposed method surpasses the existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Faming Fang and Le Hu and Jinhao Liu and Qiaosi Yi and Tieyong Zeng and Guixu Zhang},
  doi          = {10.1016/j.patcog.2024.110801},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110801},
  shortjournal = {Pattern Recognition},
  title        = {HFGN: High-frequency residual feature guided network for fast MRI reconstruction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Theoretical insights on the pre-image resolution in machine
learning. <em>PR</em>, <em>156</em>, 110800. (<a
href="https://doi.org/10.1016/j.patcog.2024.110800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many nonlinear pattern recognition and data mining tasks rely on embedding the data into a latent space, one often needs to extract the patterns in the input space. Estimating the inverse of the nonlinear embedding is the so-called pre-image problem. Several strategies have been proposed to address the estimation of the pre-image; However, there are no theoretical results so far to understand the pre-image problem and its resolution. In this paper, we provide theoretical underpinnings of the resolution of the pre-image problem in Machine Learning. These theoretical results are on the gradient descent optimization, the fixed-point iteration algorithm and Newton’s method. We provide sufficient conditions on the convexity/nonconvexity of the pre-image problem. Moreover, we show that the fixed-point iteration is a Newton update and prove that it is a Majorize-Minimization (MM) algorithm where the surrogate function is a quadratic function . These theoretical results are derived for the wide classes of radial kernels and projective kernels. We also provide other insights by connecting the resolution of this problem to the gradient density estimation problem with the so-called mean shift algorithm.},
  archive      = {J_PR},
  author       = {Paul Honeine},
  doi          = {10.1016/j.patcog.2024.110800},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110800},
  shortjournal = {Pattern Recognition},
  title        = {Theoretical insights on the pre-image resolution in machine learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation for large-scale point clouds based on
hybrid attention and dynamic fusion. <em>PR</em>, <em>156</em>, 110798.
(<a href="https://doi.org/10.1016/j.patcog.2024.110798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the semantic segmentation problem for large-scale point clouds. Recent segmentation methods usually employ an encoder–decoder architecture. However, these methods may not effectively extract neighboring information in the encoder. Additionally, they typically use nearest neighbor interpolation and skip connections in the decoder, overlooking the semantic gap between encoder and decoder features. To resolve these issues, we propose HADF-Net, which consists of a Hybrid Attention Encoder (HAE), an Edge Dynamic Fusion module (EDF), and a Dynamic Cross-attention Decoder (DCD). HAE leverages the distinctive properties of geometric and semantic relations to aggregate local features at different stages. EDF aims to alleviate information loss during decoder upsampling by dynamically integrating the neighboring information. DCD employs an enhanced fusion mechanism with spatial-wise cross-attention to bridge the semantic gap between encoder and decoder features. Experimental results on 4 datasets demonstrate that our HADF-Net achieves superior performance.},
  archive      = {J_PR},
  author       = {Ce Zhou and Zhaokun Shu and Li Shi and Qiang Ling},
  doi          = {10.1016/j.patcog.2024.110798},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110798},
  shortjournal = {Pattern Recognition},
  title        = {Semantic segmentation for large-scale point clouds based on hybrid attention and dynamic fusion},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exposure difference network for low-light image enhancement.
<em>PR</em>, <em>156</em>, 110796. (<a
href="https://doi.org/10.1016/j.patcog.2024.110796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement aims to simultaneously improve the brightness and contrast of low-light images and recover the details of the visual content. This is a challenging task that makes typical data-driven methods suffer, especially when faced with severe information loss in extreme low-light conditions. In this work, we approach this task by proposing a novel exposure difference network. The proposed network generates a set of possible exposure corrections derived from the differences between synthesized images under different exposure levels, which are fused and adaptively combined with the raw input for light compensation. By modeling the intermediate exposure differences, our model effectively eliminates the redundancy existing in the synthesized data and offers the flexibility to handle image quality degradation resulting from varying levels of inadequate illumination. To further enhance the naturalness of the output image, we propose a global-aware color calibration module to derive low-frequency global information from inputs, which is further converted into a projection matrix to calibrate the RGB output. Extensive experiments show that our method can achieve competitive light enhancement performance both quantitatively and qualitatively.},
  archive      = {J_PR},
  author       = {Shengqin Jiang and Yongyue Mei and Peng Wang and Qingshan Liu},
  doi          = {10.1016/j.patcog.2024.110796},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110796},
  shortjournal = {Pattern Recognition},
  title        = {Exposure difference network for low-light image enhancement},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-frequency and low-frequency dual-channel graph
attention network. <em>PR</em>, <em>156</em>, 110795. (<a
href="https://doi.org/10.1016/j.patcog.2024.110795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing graph convolution layers use learnable or fixed weights to sum up neighbor features to aggregate neighbor information. Since the attention values are always positive, these graph convolution layers perform as low-pass filters, which may result in their poor performance on heterophilic graphs. In this paper, two graph convolutional layers are proposed, HLGAT and NGAT . NGAT is a convolution network using only negative attention values, which only make the aggregation of high-frequency information of neighbor nodes. HLGAT makes the aggregation of low-frequency and high-frequency information by two channels, respectively, and fuses two outputs by using a learnable way. On node-classification task, both NGAT and HLGAT offer significant performance improvement compared to existing methods. The results clearly show that: (1) High-frequency information of neighborhoods plays a decisive role in heterophilic graphs. (2) The aggregation of low-frequency and high-frequency information of neighbor nodes can significantly improve the performance on heterophilic graphs.},
  archive      = {J_PR},
  author       = {Yukuan Sun and Yutai Duan and Haoran Ma and Yuelong Li and Jianming Wang},
  doi          = {10.1016/j.patcog.2024.110795},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110795},
  shortjournal = {Pattern Recognition},
  title        = {High-frequency and low-frequency dual-channel graph attention network},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of dialogic emotion analysis: Developments,
approaches and perspectives. <em>PR</em>, <em>156</em>, 110794. (<a
href="https://doi.org/10.1016/j.patcog.2024.110794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dialogic emotion analysis is an emerging and important research field in natural language processing. It aims to understand and process emotions in various forms of dialogue, such as human-human conversations, human–machine interactions, and chatbot responses. However, dialogic emotion analysis faces many challenges, such as the diversity of dialogue genres, the complexity of emotional expressions, and the difficulty of capturing the emotional needs of dialogue participants. Moreover, the current dialogue systems lack the ability to analyze emotions effectively and appropriately in different dialogue contexts. Therefore, a comprehensive review of the existing research on dialogic emotion analysis is needed. This survey aims to review dialogic emotion analysis methods based on natural language processing from 2017 to 2024. The review process follows the Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA). We summarize the research methods and emphasize their main research contributions. In addition, we also discuss current research trends and possible future research directions, as well as the impact of personal traits on emotions and potential ethical issues.},
  archive      = {J_PR},
  author       = {Chenquan Gan and Jiahao Zheng and Qingyi Zhu and Yang Cao and Ye Zhu},
  doi          = {10.1016/j.patcog.2024.110794},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110794},
  shortjournal = {Pattern Recognition},
  title        = {A survey of dialogic emotion analysis: Developments, approaches and perspectives},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source-free domain adaptation via dynamic pseudo labeling
and self-supervision. <em>PR</em>, <em>156</em>, 110793. (<a
href="https://doi.org/10.1016/j.patcog.2024.110793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, unsupervised domain adaptation (UDA) has attracted extensive interest in relieving the greedy requirement of vanilla deep learning for labeled data. It seeks for a solution to adapt the knowledge from a well-labeled training dataset (source domain) to another unlabeled target dataset (target domain). However, in some practical scenarios, the source domain data is inaccessible for a variety of reasons, and only a model trained on it can be provided, thus deriving a more challenging task, i.e., source-free unsupervised domain adaptation (SFUDA). Some pseudo labeling-based methods have been proposed to solve it by predicting pseudo labels for the unlabeled target domain data. Nevertheless, incorrectly designated pseudo labels will impose an adverse impact on the network adaptation. To alleviate this issue, we propose a dynamic confidence-based pseudo labeling strategy for SFUDA in this paper. Unlike those methods that first rigidly assign pseudo labels to all target domain data and then try to weaken the effect of incorrect pseudo labels in training, we proactively label the target samples with higher confidence in a dynamic manner. To further relieve the impact of incorrect pseudo labels, we harness the collaborative learning to constrain the consistency of the network and impose an additional soft supervision. Besides, we also investigate the possible problem brought by our labeling strategy, i.e., the neglect of wavering samples near the decision boundary, and solve it by injecting the self-supervised learning into our model. Experiments on three UDA benchmark datasets demonstrate the state-of-the-art performance of our proposed method. The code is publicly available at https://github.com/meowpass/DPLS .},
  archive      = {J_PR},
  author       = {Qiankun Ma and Jie Zeng and Jianjia Zhang and Chen Zu and Xi Wu and Jiliu Zhou and Jie Chen and Yan Wang},
  doi          = {10.1016/j.patcog.2024.110793},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110793},
  shortjournal = {Pattern Recognition},
  title        = {Source-free domain adaptation via dynamic pseudo labeling and self-supervision},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EdVAE: Mitigating codebook collapse with evidential discrete
variational autoencoders. <em>PR</em>, <em>156</em>, 110792. (<a
href="https://doi.org/10.1016/j.patcog.2024.110792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Codebook collapse is a common problem in training deep generative models with discrete representation spaces like Vector Quantized Variational Autoencoders (VQ-VAEs). We observe that the same problem arises for the alternatively designed discrete variational autoencoders (dVAEs) whose encoder directly learns a distribution over the codebook embeddings to represent the data. We hypothesize that using the softmax function to obtain a probability distribution causes the codebook collapse by assigning overconfident probabilities to the best matching codebook elements. In this paper, we propose a novel way to incorporate evidential deep learning (EDL) through a hierarchical Bayesian modeling instead of softmax to combat the codebook collapse problem of dVAE. We evidentially monitor the significance of attaining the probability distribution over the codebook embeddings, in contrast to softmax usage. Our experiments using various datasets show that our model, called EdVAE, mitigates codebook collapse while improving the reconstruction performance, and enhances the codebook usage compared to dVAE and VQ-VAE based models. Our code can be found at https://github.com/ituvisionlab/EdVAE .},
  archive      = {J_PR},
  author       = {Gulcin Baykal and Melih Kandemir and Gozde Unal},
  doi          = {10.1016/j.patcog.2024.110792},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110792},
  shortjournal = {Pattern Recognition},
  title        = {EdVAE: Mitigating codebook collapse with evidential discrete variational autoencoders},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Partial label feature selection based on noisy manifold and
label distribution. <em>PR</em>, <em>156</em>, 110791. (<a
href="https://doi.org/10.1016/j.patcog.2024.110791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In partial label learning, each training object is assigned a valid label and pseudo-labels, and a multi-class classifier is derived with inaccurate supervision. However, ambiguous labeling information adversely affects the performance of the classifier. Partial label feature selection has been shown efficiently improve the generalization performance of classifiers. Traditional manifold learning can employ intrinsic geometric information to identify discriminative features, while it is challenging due to the noisy manifold caused by pseudo-labels. Consequently, this paper proposes an embedding partial label feature selection based on noisy manifold and label distribution, which exploits feature dependency, label correlation, and instance relevance. Specifically, a linear regression function projects the feature space to the low-dimensional manifold space, which can avoid the influence of pseudo-labels affected by direct projection to the label space. The feature dependency and label correlation are obtained by manifold regularization in the feature and label space to reflect the feature significance. During optimization, instance similarity constraints variable iteration. Label distribution obtained through feature significance and instance relevance guides label space updates and reduces the impact of noise in the manifold. The effectiveness and robustness of the proposed algorithm are corroborated through experiments with three classifiers and five comparison methods on twelve datasets.},
  archive      = {J_PR},
  author       = {Wenbin Qian and Jiale Liu and Wenji Yang and Jintao Huang and Weiping Ding},
  doi          = {10.1016/j.patcog.2024.110791},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110791},
  shortjournal = {Pattern Recognition},
  title        = {Partial label feature selection based on noisy manifold and label distribution},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-collaborative comparison for effective cross-domain
few-shot learning. <em>PR</em>, <em>156</em>, 110790. (<a
href="https://doi.org/10.1016/j.patcog.2024.110790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in cross-domain few-shot learning (CD-FSL) primarily focus on learning to compare global representations between query and support images for classification. However, due to the notorious cross-domain semantic gap , the ideal global representations can be totally different across domains, thereby solely learning to compare global representations is not sufficient to achieve effective generalization in challenging cases. To mitigate this problem, we present a Me ta-collaborative Co mparison Net work (MeCo-Net) for CD-FSL, which imitates humans to recognize unfamiliar objects through collaborative comparison on both global and local representations. Following this idea, paralleling with a conventional global comparison branch, we additionally feed random crops of both query and support images into a feature encoder to separately extract their local representations. Subsequently, we associate these local representations across images through bipartite graph matching for local comparison. Thanks to the complementary global and local comparisons, we can obtain a more generalizable classifier for CD-FSL by meta-integrating them for final prediction. Experimental results on eight benchmarks demonstrate that the proposed model generalizes to multiple target domains with state-of-the-art performance without the need for fine-tuning.},
  archive      = {J_PR},
  author       = {Fei Zhou and Peng Wang and Lei Zhang and Wei Wei and Yanning Zhang},
  doi          = {10.1016/j.patcog.2024.110790},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110790},
  shortjournal = {Pattern Recognition},
  title        = {Meta-collaborative comparison for effective cross-domain few-shot learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical mixture of discriminative generalized dirichlet
classifiers. <em>PR</em>, <em>156</em>, 110789. (<a
href="https://doi.org/10.1016/j.patcog.2024.110789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a discriminative classifier for compositional data. This classifier is based on the posterior distribution of the Generalized Dirichlet which is the discriminative counterpart of Generalized Dirichlet mixture model. Moreover, following the mixture of experts paradigm, we proposed a hierarchical mixture of this classifier. In order to learn the models parameters, we use a variational approximation by deriving an upper-bound for the Generalized Dirichlet mixture. To the best of our knownledge, this is the first time this bound is proposed in the literature. Experimental results are presented for spam detection and color space identification.},
  archive      = {J_PR},
  author       = {Elvis Togban and Djemel Ziou},
  doi          = {10.1016/j.patcog.2024.110789},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110789},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical mixture of discriminative generalized dirichlet classifiers},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HTQ: Exploring the high-dimensional trade-off of
mixed-precision quantization. <em>PR</em>, <em>156</em>, 110788. (<a
href="https://doi.org/10.1016/j.patcog.2024.110788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed-precision quantization, where more sensitive layers are kept at higher precision, can achieve the trade-off between accuracy and complexity of neural networks. However, the search space for mixed-precision grows exponentially with the number of layers, making the brute force approach infeasible on deep networks. To reduce this exponential search space, recent efforts use Pareto frontier or integer linear programming to select the bit-precision of each layer. Unfortunately, we find that these prior works rely on a single constraint. In practice, model complexity includes space complexity and time complexity, and the two are weakly correlated, thus using simply one as a constraint leads to sub-optimal results. Besides this, they require manually set constraints, making them only pseudo-automatic. To address the above issues, we propose High-dimensional Trade-off Quantization (HTQ), which automatically determines the bit-precision in the high-dimensional space of model accuracy, space complexity, and time complexity without any manual intervention. Specifically, we use the saliency criterion based on connection sensitivity to indicate the accuracy perturbation after quantization, which performs similarly to Hessian information but can be calculated quickly (more than 1000 × × speedup). The bit-precision is then automatically selected according to the three-dimensional (3D) Pareto frontier of the total perturbation, model size, and bit operations (BOPs) without manual constraints. Moreover, HTQ allows for the joint optimization of weights and activations, and thus the bit-precisions of both can be computed concurrently. Compared to state-of-the-art methods, HTQ achieves higher accuracy and lower space/time complexity on various model architectures for image classification and object detection tasks. Code is available at: https://github.com/zkkli/HTQ .},
  archive      = {J_PR},
  author       = {Zhikai Li and Xianlei Long and Junrui Xiao and Qingyi Gu},
  doi          = {10.1016/j.patcog.2024.110788},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110788},
  shortjournal = {Pattern Recognition},
  title        = {HTQ: Exploring the high-dimensional trade-off of mixed-precision quantization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced online CAM: Single-stage weakly supervised semantic
segmentation via collaborative guidance. <em>PR</em>, <em>156</em>,
110787. (<a href="https://doi.org/10.1016/j.patcog.2024.110787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation with image-level annotations usually adopts multi-stage approaches, where high-quality offline CAM is generated as pseudo labels for further training, leading to a complex training process. Instead, current single-stage approaches, directly learning to segment objects with online CAM from image-level supervision, are more elegant. The quality of CAM critically determines the final segmentation performance. However, how to generate high-quality online CAM has not been deeply studied in existing single-stage methods. In this paper, we propose a new single-stage framework to mine more relative target features for enhanced online CAM. Specifically, we design a novel Collaborative Guidance Mechanism, where a prior guidance block uses the original CAM to produce class-specific feature representations, improving the quality of online CAM. However, such a prior is sensitive to discriminative regions of objects. Thus, we further propose a prior fusion block, in which the online segmentation prediction and the original CAM are fused to strengthen the prior guidance. Extensive experiments show that our approach achieves new state-of-the-art performance on both PASCAL VOC 2012 and MS COCO 2014 datasets, outperforming recent single-stage methods by a clear margin. Code is available at https://github.com/1rua11/CGM},
  archive      = {J_PR},
  author       = {Bingfeng Zhang and Xuru Gao and Siyue Yu and Weifeng Liu},
  doi          = {10.1016/j.patcog.2024.110787},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110787},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced online CAM: Single-stage weakly supervised semantic segmentation via collaborative guidance},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). STNet: Structure and texture-guided network for image
inpainting. <em>PR</em>, <em>156</em>, 110786. (<a
href="https://doi.org/10.1016/j.patcog.2024.110786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Network (GAN) has made great progress in image inpainting due to its strong generation capability. However, the previous methods based on GAN cannot understand the structure and texture of damaged images simultaneously, which leads to inconsistent structure and blurred details of inpainting results. To address this issue, we propose a Structure and Texture-guided Network for image inpainting (STNet), which consists of three networks, i.e. structure reconstruction network, texture reconstruction network, and image refinement network. STNet can restore the coherent structure and fine textures of damaged images in the first two networks, and fuse them to complete image inpainting in the last network. Experiments on three standard datasets CelebA, Places2, and Pairs StreetView show STNet is effective and outperforms other related methods. In particular, we also conducted experiments on the damaged images from tomb murals of Tang Dynasty in China, whose results also verify that our method can be applied to the restoration of historical relics. Our code is available at https://github.com/nwuAI/STNet .},
  archive      = {J_PR},
  author       = {Zhan Li and Yanan Zhang and Yingfei Du and Xiaofeng Wang and Chao Wen and Yongqin Zhang and Guohua Geng and Fan Jia},
  doi          = {10.1016/j.patcog.2024.110786},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110786},
  shortjournal = {Pattern Recognition},
  title        = {STNet: Structure and texture-guided network for image inpainting},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual multi-object tracking with re-identification and
occlusion handling using labeled random finite sets. <em>PR</em>,
<em>156</em>, 110785. (<a
href="https://doi.org/10.1016/j.patcog.2024.110785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an online visual multi-object tracking (MOT) algorithm that resolves object appearance–reappearance and occlusion. Our solution is based on the labeled random finite set (LRFS) filtering approach, which in principle, addresses disappearance, appearance, reappearance, and occlusion via a single Bayesian recursion. However, in practice, existing numerical approximations cause reappearing objects to be initialized as new tracks, especially after long periods of being undetected. In occlusion handling, the filter’s efficacy is dictated by trade-offs between the sophistication of the occlusion model and computational demand. Our contribution is a novel modeling method that exploits object features to address reappearing objects whilst maintaining a linear complexity in the number of detections. Moreover, to improve the filter’s occlusion handling, we propose a fuzzy detection model that takes into consideration the overlapping areas between tracks and their sizes. We also develop a fast version of the filter to further reduce the computational time.},
  archive      = {J_PR},
  author       = {Linh Van Ma and Tran Thien Dat Nguyen and Changbeom Shim and Du Yong Kim and Namkoo Ha and Moongu Jeon},
  doi          = {10.1016/j.patcog.2024.110785},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110785},
  shortjournal = {Pattern Recognition},
  title        = {Visual multi-object tracking with re-identification and occlusion handling using labeled random finite sets},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to match features with discriminative sparse graph
neural network. <em>PR</em>, <em>156</em>, 110784. (<a
href="https://doi.org/10.1016/j.patcog.2024.110784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a cluster-based sparse graph network to improve the efficiency of image feature matching. This architecture clusters keypoints with high correlations into the same subgraphs, where each keypoint interacts only with others within the same subgraph. This strategy effectively reduces the spread of redundant messages and boosts the efficiency of message transmission. A unique coarse-to-fine paradigm is proposed for the incremental construction of sparse graphs, facilitating the evolution of subgraphs from coarse to fine, which enhances keypoint correlation and reduces misclassification. Additionally, the introduction of global tokens within each subgraph enables the learning of global information through interactions with a limited number of global tokens, further minimizing the impact of misclassification by broadening the scope of learning beyond the limits of individual subgraphs. The methodology demonstrates competitive performance in a range of vision tasks, including pose estimation, visual localization, and homography estimation. Compared to complete graph networks, it reduces time and memory consumption by 91% and 46%, respectively, during dense matching. Moreover, building on this foundational architecture, we introduce a novel hierarchical approach for visual localization, utilizing a two-stage sparse-to-dense matching process, achieves a substantial 31.8% decrease in time consumption while maintains competitive accuracy.},
  archive      = {J_PR},
  author       = {Yan Shi and Jun-Xiong Cai and Mingyu Fan and Wensen Feng and Kai Zhang},
  doi          = {10.1016/j.patcog.2024.110784},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110784},
  shortjournal = {Pattern Recognition},
  title        = {Learning to match features with discriminative sparse graph neural network},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A knowledge graph completion model based on triple level
interaction and contrastive learning. <em>PR</em>, <em>156</em>, 110783.
(<a href="https://doi.org/10.1016/j.patcog.2024.110783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs provide credible and structured knowledge for downstream tasks such as information retrieval. Nevertheless, the ubiquitous incompleteness of knowledge graphs often limits the performance of applications. To address the incompleteness, people have proposed the knowledge graph completion task to supplement the facts of incomplete triplets. Recently, researchers have proposed introducing text descriptions to enrich entity representations. Existing methods based on triple decoupling with text description solve the combinatorial explosion problem well. Nevertheless, they still suffer from a lack of global characteristics of factual triples. In addition, the success of contrastive learning research has improved such methods, but they are still limited by existing negative sampling, which is usually more costly than embedding-based methods. In order to solve these limitations, this paper proposes an innovative triple-level interaction model for knowledge graph completion named InCL-KGC. Concretely, the proposed model employs an on-verge interaction method to reduce text redundancy information for entity representation and capture the global semantics of factual triplets. Furthermore, we design an effective hard negative sampling strategy to improve contrast learning. Additionally, we perform an improved Harbsort algorithm for the purpose of reducing the adverse impact of candidate entity sparsity on inference. Extensive experiment consequences exhibit that our model transcends recent baselines with MRR, Hit@3, and Hits@10 increased by 1.2%, 3.2%, and 6.8% on WN18RR, while the index MRR, Hit@1, Hit@3, and Hits@10 were enhanced by 2.8%, 1%, 3.3%, 4.3% on FB15K-237.},
  archive      = {J_PR},
  author       = {Jie Hu and Hongqun Yang and Fei Teng and Shengdong Du and Tianrui Li},
  doi          = {10.1016/j.patcog.2024.110783},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110783},
  shortjournal = {Pattern Recognition},
  title        = {A knowledge graph completion model based on triple level interaction and contrastive learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-schema prompting powered token-feature woven attention
network for short text classification. <em>PR</em>, <em>156</em>,
110782. (<a href="https://doi.org/10.1016/j.patcog.2024.110782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short text classification task poses challenges in natural language processing due to insufficient contextual information. This task is typically approached by extracting rich semantic features in the text and encoding it as a sentence-level representation using deep neural networks. The self-attention mechanism has emerged as one of the primary methods to tackle this problem. However, traditional attention methods only focus on the interactions between tokens, neglecting the semantic relationships between features. We propose a novel attention-based module, called token-feature woven attention fusion (TFWAF) network for sentence-level representation information aggregation, which leverages the self-attention mechanism from both token and feature perspectives. Moreover, we design a multi-schema prompting approach within machine reading comprehension and prompt learning paradigms to better utilize prior knowledge in a pre-trained language model and recognize enhanced textual semantic representation. Experimental results show our model achieves state-of-the-art performance compared to existing baselines on eight benchmark datasets in the context of short text classification. The source code is available in https://github.com/Aaronzijingcai/MP-TFWA .},
  archive      = {J_PR},
  author       = {Zijing Cai and Hua Zhang and Peiqian Zhan and Xiaohui Jia and Yongjian Yan and Xiawen Song and Bo Xie},
  doi          = {10.1016/j.patcog.2024.110782},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110782},
  shortjournal = {Pattern Recognition},
  title        = {Multi-schema prompting powered token-feature woven attention network for short text classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-semantic augmentation network for few-shot open-set
recognition. <em>PR</em>, <em>156</em>, 110781. (<a
href="https://doi.org/10.1016/j.patcog.2024.110781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot open-set recognition (FSOR) represents a relatively underexplored area of research. The primary challenge encountered by FSOR methods lies in recognizing known classes while simultaneously rejecting unknown classes utilizing only limited samples. Current FSOR methods predominantly rely on the visual information extracted from images to establish class representations, aiming to derive distinguishable classification scores for both known and unknown classes. However, these methods often overlook the benefits of leveraging semantic information derived from class names associated with images, which could provide valuable auxiliary learning insights. This study introduces a feature-semantic augmentation network to improve FSOR performance utilizing multimodal information. Specifically, we augment the class-specific features of closed-set prototypes by integrating visual and textual features from known class names across both local and global feature spaces. To facilitate prototype learning, We introduce a refinement and fusion module. Among these, the former leverages the similarity between prototype and target features at both channel and spatial dimensions to calibrate targets relative to their relevant prototypes. Meanwhile, the latter employs additional classification targets generated by the fusion module to provide learning sources from different classes. Experimental results on various few-shot learning benchmarks show that the proposed method significantly outperforms current state-of-the-art methods across both closed- and open-set scenarios.},
  archive      = {J_PR},
  author       = {Xilang Huang and Seon Han Choi},
  doi          = {10.1016/j.patcog.2024.110781},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110781},
  shortjournal = {Pattern Recognition},
  title        = {Feature-semantic augmentation network for few-shot open-set recognition},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CarvingNet: Point cloud completion by stepwise refining
multi-resolution features. <em>PR</em>, <em>156</em>, 110780. (<a
href="https://doi.org/10.1016/j.patcog.2024.110780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of 3D vision, 3D point cloud completion is a crucial task in many practical applications. Current methods use Transformer&#39;s Encoder-Decoder framework to predict the missing part of the point cloud features at low resolution, which does not fully utilize the feature information at multiple resolutions and can result in the loss of the object&#39;s geometric details. In this paper, we present a novel point cloud completion method, CarvingNet, which, to the best of our knowledge, is the first to apply the U-Net architecture to the point cloud completion task by operating directly on unordered point cloud features at multiple resolutions. Firstly, we gradually expand the receptive field and use cross-attention to purify the features of the missing part of the point cloud at each resolution and to generate the contour features of the complete point cloud at the last obtained resolution. Then, we gradually reduce the receptive field and use cross-attention to refine the features of the complete point cloud at each resolution and to generate the features of the complete point cloud with rich details at the last obtained resolution. To obtain point cloud features at different resolutions, we specifically design the up-sampling module and down-sampling module for disordered point cloud features. Furthermore, we improve the FoldingNet network to make it more suitable for generating high-quality dense point clouds. The experimental results demonstrate that our proposed CarvingNet achieves the performance of existing state-of-the-art methods on the ShapeNet-55, ShapeNet-34, and KITTI benchmarks.},
  archive      = {J_PR},
  author       = {Liangliang Li and Guihua Liu and Feng Xu and Lei Deng},
  doi          = {10.1016/j.patcog.2024.110780},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110780},
  shortjournal = {Pattern Recognition},
  title        = {CarvingNet: Point cloud completion by stepwise refining multi-resolution features},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot relational triple extraction with hierarchical
prototype optimization. <em>PR</em>, <em>156</em>, 110779. (<a
href="https://doi.org/10.1016/j.patcog.2024.110779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational Triple Extraction (RTE) aims to extract relations and entities from unstructured text. Current RTE models using supervised learning require a large amount of labeled data, which presents a challenge for real-world applications. Therefore, the research work on Few-Shot Relational Triple Extraction (FS-RTE) has been proposed. However, the existing work cannot effectively construct accurate prototypes from a small number of samples, and it is difficult to model the dependencies between entities and relations, resulting in poor performance in relational triple extraction. In this paper, we propose a Hierarchical Prototype Optimized FS-RTE method (HPO). In particular, to mitigate prototype bias built on a small number of samples, HPO uses prompt learning to merge the information of relational labels into the text. Then, the entity-level prototypes are constructed using a span encoder to avoid label dependency between entity tokens. Finally, the hierarchical contrastive learning (HCL) method is introduced to improve the metric space between the prototypes of entities and relations, respectively. Experiments conducted on two public datasets show that HPO can significantly outperform previous state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chen Gao and Xuan Zhang and Zhi Jin and Weiyi Shang and Yubin Ma and Linyu Li and Zishuo Ding and Yuqin Liang},
  doi          = {10.1016/j.patcog.2024.110779},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110779},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot relational triple extraction with hierarchical prototype optimization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MS-TCRNet: Multi-stage temporal convolutional recurrent
networks for action segmentation using sensor-augmented kinematics.
<em>PR</em>, <em>156</em>, 110778. (<a
href="https://doi.org/10.1016/j.patcog.2024.110778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action segmentation is a challenging task in high-level process analysis, typically performed on video or kinematic data obtained from various sensors. This work presents two contributions related to action segmentation on kinematic data. Firstly, we introduce two versions of Multi-Stage Temporal Convolutional Recurrent Networks (MS-TCRNet), specifically designed for kinematic data. The architectures consist of a prediction generator with intra-stage regularization and Bidirectional LSTM or GRU-based refinement stages. Secondly, we propose two new data augmentation techniques, World Frame Rotation and Hand Inversion, which utilize the strong geometric structure of kinematic data to improve algorithm performance and robustness. We evaluate our models on three datasets of surgical suturing tasks: the Variable Tissue Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS) Dataset, both of which are open surgery simulation datasets collected by us, as well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a well-known benchmark in robotic surgery. Our methods achieved state-of-the-art performance. code: https://github.com/AdamGoldbraikh/MS-TCRNet .},
  archive      = {J_PR},
  author       = {Adam Goldbraikh and Omer Shubi and Or Rubin and Carla M. Pugh and Shlomi Laufer},
  doi          = {10.1016/j.patcog.2024.110778},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110778},
  shortjournal = {Pattern Recognition},
  title        = {MS-TCRNet: Multi-stage temporal convolutional recurrent networks for action segmentation using sensor-augmented kinematics},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-query and multi-level enhanced network for semantic
segmentation. <em>PR</em>, <em>156</em>, 110777. (<a
href="https://doi.org/10.1016/j.patcog.2024.110777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plain transformer-based methods have achieved promising performance on semantic segmentation recently. These methods adopt a single set of class queries to predict masks of different semantic categories based on multi-level feature maps. We argue that this single-query design cannot fully exploit diverse information of different levels for improved semantic segmentation. To address this issue, we propose a multi-query and multi-level enhanced network for semantic segmentation (named QLSeg). Our QLSeg first performs multi-level feature enhancement on plain transformer to improve feature discriminability. Afterwards, we introduce multi-query decoder to respectively extract feature embeddings and predict mask logits at different levels, where feature embeddings are adaptively merged for classification and mask logits are summed for output masks. In addition, we introduce masked attention-to-mask to focus on local regions with the same class. We perform the experiments on three widely-used semantic segmentation datasets: ADE20K, COCO-Stuff-10K, and PASCAL-Context. Our proposed QLSeg achieves competitive results on all these three datasets.},
  archive      = {J_PR},
  author       = {Bin Xie and Jiale Cao and Rao Muhammad Anwer and Jin Xie and Jing Nie and Aiping Yang and Yanwei Pang},
  doi          = {10.1016/j.patcog.2024.110777},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110777},
  shortjournal = {Pattern Recognition},
  title        = {Multi-query and multi-level enhanced network for semantic segmentation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightCM-PNet: A lightweight pyramid network for real-time
prostate segmentation in transrectal ultrasound. <em>PR</em>,
<em>156</em>, 110776. (<a
href="https://doi.org/10.1016/j.patcog.2024.110776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and real-time automatic segmentations of prostate transrectal ultrasound (TRUS) images enable the fusion of magnetic resonance imaging (MRI) and TRUS image to guide robotic prostate biopsy systems. This segmentation, applied to intraoperative TRUS images, is crucial for accurate image registration and automatic localization of the biopsy target. However, the blurred imaging and uneven intensity distribution in TRUS make accurate prostate segmentation still challenging. Most deep learning-based image segmentation methods, like convolutional neural network (CNN) and transformer, are often characterized by large parameters, computational complexity , and slow inference speed. Therefore, we propose a lightweight and accurate segmentation network . We combine CNN and tokenized multilayer perceptron (MLP) as the backbone of feature extraction, and build a pyramid structure network for feature encoding and decoding. This structure can effectively reduce the number of parameters and computational complexity, and effectively use global context information. Additionally, we introduce an interactive hybrid attention module that sequentially derives attention maps along the channel and spatial dimensions. The attention module interacts with the encoded to focus on the prostate region in the output features. We then incorporate a feature fusion module to construct reverse attention features, which enhances areas with weak responses in the foreground. To further improve the accuracy and smoothness of the segmentation result, we include a boundary constraint term in the loss function. Experimental results demonstrate that the proposed network achieves better performance in prostate TRUS image segmentation, with fewer parameters and faster inference speed.},
  archive      = {J_PR},
  author       = {Weirong Wang and Bo Pan and Yue Ai and Gonghui Li and Yili Fu and Yanjie Liu},
  doi          = {10.1016/j.patcog.2024.110776},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110776},
  shortjournal = {Pattern Recognition},
  title        = {LightCM-PNet: A lightweight pyramid network for real-time prostate segmentation in transrectal ultrasound},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of tswin-f network based on multi-scale feature
fusion in tomato leaf lesion recognition. <em>PR</em>, <em>156</em>,
110775. (<a href="https://doi.org/10.1016/j.patcog.2024.110775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tomato leaf lesion identification can greatly help the detection and analysis of plant lesions. This study proposes Tswin-F network, a new network structure based on Transformer, to detect tomato leaf diseases. This Tswin-F network would obtain position information on images by implementing the bilateral local attention module and the self-supervised learning module. Specifically, the bilateral local attention mechanism focuses on the connection with certain continuous tokens, while the self-supervised learning module pays attention to the connection with random token positions. Then the information learned from the above two modules approaches will be combined to create the spatial connection between the final tokens. The combination of the above two modules can enhance the ability to communicate information between the windows of the input images and improve the accuracy of the models. In addition, a Feature Fuse Local Attention (FFLCA) structure is designed to solve the problem that attention distances would increase with the number of layers in the transformer network model. Furthermore, all the feature information is fused through the adaptive fusion strategy and is inputted into the classification network as the final global information of the model. Finally, an accuracy of 99.64% is obtained on 10 types of datasets, reaching the state-of-the-art level of CNN-based methods in terms of accuracy. The accuracy rate of identifying 13 types of tomato leaf lesions reaches 90.81% on average. Code is available at: https://github.com/fightpotato .},
  archive      = {J_PR},
  author       = {Yuanbo Ye and Houkui Zhou and Huimin Yu and Haoji Hu and Guangqun Zhang and Junguo Hu and Tao He},
  doi          = {10.1016/j.patcog.2024.110775},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110775},
  shortjournal = {Pattern Recognition},
  title        = {Application of tswin-F network based on multi-scale feature fusion in tomato leaf lesion recognition},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CFNet: An infrared and visible image compression fusion
network. <em>PR</em>, <em>156</em>, 110774. (<a
href="https://doi.org/10.1016/j.patcog.2024.110774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion aims to acquire a more complete image representation within a limited physical space to more effectively support practical vision applications. Although the currently popular infrared and visible image fusion algorithms take practical applications into consideration. However, they did not fully consider the redundancy and transmission efficiency of image data. To address this limitation, this paper proposes a compression fusion network for infrared and visible images based on joint CNN and Transformer, termed CFNet. First of all, the idea of variational autoencoder image compression is introduced into the image fusion framework, achieving data compression while maintaining image fusion quality and reducing redundancy. Moreover, a joint CNN and Transformer network structure is proposed, which comprehensively considers the local information extracted by CNN and the global long-distance dependencies emphasized by Transformer. Finally, multi-channel loss based on region of interest is used to guide network training. Not only can color visible and infrared images be fused directly but more bits can be allocated to the foreground region of interest, resulting in a superior compression ratio. Extensive qualitative and quantitative analyses affirm that the proposed compression fusion algorithm achieves state-of-the-art performance. In particular, rate–distortion performance experiments demonstrate the great advantages of the proposed algorithm for data storage and transmission. The source code is available at https://github.com/Xiaoxing0503/CFNet .},
  archive      = {J_PR},
  author       = {Mengliang Xing and Gang Liu and Haojie Tang and Yao Qian and Jun Zhang},
  doi          = {10.1016/j.patcog.2024.110774},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110774},
  shortjournal = {Pattern Recognition},
  title        = {CFNet: An infrared and visible image compression fusion network},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence correction for trained graph convolutional
networks. <em>PR</em>, <em>156</em>, 110773. (<a
href="https://doi.org/10.1016/j.patcog.2024.110773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adopting Graph Convolutional Networks (GCNs) for transductive node classification is a hot research direction in artificial intelligence. Vanilla GCNs are primarily under-confident and struggle to clarify the final classification results explicitly due to the lack of supervision. Existing works mainly alleviated this issue by improving annotation deficiency and introducing addition regularization terms. However, these methods need to re-train the model from the beginning, which is computationally expensive for large dataset and model. To deal with this problem, a novel confidence correction mechanism (CCM) for trained GCNs is proposed in this work. Such mechanism aims at calibrating the confidence output of each node in the inference stage by jointly inferring the feature and predicted pseudo label. Specifically, in the inference stage, it uses the predicted pseudo label to select target-related features over all network to obtain a more confident and better result. Such selectivity is formulated as an optimization problem to maximize the category score of each node. In addition, the greedy optimization strategy is utilized to solve this problem and we have mathematically proven that the proposed mechanism can reach the local optimum by mathematical induction. Note that such mechanism is flexible and can be introduced to most GCN-based model. Extensive experimental results on benchmark datasets show that the proposed method can promote the confidence of the final target category and improve the performance of GCNs in the inference stage.},
  archive      = {J_PR},
  author       = {Junqing Yuan and Huanlei Guo and Chenyi Zhou and Jiajun Ding and Zhenzhong Kuang and Zhou Yu and Yuan Liu},
  doi          = {10.1016/j.patcog.2024.110773},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110773},
  shortjournal = {Pattern Recognition},
  title        = {Confidence correction for trained graph convolutional networks},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible density peak clustering for real-world data.
<em>PR</em>, <em>156</em>, 110772. (<a
href="https://doi.org/10.1016/j.patcog.2024.110772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In density based clustering, the density peak algorithm has attracted much attention due to its effectiveness and simplicity, and a vast amount of clustering approaches have been proposed based on this algorithm. Some of these works require manual selection of cluster centers with a decision graph, where human involvement leads to uncertainty in clustering results. In order to avoid human involvement, some other algorithms depend on user-specified number of clusters to determine cluster centers automatically. However, it is well known that accurate estimation of number of clusters is a long-standing difficulty in data clustering. In this paper we present a sequential density peak clustering algorithm to extract clusters one by one, thereby determining the number of clusters automatically and avoiding manual selection of cluster centers in the meanwhile. Starting from a density peak, our algorithm generates an initial cluster surrounding the density peak in the first step, and then obtains the final cluster by expanding the initial cluster based on the relative density relationship among neighboring data points. With a peeling-off strategy, we obtain all the clusters sequentially. Our algorithm works well with clusters of Gaussian distribution and is therefore potential for clustering of real-world data. Experiments with a large number of synthetic and real datasets and comparisons with existing algorithms demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Jian Hou and Houshen Lin and Huaqiang Yuan and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2024.110772},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110772},
  shortjournal = {Pattern Recognition},
  title        = {Flexible density peak clustering for real-world data},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shadow-aware decomposed transformer network for shadow
detection and removal. <em>PR</em>, <em>156</em>, 110771. (<a
href="https://doi.org/10.1016/j.patcog.2024.110771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection and removal are important yet challenging computer vision tasks. Existing methods simultaneously contend with the brightness and color information of input image while treating different regions of input images equally. We argue that these operations fail to effectively extract the relationship between shadow and non-shadow regions. To relieve these problems, this paper proposes a shadow-aware decomposed transformer network for shadow detection and removal. The network decomposes the input image into brightness and color maps using its bright channel, which are concatenated with the original image as the input data, enabling the network to pay balanced attention to the brightness and color information when calculating the relationship between regions. Additionally, given that the correlation matrix of the transformer measures the relative dependency between regions, the proposed shadow-aware transformer block can extract the relationship between shadow and non-shadow regions more effectively by retaining the specific elements of the correlation matrix. We conduct extensive experiments on three shadow detection benchmark datasets and two shadow removal benchmark datasets. Experimental results show that the proposed method performs favorably against state-of-the-art methods. Codes have been made available at https://github.com/XIAOWANG914/SADT .},
  archive      = {J_PR},
  author       = {Xiao Wang and Siyuan Yao and Yong Tang and Sili Yang and Zhenbao Liu},
  doi          = {10.1016/j.patcog.2024.110771},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110771},
  shortjournal = {Pattern Recognition},
  title        = {Shadow-aware decomposed transformer network for shadow detection and removal},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph semantic information for self-supervised monocular
depth estimation. <em>PR</em>, <em>156</em>, 110770. (<a
href="https://doi.org/10.1016/j.patcog.2024.110770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised monocular depth estimation has garnered significant attention in recent years due to its practical value in applications, as it eliminates the need for ground truth depth maps during training. However, its performance usually drops when estimating weakly textured regions and boundary regions, primarily due to the limited depth representation capability of traditional Convolutional Neural Networks (CNNs) that do not support topology. To address these issues, we propose a Graph Semantic Model (GSM) to improve self-supervised monocular depth estimation by utilizing graph learning and semantic information. Our focus is on improving feature representation through graph semantic information. Therefore, we incorporate semantic segmentation and depth estimation into one framework and enhance the interaction of different modal information through the Inter-Directed Graph Reasoning (IDGR) module. In addition, we design the Semantic-Guided Edge Graph Reasoning (SGEGR) module, aiming to boost the network&#39;s ability to perceive local depth. Extensive experiments on the KITTI dataset show that our method outperforms the state-of-the-art methods, particularly in accurately estimating depth within weakly textured regions and boundary regions.},
  archive      = {J_PR},
  author       = {Dongdong Zhang and Chunping Wang and Huiying Wang and Qiang Fu},
  doi          = {10.1016/j.patcog.2024.110770},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110770},
  shortjournal = {Pattern Recognition},
  title        = {Graph semantic information for self-supervised monocular depth estimation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable compressive sampling network with progressive
hierarchical subspace learning. <em>PR</em>, <em>156</em>, 110769. (<a
href="https://doi.org/10.1016/j.patcog.2024.110769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional compressive sampling does not sufficiently exploit the sparsity of signals to learn the sampling matrix adaptively. Moreover, they do not independently sample different frequency bands, which makes them ineffective in utilizing information from specific frequency bands. The existing deep learning-based compressive sensing methods achieve good performance with high model complexity, which limits their application to devices with low computing resources or small storage space. To address the above issues and improve the compressive sensing performance of natural images, we propose a novel scalable compressive sampling network with progressive hierarchical subspace learning (called SPHSL-CSNet) in an end-to-end mode. Specifically, the progressive hierarchical sampling strategy based on a three-level wavelet transform is presented, achieving band-separated sampling by extracting the low frequency, low-medium frequency, low-mid-second high frequency and the whole wavelet frequency band of the wavelet transform. This enables our model to obtain more image information with fewer sampling measurements and pay more attention to the reconstruction of texture details. The independent sampling of specific frequency bands is realized through the band-aware mask, which effectively reduces the parameter quantity of the sampling matrix and easier to deploy terminal devices in resource-limited scenarios. Extensive experiments on widely used benchmark datasets not only demonstrate that the proposed SPHSL-CSNet outperforms state-of-the-art performance under the premise of being lightweight, but also effective for the multispectral image compression. Furthermore, SPHSL-CSNet achieves excellent performance on antinoise performance with respect to the existing deep learning-based image CS method in most cases.},
  archive      = {J_PR},
  author       = {Zhu Yin and Zhongcheng Wu and Wuzhen Shi},
  doi          = {10.1016/j.patcog.2024.110769},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110769},
  shortjournal = {Pattern Recognition},
  title        = {Scalable compressive sampling network with progressive hierarchical subspace learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D2GL: Dual-level dual-scale graph learning for sketch-based
3D shape retrieval. <em>PR</em>, <em>156</em>, 110768. (<a
href="https://doi.org/10.1016/j.patcog.2024.110768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch-based 3D shape retrieval (SBSR) is an active research area in the computer vision community, but it is still very challenging. One main reason is that existing deep learning-based methods usually treat sketches as 2D images, neglecting the sparsity and diversity. In this paper, we propose a novel Dual-level Dual-scale Graph Learning (D 2 GL) method to effectively enhance structural information and produce robust representations for sparse and diverse hand-drawn sketches. Specifically, in addition to the traditional branches for SBSR, we introduce a Dual-level Dual-scale Graph Self-attention (DLDS-GSA) as an auxiliary branch. DLDS-GSA further consists of two levels of encoders, i.e., a local structural encoder and a dual-scale global structural encoder, to capture both local discriminative and multi-scale global structures while minimizing the impact of various sketch drawing details. Comprehensive experiments on SHREC’13 and SHREC’14 datasets demonstrate the superiority of D 2 GL for SBSR, with extended experiments on PART-SHREC’14 confirming its generalization for unseen classes in SBSR.},
  archive      = {J_PR},
  author       = {Wenjing Li and Jing Bai and Hu Zheng},
  doi          = {10.1016/j.patcog.2024.110768},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110768},
  shortjournal = {Pattern Recognition},
  title        = {D2GL: Dual-level dual-scale graph learning for sketch-based 3D shape retrieval},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding score-based representative samples for cancer risk
prediction. <em>PR</em>, <em>156</em>, 110767. (<a
href="https://doi.org/10.1016/j.patcog.2024.110767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding representative samples is important for predicting cancer risk. In particular, it is crucial to identify each representative sample as responsible for the prediction performance. In this article, we present a general framework for finding representative samples by explicitly estimating their inherit contribution levels (or scores). By leveraging explainable models as our score functions such as Shapley value, LIME and influence function, our framework can quantitatively identify the representative level of each sample in cancer risk prediction. Furthermore, a score ensembler is introduced to integrate these scores obtained from various score functions with an additional vector of weight variables optimized by the Fast Iterative Shrinkage-Thresholding Algorithm. Empirical evaluations on four cancer risk datasets with different challenges by using five classifiers suggest that our approach significantly outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jun Liao and Hao Luo and Xuewen Yan and Ting Ye and Shanshan Huang and Li Liu},
  doi          = {10.1016/j.patcog.2024.110767},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110767},
  shortjournal = {Pattern Recognition},
  title        = {Finding score-based representative samples for cancer risk prediction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A neuroinspired contrast mechanism enables few-shot object
detection. <em>PR</em>, <em>156</em>, 110766. (<a
href="https://doi.org/10.1016/j.patcog.2024.110766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based object detectors often demand abundant annotated data for training. However, in practice, only limited training data are available, making Few-Shot Object Detection (FSOD) an attractive research topic. Existing two-stage proposal-based Faster R-CNN detectors for FSOD struggle to match the performance of models trained on large datasets. We argue that detectors trained with limited samples cannot establish robust comparison-based relationships. Additionally, FSOD methods only explore these relationships during the training phase. To address these issues, we draw inspiration from neuroscience studies and propose Residual Contrast Faster R-CNN (RcFRCN). RcFRCN incorporates two novel customized contrast blocks: a Residual Spatial Contrast Block and a Residual Proposal Contrast Block. These blocks capture cross-spatial and cross-proposal contrast information, enhancing both training and testing phases. We conduct comprehensive experiments on two FSOD benchmarks: PASCAL VOC and MS-COCO. Our RcFRCN achieves a mAP of 21.9 under a 30-shot setting on MS-COCO. It also achieves AP scores of 69.1, 55.8, and 64.0 under 10-shot settings of different splits on PASCAL VOC, respectively. Moreover, we apply RcFRCN on remote sensing and use our contrast blocks for open-vocabulary detection. Experiment results on these tasks also demonstrate the robustness and generalization ability of our methods.},
  archive      = {J_PR},
  author       = {Lingxiao Yang and Dapeng Chen and Yifei Chen and Wei Peng and Xiaohua Xie},
  doi          = {10.1016/j.patcog.2024.110766},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110766},
  shortjournal = {Pattern Recognition},
  title        = {A neuroinspired contrast mechanism enables few-shot object detection},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R2C-GAN: Restore-to-classify generative adversarial networks
for blind x-ray restoration and COVID-19 classification. <em>PR</em>,
<em>156</em>, 110765. (<a
href="https://doi.org/10.1016/j.patcog.2024.110765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Restoration of poor-quality medical images with a blended set of artifacts plays a vital role in a reliable diagnosis. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). This is the first generic restoration approach forming an Image-to-Image translation task from poor-quality having noisy, blurry, or over/under-exposed images to high-quality image domain where forward and inverse transformations are learned using unpaired training samples. Simultaneously, the joint classification preserves the diagnostic-related label during restoration. Each R2C-GAN is equipped with operational layers/neurons in a compact architecture. The proposed joint model successfully restores images while achieving state-of-the-art Coronavirus Disease 2019 (COVID-19) classification with above 90% in F 1 F1 -Score. In qualitative analysis, the restoration performance is confirmed by medical doctors where 68% of the restored images are selected against the original images. We share the software implementation at https://github.com/meteahishali/R2C-GAN .},
  archive      = {J_PR},
  author       = {Mete Ahishali and Aysen Degerli and Serkan Kiranyaz and Tahir Hamid and Rashid Mazhar and Moncef Gabbouj},
  doi          = {10.1016/j.patcog.2024.110765},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110765},
  shortjournal = {Pattern Recognition},
  title        = {R2C-GAN: Restore-to-classify generative adversarial networks for blind X-ray restoration and COVID-19 classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A2GCN: Graph convolutional networks with adaptive frequency
and arbitrary order. <em>PR</em>, <em>156</em>, 110764. (<a
href="https://doi.org/10.1016/j.patcog.2024.110764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) gain remarkable success in various graph learning tasks under homophily graph assumption. This assumption is extremely fragile since real-world graphs with heterophily are ubiquitous. Under this circumstance, existing GNNs attempt to design or learn graph spectral filters for observed graphs. The representation ability of them, however, is limited due to: (1) Constant frequency response is incapable of simulating complicated filters that real-world applications require. (2) Fixed polynomial order fails to effectively uncover the node label patterns concealing different order neighborhoods. To this end, we propose a novel G raph C onvolutional N etworks with A daptive Frequency and A rbitrary Order (A2GCN) to learn various graph spectral filters suitable for distinct networks. Specifically, a simple but elegant filter with adaptive frequency response is designed to span across multiple layers for capturing different frequency components hiding in varying orders, producing A2GCN filter bases. Afterward, the coefficients of the A2GCN basis for each node are learned to achieve A2GCN filters with arbitrary polynomial order. The resulting A2GCN filters possess flexible frequency response that can automatically adapt to the node label pattern, as such, it empowers A2GCN with stronger expressiveness and naturally alleviates the over-smoothing problem. Theoretical analysis is provided to show the superiority of the proposed A2GCN. Additionally, extensive experiments on both node-level and graph-level tasks validate that the proposed A2GCN accomplishes highly competitive performance and improves classification accuracy . Codes are available at https://github.com/AIG22/A2GCN .},
  archive      = {J_PR},
  author       = {Guoguo Ai and Hui Yan and Huan Wang and Xin Li},
  doi          = {10.1016/j.patcog.2024.110764},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110764},
  shortjournal = {Pattern Recognition},
  title        = {A2GCN: Graph convolutional networks with adaptive frequency and arbitrary order},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised multi-branch network with high-frequency
enhancement for image dehazing. <em>PR</em>, <em>156</em>, 110763. (<a
href="https://doi.org/10.1016/j.patcog.2024.110763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, CycleGAN-based methods have been widely applied to the unsupervised image dehazing and achieved significant results. However, most existing CycleGAN-based methods ignore that the input of the generator contains two different distributions of data which can often lead to confusion in the learning process of the generator, consequently limiting the final dehazing performance. Moreover, reconstructing clear images through model architecture design and loss functions is an indirect constraint, making it difficult to compensate for the missing high-frequency information, such as textures and structures in the extracted features from hazy images. To address these issues, in this paper, we propose an Unsupervised Multi-Branch with High-Frequency Enhancement Network (UME-Net) which contain an Multi-Branch Dehazing Network (MBDN) and a High-Frequency Components Enhancement Module (HFEM). Specifically, MBDN constructs a single unsupervised dehazing network with Shared Encoding Module (SEM) and Multi-Branch Decoding Module (MDM). SEM enhance the consistency of feature representation and MDM effectively addresses the confusion during the generator learning process in CycleGAN-based methods. Furthermore, based on a key observation that hazy images and their corresponding clear images exhibit only subtle differences in high-frequency information, the HFEM is designed to compensates for the missing high-frequency information in the network which further enhances the reconstruction capability of the UME-Net for restore edge and texture information in obscured by dense haze. Experimental results on challenging benchmark datasets demonstrate the superiority of our UME-Net over SOTA unsupervised image dehazing methods. The source code is available at https://www.github.com/thislzm/UME-Net .},
  archive      = {J_PR},
  author       = {Hang Sun and Zhiming Luo and Dong Ren and Bo Du and Laibin Chang and Jun Wan},
  doi          = {10.1016/j.patcog.2024.110763},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110763},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised multi-branch network with high-frequency enhancement for image dehazing},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Back-to-bones: Rediscovering the role of backbones in domain
generalization. <em>PR</em>, <em>156</em>, 110762. (<a
href="https://doi.org/10.1016/j.patcog.2024.110762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and little to no attention has been given to the effects of different backbones yet. In this paper, we go “back to the backbones”, proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been overlooked by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain classification accuracy and DG capability. Our extensive experimentation shows that by adopting competitive backbones in conjunction with effective data augmentation, plain ERM outperforms recent DG solutions and achieves state-of-the-art accuracy. Moreover, our additional qualitative studies reveal that novel backbones give more similar representations to same-class samples, separating different domains in the feature space. This boost in generalization capabilities leaves marginal room for DG algorithms. It suggests a new paradigm for investigating the problem, placing backbones in the spotlight and encouraging the development of consistent algorithms on top of them. The code is available at https://github.com/PIC4SeR/Back-to-Bones .},
  archive      = {J_PR},
  author       = {Simone Angarano and Mauro Martini and Francesco Salvetti and Vittorio Mazzia and Marcello Chiaberge},
  doi          = {10.1016/j.patcog.2024.110762},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110762},
  shortjournal = {Pattern Recognition},
  title        = {Back-to-bones: Rediscovering the role of backbones in domain generalization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complementary pseudo multimodal feature for point cloud
anomaly detection. <em>PR</em>, <em>156</em>, 110761. (<a
href="https://doi.org/10.1016/j.patcog.2024.110761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud anomaly detection is steadily emerging as a promising research area. Recognizing the importance of feature descriptiveness in this task, this study introduces the Complementary Pseudo Multimodal Feature (CPMF), which combines local geometrical information extracted by 3D handcrafted descriptors with global semantic information extracted from 2D pre-trained neural networks. Specifically, to leverage 2D pre-trained neural networks for point-wise feature extraction, this study projects original point clouds into multi-view images. These images are then fed into a pre-trained 2D neural network for informative 2D modality feature extraction. Following the 2D–3D correspondence, the multi-view 2D modality features are projected back to 3D space and aggregated to obtain point-wise 2D modality features. Finally, the point-wise 3D and 2D modality features are fused to derive the CPMF for point cloud anomaly detection. Extensive experiments conducted on MVTec 3D and Real3D datasets demonstrate the complementary capacity between 2D and 3D modality features and the effectiveness of CPMF. Notably, CPMF achieves a significantly higher object-level AUROC of 95.15% compared to other methods on the MVTec 3D benchmark. Code is available at https://github.com/caoyunkang/CPMF .},
  archive      = {J_PR},
  author       = {Yunkang Cao and Xiaohao Xu and Weiming Shen},
  doi          = {10.1016/j.patcog.2024.110761},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110761},
  shortjournal = {Pattern Recognition},
  title        = {Complementary pseudo multimodal feature for point cloud anomaly detection},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascaded learning with transformer for simultaneous eye
landmark, eye state and gaze estimation. <em>PR</em>, <em>156</em>,
110760. (<a href="https://doi.org/10.1016/j.patcog.2024.110760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye tracking have garnered attention in human–machine interaction, disease monitoring, biometrics, etc. Existing investigations for eye tracking have predominantly concentrated on individual task for pupil detection or gaze estimation, overlooking the implicit relationships that exist among different tasks for eye tracking. In this work, we introduce a cascaded framework with transformer to collaboratively realize eye landmark detection, eye state detection and gaze estimation. Within our framework, we leverage Transformer to capture long dependencies with explicit eye-related structural information and implicit correlation among different tasks. Furthermore, the proposed cascade iteration framework alternatively optimize each task and boost the overall performance for pupil center, eye state and gaze estimation simultaneously. To address the problem of manual annotation, we further introduce the Control-Eye Diffusion Model (CEDM), a controllable eye image generation method conditioned on a simple contour with structure information. The proposed methods are evaluated on challenging datasets such as GI4E, BioID and MPIIGaze, and the results show that our methods outperform state-of-the-art methods in several tasks.},
  archive      = {J_PR},
  author       = {Chao Gou and Yuezhao Yu and Zipeng Guo and Chen Xiong and Ming Cai},
  doi          = {10.1016/j.patcog.2024.110760},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110760},
  shortjournal = {Pattern Recognition},
  title        = {Cascaded learning with transformer for simultaneous eye landmark, eye state and gaze estimation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating texture and silhouette for video-based person
re-identification. <em>PR</em>, <em>156</em>, 110759. (<a
href="https://doi.org/10.1016/j.patcog.2024.110759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Silhouette is an effective modality in video-based person re-identification (ReID) since it contains features ( e . g ., stature and gait) complementary to the RGB modality. However, recent silhouette-assisted methods have not fully explored the spatial–temporal relations within each modality or considered the cross-modal complementarity in fusion. To address these two issues, we propose a Complete Relational Framework that includes two key components. The first component, Spatial-Temporal Relational Module (STRM), explores the spatiotemporal relations. STRM decomposes the video’s spatiotemporal context into local/fine-grained and global/semantic aspects, modeling them sequentially to enhance the representation of each modality. The second component, Modality-Channel Relational Module (MCRM), explores the complementarity between RGB and silhouette videos. MCRM aligns two modalities semantically and multiplies them to capture complementary interrelations. With these two modules focusing on intra- and cross-modal relationships, our method achieves superior results across multiple benchmarks with minimal additional parameters and FLOPs. Code and models are available at https://github.com/baist/crf .},
  archive      = {J_PR},
  author       = {Shutao Bai and Hong Chang and Bingpeng Ma},
  doi          = {10.1016/j.patcog.2024.110759},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110759},
  shortjournal = {Pattern Recognition},
  title        = {Incorporating texture and silhouette for video-based person re-identification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient neural implicit representation for 3D human
reconstruction. <em>PR</em>, <em>156</em>, 110758. (<a
href="https://doi.org/10.1016/j.patcog.2024.110758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity digital human representations are increasingly in demand in the digital world, particularly for interactive telepresence , AR/VR, 3D graphics , and the rapidly evolving metaverse . Even though they work well in small spaces, conventional methods for reconstructing 3D human motion frequently require the use of expensive hardware and have high processing costs. This study presents HumanAvatar, an innovative approach that efficiently reconstructs precise human avatars from monocular video sources. At the core of our methodology, we integrate the pre-trained HuMoR, a model celebrated for its proficiency in human motion estimation. This is adeptly fused with the cutting-edge neural radiance field technology, Instant-NGP, and the state-of-the-art articulated model, Fast-SNARF, to enhance the reconstruction fidelity and speed. By combining these two technologies, a system is created that can render quickly and effectively while also providing estimation of human pose parameters that are unmatched in accuracy. We have enhanced our system with an advanced posture-sensitive space reduction technique, which optimally balances rendering quality with computational efficiency. In our detailed experimental analysis using both artificial and real-world monocular videos, we establish the advanced performance of our approach. HumanAvatar consistently equals or surpasses contemporary leading-edge reconstruction techniques in quality. Furthermore, it achieves these complex reconstructions in minutes, a fraction of the time typically required by existing methods. Our models achieve a training speed that is 110 × faster than that of State-of-The-Art (SoTA) NeRF-based models. Our technique performs noticeably better than SoTA dynamic human NeRF methods if given an identical runtime limit. HumanAvatar can provide effective visuals after only 30 s of training. Please visit https://github.com/HZXu-526/Human-Avatar for further demo results and code.},
  archive      = {J_PR},
  author       = {Zexu Huang and Sarah Monazam Erfani and Siying Lu and Mingming Gong},
  doi          = {10.1016/j.patcog.2024.110758},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110758},
  shortjournal = {Pattern Recognition},
  title        = {Efficient neural implicit representation for 3D human reconstruction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative atoms embedding relation dual network for
classification of choroidal neovascularization in OCT images.
<em>PR</em>, <em>156</em>, 110757. (<a
href="https://doi.org/10.1016/j.patcog.2024.110757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choroidal neovascularization (CNV) is an eye disease that can cause vision loss. Automatic CNV classification in OCT images is crucial in the treatment of CNV. However, two problems arise for CNV classification in OCT images. The subtle visual differences between different CNV types render classification difficult. Additionally, it is difficult to obtain sufficient labeled data, which results in performance degradation. In order to solve these two problems, a discriminative atom-embedding relation dual network is proposed in this paper. Considering that semi-supervised learning (SSL) is an effective machine learning framework to make full use of limited labeled data and a large amount of unlabeled data, the proposed network is developed within an SSL framework. To capture the visual differences, novel discriminative atoms are first introduced to mine discriminative information between different CNV types. Subsequently, a relation module is incorporated to embed the learned discriminative atom information into the features. This makes the learned features capable of distinguishing between different CNV types. Moreover, a novel relation consistency loss is proposed to further improve the robustness of the learned features. Experimental results on private and public datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Ruifeng Wang and Guang Zhang and Xiaoming Xi and Longsheng Xu and Xiushan Nie and Jianhua Nie and Xianjing Meng and Yanwei Zhang and Xinjian Chen and Yilong Yin},
  doi          = {10.1016/j.patcog.2024.110757},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110757},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative atoms embedding relation dual network for classification of choroidal neovascularization in OCT images},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dehazing &amp; reasoning YOLO: Prior knowledge-guided
network for object detection in foggy weather. <em>PR</em>,
<em>156</em>, 110756. (<a
href="https://doi.org/10.1016/j.patcog.2024.110756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast and accurate object detection in foggy weather is crucial for visual tasks such as autonomous driving and video surveillance. Existing methods typically preprocess images with enhancement techniques before the object detector, so that the real-time performance of object detection decreases to some extent. Meanwhile, many popular object detection models rely solely on visual features for localization and classification. When fog is present, visual features would be so adversely impacted that the detection accuracy sharply decreases. Therefore, we propose an end-to-end prior knowledge-guided network called DR-YOLO for object detection in foggy weather. DR-YOLO integrates the atmospheric scattering model and the co-occurrence relation graph as prior knowledge into the entire training process of the detector. Firstly, Restoration Subnet Module (RSM) is designed to employ the atmospheric scattering model to guide the learning direction of the detector for dehazing features. Specifically, it is only adopted during the training process and does not increase the time cost of detection process. Secondly, for guiding the detector to pay more attention to potential co-occurring objects in the same scene, we introduce Relation Reasoning Attention Module (RRAM) that utilizes the co-occurrence relation graph to supplement deficient visual features in foggy weather. In addition, DR-YOLO employs Adaptive Feature Fusion Module (AFFM) to effectively merge the key features from the backbone and neck for the needs of RRAM and RSM. Finally, we conduct experiments on clear, synthetic and real-world foggy datasets to demonstrate the effectiveness of DR-YOLO. The source code is available at https://github.com/wenxinss/DR-YOLO .},
  archive      = {J_PR},
  author       = {Fujin Zhong and Wenxin Shen and Hong Yu and Guoyin Wang and Jun Hu},
  doi          = {10.1016/j.patcog.2024.110756},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110756},
  shortjournal = {Pattern Recognition},
  title        = {Dehazing &amp; reasoning YOLO: Prior knowledge-guided network for object detection in foggy weather},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FeverNet: Enabling accurate and robust remote fever
screening. <em>PR</em>, <em>156</em>, 110755. (<a
href="https://doi.org/10.1016/j.patcog.2024.110755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote human fever screening via thermal infrared imaging helps reduce the risk of respiratory disease transmission and plays an important role in public health monitoring. However, the accuracy of such systems often falls prey to variations in measurement distance and environment temperature. Most previous methods tend to employ sensors to overcome these variations, which are expensive schemes and have limited performance improvement. To address above problems, this paper presents a novel and robust remote fever screening framework named FeverNet. Specifically, FeverNet introduces depth estimation network and temperature distribution constraints across time periods to reduce the influence of distance variations and environment temperature changes. The fever attention module is thus proposed to enhance feature representation and expand the difference between fever faces and normal ones. In addition, we provide the Extended Thermal Infrared Face dataset (ETIF), which further gives visible images (paired with thermal infrared images) for depth estimation and improve the fever face generated method based on the maximum temperature of the face. Extensive experiments on ETIF demonstrate the advantages of our FeverNet over the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Mengkai Yan and Jianjun Qian and Hang Shao and Lei Luo and Jian Yang},
  doi          = {10.1016/j.patcog.2024.110755},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110755},
  shortjournal = {Pattern Recognition},
  title        = {FeverNet: Enabling accurate and robust remote fever screening},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-guided distillation learning to diversify video
embeddings for text-video retrieval. <em>PR</em>, <em>156</em>, 110754.
(<a href="https://doi.org/10.1016/j.patcog.2024.110754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional text-video retrieval methods typically match a video with a text on a one-to-one manner. However, a single video can contain diverse semantics, and text descriptions can vary significantly. Therefore, such methods fail to match a video with multiple texts simultaneously. In this paper, we propose a novel approach to tackle this one-to-many correspondence problem in text-video retrieval. We devise diverse temporal aggregation and a multi-key memory to address temporal and semantic diversity, consequently constructing multiple video embedding paths from a single video. Additionally, we introduce text-guided distillation learning that enables each video path to acquire meaningful distinct competencies in representing varied semantics. Our video embedding approach is text-agnostic, allowing the prepared video embeddings to be used continuously for any new text query. Experiments show our method outperforms existing methods on four datasets. We further validate the effectiveness of our designs with ablation studies and analyses on diverse video embeddings.},
  archive      = {J_PR},
  author       = {Sangmin Lee and Hyung-Il Kim and Yong Man Ro},
  doi          = {10.1016/j.patcog.2024.110754},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110754},
  shortjournal = {Pattern Recognition},
  title        = {Text-guided distillation learning to diversify video embeddings for text-video retrieval},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view hypergraph regularized lp norm least squares twin
support vector machines for semi-supervised learning. <em>PR</em>,
<em>156</em>, 110753. (<a
href="https://doi.org/10.1016/j.patcog.2024.110753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-view semi-supervised learning has gradually become a popular research direction. The classic binary classification methods in this field are multi-view Laplacian support vector machines (MvLapSVM) and multi-view Laplacian twin support vector machines (MvLapTSVM), which extend semi-supervised support vector machine to multi-view learning. Nevertheless, similar to the majority of SVM-based multi-view methods, the above methods are two-view methods that cannot fully leverage the information from all views and are constructed based on the L2 norm. Additionally, in semi-supervised graph learning, the quality of the graph often has a significant impact on the results. Therefore, we propose a novel multi-view hypergraph regularized Lp norm least squares twin support vector machines (MvHGLpLSTSVM) that can handle general multi-view data for semi-supervised learning. It extends hypergraph learning to multi-view learning and combines Lp norm to further explore the manifold structure and embedded geometric information of multi-view data. By using equality constraints, we design a simple and effective iterative algorithm. In the classification of six multi-view datasets, we compare the proposed method with some other state-of-the-art methods, and the results show that the proposed method is effective.},
  archive      = {J_PR},
  author       = {Junqi Lu and Xijiong Xie and Yujie Xiong},
  doi          = {10.1016/j.patcog.2024.110753},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110753},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view hypergraph regularized lp norm least squares twin support vector machines for semi-supervised learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and lightweight convolutional neural network
architecture search methods for object classification. <em>PR</em>,
<em>156</em>, 110752. (<a
href="https://doi.org/10.1016/j.patcog.2024.110752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the architecture of deep learning models is a complex task. Several automated search techniques have been proposed, but these methods typically require high-performance graphics processing units (GPUs), manual parameter adjustments, and specific training approaches. This study introduces an efficient, lightweight convolutional neural network architecture search approach tailored for object classification. It features an optimized search space design and a novel controller design. This study introduces a refined search space design incorporating optimizations in both spatial and operational aspects. The focus is on the synergistic integration of convolutional units, dimension reduction units, and the stacking of Convolutional Neural Network (CNN) architectures. To enhance the search space, ShuffleNet modules are integrated, reducing the number of parameters and training time. Additionally, BlurPool is implemented in the dimension reduction unit operation to achieve translational invariance, alleviate the gradient vanishing problem, and optimize unit compositions. Moreover, an innovative controller model, Stage LSTM, is proposed based on Long Short-Term Memory (LSTM) to generate lightweight architectural sequences. In conclusion, the refined search space design and the Stage LSTM controller model are synergistically combined to establish an efficient and lightweight architecture search technique termed Stage and Lightweight Network Architecture Search (SLNAS). The experimental results highlight the superior performance of the optimized search space design, primarily when implemented with the Stage LSTM controller model. This approach shows significantly improved accuracy and stability compared to random, traditional LSTM, and Genetic Algorithm (GA) controller models, with statistically significant differences. Notably, the Stage LSTM controller excels in accuracy while producing models with fewer parameters within the expanded architecture search space. The study adopts the Stage LSTM controller model due to its ability to approximate optimal sequence structures, particularly when combined with the optimized search space design, referred to as SLNAS. SLNAS&#39;s performance is evaluated through experiments and comparisons with other Neural Architecture Search (NAS) and object classification methods from different researchers. These experiments consider model parameters, hardware resources, model stability, and multiple datasets. The results show that SLNAS achieves a low error rate of 2.86 % on the CIFAR-10 dataset after just 0.2 days of architecture search, matching the performance of manually designed models but using only 2 % of the parameters. SLNAS consistently demonstrates robust performance across various image classification domains, with an approximate parameter count 700,000. To summarize, SLNAS emerges as a highly effective automated network architecture search method tailored for image classification. It streamlines the model design process, making it accessible to researchers without specialized knowledge in deep learning. Optimizing this method unlocks the full potential of deep learning across diverse research areas. Interested parties can publicly access the source code and pre-trained models through the following link: https://github.com/huanyu-chen/LNASG-and-SLNAS-model .},
  archive      = {J_PR},
  author       = {Chuen-Horng Lin and Tsung-Yi Chen and Huan-Yu Chen and Yung-Kuan Chan},
  doi          = {10.1016/j.patcog.2024.110752},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110752},
  shortjournal = {Pattern Recognition},
  title        = {Efficient and lightweight convolutional neural network architecture search methods for object classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised clustering guided by pairwise constraints
and local density structures. <em>PR</em>, <em>156</em>, 110751. (<a
href="https://doi.org/10.1016/j.patcog.2024.110751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering based on local density peaks and graph cut (LDP-SC) is one of the state-of-the-art algorithms in unsupervised clustering, which first divides the data set to be multiple local trees, and then aggregates these local trees to obtain the final clustering result. However, for complex data sets, there might exist data points from different classes in the same local tree. In this article, we use pairwise constraint information to resolve this issue and propose a semi-supervised local density peaks and graph cut based clustering algorithm (SLDPC). In particular, SLDPC proposes intra-cluster conflict resolution and inter-cluster conflict resolution steps to split the local trees which are inconsistent with the provided pairwise constraint information. Theoretically, we show that the two steps will finish in a finite number of operations and the split local trees will be consistent with the pairwise constraint information. Subsequently, root node redirection and noise filtering steps are designed to avoid the local trees becoming too fragmented. Finally, we exploit the E2CP algorithm to further improve the similarity matrix between local trees using the pairwise constraint information, and the spectral clustering algorithm is adopted to obtain the clustering result. Experiments on multiple widely used synthetic and real-world data sets show that SLDPC is superior to LDP-SC and several other semi-supervised prominent clustering algorithms for most of the cases.},
  archive      = {J_PR},
  author       = {Zhiguo Long and Yang Gao and Hua Meng and Yuxu Chen and Hui Kou},
  doi          = {10.1016/j.patcog.2024.110751},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110751},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised clustering guided by pairwise constraints and local density structures},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype rectification for zero-shot learning. <em>PR</em>,
<em>156</em>, 110750. (<a
href="https://doi.org/10.1016/j.patcog.2024.110750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to recognize unseen objects is the goal of zero-shot learning (ZSL), building on transferring the class-level semantic descriptions. Previous methods devote to bridging the instance-level objects with class-level semantics through feature generation or co-embedding, neglecting prototype-level and distribution-level associations, which is not conducive to narrowing the visual-semantic gap. This paper yields a novel prototype rectification framework for ZSL, termed PRZSL, which is dedicated to learning and calibrating the dual prototype distributions in a meta-domain. We first propose a contrastive embedding module with a compatibility loss and an angular loss to make inter-class prototypes well-separated. We further collaboratively rectify the dual prototypes by injecting the prototype distribution information of another modality, boosting the visual-semantic alignment at the distribution level. Unlike previous methods that anchor the semantic position, semantic prototypes also participate in collaborative updates, thereby promoting alignment from semantic to vision. Comprehensive experimental results on five zero-shot benchmarks demonstrate that our proposed method can achieve competitive performance compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yuanyuan Yi and Guolei Zeng and Bocheng Ren and Laurence T. Yang and Bin Chai and Yuxin Li},
  doi          = {10.1016/j.patcog.2024.110750},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110750},
  shortjournal = {Pattern Recognition},
  title        = {Prototype rectification for zero-shot learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DACBN: Dual attention convolutional broad network for
fine-grained visual recognition. <em>PR</em>, <em>156</em>, 110749. (<a
href="https://doi.org/10.1016/j.patcog.2024.110749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is a challenging task due to its small inter-class differences and large intra-class differences. Most existing methods rely on manual labeling of key identification areas, which requires high labor costs. In addition, existing methods also tend to ignore the differences effect of different feature channels in the feature map, which has a certain impact on the model classification accuracy. To solve the above problems, this paper proposes a dual attention convolutional broad network. Firstly, a new dual attention mechanism is designed to suppress the background noise of fine-grained images and give greater weight to the discriminative feature regions and channels. Secondly, the ensemble broad learning system framework is used to further enhance the dual attention features, so that the discriminative features can further improve the recognition ability of the model. Finally, by multiple comparative experiments, it is reported that the method proposed in this article has achieved excellent recognition results on three commonly used datasets.},
  archive      = {J_PR},
  author       = {Tao Chen and Lijie Wang and Yang Liu and Haisheng Yu},
  doi          = {10.1016/j.patcog.2024.110749},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110749},
  shortjournal = {Pattern Recognition},
  title        = {DACBN: Dual attention convolutional broad network for fine-grained visual recognition},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CORE: Learning consistent ordinal representations with
convex optimization for image ordinal estimation. <em>PR</em>,
<em>156</em>, 110748. (<a
href="https://doi.org/10.1016/j.patcog.2024.110748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image ordinal estimation is to estimate the ordinal label of a given image. Existing methods primarily rely on ordinal regression, mapping feature representations directly to ordinal labels. However, these methods often struggle to preserve the inherent order within the learned feature representations. To this end, this paper proposes learning intrinsic Consistent Ordinal REpresentations (CORE), a novel approach that learns intrinsic ordinal relationships directly from ground-truth labels. First, it constructs an ordinal manifold using an ordinal totally ordered set ( toset ) distribution (OTD), capturing the inherent order of labels while regularizing feature embeddings. Second, the CORE leverages the toset distribution to convert both feature representations and labels into a unified embedding space, enabling consistent manifold alignment. Third, CORE employs an ordinal prototype-constrained convex programming formulation with dual decomposition, minimizing the Kullback–Leibler (KL) divergence between the toset distributions of labels and feature representations. Extensive experiments demonstrate that CORE, when combined with existing deep ordinal regression methods, significantly improves their performance in preserving ordinal relationships and achieves superior quantitative results across four real-world scenarios.},
  archive      = {J_PR},
  author       = {Yiming Lei and Zilong Li and Yangyang Li and Junping Zhang and Hongming Shan},
  doi          = {10.1016/j.patcog.2024.110748},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110748},
  shortjournal = {Pattern Recognition},
  title        = {CORE: Learning consistent ordinal representations with convex optimization for image ordinal estimation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-training-transductive-learning broad learning system
(STTL-BLS): A model for effective and efficient image classification.
<em>PR</em>, <em>156</em>, 110747. (<a
href="https://doi.org/10.1016/j.patcog.2024.110747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel model called Self-Training-Transductive-Learning Broad Learning System (STTL-BLS) is proposed for image classification. The model consists of two key blocks: Feature Block (FB) and Enhancement Block (EB). The FB utilizes the Proportion of Large Values Attention (PLVA) technique and an Encoder for feature extraction. Multiple FBs are cascaded in the model to learn discriminative features. The Enhancement Block (EB) enhances feature learning and prevents under-fitting on complex datasets. Additionally, an architecture that combines characteristics of Broad Learning System (BLS) and gradient descent is designed for STTL-BLS, enabling the model to leverage the advantages of both BLS and Convolutional Neural Networks (CNNs). Moreover, a training algorithm (STTL) that combines self-training and transductive learning is presented for the model to improve its generalization ability. Experimental results demonstrate that the accuracy of the proposed model surpasses all compared BLS variants and performs comparably or even superior to deep networks: on small-scale datasets, STTL-BLS has an average accuracy improvement of 14.82 percentage points compared to other models; on large-scale datasets, 12.95 percentage points. Notably, the proposed model exhibits low time complexity, particularly with the shortest testing time on the small-scale datasets among all compared models: it has an average testing time of 46.4 s less than other models. It proves to be an additional valuable solution for image classification tasks on both small- and large-scale datasets. The source code for this paper can be accessed at https://github.com/threedteam/sttl_bls .},
  archive      = {J_PR},
  author       = {Lin Yi and Di Lv and Dinghao Liu and Suhuan Li and Ran Liu},
  doi          = {10.1016/j.patcog.2024.110747},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110747},
  shortjournal = {Pattern Recognition},
  title        = {Self-training-transductive-learning broad learning system (STTL-BLS): A model for effective and efficient image classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A restarted large-scale spectral clustering with
self-guiding and block diagonal representation. <em>PR</em>,
<em>156</em>, 110746. (<a
href="https://doi.org/10.1016/j.patcog.2024.110746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering, a prominent unsupervised machine learning method, involves a critical task of constructing a similarity matrix. In existing approaches, this matrix is either computed once for all or updated alternatively. However, the former struggles to capture comprehensive relationships among data points, and the latter is often impractical for large-scale problems. This study introduces a new clustering framework with self-guidance and a block diagonal representation, retaining valuable information from previous cycles. To our knowledge, this is the first application of such a framework to spectral clustering, with a key distinction being the reclassification of samples in each cycle. To reduce computational overhead, we employ a block diagonal representation with Nyström approximation for constructing the similarity matrix. Theoretical results justify the rationality of approximate computations in spectral clustering. Comprehensive experiments on benchmark datasets demonstrate the superiority of our proposed algorithms over state-of-the-art methods for large-scale clustering. Notably, our framework has the potential to enhance clustering algorithms, performing well even with a randomly chosen initial guess.},
  archive      = {J_PR},
  author       = {Yongyan Guo and Gang Wu},
  doi          = {10.1016/j.patcog.2024.110746},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110746},
  shortjournal = {Pattern Recognition},
  title        = {A restarted large-scale spectral clustering with self-guiding and block diagonal representation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global contrast-masked autoencoders are powerful
pathological representation learners. <em>PR</em>, <em>156</em>, 110745.
(<a href="https://doi.org/10.1016/j.patcog.2024.110745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using digital pathology slide scanning technology, artificial intelligence algorithms, particularly deep learning, have achieved significant results in the field of computational pathology. Compared to other medical images, pathology images are more difficult to annotate, and thus, there is an extreme lack of available datasets for conducting supervised learning to train robust deep learning models. In this paper, we introduce a self-supervised learning (SSL) model, the Global Contrast-masked Autoencoder (GCMAE), designed to train encoders to capture both local and global features of pathological images and significantly enhance the performance of transfer learning across datasets. Our study demonstrates the capability of the GCMAE to learn transferable representations through extensive experiments on three distinct disease-specific hematoxylin and eosin (H&amp;E)-stained pathology datasets: Camelyon16, NCT-CRC, and BreakHis. Moreover, we propose an effective automated pathology diagnosis process based on the GCMAE for clinical applications. The source code of this paper is publicly available at https://github.com/StarUniversus/gcmae .},
  archive      = {J_PR},
  author       = {Hao Quan and Xingyu Li and Weixing Chen and Qun Bai and Mingchen Zou and Ruijie Yang and Tingting Zheng and Ruiqun Qi and Xinghua Gao and Xiaoyu Cui},
  doi          = {10.1016/j.patcog.2024.110745},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110745},
  shortjournal = {Pattern Recognition},
  title        = {Global contrast-masked autoencoders are powerful pathological representation learners},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethink video retrieval representation for video captioning.
<em>PR</em>, <em>156</em>, 110744. (<a
href="https://doi.org/10.1016/j.patcog.2024.110744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning, a challenging task targeting the automatic generation of accurate and comprehensive descriptions based on video content, has witnessed substantial success recently driven by bridging video representations and textual semantics. Inspired by the nature of the video retrieval task, which learns visual features strongly related to text queries, we propose to take advantage of visual representation learning from the video retrieval framework to tackle video captioning tasks and construct adequate multi-grained cross-modal matching while extracting visual features. However, a simple direct application of recent video retrieval models fails to capture sufficient temporal details and the rich visual features of local patch tokens of video frames lack semantic information essential for captioning tasks. These deficiencies are primarily due to these models lack fine-grained interactions between video frames and offer only weak textual supervision over frame patch tokens. To increase the attention on temporal details, we propose a learnable token shift module, which flexibly captures subtle movements in local regions across the temporal sequence. Furthermore, we devise a Refineformer, which learns to integrate local video patch tokens strongly related to desired captions via a cross-attention mechanism. Extensive experiments on MSVD, MSR-VTT and VATEX demonstrate the favorable performance of our method. Code will be available at https://github.com/tiesanguaixia/IVRC .},
  archive      = {J_PR},
  author       = {Mingkai Tian and Guorong Li and Yuankai Qi and Shuhui Wang and Quan Z. Sheng and Qingming Huang},
  doi          = {10.1016/j.patcog.2024.110744},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110744},
  shortjournal = {Pattern Recognition},
  title        = {Rethink video retrieval representation for video captioning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Restoring vision in rain-by-snow weather with simple
attention-based sampling cross-hierarchy transformer. <em>PR</em>,
<em>156</em>, 110743. (<a
href="https://doi.org/10.1016/j.patcog.2024.110743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an unnoticed specialized task in image restoration, rain-by-snow weather removal aims to eliminate the complicated coexisting rain streaks and snow particles. In this work, we propose a simple attention-based sampling cross-hierarchy Transformer (SASCFormer). Initially, we explore the proximity of convolution network and Transformer in hierarchical architectures and experimentally find they perform approximately for intra-stage feature representation. On this basis, we utilize a Transformer-like convolution block (TCB) to replace the computation-heavy self-attention while preserving the attention characteristics for adapting to the input content. Meanwhile, we demonstrate that cross-stage sampling progression is critical for the performance improvement in rain-by-snow weather removal, and propose a global–local self-attention sampling mechanism (GLSASM) that samples the features while preserving both the global and local dependencies. Finally, we synthesize two novel rain-by-snow weather-degraded benchmarks, RSCityscapes and RS100K datasets. Extensive experiments verify that our proposed SASCFormer achieves the best trade-off between the performance and inference time. In particular, our approach advances existing methods by 1 . 14 dB ∼ 4 . 89 dB in peak signal-to-noise ratio. Related resources are available at https://github.com/chdwyb/Rain-by-snow .},
  archive      = {J_PR},
  author       = {Yuanbo Wen and Tao Gao and Kaihao Zhang and Peng Cheng and Ting Chen},
  doi          = {10.1016/j.patcog.2024.110743},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110743},
  shortjournal = {Pattern Recognition},
  title        = {Restoring vision in rain-by-snow weather with simple attention-based sampling cross-hierarchy transformer},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-contrastive feature guidance based multidimensional
collaborative network of metadata and image features for skin disease
classification. <em>PR</em>, <em>156</em>, 110742. (<a
href="https://doi.org/10.1016/j.patcog.2024.110742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both clinical images and metadata are the foundation of clinical diagnosis, effectively fusing these two resources is a major difficulty in the detection of skin cancer. Even though existing fusion methods produced better fusion outcomes, they only carried out single-level fusion prior to making decisions and used distinct feature extraction for each modal data. The ability of inter-modal synergy is diminished by this fusion strategy, resulting in coarse fusion features. To enhance the multidimensional representation of images, we suggest a Self-contrastive Feature Guidance Based Multidimensional Collaborative Network (SGMC Net). Specifically, we split the fusion method into three steps: spatial dimension fusion, channel dimension fusion, and adaptive corrective outputting to establish multidimensional collaboration between metadata and image features in the feature extraction process. Accordingly, we build three blocks: channel fusion block, spatial fusion block, and feature rectification block. On this basis, we propose a Self-contrastive Feature Guidance method that utilizes the contrast loss between shallow and deep features of the image as a supervisory signal in a non-enhanced manner to optimize shallow features. Finally, extensive experiments were conducted on PAD-UFES-20 and Der7pt dataset, our method achieved an accuracy of 83.3% beyond other state-of-the-art models. We further validated the effectiveness of the feature guidance method, showing a 5.2% improvement in accuracy for SGMC18.},
  archive      = {J_PR},
  author       = {Feng Li and Min Li and Enguang Zuo and Chen Chen and Cheng Chen and Xiaoyi Lv},
  doi          = {10.1016/j.patcog.2024.110742},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110742},
  shortjournal = {Pattern Recognition},
  title        = {Self-contrastive feature guidance based multidimensional collaborative network of metadata and image features for skin disease classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CL-TransFER: Collaborative learning based transformer for
facial expression recognition with masked reconstruction. <em>PR</em>,
<em>156</em>, 110741. (<a
href="https://doi.org/10.1016/j.patcog.2024.110741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has attracted intensive attention due to its critical role in various computer vision tasks. However, existing FER approaches suffer from either noisy annotations or expression ambiguity (high inter-class and low intra-class similarity), limiting the FER performance. To this end, we propose a robust end-to-end collaborative learning based transformer for FER (CL-TransFER) in this paper. Specifically, CL-TransFER co-trains a CNN feature extractor and a transformer feature extractor jointly to extract both rich local semantic features as well as global structural information from facial images. By enforcing the consensus between the predictions of two extractors, the CL-TransFER could suppress the influence of noisy annotations. To further tackle the expression ambiguity problem, we design a simple yet efficient self-supervised masked reconstruction (SSMR) task to pre-train the transformer feature extractor of CL-TransFER. This enhances the model&#39;s capability of learning fine-grained discriminative representations. Extensive experiments on three popular benchmarks have demonstrated the effectiveness and superiority of our method.},
  archive      = {J_PR},
  author       = {Yujie Yang and Lin Hu and Chen Zu and Jianjia Zhang and Yun Hou and Ying Chen and Jiliu Zhou and Luping Zhou and Yan Wang},
  doi          = {10.1016/j.patcog.2024.110741},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110741},
  shortjournal = {Pattern Recognition},
  title        = {CL-TransFER: Collaborative learning based transformer for facial expression recognition with masked reconstruction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosted multilayer feedforward neural network with multiple
output layers. <em>PR</em>, <em>156</em>, 110740. (<a
href="https://doi.org/10.1016/j.patcog.2024.110740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces the Boosted Ensemble deep Multi-Layer Layer Perceptron (EdMLP) architecture with multiple output layers, a novel enhancement for the traditional Multi-Layer Perceptron (MLP). By adopting a layer-wise training approach, EdMLP enables the integration of boosting techniques within a single model, treating each layer as a weak learner, resulting in substantial performance gains. Additionally, the inclusion of layer-wise hyperparameter tuning allows optimization of individual layers thereby reducing the tuning time. Furthermore, the ensemble deep architecture’s versatility can be extended to other neural network-based models, such as the Self Normalized Network (SNN) where experiments demonstrate substantial performance enhancements yielded by the EdSNN compared to the standard original SNN model. This research underscores the potential of the EdMLP, and the Ed architecture in general as a powerful tool for improving the performance of various multilayer feedforward neural network models. The source code of this work is publicly accessible from the authors GitHub.},
  archive      = {J_PR},
  author       = {Hussein Aly and Abdulaziz K. Al-Ali and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.patcog.2024.110740},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110740},
  shortjournal = {Pattern Recognition},
  title        = {Boosted multilayer feedforward neural network with multiple output layers},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multilayer bootstrap networks in ensemble for
unsupervised representation learning and clustering. <em>PR</em>,
<em>156</em>, 110739. (<a
href="https://doi.org/10.1016/j.patcog.2024.110739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is known that unsupervised nonlinear learning is sensitive to the selection of hyperparameters, which hinders its practical use. How to determine the optimal hyperparameter setting that may be dramatically different across applications is a hard issue. In this paper, we aim to address this issue for multilayer bootstrap networks (MBN), a recent unsupervised model, in a way as simple as possible. Specifically, we first propose an MBN ensemble (MBN-E) algorithm which concatenates the sparse outputs of a set of MBN base models with different network structures into a new representation. Then, we take the new representation produced by MBN-E as a reference for selecting the optimal MBN base models. Moreover, we propose a fast version of MBN-E (fMBN-E), which is not only theoretically even faster than a single standard MBN but also does not increase the estimation error of MBN-E. Empirically, comparing to a number of advanced clustering methods, the proposed methods reach reasonable performance in their default settings. fMBN-E is empirically hundreds of times faster than MBN-E without suffering performance degradation. The applications to image segmentation and graph data mining further demonstrate the advantage of the proposed methods.},
  archive      = {J_PR},
  author       = {Xiao-Lei Zhang and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110739},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110739},
  shortjournal = {Pattern Recognition},
  title        = {Robust multilayer bootstrap networks in ensemble for unsupervised representation learning and clustering},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Heterophily-aware graph attention network. <em>PR</em>,
<em>156</em>, 110738. (<a
href="https://doi.org/10.1016/j.patcog.2024.110738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have shown remarkable success in graph representation learning. Unfortunately, current weight assignment schemes in standard GNNs, such as the calculation based on node degrees or pair-wise representations, can hardly be effective in processing the networks with heterophily, in which the connected nodes usually possess different labels or features. Existing heterophilic GNNs tend to ignore the modeling of heterophily of each edge, which is also a vital part in tackling the heterophily problem. In this paper, we first propose a heterophily-aware attention scheme and reveal the benefits of modeling the edge heterophily, i.e., if a GNN assigns different weights to edges according to different heterophilic types, it can learn effective local attention patterns, enabling nodes to acquire appropriate information from distinct neighbors. Then, we propose a novel Heterophily-Aware Graph Attention Network (HA-GAT) by fully exploring and utilizing the local distribution as the underlying heterophily, to handle the networks with different homophily ratios. To demonstrate the effectiveness of the proposed HA-GAT, we analyze the proposed heterophily-aware attention scheme and local distribution exploration, by seeking an interpretation from their mechanism. Extensive results demonstrate that our HA-GAT achieves state-of-the-art performances on eight datasets with different homophily ratios in both the supervised and semi-supervised node classification tasks.},
  archive      = {J_PR},
  author       = {Junfu Wang and Yuanfang Guo and Liang Yang and Yunhong Wang},
  doi          = {10.1016/j.patcog.2024.110738},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110738},
  shortjournal = {Pattern Recognition},
  title        = {Heterophily-aware graph attention network},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FairScene: Learning unbiased object interactions for indoor
scene synthesis. <em>PR</em>, <em>156</em>, 110737. (<a
href="https://doi.org/10.1016/j.patcog.2024.110737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an unbiased graph neural network learning method called FairScene for indoor scene synthesis. Conventional methods directly apply graphical models to represent the correlation of objects for subsequent furniture insertion. However, due to the object category imbalance in dataset collection and complex object entanglement with implicit confounders, these methods usually generate significantly biased scenes. Moreover, the performance of these methods varies greatly for different indoor scenes. To address this, we propose a framework named FairScene which can fully exploit unbiased object interactions through causal reasoning, so that fair scene synthesis is achieved by calibrating the long-tailed category distribution and mitigating the confounder effects. Specifically, we remove the long-tailed object priors subtract the counterfactual prediction obtained from default input, and intervene in the input feature by cutting off the causal link to confounders based on the causal graph. Extensive experiments on the 3D-FRONT dataset show that our proposed method outperforms the state-of-the-art indoor scene generation methods and enhances vanilla models on a wide variety of vision tasks including scene completion and object recognition.},
  archive      = {J_PR},
  author       = {Zhenyu Wu and Ziwei Wang and Shengyu Liu and Hao Luo and Jiwen Lu and Haibin Yan},
  doi          = {10.1016/j.patcog.2024.110737},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110737},
  shortjournal = {Pattern Recognition},
  title        = {FairScene: Learning unbiased object interactions for indoor scene synthesis},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A simple scheme to amplify inter-class discrepancy for
improving few-shot fine-grained image classification. <em>PR</em>,
<em>156</em>, 110736. (<a
href="https://doi.org/10.1016/j.patcog.2024.110736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification is a challenging topic in pattern recognition and computer vision. Few-shot fine-grained image classification is even more challenging, due to not only the few shots of labelled samples but also the subtle differences to distinguish subcategories in fine-grained images. A recent method called task discrepancy maximisation (TDM) can be embedded into the feature map reconstruction network (FRN) to generate discriminative features, by preserving the appearance details through reconstructing the query image and then assigning higher weights to more discriminative channels, producing the state-of-the-art performance for few-shot fine-grained image classification. However, due to the small inter-class discrepancy in fine-grained images and the small training set in few-shot learning, the training of FRN+TDM can result in excessively flexible boundaries between subcategories and hence overfitting. To resolve this problem, we propose a simple scheme to amplify inter-class discrepancy and thus improve FRN+TDM. To achieve this aim, instead of developing new modules, our scheme only involves two simple amendments to FRN+TDM: relaxing the inter-class score in TDM, and adding a centre loss to FRN. Extensive experiments on five benchmark datasets showcase that, although embarrassingly simple, our scheme is quite effective to improve the performance of few-shot fine-grained image classification. The code is available at https://github.com/Airgods/AFRN.git .},
  archive      = {J_PR},
  author       = {Xiaoxu Li and Zijie Guo and Rui Zhu and Zhanyu Ma and Jun Guo and Jing-Hao Xue},
  doi          = {10.1016/j.patcog.2024.110736},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110736},
  shortjournal = {Pattern Recognition},
  title        = {A simple scheme to amplify inter-class discrepancy for improving few-shot fine-grained image classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Poisson tensor completion with transformed correlated total
variation regularization. <em>PR</em>, <em>156</em>, 110735. (<a
href="https://doi.org/10.1016/j.patcog.2024.110735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor completion involves recovering the underlying tensor from partial observations, and in this paper we focus on the point that these observations obey the Poisson distribution. To contend with this problem, we adopt a popular method that minimizes the sum of the data-fitting term and the regularization term under a uniform sampling mechanism. Specifically, we consider the negative logarithmic maximum likelihood estimate of the Poisson distribution as the data-fitting term. To effectively characterize the intrinsic structure of the tensor data, we propose a parameter-free regularization term that can simultaneously capture the low rankness and local smoothness of the underlying tensor. Here, the transformed tensor nuclear norm is used to explore the low rankness under suitable unitary transformations. We present theoretical derivations to demonstrate the feasibility of the proposed model. Furthermore, we develop an algorithm based on the alternating direction multiplier method (ADMM) to efficiently solve the proposed optimization problem, with its overall convergence being established. A series of numerical experiments show that proposed model yields a pleasing accuracy over several state-of-the-art models.},
  archive      = {J_PR},
  author       = {Qingrong Feng and Jingyao Hou and Weichao Kong and Chen Xu and Jianjun Wang},
  doi          = {10.1016/j.patcog.2024.110735},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110735},
  shortjournal = {Pattern Recognition},
  title        = {Poisson tensor completion with transformed correlated total variation regularization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Table representation learning using heterogeneous graph
embedding. <em>PR</em>, <em>156</em>, 110734. (<a
href="https://doi.org/10.1016/j.patcog.2024.110734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tables, especially when having complex layouts, contain rich semantic information. However, effectively learning from tables to uncover such semantic information remains challenging. The rapid progress in natural language processing does not necessarily correspond to equivalent advancements in table parsing, which often requires joint visual and language modeling. Indeed, humans can quickly derive semantic meaning from table entries by associating them with corresponding column and/or row headers. Motivated by this observation, we propose a new heterogeneous Graph-based Table Representation Learning (GTRL) framework. GTRL combines graph-based visual modeling with sequence-based language modeling to learn granular per-cell embeddings that are sensitive to the semantic meaning of cells within their corresponding table context. We systematically evaluate the proposed GTRL framework using two datasets: a new adhesive table benchmark comprising complex tables extracted from industrial documents for learning per-entry semantics, and a publicly available large-scale dataset that enables learning header semantics from column tables. Experimental results demonstrate the competitive performance of the proposed GTRL, which often exhibits reduced computational complexity compared to state-of-the-art table representation learning models.},
  archive      = {J_PR},
  author       = {Willy Carlos Tchuitcheu and Tan Lu and Ann Dooms},
  doi          = {10.1016/j.patcog.2024.110734},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110734},
  shortjournal = {Pattern Recognition},
  title        = {Table representation learning using heterogeneous graph embedding},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early gesture detection in untrimmed streams: A controlled
CTC approach for reliable decision-making. <em>PR</em>, <em>156</em>,
110733. (<a href="https://doi.org/10.1016/j.patcog.2024.110733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the problem of online action detection for interactive systems, with a special emphasis on earliness. Online Action Detection (OAD) refers to the challenging task of recognizing gestures in untrimmed, streaming videos where the actions occur in unpredictable orders and durations. To address these challenges, we present a skeleton-based system for OAD incorporating a decision mechanism to accurately detect ongoing gestures. This allows us to provide instance-level output, achieving a high level of stream understanding. This mechanism relies on a novel Connectionist Temporal Classification (CTC) loss design that restricts the path possibilities according to the action boundaries. We also present a mechanism to tune the trade-off between accuracy and earliness according to the needs of the interactive system using a weighted label prior. This system includes a 3D CNN network, referred to as DOLT-C3D, exploiting the spatial–temporal information provided by the euclidean skeleton representation. We extensively evaluate our approach on eight publicly available datasets, demonstrating its superior performance compared to state-of-the-art methods in terms of both accuracy and earliness. We also successfully applied our approach to early 2D gestures detection. Furthermore, our system shows real-time performance, making it a suitable choice for interactive systems.},
  archive      = {J_PR},
  author       = {William Mocaër and Eric Anquetil and Richard Kulpa},
  doi          = {10.1016/j.patcog.2024.110733},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110733},
  shortjournal = {Pattern Recognition},
  title        = {Early gesture detection in untrimmed streams: A controlled CTC approach for reliable decision-making},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CTNeRF: Cross-time transformer for dynamic neural radiance
field from monocular video. <em>PR</em>, <em>156</em>, 110729. (<a
href="https://doi.org/10.1016/j.patcog.2024.110729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of our work is to generate high-quality novel views from monocular videos of complex and dynamic scenes. Prior methods, such as DynamicNeRF, have shown impressive performance by leveraging time-varying dynamic radiation fields. However, these methods have limitations when it comes to accurately modeling the motion of complex objects, which can lead to inaccurate and blurry renderings of details. To address this limitation, we propose a novel approach that builds upon a recent generalization NeRF, which aggregates nearby views onto new viewpoints. However, such methods are typically only effective for static scenes. To overcome this challenge, we introduce a module that operates in both the time and frequency domains to aggregate the features of object motion. This allows us to learn the relationship between frames and generate higher-quality images. Our experiments demonstrate significant improvements over state-of-the-art methods on dynamic scene datasets. Specifically, our approach outperforms existing methods in terms of both the accuracy and visual quality of the synthesized views. Our code is available on https://github.com/xingy038/CTNeRF .},
  archive      = {J_PR},
  author       = {Xingyu Miao and Yang Bai and Haoran Duan and Fan Wan and Yawen Huang and Yang Long and Yefeng Zheng},
  doi          = {10.1016/j.patcog.2024.110729},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110729},
  shortjournal = {Pattern Recognition},
  title        = {CTNeRF: Cross-time transformer for dynamic neural radiance field from monocular video},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structured pruning adapters. <em>PR</em>, <em>156</em>,
110724. (<a href="https://doi.org/10.1016/j.patcog.2024.110724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapters are a parameter-efficient alternative to fine-tuning, which augment a frozen base network to learn new tasks. Yet, the inference of the adapted model is often slower than the corresponding fine-tuned model. To improve on this, we introduce the concept of Structured Pruning Adapters (SPAs), a family of compressing, task-switching network adapters, that accelerate and specialize networks using tiny parameter sets and structured pruning. Specifically, we propose the Structured Pruning Low-rank Adapter (SPLoRA) and the Structured Pruning Residual Adapter (SPPaRA) and evaluate them on a suite of pruning methods, architectures, and image recognition benchmarks. Compared to regular structured pruning with fine-tuning, SPLoRA improves image recognition accuracy by 6.9% on average for ResNet50 while using half the parameters at 90% pruned weights. Alternatively, a SPLoRA augmented model can learn adaptations with 17 × × fewer parameters at 70% pruning with 1.6% lower accuracy. For ViT-b/16 models, SPLoRA improves accuracy by an average of 43%-points at 75% pruned weights while learning 6.8 × × fewer parameters. Our experimental code and Python library of adapters are available at www.github.com/lukashedegaard/structured-pruning-adapters .},
  archive      = {J_PR},
  author       = {Lukas Hedegaard and Aman Alok and Juby Jose and Alexandros Iosifidis},
  doi          = {10.1016/j.patcog.2024.110724},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110724},
  shortjournal = {Pattern Recognition},
  title        = {Structured pruning adapters},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Multi-view clustering via dynamic unified bipartite graph
learning. <em>PR</em>, <em>156</em>, 110715. (<a
href="https://doi.org/10.1016/j.patcog.2024.110715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering algorithms based on graph learning have the ability to extract the potential association between data samples, which has been a concern of many researchers in recent years. However, existing algorithms have two limitations: (1) they directly learn from the raw graph, which includes noise and outliers, and they construct the graph filter statically, biasing the clustering results; (2) during graph construction, they mainly use the information of a single structure and fail to fully extract the multi-granular structural information among the data. To address these issues, this paper proposes a novel multi-view clustering method via dynamic unified bipartite graph learning. Specifically, a learnable graph filter is first refined to dynamically filter the original data feature space, gradually filtering out the undesirable high-frequency noise and achieving a clustering-friendly smooth representation. Second, a unified bipartite graph is constructed by combining the multi-granular structural information of different views to better explore the distinct and common information of each view. In one framework, the dynamic filter and multi-granular structure information are combined to iteratively learn the unified bipartite graph. An efficient iterative algorithm is designed to decompose the objective function into small-scale subproblems for solving. Extensive experiments on benchmark datasets show the superiority of the proposed algorithm over several existing state-of-the-art multi-view clustering algorithms.},
  archive      = {J_PR},
  author       = {Xingwang Zhao and Shujun Wang and Xiaolin Liu and Jiye Liang},
  doi          = {10.1016/j.patcog.2024.110715},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110715},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view clustering via dynamic unified bipartite graph learning},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-threshold deep metric learning for facial expression
recognition. <em>PR</em>, <em>156</em>, 110711. (<a
href="https://doi.org/10.1016/j.patcog.2024.110711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representations generated through triplet-based deep metric learning offer significant advantages for facial expression recognition (FER). Each threshold in triplet loss inherently shapes a distinct distribution of inter-class variations, leading to unique representations of expression features. Nonetheless, pinpointing the optimal threshold for triplet loss presents a formidable challenge, as the ideal threshold varies not only across different datasets but also among classes within the same dataset. In this paper, we propose a novel multi-threshold deep metric learning approach that bypasses the complex process of threshold validation and markedly improves the effectiveness in creating expression feature representations. Instead of choosing a single optimal threshold from a valid range, we comprehensively sample thresholds throughout this range, which ensures that the representation characteristics exhibited by the thresholds within this spectrum are fully captured and utilized for enhancing FER. Specifically, we segment the embedding layer of the deep metric learning network into multiple slices, with each slice representing a specific threshold sample. We subsequently train these embedding slices in an end-to-end fashion, applying triplet loss at its associated threshold to each slice, which results in a collection of unique expression features corresponding to each embedding slice. Moreover, we identify the issue that the traditional triplet loss may struggle to converge when employing the widely-used Batch Hard strategy for mining informative triplets, and introduce a novel loss termed dual triplet loss to address it. Extensive evaluations demonstrate the superior performance of the proposed approach on both posed and spontaneous facial expression datasets.},
  archive      = {J_PR},
  author       = {Wenwu Yang and Jinyi Yu and Tuo Chen and Zhenguang Liu and Xun Wang and Jianbing Shen},
  doi          = {10.1016/j.patcog.2024.110711},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110711},
  shortjournal = {Pattern Recognition},
  title        = {Multi-threshold deep metric learning for facial expression recognition},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Momentum recursive DARTS. <em>PR</em>, <em>156</em>, 110710.
(<a href="https://doi.org/10.1016/j.patcog.2024.110710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DARTS has emerged as a popular method for neural architecture search (NAS) owing to its efficiency and simplicity. It employs gradient-based bi-level optimization to iteratively optimize the upper-level architecture parameters and lower-level super-network weights. The key challenge in DARTS is the accurate estimation of gradients for two-level object functions, leading to significant errors in gradient approximation. To address this issue, we propose a new approach, MR-DARTS, that incorporates a momentum term and a recursive scheme to improve gradient estimation. Specifically, we leverage historical information by using a running average of past observed gradients to enhance the quality of current gradient estimation in both upper-level and lower-level functions. Our theoretical analysis shows that the variance of our estimated gradient decreases with each iteration. By utilizing momentum and a recursive scheme, MR-DARTS effectively controls the error in stochastic gradient updates that result from inaccurate gradient estimation. Furthermore, we utilize the Neumann series approximation and Hessian Vector Product scheme to reduce computational requirements and memory usage. We evaluate our proposed method on several benchmarks and demonstrate its effectiveness through comprehensive experiments.},
  archive      = {J_PR},
  author       = {Benteng Ma and Yanning Zhang and Yong Xia},
  doi          = {10.1016/j.patcog.2024.110710},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110710},
  shortjournal = {Pattern Recognition},
  title        = {Momentum recursive DARTS},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local context attention learning for fine-grained scene
graph generation. <em>PR</em>, <em>156</em>, 110708. (<a
href="https://doi.org/10.1016/j.patcog.2024.110708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained scene graph generation aims to parse the objects and their fine-grained relationships within scenes. Despite the significant progress in recent years, their performance is still limited by two major issues: (1) ambiguous perception under a global view; (2) the lack of reliable, fine-grained annotations. We argue that understanding the local context is important in addressing the two issues. However, previous works often overlook it, which limits their effectiveness in fine-grained scene graph generation. To tackle this challenge, we introduce a Local-context Attention Learning method that concentrates on local context and can generate high-reliability, fine-grained annotations. It comprises two components: (1) The Fine-grained Location Attention Network (FLAN), a multi-branch network that encompasses global and local branches, can attend to local informative context and perceive granularity levels in different regions, thereby adaptively enhancing the learning of fine-grained locations. (2) The Fine-grained Location Label Transfer (FLLT) method identifies coarse-grained labels inconsistent with the local context and determines which labels should be transferred through the global confidence thresholding strategy, finally transferring them to reliable local context-consistent fine-grained ones. Experiments conducted on the Visual Genome, OpenImage, and GQA-200 datasets show that the proposed methods achieve significant improvements on the fine-grained scene graph generation task. By addressing the challenge mentioned above, our method also achieves state-of-the-art performances on the three datasets.},
  archive      = {J_PR},
  author       = {Xuhan Zhu and Ruiping Wang and Xiangyuan Lan and Yaowei Wang},
  doi          = {10.1016/j.patcog.2024.110708},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110708},
  shortjournal = {Pattern Recognition},
  title        = {Local context attention learning for fine-grained scene graph generation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hunt-inspired transformer for visual object tracking.
<em>PR</em>, <em>156</em>, 110703. (<a
href="https://doi.org/10.1016/j.patcog.2024.110703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a hunt-inspired Transformer for visual object tracking, dubbed as HuntFormer. The HuntFormer focuses on robust target detection and identification, simulating natural hunting processes. Specifically, the HuntFormer comprises two essential module designs including a predictor for detection and a verifier for identification. The predictor emulates the detection stage by designing a motion trajectory guided particle filter, which identifies potential target locations by predicting the motion state within a particle filtering framework. The predictor utilizes spatio-temporal correlation scores between dynamic target templates and the search region to guide the learning process to generate a set of reliable particles. This enables the base tracker to narrow its search range to focus on the target, and swiftly re-detect the target in case of model drift. Once the target is re-detected, the verifier assesses the detection result as a reliable tracked item. The verifier initially maintains a dynamic memory that stores reliable target templates and their corresponding locations in the motion trajectory. It then models the uncertainty of appearance information within this memory probabilistically. The output uncertainty score determines whether the memory gets updated or not. Ultimately, the predictor and the verifier collaborate, ensuring a robust tracking outcome. Extensive evaluations on six challenging benchmark datasets demonstrate HuntFormer’s favorable performance against various state-of-the-art trackers. Notably, in the VOT-LT2022 tracking challenge, the HuntFormer won the third place with an F-score of 0.598, closely competing for the second place with an F-score of 0.600.},
  archive      = {J_PR},
  author       = {Zhibin Zhang and Wanli Xue and Yuxi Zhou and Kaihua Zhang and Shengyong Chen},
  doi          = {10.1016/j.patcog.2024.110703},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110703},
  shortjournal = {Pattern Recognition},
  title        = {Hunt-inspired transformer for visual object tracking},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-imbalanced semi-supervised learning for large-scale
point cloud semantic segmentation via decoupling optimization.
<em>PR</em>, <em>156</em>, 110701. (<a
href="https://doi.org/10.1016/j.patcog.2024.110701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL), thanks to the significant reduction of data annotation costs, has been an active research topic for large-scale 3D scene understanding. However, the existing SSL-based methods suffer from severe training bias, mainly due to class imbalance and long-tail distributions of the point cloud data. As a result, they lead to a biased prediction for the tail class segmentation. In this paper, we introduce a new decoupling optimization framework, which disentangles feature representation learning and classifier in an alternative optimization manner to shift the bias decision boundary effectively. In particular, we first employ two-round pseudo-label generation to select unlabeled points across head-to-tail classes. We further introduce multi-class imbalanced focus loss to adaptively pay more attention to feature learning across head-to-tail classes. We fix the backbone parameters after feature learning and retrain the classifier using ground-truth points to update its parameters. Extensive experiments demonstrate the effectiveness of our method outperforming previous state-of-the-art methods on both indoor and outdoor 3D point cloud datasets ( i.e. , S3DIS, ScanNet-V2, Semantic3D, and SemanticKITTI) using 1% and 1pt evaluation.},
  archive      = {J_PR},
  author       = {Mengtian Li and Shaohui Lin and Zihan Wang and Yunhang Shen and Baochang Zhang and Lizhuang Ma},
  doi          = {10.1016/j.patcog.2024.110701},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110701},
  shortjournal = {Pattern Recognition},
  title        = {Class-imbalanced semi-supervised learning for large-scale point cloud semantic segmentation via decoupling optimization},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). FMGNet: An efficient feature-multiplex group network for
real-time vision task. <em>PR</em>, <em>156</em>, 110698. (<a
href="https://doi.org/10.1016/j.patcog.2024.110698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight network design is crucial for optimizing speed and accuracy in computer vision tasks on mobile platforms with limited resources. Widely adopted models, such as EfficientNet and RegNet have achieved significant success by integrating key elements like Pointwise Convolutions (PWConvs) and Squeeze-and-Excitation (SE) blocks. However, a notable observation is that the output feature of the PWConv closely resembles its input, particularly in the absence of an activation function. This similarity and redundancy lead to wasted computational complexity and adversely affect the inference speed. To address these issues, we propose an efficient lightweight network called Efficient Feature-Multiplex Group Network (FMGNet). FMGNet is composed of two key components: the Cross-layer Feature-multiplex Group (CFG) block and the CFG-aligned Cross-layer Attention (CCA) block. The CFG block enables more compact feature learning with fewer parameters by multiplexing the input features of the PWConv. Meanwhile, the CCA block leverages the pre-modified features derived from the CFG block’s PWConv, allowing for simultaneous and parallel channel attention modeling. Our extensive experiments across various tasks, including image classification (ImageNet), object detection (PASCAL VOC), human pose estimation (MPII), person re-identification (Market-1501, DukeMTMC-ReID, CUHK03-NP), and semantic segmentation (Cityscapes), indicate that FMGNet achieves comparable performance to state-of-the-art lightweight convolutional neural networks, offering faster inference times. Remarkably, FMGNet even surpasses recent transformer-based models, such as SwiftFormer and EfficientFormerV2, achieving superior results with lower inference latency.},
  archive      = {J_PR},
  author       = {Hao Zhang and Yongqiang Ma and Kaipeng Zhang and Nanning Zheng and Shenqi Lai},
  doi          = {10.1016/j.patcog.2024.110698},
  journal      = {Pattern Recognition},
  month        = {12},
  pages        = {110698},
  shortjournal = {Pattern Recognition},
  title        = {FMGNet: An efficient feature-multiplex group network for real-time vision task},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional advancement of machine learning algorithm via
fuzzy neural network. <em>PR</em>, <em>155</em>, 110732. (<a
href="https://doi.org/10.1016/j.patcog.2024.110732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving overall performance is the ultimate goal of any machine learning (ML) algorithm. While it is a trivial task to explore multiple individual validation measurements, evaluating and monitoring overall performance can be complicated due to the highly nonlinear nature of the functions describing the relationships among different validation metrics, such as the Dice Similarity Coefficient (DSC) and Jaccard Index (JI). Therefore, it is naturally desirable to have a reliable validation algorithm or model that can integrate all existing validation metrics into a single value. This consolidated metric would enable straightforward assessment of an ML algorithm’s performance and identify areas for improvement. To deal with such a complex nonlinear problem, this study suggests a novel parameterized model named Adaptive Neuro-Fuzzy Inference Systems (ANFIS), which takes any set of input–output precise-imprecise data and uses a neuro-adaptive learning strategy to tune the parameters of the pre-defined membership functions. Our method can be accepted as an elegant and the state-of-the-art method for the nonlinear function approximation, which could be added directly to any convolutional neural networks (CNN) loss functions as the regularization term to generate a constrained-CNN-FUZZY model optimization. To demonstrate the ability of the purposed method and provide a practical explanation of the capability of ANFIS, we use deep CNN as a testing platform to consider the fact that one of the biggest challenges CNN-developers faced today is to reduce the mismatching between the provided input data and the predicted results monitored by different validation metrics. We first create a toy dataset using MNIST and investigate the properties of the proposed model. We then use a medical dataset to demonstrate our method’s efficacy on brain lesion segmentation. In both datasets, our method shows reliable validation results to guide researchers towards choosing performance metrics in a problem-aware manner, especially when the results of different validation metrics are too similar among models to determine the best one.},
  archive      = {J_PR},
  author       = {Kevin Bronik and Le Zhang},
  doi          = {10.1016/j.patcog.2024.110732},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110732},
  shortjournal = {Pattern Recognition},
  title        = {Conditional advancement of machine learning algorithm via fuzzy neural network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic weighted knowledge distillation for brain tumor
segmentation. <em>PR</em>, <em>155</em>, 110731. (<a
href="https://doi.org/10.1016/j.patcog.2024.110731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D MRI brain tumor segmentation holds a crucial position in the field of medical image analysis, contributing significantly to the clinical diagnosis and treatment of brain tumors. However, traditional 3D brain tumor segmentation methods often entail extensive parameters and computational demands, posing substantial challenges in model training and deployment. To overcome these challenges, this paper introduces a brain tumor segmentation framework based on knowledge distillation. This framework includes training a lightweight network by extracting knowledge from a well-established brain tumor segmentation network. Firstly, this framework replaces the conventional static knowledge distillation (SKD) with the proposed dynamic weighted knowledge distillation (DWKD). DWKD dynamically adjusts the distillation loss weights for each pixel based on the learning state of the student network. Secondly, to enhance the student network&#39;s generalization capability, this paper customizes a loss function for DWKD, known as regularized cross-entropy (RCE). RCE introduces controlled noise into the model, enhancing its robustness and diminishing the risk of overfitting. This controlled injection of noise aids in fortifying the model&#39;s robustness. Lastly, Empirical validation of the proposed methodology is conducted using two distinct backbone networks, namely Attention U-Net and Residual U-Net. Rigorous experimentation is executed across the BraTS 2019, BraTS 2020, and BraTS 2021 datasets. Experimental results demonstrate that DWKD exhibits significant advantages over SKD in enhancing the segmentation performance of the student network. Furthermore, when dealing with limited training data, the RCE method can further improve the student network&#39;s segmentation performance. Additionally, this paper quantitatively analyzes the number of concept detectors identified in network dissection. It assesses the impact of DWKD on model interpretability and finds that compared to SKD, DWKD can more effectively enhance model interpretability. The source code is available at https://github.com/YuBinLab-QUST/DWKD/ .},
  archive      = {J_PR},
  author       = {Dianlong An and Panpan Liu and Yan Feng and Pengju Ding and Weifeng Zhou and Bin Yu},
  doi          = {10.1016/j.patcog.2024.110731},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110731},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic weighted knowledge distillation for brain tumor segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLNAS: Greedy layer-wise network architecture search for low
cost and fast network generation. <em>PR</em>, <em>155</em>, 110730. (<a
href="https://doi.org/10.1016/j.patcog.2024.110730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of applying machine learning algorithms to practical problems can be a challenging and tedious task for non-experts. Previous research has sought to alleviate this burden by introducing automated machine learning techniques, including Network Architecture Search (NAS) and Differentiable Architecture Search (DARTS). However, these methods use a fixed number of layers and predefined skip connections which impose limitations on the generation of an optimal network architecture. In this paper, we propose a novel approach called Greedy Layer-wise Network Architecture Search (GLNAS), which trains network layers one after another and evaluates the network’s performance after each layer is added. GLNAS also assesses the effectiveness of skip connections between layers by testing various outputs of previous layers as an input to the current layer. Our experiment results demonstrate that the network generated by GLNAS requires fewer parameters (i.e., 3.5 millions in both CIFAR-10 and CIFAR-100 datasets) and GPU resources during the searching phase (i.e., 0.17 and 0.24 GPU days in CIFAR-10 and CIFAR-100 datasets respectively) than many existing methods.},
  archive      = {J_PR},
  author       = {Jiacang Ho and Kyongseok Park and Dae-Ki Kang},
  doi          = {10.1016/j.patcog.2024.110730},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110730},
  shortjournal = {Pattern Recognition},
  title        = {GLNAS: Greedy layer-wise network architecture search for low cost and fast network generation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic attention-based heterogeneous feature aggregation
network for image fusion. <em>PR</em>, <em>155</em>, 110728. (<a
href="https://doi.org/10.1016/j.patcog.2024.110728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to generate a comprehensive image that retains both salient targets of the infrared image and texture details of the visible image. However, existing methods overlook the differences in attention to semantic information among different fused images. To address this issue, we propose a semantic attention-based heterogeneous feature aggregation network for image fusion. The key component of our network is the semantic attention-based fusion module, which leverages the weights derived from semantic feature maps to dynamically adjust the significance of various semantic objects within the fusion feature maps. By using semantic weights as guidance, our fusion process concentrates on regions with crucial semantics, resulting in a more focused fusion that preserves rich semantic information. Moreover, we propose an innovative component called the attentive dense block. This block effectively filters out irrelevant features during extraction, accentuates essential features to their maximum potential, and enhances the visual quality of the fused images. Importantly, our network demonstrates strong generalization capabilities. Extensive experiments validate the superiority of our proposed network over current state-of-the-art techniques in terms of both visual appeal and semantics-driven evaluation.},
  archive      = {J_PR},
  author       = {Zhiqiang Ruan and Jie Wan and Guobao Xiao and Zhimin Tang and Jiayi Ma},
  doi          = {10.1016/j.patcog.2024.110728},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110728},
  shortjournal = {Pattern Recognition},
  title        = {Semantic attention-based heterogeneous feature aggregation network for image fusion},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). D-net: A dual-encoder network for image splicing forgery
detection and localization. <em>PR</em>, <em>155</em>, 110727. (<a
href="https://doi.org/10.1016/j.patcog.2024.110727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many detection methods based on convolutional neural networks (CNNs) have been proposed for image splicing forgery detection. Most of these methods focus on validating local patches or local objects. We regard image splicing forgery detection as a binary classification task that distinguishes tampered and non-tampered regions by forensic fingerprints rather than semantic features. As the network goes deep, its representation ability becomes strong. However, the non-semantic forensic fingerprints can hardly be retained by normal CNNs in deep layers. We proposed a novel dual-encoder network (D-Net) for image splicing forgery detection to resolve these issues, employing an unfixed and a fixed encoder. The unfixed encoder autonomously learns the image fingerprints that differentiate between the tampered and non-tampered regions, whereas the fixed encoder intentionally provides structural information that assists the learning and detection of the forgeries. This dual-encoder is followed by a spatial pyramid global-feature extraction module that expands the global insight of D-Net for classifying the tampered and non-tampered regions more accurately. In an experimental comparison study of D-Net and state-of-the-art methods, D-Net, without pre-training or training on a large number of forgery images, outperformed the other methods in pixel-level forgery detection. Moreover, it is stably robust to different anti-forensic attacks.},
  archive      = {J_PR},
  author       = {Zonglin Yang and Bo Liu and Xiuli Bi and Bin Xiao and Weisheng Li and Guoyin Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2024.110727},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110727},
  shortjournal = {Pattern Recognition},
  title        = {D-net: A dual-encoder network for image splicing forgery detection and localization},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel hybrid decoding neural network for EEG signal
representation. <em>PR</em>, <em>155</em>, 110726. (<a
href="https://doi.org/10.1016/j.patcog.2024.110726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed a novel hybrid decoding model that combines the superiority of CNNs and multi-head self-attention mechanisms, called HCANN, to finely characterizing EEG features. Depthwise separable convolution with multi-scale factors efficiently decouples temporal relevant information between brain-computer interface (BCI) tasks and EEG signals. Multi-head mechanism adaptively modified for EEG focuses on brain spatial activation patterns and extracts complementary spatial representation information from multiple subspaces. The proposed HCANN decodes the intent information of EEG recorded by three BCI paradigms, including one active and two passive BCI paradigms: rapid serial visual presentation, motor imagery, and imagined speech. We evaluated HCANN by comparing with the current state-of-the-art methods. The experimental results demonstrated that HCANN can effectively decode EEG and improves classification performance for all three BCI tasks. In addition, the visualization of spatial-temporal features at different decoding stages demonstrated that the proposed HCANN gradually extracts effective features related to the BCI tasks. The code of HCANN is publicly available at https://github.com/youshuoji/HCANN .},
  archive      = {J_PR},
  author       = {Youshuo Ji and Fu Li and Boxun Fu and Yijin Zhou and Hao Wu and Yang Li and Xiaoli Li and Guangming Shi},
  doi          = {10.1016/j.patcog.2024.110726},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110726},
  shortjournal = {Pattern Recognition},
  title        = {A novel hybrid decoding neural network for EEG signal representation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-embedded similarity prototype for scene
recognition. <em>PR</em>, <em>155</em>, 110725. (<a
href="https://doi.org/10.1016/j.patcog.2024.110725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high inter-class similarity caused by the complex composition and the co-existing objects across scenes, numerous studies have explored object semantic knowledge within scenes to improve scene recognition. However, a resulting challenge emerges as object information extraction techniques require heavy computational costs, thereby burdening the network considerably. This limitation often renders object-assisted approaches incompatible with edge devices in practical deployment. In contrast, this paper proposes a semantic knowledge-based similarity prototype, which can help the scene recognition network achieve superior accuracy without increasing the computational cost in practice. It is simple and can be plug-and-played into existing pipelines. More specifically, a statistical strategy is introduced to depict semantic knowledge in scenes as class-level semantic representations. These representations are used to explore correlations between scene classes, ultimately constructing a similarity prototype. Furthermore, we propose to leverage the similarity prototype to support network training from the perspective of Gradient Label Softening and Batch-level Contrastive Loss, respectively. Comprehensive evaluations on multiple benchmarks show that our similarity prototype enhances the performance of existing networks, all while avoiding any additional computational burden in practical deployments. Code and the statistical similarity prototype will be available at https://github.com/ChuanxinSong/SimilarityPrototype .},
  archive      = {J_PR},
  author       = {Chuanxin Song and Hanbo Wu and Xin Ma and Yibin Li},
  doi          = {10.1016/j.patcog.2024.110725},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110725},
  shortjournal = {Pattern Recognition},
  title        = {Semantic-embedded similarity prototype for scene recognition},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guided contrastive boundary learning for semantic
segmentation. <em>PR</em>, <em>155</em>, 110723. (<a
href="https://doi.org/10.1016/j.patcog.2024.110723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation, a fundamental task in environmental understanding, aims to assign each image pixel to a specific class. Despite recent progress, segmentation accuracy in boundary regions remains suboptimal. This paper introduces Guided Contrastive Boundary Learning (GCBL), a novel framework designed to enhance feature representation learning, thereby improving boundary segmentation performance. Unlike conventional contrastive learning, GCBL guides inter-class representation learning by weighting pixel contributions based on their estimated probabilities. For intra-class learning, it leverages the neural collapse phenomenon, encouraging representations to align with last-layer classifier weights. Additionally, an asymmetric distance boundary pixel search strategy ensures a more reasonable selection of contrastive pairs. To prevent weight collapse in learning, a regularization term is applied to the last-layer classifier’s weights. The GCBL method is readily integrable into existing and future segmentation frameworks. Extensive experiments on the Cityscapes, ADE20K, and S3DIS datasets demonstrate the effectiveness and generalizability of our approach. Code is available at https://github.com/skyshoumeng/GCBL .},
  archive      = {J_PR},
  author       = {Shoumeng Qiu and Jie Chen and Haiqiang Zhang and Ru Wan and Xiangyang Xue and Jian Pu},
  doi          = {10.1016/j.patcog.2024.110723},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110723},
  shortjournal = {Pattern Recognition},
  title        = {Guided contrastive boundary learning for semantic segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LM-metric: Learned pair weighting and contextual memory for
deep metric learning. <em>PR</em>, <em>155</em>, 110722. (<a
href="https://doi.org/10.1016/j.patcog.2024.110722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Metric Learning (DML) is a crucial machine learning method in computer vision, and constructing an effective loss function. Average Precision (AP) is a well-known evaluation metric for image retrieval tasks. However, AP’s non-differentiable and non-decomposable nature constrains its potential for adoption as an optimization goal. We propose a deep metric learning method with learned parametric metric learning loss and a contextual memory block ( LM-Metric ) for large-scale image retrieval tasks, which overcome AP’s drawbacks and integrate AP within DML loss. We first introduce a parametric pairwise weighting scheme via policy gradient optimization and model the batch-wise inter-sample relationship via a Gated Recurrent Unit (GRU). Furthermore, a conditional Normalizing Flow-based contextual memory feature block to learn a compact single embedding for each image containing the contextual information during retrieval. We perform experiments on retrieval benchmark datasets and improve performance over the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Shiyang Yan and Lin Xu and Xinyao Shu and Zhenyu Lu and Jialie Shen},
  doi          = {10.1016/j.patcog.2024.110722},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110722},
  shortjournal = {Pattern Recognition},
  title        = {LM-metric: Learned pair weighting and contextual memory for deep metric learning},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSANet: Forecasting traffic congestion patterns from aerial
videos using graphs and transformers. <em>PR</em>, <em>155</em>, 110721.
(<a href="https://doi.org/10.1016/j.patcog.2024.110721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting traffic congestion patterns in lane-less traffic scenarios is a complex task because of the combination of high &amp; irregular vehicle densities, fluctuating speeds, and the presence of environmental obstacles. Existing techniques like vehicle counting and density prediction, which successfully estimate congestion in lane-based traffic, are unsuitable for lane-less traffic scenarios due to the irregular and unpredictable nature of traffic density patterns. To overcome these challenges, we propose traffic states to measure congestion patterns in lane-less traffic scenarios. Each traffic state is characterized by the spatio-temporal distribution of neighbouring road users, including vehicles and motorcyclists. We employ traffic graphs to capture the spatial distribution of neighbouring road users. Also, we propose a novel method for the automated construction of traffic graphs by leveraging the detection and tracking of individual road users in aerial videos. Further, in order to incorporate the temporal distribution, we utilize a transformer model to capture the evolution of spatial traffic graphs over time. This enables us to forecast future spatio-temporal distributions and their associated traffic states. Our proposed model, named Traffic State Anticipation Network (TSANet), can effectively forecast future traffic states by analysing sequences of current traffic graphs, thereby enhancing our understanding of evolving traffic patterns in lane-less scenarios. Also, to address the lack of publicly available lane-less traffic datasets, we introduce EyeonTraffic (EoT), a large-scale lane-less traffic dataset containing three hours of aerial videos captured at three busy intersections in Ahmedabad city, India. Experimental results on the EoT dataset demonstrate the efficacy of our proposed TSANet in effectively anticipating traffic states across diverse spatial regions within an intersection. In addition, we also show that TSANet generalizes well for previously unseen intersections, making it suitable for analysing various traffic scenarios without the need for explicit training, thereby enhancing its practical applicability.},
  archive      = {J_PR},
  author       = {K. Naveen Kumar and Debaditya Roy and Thakur Ashutosh Suman and Chalavadi Vishnu and C. Krishna Mohan},
  doi          = {10.1016/j.patcog.2024.110721},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110721},
  shortjournal = {Pattern Recognition},
  title        = {TSANet: Forecasting traffic congestion patterns from aerial videos using graphs and transformers},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel weighted approach for time series forecasting based
on visibility graph. <em>PR</em>, <em>155</em>, 110720. (<a
href="https://doi.org/10.1016/j.patcog.2024.110720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series has attracted a lot of attention in many fields today. Time series forecasting algorithm based on complex network analysis is a research hotspot. How to use time series information to achieve more accurate forecasting is a problem. To solve this problem, this paper proposes a weighted network forecasting method to improve the forecasting accuracy. Firstly, the time series will be transformed into a complex network, and the similarity between nodes will be found. Then, the similarity will be used as a weight to make weighted forecasting on the predicted values produced by different nodes. Compared with the previous method, the proposed method is more accurate. In order to verify the effect of the proposed method, the experimental part is tested on M1, M3 datasets and Construction Cost Index (CCI) dataset, which shows that the proposed method has more accurate forecasting performance.},
  archive      = {J_PR},
  author       = {Tianxiang Zhan and Fuyuan Xiao},
  doi          = {10.1016/j.patcog.2024.110720},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110720},
  shortjournal = {Pattern Recognition},
  title        = {A novel weighted approach for time series forecasting based on visibility graph},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Token-word mixer meets object-aware transformer for
referring image segmentation. <em>PR</em>, <em>155</em>, 110719. (<a
href="https://doi.org/10.1016/j.patcog.2024.110719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring image segmentation aims to generate a binary mask of the target object according to a referring expression. Some recent works argue that post-fusion paradigm may result in inconsistency and insufficiency issue and propose to integrate textual features during the visual encoding process. Although effective, they do fusion in a single way at each stage of encoder, e.g. utilizing cross attention mechanisms. This single fusion method ignores local and detailed image information correlated with language due to the incapability of attention in capturing high-frequencies information. To address this issue, we propose a Token-Word Mixer, which takes into consideration the characteristics of convolution and attention, and achieves more comprehensive interactions and alignments of multi-modal features through a mix operation. Furthermore, existing methods that rely solely on grid features lack perception of the target object and inference of relationships between objects, making it difficult to associate and align semantic information of target objects during multi-modal fusion when referring expressions or image scenes are complex. Therefore, we propose to incorporates object-level information by exploiting a DETR-based detector to provide region features, and the Object-Aware Transformer encoder with an additional learnable token is proposed to perceive effective information associated with the target object. Based on the enhanced cross-modal features and the aggregated token, we adopt query-based mask generation method instead of pixel classification framework for referring image segmentation. Extensive experiments and ablation studies indicate the effectiveness of our proposed methods.},
  archive      = {J_PR},
  author       = {Zhenliang Zhang and Zhu Teng and Jack Fan and Baopeng Zhang and Jianping Fan},
  doi          = {10.1016/j.patcog.2024.110719},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110719},
  shortjournal = {Pattern Recognition},
  title        = {Token-word mixer meets object-aware transformer for referring image segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-network node classification method in open-set
scenario. <em>PR</em>, <em>155</em>, 110718. (<a
href="https://doi.org/10.1016/j.patcog.2024.110718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-network node classification aims to classify the nodes of unlabeled target network using a labeled source network. Existing methods introduce domain adaptation to address representation discrepancy in closed-set scenario. However, the open-set scenario is widespread in applications, in which, the coexistence and interaction of representation discrepancy and label discrepancy pose a great challenge. To this end, we make the first attempt for cross-network node classification in open-set scenario and propose a novel method based on reconstruction. Firstly, the pseudo unknown class nodes from target network are reconstructed into source network, which addresses label discrepancy by transforming open-set into closed-set with K+1 classes. Secondly, the contrastive-center loss is introduced to enhance the node representations, which aims to identify the unknown nodes from known nodes in networks. And then the invariant representations are learned better to address representation discrepancy. Extensive experiments demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Yuhong Zhang and Yunlong Ji and Kui Yu and Xuegang Hu and Xindong Wu},
  doi          = {10.1016/j.patcog.2024.110718},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110718},
  shortjournal = {Pattern Recognition},
  title        = {A cross-network node classification method in open-set scenario},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage zero-shot sparse hashing with missing labels for
cross-modal retrieval. <em>PR</em>, <em>155</em>, 110717. (<a
href="https://doi.org/10.1016/j.patcog.2024.110717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, zero-shot cross-modal hashing has gained significant popularity due to its ability to effectively realize the retrieval of emerging concepts within multimedia data. Although the existing approaches have shown impressive results, the following limitations still need to be solved: (1) Labels in large-scale multimodal datasets in real scenes are usually incomplete or partially missing. (2) The existing methods ignore the influence of features-wise low-level similarity and label distribution on retrieval performance. (3) The representation ability of dense hash codes limits its discriminative potential. To solve these issues, we introduce an effective cross-modal retrieval framework called two-stage zero-shot sparse hashing with missing labels (TZSHML). Specifically, we learn a classifier through the partially known labeled samples to predict the labels of unlabeled data. Then, we use the reliable information in the correctly marked labels to recover the missing labels. The predicted and recovered labels are combined to obtain more accurate labels for the samples with missing labels. In addition, we employ sample-wise fine-grained similarity and cluster-wise similarity to learn hash codes. Therefore, TZSHML ensures that more samples with similar semantics are clustered together. Besides, we apply high-dimensional sparse hash codes to explore richer semantic information. Finally, the drift and interaction terms are introduced into the learning of the hash function to further narrow the gap between different modalities. Extensive experimental results demonstrate the competitiveness of our approach over other state-of-the-art methods in zero-shot retrieval scenarios with missing labels. The source code of this paper can be obtained from https://github.com/szq0816/TZSHML .},
  archive      = {J_PR},
  author       = {Kailing Yong and Zhenqiu Shu and Hongbin Wang and Zhengtao Yu},
  doi          = {10.1016/j.patcog.2024.110717},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110717},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage zero-shot sparse hashing with missing labels for cross-modal retrieval},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale multi-view clustering via matrix factorization
of consensus graph. <em>PR</em>, <em>155</em>, 110716. (<a
href="https://doi.org/10.1016/j.patcog.2024.110716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, anchors-based multi-view clustering methods have been widely concerned for they can not only significantly reduce the time complexity but also have good interpretability. However, the time consumption of optimization and spectral embedding with Singular Value Decomposition (SVD) is expensive for large-scale multi-view clustering due to the large-scale consensus graph. This paper proposes to factorize the consensus graph to directly obtain the low-dimension consensus embedding matrix by optimizing the objective function. Specifically, the consensus graph is factorized into a transition matrix and a low-dimension embedding matrix. Among them, the transition matrix is used to prevent the clustering time consumption from increasing significantly with the number of anchors, and the low-dimension embedding matrix is used to mine the low-dimension consensus information of each view. The proposed method demonstrates its superiority by outperforming eight multi-view clustering algorithms on nine datasets, as evidenced by the clustering results.},
  archive      = {J_PR},
  author       = {Zengbiao Yang and Yihua Tan and Tao Yang},
  doi          = {10.1016/j.patcog.2024.110716},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110716},
  shortjournal = {Pattern Recognition},
  title        = {Large-scale multi-view clustering via matrix factorization of consensus graph},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-FaceV2: A scale and occlusion aware face detector.
<em>PR</em>, <em>155</em>, 110714. (<a
href="https://doi.org/10.1016/j.patcog.2024.110714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, face detection algorithms based on deep learning have made great progress. Nevertheless, the effective utilization of face detectors for small and occlusion faces remains challenging, primarily stemming from the limitations in pixel information and the presence of missing features. In this paper, we propose a novel real-time face detector, YOLO-FaceV2, built upon the YOLOv5 architecture. Our approach introduces a Receptive Field Enhancement (RFE) module designed to extract multi-scale pixel information and augment the receptive field for accurately detecting small faces. To address issues related to face occlusion, we introduce an attention mechanism termed the Separated and Enhancement Attention Module (SEAM), which effectively focuses on the regions affected by occlusion. Furthermore, we propose a Slide Weight Function (SWF) to mitigate the imbalance between easy and hard samples. The experiments demonstrate that our YOLO-FaceV2 achieves performance exceeding the state-of-the-art on the WiderFace validation dataset. Source code and pre-trained model are available at https://github.com/Krasjet-Yu/YOLO-FaceV2 .},
  archive      = {J_PR},
  author       = {Ziping Yu and Hongbo Huang and Weijun Chen and Yongxin Su and Yahui Liu and Xiuying Wang},
  doi          = {10.1016/j.patcog.2024.110714},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110714},
  shortjournal = {Pattern Recognition},
  title        = {YOLO-FaceV2: A scale and occlusion aware face detector},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion-guided spatiotemporal multitask feature
discrimination for self-supervised video representation learning.
<em>PR</em>, <em>155</em>, 110713. (<a
href="https://doi.org/10.1016/j.patcog.2024.110713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Powerful self-supervised representation models are able to step out of the traditional supervised paradigm and rely merely on unlabeled data to achieve a deep understanding of visual semantic features. However, previous approaches may suffer from background scene bias, making it difficult to perform a comprehensive exploration of video spatiotemporal structure. To address this challenge, this paper proposes a self-supervised video representation learning framework of m otion-guided s patiotemporal m ultitask f eature d iscrimination (MSMFD). The method mainly utilizes the consistency of motion cues between different views to guide the model for spatial and temporal feature similarity discrimination. Specifically, the model first selects video clips with large motion amplitudes based on the collected optical flow maps. Subsequently, the model introduces an instance discrimination task for overall spatiotemporal structure perception of the video, while a shuffled triplet and an augmented quadruple task are created to further enhance the exploration of intraframe sequence order and local spatial fine-grained. Furthermore, we propose joint motion alignment of spatial, temporal, and spatiotemporal dimensions under different views as a powerful compensation for acquiring motion features. Experimental results demonstrate that our self-supervised method is effective for learning video representations and achieves competitive performance in action recognition and video retrieval tasks compared to other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Shuai Bi and Zhengping Hu and Hehao Zhang and Jirui Di and Zhe Sun},
  doi          = {10.1016/j.patcog.2024.110713},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110713},
  shortjournal = {Pattern Recognition},
  title        = {Motion-guided spatiotemporal multitask feature discrimination for self-supervised video representation learning},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond local patches: Preserving global–local interactions
by enhancing self-attention via 3D point cloud tokenization.
<em>PR</em>, <em>155</em>, 110712. (<a
href="https://doi.org/10.1016/j.patcog.2024.110712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based architectures have recently shown impressive performance on various point cloud understanding tasks such as 3D object shape classification and semantic segmentation. Particularly, this can be attributed to their self-attention mechanism, which has the ability to capture long-range dependencies. However, current methods have constrained it to operate in local patches due to its quadratic memory constraints. This hinders their generalization ability and scaling capacity due to the loss of non-locality in early layers. To tackle this issue, we propose a window-based transformer architecture that captures long-range dependencies while aggregating information in the local patches. We do this by interacting each window with a set of global point cloud tokens — a representative subset of the entire scene — and augmenting the local geometry through a 3D Histogram of Oriented Gradients (HOG) descriptor. Through a series of experiments on segmentation and classification tasks, we show that our model exceeds the state-of-the-art on S3DIS semantic segmentation (+1.67% mIoU), ShapeNetPart part segmentation (+1.03% instance mIoU) and performs competitively on ScanObjectNN 3D object classification. 1},
  archive      = {J_PR},
  author       = {M.Q. Khan and M. Shahzad and S.A. Khan and M.M. Fraz and X.X. Zhu},
  doi          = {10.1016/j.patcog.2024.110712},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110712},
  shortjournal = {Pattern Recognition},
  title        = {Beyond local patches: Preserving global–local interactions by enhancing self-attention via 3D point cloud tokenization},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-lingual font style transfer with full-domain
convolutional attention. <em>PR</em>, <em>155</em>, 110709. (<a
href="https://doi.org/10.1016/j.patcog.2024.110709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new cross-lingual font style transfer model, FCAGAN, which enables font style transfer between different languages by observing a small number of samples. Most previous work has been on style transfer of different fonts for single language content, but in our task we can learn the font style of one language and migrate it to another. We investigated the drawbacks of related studies and found that existing cross-lingual approaches cannot perfectly learn styles from other languages and maintain the integrity of their own content. Therefore, we designed a new full-domain convolutional attention (FCA) module in combination with other modules to better learn font styles, and a multi-layer perceptual discriminator to ensure character integrity. Experiments show that using this model provides more satisfying results than the current cross-lingual font style transfer methods. Code can be found at https://github.com/jtlxlf/FCAGAN .},
  archive      = {J_PR},
  author       = {Hui-huang Zhao and Tian-le Ji and Paul L. Rosin and Yu-Kun Lai and Wei-liang Meng and Yao-nan Wang},
  doi          = {10.1016/j.patcog.2024.110709},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110709},
  shortjournal = {Pattern Recognition},
  title        = {Cross-lingual font style transfer with full-domain convolutional attention},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring target-related information with reliable global
pixel relationships for robust RGB-t tracking. <em>PR</em>,
<em>155</em>, 110707. (<a
href="https://doi.org/10.1016/j.patcog.2024.110707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T Siamese trackers have drawn continuous interest in recent years due to their proper trade-off between accuracy and speed. However, they are sensitive to the background distractors in some challenging cases, thereby inducing unreliable response positions. To overcome such drawbacks, we advance a new RGB-T Siamese tracker, named SiamTIH, which will advance the RGB-T Siamese trackers’ discriminability against distractors by exploiting target-related information and reliable global pixel relationships within multi-modal data. Specifically, we propose a target-related feature enhancement module (TFE) to highlight such areas in the detection branch that are similar to the templates and suppress those background distractor regions that are significantly different from the templates but are greatly informative. Then, we propose an intra- and mutual-modal attention based multi-modal feature fusion module (IMA-MF) to capture the reliable global pixel relationships within multi-modal data. Especially, the intra-modal attention is used to capture the global pixel relationships within each single modality data, and the mutual-modal attention is utilized to enhance the feature representation of the current modality by overall pixel relationships as well as modality-specific relationships. Finally, we propose a hard-focused online classifier (HFOC) that combines an offline classifier and an online classifier to further improve the robustness of our tracker. Besides, the proposed framework is further extended to a Transformer based tracker to verify its generality. Extensive experiments on three RGB-T benchmarks demonstrate that our new RGB-T tracker outperforms the existing ones and maintains real-time performance, exceeding on average 30 frames per second (FPS). The code will be available at https://github.com/Tianlu-Zhang/SiamTIH .},
  archive      = {J_PR},
  author       = {Tianlu Zhang and Xiaoyi He and Yongjiang Luo and Qiang Zhang and Jungong Han},
  doi          = {10.1016/j.patcog.2024.110707},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110707},
  shortjournal = {Pattern Recognition},
  title        = {Exploring target-related information with reliable global pixel relationships for robust RGB-T tracking},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). View-unaligned clustering with graph regularization.
<em>PR</em>, <em>155</em>, 110706. (<a
href="https://doi.org/10.1016/j.patcog.2024.110706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current multi-view clustering modeling scenarios, the cross-view correspondence of the data is generally presumed in advance. However, this assumption is inevitably violated in practical applications as each view is independently processed during data collection and transmission, thus resulting in the view-unaligned problem (VuP). The absence of cross-view correspondence between the data renders most existing multi-view clustering methods ineffective in addressing the VuP. To address this problem, we propose a novel view-unaligned clustering method, termed View-unaligned Clustering with Graph regularization (VuCG), which performs latent embedding learning on manifold, latent embedding alignment and partition generation in a one-stage manner. Specifically, we implement the latent embedding learning on manifold by seeking a matrix factorization respecting the graph structure, and perform latent embedding alignment by identifying the counterpart from a selected template of the shuffled embedding using global and local information of the data in the latent space. Additionally, we obtain the partition of the aligned data by applying the relaxed k k -means algorithm to the aligned embeddings. Extensive experimental results on four practical datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Junfeng Cao and Wenhua Dong and Jing Chen},
  doi          = {10.1016/j.patcog.2024.110706},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110706},
  shortjournal = {Pattern Recognition},
  title        = {View-unaligned clustering with graph regularization},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A transformer-based visual object tracker via learning
immediate appearance change. <em>PR</em>, <em>155</em>, 110705. (<a
href="https://doi.org/10.1016/j.patcog.2024.110705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has shown its great strength in visual object tracking due to its effective attention mechanism, but most prevailing transformer-based trackers only explore temporal information frame by frame, thus overlooking the rich context information inherent in videos. To alleviate this problem, we propose a transformer-based tracker via learning immediate appearance change information in videos, called IAC-tracker. The proposed tracker enhances the perception of the immediate motion state to improve the performance of single target tracking. IAC-tracker contains three key components: a spatial information extractor (SIE) with a superior attention mechanism to progressively extract spatial information, a temporal information extractor (TIE) with a designed temporal attention mechanism to progressively learn target immediate appearance change, and a novel spatial–temporal context enhanced fusion module integrating the information from SIE and TIE to prepare for the final prediction head. Comparison experiments with state-of-the-art trackers on six challenging datasets demonstrate the superior performance of IAC-tracker with real-time running speed.},
  archive      = {J_PR},
  author       = {Yifan Li and Xiaotao Liu and Dian Yuan and Jiaoying Wang and Peng Wu and Jing Liu},
  doi          = {10.1016/j.patcog.2024.110705},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110705},
  shortjournal = {Pattern Recognition},
  title        = {A transformer-based visual object tracker via learning immediate appearance change},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative action tubelet detector for weakly-supervised
action detection. <em>PR</em>, <em>155</em>, 110704. (<a
href="https://doi.org/10.1016/j.patcog.2024.110704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel framework for spatiotemporal action detection using only video-level class labels as weak supervision. Traditional fully-supervised approaches rely on labor-intensive manual annotation of bounding boxes for each frame. In contrast, collecting video-level class labels is significantly less tedious and more feasible compared to annotating frame-level sequences with bounding boxes. To address this challenge, we propose a discriminative action tubelet detector, called DAT-detector, designed to discern discriminative tubelets from action tubelet proposals (ATPs). Whereas the previous approaches have only focused on tubelet selection among the predefined object proposals, our DAT-detector prioritizes the generation of more precise action tubelets using regression and attention modules. Moreover, we introduce an ATP generation method that enhances the quality of tubelet proposals. Our approach achieves state-of-the-art performance on several benchmarks, and also demonstrates competitive performance even with fully-supervised approaches.},
  archive      = {J_PR},
  author       = {Jiyoung Lee and Seungryong Kim and Sunok Kim and Kwanghoon Sohn},
  doi          = {10.1016/j.patcog.2024.110704},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110704},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative action tubelet detector for weakly-supervised action detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). M3Net: Movement enhancement with multi-relation toward
multi-scale video representation for temporal action detection.
<em>PR</em>, <em>155</em>, 110702. (<a
href="https://doi.org/10.1016/j.patcog.2024.110702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locating boundary is very important for Temporal Action Detection (TAD) and is a key factor affecting the performance of TAD. However, two factors lead to inaccurate boundary localization: the movement feature submergence and the existence of multi-scale actions. In this work, to address the submergence of movement feature, we design the Movement Enhance Module (MEM), in which the Movement Feature Extractor (MFE) and Multi-Relation Module (MRM) are used to highlight short-term and long-term movement information respectively. To address the characteristic of multi-scale actions, we propose a Scale Feature Pyramid Network (SFPN) to detect multi-scale actions and design a two-stage training strategy that makes each layer focus on a specific scale action. These tow modules are integrated as M 3 N e t M3Net , and extensive experiments demonstrate its effectiveness. M 3 N e t M3Net outperforms other representative TAD methods on ActivityNet-1.3 and THUMOS-14.},
  archive      = {J_PR},
  author       = {Zixuan Zhao and Dongqi Wang and Xu Zhao},
  doi          = {10.1016/j.patcog.2024.110702},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110702},
  shortjournal = {Pattern Recognition},
  title        = {M3Net: Movement enhancement with multi-relation toward multi-scale video representation for temporal action detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incomplete RGB-d salient object detection: Conceal,
correlate and fuse. <em>PR</em>, <em>155</em>, 110700. (<a
href="https://doi.org/10.1016/j.patcog.2024.110700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating RGB and depth information has advanced salient object detection but low-quality depth maps lead to inaccurate results. The current methods address this issue by employing either a weighting approach or by estimating depth images directly from RGB images. However, these methods face limitations in low-contrast RGB scenarios and fluctuating illumination conditions. To overcome these limitations, a new model has been proposed that discards low-quality depth images and formulates an incomplete multi-modality salient object detection learning. To the best of our knowledge, this is the first incomplete multi-modality salient object detection model that is capable of describing the common latent multi-modality correlation representation between RGB and depth modalities. The model acquires a resilient representation of multiple modalities even when some depth samples are missing due to noise or data scarcity. The proposed approach follows a three-step process: concealing modality-specific representation, correlating common latent representation, and fusing multilevel representation. We processed shallow and deep features separately in Shallow Common Latent Representation (SCLR) block and Deep Common Latent Representation (DCLR) block, respectively. The model outperforms 14 state-of-the-art saliency detectors on 6 benchmark datasets.},
  archive      = {J_PR},
  author       = {Samra Kanwal and Imtiaz Ahmad Taj},
  doi          = {10.1016/j.patcog.2024.110700},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110700},
  shortjournal = {Pattern Recognition},
  title        = {Incomplete RGB-D salient object detection: Conceal, correlate and fuse},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting edge detection via fusing spatial and frequency
domains. <em>PR</em>, <em>155</em>, 110699. (<a
href="https://doi.org/10.1016/j.patcog.2024.110699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based edge detection methods have shown great advantages and obtained promising performance. However, most of the current methods only extract features from the spatial (RGB) domain for edge detection and the information that can be mined is limited. As a result, they could not work well for the scenarios where the object is similar in color to the background. To combat this challenge, we propose a novel edge detection method by incorporating the features of both spatial and frequency domains, named Fusing Spatial and Frequency Domains (FSFD). A Frequency Perception (FP) module is constructed to extract the edges of objects in the frequency domain, which can avoid the indistinguishable situation in the spatial domain due to similar colors. A Multi-Scale Enhancement (MSE) module is designed to learn multi-scale feature in spatial domain, enabling the model to perceive edges of small objects. Spatial-Frequency Fusion (S2F) module is further introduced to fuse the features of spatial and frequency domains using an online learning manner. Adequate experiments are conducted on popular BSDS500, NYUDV2, and Multicue datasets. The results show that our method can outperform other methods as the state-of-the-art when dealing with the problem of edge detection. The codes will be released on https://github.com/JingDongdong/FSFD .},
  archive      = {J_PR},
  author       = {Dongdong Jing and Huikai Shao and Dexing Zhong},
  doi          = {10.1016/j.patcog.2024.110699},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110699},
  shortjournal = {Pattern Recognition},
  title        = {Boosting edge detection via fusing spatial and frequency domains},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep graph matching meets mixed-integer linear programming:
Relax or not ? <em>PR</em>, <em>155</em>, 110697. (<a
href="https://doi.org/10.1016/j.patcog.2024.110697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph matching is an important problem that has received widespread attention, especially in the field of computer vision. Recently, state-of-the-art methods seek to incorporate graph matching with deep learning. However, there is no research to explain what role the graph matching algorithm plays in the model. Therefore, we propose an approach integrating a MILP formulation of the graph matching problem. This formulation is solved to optimal and it provides inherent baseline. Meanwhile, similar approaches are derived by releasing the optimal guarantee of the graph matching solver and by introducing a quality level. This quality level controls the quality of the solutions provided by the graph matching solver. In addition, several relaxations of the graph matching problem are put to the test. Our experimental evaluation gives several theoretical insights and guides the direction of deep graph matching methods.},
  archive      = {J_PR},
  author       = {Zhoubo Xu and Puqing Chen and Romain Raveaux and Xin Yang and Huadong Liu},
  doi          = {10.1016/j.patcog.2024.110697},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110697},
  shortjournal = {Pattern Recognition},
  title        = {Deep graph matching meets mixed-integer linear programming: Relax or not ?},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global–local consistent semi-supervised segmentation of
histopathological image with different perturbations. <em>PR</em>,
<em>155</em>, 110696. (<a
href="https://doi.org/10.1016/j.patcog.2024.110696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A histopathological image is a microscopic image applied to examine cellular and tissue structures and identify any abnormalities or disease processes. Histopathological image segmentation is a prerequisite step for analyzing histopathological images that can divide an image into meaningful regions or objects to accurately classify and analyze tissue structures, cellular regions, or particular histological entities. However, the existing deep learning based pathological image segmentation methods require huge annotation efforts from the pathologists, which is labor-intensive and time-consuming. In this scenario, it has become a hotspot to leverage abundantly available unlabeled data to help learn segmentation models given limited labeled data. In this paper, we propose a global–local consistent semi-supervised segmentation (GLCS) model that enforce the consistency of the segmentation results with weak and strong perturbations on unlabeled data. In GLCS, we firstly generate different weak perturbations for each unlabeled sample, and then add a regularization term to ensure the segmentation consistency among different weak perturbations. Next, different from the existing studies applying the regression methods to match the segmentation results among different perturbations, our methods are based on the generative adversarial learning that can keep the global structure consistency among unlabeled data with different strength of perturbations. Finally, we also add a patch-correlation based regularization term to preserve the local structure similarity among different perturbations images. We validate our GLCS on three datasets, i.e. Glas, Crag and MoNuSeg. The experimental results show that our method can achieve to the dice ratio of 90.35, 82.61 and 81.60 with 1:1 proportion of labeled data, which are significantly superior to the state-of-the-art semi-supervised histopathological image segmentation methods. Our code is public available at https://github.com/ISBELLAG/GLCS .},
  archive      = {J_PR},
  author       = {Xi Guan and Qi Zhu and Liang Sun and Junyong Zhao and Daoqiang Zhang and Peng Wan and Wei Shao},
  doi          = {10.1016/j.patcog.2024.110696},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110696},
  shortjournal = {Pattern Recognition},
  title        = {Global–local consistent semi-supervised segmentation of histopathological image with different perturbations},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Robust self-expression learning with adaptive noise
perception. <em>PR</em>, <em>155</em>, 110695. (<a
href="https://doi.org/10.1016/j.patcog.2024.110695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-expression learning methods often obtain a coefficient matrix to measure the similarity between pairs of samples. However, directly using the raw data to represent each sample under the self-expression framework may not be ideal, as noise points are inevitably involved in the process of representing clean samples. To address this issue, this work proposes a novel self-expression model called robust Self-Expression learning with adaptive Noise Perception (SENP). SENP decomposes each sample into a clean part and a noisy part, and samples with large self-expression losses can be recognized as the noise points. A reliable coefficient matrix can then be learned by using only the clean points to reconstruct the clean part of each sample. By simultaneously detecting the noisy part of each sample and noise points, and adaptively mitigating their negative impacts, the representative ability of the generated coefficient matrix is improved. Moreover, inspired by the solution of non-negative matrix factorization (NMF), an effective algorithm is formed to optimize SENP. Extensive experiments on well-known benchmark datasets demonstrate the superiority of SENP compared to several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yangbo Wang and Jie Zhou and Jianglin Lu and Jun Wan and Can Gao and Qingshui Lin},
  doi          = {10.1016/j.patcog.2024.110695},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110695},
  shortjournal = {Pattern Recognition},
  title        = {Robust self-expression learning with adaptive noise perception},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLAN: A graph-based linear assignment network. <em>PR</em>,
<em>155</em>, 110694. (<a
href="https://doi.org/10.1016/j.patcog.2024.110694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiable solvers for the linear assignment problem (LAP) have attracted much research attention in recent years, which are usually embedded into learning frameworks as components. However, previous algorithms, with or without learning strategies, usually suffer from the degradation of the optimality with the increment of the problem size. In this paper, we propose a learnable linear assignment solver based on deep graph networks. Specifically, we first transform the cost matrix to a bipartite graph and convert the assignment task to the problem of selecting reliable edges from the constructed graph. Subsequently, a deep graph network is developed to aggregate and update the features of nodes and edges. Finally, the network predicts a label for each edge that indicates the assignment relationship. The experimental results on a synthetic dataset reveal that our method outperforms state-of-the-art baselines and achieves consistently high accuracy with the increment of the problem size. Furthermore, we also embed the proposed solver, in comparison with state-of-the-art baseline solvers, into a popular multi-object tracking (MOT) framework to train the tracker in an end-to-end manner. The experimental results on MOT benchmarks illustrate that the proposed LAP solver improves the tracker by the largest margin.},
  archive      = {J_PR},
  author       = {He Liu and Tao Wang and Congyan Lang and Songhe Feng and Yi Jin and Yidong Li},
  doi          = {10.1016/j.patcog.2024.110694},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110694},
  shortjournal = {Pattern Recognition},
  title        = {GLAN: A graph-based linear assignment network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMIGNet: Cross-modal inverse guidance network for RGB-depth
salient object detection. <em>PR</em>, <em>155</em>, 110693. (<a
href="https://doi.org/10.1016/j.patcog.2024.110693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the majority of RGB-Depth salient object detection (SOD) methods utilize the encoder–decoder architecture. However, they often fail to utilize the encoding and decoding features fully. This paper rethinks the differences and correlations between them and proposes the Cross-Modal Inverse Guidance Network (CMIGNet) for SOD. Specifically, a Multi-level Feature Guidance Enhancement (MFGE) module is integrated into every layer of the foundational network. It employs a high-level decoding feature to guide the low-level RGB and depth encoding features, facilitating the rapid identification of salient regions and noise removal. The dual-stream encoding features guided by the MFGE module are combined using the proposed Dual-Stream Interactive Fusion (DSIF) module. It could simultaneously reduce dependence on two modal features during the fusion process. Thus, the impact on the results can be reduced in complex scenes when one modality is absent or confusing. Finally, the edge information is supplemented using the proposed Edge Refinement Awareness (ERA) module to generate the final salient map. Comparisons on seven widely used and one latest challenging RGB-D datasets show that the performance of the proposed CMIGNet is highly competitive with the state-of-the-art RGB-Depth SOD models. Additionally, our model is lighter and faster.},
  archive      = {J_PR},
  author       = {Hegui Zhu and Jia Ni and Xi Yang and Libo Zhang},
  doi          = {10.1016/j.patcog.2024.110693},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110693},
  shortjournal = {Pattern Recognition},
  title        = {CMIGNet: Cross-modal inverse guidance network for RGB-depth salient object detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DGRM: Diffusion-GAN recommendation model to alleviate the
mode collapse problem in sparse environments. <em>PR</em>, <em>155</em>,
110692. (<a href="https://doi.org/10.1016/j.patcog.2024.110692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) has been widely adopted in recommender systems (RSs) to improve the recommendation accuracy. However, existing GAN-based models often suffer from the mode collapse problem in sparse environments and fail to adequately capture the complexity of user preferences and behaviors, which affects recommendation performance. To address these issues, we introduce a diffusion model (DM) into the GAN framework, proposing an efficient Diffusion-GAN recommendation model (DGRM) to achieve mutual enhancement between the two generative models. This model first utilizes the forward process of DM to generate conditional vectors that guide the training of the GAN generator. Subsequently, the backward process of DM assists the GAN discriminator using Wasserstein distance during adversarial training. The Wasserstein distance is adopted to solve the asymmetry of Kullback-Leibler (KL) divergence as a loss function in traditional GANs. Experiments on multiple datasets demonstrate that the proposed model effectively alleviates mode collapse and surpasses other state-of-the-art (SOTA) methods in various evaluation metrics.},
  archive      = {J_PR},
  author       = {Deng Jiangzhou and Wang Songli and Ye Jianmei and Ji Lianghao and Wang Yong},
  doi          = {10.1016/j.patcog.2024.110692},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110692},
  shortjournal = {Pattern Recognition},
  title        = {DGRM: Diffusion-GAN recommendation model to alleviate the mode collapse problem in sparse environments},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust image hiding network with frequency and spatial
attentions. <em>PR</em>, <em>155</em>, 110691. (<a
href="https://doi.org/10.1016/j.patcog.2024.110691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convert Image Communication (CIC) is a promising technology to protect the privacy of images. Recently, the emergence of robust CIC resistant to JPEG compression has gained due to the widespread use of JPEG compression in image communication. This paper introduces a R ̲ R̲ obust image hiding network with F ̲ F̲ requency and S ̲ S̲ patial A ̲ A̲ ttentions (RFSA) to implement robust CIC. RFSA can hide an image within another image with high robust. It incorporates multiple image attentions corresponding to imperceptibility, recovered image quality, and resistance to JPEG compression, which ensure that secret images are hidden within regions that cause little distortion and can well withstand JPEG compression. Additionally, two encoders, that is, a frequency encoder and a spatial encoder, are mixed to adaptively embed secret images across both frequency and spatial domains. Experimental results demonstrate that the proposed scheme not only maintains high image quality and capacity but also exhibits exceptional resistance to JPEG compression compared to other state-of-the-art image hiding methods. The average Peak Signal-to-Noise Ratio (PSNR) of the recovered image remains at 24.96 dB even under JPEG compression with a quality factor of 55.},
  archive      = {J_PR},
  author       = {Xiaobin Zeng and Bingwen Feng and Zhihua Xia and Zecheng Peng and Tiewei Qin and Wei Lu},
  doi          = {10.1016/j.patcog.2024.110691},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110691},
  shortjournal = {Pattern Recognition},
  title        = {Robust image hiding network with frequency and spatial attentions},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph attentive dual ensemble learning for unsupervised
domain adaptation on point clouds. <em>PR</em>, <em>155</em>, 110690.
(<a href="https://doi.org/10.1016/j.patcog.2024.110690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the annotation difficulty of point clouds, Unsupervised Domain Adaptation (UDA) is a promising direction to address unlabeled point cloud classification and segmentation. Recent works show that adding a self-supervised learning branch for target domain training consistently boosts UDA point cloud tasks. However, most of these works simply resort to geometric deformation, which ignores semantic information and is hard to bridge the domain gap. In this paper, we propose a novel self-learning strategy for UDA on point clouds, termed as Graph Attentive Dual Ensemble learning (GRADE), which delivers semantic information directly. Specifically, with a pre-training process on the source domain, GRADE further builds dual collaborative training branches on the target domain, where each of them constructs a temporal average teacher model and distills its pseudo labels to the other branch. To achieve faithful labels from each teacher model, we improve the popular DGCNN architecture by introducing a dynamic graph attentive module to mine the relation between local neighborhood points. We conduct extensive experiments on several UDA point cloud benchmarks, and the results demonstrate that our GRADE method outperforms the state-of-the-art methods on both classification and segmentation tasks with clear margins.},
  archive      = {J_PR},
  author       = {Qing Li and Chuan Yan and Qi Hao and Xiaojiang Peng and Li Liu},
  doi          = {10.1016/j.patcog.2024.110690},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110690},
  shortjournal = {Pattern Recognition},
  title        = {Graph attentive dual ensemble learning for unsupervised domain adaptation on point clouds},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic and static fusion mechanisms of infrared and visible
images. <em>PR</em>, <em>155</em>, 110689. (<a
href="https://doi.org/10.1016/j.patcog.2024.110689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper propose a dynamic fusion mechanism of infrared and visible images, named DIFM, capable of solving the static fusion optimization problem. The DIFM correlates the image fusion quality with the image restoration quality to construct a unified optimization loss function. According to the DIFM, a dynamic image fusion network of infrared and visible images is constructed and is therefore denoted with DF-Net. Specifically, the DF-Net comprises two modules, i.e., the dynamic fusion module (DFM) and the self-learning dynamic restoration module (SLDRM). In order to solve the static fusion problem of existing methods, the DFM is proposed to learn the fusion weight dynamically. Specifically, the DFM comprises a classification module (CM) and an image fusion module (IFM), which determine whether and how to fuse source images. In addition, a unified fusion loss function is introduced to obtain more hidden features of infrared and visible images in complex environments. Therefore, the stumbling block of deep learning in image fusion, i.e., static fusion, is significantly mitigated. Extensive experiments demonstrate that the dynamic fusion optimization method neatly outperforms the state-of-the-art methods in most metrics.},
  archive      = {J_PR},
  author       = {Aiqing Fang and Ying Li},
  doi          = {10.1016/j.patcog.2024.110689},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110689},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic and static fusion mechanisms of infrared and visible images},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete online cross-modal hashing with consistency
preservation. <em>PR</em>, <em>155</em>, 110688. (<a
href="https://doi.org/10.1016/j.patcog.2024.110688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online cross-modal hashing has attracted widespread attention with the rapid expansion of large-scale streaming data, which can reduce storage requirements and enhance efficiency for online cross-modal retrieval. However, despite promising progress, existing methods still suffer from defective accuracy in a way, primarily attributed to two issues: insufficient semantic information exploitation and mismatched training-retrieval process. To address these challenges, we propose a novel supervised hashing method with dual consistency preservation, called Discrete Online Cross-Modal Hashing (DOCMH). On the one hand, we design more informative continuous semantic labels and fine-grained similarity graphs to preserve semantic consistency across different streaming data chunks and modality representations. On the other hand, we propose an effective modality deviation calibration mechanism for preserving learning process consistency between the training and retrieval phases. Extensive experiments on three widely used benchmark datasets demonstrate the superior performance of the proposed DOCMH under various scenarios.},
  archive      = {J_PR},
  author       = {Xiao Kang and Xingbo Liu and Wen Xue and Xuening Zhang and Xiushan Nie and Yilong Yin},
  doi          = {10.1016/j.patcog.2024.110688},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110688},
  shortjournal = {Pattern Recognition},
  title        = {Discrete online cross-modal hashing with consistency preservation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating brain group structure methods using hierarchical
dynamic models. <em>PR</em>, <em>155</em>, 110687. (<a
href="https://doi.org/10.1016/j.patcog.2024.110687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph theory, complex structures are studied, as well as the dynamics of the connectivity strength of this structure. However, in the estimation procedure, particular characteristics need to be considered at some level as informative when estimating the characteristics of a group. This work proposes a model that provides dynamic estimation of the network structure based on a model that makes it possible to incorporate hierarchy (individual information) in the process. In addition, we show the feasibility of modeling a complex structure by levels, exemplifying this by cluster analysis as a visualization of the embedding projection reduction space. Our case study is a neuroscience experiment, which needs to estimate the brain connectivity map, that is, to study the information flow of the brain in resting-stage subjects. Methods for estimating group networks can be grouped into the following 4 categories: group-structure (GS), virtual-typical-subject (VTS), common-structure (CS), and individual-structure (IS). These four group-structure estimation methods were compared in the context of the Multiregression Dynamic Models. Results showed that the proposed Bayesian Network Structure Dynamic estimation, using GS and hierarchical dynamic models, accommodates the latent/personal information in the estimation process by extracting the pattern shared between them. Moreover, the cluster analysis estimation corroborates the empirical results and expert judgments.},
  archive      = {J_PR},
  author       = {Lilia Costa and Osvaldo Anacleto and Diego C. Nascimento and James Q. Smith and Catriona M. Queen and Francisco Louzada and Thomas Nichols},
  doi          = {10.1016/j.patcog.2024.110687},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110687},
  shortjournal = {Pattern Recognition},
  title        = {Evaluating brain group structure methods using hierarchical dynamic models},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo-set frequency refinement architecture for
fine-grained few-shot class-incremental learning. <em>PR</em>,
<em>155</em>, 110686. (<a
href="https://doi.org/10.1016/j.patcog.2024.110686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot class-incremental learning was introduced to solve the model adaptation problem for new incremental classes with only a few examples while still remaining effective for old data. Although recent state-of-the-art methods make some progress in improving system robustness on common datasets, they fail to work on fine-grained datasets where inter-class differences are small. The problem is mainly caused by: (1) the overlapping of new data and old data in the feature space during incremental learning, which means old samples can be falsely classified as newly introduced classes and induce catastrophic forgetting phenomena; (2) lacking discriminative feature learning ability to identify fine-grained objects. In this paper, a novel Pseudo-set Frequency Refinement (PFR) architecture is proposed to tackle these problems. We design a pseudo-set training strategy to mimic the incremental learning scenarios so that the model can better adapt to novel data in future incremental sessions. Furthermore, separate adaptation tasks are developed by utilizing frequency-based information to refine the original features and address the above challenging problems. More specifically, the high and low-frequency components of the images are employed to enrich the discriminative feature analysis ability and incremental learning ability of the model respectively. The refined features are used to perform inter-class and inter-set analyses. Extensive experiments show that the proposed method consistently outperforms the state-of-the-art methods on four fine-grained datasets.},
  archive      = {J_PR},
  author       = {Zicheng Pan and Weichuan Zhang and Xiaohan Yu and Miaohua Zhang and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2024.110686},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110686},
  shortjournal = {Pattern Recognition},
  title        = {Pseudo-set frequency refinement architecture for fine-grained few-shot class-incremental learning},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An empirical study on the robustness of the segment anything
model (SAM). <em>PR</em>, <em>155</em>, 110685. (<a
href="https://doi.org/10.1016/j.patcog.2024.110685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Segment Anything Model (SAM) is a foundation model for general image segmentation. Although it exhibits impressive performance predominantly on natural images, understanding its robustness against various image perturbations and domains is critical for real-world applications where such challenges frequently arise. In this study we conduct a comprehensive robustness investigation of SAM under diverse real-world conditions. Our experiments encompass a wide range of image perturbations. Our experimental results demonstrate that SAM’s performance generally declines under perturbed images, with varying degrees of vulnerability across different perturbations. By customizing prompting techniques and leveraging domain knowledge based on the unique characteristics of each dataset, the model’s resilience to these perturbations can be enhanced, addressing dataset-specific challenges. This work sheds light on the limitations and strengths of SAM in real-world applications, promoting the development of more robust and versatile image segmentation solutions. Our code is available at https://github.com/EternityYW/SAM-Robustness/ .},
  archive      = {J_PR},
  author       = {Yuqing Wang and Yun Zhao and Linda Petzold},
  doi          = {10.1016/j.patcog.2024.110685},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110685},
  shortjournal = {Pattern Recognition},
  title        = {An empirical study on the robustness of the segment anything model (SAM)},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label feature selection using self-information in
divergence-based fuzzy neighborhood rough sets. <em>PR</em>,
<em>155</em>, 110684. (<a
href="https://doi.org/10.1016/j.patcog.2024.110684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection corresponds to pattern recognition and knowledge mining, and its application has been expanded to different scenarios. As an excellent processing platform for uncertain and ambiguous information, divergence-based fuzzy rough sets (Div-FRSs) have been proposed and applied to feature selection. However, there are three critical problems to be solved when applying Div-FRSs to multi-label learning. The first is how to effectively dispose the noise produced by features in multi-label data. The second is how to synthetically consider the relevance among all labels. The last is how to thoroughly mine the uncertainty brought by upper approximations neglected in Div-FRSs existing researches. To address these issues, this study presents a new divergence-based fuzzy neighborhood rough set model (Div-FNRSs) for multi-label learning using self-information. First, the divergence-based fuzzy neighborhood relation and class are gradually raised to manage the noise in multi-label data, and fuzzy decision is introduced to dispose all labels as a whole. Combining them together, a new model Div-FNRSs is constructed. Then, divergence-based fuzzy neighborhood self-information containing upper approximations and lower approximations is designed to depict distinguishing ability of features through three-level uncertainty measure establishment and granulation property exploration. Furthermore, feature significance for choosing the optimal features is given and it motivates a heuristic feature-selection algorithm DivFNSI-FS. Finally, data experiments are completed to validate DivFNSI-FS effectiveness with six state-of-the-art multi-label feature selection approaches on fourteen multi-label datasets. A conclusion can be drawn that DivFNSI-FS outperforms existing algorithms to obtain better performance on eight commonly-used evaluation indexes.},
  archive      = {J_PR},
  author       = {Jiefang Jiang and Xianyong Zhang and Zhong Yuan},
  doi          = {10.1016/j.patcog.2024.110684},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110684},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection using self-information in divergence-based fuzzy neighborhood rough sets},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual space-based fuzzy graphs and orthogonal basis
clustering for unsupervised feature selection. <em>PR</em>,
<em>155</em>, 110683. (<a
href="https://doi.org/10.1016/j.patcog.2024.110683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection (UFS) takes an important position because gaining the class labels is laborious or even impossible. In the domain of UFS, clustering is a major means to exploit label information. The existing methods either could not model a clear clustering structure or could not utilize the local structure of data. Consequently, a new clustering method in combination with graph learning is proposed in this work. Specifically, for clustering, orthogonal basis clustering is introduced, where orthogonal constraints are imposed on the cluster center matrix and the clustering matrix. The clustering indicator matrix is also imposed by a non-negative constraint. A clearer clustering structure and more independent clustering centers are obtained through these constraints. For local preservation, given that traditional graphs for keeping the local manifold are faced with the problem of imbalanced neighbors, the fuzzy graph is introduced to acquire a robust structure, which is applied to both data space and feature space. The topological structure in these spaces can be well maintained. For the choice of salient features, ℓ 2 , 0 ℓ2,0 -norm regularization is imposed on the projection matrix. The object function is solved alternately. Then, a feature selection algorithm is designed. Experiments are designed and performed on nine real-world data sets. The results attest to the effectiveness of the proposed algorithm compared with other relevant algorithms.},
  archive      = {J_PR},
  author       = {Duanzhang Li and Hongmei Chen and Yong Mi and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.patcog.2024.110683},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110683},
  shortjournal = {Pattern Recognition},
  title        = {Dual space-based fuzzy graphs and orthogonal basis clustering for unsupervised feature selection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). G2-SCANN: Gaussian-kernel graph-based SLD clustering
algorithm with natural neighbourhood. <em>PR</em>, <em>155</em>, 110682.
(<a href="https://doi.org/10.1016/j.patcog.2024.110682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For most clustering methods, not only the number of clusters must be set in advance, but also various hyperparameters such as initial centroids, number of nearest neighbours, the minimum number of points, neighbourhood radius, and cutoff distance all require pre-specification. As one of the most promising unsupervised learning methods in machine intelligence, existing clustering methods cannot simultaneously handle datasets with arbitrary shapes, different densities, distinct sizes, and overlapping. Background outliers and high dimensionality make clustering problems more challenging. In this paper, we propose a novel universal clustering methodology, called G2-SCANN, which yields the best clustering performance for all 30 synthetic and real datasets without any hyperparameter tuning if the exact number of clusters is known. Firstly, the shortest path length (SPL) in complex network or graph-based geodesic distance is used to give a locally backbone-structured description of graph vertex similarity. Accordingly, SPL-weighted local degree (SLD) is defined as vertex attributes of a SPL-weighted graph expressed by G2-SPL adjacency matrix with ε-natural neighbourhood. Secondly, the process of calculating SLD for every data point in a bottom-up way directly leads to division from a complete graph constituted by all data points to a group of SLD trees. This brings the interpretability and the elimination of lone trees. Thirdly, contrastive learning of largest SLD values for finding root vertices of each divisive tree is conducted and top-down category message is then transmitted from the root vertices to all the leaf ones of a SLD tree. It eventually produces tree-like clusters. Totally, the proposed G2-SCANN method leverages both local neighbouring similarity of data points and global information about data distribution and makes it perform better than other methods.},
  archive      = {J_PR},
  author       = {Zhidong Deng and Jingyi Wang},
  doi          = {10.1016/j.patcog.2024.110682},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110682},
  shortjournal = {Pattern Recognition},
  title        = {G2-SCANN: Gaussian-kernel graph-based SLD clustering algorithm with natural neighbourhood},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lie group semi-supervised FCM clustering method for image
segmentation. <em>PR</em>, <em>155</em>, 110681. (<a
href="https://doi.org/10.1016/j.patcog.2024.110681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an unsupervised clustering method with low overhead, Fuzzy C-means (FCM) clustering has been widely used in a variety of image segmentation tasks. However, existing FCM clustering methods are sensitive to image noises and are either suffer from losing of image detail or falling into local optima in identifying cluster centers. Aiming at these problems, this paper proposes a Lie group semi-supervised FCM (LieSSFCM) clustering method for image segmentation. The method maps the input image from Euclidean space to Lie group manifold by representing each image pixel as a matrix Lie group and calculates geodesic distances between group elements and cluster centers on Lie group manifold. Prior information of the image and neighborhood relationships of pixels are used to guide the initialization and constrain the update of cluster centers and the corresponding fuzzy membership matrix. The proposed LieSSFCM has been validated against two medical image datasets and was compared with seven FCM clustering methods. Experimental results along with a systematic evaluation demonstrated that the method was superior in segmentation accuracy both visually and statistically, robustness to noises, adaptability to different tasks, and stability while maintaining a moderate computational complexity.},
  archive      = {J_PR},
  author       = {Haocheng Sun and Li Liu and Fanzhang Li},
  doi          = {10.1016/j.patcog.2024.110681},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110681},
  shortjournal = {Pattern Recognition},
  title        = {A lie group semi-supervised FCM clustering method for image segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMMD: Attentive maximum mean discrepancy for few-shot image
classification. <em>PR</em>, <em>155</em>, 110680. (<a
href="https://doi.org/10.1016/j.patcog.2024.110680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based methods have attained promising performance for few-shot image classification. Maximum Mean Discrepancy (MMD) is a typical distance between distributions, requiring to compute expectations w.r.t. data distributions. In this paper, we propose Attentive Maximum Mean Discrepancy (AMMD) to measure the distances between query images and support classes for few-shot classification. Each query image is classified as the support class with minimal AMMD distance. The proposed AMMD assists MMD with distributions adaptively estimated by an Attention-based Distribution Generation Module (ADGM). ADGM is learned to put more mass on more discriminative features, which makes the proposed AMMD distance emphasize discriminative features and overlook spurious features. Extensive experiments show that our AMMD achieves competitive or state-of-the-art performance on multiple few-shot classification benchmark datasets. Code is available at https://github.com/WuJi1/AMMD .},
  archive      = {J_PR},
  author       = {Ji Wu and Shipeng Wang and Jian Sun},
  doi          = {10.1016/j.patcog.2024.110680},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110680},
  shortjournal = {Pattern Recognition},
  title        = {AMMD: Attentive maximum mean discrepancy for few-shot image classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global self-sustaining and local inheritance for source-free
unsupervised domain adaptation. <em>PR</em>, <em>155</em>, 110679. (<a
href="https://doi.org/10.1016/j.patcog.2024.110679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a practical problem called source-free unsupervised domain adaptation, which adapts a source-trained model to the target domain with unlabeled target data. To address this problem, we propose a novel GlObal self-sustAining and Local inheritance (GOAL) method. GOAL contains three components. (1) A backbone follows a mean teacher scheme. The teacher model serves as a smoothing functionality, facilitating a more consistent convergence of the student model. This capability alleviates the student model’s sensitivity to minor input data variations and enhances the overall robustness of the model. Additionally, disparities in predictions between the student and teacher models can be leveraged to identify potential noise in the data. (2) A Global Consistency Self-Sustaining mechanism for learning a stable, discriminative, and diverse prediction space. On the one hand, we employ neighbor samples and mean-teacher schemes to enhance the discriminability and stability of model predictions. On the other hand, non-neighbor samples are leveraged to augment the diversity of model predictions. Furthermore, to mitigate the impact of potential negative neighbors, we derive a weighting factor by incorporating both neighbor entropy and the top- n d nd similarity of features. (3) A Local Topology Inheritance mechanism to improve the semantic structure of the feature space. We construct a semantic topology graph based on the output predictions of the teacher model and subsequently transmit the teacher topology to the feature space of the student utilizing a local topology inheritance loss. Combining these three components, GOAL can effectively solve the source-free unsupervised domain adaptation. To the best of our knowledge, GOAL is the first attempt to perform topology inheritance for global consistency domain adaptation. Comprehensive experiments illustrate the effectiveness and superiority of GOAL in addressing source-free unsupervised domain adaptation.},
  archive      = {J_PR},
  author       = {Lin Peng and Yuhang He and Shaokun Wang and Xiang Song and Songlin Dong and Xing Wei and Yihong Gong},
  doi          = {10.1016/j.patcog.2024.110679},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110679},
  shortjournal = {Pattern Recognition},
  title        = {Global self-sustaining and local inheritance for source-free unsupervised domain adaptation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalizable framework for low-rank tensor completion
with numerical priors. <em>PR</em>, <em>155</em>, 110678. (<a
href="https://doi.org/10.1016/j.patcog.2024.110678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-Rank Tensor Completion, a method which exploits the inherent structure of tensors, has been studied extensively as an effective approach to tensor completion. Whilst such methods attained great success, none have systematically considered exploiting the numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. Despite the existence of some individual works which consider ad hoc numerical priors for specific tasks, no generalizable frameworks for incorporating numerical priors have appeared. We present the Generalized CP Decomposition Tensor Completion (GCDTC) framework, the first generalizable framework for low-rank tensor completion that takes numerical priors of the data into account. We test GCDTC by further proposing the Smooth Poisson Tensor Completion (SPTC) algorithm, an instantiation of the GCDTC framework, whose performance exceeds current state-of-the-arts by considerable margins in the task of non-negative tensor completion, exemplifying GCDTC’s effectiveness. Our code is open-source.},
  archive      = {J_PR},
  author       = {Shiran Yuan and Kaizhu Huang},
  doi          = {10.1016/j.patcog.2024.110678},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110678},
  shortjournal = {Pattern Recognition},
  title        = {A generalizable framework for low-rank tensor completion with numerical priors},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear gaussian bounding box representation and ring-shaped
rotated convolution for oriented object detection. <em>PR</em>,
<em>155</em>, 110677. (<a
href="https://doi.org/10.1016/j.patcog.2024.110677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In oriented object detection, current representations of oriented bounding boxes (OBBs) often suffer from the boundary discontinuity problem. Methods of designing continuous regression losses do not essentially solve this problem. Although Gaussian bounding box (GBB) representation avoids this problem, directly regressing GBB is susceptible to numerical instability. We propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB avoids the boundary discontinuity problem and has high numerical stability. In addition, existing convolution-based rotation-sensitive feature extraction methods only have local receptive fields, resulting in slow feature aggregation. We propose ring-shaped rotated convolution (RRC), which adaptively rotates feature maps to arbitrary orientations to extract rotation-sensitive features under a ring-shaped receptive field, rapidly aggregating features and contextual information. Experimental results demonstrate that LGBB and RRC achieve state-of-the-art performance. Furthermore, integrating LGBB and RRC into various models effectively improves detection accuracy.},
  archive      = {J_PR},
  author       = {Zhen Zhou and Yunkai Ma and Junfeng Fan and Zhaoyang Liu and Fengshui Jing and Min Tan},
  doi          = {10.1016/j.patcog.2024.110677},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110677},
  shortjournal = {Pattern Recognition},
  title        = {Linear gaussian bounding box representation and ring-shaped rotated convolution for oriented object detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DIVA: Deep unfolded network from quantum interactive patches
for image restoration. <em>PR</em>, <em>155</em>, 110676. (<a
href="https://doi.org/10.1016/j.patcog.2024.110676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a deep neural network called DIVA unfolding a baseline adaptive denoising algorithm (DeQuIP), relying on the theory of quantum many-body physics. Furthermore, it is shown that with very slight modifications, this network can be enhanced to solve more challenging image restoration tasks such as image deblurring, super-resolution and inpainting. Despite a compact and interpretable (from a physical perspective) architecture, the proposed deep learning network outperforms several recent algorithms from the literature, designed specifically for each task. The key ingredients of the proposed method are on one hand, its ability to handle non-local image structures through the patch-interaction term and the quantum-based Hamiltonian operator, and, on the other hand, its flexibility to adapt the hyperparameters patch-wisely, due to the training process.},
  archive      = {J_PR},
  author       = {Sayantan Dutta and Adrian Basarab and Bertrand Georgeot and Denis Kouamé},
  doi          = {10.1016/j.patcog.2024.110676},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110676},
  shortjournal = {Pattern Recognition},
  title        = {DIVA: Deep unfolded network from quantum interactive patches for image restoration},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view reduced dimensionality k-means clustering with
σ-norm and schatten p-norm. <em>PR</em>, <em>155</em>, 110675. (<a
href="https://doi.org/10.1016/j.patcog.2024.110675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-view high dimensional data obtained from diverse domains or various feature extractors has drawn great attention due to its reflection of different properties or distributions. In this paper, we propose a novel unsupervised multi-view clustering method, which is called Multi-View Reduced Dimensionality K-means clustering (MRDKM) and integrates the dimension reduction mechanism, σ σ -norm, Schatten p -norm, and multi-view K-means clustering. Moreover, an unsupervised optimization scheme was proposed to solve the minimization problem with good convergence properties. Comprehensive evaluations of five benchmark datasets and comparisons with several multi-view clustering algorithms demonstrate the superiority of the proposed work.},
  archive      = {J_PR},
  author       = {Xiangdong Zhang and Fangfang Li and Zhaoyang Shi and Ming Yang},
  doi          = {10.1016/j.patcog.2024.110675},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110675},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view reduced dimensionality K-means clustering with σ-norm and schatten p-norm},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EDMD: An entropy based dissimilarity measure to cluster
mixed-categorical data. <em>PR</em>, <em>155</em>, 110674. (<a
href="https://doi.org/10.1016/j.patcog.2024.110674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of clustering techniques is significantly influenced by proximity measures irrespective of type of data and categorical data is no exception. Most of the existing proximity measures for categorical data assume that all attributes contribute equally to the distance measurement which is not true. Usually, frequency or probability-based approaches are better equipped in principle to counter this issue by appropriately weighting the attributes based on the intra-attribute statistical information. However, owing to the qualitative nature of categorical features, the intra-attribute disorder is not captured effectively by the popularly used continuum form of entropy known as Shannon or information entropy. If the categorical data contains ordinal features, then the problem multiplies because the existing measures treat all attributes as nominal. To address these issues, we propose a new Entropy-based Dissimilarity measure for Mixed categorical Data (EDMD) composed of both nominal and ordinal attributes. EDMD treats both nominal and ordinal attributes separately to capture the intrinsic information from the values of two different attribute types. We apply Boltzmann’s definition of entropy, which is based on the principle of counting microstates, to exploit the intra-attribute statistical information of nominal attributes while preserving the order relationships among ordinal values in distance formulation. Additionally, the statistical significance of different attributes of the data towards dissimilarity computation is taken care of through attribute weighting. The proposed measure is free from any user-defined or domain-specific parameters and there is no prior assumption about the distribution of the data sets. Experimental results demonstrate the efficacy of EDMD in terms of cluster quality, accuracy, cluster discrimination ability, and execution time to handle mixed categorical data sets of different characteristics.},
  archive      = {J_PR},
  author       = {Amit Kumar Kar and Mohammad Maksood Akhter and Amaresh Chandra Mishra and Sraban Kumar Mohanty},
  doi          = {10.1016/j.patcog.2024.110674},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110674},
  shortjournal = {Pattern Recognition},
  title        = {EDMD: An entropy based dissimilarity measure to cluster mixed-categorical data},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel active contour model based on features for image
segmentation. <em>PR</em>, <em>155</em>, 110673. (<a
href="https://doi.org/10.1016/j.patcog.2024.110673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active contour model is an extraordinarily valuable technique in image segmentation, which is essential for image analysis and understanding. Active contour model has been widely studied because it delineates closed and smooth contours or surfaces of target objects. However, traditional active contour models underperform on complex natural images. To tackle this problem, we propose a novel active contour model framework, called FeaACM. We introduce the feature energy function into the conventional energy functional to minimize the energy functional to maintain the consistency of the object region and account for different distributions of objects and backgrounds in the feature space. To demonstrate the advantages of our method, we compare our method with the state-of-the-art methods, and show that our method achieves competitive performance. In addition, we utilize AutoEncoder technology to extract the feature of the image verifying the generality of our framework. Extensive and numerous experiments indicate that our method can segment complex natural images effectively. Our code is available at https://github.com/xuepeng1234/FeaACM .},
  archive      = {J_PR},
  author       = {Peng Xue and Sijie Niu},
  doi          = {10.1016/j.patcog.2024.110673},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110673},
  shortjournal = {Pattern Recognition},
  title        = {A novel active contour model based on features for image segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The spiking neural network based on fMRI for speech
recognition. <em>PR</em>, <em>155</em>, 110672. (<a
href="https://doi.org/10.1016/j.patcog.2024.110672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of the human brain has evolved to achieve extraordinary computing power through continuous refinement by natural selection. At present, the topology of brain-like model lacks biological plausibility. In this paper, a new brain-like model is proposed, called fMRI-SNN, which is a spiking neural network (SNN) constrained by the topology of a functional brain network from human functional Magnetic Resonance Imaging (fMRI) data. To verify its performance, this fMRI-SNN is applied to speech recognition. Our results indicate that the recognition accuracy of fMRI-SNN is superior to that of other SNNs and reported methods, and exhibits stronger performance on more difficult speech recognition tasks. Our discussion on recognition mechanism finds the advantage of fMRI-SNN is that the differences in its neuronal firing patterns are greater than those of other SNNs, since it has better information transmission ability.},
  archive      = {J_PR},
  author       = {Yihua Song and Lei Guo and Menghua Man and Youxi Wu},
  doi          = {10.1016/j.patcog.2024.110672},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110672},
  shortjournal = {Pattern Recognition},
  title        = {The spiking neural network based on fMRI for speech recognition},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ResPrune: An energy-efficient restorative filter pruning
method using stochastic optimization for accelerating CNN. <em>PR</em>,
<em>155</em>, 110671. (<a
href="https://doi.org/10.1016/j.patcog.2024.110671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) are frequently employed for image pattern recognition and other computer vision tasks. When over-parameterized deep learning models are used for inference, resource-constrained edge devices may struggle. As a result, model compression, particularly filter pruning, has become critical. A reduction in model size might result in less calculation, resulting in faster hardware execution and lower energy consumption. One of the drawbacks of current pruning strategies is that once the filters are pruned, their weights are permanently lost. To address this constraint, we propose a unique two-phase pruning technique in which the filters to be pruned are selected using two criteria: l 2 l2 -norm and redundancy. Second, rather than omitting the selected filters for all future epochs, we restore them to their original value with some stochasticity. Retaining the most optimal filter weights in earlier epochs enables the survival of the fittest filters, resulting in higher model convergence. Experiments on three benchmark datasets, CIFAR-10, CIFAR-100, and ILSVRC-2012, reveal that our strategy outperforms other state-of-the-art pruning methods by a minimum reduction of 57% FLOPs with an accuracy loss as minimal as 0.08 %.},
  archive      = {J_PR},
  author       = {Anusha Jayasimhan and Pabitha P.},
  doi          = {10.1016/j.patcog.2024.110671},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110671},
  shortjournal = {Pattern Recognition},
  title        = {ResPrune: An energy-efficient restorative filter pruning method using stochastic optimization for accelerating CNN},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IRCNN: A novel signal decomposition approach based on
iterative residue convolutional neural network. <em>PR</em>,
<em>155</em>, 110670. (<a
href="https://doi.org/10.1016/j.patcog.2024.110670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decomposition of non-stationary signals is an important and challenging task in the field of signal time–frequency analysis. In the recent two decades, many decomposition methods have been proposed, inspired by the empirical mode decomposition method, first published by Huang et al. in 1998. However, they still have some limitations. For example, they are generally prone to boundary and mode mixing effects and are not very robust to noise. Inspired by the successful applications of deep learning, and given the lack in the literature of works in which deep learning techniques are used directly to decompose non-stationary signals into simple oscillatory components, we use the convolutional neural network, residual structure and nonlinear activation function to compute in an innovative way the local average of the signal, and study a new non-stationary signal decomposition method under the framework of deep learning. We discuss the training process of the proposed model and study the convergence analysis of the learning algorithm. In the experiments, we evaluate the performance of the proposed model from two points of view: the calculation of the local average and the signal decomposition. Furthermore, we study the mode mixing, noise interference, and orthogonality properties of the decomposed components produced by the proposed method, and compare it with the state-of-the-art ones. All results show that the proposed model allows for better handling boundary effect, mode mixing effect, robustness, and the orthogonality of the decomposed components than existing methods.},
  archive      = {J_PR},
  author       = {Feng Zhou and Antonio Cicone and Haomin Zhou},
  doi          = {10.1016/j.patcog.2024.110670},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110670},
  shortjournal = {Pattern Recognition},
  title        = {IRCNN: A novel signal decomposition approach based on iterative residue convolutional neural network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdaptBIR: Adaptive blind image restoration with latent
diffusion prior for higher fidelity. <em>PR</em>, <em>155</em>, 110659.
(<a href="https://doi.org/10.1016/j.patcog.2024.110659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims to help diffusion models get their footing in the low-level vision field, solving the pain point of insufficient fidelity. Specifically, we propose an Adaptive Blind Image Restoration framework with latent diffusion prior — AdaptBIR, which can adaptively distinguish and address various ranges of degradations. First, we quantitatively categorize images through an Image Quality Assessment (IQA) method. Then, a dual-encoder degradation removal module is employed with the guidance of IQA scores to reach better information preservation. Lastly, we utilize a two-phase controller to handle the reconstruction process in an organized manner. Extensive experiments show that applying such an adaptive framework achieves better performance on both fidelity and perceptual metrics. In this way, AdaptBIR represents more than just a novel framework, it paves the way for a broader application of the diffusion model in blind image restoration tasks.},
  archive      = {J_PR},
  author       = {Yingqi Liu and Jingwen He and Yihao Liu and Xinqi Lin and Fanghua Yu and Jinfan Hu and Yu Qiao and Chao Dong},
  doi          = {10.1016/j.patcog.2024.110659},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110659},
  shortjournal = {Pattern Recognition},
  title        = {AdaptBIR: Adaptive blind image restoration with latent diffusion prior for higher fidelity},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DH-GAN: Image manipulation localization via a dual
homology-aware generative adversarial network. <em>PR</em>,
<em>155</em>, 110658. (<a
href="https://doi.org/10.1016/j.patcog.2024.110658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image manipulation localization is a binary segmentation task that sensitive to the tampered artifacts other than awareness of the object. Thus, both traditional and learning-based methods highly rely on hand-crafted features. However, these specifically-defined features limit the ability of the network for general scenes. To tackle this problem, we propose a dual homology-aware generative adversarial network (DH-GAN), a novel GAN-based framework to localize the manipulated region. Firstly, we localize the forgery region via re-calibrating the multi-scale encoded features with a selective pyramid generator. Then, we perform the homology identification in the discriminator. The proposed homology-aware discriminators contain a stack of masked convolution (MConv) layers and learn to identify the real/fake of the segmented pixels on the predicted/target masked image in a hard-gating manner. Overall, the networks are optimized under a standard GAN. Experiments show that the proposed method outperforms other state-of-the-art algorithms on four popular image manipulation datasets.},
  archive      = {J_PR},
  author       = {Weihuang Liu and Xiaodong Cun and Chi-Man Pun},
  doi          = {10.1016/j.patcog.2024.110658},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110658},
  shortjournal = {Pattern Recognition},
  title        = {DH-GAN: Image manipulation localization via a dual homology-aware generative adversarial network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GoogLeNet-AL: A fully automated adaptive model for lung
cancer detection. <em>PR</em>, <em>155</em>, 110657. (<a
href="https://doi.org/10.1016/j.patcog.2024.110657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As lung cancer has emerged as the top contributor to cancer-related fatalities, efficient and precise diagnostic methods are essential for efficient diagnosis. This research introduces a novel CNN architecture GoogLeNet with Adaptive Layers (GoogLeNet-AL) for lung cancer detection. The GoogLeNet-AL architecture integrates innovative features such as squeeze-and-excitation blocks, dilated convolutions, depthwise separable convolutions, group convolutions, non-local blocks, octave convolutions, inverted Residuals, and ghost convolutions in the inception layers to boost the potential of GoogLeNet-AL to capture multi-scale features efficiently. The GoogLeNet-AL model has been implemented in the PyTorch 1.8.1 platform and trained using publicly accessible datasets IQ-OTH/NCCD and Chest CT-Scan for comprehensive performance evaluation. Additionally, we employ data augmentation, stratified sampling, and fairness-aware training to enhance robustness and mitigate biases. The experimental assessment demonstrate that the GoogLeNet-AL method achieves an accuracy of 98.74 %, an F1-score of 98.96 %, and a precision of 99.74 % in lung cancer detection and also demonstrates its superior performance by outperforming traditional GoogLeNet and other baseline models. Overall, the proposed architecture enhanced the detection and categorization of lung nodules by reducing false positives and negatives, thus offering a valuable tool for combating lung cancer.},
  archive      = {J_PR},
  author       = {Lei Ma and Huiqun Wu and P. Samundeeswari},
  doi          = {10.1016/j.patcog.2024.110657},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110657},
  shortjournal = {Pattern Recognition},
  title        = {GoogLeNet-AL: A fully automated adaptive model for lung cancer detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual feature disentanglement for face anti-spoofing.
<em>PR</em>, <em>155</em>, 110656. (<a
href="https://doi.org/10.1016/j.patcog.2024.110656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization for Face Anti-Spoofing (FAS) is increasingly crucial with face attacks across unseen domains. Some existing face anti-spoofing methods aim to disentangle spoof feature for both seen and unseen scenarios. However, it is still a challenging problem to capture spoof feature from facial image, because of spoof pattern are often mixed with various facial attributes such as identity, expression, age and gender. To solve the above problem, we propose a Dual Feature Disentanglement Network (DFDN), which leverages feature projection scheme based on domain-invariant feature in conjunction with rearranging the facial structure to learn spoof feature jointly. Specifically, DFDN introduces the local mask module and domain discriminator to enhance the domain-invariant feature. Based on this, we employ the geometry projection relations to achieve the refined spoof pattern via the feature projection. Meanwhile, we disrupt facial structure to weaken face attributes feature learning and guide CNNs to learn spoof feature. Subsequently, consistent regularization is developed to reduce the gap between the above two kinds of spoof pattern by introducing optimal transport and cosine similarity. Extensive experiments and visualizations demonstrate the advantages of our DFDN over the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yimei Ma and Jianjun Qian and Jun Li and Jian Yang},
  doi          = {10.1016/j.patcog.2024.110656},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110656},
  shortjournal = {Pattern Recognition},
  title        = {Dual feature disentanglement for face anti-spoofing},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward comprehensive and effective palmprint reconstruction
attack. <em>PR</em>, <em>155</em>, 110655. (<a
href="https://doi.org/10.1016/j.patcog.2024.110655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge posed by the template-based reconstruction attack significantly impacts the security and privacy of biometric systems. Current reconstruction techniques rely on extensive training data or encounter limitations in adaptability, resulting in subpar reconstruction performance. In this paper, we propose a black-box palmprint template reconstruction method based on the modified Progressive GAN (ProGAN), which achieves a substantial success rate in attacking deep-learning-based and hand-crafted-based templates. Our approach incorporates the dropout mechanism into the generator of ProGAN and introduces a Double Reuse Training Strategy to enable effective training of the reconstruction network despite limited data. Furthermore, we devise a novel Scale-Adaptive Multi-Texture Complementarity loss, enhancing the texture quality of reconstructed images. We conduct extensive experiments on diverse palmprint recognition techniques. The resulting reconstructed images exhibit exceptional image quality. Additionally, we thoroughly examine the security and privacy aspects of the palmprint recognition algorithm based on the insights gained from the reconstruction attacks.},
  archive      = {J_PR},
  author       = {Licheng Yan and Fei Wang and Lu Leng and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.patcog.2024.110655},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110655},
  shortjournal = {Pattern Recognition},
  title        = {Toward comprehensive and effective palmprint reconstruction attack},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AnatPose: Bidirectionally learning anatomy-aware heatmaps
for human pose estimation. <em>PR</em>, <em>155</em>, 110654. (<a
href="https://doi.org/10.1016/j.patcog.2024.110654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human pose from images is the key to enabling machines to understand human actions. Existing works on human pose estimation mainly focus on designing more resultful deep neural networks to regress the locations of human joints. Although the human pose is obedient to anatomy and shows rich anatomical features, reasoning human body structure by machine in a complex environment is still an open problem. This paper proposes AnatPose which can effectively capture the structural dependency among human body parts by both deep neural network architecture and learning objectives: (1) For the deep neural network architecture, a bidirectional learning paradigm is proposed to learn body-part proportions and dependencies by organizing human body parts as sequential data. This innovation enables the messages to pass in a bidirectional way and makes the human body exchange information about each part deeper during training. (2) For the learning objective, the proposed AnatPose learns a probabilistic representation of multi-scale anatomical features, including keypoint heatmaps, bone heatmaps, and symmetry heatmaps. This innovation enables the multi-scale anatomical features to successfully capture the structural dependency at both low-level joints and high-level associations from the anatomical priors of the human body. Extensive experimental results demonstrate that the proposed AnatPose shows state-of-the-art performance on three challenging datasets. It achieves a PCK@0.2 detection rate of 95.2% on the LSP dataset, a PCKh@0.5 detection rate of 92.9% on the MPII dataset, and an mAP of 76.6% on the Microsoft COCO dataset. Benefiting from its state-of-the-art accuracy, the proposed approach is expected to be widely used in various human pose estimation-driven applications.},
  archive      = {J_PR},
  author       = {Songlin Du and Zhiwen Zhang and Takeshi Ikenaga},
  doi          = {10.1016/j.patcog.2024.110654},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110654},
  shortjournal = {Pattern Recognition},
  title        = {AnatPose: Bidirectionally learning anatomy-aware heatmaps for human pose estimation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype learning for adversarial domain adaptation.
<em>PR</em>, <em>155</em>, 110653. (<a
href="https://doi.org/10.1016/j.patcog.2024.110653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial learning has been widely used in recent years to address the issue of domain shift in domain adaptation. However, this approach focuses on global cross-domain alignment and overlooks the alignment of class boundaries. To tackle this limitation, we introduce a novel method called PLADA. PLADA leverages prototype learning to align category distributions across domains. The prototypes in PLADA represent the source category distribution, which is constructed using labelled data and transferred to the target domain. In addition to adversarial learning for global domain-invariant feature learning, we propose the weighted prototype loss (WPL) to embed prototype information. WPL transforms the local category distribution alignment problem into a distance measurement between the prediction and prototypes, resulting in a more discriminative representation. Experimental results demonstrate that our proposed model performs comparably well on multiple classic domain adaptation tasks, showcasing the potential of PLADA.},
  archive      = {J_PR},
  author       = {Yuchun Fang and Chen Chen and Wei Zhang and Jiahua Wu and Zhaoxiang Zhang and Shaorong Xie},
  doi          = {10.1016/j.patcog.2024.110653},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110653},
  shortjournal = {Pattern Recognition},
  title        = {Prototype learning for adversarial domain adaptation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistency approximation: Incremental feature selection
based on fuzzy rough set theory. <em>PR</em>, <em>155</em>, 110652. (<a
href="https://doi.org/10.1016/j.patcog.2024.110652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Rough Set Theory (FRST)-based feature selection has been widely used as a preprocessing step to handle dynamic and large datasets. However, large-scale or high-dimensional datasets remain intractable for FRST-based feature selection approaches due to high space complexity and unsatisfactory classification performance. To overcome these challenges, we propose a Consistency Approximation (CA)-based framework for incremental feature selection. By exploring CA, we introduce a novel significance measure and a tri-accelerator. The CA-based significance measure provides a mechanism for each sample in the universe to keep members with different class labels within its fuzzy neighbourhood as far as possible, while keeping members with the same label as close as possible. Furthermore, our tri-accelerator reduces the search space and decreases the computational space with a theoretical lower bound. The experimental results demonstrate the superiority of our proposed algorithm compared to state-of-the-art methods on efficiency and classification accuracy, especially for large-scale and high-dimensional datasets.},
  archive      = {J_PR},
  author       = {Jie Zhao and Daiyang Wu and JiaXin Wu and Wenhao Ye and Faliang Huang and Jiahai Wang and Eric W.K. See-To},
  doi          = {10.1016/j.patcog.2024.110652},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110652},
  shortjournal = {Pattern Recognition},
  title        = {Consistency approximation: Incremental feature selection based on fuzzy rough set theory},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MOVES: Movable and moving LiDAR scene segmentation in
label-free settings using static reconstruction. <em>PR</em>,
<em>155</em>, 110651. (<a
href="https://doi.org/10.1016/j.patcog.2024.110651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate static structure reconstruction and segmentation of non-stationary objects is of vital importance for autonomous navigation applications. These applications assume a LiDAR scan to consist of only static structures. In the real world however, LiDAR scans consist of non-stationary dynamic structures — moving and movable objects. Current solutions use segmentation information to isolate and remove moving structures from LiDAR scan. This strategy fails in several important use-cases where segmentation information is not available. In such scenarios, moving objects and objects with high uncertainty in their motion i.e. movable objects , may escape detection. This violates the above assumption. We present MOVES , a novel GAN based adversarial model that segments out moving as well as movable objects in the absence of segmentation information. We achieve this by accurately transforming a dynamic LiDAR scan to its corresponding static scan. This is obtained by replacing dynamic objects and corresponding occlusions with static structures which were occluded by dynamic objects. We leverage corresponding static-dynamic LiDAR pairs. We design a novel discriminator, coupled with a contrastive loss on a smartly selected LiDAR scan triplet. For datasets lacking paired information, we propose MOVES-MMD that integrates Unsupervised Domain Adaptation into the network. We perform rigorous experiments to demonstrate state of the art dynamic to static translation performance on a sparse real world industrial dataset, an urban and a simulated dataset. MOVES also segments out movable and moving objects without using segmentation information. Without utilizing segmentation labels, MOVES performs better than segmentation based navigation baseline in highly dynamic and long LiDAR sequences. The code is available here .},
  archive      = {J_PR},
  author       = {Prashant Kumar and Dhruv Makwana and Onkar Susladkar and Anurag Mittal and Prem Kumar Kalra},
  doi          = {10.1016/j.patcog.2024.110651},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110651},
  shortjournal = {Pattern Recognition},
  title        = {MOVES: Movable and moving LiDAR scene segmentation in label-free settings using static reconstruction},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Degradation-removed multiscale fusion for low-light salient
object detection. <em>PR</em>, <em>155</em>, 110650. (<a
href="https://doi.org/10.1016/j.patcog.2024.110650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light is practical in real-life applications and leads to decreased performance of visual perception. For example, the challenges significantly burden the applications of salient object detection (SOD). Existing methods primarily focus on SOD under normal-light conditions. Although some forward-looking work has attempted to address the problem, the model design fails to directly target the physical degradation factor of darkness. Introducing extra sensors (e.g., depth or infrared) may supplement more necessary saliency information, however, acquisition costs and computational overhead will be involved. To improve the SOD under low-light condition, we devise a new image enhancement method and integrate it into the SOD network to form a new learning framework. Specifically, we employ Retinex-guided self-enhancement in combination with multiscale cross-channel detection, effectively mitigating the influence of factors such as image dark degradation and low contrast. This approach enhances the detection performance without incurring additional costs. Additionally, to promote efforts towards this task, we construct a comprehensive low-light SOD dataset benchmark, named YLLSOD. Finally, we conduct extensive comparative experiments between our proposed method and the state-of-the-art single-modal methods, validating the competitiveness of our approach. Comparative experiments with some representative bi-modal methods further illustrate the advantages of our proposed method. Our new dataset will be available at https://github.com/ynn1030/YLLSOD .},
  archive      = {J_PR},
  author       = {Nana Yu and Jie Wang and Hong Shi and Zihao Zhang and Yahong Han},
  doi          = {10.1016/j.patcog.2024.110650},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110650},
  shortjournal = {Pattern Recognition},
  title        = {Degradation-removed multiscale fusion for low-light salient object detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Underwater object detection in noisy imbalanced datasets.
<em>PR</em>, <em>155</em>, 110649. (<a
href="https://doi.org/10.1016/j.patcog.2024.110649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance occurs in the datasets with a disproportionate ratio of observations. The class imbalance problem drives the detection and classification systems to be more biased towards the over-represented classes while the under-represented classes may not receive sufficient learning. Previous works often deploy distribution based re-balancing approaches to address this problem. However, these established techniques do not work properly for underwater object detection where label noise commonly exists. In our experiments, we observe that the imbalanced detection problem may be caused by imbalance data distributions or label noise. To deal with these challenges, we first propose a noise removal (NR) algorithm to remove label noise in the datasets, and then propose a factor-agnostic gradient re-weighting algorithm (FAGR) to address the imbalanced detection problem. FAGR provides a rebalanced gradient to each class, which encourages the detection network to treat all the classes equally whilst minimising the detection discrepancy. Our proposed NR+FAGR framework achieves state-of-the-art (SOAT) performance on three underwater object datasets due to its high capacity in handling the class imbalance and noise issues. The source code will be made available at: https://github.com/IanDragon .},
  archive      = {J_PR},
  author       = {Long Chen and Tengyue Li and Andy Zhou and Shengke Wang and Junyu Dong and Huiyu Zhou},
  doi          = {10.1016/j.patcog.2024.110649},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110649},
  shortjournal = {Pattern Recognition},
  title        = {Underwater object detection in noisy imbalanced datasets},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompt-guided DETR with RoI-pruned masked attention for
open-vocabulary object detection. <em>PR</em>, <em>155</em>, 110648. (<a
href="https://doi.org/10.1016/j.patcog.2024.110648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt-OVD is an efficient and effective DETR-based framework for open-vocabulary object detection that utilizes class embeddings from CLIP as prompts, guiding the Transformer decoder to detect objects in base and novel classes. Additionally, our RoI-pruned masked attention helps leverage the zero-shot classification ability of the Vision Transformer-based CLIP, resulting in improved detection performance at a minimal computational cost. Our experiments on the OV-COCO and OV-LVIS datasets demonstrate that Prompt-OVD achieves an impressive 21.2 times faster inference speed than the first end-to-end open-vocabulary detection method (OV-DETR), while also achieving higher APs than four two-stage methods operating within similar inference time ranges. We release the code at https://github.com/DISL-Lab/Prompt-OVD .},
  archive      = {J_PR},
  author       = {Hwanjun Song and Jihwan Bang},
  doi          = {10.1016/j.patcog.2024.110648},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110648},
  shortjournal = {Pattern Recognition},
  title        = {Prompt-guided DETR with RoI-pruned masked attention for open-vocabulary object detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-granularity relationship reasoning network for
high-fidelity 3D shape reconstruction. <em>PR</em>, <em>155</em>,
110647. (<a href="https://doi.org/10.1016/j.patcog.2024.110647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular image-based 3D reconstruction is widely used in virtual reality, augmented reality, and autonomous driving, which benefits from the rapid development of deep learning approaches. Most of the available methods focused on reconstructing the overall shape of the object while ignoring some fine-grained details. Moreover, these methods make it hard to exactly reconstruct complex topological structures. In this paper, we propose a multi-granularity relationship reasoning network (MGRRNet), which aims to recover 3D shapes with high fidelity and rich details via the relationship reasoning between different granularity information. Specifically, our model captures the discriminative and detailed features at different granularities for extracting attentional regions. Then we perform the relationship reasoning between different granularities to reinforce the multi-granularity consistency and inter-granularity correlation. By doing this, our network is able to achieve robust feature representation and fine reconstruction. During the learning process, we jointly optimize procedures of different granularity feature representations via a sequence of inter-granularity cycle loss iterations. Extensive experimental results on two publicly available datasets justify that our approach achieves competitive performance compared to the state-of-the-art methods. Codes and all resources will be publicly available at https://github.com/Ray-tju/MGRRNet .},
  archive      = {J_PR},
  author       = {Lei Li and Zhiyuan Zhou and Suping Wu and Pan Li and Boyang Zhang},
  doi          = {10.1016/j.patcog.2024.110647},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110647},
  shortjournal = {Pattern Recognition},
  title        = {Multi-granularity relationship reasoning network for high-fidelity 3D shape reconstruction},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A multi-modal extraction integrated model for
neuropsychiatric disorders classification. <em>PR</em>, <em>155</em>,
110646. (<a href="https://doi.org/10.1016/j.patcog.2024.110646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) provide high-precision automatic classification of neuropsychiatric disorders based on images. However, the “black box” nature leads to poor interpretability of CNN. This study constructs an integrated model for neuropsychiatric disorders classification from multi-modal data. The proposed model consists of a novel multi-scale image features extraction neural network (MSFM) and a XGBoost. The proposed MSFM extracts the pixel context semantic information from fMRI images with different scales, which employs token and channel-mixing strategy to enhance the information communication between context semantic information. XGBoost is used to extract phenotypic feature from phenotypic records. Based on the integration of phenotypic and image features, a comparative interpretable classification of mental disorders can be achieved. The overall accuracy, sensitivity, and recall of the binary classification (healthy controls &amp; neuropsychiatric disorders) of the integrated model are 90.23%, 91.08%, and 89.33%, respectively. The visualization of image features and the phenotypic features present consistency in the brain regions, increasing the interpretability of the MSFM. Especially, through visual statistical analysis of the test set, it was found that there are differences in the distribution of ADHD, BD, and SD in the brain regions. Our solution may provide psychiatrists with ideas for comparative examinations and diagnosis.},
  archive      = {J_PR},
  author       = {Liangliang Liu and Zhihong Liu and Jing Chang and Xue Xu},
  doi          = {10.1016/j.patcog.2024.110646},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110646},
  shortjournal = {Pattern Recognition},
  title        = {A multi-modal extraction integrated model for neuropsychiatric disorders classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive cross-modal clustering with twin network.
<em>PR</em>, <em>155</em>, 110645. (<a
href="https://doi.org/10.1016/j.patcog.2024.110645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal clustering (CMC) methods explore the correlation information between multiple modalities to improve clustering performance. However, the obvious differences between heterogeneous modalities make it difficult to obtain the correlation information directly. In this paper, we propose a novel Contrastive Cross-modal Clustering with Twin Network (3CTnet) for CMC, which contrasts the differences of multiple modalities to fully mine the correlation information. The 3CTnet contains two modal-special encoders and an attention-based correlation propagate module (CPM). First, the modal-special encoders are trained by pseudo-labels to learn the clustering structure and feature of single modality. Then we contrast the clustering structures and features of different modalities to explore the inter-cluster and inter-feature correlation information simultaneously. Finally, the CPM is designed to propagate the learned correlation information among modal-special encoders to further optimize the learning of features and clustering structures. The experiments show that 3CTnet outperforms the state-of-the-art CMC methods on six large datasets.},
  archive      = {J_PR},
  author       = {Yiqiao Mao and Xiaoqiang Yan and Shizhe Hu and Yangdong Ye},
  doi          = {10.1016/j.patcog.2024.110645},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110645},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive cross-modal clustering with twin network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A safe screening rule with bi-level optimization of ν
support vector machine. <em>PR</em>, <em>155</em>, 110644. (<a
href="https://doi.org/10.1016/j.patcog.2024.110644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the ν ν support vector machine ( ν ν -SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for ν ν -SVM (SRBO- ν ν -SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO- ν ν -SVM is strictly deduced by integrating the Karush–Kuhn–Tucker (KKT) conditions, the variational inequalities of convex problems and the ν ν -property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to accelerate many SVM-type models, and it is successfully applied to one-class SVM. Experimental results on 6 artificial data sets and 30 benchmark data sets have verified the effectiveness and safety of our proposed methods in supervised and unsupervised tasks.},
  archive      = {J_PR},
  author       = {Zhiji Yang and Wanyi Chen and Huan Zhang and Yitian Xu and Lei Shi and Jianhua Zhao},
  doi          = {10.1016/j.patcog.2024.110644},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110644},
  shortjournal = {Pattern Recognition},
  title        = {A safe screening rule with bi-level optimization of ν support vector machine},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering attention-guided cross-modality correlation for
visible–infrared person re-identification. <em>PR</em>, <em>155</em>,
110643. (<a href="https://doi.org/10.1016/j.patcog.2024.110643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared person re-identification (VI Re-ID) is an essential and challenging task. Existing studies mainly focus on learning the unified modality-invariant representations directly from visible and infrared images. However, it is hard to obtain the identity-aware patterns due to the co-existence of inter- and intra-modality discrepancies. In this paper, we propose a novel attention-guided cross-modality correlation method (AGCC) to achieve the modality-invariant and identity-discriminative representations for visible–infrared person Re-ID. Specifically, we introduce a modality-aware attention (MAA) mechanism to model the inter- and intra-modality variations, which generates attention masks of two modalities for preserving the most significant region and obtaining the discriminative patterns in each identity. Further, we present an attention-guided channel and spatial correlation scheme (AGCSC) to establish the attention-guided cross-modality correlation, which can bridge the gap between inter- and intra-modalities. Moreover, a novel joint-modality learning head (JMLH) is developed to promote the metric and mutual learning from both feature distribution and classification logit levels. Extensive experiments on two public SYSU-MM01 and RegDB datasets demonstrate the remarkable superiority of our method over the state of the arts. The implementation codes will be made available soon.},
  archive      = {J_PR},
  author       = {Hao Yu and Xu Cheng and Kevin Ho Man Cheng and Wei Peng and Zitong Yu and Guoying Zhao},
  doi          = {10.1016/j.patcog.2024.110643},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110643},
  shortjournal = {Pattern Recognition},
  title        = {Discovering attention-guided cross-modality correlation for visible–infrared person re-identification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compressing spectral kernels in gaussian process: Enhanced
generalization and interpretability. <em>PR</em>, <em>155</em>, 110642.
(<a href="https://doi.org/10.1016/j.patcog.2024.110642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The modeling capabilities of a Gaussian Process (GP), such as generalization, nonlinearity, and smoothness, are largely determined by the choice of its kernel. A popular family of kernels for GPs, the spectral mixture (SM) kernels, have the desirable property that with a large number of spectral components they can approximate any stationary kernel. However, using a large number of SM components increases the risk of overfitting and hinders interpretability. To overcome these challenges, we propose a compression algorithm incorporating component pruning and component merging for GPs. Here SM components with small signal variance are removed, and a moment-matching merge method is proposed to further reduce the number of SM components. The main novelty of the proposed method is a similarity measure between SM components based on their normalized cross-correlation, which is related to the Bhattacharyya coefficient. We derive a greedy GP compression algorithm and perform a comparative evaluation over various learning tasks in terms of forecasting performance and compression capability. Results substantiate the beneficial effect of the method, both in terms of generalization and interpretability. 1},
  archive      = {J_PR},
  author       = {Kai Chen and Twan van Laarhoven and Elena Marchiori},
  doi          = {10.1016/j.patcog.2024.110642},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110642},
  shortjournal = {Pattern Recognition},
  title        = {Compressing spectral kernels in gaussian process: Enhanced generalization and interpretability},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot intent detection with self-supervised pretraining
and prototype-aware attention. <em>PR</em>, <em>155</em>, 110641. (<a
href="https://doi.org/10.1016/j.patcog.2024.110641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot intent detection is a more challenging application. However, traditional prototypical networks based on averaging often suffer from issues such as missing key information, poor generalization capabilities. In previous work, using three-dimensional convolutional neural networks (3DCNN) to generate prototype representations faces challenges with long-distance dependencies. Furthermore, a pretrained encoder’s performance in a specific domain is often suboptimal because its knowledge of the specific domain is fragmented. Therefore, in this paper, we propose a simple yet effective two-stage learning strategy to address these issues. In the first stage, we propose a self-supervised multi-task pretraining (SMTP) strategy. SMTP utilizes unlabeled data from the current domain to help the pretrained encoder learn the semantic information of the text and implicitly distinguish semantically similar text representations without using any labels. SMTP aims to enhance the representation capability of the pretrained encoder in a specific domain. In the second stage, we propose a prototype-aware attention (PaAT) model to generate prototype representations of the same class. PaAT generates prototype representations by calculating the attention between class texts, which can effectively solve the long-distance dependence problem of 3DCNN. PaAT is a siamese architecture that can simultaneously generate prototype representations and sentence-level representations of unseen data. In addition, to prevent overfitting in few-shot learning, we introduce an unsupervised contrastive regularization term to constrain PaAT. Our method achieves state-of-the-art performance on four public datasets. 1},
  archive      = {J_PR},
  author       = {Shun Yang and YaJun Du and Xin Zheng and XianYong Li and XiaoLiang Chen and YanLi Li and ChunZhi Xie},
  doi          = {10.1016/j.patcog.2024.110641},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110641},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot intent detection with self-supervised pretraining and prototype-aware attention},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot image classification via hybrid representation.
<em>PR</em>, <em>155</em>, 110640. (<a
href="https://doi.org/10.1016/j.patcog.2024.110640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot image classification aims to learn an embedding model on the base datasets and design a base learner to recognize novel categories. The few-shot image classification framework is a two-phase process. First, the pre-train phase utilizes the base data to train a CNN-based feature extractor. Next, in the meta-test phase, the frozen feature extractor is applied to novel data with categories different from the base data. A base learner is then designed for recognition. Several simple base learners, including nearest neighbor, support vector machine, and logistic regression classifiers, have been recently introduced for few-shot learning tasks. However, these base learners are separately designed to consider specific representations (e.g., the class center) or shared representations (e.g., the boundaries). This paper mainly focuses on exploring the representation-residual base learners, which aim to represent a query sample with the support set and predict the query sample’s label based on the minimal residual error. We first introduce two representation-residual base learners: a specific representation base learner and a shared representation base learner. Then, we propose a novel hybrid representation base learner that combines both base learners to generate competitive representation. Additionally, we extend our approach by incorporating a self-training framework to utilize the query data fully. We evaluate our proposed method on several benchmark few-shot image classification datasets, such as miniImageNet, tieredImageNet, CIFAR-FS, FC100, and CUB datasets. The experimental results indicate that our proposed approach shows a significant performance improvement.},
  archive      = {J_PR},
  author       = {Bao-Di Liu and Shuai Shao and Chunyan Zhao and Lei Xing and Weifeng Liu and Weijia Cao and Yicong Zhou},
  doi          = {10.1016/j.patcog.2024.110640},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110640},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot image classification via hybrid representation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel k-means and k-medoids algorithms for clustering
non-spherical-shape clusters non-sensitive to outliers. <em>PR</em>,
<em>155</em>, 110639. (<a
href="https://doi.org/10.1016/j.patcog.2024.110639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determination of the optimal number of clusters, the random selection of the initial centers, the non-detection of non-spherical clusters, and the negative impact of outliers are the main challenges of the K-means algorithm. In this paper, to tackle these issues three simple and intelligent algorithms are proposed by changing the structure of the K-means and K-medoids algorithms. The difference between these algorithms is in the selection of the initial centers and the stop condition. A method has been proposed to obtain the overlap space between the clusters. Using this method, a modified K-means algorithm is developed for the clustering of non-spherical data. These algorithms are designed in a way that they are not sensitive to outliers and can identify clusters having non-spherical shapes. The performance of the proposed methods is illustrated by applying the proposed algorithms to the different data sets and by comparing the results of the algorithms with other methods.},
  archive      = {J_PR},
  author       = {J. Heidari and N. Daneshpour and A. Zangeneh},
  doi          = {10.1016/j.patcog.2024.110639},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110639},
  shortjournal = {Pattern Recognition},
  title        = {A novel K-means and K-medoids algorithms for clustering non-spherical-shape clusters non-sensitive to outliers},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Reconstruction flow recurrent network for compressed video
quality enhancement. <em>PR</em>, <em>155</em>, 110638. (<a
href="https://doi.org/10.1016/j.patcog.2024.110638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a reconstruction flow for the task of compressed video quality enhancement (VQE). Compressed videos often suffer from various coding artifacts, such as blocking and blurring, especially under low bit-rate. VQE aims to suppress these artifacts to improve the visual quality. Frame similarity can be utilized to enhance low-quality frames given their neighboring high-quality frames, for which motion estimation becomes important. Previous approaches often calculate optical flow for the motion compensation. On the other hand, video coding contains a rich set of block motion vectors, forming a coding flow, which may or may not correspond to the scene motion, but to places that deliver the minimum compression error. In contrast, such a valuable coding flow has always been ignored in VQE previously. In this work, we combine these two motion sources into a new flow, namely reconstruction flow, for the purpose of high-quality VQE. Specifically, we estimate optical flows from RGB frames and extract coding flows from coding streams, which are then merged by a fusion module to generate reconstruction flow. Besides, our network is built upon a recurrent network to utilize global temporal information. The deep features are warped according to the reconstruction flow and fed into the subsequent reconstruction module with spatial-variant kernel attention. Our method is evaluated on the leading MFQE2.0 dataset, which demonstrates superior performances when compared to the existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhengning Wang and Xuhang Liu and Chuan Wang and Ting Jiang and Tianjiao Zeng and Zhenni Zeng and Guoqing Wang and Shuaicheng Liu},
  doi          = {10.1016/j.patcog.2024.110638},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110638},
  shortjournal = {Pattern Recognition},
  title        = {Reconstruction flow recurrent network for compressed video quality enhancement},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing supervised learning with the wave loss function: A
robust and smooth approach. <em>PR</em>, <em>155</em>, 110637. (<a
href="https://doi.org/10.1016/j.patcog.2024.110637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss function plays a vital role in supervised learning frameworks. The selection of the appropriate loss function holds the potential to have a substantial impact on the proficiency attained by the acquired model. The training of supervised learning algorithms inherently adheres to predetermined loss functions during the optimization process. In this paper, we present a novel contribution to the realm of supervised machine learning: an asymmetric loss function named wave loss. It exhibits robustness against outliers, insensitivity to noise, boundedness, and a crucial smoothness property. Theoretically, we establish that the proposed wave loss function manifests the essential characteristic of being classification-calibrated. Leveraging this breakthrough, we incorporate the proposed wave loss function into the least squares setting of support vector machines (SVM) and twin support vector machines (TSVM), resulting in two robust and smooth models termed as Wave-SVM and Wave-TSVM, respectively. To address the optimization problem inherent in Wave-SVM, we utilize the adaptive moment estimation (Adam) algorithm, which confers multiple benefits, including the incorporation of adaptive learning rates, efficient memory utilization, and faster convergence during training. It is noteworthy that this paper marks the first instance of Adam’s application to solve an SVM model. Further, we devise an iterative algorithm to solve the optimization problems of Wave-TSVM. To empirically showcase the effectiveness of the proposed Wave-SVM and Wave-TSVM, we evaluate them on benchmark UCI and KEEL datasets (with and without feature noise) from diverse domains. Moreover, to exemplify the applicability of Wave-SVM in the biomedical domain, we evaluate it on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset. The experimental outcomes unequivocally reveal the prowess of Wave-SVM and Wave-TSVM in achieving superior prediction accuracy against the baseline models. The source codes of the proposed models are publicly available at https://github.com/mtanveer1/Wave-SVM .},
  archive      = {J_PR},
  author       = {Mushir Akhtar and M. Tanveer and Mohd. Arshad and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.patcog.2024.110637},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110637},
  shortjournal = {Pattern Recognition},
  title        = {Advancing supervised learning with the wave loss function: A robust and smooth approach},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Many birds, one stone: Medical image segmentation with
multiple partially labeled datasets. <em>PR</em>, <em>155</em>, 110636.
(<a href="https://doi.org/10.1016/j.patcog.2024.110636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is fundamental in the field of medical image analysis and has wide clinical applications in disease diagnosis and surgical planning etc. Current prevalent solution is to train a deep network in a fully supervised way with a large-scale fully labeled dataset. However, due to the high labor cost and requirement on medical expertise, such dataset is always absent. Instead, there are multiple partially labeled datasets which are originally established for specific purposes. To make full use of these partially labeled datasets, we propose a novel partially supervised segmentation network, named PSSNet, which consists of a task-specific feature learning network followed by a cross-task attention module (xTA) to exploit task dependencies to enhance task-specific features. To solve the challenges raised by unlabeled classes and domain shift across datasets, we propose an adversarial self-training strategy. We conduct experiments on two medical image segmentation tasks. One is the fine-grained fundus image segmentation aiming to simultaneously segment four-class lesions, OD and OC, and vessels. Validation on seven datasets demonstrates that our PSSNet performs the best among three baselines and three state-of-the-arts. The other is the multiple abdominal organ segmentation in CT images. Our PSSNet is trained on three partially labeled datasets, i.e., LiTS, KiTS and Spleen. Validation on one fully labeled dataset, i.e., BTCV, demonstrates that our PSSNet achieves better performances than four state-of-the-arts. The code is publicly available at https://github.com/CVIU-CSU/PSSNet .},
  archive      = {J_PR},
  author       = {Qing Liu and Hailong Zeng and Zhaodong Sun and Xiaobai Li and Guoying Zhao and Yixiong Liang},
  doi          = {10.1016/j.patcog.2024.110636},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110636},
  shortjournal = {Pattern Recognition},
  title        = {Many birds, one stone: Medical image segmentation with multiple partially labeled datasets},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and robust clustering based on backbone
identification. <em>PR</em>, <em>155</em>, 110635. (<a
href="https://doi.org/10.1016/j.patcog.2024.110635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is the process of grouping similar data objects into different subsets based on their similarities. Inspired by the concept of the popularity of individuals in a community, we rate the popularity of each sample which reflects the centrality of that sample in the dataset. With the aim of identifying clusters with arbitrary shapes and varying densities, we propose a clustering approach that divides samples into separate population groups. This approach is based on identifying the backbone of data, characterized by a set of popular points surrounded by less popular points. To distinguish poorly separated clusters, a proximity measure is defined based on the popularity of samples. We also use the popularity of samples to assign halo points to clusters and calculate cohesion between clusters. The proposed clustering method can detect arbitrary-shaped clusters with varying densities without requiring to specify the number of clusters. Outliers are also identified according to popularity. We demonstrate the effectiveness of the approach on synthetic and real-world datasets.},
  archive      = {J_PR},
  author       = {Hassan Motallebi},
  doi          = {10.1016/j.patcog.2024.110635},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110635},
  shortjournal = {Pattern Recognition},
  title        = {Efficient and robust clustering based on backbone identification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linear centroid encoder for supervised principal component
analysis. <em>PR</em>, <em>155</em>, 110634. (<a
href="https://doi.org/10.1016/j.patcog.2024.110634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new supervised dimensionality reduction technique called Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the nonlinear Centroid-Encoder (CE) (Ghosh and Kirby, 2022). SLCE works by mapping the samples of a class to its class centroid using a linear transformation. The transformation is a projection that reconstructs a point such that its distance from the corresponding class centroid, i.e., centroid-reconstruction loss, is minimized in the ambient space. We derive a closed-form solution using an eigendecomposition of a symmetric matrix. We did a detailed analysis and presented some crucial mathematical properties of the proposed approach. We establish a connection between the eigenvalues and the centroid-reconstruction loss. In contrast to Principal Component Analysis (PCA) which reconstructs a sample in the ambient space, the transformation of SLCE uses the instances of a class to rebuild the corresponding class centroid. Therefore the proposed method can be considered a form of supervised PCA. Experimental results show the performance advantage of SLCE over other supervised methods.},
  archive      = {J_PR},
  author       = {Tomojit Ghosh and Michael Kirby},
  doi          = {10.1016/j.patcog.2024.110634},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110634},
  shortjournal = {Pattern Recognition},
  title        = {Linear centroid encoder for supervised principal component analysis},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid-context-based multi-prior entropy modeling for
learned lossless image compression. <em>PR</em>, <em>155</em>, 110632.
(<a href="https://doi.org/10.1016/j.patcog.2024.110632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossless image compression is an essential aspect of image processing, particularly in many fields that require high information fidelity. In recent years, learned lossless image compression methods have shown promising results. However, many of these methods do not make optimal use of available information, leading to sub-optimal performance. This paper proposes a multi-prior entropy model for lossless image compression, which effectively leverages available information to achieve better compression performance. The proposed multi-prior comprises a cross-channel prior, hybrid local context, and hyperprior, allowing it to effectively utilize all available information. To remove redundancy across color channels, the original image is first losslessly transformed into YUV color space. The network then learns priors from the original image, the prior-coding channels, and the local context, which are fused to form the multi-prior used for GMM parameters estimation. Moreover, to capture the features of different images, a hybrid local context is abstracted using different kernel sizes of mask convolutions in a local context. The experimental results on several datasets demonstrate that our algorithm outperforms several existing learning-based image compression methods and traditional methods, such as JPEG2000, WebP, and FLIF.},
  archive      = {J_PR},
  author       = {Chuan Fu and Bo Du and Liangpei Zhang},
  doi          = {10.1016/j.patcog.2024.110632},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110632},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid-context-based multi-prior entropy modeling for learned lossless image compression},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MetaTKG++: Learning evolving factor enhanced meta-knowledge
for temporal knowledge graph reasoning. <em>PR</em>, <em>155</em>,
110629. (<a href="https://doi.org/10.1016/j.patcog.2024.110629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning over Temporal Knowledge Graphs (TKGs) aims to predict future facts based on the given history. One of the key challenges for prediction is to analyze the evolution process of facts. Most existing works focus on exploring evolutionary information in history to obtain effective temporal embeddings for entities and relations, but they ignore the variation in evolution patterns of facts caused by numerous diverse entities and latent evolving factors, which makes them struggle to adapt to future data with different evolution patterns. Moreover, new entities continue to emerge along with the evolution of facts over time. Since existing models highly rely on historical information to learn embeddings for entities, they perform poorly on such entities with little historical information. To tackle these issues, we propose a novel evolving factor enhanced temporal meta-learner framework for TKG reasoning, MetaTKG++ for brevity. Specifically, we first propose a temporal meta-learner which regards TKG reasoning as many temporal meta-tasks for training. From the training process of each meta-task, the obtained meta-knowledge can guide backbones to adapt to future data exhibiting various evolution patterns and to effectively learn entities with little historical information. Then, we design an Evolving Factor Learning module, which aims to assist backbones in learning evolution patterns by modeling latent evolving factors. Meanwhile, during the training process with the proposed meta-learner, the learnable evolving factor can enhance the meta-knowledge with providing more comprehensive information on learning evolution patterns. Extensive experiments on five widely-used datasets and four backbones demonstrate that our method can greatly improve the performance on TKG prediction.},
  archive      = {J_PR},
  author       = {Yuwei Xia and Mengqi Zhang and Qiang Liu and Liang Wang and Shu Wu and Xiaoyu Zhang and Liang Wang},
  doi          = {10.1016/j.patcog.2024.110629},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110629},
  shortjournal = {Pattern Recognition},
  title        = {MetaTKG++: Learning evolving factor enhanced meta-knowledge for temporal knowledge graph reasoning},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph domain adaptation with localized graph signal
representations. <em>PR</em>, <em>155</em>, 110628. (<a
href="https://doi.org/10.1016/j.patcog.2024.110628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a domain adaptation algorithm designed for graph domains. Given a source graph with many labeled nodes and a target graph with few or no labeled nodes, we aim to estimate the target labels by making use of the similarity between the characteristics of the variation of the label functions on the two graphs. Our assumption about the source and the target domains is that the local behavior of the label function, such as its spread and speed of variation on the graph, bears resemblance between the two graphs. We estimate the unknown target labels by solving an optimization problem where the label information is transferred from the source graph to the target graph based on the prior that the projections of the label functions onto localized graph bases be similar between the source and the target graphs. In order to efficiently capture the local variation of the label functions on the graphs, spectral graph wavelets are used as the graph bases. Experimentation on various data sets shows that the proposed method yields quite satisfactory classification accuracy compared to reference domain adaptation methods.},
  archive      = {J_PR},
  author       = {Yusuf Yiğit Pilavcı and Eylem Tuğçe Güneyi and Cemil Cengiz and Elif Vural},
  doi          = {10.1016/j.patcog.2024.110628},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110628},
  shortjournal = {Pattern Recognition},
  title        = {Graph domain adaptation with localized graph signal representations},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal interaction with token division strategy for
RGB-t tracking. <em>PR</em>, <em>155</em>, 110626. (<a
href="https://doi.org/10.1016/j.patcog.2024.110626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-T tracking takes visible and infrared images as inputs, which is an extended application of multi-modal fusion in the field of visual object tracking. The complementarity between visible and infrared modalities can enhance the robustness of tracker in complex scenes. Cross-modal interaction can facilitate the fusion and synergy of different modalities, but most previous methods lack clear target information in multi-modal fusion, leading to some undesired cross-relation in interaction. To reduce these undesired cross-relations, we propose a Multi-modal Interaction scheme Guided by Token Division strategy (MIGTD). This scheme divides the input multi-modal tokens into several categories and restricts the interaction between tokens by setting different rules. The above operation is implemented in parallel through an attention masking strategy. To accurately classify search tokens, an instance segmentation task with box-supervised loss is employed. We conduct extensive experiments on three popular benchmark datasets, RGBT234, LasHeR and VTUAV. The experimental results indicate that the tracker proposed in this article reach the world’s advanced level in performance.},
  archive      = {J_PR},
  author       = {Yujue Cai and Xiubao Sui and Guohua Gu and Qian Chen},
  doi          = {10.1016/j.patcog.2024.110626},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110626},
  shortjournal = {Pattern Recognition},
  title        = {Multi-modal interaction with token division strategy for RGB-T tracking},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Toward a deeper understanding: RetNet viewed through
convolution. <em>PR</em>, <em>155</em>, 110625. (<a
href="https://doi.org/10.1016/j.patcog.2024.110625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The success of Vision Transformer (ViT) has been widely reported on a wide range of image recognition tasks. ViT can learn global dependencies superior to CNN, yet CNN’s inherent locality can substitute for expensive training resources. Recently, the outstanding performance of RetNet in the field of language modeling has garnered attention, surpassing that of the Transformer with explicit local modeling, shifting researchers’ focus toward Transformers in the CV field. This paper investigates the effectiveness of RetNet from a CNN perspective and presents a variant of RetNet tailored to the visual domain. Similar to RetNet we improves ViT’s local modeling by applying a weight mask on the original self-attention matrix. A straightforward way to locally adapt the self-attention matrix can be realized by an Element-wise Learnable Mask (ELM), for which our preliminary results show promising results. However, the Element-wise Learnable Mask not only induces a non-trivial additional parameter overhead but also increases the optimization complexity. To this end, this work proposes a novel Gaussian mixture mask (GMM) in which one mask only has two learnable parameters and it can be conveniently used in any ViT variants whose attention mechanism allows the use of masks. Experimental results on multiple small datasets demonstrate that the effectiveness of our proposed Gaussian mask for boosting ViTs for free (almost zero additional parameter or computation cost). Our code is publicly available at https://github.com/CatworldLee/Gaussian-Mixture-Mask-Attention .},
  archive      = {J_PR},
  author       = {Chenghao Li and Chaoning Zhang},
  doi          = {10.1016/j.patcog.2024.110625},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110625},
  shortjournal = {Pattern Recognition},
  title        = {Toward a deeper understanding: RetNet viewed through convolution},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GHOST: Graph-based higher-order similarity transformation
for classification. <em>PR</em>, <em>155</em>, 110623. (<a
href="https://doi.org/10.1016/j.patcog.2024.110623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring and identifying a good feature representation to describe high-dimensional datasets is a challenge of prime importance. However, plenty of feature selection techniques and distance metrics exist, which entails an intricacy for identifying the one best suited to the task. This paper provides an algorithm to design high-order distance metrics over a sparse selection of features dedicated to classification. Our approach is based on Conditional Random Field (CRF) energy minimization and Dual Decomposition, which allow efficiency and great flexibility in the considered features. The optimization technique ensures the tractability of high-dimensionality problems using hundreds of features and samples. Our approach is evaluated on synthetic data as well as on Covid-19 patient stratification. Comparisons with state-of-the-art baselines and our proposed method on different classification results prove the learned metric’s relevance.},
  archive      = {J_PR},
  author       = {Enzo Battistella and Maria Vakalopoulou and Nikos Paragios and Éric Deutsch},
  doi          = {10.1016/j.patcog.2024.110623},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110623},
  shortjournal = {Pattern Recognition},
  title        = {GHOST: Graph-based higher-order similarity transformation for classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving CNN-based semantic segmentation on structurally
similar data using contrastive graph convolutional networks.
<em>PR</em>, <em>155</em>, 110622. (<a
href="https://doi.org/10.1016/j.patcog.2024.110622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structurally similar data exist in most practical semantic segmentation applications. For example, objects can appear identical or positionally similar in many images, such as video frames. Objects with structural similarity in data samples can confuse deep neural networks (DNNs) in semantic segmentation applications. These challenges often lead to lower pixel classification accuracy of natural object segmentation. This study proposes a novel approach (S2-GCN) that enhances CNN-based semantic segmentation for structurally similar data using a contrastive graph convolutional network (GCN). By selecting specific label pairs and developing a customized GCN branch parallel to an encoder-decoder backbone, our method significantly improves accuracy, IoU, and F1-score, by up to 8 %, as demonstrated through an extensive evaluation of five datasets. Our findings show that the proposed method effectively addresses the structural similarity problem of CNN-based semantic segmentation and can be applied to a wide range of practical applications.},
  archive      = {J_PR},
  author       = {Ling Chen and Zedong Tang and Hao Li},
  doi          = {10.1016/j.patcog.2024.110622},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110622},
  shortjournal = {Pattern Recognition},
  title        = {Improving CNN-based semantic segmentation on structurally similar data using contrastive graph convolutional networks},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-resolution self-supervised learning framework for
semantic segmentation in histopathology. <em>PR</em>, <em>155</em>,
110621. (<a href="https://doi.org/10.1016/j.patcog.2024.110621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern whole slide imaging technique together with supervised deep learning approaches have been advancing the field of histopathology, enabling accurate analysis of tissues. These approaches use whole slide images (WSIs) at various resolutions, utilising low-resolution WSIs to identify regions of interest in the tissue and high-resolution for detailed analysis of cellular structures. Due to the labour-intensive process of annotating gigapixels WSIs, accurate analysis of WSIs remains challenging for supervised approaches. Self-supervised learning (SSL) has emerged as an approach to build efficient and robust models using unlabelled data. It has been successfully used to pre-train models to learn meaningful image features which are then fine-tuned with downstream tasks for improved performance compared to training models from scratch. Yet, existing SSL methods optimised for WSI are unable to leverage the multi-resolutions and instead, work only in an individual resolution neglecting the hierarchical structure of multi-resolution inputs. This limitation prevents from the effective utilisation of complementary information between different resolutions, hampering discriminative WSI representation learning. In this paper we propose a Multi-resolution SSL Framework for WSI semantic segmentation (MSF-WSI) that effectively learns histopathological features. Our MSF-WSI learns complementary information from multiple WSI resolutions during the pre-training stage; this contrasts with existing works that only learn between the resolutions at the fine-tuning stage. Our pre-training initialises the model with a comprehensive understanding of multi-resolution features which can lead to improved performance in the subsequent tasks. To achieve this, we introduced a novel Context-Target Fusion Module (CTFM) and a masked jigsaw pretext task to facilitate the learning of multi-resolution features. Additionally, we designed Dense SimSiam Learning (DSL) strategy to maximise the similarities of image features from early model layers to enable discriminative learned representations. We evaluated our method using three public datasets on breast and liver cancer segmentation tasks. Our experiment results demonstrated that our MSF-WSI surpassed the accuracy of other state-of-the-art methods in downstream fine-tuning and semi-supervised settings.},
  archive      = {J_PR},
  author       = {Hao Wang and Euijoon Ahn and Jinman Kim},
  doi          = {10.1016/j.patcog.2024.110621},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110621},
  shortjournal = {Pattern Recognition},
  title        = {A multi-resolution self-supervised learning framework for semantic segmentation in histopathology},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MixingMask: A contour-aware approach for joint object
detection and instance segmentation. <em>PR</em>, <em>155</em>, 110620.
(<a href="https://doi.org/10.1016/j.patcog.2024.110620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remarkable achievements have been made in object detection and segmentation tasks. However, there remains a noticeable scarcity of methodologies that can achieve satisfactory results in both tasks simultaneously. To address this problem, we present a solution called MixingMask, in which we have a key insight to provide specific attention to boundary features by leveraging contour-based segmentation methods. Specifically, our approach commences with a novel contour deformation module that employs mixing operations coupled with the proposed adaptive feature sampling. Successively, the contours are encoded using a decoupled vector for bounding box regression, thereby effectively associating the contour shape with its scale and position. Lastly, we incorporate the proposed contour regression module into the baseline method to achieve specialized attention to boundary features. Such a design not only successfully remedies the prevailing disregard towards boundary features but also forms an implicit liaison between object detection and instance segmentation tasks. Comprehensive experimental assessments validate the superior performance of the proposed method in both object detection and instance segmentation tasks.},
  archive      = {J_PR},
  author       = {Wenzhe Ouyang and Zenglin Xu and Jing Xu and Qifan Wang and Yong Xu},
  doi          = {10.1016/j.patcog.2024.110620},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110620},
  shortjournal = {Pattern Recognition},
  title        = {MixingMask: A contour-aware approach for joint object detection and instance segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMANet: An adaptive memory attention network for video cloud
detection. <em>PR</em>, <em>155</em>, 110616. (<a
href="https://doi.org/10.1016/j.patcog.2024.110616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {According to their orbits, meteorological satellites can be divided into polar-orbiting satellites and geostationary satellites. Existing cloud detection methods mainly focus on polar-orbiting satellite datasets. The geostationary satellite datasets contain, in contrast, time-continuous frames of particular locations. The temporal consistent information in these consecutive frames aid to increase the detection accuracy, but is challenging to be exploited. Besides, powered by the advanced technology of satellites, the onboard cloud detection application becomes a trend. Considering that satellites have resource limitations on energy and storage, applications deployed on them should be lightweight enough. However, the existing cloud detection models never concentrated on this lightweight video cloud detection task before. In this task, the temporal consistent features provided by time-continuous frames should be exploited for accuracy enhancement with low resource consumption. To tackle this problem, we design a lightweight deep learning video cloud detection model: Adaptive Memory Attention Network (AMANet). The proposed network is based on the encoder–decoder structure. The encoder consists of two branches. In the main branch, spatial and semantic features of the current frame are extracted. In the TemporalAttentionFlow branch, the proposed PyramidEncodingModule adaptively extracts context information from frames in sequence based on their distance to the current frame. In addition, in the proposed AdaptiveMemoryAttentionModule, the temporal relation among frames is extracted and propagated adaptively. The lightweight decoder is designed to gradually recover the cloud masks to the same scale as the input image. Experiments on a Video Cloud Detection dataset based on the dataset Fengyun4aCloud demonstrate that the designed AMANet achieves a remarkable balance between accuracy and resource consumption in comparison with current cloud detection methods, lightweight semantic segmentation methods, and video semantic segmentation methods.},
  archive      = {J_PR},
  author       = {Chen Luo and Shanshan Feng and YingLing Quan and Yunming Ye and Yong Xu and Xutao Li and Baoquan Zhang},
  doi          = {10.1016/j.patcog.2024.110616},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110616},
  shortjournal = {Pattern Recognition},
  title        = {AMANet: An adaptive memory attention network for video cloud detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework for convolution-based graph neural
networks. <em>PR</em>, <em>155</em>, 110597. (<a
href="https://doi.org/10.1016/j.patcog.2024.110597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have attracted a lot of research interest in machine learning, and many variants have been proposed recently. In this paper, we take a step forward to establish a unified framework for convolution-based graph neural networks, aiming to provide a systematic view of different GCN variants and deep understanding of the relations among them. Our key idea is formulating the basic graph convolution operation as an optimization problem in the graph Fourier space. Under this framework, a variety of popular GCN models, including vanilla-GCNs, attention-based GCNs and topology-based GCNs, can be interpreted as a similar optimization problem but with different regularizers. This novel perspective enables a better understanding of the similarities and differences among many widely used GCNs, and may inspire new model designs. As a showcase, we present a novel regularization technique under the proposed framework to tackle the oversmoothing problem in graph convolution. The effectiveness of newly designed model is validated empirically.},
  archive      = {J_PR},
  author       = {Xuran Pan and Xiaoyan Han and Chaofei Wang and Zhuo Li and Shiji Song and Gao Huang and Cheng Wu},
  doi          = {10.1016/j.patcog.2024.110597},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110597},
  shortjournal = {Pattern Recognition},
  title        = {A unified framework for convolution-based graph neural networks},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning for RGB-d object tracking.
<em>PR</em>, <em>155</em>, 110543. (<a
href="https://doi.org/10.1016/j.patcog.2024.110543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a growing interest in RGB-D object tracking thanks to its promising performance achieved by combining visual information with auxiliary depth cues. However, the limited volume of annotated RGB-D tracking data for offline training has hindered the development of a dedicated end-to-end RGB-D tracker design. Consequently, the current state-of-the-art RGB-D trackers mainly rely on the visual branch to support the appearance modelling, with the depth map utilised for elementary information fusion or failure reasoning of online tracking. Despite the achieved progress, the current paradigms for RGB-D tracking have not fully harnessed the inherent potential of depth information, nor fully exploited the synergy of vision-depth information. Considering the availability of ample unlabelled RGB-D data and the advancement in self-supervised learning, we address the problem of self-supervised learning for RGB-D object tracking. Specifically, an RGB-D backbone network is trained on unlabelled RGB-D datasets using masked image modelling. To train the network, the masking mechanism creates a selective occlusion of the input visible image to force the corresponding aligned depth map to help with discerning and learning vision-depth cues for the reconstruction of the masked visible image. As a result, the pre-trained backbone network is capable of cooperating with crucial visual and depth features of the diverse objects and background in the RGB-D image. The intermediate RGB-D features output by the pre-trained network can effectively be used for object tracking. We thus embed the pre-trained RGB-D network into a transformer-based tracking framework for stable tracking. Comprehensive experiments and the analysis of the results obtained on several RGB-D tracking datasets demonstrate the effectiveness and superiority of the proposed RGB-D self-supervised learning framework and the following tracking approach.},
  archive      = {J_PR},
  author       = {Xue-Feng Zhu and Tianyang Xu and Sara Atito and Muhammad Awais and Xiao-Jun Wu and Zhenhua Feng and Josef Kittler},
  doi          = {10.1016/j.patcog.2024.110543},
  journal      = {Pattern Recognition},
  month        = {11},
  pages        = {110543},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised learning for RGB-D object tracking},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSFusion: Infrared and visible image fusion method combining
detail and scene information. <em>PR</em>, <em>154</em>, 110633. (<a
href="https://doi.org/10.1016/j.patcog.2024.110633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of image fusion is to combine the complementary features of two images to generate an information-rich fused image. However, taking into account both detail and scene information is difficult for existing image fusion algorithms. Therefore, we propose an infrared and visible image fusion method combining details and scene information (DSFusion). Specifically, we first design a local attention module (LAM), which performs feature extraction on the source image from multiple perspectives in order to better preserve minute detail information. Moreover, in order to distinguish and highlight the differences between the two modal images, we improved the channel attention module. Finally, we design a new loss function that can effectively balance the detail and scene information of the fusion image. Extensive testing on publicly available datasets demonstrates that DSFusion surpasses state of the art in both qualitative and quantitative evaluation. Furthermore, promising outcomes have been obtained in generalization experiments by directly expanding the trained model to other datasets, indicating the model’s excellent generalization capability. The code is available at https://github.com/LKZ1584905069/DSFusion .},
  archive      = {J_PR},
  author       = {Kuizhuang Liu and Min Li and Cheng Chen and Chengwei Rao and Enguang Zuo and Yunling Wang and Ziwei Yan and Bo Wang and Chen Chen and Xiaoyi Lv},
  doi          = {10.1016/j.patcog.2024.110633},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110633},
  shortjournal = {Pattern Recognition},
  title        = {DSFusion: Infrared and visible image fusion method combining detail and scene information},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive visual clustering for improving instance-level
contrastive learning as a plugin. <em>PR</em>, <em>154</em>, 110631. (<a
href="https://doi.org/10.1016/j.patcog.2024.110631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has achieved remarkable success in computer vision, however it is built on instance-level discrimination which leaves the valuable intra-class correlation in dataset unexploited. Current semantic clustering methods are proven to be helpful but they would suffer from the error accumulated in the iteration process without ground-truth guidance. In an attempt to remedy the clustering error accumulation when utilizing intra-class correlation for contrastive learning, we propose an online Contrastive Visual Clustering (CVC) method with two actions: gathering instances with highly similar feature embeddings, and penalizing instances being clustered with low confidence. CVC can integrate with not only contrastive learning but also arbitrary self-supervised learning frameworks simply as a plugin. Under various experiment settings, we show that CVC improves the linear classification performance by a large margin for models pre-trained with self-supervised representation learning, in both image and video scenarios. The code is available at https://github.com/yliu1229/CVC .},
  archive      = {J_PR},
  author       = {Yue Liu and Xiangzhen Zan and Xianbin Li and Wenbin Liu and Gang Fang},
  doi          = {10.1016/j.patcog.2024.110631},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110631},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive visual clustering for improving instance-level contrastive learning as a plugin},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TCFAP-net: Transformer-based cross-feature fusion and
adaptive perception network for large-scale point cloud semantic
segmentation. <em>PR</em>, <em>154</em>, 110630. (<a
href="https://doi.org/10.1016/j.patcog.2024.110630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud semantic segmentation is an ingredient in understanding real-world scenes. Most existing approaches perform poorly on scene boundaries and struggle with recognizing objects of different scales. In this paper, we propose a novel framework that incorporates Transformer into the U-Net architecture for inferring pointwise semantics. Specifically, the Transformer-based cross-feature fusion module is designed first to employ geometric and semantic information to learn feature offsets to overcome the border ambiguity of segmentation results, and then it utilizes the Transformer to learn cross-feature enhanced and fused encoder features. Additionally, to facilitate the overall network’s structure-to-detail perception capabilities, the adaptive perception module is designed, which employs cross-attention to adaptively allocate weights to encoder features at varying resolutions, establishing long-range contextual dependencies. Ablation studies validate the individual contributions of our module design choices. Compared with the existing competitive methods, our approach achieves state-of-the-art performance and exhibits superior results on benchmarks. Code is available at https://github.com/xiluo-cug/TCFAP-Net .},
  archive      = {J_PR},
  author       = {Jianjun Zhang and Zhipeng Jiang and Qinjun Qiu and Zheng Liu},
  doi          = {10.1016/j.patcog.2024.110630},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110630},
  shortjournal = {Pattern Recognition},
  title        = {TCFAP-net: Transformer-based cross-feature fusion and adaptive perception network for large-scale point cloud semantic segmentation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Rethinking local-to-global representation learning for
rotation-invariant point cloud analysis. <em>PR</em>, <em>154</em>,
110624. (<a href="https://doi.org/10.1016/j.patcog.2024.110624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud analysis has drawn much attention in recent years, whereas most existing point-based deep networks ignore the rotation-invariant property of the encoded features, which leads to poor performance given 3D shapes with arbitrary rotation. In this paper, we propose a novel rotation-invariant method that embeds both distinctive local and global rotation-invariant information. Specifically, we design a two-branch network that separately extracts purely local and global rotation-invariant features. In the global branch, we leverage canonical transformation to extract global representations, while in the local branch, we utilize hand-crafted geometric features (e.g., relative distances and angles) to embed local representations. To fuse the features from distinct branches, we introduce an attention-based fusion module to adaptively integrate the local-to-global representation by considering the geometry contexts of each point. Particularly, different from existing rotation-invariant works, we further introduce a self-attention unit into the global branch for embedding non-local information and also insert multiple fusion modules into the local branch to emphasize the global features. Extensive experiments on standard benchmarks show that our method achieves consistent and competitive performance on various downstream tasks, and also the best performance on the shape classification task on the ModelNet40 dataset with a 0.8% accuracy gain, compared to state-of-the-art methods. The code and pre-trained models are available at https://github.com/CentauriStar/Rotation-Invariant-Point-Cloud-Analysis .},
  archive      = {J_PR},
  author       = {Zhaoxuan Wang and Yunlong Yu and Xianzhi Li},
  doi          = {10.1016/j.patcog.2024.110624},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110624},
  shortjournal = {Pattern Recognition},
  title        = {Rethinking local-to-global representation learning for rotation-invariant point cloud analysis},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Nonconvex submodule clustering via joint sliced sparse
gradient and cluster-aware approach. <em>PR</em>, <em>154</em>, 110619.
(<a href="https://doi.org/10.1016/j.patcog.2024.110619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing subspace clustering methods preprocess image data by converting them into vectors, which lacks exploration of the spatial structure of high-dimensional data. Therefore, we proposes a nonconvex submodule clustering model (NSSGCA) via joint sliced sparse gradient and cluster-aware approach. NSSGCA arranges each 2D image as lateral slices of a 3rd-order tensor, and utilizes the t t -product under the model of the union of free submodules to represent 3rd-order tensor samples, thereby exploring the latent spatial structure of samples. To more accurately approximate tensor rank, a nonconvex Schatten p p -norm constraint is imposed on the rotated representation tensor. Under the submodule framework, a consistent gradient matrix is derived based on the δ δ -nearest neighbor adjacency graph to construct sliced sparse gradient (SSG) regularization, which is more conducive to clustering tasks. NSSGCA learns representation tensor with clearer block-like structure based on ℓ q ℓq norm and cluster-aware attention mechanism. The convergence of the constructed sequence to the stationary Karush–Kuhn–Tucker (KKT) point is proven. Experimental results on real-world image datasets confirm the effectiveness of NSSGCA.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Tingquan Deng and Ming Yang},
  doi          = {10.1016/j.patcog.2024.110619},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110619},
  shortjournal = {Pattern Recognition},
  title        = {Nonconvex submodule clustering via joint sliced sparse gradient and cluster-aware approach},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-ensembling depth completion via density-aware
consistency. <em>PR</em>, <em>154</em>, 110618. (<a
href="https://doi.org/10.1016/j.patcog.2024.110618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion can predict a dense depth map by taking a sparse depth map and the aligned RGB image as input, but the acquisition of ground truth annotations is labor-intensive and non-scalable. Therefore, we resort to semi-supervised learning, where we only need to annotate a few images and leverage massive unlabeled data without ground truth labels to facilitate model learning. In this paper, we propose SEED , a SE lf- E nsembling D epth completion framework to enhance the generalization of the model on unlabeled data. Specifically, SEED contains a pair of the teacher and student models, which are given high-density and low-density sparse depth maps as input respectively. The main idea underpinning SEED is to enforce the density-aware consistency by encouraging consistent prediction across different-density input depth maps. One empirical challenge is that the pseudo-depth labels produced by the teacher model inevitably contain wrong depth values, which would mislead the convergence of the student model. To resist the noisy labels, we propose an automatic method to measure the reliability of the generated pseudo-depth labels adaptively. By leveraging the discrepancy of prediction distributions, we model the pixel-wise uncertainty map as the prediction variance and rectify the training process from noisy labels explicitly. To our knowledge, we are among the early semi-supervised attempts on the depth completion task. Extensive experiments on both outdoor and indoor datasets demonstrate that SEED consistently improves the performance of the baseline model by a large margin and even is on par with several fully-supervised methods.},
  archive      = {J_PR},
  author       = {Xuanmeng Zhang and Zhedong Zheng and Minyue Jiang and Xiaoqing Ye},
  doi          = {10.1016/j.patcog.2024.110618},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110618},
  shortjournal = {Pattern Recognition},
  title        = {Self-ensembling depth completion via density-aware consistency},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust feature selection via central point link information
and sparse latent representation. <em>PR</em>, <em>154</em>, 110617. (<a
href="https://doi.org/10.1016/j.patcog.2024.110617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Before conducting unsupervised feature selection, it is usually assumed that these data are independent of each other. On the contrary, real data will influence each other. Therefore, traditional feature selection methods may lose information related to each other between data. This can lead to inaccurately generated pseudo-label information and may result in poor feature selection results. To find solutions to this issue, this paper proposes robust feature selection via central point link information and sparse latent representation (CPSLR). Firstly, structure a link graph by calculating the center matrix to store the distance information from the sample to the center point. If two samples have similar distances to the center point, it can be determined that they belong to the same class. Therefore, the similarity between samples is preserved, and more accurate pseudo-label information is obtained. Secondly, CPSLR uses data graph and link graph to form a dual graph structure. It can not only retain the link information between samples but also retain the manifold structures of the samples. Then, CPSLR saves the interconnection contents between samples by sparse latent representation. That is, the constraint l 2,1 -norm is exerted on the expression of latent representation, and sparse non-redundant interconnection information is preserved. And by combining central point link information with sparse latent representation makes the interconnections between data reserved more comprehensive. That is to say, the pseudo-labels obtained are more like the real labels of the classes. Finally, CPSLR constrains the feature transformation matrix by l 2,1/2 -norm constraint so as to select robust and sparse features. CPSLR uses l 2,1/2 -norm constraint to assure that the feature transformation matrix is sparse, selecting more discriminative features, thereby obtaining the feature selection that can improve its efficiency. The experiments demonstrate that the clustering result of CPSLR outperform six classical or latest compared algorithms on eight datasets.},
  archive      = {J_PR},
  author       = {Jiarui Kong and Ronghua Shang and Weitong Zhang and Chao Wang and Songhua Xu},
  doi          = {10.1016/j.patcog.2024.110617},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110617},
  shortjournal = {Pattern Recognition},
  title        = {Robust feature selection via central point link information and sparse latent representation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C2FResMorph: A high-performance framework for unsupervised
2D medical image registration. <em>PR</em>, <em>154</em>, 110615. (<a
href="https://doi.org/10.1016/j.patcog.2024.110615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable medical image registration is an important precursor task for surgical automation, while enhancing the registration performance of 2D medical images remains a challenging work. Existing methods primarily minimize the similarity loss between image pairs as the main optimization objective, leading to limited registration accuracy and a lack of pixel matching. Moreover, the scarcity of informative features in 2D images often results in overfitting on the training set, hampering generalization. To address these issues, we propose C2FResMorph, a learning-based deformable registration algorithm specifically designed for 2D medical images. C2FResMorph employs a two-stage framework that improves registration accuracy and preserves topology during deformation in a coarse-to-fine manner. Inside the framework, by leveraging the convolutional neural network’s locality and the multi-head self-attention mechanism’s globality, a ResMorph registration network is designed. Additionally, the integration of residual image knowledge addresses deformation folding in 2D image registration, enhancing the preservation of local structures and improving generalization. Experimental evaluations on three datasets demonstrate that C2FResMorph outperforms existing learning-based methods in terms of accuracy, generalization ability for 2D medical image registration, and also retains the efficiency advantages.},
  archive      = {J_PR},
  author       = {Yi Ding and Junjian Bu and Zhen Qin and Li You and Mingsheng Cao and Zhiguang Qin and Minghui Pang},
  doi          = {10.1016/j.patcog.2024.110615},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110615},
  shortjournal = {Pattern Recognition},
  title        = {C2FResMorph: A high-performance framework for unsupervised 2D medical image registration},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MLENet: Multi-level extraction network for video action
recognition. <em>PR</em>, <em>154</em>, 110614. (<a
href="https://doi.org/10.1016/j.patcog.2024.110614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is a well-established task in the field of computer vision. However, accurately representing spatio-temporal information remains a challenge due to the complex interplay between human actions, video timing, and scene changes. To address this challenge and improve the efficiency of temporal modeling in videos, we propose MLENet, a novel approach that eliminates contextual data and eliminates the need for laborious optical flow extraction.MLENet incorporates a Temporal Feature Refinement Extraction Module (TFREM) that utilizes Optical Flow Guided Features to enhance attention to local deep detail information. This refinement process significantly enhances the network’s capacity for feature learning and expression. Moreover, MLENet is designed to be trained end-to-end, facilitating seamless integration into existing frameworks. Additionally, our model adopts a temporal segmentation structure for sampling, effectively reducing redundant information and improving computational efficiency. Compared to existing video-based action recognition models that require optical flow or other modalities, MLENet achieves substantial performance enhancements while requiring fewer inputs. We validate the effectiveness of our proposed approach on benchmark datasets, including Something-Something V1&amp;V2, UCF-101, and HMDB-51, where MLENet consistently outperforms state-of-the-art models.},
  archive      = {J_PR},
  author       = {Fan Wang and Xinke Li and Han Xiong and Haofan Mo and Yongming Li},
  doi          = {10.1016/j.patcog.2024.110614},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110614},
  shortjournal = {Pattern Recognition},
  title        = {MLENet: Multi-level extraction network for video action recognition},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive propagation deep graph neural networks.
<em>PR</em>, <em>154</em>, 110607. (<a
href="https://doi.org/10.1016/j.patcog.2024.110607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) with adaptive propagation combinations represent a specialized deep learning paradigm, engineered to capture complex nodal interconnections within graph data. The primary challenge of this model lies in distilling and representing features extracted over varying nodal distances. This paper delves into an array of adaptive propagation strategies, with a focus on the influence of nodal distances and information aggregation on model efficacy. Our investigation identifies a critical performance drop in scenarios featuring overly brief propagation paths or an insufficient number of layers. Addressing this, we propose an innovative adaptive propagation technique in deep graph neural networks, named AP-DGNN, aimed at reconstructing high-order graph convolutional neural networks (GCNs). The AP-DGNN model assigns unique aggregation combination weights to each node and category, culminating in a final model representation through a process of weighted aggregation. Notably, these weights are capable of assimilating both subjective and objective information characteristics within the network. To substantiate our model’s effectiveness and scalability, we employed often-used benchmark datasets for experimental validation. A notable aspect of our AP-DGNN model is its minimal training parameter requirement and reduced computational demand. Furthermore, we demonstrate the model’s enhanced performance, which remains consistent across various hyperparameter configurations. This aspect was rigorously tested under diverse hyperparameter settings. Our findings contribute significantly to the evolution of graph neural networks, potentially revolutionizing their application across multiple domains. The research presented herein not only advances the understanding of GNNs but also paves the way for their robust application in varied scenarios. Codes are available at https://github.com/CW112/AP_DGNN .},
  archive      = {J_PR},
  author       = {Wei Chen and Wenxu Yan and Wenyuan Wang},
  doi          = {10.1016/j.patcog.2024.110607},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110607},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive propagation deep graph neural networks},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Module-based graph pooling for graph classification.
<em>PR</em>, <em>154</em>, 110606. (<a
href="https://doi.org/10.1016/j.patcog.2024.110606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Network (GNN) models are recently proposed to process the graph-structured data for the learning tasks on graphs, e.g., node classification, link prediction, and so on. This work focuses on the graph classification task, aiming to obtain the graph representation and predict the class label for a graph. Existing works proposed applying graph pooling to obtain graph embedding but still suffer from several issues. First, node embeddings are generated according to the topological information of the whole graph, but ignoring the local isomorphic substructures commonly seen in bioinformatics and chemistry. Another limitation arises when aggregating node embeddings. The hard assignment obtained through clustering algorithms, which rely on preset and fixed parameters instead of considering the graph’s properties adaptively, restricts the flexibility in handling graphs of varying scales. To address the above problems, a module-based graph pooling framework (MGPool) is proposed in this work. Inspired by the rules of bioinformatics, MGPool assumes that a graph consists of multiple modules (also known as sub structures), which are identified based on the natural organization of the graph rather than the hard allocation of nodes. Benefiting from the hypothesis, MGPool generates node embeddings from graph-view and module-view, which is capable to capture global graph information and local isomorphic information respectively. Then module-level pooling is used to capture the intra-module information, while the inter-module information in terms of the correlation between modules is obtained through graph-level pooling. Finally, an entropy-based weighting mechanism is proposed to adjust the modules’ weights for the graph aggregation. Experiments conducted on bioinformatics benchmark datasets demonstrate the effectiveness of MGPool by outperforming other state-of-the-art graph pooling methods. For social network datasets, MGPool also provides competitive performance. Moreover, the visualization of module entropy weights is given to reveal the interpretability of the model.},
  archive      = {J_PR},
  author       = {Sucheng Deng and Geping Yang and Yiyang Yang and Zhiguo Gong and Can Chen and Xiang Chen and Zhifeng Hao},
  doi          = {10.1016/j.patcog.2024.110606},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110606},
  shortjournal = {Pattern Recognition},
  title        = {Module-based graph pooling for graph classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-convex tensorial multi-view clustering by integrating
ℓ1-based sliced-laplacian regularization and ℓ2,p-sparsity. <em>PR</em>,
<em>154</em>, 110605. (<a
href="https://doi.org/10.1016/j.patcog.2024.110605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the recent upswing in interest around multi-view clustering procedures. Such methods aim to boost clustering efficiency by leveraging information from numerous perspectives. Much research has been devoted to tensorial representation to exploit high-order correlations underlying disparate views while preserving the local geometric structure inside each view. Our research introduces a novel multi-view clustering approach. This approach creates a 3rd-order tensor, assimilating features from all perspectives. We use the t-product in the tensor space to generate the self-representation tensor from the tensorial data. We incorporate the ℓ1 -based sliced-Laplacian regularization to increase our model’s resilience and introduce a fresh column-wise sparse norm: the ℓ2,p -norm with 0&amp;lt;p&amp;lt;1 . This norm displays attributes of invariance, continuity, and differentiability. We present a closed-form answer to the ℓ2,p -regularized shrinkage problem, broadening its relevance to other generalized problems. Simultaneously, we propose a tensorial arctan -function as an improved surrogate for the tensor rank. This function has proven more proficient at assessing consistency across multiple viewpoints. By integrating these two components, we formulate an effective algorithm that refines our suggested model, ensuring that the constructed sequence gravitates toward the stationary KKT point. Our team conducts extensive experiments on various datasets to evaluate our model’s effectiveness, spanning diverse situations and scales. Results from these experiments emphasize that our approach establishes a novel performance standard.},
  archive      = {J_PR},
  author       = {Deyan Xie and Ming Yang and Quanxue Gao and Wei Song},
  doi          = {10.1016/j.patcog.2024.110605},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110605},
  shortjournal = {Pattern Recognition},
  title        = {Non-convex tensorial multi-view clustering by integrating ℓ1-based sliced-laplacian regularization and ℓ2,p-sparsity},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint-individual fusion structure with fusion attention
module for multi-modal skin cancer classification. <em>PR</em>,
<em>154</em>, 110604. (<a
href="https://doi.org/10.1016/j.patcog.2024.110604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many convolutional neural network (CNN) based approaches for skin cancer classification primarily rely on dermatological images, yielding commendable results in classification accuracy. However, leveraging patient metadata, a crucial source of clinical information for dermatologists, can further enhance accuracy. Current methodologies predominantly employ basic joint fusion structures (FS) and fusion modules (FMs) for multi-modal classification, leaving room for advancement in enhancing accuracy through exploration of more sophisticated FS and FM architectures. Thus, this paper introduces a novel fusion method that integrates dermatological images (dermoscopy images or clinical images) with patient metadata for skin cancer classification, focusing on enhancing FS and FM components. Initially, we propose a joint-individual fusion (JIF) structure that simultaneously learns shared features across multi-modality data while preserving specific characteristics. Subsequently, we introduce a multi-modal fusion attention (MMFA) module designed to amplify the most relevant image and metadata features through a combination of self and mutual attention mechanisms, thereby bolstering the decision-making pipeline. Our study compares the efficacy of the proposed JIF-MMFA method with other state-of-the-art fusion techniques across three distinct public datasets. Results demonstrate that the JIF-MMFA method consistently enhances classification outcomes across various CNN backbones, outperforming alternative fusion methodologies on all three datasets. These findings underscore the effectiveness and robustness of our proposed approach in skin cancer classification.},
  archive      = {J_PR},
  author       = {Peng Tang and Xintong Yan and Yang Nan and Xiaobin Hu and Bjoern H. Menze and Sebastian Krammer and Tobias Lasser},
  doi          = {10.1016/j.patcog.2024.110604},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110604},
  shortjournal = {Pattern Recognition},
  title        = {Joint-individual fusion structure with fusion attention module for multi-modal skin cancer classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-precision ellipse detection method based on quadrant
representation and top-down fitting. <em>PR</em>, <em>154</em>, 110603.
(<a href="https://doi.org/10.1016/j.patcog.2024.110603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ellipse detection is a basic task in many computer-vision related problems. While widely studied in recent years, accurate and efficient detection in real-world images is still a challenge. In this paper, a novel ellipse detector, with high accuracy and efficiency, is proposed. The detector models edge by block sequences, and extracts a set of elliptical arcs, which are classified into four sets. Then top-down ellipse fitting strategy that also makes the method able to detect small and flat ellipses is designed. A two-level validation process is used to select highly probable potential ellipses, especially for fragmented ellipses. Experiments on four synthetic datasets show that the proposed method performs far better than existing methods. In images with severe cluttering and occlusion, the F-measure can still be around 0.9. On four real image datasets the proposed method achieves better F-measure scores with competitive speed than state-of-the-art techniques.},
  archive      = {J_PR},
  author       = {Hongxia Zhou and Lixin Han and Shaojun Zhu and Hong Yan},
  doi          = {10.1016/j.patcog.2024.110603},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110603},
  shortjournal = {Pattern Recognition},
  title        = {A high-precision ellipse detection method based on quadrant representation and top-down fitting},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Looking beyond input frames: Self-supervised adaptation for
video super-resolution. <em>PR</em>, <em>154</em>, 110602. (<a
href="https://doi.org/10.1016/j.patcog.2024.110602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent test-time adaptive video super-resolution (VSR) methods have elevated the performance by exploiting the self-similar patches within the low-resolution (LR) frames to adapt to given input frames. However, the LR frames contain a limited amount of such patches, limiting the performance of such adaptation methods, especially for a challenging scale factor ( e.g. , × × 4). In this work, we propose to explore beyond the input LR frames. In particular, we observe that a greater amount of self-similar patches across various scales can be found from estimated high-resolution (HR) frames ( i.e. , initially restored frames) produced by a pre-trained VSR network. Upon the observation, we propose a new self-supervision test-time adaptation approach via self-distillation to exploit such rich amount of self-similar patches from initially restored frames. Specifically, we perform self-distillation by exploiting multi-scale relationship: distilling knowledge from larger patches to smaller ones with similar semantics. Our framework is flexible and effective as the knowledge can be distilled either from the network itself or the larger one. Furthermore, our framework demonstrates the robustness, being able to recover from undesirable artifacts present in initially restored frames. Extensive evaluation with various VSR networks on numerous datasets reveals that our algorithm consistently improves the restoration quality by a large margin without ground-truth HR video frames . Code is available at: https://github.com/jinsuyoo/bissa .},
  archive      = {J_PR},
  author       = {Jinsu Yoo and Jihoon Nam and Sungyong Baik and Tae Hyun Kim},
  doi          = {10.1016/j.patcog.2024.110602},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110602},
  shortjournal = {Pattern Recognition},
  title        = {Looking beyond input frames: Self-supervised adaptation for video super-resolution},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient multi-task progressive learning for semantic
segmentation and disparity estimation. <em>PR</em>, <em>154</em>,
110601. (<a href="https://doi.org/10.1016/j.patcog.2024.110601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene understanding is an important area in robotics and autonomous driving. To accomplish these tasks, the 3D structures in the scene have to be inferred to know what the objects and their locations are. To this end, semantic segmentation and disparity estimation networks are typically used, but running them individually is inefficient since they require high-performance resources. A possible solution is to learn both tasks together using a multi-task approach. Some current methods address this problem by learning semantic segmentation and monocular depth together. However, monocular depth estimation from single images is an ill-posed problem. A better solution is to estimate the disparity between two stereo images and take advantage of this additional information to improve the segmentation. This work proposes an efficient multi-task method that jointly learns disparity and semantic segmentation. Employing a Siamese backbone architecture for multi-scale feature extraction, the method integrates specialized branches for disparity estimation and coarse and refined segmentations, leveraging progressive task-specific feature sharing and attention mechanisms to enhance accuracy for solving both tasks concurrently. The proposal achieves state-of-the-art results for joint segmentation and disparity estimation on three distinct datasets: Cityscapes, TrimBot2020 Garden, and S-ROSeS, using only 1 / 3 1/3 of the parameters of previous approaches.},
  archive      = {J_PR},
  author       = {Hanz Cuevas-Velasquez and Alejandro Galán-Cuenca and Robert B. Fisher and Antonio Javier Gallego},
  doi          = {10.1016/j.patcog.2024.110601},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110601},
  shortjournal = {Pattern Recognition},
  title        = {Efficient multi-task progressive learning for semantic segmentation and disparity estimation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative compensative transformer network for salient
object detection. <em>PR</em>, <em>154</em>, 110600. (<a
href="https://doi.org/10.1016/j.patcog.2024.110600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is of high significance for various computer vision applications but is a challenging task due to the complicated scenes in real-world images. Most state-of-the-art SOD methods aim to build long-range dependency for improving global contrast modeling in complicated scenes. However, most of them suffer from the prior assumption of treating image patches as visual tokens for building long-range dependency. This is because this assumption leads to localizing salient regions with uncertain boundaries due to the lost object structure information. In this paper, to address this issue, we re-construct the prior assumption of treating both patches and superpixels as visual tokens for building long-range dependency, which takes into account the properties of superpixels and patches in preserving detailed structural-aware information and local context information, respectively. Based on the re-constructed prior assumption, we propose a Collaborative Compensative Transformer Network (CCTNet) for the SOD task. CCTNet firstly alternates the computation within the same kind of vision tokens and among different vision tokens to build their dependencies. By this means, the relationship between multi-level global context and detailed structure representation can be explicitly modeled for consistent semantic and object structure understanding. Then, CCTNet performs feature joint decoding for SOD by fusing the complementary global context and detailed structure for locating objects with certain boundaries. Extensive experiments were conducted to validate the effectiveness of the proposed modules. Furthermore, the experiments on ten benchmark datasets demonstrated the state-of-the-art performance of CCTNet on both RGB and RGB-D SOD.},
  archive      = {J_PR},
  author       = {Jun Chen and Heye Zhang and Mingming Gong and Zhifan Gao},
  doi          = {10.1016/j.patcog.2024.110600},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110600},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative compensative transformer network for salient object detection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-contrast mutual fusion network for joint MRI
reconstruction and super-resolution. <em>PR</em>, <em>154</em>, 110599.
(<a href="https://doi.org/10.1016/j.patcog.2024.110599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) is a widely used medical imaging technique that has become an essential tool for diagnosing various diseases and visualizing internal structures and tissues in the human body. MRI reconstruction and super-resolution are two techniques that can enhance image quality and accelerate the imaging process. However, current methods perform these tasks independently and fail to consider the correlations between them. Additionally, multi-contrast SR methods typically concatenate features from different contrasts without considering their correlation. In this paper, we propose a novel Cross-contrast Mutual Fusion Network (CMF-Net) that performs joint MRI reconstruction and super-resolution by enabling mutual propagation of feature representations between the two tasks. The CMF-Net framework consists of two stages: the first stage focuses on fusing multi-contrast features, while the second stage aims to learn task-specific information for joint MRI reconstruction and super-resolution. We propose a Multi-contrast Feature Aggregation (MFA) module to facilitate the integration of multi-contrast features. This module captures multi-scale information from auxiliary contrast to enhance the feature representation’s capability. Furthermore, a Multi-task Mutual Fusion (MMF) module is presented to integrate task-specific features, which explores the correlation between the two tasks to improve MR super-resolution performance. We evaluate the proposed CMF-Net approach on two public MR datasets. Quantitative and qualitative results demonstrate that our CMF-Net outperforms other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yue Ding and Tao Zhou and Lei Xiang and Ye Wu},
  doi          = {10.1016/j.patcog.2024.110599},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110599},
  shortjournal = {Pattern Recognition},
  title        = {Cross-contrast mutual fusion network for joint MRI reconstruction and super-resolution},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent graph learning for multi-view spectral
clustering. <em>PR</em>, <em>154</em>, 110598. (<a
href="https://doi.org/10.1016/j.patcog.2024.110598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the heterogeneous information of multiple views and the possible noise embedded in multi-view data, it is difficult to directly learn a consistent representation from multiple graphs to depict the intrinsic structure of all views. We propose a consistent graph learning method by exploiting both the high-order correlations underlying multiple views and the global structure of each single view. First, we calculate a transition probability matrix from each view. Second, a tensor is constructed by stacking each transition matrix as its frontal slice and then decomposed into the latent and error tensors. For the latent tensor, after rotated, a weighted tensor nuclear norm is used to encourage the rotate tensor to fully exploit the high-order correlations underlying multiple views. Furthermore, each frontal slice of the latent tensor is regularized by the nuclear norm to capture the global structure of each single view, and is restricted by probability constraints. Besides, we adopt the Frobenius-norm-based regularization to directly learn a common affinity matrix from the latent tensor. The established model is readily optimized by the alternating Lagrangian method. Extensive experiments on six real world datasets demonstrate that our method outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Deyan Xie and Quanxue Gao and Yougang Zhao and Fan Yang and Wei Song},
  doi          = {10.1016/j.patcog.2024.110598},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110598},
  shortjournal = {Pattern Recognition},
  title        = {Consistent graph learning for multi-view spectral clustering},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light dual hypergraph convolution for collaborative
filtering. <em>PR</em>, <em>154</em>, 110596. (<a
href="https://doi.org/10.1016/j.patcog.2024.110596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems filter information to meet users’ personalized interests actively. Existing graph-based models typically extract users’ interests from a heterogeneous interaction graph. They do not distinguish learning between users and items, ignoring the heterogeneous property. In addition, the interaction sparsity and long-tail bias issues still limit the recommendation performance significantly. Fortunately, hidden homogeneous correlations that have a considerable volume can entangle abundant CF signals. In this paper, we propose a light dual hypergraph convolution (LDHC) for collaborative filtering, which designs a hypergraph to involve heterogeneous and homogeneous correlations with more CF signals confronting the challenges. Over the integrated hypergraph, a two-level interest propagation is performed within the heterogeneous interaction graph and between the homogeneous user/item graphs to model users’ interests, where learning on users and items is distinguished and collaborated by the homogeneous propagation. Specifically, hypergraph convolution is lightened by removing unnecessary parameters to propagate users’ interests. Extensive experiments on publicly available datasets demonstrate that the proposed LDHC outperforms the state-of-the-art baselines.},
  archive      = {J_PR},
  author       = {Meng Jian and Langchen Lang and Jingjing Guo and Zun Li and Tuo Wang and Lifang Wu},
  doi          = {10.1016/j.patcog.2024.110596},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110596},
  shortjournal = {Pattern Recognition},
  title        = {Light dual hypergraph convolution for collaborative filtering},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Influence maximization for heterogeneous networks based on
self-supervised clustered heterogeneous graph transformer. <em>PR</em>,
<em>154</em>, 110595. (<a
href="https://doi.org/10.1016/j.patcog.2024.110595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) has drawn significant attention in recent years. Most existing IM methods primarily focus on homogeneous networks, and do not take into account the heterogeneity and the attributes of different types of nodes in heterogeneous networks. However, heterogeneous networks are ubiquitous in real world, encompassing rich semantics and complex structural information. Additionally, the clustering characteristics inherent in a network have a critical and substantial impact on the process of information diffusion, which is often overlooked in IM models designed for heterogeneous networks. To address the challenges posed by the heterogeneity and clustering structure in heterogeneous networks, we propose a novel deep learning framework based on a self-supervised clustered heterogeneous graph transformer for IM in heterogeneous networks, which we have named SCHGT-IM. SCHGT-IM aggregates the heterogeneity and clustering information in heterogeneous networks and incorporates a clustered cascade (CC) model as an information diffusion model to enhance the realism of simulations. We evaluate the performance of SCHGT-IM in comparison with that of state-of-the-art IM models using three academic heterogeneous networks extracted from the DBLP dataset. The experimental results on influence spread demonstrate that SCHGT-IM is superior to fourteen state-of-the-art algorithms and is highly effective in selecting influential seed nodes of different types from heterogeneous networks.},
  archive      = {J_PR},
  author       = {Ying Li and Linlin Li and Xiangyu Liu and Yijun Liu and Qianqian Li},
  doi          = {10.1016/j.patcog.2024.110595},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110595},
  shortjournal = {Pattern Recognition},
  title        = {Influence maximization for heterogeneous networks based on self-supervised clustered heterogeneous graph transformer},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised feature-gate coupling for dynamic network
pruning. <em>PR</em>, <em>154</em>, 110594. (<a
href="https://doi.org/10.1016/j.patcog.2024.110594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gating modules have been widely explored in dynamic network pruning (DNP) to reduce the run-time computational cost of deep neural networks while keeping the features representative. Despite the substantial progress, existing methods remain ignoring the consistency between feature and gate distributions, which may lead to distortion of gated features. In this paper, we propose a feature-gate coupling (FGC) approach aiming to align distributions of features and gates. FGC is a plug-and-play module that consists of two steps carried out in an iterative self-supervised manner. In the first step, FGC utilizes the k k -Nearest Neighbor algorithm in the feature space to explore instance neighborhood relationships, which are treated as self-supervisory signals. In the second step, FGC exploits contrastive self-supervised learning (CSL) to regularize gating modules, leading to the alignment of instance neighborhood relationships within the feature and gate spaces. Experimental results validate that the proposed FGC method improves the baseline approach with significant margins, outperforming state-of-the-art methods with a better accuracy-computation trade-off. Code is publicly available at github.com/smn2010/FGC-PR .},
  archive      = {J_PR},
  author       = {Mengnan Shi and Chang Liu and Jianbin Jiao and Qixiang Ye},
  doi          = {10.1016/j.patcog.2024.110594},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110594},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised feature-gate coupling for dynamic network pruning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep graph layer information mining convolutional network.
<em>PR</em>, <em>154</em>, 110593. (<a
href="https://doi.org/10.1016/j.patcog.2024.110593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution network is a powerful method of deep learning of graph structure data. Existing methods usually adjust the neighborhood information aggregation mode or optimize the graph topology layer by layer for improving the graph convolution network. However, these methods seldom consider the discriminative information about hierarchical characteristics nodes (some special nodes only can be correctly classified in one layer and are the misclassification nodes in the other layers of deep graph convolutional networks) in the different layers for complementing the neighborhood topology information. To further find these information, a deep graph layer information mining convolutional network (GLIM) can alternately measure the neighborhood ranking information on topology structure and update the residual identity mapping node information on the different layers for enhancing the model classification performance. Moreover, GLIM can construct a unified framework with the various hyper-parameters for the different graph learning method based on graph convolution network. Experiments show GLIM outperforms the state-of-the-art methods for semi-supervised node classification in three cite datasets (Cora, CiteSeer,and PubMed) and three image datasets (MNIST, Cifar10 and Cifar100).},
  archive      = {J_PR},
  author       = {Guangfeng Lin and Wenchao Wei and Xiaobing Kang and Kaiyang Liao and Erhu Zhang},
  doi          = {10.1016/j.patcog.2024.110593},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110593},
  shortjournal = {Pattern Recognition},
  title        = {Deep graph layer information mining convolutional network},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Joint learning of latent subspace and structured graph for
multi-view clustering. <em>PR</em>, <em>154</em>, 110592. (<a
href="https://doi.org/10.1016/j.patcog.2024.110592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing multi-view clustering methods rely solely on subspace clustering or graph-based clustering. Subspace clustering reduces the redundant information in high dimensional data, but it neglects the intrinsic structural dependencies among samples. Graph-based clustering can model the similarity among samples but tends to suffer from redundant information. In this paper, a novel framework jointing subspace learning and structured graph learning for multi-view clustering (SSMC) is proposed, which benefits from the merits of both subspace learning and structured graph learning. SSMC utilizes graph regularized subspace learning to obtain low dimensional consensus features, where the embedded features are ensured to have maximized correlation to reduce the redundant information, and the graph regularization forces embedded features to preserve their sample similarities. Meanwhile, an adaptive structured graph is learned based on the consensus features in the embedded feature space, avoiding the curse of dimensionality in the graph learning procedure. A rank constraint forces the learned graph to have exactly the same number of connected components as the number of clusters, to obtain a more reliable structured graph. Moreover, an effective algorithm is proposed to optimize the SSMC, where the graph regularized subspace learning part and the structured graph learning part are jointly optimized in a mutual reinforcement manner. The experimental results on real-world benchmark datasets show that the SSMC outperforms the state-of-the-arts in multi-view clustering tasks.},
  archive      = {J_PR},
  author       = {Yinuo Wang and Yu Guo and Zheng Wang and Fei Wang},
  doi          = {10.1016/j.patcog.2024.110592},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110592},
  shortjournal = {Pattern Recognition},
  title        = {Joint learning of latent subspace and structured graph for multi-view clustering},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix normal PCA for interpretable dimension reduction and
graphical noise modeling. <em>PR</em>, <em>154</em>, 110591. (<a
href="https://doi.org/10.1016/j.patcog.2024.110591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis (PCA) is one of the most widely used dimension reduction and multivariate statistical techniques. From a probabilistic perspective, PCA seeks a low-dimensional representation of data in the presence of independent identical Gaussian noise. Probabilistic PCA (PPCA) and its variants have been extensively studied for decades. Most of them assume the underlying noise follows a certain independent identical distribution. However, the noise in the real world is usually complicated and structured. To address this challenge, some variants of PCA for non-IID data have been proposed. However, most of the existing methods only assume that the noise is correlated in the feature space while there may exist two-way structured noise. To this end, we propose a powerful and intuitive PCA method (MN-PCA) through modeling the graphical noise by the matrix normal distribution, which enables us to explore the structure of noise in both the feature space and the sample space. MN-PCA obtains a low-rank representation of data and the structure of noise simultaneously. And it can be explained as approximating data over the generalized Mahalanobis distance. We first solve this model by a standard approach – maximizing the regularized likelihood – and then develop a novel algorithm that exploits the Wasserstein distance, which is more robust. Extensive experiments on various data demonstrate their effectiveness.},
  archive      = {J_PR},
  author       = {Chihao Zhang and Kuo Gai and Shihua Zhang},
  doi          = {10.1016/j.patcog.2024.110591},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110591},
  shortjournal = {Pattern Recognition},
  title        = {Matrix normal PCA for interpretable dimension reduction and graphical noise modeling},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient blur kernel estimation method for blind image
super-resolution. <em>PR</em>, <em>154</em>, 110590. (<a
href="https://doi.org/10.1016/j.patcog.2024.110590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many existing space-variant blind image Super-Resolution (SR) methods require the generation of blur kernels for all input pixels. However, the existing methods overlook the possibility of similar blur kernel patterns among adjacent pixels, leading to spatial incoherence and wasteful computational resources. In this study, we introduce an efficient two-stage method for estimating blur kernels. Instead of generating pixel-wise kernels, a limited set of kernels at pixels with distinct information are generated first and the remaining kernels are reconstructed based on this initial set. This novel method, referred to as Anchor Detection and Kernel Reconstruction, comprises two main components: an Anchor Detection Module (ADM) and a Kernel Reconstruction Module (KRM). The objective of ADM is to identify pixels in a given Low-Resolution image that contain rich information, which are called anchors. The corresponding blur kernels, denoted as anchor kernels, are then generated for these identified pixels using a complete backbone network. The remaining blur kernels are reconstructed using KRM with a lightweight interpolation method based on the anchor kernels to enhance spatial consistency among the reconstructed pixels. Extensive experiments demonstrate that the proposed ADKR method maintains comparable performance while estimating only 50% of blur kernels with a full backbone network, reaching approximately 20% reduction in FLOPs. The code has been made available at https://github.com/xuyimin0926/ADKR .},
  archive      = {J_PR},
  author       = {Yimin Xu and Nanxi Gao and Fei Chao and Rongrong Ji},
  doi          = {10.1016/j.patcog.2024.110590},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110590},
  shortjournal = {Pattern Recognition},
  title        = {An efficient blur kernel estimation method for blind image super-resolution},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D surface segmentation from point clouds via quadric fits
based on DBSCAN clustering. <em>PR</em>, <em>154</em>, 110589. (<a
href="https://doi.org/10.1016/j.patcog.2024.110589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting surfaces from 3D point clouds is significant in reconstructing and transforming these discrete points into their corresponding models. Scanned point clouds are often accompanied by noise, and the existing methods mainly rely on local feature similarities for surface extractions. Errors in estimating the feature information may lead to incorrect surface detection. In this paper we propose a surface extraction and boundary detection method based on clustering technique. The method can be described in three steps: In the first step, a normal correction is carried out using the information from the neighborhood of points with sharp features. The second step is to cluster the points that meet the coplanar condition of the local quadric surface (LQS). In the third step, surface merger is performed by merging the local surfaces satisfying the merging conditions. Experimental validation is carried out to determine the effectiveness of the proposed method. The experimental results show improved surface extraction accuracy of the proposed method in comparison to RANSAC, RG, LCCP, C2NO and HT methods.},
  archive      = {J_PR},
  author       = {Tingting Xie and Hui Chen and Wanquan Liu and Rongyu Zhou and Qilin Li},
  doi          = {10.1016/j.patcog.2024.110589},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110589},
  shortjournal = {Pattern Recognition},
  title        = {3D surface segmentation from point clouds via quadric fits based on DBSCAN clustering},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype learning based generic multiple object tracking
via point-to-box supervision. <em>PR</em>, <em>154</em>, 110588. (<a
href="https://doi.org/10.1016/j.patcog.2024.110588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generic multiple object tracking aims to recover the trajectories for generic moving objects of the same category. This task relies on the ability of effectively extracting representative features of the target objects. To this end, we propose a novel prototype learning based model, PLGMOT, that can explore the template features of an exemplar object and extend to more objects to acquire their prototype. Their prototype features can be continuously updated during the video, in favor of generalization to all the target objects with different appearances. More importantly, on the public benchmark GMOT-40, our method achieves more than 14% advantage over the state-of-the-art methods, with less than 0.5% of the training data that is not even completely annotated in the form of bounding boxes, thanks to our proposed point-to-box label refinement training algorithm and hierarchical motion-aware association algorithm.},
  archive      = {J_PR},
  author       = {Wenxi Liu and Yuhao Lin and Qi Li and Yinhua She and Yuanlong Yu and Jia Pan and Jason Gu},
  doi          = {10.1016/j.patcog.2024.110588},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110588},
  shortjournal = {Pattern Recognition},
  title        = {Prototype learning based generic multiple object tracking via point-to-box supervision},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint variational inference network for domain
generalization. <em>PR</em>, <em>154</em>, 110587. (<a
href="https://doi.org/10.1016/j.patcog.2024.110587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization is a machine learning task that involves training a model on a set of domains with the goal of achieving high performance on unseen domains. While most domain generalization methods focus on extracting domain-invariant features, they may ignore domain-specific information beyond object styles which is label-relevant for classification. In this paper, we first propose a two-step data generation process in which the domain label and observed data are sampled from two distributions sequentially, and accordingly develop an end-to-end Joint Variational Inference Network (JVINet) for domain generalization. JVINet is a framework that admits a variational-based discriminative structure, which the domain-specific latent variable is learned and integrated to enhance the feature for classification. When testing on unseen target domains, the potential beneficial domain information is hence utilized to improve generalization ability. The overall objective is to optimize the variational lower bound of the conditional joint likelihood functions for both class and domain labels. We provide theoretical proof that JVINet can achieve an optimal lower bound using a variational-based discriminative approach. To evaluate the effectiveness of our method, we compare it with state-of-the-art methods on both simulated and real-world datasets.},
  archive      = {J_PR},
  author       = {Jun-Zheng Chu and Bin Pan and Xia Xu and Tian-Yang Shi and Zhen-Wei Shi and Tao Li},
  doi          = {10.1016/j.patcog.2024.110587},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110587},
  shortjournal = {Pattern Recognition},
  title        = {Joint variational inference network for domain generalization},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale collaborative representation for face recognition
via class-information fusion. <em>PR</em>, <em>154</em>, 110586. (<a
href="https://doi.org/10.1016/j.patcog.2024.110586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most challenging issues in face recognition is having only a limited number of training images. Multiscale patch collaborative representation (MSPCRC) is an effective approach to address this problem. However, the existing MSPCRC methods only defined a single patch-scale weight vector for all classes to indicate the importance of different patch scales, ignoring the role of class information when fusing multiscale recognition results. In this work, we consider the effect of class information on face recognition and propose a novel multiscale collaborative-representation face recognition method. Specifically, we first construct the multiscale decision matrices of image subsets from different classes according to patch collaborative representation, and define a patch-scale weight vector for each class to describe the importance of different patch scales. Each element in a scale-weight vector represents the weight value of a certain scale in the corresponding class. Then, we construct the respective optimization objective function for each class, which takes into account the classification information both from the same class and from different classes. Finally, we propose a multiscale fusion-recognition output rule based on the patch-weight vectors. Experimental results demonstrate that the proposed method enhances classification accuracy by approximately 2% to 5% across multiple datasets, surpassing the majority of the competing methods.},
  archive      = {J_PR},
  author       = {Changzhong Wang and Shibing Pei and Xiang Lv and Weiping Ding},
  doi          = {10.1016/j.patcog.2024.110586},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110586},
  shortjournal = {Pattern Recognition},
  title        = {Multiscale collaborative representation for face recognition via class-information fusion},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LGNet: Local and global point dependency network for 3D
object detection. <em>PR</em>, <em>154</em>, 110585. (<a
href="https://doi.org/10.1016/j.patcog.2024.110585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection is a challenging task in autonomous driving industry scenarios. Many pre-existing methods employ the set-abstraction operation for generating key-point representations, which, however, cannot learn the long-range context dependency properly. In addition, the pooling operator, which only focuses on maximum channel response, is adopted to aggregate features of neighbor points without semantic information. To fix these issues, we propose LGNet, a new framework that simultaneously captures local and global point dependencies for enhancing 3D object detection. Specifically, we first introduce a new local point-graph pooling module to compute point-to-point correlations in a local region and aggregate features from neighboring points. To further capture the long-range dependency in a global context, we devised a global point-aware module to integrate local and global features at higher resolution. Experiments on the KITTI 3D detection dataset and Waymo Open Dataset benchmark show that LGNet achieves state-of-the-art performance in multiple classes. We will upload the code on https://github.com/MWPony/LGNet .},
  archive      = {J_PR},
  author       = {Jianwei Ma and Yan Huang and Cheng Qian and Jian Kang and Jiang Liu and Hui Zhang and Wei Hong},
  doi          = {10.1016/j.patcog.2024.110585},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110585},
  shortjournal = {Pattern Recognition},
  title        = {LGNet: Local and global point dependency network for 3D object detection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emotion-aware hierarchical interaction network for
multimodal image aesthetics assessment. <em>PR</em>, <em>154</em>,
110584. (<a href="https://doi.org/10.1016/j.patcog.2024.110584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetics assessment (IAA) has attracted increasing attention recently but is still challenging due to its high abstraction and complexity. Intuitively, image emotion and aesthetics are both human subjective feelings evoked by visual content. Previous researches have demonstrated that image aesthetics has intrinsic relationships with emotion. In fact, human emotional experience has potential impact on the perception of image aesthetics. Therefore, emotion information can be exploited to enhance aesthetic representation learning. Inspired by this, this paper presents an Emotion-Aware Hierarchical Interaction NETwork (EAHI-NET) for multimodal image aesthetics assessment, which explores both intra-modal and inter-modal interactions between aesthetics and emotions hierarchically. Specifically, we first propose the intra-modal emotion-aesthetics interaction module to obtain emotion-enhanced visual and textual aesthetic representations respectively. Then we propose the inter-modal feature enhancement to obtain the cross-modal aesthetic and emotion features. Finally, we design the inter-modal emotion-aesthetics interaction module to further investigate the cross-modal interplay between aesthetics and emotion, based on which hierarchical feature representations are achieved for multimodal IAA. The extensive experiments show that the proposed method can outperform the state-of-the-arts on multimodal AVA and Photo.net datasets.},
  archive      = {J_PR},
  author       = {Tong Zhu and Leida Li and Pengfei Chen and Jinjian Wu and Yuzhe Yang and Yaqian Li},
  doi          = {10.1016/j.patcog.2024.110584},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110584},
  shortjournal = {Pattern Recognition},
  title        = {Emotion-aware hierarchical interaction network for multimodal image aesthetics assessment},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative label correlation based robust structure
learning for multi-label feature selection. <em>PR</em>, <em>154</em>,
110583. (<a href="https://doi.org/10.1016/j.patcog.2024.110583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a key technique to tackle the curse of dimensionality in multi-label learning. Lots of embedded multi-label feature selection methods have been developed. However, they face challenges in identifying and excluding redundant features. To address these issues, this paper proposes a multi-label feature selection method that combines robust structural learning and discriminative label regularization. The proposed method starts from the feature space rather than data space, motivated by the principle that redundant features have high similarity or strong correlation. To exclude redundant features, a regularization on the feature selection matrix is designed by combining ℓ 2 , 1 ℓ2,1 -norm penalty with inner products of feature weight vectors. This regularization can help to learn a robust structure in the feature selection matrix. Meanwhile, both of the similarity and dissimilarity of labels of instances are involved in exploring discriminative label correlations. Extensive experiments verified the effectiveness of the proposed model for feature selection.},
  archive      = {J_PR},
  author       = {Qingwei Jia and Tingquan Deng and Yan Wang and Changzhong Wang},
  doi          = {10.1016/j.patcog.2024.110583},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110583},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative label correlation based robust structure learning for multi-label feature selection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). CRTrack: Learning correlation-refine network for visual
object tracking. <em>PR</em>, <em>154</em>, 110582. (<a
href="https://doi.org/10.1016/j.patcog.2024.110582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conducting reliable feature interaction plays a critical role in the visual tracking community, especially in recent dominated Siamese-based tracking paradigm. In general, there are two primary approaches for fusing representations from template and search area in the Siamese setting, i.e. , cross-correlation and transformer modeling. The former provides a straightforward interaction solution, which may have limitations in handling complex scenarios, such as appearance variations and occlusion. While the latter offers an effective interaction mechanism, albeit with higher computation complexity and model cost. In contrast to traditional Siamese-based trackers which rely on two mentioned feature cross-correlation operators, this paper proposes a novel Correlation-Refine network to address the issue of lacking semantic information caused by local linear matching in correlation, from both spatial and channel perspectives. Correlation-Refine network (named CR) is solely built on top of fully convolutional layers, without employing intricate transformer mechanisms or complex methods to fuse features from multiple scales. Moreover, we present a concise yet effective convolutional tracking framework based on the correlation-refine network. CR network can increase the discriminative ability of semantic information in a coarse-to-fine manner: it gradually learns the semantic features of the target to be tracked and suppresses interference from similar objects by stacking multiple CR layers. Extensive experiments and comparisons with recent competitive trackers in challenging large-scale benchmarks demonstrate that, our tracker outperforms all previous convolutional trackers and has competitive results with transformer-based method. The code will be made available.},
  archive      = {J_PR},
  author       = {Wenkang Zhang and Fei Xie and Tianyang Xu and Jiang Zhai and Wankou Yang},
  doi          = {10.1016/j.patcog.2024.110582},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110582},
  shortjournal = {Pattern Recognition},
  title        = {CRTrack: Learning correlation-refine network for visual object tracking},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble filter-transfer learning algorithm. <em>PR</em>,
<em>154</em>, 110581. (<a
href="https://doi.org/10.1016/j.patcog.2024.110581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning algorithms are capable to apply previously learned knowledge in source domain, which alleviates much expensive efforts of knowledge recollection in target domain. But the knowledge in source domain is always imperfect due to redundant or contaminated information. To solve this problem, an ensemble filter-transfer learning (EFTL) algorithm based on the source knowledge reconstruction is proposed in this paper. First, a knowledge partition strategy based on model is developed to classify the source knowledge into different types. Then, the positive knowledge can be identified, which contributes to target domain with a rejection of the negative transfer. Second, a knowledge filter algorithm is introduced to filter out the redundant information in non-positive knowledge. Then, the non-positive knowledge can be reconstructed by this algorithm to prevent the loss of available information. Third, an ensemble transfer mechanism is established to realize the synchronous transfer of omnidirectional knowledge for the target domain. Finally, comparative experiments on model prediction in practical applications are provided to illustrate the dependability of EFTL.},
  archive      = {J_PR},
  author       = {Honggui Han and Mengmeng Li and Hongyan Yang and Xiaolong Wu and Huayun Han},
  doi          = {10.1016/j.patcog.2024.110581},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110581},
  shortjournal = {Pattern Recognition},
  title        = {Ensemble filter-transfer learning algorithm},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection for multilabel classification with missing
labels via multi-scale fusion fuzzy uncertainty measures. <em>PR</em>,
<em>154</em>, 110580. (<a
href="https://doi.org/10.1016/j.patcog.2024.110580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous high-dimension multilabel data are generated, posing a challenge for multilabel learning. Building effective learning models with discriminative features is essential to improve the performance of multilabel learning. Multilabel feature selection can filter out the discriminative features according to their contribution to classification. However, ambiguity, uncertainty, and missing labels coexist in real-life multilabel data, which brings adverse effects to multilabel feature selection. The multi-scale fuzzy rough set gives an effective way to mine intrinsic knowledge hidden in uncertain data. This paper first extends the multi-scale learning to multilabel data with missing labels and proposes a feature selection method for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures called FSMML. The missing label space construction and feature evaluation metric are carefully investigated in the framework of multi-scale learning. A multilabel multi-scale learning strategy is formalized with the fuzzy granularity cognitive mechanism as the core, and the multi-scale fusion fuzzy label learning is given to reconstruct the missing label space. Then, a novel multilabel multi-scale fuzzy rough sets with missing labels is developed, and the significance of each scale is quantified. Moreover, some multi-scale fusion fuzzy uncertainty measures are defined by capturing the sample fuzzy similarity in the feature and reconstructed label spaces. Accordingly, the relevance between features and label set and the interactivity and redundancy between features in feature evaluation are discussed. Finally, FSMML chooses high-quality features to maximize relevance and interactivity and minimize redundancy. Extensive experiments demonstrate the effectiveness of FSMML on fifteen datasets with missing labels.},
  archive      = {J_PR},
  author       = {Tengyu Yin and Hongmei Chen and Zhihong Wang and Keyu Liu and Zhong Yuan and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.patcog.2024.110580},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110580},
  shortjournal = {Pattern Recognition},
  title        = {Feature selection for multilabel classification with missing labels via multi-scale fusion fuzzy uncertainty measures},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). WBNet: Weakly-supervised salient object detection via
scribble and pseudo-background priors. <em>PR</em>, <em>154</em>,
110579. (<a href="https://doi.org/10.1016/j.patcog.2024.110579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised salient object detection (WSOD) methods endeavor to boost sparse labels to get more salient cues in various ways. Among them, an effective approach is using pseudo labels from multiple unsupervised self-learning methods, but inaccurate and inconsistent pseudo labels could ultimately lead to detection performance degradation. To tackle this problem, we develop a new multi-source WSOD framework, WBNet, that can effectively utilize pseudo-background (non-salient region) labels combined with scribble labels to obtain more accurate salient features. We first design a comprehensive salient pseudo-mask generator from multiple self-learning features. Then, we pioneer the exploration of generating salient pseudo-labels via point-prompted and box-prompted Segment Anything Models (SAM). Then, WBNet leverages a pixel-level Feature Aggregation Module (FAM), a mask-level Transformer-decoder (TFD), and an auxiliary Boundary Prediction Module (EPM) with a hybrid loss function to handle complex saliency detection tasks. Comprehensively evaluated with state-of-the-art methods on five widely used datasets, the proposed method significantly improves saliency detection performance. The code and results are publicly available at https://github.com/yiwangtz/WBNet .},
  archive      = {J_PR},
  author       = {Yi Wang and Ruili Wang and Xiangjian He and Chi Lin and Tianzhu Wang and Qi Jia and Xin Fan},
  doi          = {10.1016/j.patcog.2024.110579},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110579},
  shortjournal = {Pattern Recognition},
  title        = {WBNet: Weakly-supervised salient object detection via scribble and pseudo-background priors},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NCART: Neural classification and regression tree for tabular
data. <em>PR</em>, <em>154</em>, 110578. (<a
href="https://doi.org/10.1016/j.patcog.2024.110578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models have become popular in the analysis of tabular data, as they address the limitations of decision trees and enable valuable applications like semi-supervised learning, online learning, and transfer learning. However, these deep-learning approaches often encounter a trade-off. On one hand, they can be computationally demanding when dealing with large-scale or high-dimensional datasets. On the other hand, they may lack interpretability and may not be suitable for small-scale datasets. In this study, we propose a novel interpretable neural network called Neural Classification and Regression Tree (NCART) to overcome these challenges. NCART is a modified version of Residual Networks that replaces fully-connected layers with multiple differentiable oblivious decision trees. By integrating decision trees into the architecture, NCART maintains its interpretability while benefiting from the end-to-end capabilities of neural networks. The simplicity of the NCART architecture makes it well-suited for datasets of varying sizes and reduces computational costs compared to state-of-the-art deep learning models. Extensive numerical experiments demonstrate the superior performance of NCART compared to existing deep learning models, establishing it as a strong competitor to tree-based models. The code is available at https://github.com/Luojiaqimath/NCART .},
  archive      = {J_PR},
  author       = {Jiaqi Luo and Shixin Xu},
  doi          = {10.1016/j.patcog.2024.110578},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110578},
  shortjournal = {Pattern Recognition},
  title        = {NCART: Neural classification and regression tree for tabular data},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A recurrent graph neural network for inductive
representation learning on dynamic graphs. <em>PR</em>, <em>154</em>,
110577. (<a href="https://doi.org/10.1016/j.patcog.2024.110577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning has recently garnered significant attention due to its wide applications in graph analysis tasks. It is well-known that real-world networks are dynamic, with edges and nodes evolving over time. This presents unique challenges that are distinct from those of static networks. However, most graph representation learning methods are either designed for static graphs, or address only partial challenges associated with dynamic graphs. They overlook the intricate interplay between topology and temporality in the evolution of dynamic graphs and the complexity of sequence modeling. Therefore, we propose a new dynamic graph representation learning model, called as R-GraphSAGE, which takes comprehensive considerations for embedding dynamic graphs. By incorporating a recurrent structure into GraphSAGE, the proposed R-GraphSAGE explores structural and temporal patterns integrally to capture more fine-grained evolving patterns of dynamic graphs. Additionally, it offers a lightweight architecture to decrease the computational costs for handling snapshot sequences, achieving a balance between performance and complexity. Moreover, it can inductively process the addition of new nodes and adapt to the situations without labels and node attributes. The performance of the proposed R-GraphSAGE is evaluated across various downstream tasks with both synthetic and real-world networks. The experimental results demonstrate that it outperforms state-of-the-art baselines by a significant margin in most cases.},
  archive      = {J_PR},
  author       = {Hong-Yu Yao and Chun-Yang Zhang and Zhi-Liang Yao and C.L. Philip Chen and Junfeng Hu},
  doi          = {10.1016/j.patcog.2024.110577},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110577},
  shortjournal = {Pattern Recognition},
  title        = {A recurrent graph neural network for inductive representation learning on dynamic graphs},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OA-pose: Occlusion-aware monocular 6-DoF object pose
estimation under geometry alignment for robot manipulation. <em>PR</em>,
<em>154</em>, 110576. (<a
href="https://doi.org/10.1016/j.patcog.2024.110576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation is the fundamental technology of robot manipulation systems. Recently, various learning-based monocular pose estimation methods have achieved outstanding performance by establishing sparse/dense 2D–3D correspondences. However, in cluttered environments, occlusion has been a challenging problem for pose estimation due to limited information provided by visible parts. In this work, we propose an efficient occlusion-aware monocular pose estimation method, called OA-Pose, to learn geometric feature information of occluded objects from cluttered scenes. Our framework takes RGB images as input and generates 2D–3D correspondences of visible and invisible parts based on the proposed Occlusion-aware Geometry Alignment Module. Extensive experiments show that our method is superior and competitive with state-of-the-art on multiple public datasets. We also conduct grasping experiments with different degrees of object occlusion, demonstrating the usability of our algorithm to deploy on robots in unstructured environments.},
  archive      = {J_PR},
  author       = {Jikun Wang and Luqing Luo and Weixiang Liang and Zhi-Xin Yang},
  doi          = {10.1016/j.patcog.2024.110576},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110576},
  shortjournal = {Pattern Recognition},
  title        = {OA-pose: Occlusion-aware monocular 6-DoF object pose estimation under geometry alignment for robot manipulation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Color matching in the wild. <em>PR</em>, <em>154</em>,
110575. (<a href="https://doi.org/10.1016/j.patcog.2024.110575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method that, given two different views of the same scene taken by two cameras with unknown settings and internal parameters, corrects the colors of one of the images making it look as if it was captured under the other camera settings. Our method is able to deal with any standard non-linear encoded images (gamma-corrected, logarithmic-encoded, or any other) without requiring any previous knowledge of the encoding. To this end, our method makes use of two important observations. First, the camera imaging pipeline from RAW to sRGB can be well approximated by considering just a per-pixel shading and a color transformation matrix, and second, for correcting the images we only need to estimate a single matrix –that will contain information from both of the original images– and an approximation of the shading term (that emulates the non-linearity). Our proposed method is fast and the results have no spurious artifacts. The method outperforms the state-of-the-art when compared with other methods that do not require knowledge of the encoding used. It is also able to compete with –and even surpass in some cases– methods that consider information about image encoding.},
  archive      = {J_PR},
  author       = {Raquel Gil Rodríguez and Javier Vazquez-Corral and Marcelo Bertalmío and Graham D. Finlayson},
  doi          = {10.1016/j.patcog.2024.110575},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110575},
  shortjournal = {Pattern Recognition},
  title        = {Color matching in the wild},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PIPE: Parallelized inference through ensembling of residual
quantization expansions. <em>PR</em>, <em>154</em>, 110571. (<a
href="https://doi.org/10.1016/j.patcog.2024.110571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are ubiquitous in computer vision and natural language processing, but suffer from high inference cost. This problem can be addressed by quantization, which consists in converting floating point operations into a lower bit-width format. With the growing concerns on privacy rights, we focus our efforts on data-free methods. However, such techniques suffer from their lack of adaptability to the target devices, as a hardware typically only support specific bit widths. Thus, to adapt to a variety of devices, a quantization method shall be flexible enough to find good accuracy v.s. speed trade-offs for every bit width and target device. To achieve this, we propose PIPE , a quantization method that leverages residual error expansion, along with group sparsity and an ensemble approximation for better parallelization. PIPE is backed off by strong theoretical guarantees and achieves superior performance on every benchmarked application (from vision to NLP tasks), architecture (ConvNets, transformers) and bit-width (from int8 to ternary quantization).},
  archive      = {J_PR},
  author       = {Edouard Yvinec and Arnaud Dapogny and Kevin Bailly},
  doi          = {10.1016/j.patcog.2024.110571},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110571},
  shortjournal = {Pattern Recognition},
  title        = {PIPE: Parallelized inference through ensembling of residual quantization expansions},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDDMM-face: Large deformation diffeomorphic metric learning
for cross-annotation face alignment. <em>PR</em>, <em>154</em>, 110569.
(<a href="https://doi.org/10.1016/j.patcog.2024.110569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an innovative, flexible, and consistent cross-annotation face alignment framework, LDDMM-Face, the key contribution of which is a deformation layer that naturally embeds facial geometry in a diffeomorphic way. This enables and solves cross-annotation face alignment tasks that were impossible in the existing works. Instead of predicting facial landmarks via a heatmap or coordinate regression, we formulate the face alignment task in a diffeomorphic registration manner and predict momenta that uniquely parameterize the deformation between the initial boundary and true boundary. We then perform large deformation diffeomorphic metric mapping (LDDMM) simultaneously for curve and landmark to localize the facial landmarks. The novel embedding of LDDMM into a deep network allows LDDMM-Face to consistently annotate facial landmarks without ambiguity and flexibly handle various annotation schemes, and can even predict dense annotations from sparse ones. To the best of our knowledge, this is the first study to leverage learning-based diffeomorphic mapping for face alignment. Our method can be easily integrated into various face alignment networks. We extensively evaluate LDDMM-Face on five benchmark datasets: 300W, WFLW, HELEN, COFW-68, and AFLW. LDDMM-Face distinguishes itself with outstanding performance when dealing with within-dataset cross-annotation learning (sparse-to-dense) and cross-dataset learning (different training and testing datasets). In addition, LDDMM-Face shows promising results on the most challenging task of cross-dataset cross-annotation learning (different training and testing datasets with different annotations). Our codes are available at https://github.com/CRazorback/LDDMM-Face .},
  archive      = {J_PR},
  author       = {Huilin Yang and Junyan Lyu and Pujin Cheng and Roger Tam and Xiaoying Tang},
  doi          = {10.1016/j.patcog.2024.110569},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110569},
  shortjournal = {Pattern Recognition},
  title        = {LDDMM-face: Large deformation diffeomorphic metric learning for cross-annotation face alignment},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a complex network representation for shape
classification. <em>PR</em>, <em>154</em>, 110566. (<a
href="https://doi.org/10.1016/j.patcog.2024.110566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape contour is a key low-level characteristic, making shape description an important aspect in many computer vision problems, with several challenges such as variations in scale, rotation, and noise. In this paper, we introduce an approach for shape analysis and classification from binary images based on representations learned by applying Randomized Neural Networks (RNNs) on feature maps derived from a Complex Network (CN) framework. Our approach models the contour in a complex network and computes their topological measures using a dynamic evolution strategy. This evolution of the CN provides significant information into the physical aspects of the shape’s contour. Therefore, we propose embedding the topological measures computed from the dynamics of the CN evolution into a matrix representation, which we have named the Topological Feature Map (TFM). Then, we employ the RNN to learn representations from the TFM through a sliding window strategy. The proposed representation is formed by the learned weights between the hidden and output layers of the RNN. Our experimental results show performance improvements in shape classification using the proposed method across two generic shape datasets. We also applied our approach to the recognition of plant leaves, achieving high performance in this challenging task. Furthermore, the proposed approach has demonstrated robustness to noise and invariance to transformations in scale and orientation of the shapes.},
  archive      = {J_PR},
  author       = {Lucas C. Ribas and Odemir M. Bruno},
  doi          = {10.1016/j.patcog.2024.110566},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110566},
  shortjournal = {Pattern Recognition},
  title        = {Learning a complex network representation for shape classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSAR-MVS: Textureless-aware segmentation and correlative
refinement guided multi-view stereo. <em>PR</em>, <em>154</em>, 110565.
(<a href="https://doi.org/10.1016/j.patcog.2024.110565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of textureless areas has long been a challenging problem in MVS due to lack of reliable pixel correspondences between images. In this paper, we propose the Textureless-aware Segmentation And Correlative Refinement guided Multi-View Stereo (TSAR-MVS), a novel method that effectively tackles challenges posed by textureless areas in 3D reconstruction through filtering, refinement and segmentation. First, we implement the joint hypothesis filtering, a technique that merges a confidence estimator with a disparity discontinuity detector to eliminate incorrect depth estimations. Second, to spread the pixels with confident depth, we introduce an iterative correlation refinement strategy that leverages RANSAC to generate 3D planes based on superpixels, succeeded by a weighted median filter for broadening the influence of accurately determined pixels. Finally, we present a textureless-aware segmentation method that leverages edge detection and line detection for accurately identify large textureless regions for further depth completion. Experiments on ETH3D, Tanks &amp; Temples and Strecha datasets demonstrate the superior performance and strong generalization capability of our proposed method.},
  archive      = {J_PR},
  author       = {Zhenlong Yuan and Jiakai Cao and Zhaoqi Wang and Zhaoxin Li},
  doi          = {10.1016/j.patcog.2024.110565},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110565},
  shortjournal = {Pattern Recognition},
  title        = {TSAR-MVS: Textureless-aware segmentation and correlative refinement guided multi-view stereo},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative brightening and amplification of low-light
imagery via bi-level adversarial learning. <em>PR</em>, <em>154</em>,
110558. (<a href="https://doi.org/10.1016/j.patcog.2024.110558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Poor light conditions constrain the high pursuit of clarity and visible quality of photography especially smartphone devices. Admittedly, existing specific image processing methods, whether super-resolution methods or low-light enhancement methods, can hardly simultaneously enhance the resolution and brightness of low-light images at the same time. This paper dedicates a specialized enhancer with a dual-path modulated-interactive structure to recover high-quality sharp images in conditions of near absence of light, dubbed CollaBA , which learns the direct mapping from low-resolution dark-light images to their high-resolution normal sharp version. Specifically, we construct the generative modulation prior, serving as illuminance attention information, to regulate the exposure level of the neighborhood range. In addition, we construct an interactive degradation removal branch that progressively embeds the generated intrinsic prior to recover high-frequency detail and contrast at the feature level. We also introduce a multi-substrate up-scaler to integrate multi-scale sampling features, effectively addressing artifact-related problems. Rather than adopting the naive time-consuming learning strategy, we design a novel bi-level implicit adversarial learning mechanism as our fast training strategy. Extensive experiments on benchmark datasets — demonstrate our model’s wide-ranging applicability in various ultra-low-light scenarios, across 8 key performance metrics with significant improvements, notably achieving a 35.8% improvement in LPIPS and a 23.1% increase in RMSE. The code will be available at https://github.com/moriyaya/CollaBA .},
  archive      = {J_PR},
  author       = {Jiaxin Gao and Yaohua Liu and Ziyu Yue and Xin Fan and Risheng Liu},
  doi          = {10.1016/j.patcog.2024.110558},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110558},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative brightening and amplification of low-light imagery via bi-level adversarial learning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel non-pretrained deep supervision network for polyp
segmentation. <em>PR</em>, <em>154</em>, 110554. (<a
href="https://doi.org/10.1016/j.patcog.2024.110554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a non-pretrained deep supervision network (NPD-Net) for polyp segmentation. Unlike previous deep supervision networks that rely on ground truth (GT) or pre-training with GT to supervise deep features(the prediction maps from decoder), we propose a novel deep supervision strategy that directly utilizes the GT encoder (that encodes GT to get its maps) after initialization to mitigate overfitting and enhance generalization ability without pre-training, in other words, a non-pretrained. This strategy makes up the gap of directly using GT for deep supervision while mitigates the risk of overfitting due to leverage the well-train pre-trained weights on a small polyp datasets. In addition, we introduce a simple and efficient parallel dual attention module (PDA) to enhance the global modeling ability. PDA executes spatial and channel attention in parallel, and adopts implicit positional encoding and transpose operation to reduce computational complexity. Finally, NPD-Net is able to effectively supervise deep features, expand the range of context information acquisition and improve segmentation performance, particularly in terms of generalization ability. Our experimental results on five benchmark datasets demonstrate that NPD-Net outperforms other state-of-the-art methods. The code will be available at https://github.com/guobaoxiao/NPD-Net .},
  archive      = {J_PR},
  author       = {Zhenni Yu and Li Zhao and Tangfei Liao and Xiaoqin Zhang and Geng Chen and Guobao Xiao},
  doi          = {10.1016/j.patcog.2024.110554},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110554},
  shortjournal = {Pattern Recognition},
  title        = {A novel non-pretrained deep supervision network for polyp segmentation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Observation weights matching approach for causal inference.
<em>PR</em>, <em>154</em>, 110549. (<a
href="https://doi.org/10.1016/j.patcog.2024.110549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel method integrating pattern recognition models with causal inference methodologies to adeptly identify and manage overlapping regions between treatment and control groups. Our approach, Observation Weights Matching (OWM), addresses the intrinsic challenges in observational studies—specifically, the fixed sample size and the lack of complete overlap in pretreatment variables. Through ensemble learning, OWM effectively retains examples within these critical overlapping regions, systematically generating weighted data distributions that aid in the precise identification of these instances. By prioritizing hard-to-classify observations and employing a novel metric of critical values for matched samples, our approach optimizes matching performance and provides greater robustness in causal analysis. Through empirical and simulation studies, we demonstrate OWM&#39;s notable advantage over traditional matching methods, enhancing causal inference in observational research. Furthermore, we show that OWM provides richer balance scores than propensity scores, ensuring unbiased estimations and advancing the field significantly.},
  archive      = {J_PR},
  author       = {Kangbok Lee and Sumin Han and Hyeoncheol Baik and Yeasung Jeong and Young Woong Park},
  doi          = {10.1016/j.patcog.2024.110549},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110549},
  shortjournal = {Pattern Recognition},
  title        = {Observation weights matching approach for causal inference},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-cost orthogonal basis-core extraction for classification
and reconstruction using tensor ring. <em>PR</em>, <em>154</em>, 110548.
(<a href="https://doi.org/10.1016/j.patcog.2024.110548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor based methods have gained popularity for being able to represent multi-aspect real world data in a lower dimensional space. Among them, methods with orthogonal factors perform relatively better in classification. However, most of them cannot handle higher order data. Recently, Tensor Ring (TR) based methods are proposed to combat with the higher order issue more effectively focusing on both classification and reconstruction. A TR-based method with orthogonal cores performs reasonably well for a given error. However, its computational complexity is very high and might produce extra features. To solve these issues, in this paper, we propose a method named as Orthogonal basis-core extraction using Tensor Ring (OTR) that can facilitate better discrimination and reconstruction at a lower cost. To maintain the ring property, we also show, theoretically, that reshaping of the product of semi-orthogonal reshaped cores remains semi-orthogonal. Rigorous experiments over eighteen benchmark datasets from different fields demonstrate the superiority of OTR over state-of-the-art methods in terms of classification and reconstruction.},
  archive      = {J_PR},
  author       = {Suravi Akhter and Muhammad Mahbub Alam and Md. Shariful Islam and M. Arshad Momen and Md. Shariful Islam and Mohammad Shoyaib},
  doi          = {10.1016/j.patcog.2024.110548},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110548},
  shortjournal = {Pattern Recognition},
  title        = {Low-cost orthogonal basis-core extraction for classification and reconstruction using tensor ring},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blessing few-shot segmentation via semi-supervised learning
with noisy support images. <em>PR</em>, <em>154</em>, 110503. (<a
href="https://doi.org/10.1016/j.patcog.2024.110503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream few-shot segmentation methods meet performance bottleneck due to the data scarcity of novel classes with insufficient intra-class variations, which results in a biased model primarily favoring the base classes. Fortunately, owing to the evolution of the Internet, an extensive repository of unlabeled images has become accessible from diverse sources such as search engines and publicly available datasets. However, such unlabeled images are not a free lunch. There are noisy inter-class and intra-class samples causing severe feature bias and performance degradation. Therefore, we propose a semi-supervised few-shot segmentation framework named F4S , which incorporates a ranking algorithm designed to eliminate noisy samples and select superior pseudo-labeled images, thereby fostering the improvement of few-shot segmentation within a semi-supervised paradigm. The proposed F4S framework can not only enrich the intra-class variations of novel classes during the test phase, but also enhance meta-learning of the network during the training phase. Furthermore, it can be readily implemented with ease on any off-the-shelf few-shot segmentation methods. Additionally, based on a Structural Causal Model (SCM), we further theoretically explain why the proposed method can solve the noise problem: the severe noise effects are removed by cutting off the backdoor path between pseudo labels and noisy support images via causal intervention. On PASCAL-5 i i and COCO-20 i i datasets, we show that the proposed F4S can boost various popular few-shot segmentation methods to new state-of-the-art performances.},
  archive      = {J_PR},
  author       = {Runtong Zhang and Hongyuan Zhu and Hanwang Zhang and Chen Gong and Joey Tianyi Zhou and Fanman Meng},
  doi          = {10.1016/j.patcog.2024.110503},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110503},
  shortjournal = {Pattern Recognition},
  title        = {Blessing few-shot segmentation via semi-supervised learning with noisy support images},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GANN: Graph alignment neural network for semi-supervised
learning. <em>PR</em>, <em>154</em>, 110484. (<a
href="https://doi.org/10.1016/j.patcog.2024.110484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have been widely investigated in the field of semi-supervised graph machine learning. Most methods fail to exploit adequate graph information when labeled data is limited, leading to the problem of oversmoothing. To overcome this issue, we propose the Graph Alignment Neural Network (GANN), a simple and effective graph neural architecture. A unique learning algorithm with three alignment rules is proposed to thoroughly explore hidden information for insufficient labels. Firstly, to better investigate attribute specifics, we suggest the feature alignment rule to align the inner product of both the attribute and embedding matrices. Secondly, to properly utilize the higher-order neighbor information, we propose the cluster center alignment rule, which involves aligning the inner product of the cluster center matrix with the unit matrix. Finally, to get reliable prediction results with few labels, we establish the minimum entropy alignment rule by lining up the prediction probability matrix with its sharpened result. Extensive studies on graph benchmark datasets demonstrate that GANN can achieve considerable benefits in semi-supervised node classification and outperform state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Linxuan Song and Wenxuan Tu and Sihang Zhou and En Zhu},
  doi          = {10.1016/j.patcog.2024.110484},
  journal      = {Pattern Recognition},
  month        = {10},
  pages        = {110484},
  shortjournal = {Pattern Recognition},
  title        = {GANN: Graph alignment neural network for semi-supervised learning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked face recognition using domain adaptation.
<em>PR</em>, <em>153</em>, 110574. (<a
href="https://doi.org/10.1016/j.patcog.2024.110574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearing facial masks has become a must in our daily life due to the global COVID-19 pandemic. However, the performance of a face recognition system is severely degraded due to the fact that the face images in the gallery are unmasked faces while the probe face images captured by the camera are masked faces, making the probe face images different from gallery face images in the activated region and the distribution domain. In this paper, we propose a novel face recognition system to address the issue. The system is integrated with a domain adaptation layer and a feature refinement layer. The feature refinement layer is based on the structure of the self-attention mechanism to align activated regions of unmasked faces with those of masked faces. The domain adaptation layer works by adapting the system from the unmasked face domain to the synthetically masked face domain and the real- world masked face domain. The system is tested on real-world data through face verification and face identification. The face verification accuracy is improved by 6.83% for the RMFD_FV dataset and 4.2% for the MFR2 dataset, and the face identification accuracy is improved by 15.43% for the MFRFI dataset.},
  archive      = {J_PR},
  author       = {Yu-Chieh Huang and David Akas Bedjo Rahardjo and Ren-Hau Shiue and Homer H. Chen},
  doi          = {10.1016/j.patcog.2024.110574},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110574},
  shortjournal = {Pattern Recognition},
  title        = {Masked face recognition using domain adaptation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UVMO: Deep unsupervised visual reconstruction-based
multimodal-assisted odometry. <em>PR</em>, <em>153</em>, 110573. (<a
href="https://doi.org/10.1016/j.patcog.2024.110573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, unsupervised visual odometry (VO) based on visual reconstruction has attracted lots of attention due to its end-to-end pose estimation approach and the advantage of not requiring real labels for training. Unsupervised VO inputs monocular video frames into a pose estimation network to output the predicted poses, and optimizes the pose prediction by minimizing visual reconstruction loss with epipolar geometry constraint. However, lack of depth information and complex environments such as rapid turns and uneven lighting in monocular video frames can result in insufficient visual information for pose estimation. Additionally, dynamic objects and discontinuous occlusions in monocular video frames can introduce inappropriate errors in visual reconstruction. In this paper, an U U nsupervised V V isual reconstruction-based M M ultimodal-assisted O O dometry (UVMO) is proposed. UVMO leverages inertial and lidar information to complement visual information to acquire more accurate pose estimation. Specifically, a triple-modal fusion strategy called SMPF is proposed to conduct a more comprehensive and stable fusion of the three modalities’ data. Additionally, an image-based mask is introduced to filter out the dynamic occlusion regions in video frames, improving the accuracy of visual reconstruction. To the best of our knowledge, this paper is the first to propose a pure deep learning-based visual-inertial-lidar odometry. Experiments show that UVMO achieves state-of-the-art performance among pure deep learning-based unsupervised odometry.},
  archive      = {J_PR},
  author       = {Songrui Han and Mingchi Li and Hongying Tang and Yaozhe Song and Guanjun Tong},
  doi          = {10.1016/j.patcog.2024.110573},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110573},
  shortjournal = {Pattern Recognition},
  title        = {UVMO: Deep unsupervised visual reconstruction-based multimodal-assisted odometry},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust image matching in low-luminance environments:
Self-supervised keypoint detection and descriptor-free cross-fusion
matching. <em>PR</em>, <em>153</em>, 110572. (<a
href="https://doi.org/10.1016/j.patcog.2024.110572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image keypoint detection and feature matching are fundamental steps in computer vision tasks. However, variations in environment, time, and viewpoint pose a challenge to the stability of image keypoint detection and matching. Most traditional and deep learning-based methods cannot accurately and efficiently extract highly repeatable keypoints and robust match pairs in low-luminance environments. Therefore, we propose a two-step ‘detection + matching’ framework, which consists of deep neural networks in each step. Firstly, we design a self-supervised robust keypoint detection network, which utilizes multi-scale, multi-angle, and multi-luminance transformation techniques to create pseudo-labeled datasets to improve the model’s keypoint detection repeatability and luminance invariance. Secondly, we propose a descriptor-free cross-fusion matching network, which uses the cross-fusion attention mechanism to establish connections between keypoint-centered image patches and converts the feature-matching task into an image patch assignment task to improve the accuracy and efficiency of matching. Thirdly, the proposed framework is used to replace traditional SIFT in SfM. Experimental results on testing datasets show that the self-supervised robust keypoint detection network achieves higher keypoint repeatability in low luminance environments compared to SIFT, ORB, LIFT, and Superpoint. The descriptor-free cross-fusion matching network’s mean matching accuracy and efficiency are higher than the mainstream Superglue algorithm. Also, SfM achieves better performance regarding the number of sparse point clouds and accuracy.},
  archive      = {J_PR},
  author       = {Sikang Liu and Yida Wei and Zhichao Wen and Xueli Guo and Zhigang Tu and You Li},
  doi          = {10.1016/j.patcog.2024.110572},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110572},
  shortjournal = {Pattern Recognition},
  title        = {Towards robust image matching in low-luminance environments: Self-supervised keypoint detection and descriptor-free cross-fusion matching},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OpenInst: A simple query-based method for open-world
instance segmentation. <em>PR</em>, <em>153</em>, 110570. (<a
href="https://doi.org/10.1016/j.patcog.2024.110570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-world instance segmentation has recently gained significant popularity due to its importance in many real-world applications, such as autonomous driving, robot perception, and remote sensing. However, previous methods have either produced unsatisfactory results or relied on complex systems and paradigms. We wonder if there is a simple way to obtain state-of-the-art results. Fortunately, we have identified two observations that help us achieve the best of both worlds: (1) query-based methods demonstrate superiority over dense proposal-based methods in open-world instance segmentation, and (2) learning localization cues is sufficient for open-world instance segmentation. Based on these observations, we propose a simple query-based method named OpenInst for open-world instance segmentation. OpenInst leverages advanced query-based methods like QueryInst and focuses on learning localization cues. Notably, OpenInst is an extremely simple and straightforward framework without any auxiliary modules or post-processing, yet achieves state-of-the-art results on multiple benchmarks. Specifically, in the COCO → → UVO scenario, OpenInst achieves a mask Average Recall (AR) of 53.3, outperforming the previous best methods by 2.0 AR with a simpler structure. We hope that OpenInst can serve as a solid baseline for future research in this area. The source codes are available at https://github.com/hustvl/OpenInst .},
  archive      = {J_PR},
  author       = {Cheng Wang and Guoli Wang and Qian Zhang and Peng Guo and Wenyu Liu and Xinggang Wang},
  doi          = {10.1016/j.patcog.2024.110570},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110570},
  shortjournal = {Pattern Recognition},
  title        = {OpenInst: A simple query-based method for open-world instance segmentation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality-aware blind image motion deblurring. <em>PR</em>,
<em>153</em>, 110568. (<a
href="https://doi.org/10.1016/j.patcog.2024.110568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent mapping-based motion deblurring methods lack the regularization of prior knowledge, resulting in an over-reliance on the training data and limited generalization ability. As deblurring aims to improve image quality, we quantitatively analyze and further discover the strong correlation between image quality and sharpness. Motivated by the above facts and notable accomplishments of recent no-reference image quality assessment (NR-IQA), we present a novel framework that incorporates quality knowledge into mapping-based deblurring models. Specifically, we extract quality-aware features from NR-IQA models as prior knowledge, and subsequently propose a prediction-based strategy and an encoder-reuse strategy to integrate knowledge into the encoder and decoder, respectively. After training, the model can simultaneously deblur images and predict quality features, indicating that it has grasped the knowledge and validating the effectiveness of the proposed embedding strategies. Extensive experimental results show that embedding quality knowledge consistently improves model performance and the model achieves state-of-the-art intra/cross-dataset results. Code and pre-trained models are available at https://github.com/esnthere/QAMD .},
  archive      = {J_PR},
  author       = {Tianshu Song and Leida Li and Jinjian Wu and Weisheng Dong and Deqiang Cheng},
  doi          = {10.1016/j.patcog.2024.110568},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110568},
  shortjournal = {Pattern Recognition},
  title        = {Quality-aware blind image motion deblurring},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Focus on informative graphs! Semi-supervised active learning
for graph-level classification. <em>PR</em>, <em>153</em>, 110567. (<a
href="https://doi.org/10.1016/j.patcog.2024.110567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-level classification is a critical problem in social analysis and bioinformatics. Since annotated labels are typically costly, we intend to study this challenging task in semi-supervised scenarios with limited budgets. Inspired by the fact that active learning is capable of interactively querying an oracle to annotate a small number of informative examples in the unlabeled dataset, we develop a novel S emi-su p ervised a ctive learning framework termed GraphSpa for graph-level classification. To make the most of labeling budgets, we propose an effective unlabeled data selection strategy that takes both local similarity and global semantic structure into account. Specifically, we first construct an adaptive queue with labeled samples and select informative samples that have a low degree of similarity to the queue using the Min-Max principle from the local view. Further, we introduce class prototypes and select samples with a large predictive loss discrepancy from the global view. To harness the full potential of unlabeled data, we develop a semi-supervised active learning framework on the basis of our fusion selection strategy coupled with graph contrastive learning during active learning. The effectiveness of our GraphSpa is validated against state-of-the-art methods through experimental results on diverse real-world benchmark datasets.},
  archive      = {J_PR},
  author       = {Wei Ju and Zhengyang Mao and Ziyue Qiao and Yifang Qin and Siyu Yi and Zhiping Xiao and Xiao Luo and Yanjie Fu and Ming Zhang},
  doi          = {10.1016/j.patcog.2024.110567},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110567},
  shortjournal = {Pattern Recognition},
  title        = {Focus on informative graphs! semi-supervised active learning for graph-level classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the vulnerability of skeleton-based human
activity recognition via black-box attack. <em>PR</em>, <em>153</em>,
110564. (<a href="https://doi.org/10.1016/j.patcog.2024.110564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human Activity Recognition (HAR) has been employed in a wide range of applications, e.g. self-driving cars, where safety and lives are at stake. Recently, the robustness of skeleton-based HAR methods have been questioned due to their vulnerability to adversarial attacks. However, the proposed attacks require the full-knowledge of the attacked classifier, which is overly restrictive. In this paper, we show such threats indeed exist, even when the attacker only has access to the input/output of the model. To this end, we propose the very first black-box adversarial attack approach in skeleton-based HAR called BASAR. BASAR explores the interplay between the classification boundary and the natural motion manifold. To our best knowledge, this is the first time data manifold is introduced in adversarial attacks on time series. Via BASAR, we find on-manifold adversarial samples are extremely deceitful and rather common in skeletal motions, in contrast to the common belief that adversarial samples only exist off-manifold. Through exhaustive evaluation, we show that BASAR can deliver successful attacks across classifiers, datasets, and attack modes. By attack, BASAR helps identify the potential causes of the model vulnerability and provides insights on possible improvements. Finally, to mitigate the newly identified threat, we propose a new adversarial training approach by leveraging the sophisticated distributions of on/off-manifold adversarial samples, called mixed manifold-based adversarial training (MMAT). MMAT can successfully help defend against adversarial attacks without compromising classification accuracy.},
  archive      = {J_PR},
  author       = {Yunfeng Diao and He Wang and Tianjia Shao and Yongliang Yang and Kun Zhou and David Hogg and Meng Wang},
  doi          = {10.1016/j.patcog.2024.110564},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110564},
  shortjournal = {Pattern Recognition},
  title        = {Understanding the vulnerability of skeleton-based human activity recognition via black-box attack},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional image denoising with blurred image feature.
<em>PR</em>, <em>153</em>, 110563. (<a
href="https://doi.org/10.1016/j.patcog.2024.110563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising remains a classical yet still challenging problem, because the noise can destroy details and cause severe information loss. In recent years, various well-designed CNN-based methods have been extensively applied in image denoising because of the strong learning ability. However, most of them share an unidirectional procedure, which directly learns a mapping from noisy input to a clean image without focusing on the over-smoothed state of the denoising process, limiting the richness of extracted features. Different from previous works, we propose a blurred image feature guided CNN (BFCNN) network that implements a novel blurring-adjusting strategy to address the complex denoising problem via two stages. In stage 1, we build a blurring module (BM) to capture over-smoothed features from noisy observations and generate the blurred image restoration, which is a less informative version of the clean image. Furthermore, a multi-level concatenating module (CM) and an adjusting module (AM) are then designed to recover more detailed information in stage 2. These two modules are jointly designed to restore a properly-smoothed image from the over-smoothed blurred image and the given under-smoothed noisy image. Comparing to the traditional denoising process, the proposed blurring-adjusting strategy produces a precise denoised image more efficiently by converting the unidirectional denoising process into a bidirectional denoising process. To our knowledge, this is the first study that utilizes the over-smoothed image to address the denoising problem. Extensive experiments demonstrate the superiority of our BFCNN with more accurate reconstruction quality and achieve competitive quantitative results among current CNN-based methods. This research will release the code upon acceptance.},
  archive      = {J_PR},
  author       = {Linwei Fan and Xiaoyu Yan and Huiyu Li and Yongxia Zhang and Hui Liu and Caiming Zhang},
  doi          = {10.1016/j.patcog.2024.110563},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110563},
  shortjournal = {Pattern Recognition},
  title        = {Bidirectional image denoising with blurred image feature},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Granular3D: Delving into multi-granularity 3D scene graph
prediction. <em>PR</em>, <em>153</em>, 110562. (<a
href="https://doi.org/10.1016/j.patcog.2024.110562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the significant challenges in 3D Semantic Scene Graph (3DSSG) prediction, essential for understanding complex 3D environments. Traditional approaches, primarily using PointNet and Graph Convolutional Networks, struggle with effectively extracting multi-grained features from intricate 3D scenes, largely due to a focus on global scene processing and single-scale feature extraction. To overcome these limitations, we introduce Granular3D, a novel approach that shifts the focus towards multi-granularity analysis by predicting relation triplets from specific sub-scenes. One key is the Adaptive Instance Enveloping Method (AIEM), which establishes an approximate envelope structure around irregular instances, providing shape-adaptive local point cloud sampling, thereby comprehensively covering the contextual environments of instances. Moreover, Granular3D incorporates a Hierarchical Dual-Stage Network (HDSN), which differentiates and processes features of instances and their pairs at varying scales, leading to a targeted prediction of instance categories and their relationships. To advance the perception of sub-scene in HDSN, we design a Gather Point Transformer structure (GaPT) that enables the combinatorial interaction of local information from multiple point cloud sets, achieving a more comprehensive local contextual feature extraction. Extensive evaluations on the challenging 3DSSG benchmark demonstrate that our methods provide substantial improvements, establishing a new state-of-the-art in 3DSSG prediction, boosting the top-50 triplet accuracy by ＋2.8%.},
  archive      = {J_PR},
  author       = {Kaixiang Huang and Jingru Yang and Jin Wang and Shengfeng He and Zhan Wang and Haiyan He and Qifeng Zhang and Guodong Lu},
  doi          = {10.1016/j.patcog.2024.110562},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110562},
  shortjournal = {Pattern Recognition},
  title        = {Granular3D: Delving into multi-granularity 3D scene graph prediction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transductive zero-shot learning with generative model-driven
structure alignment. <em>PR</em>, <em>153</em>, 110561. (<a
href="https://doi.org/10.1016/j.patcog.2024.110561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) facilitates the transfer of knowledge from seen to unseen categories through high-dimensional vectors that capture both known and unknown class names. However it encounters challenges with domain shift arising from a lack of sufficient labeled data. Although transductive zero-shot learning (TZSL) addresses this bias by including samples from unseen classes, it still faces obstacles in enhancing TZSL performance. In this study, We introduce the Structure Alignment Variational Autoencoder Generative Adversarial Network (SA-VAEGAN), a novel approach that enhances the alignment between visual and auxiliary spaces. We delved into the underlying causes of domain shift and introduced a structural alignment (SA) strategy to tackle these challenges. The SA model thoroughly accounts for both inter-class and intra-class dynamics, designed to leverage the model’s comprehension of high-level semantic relations to disambiguate confusion among similar classes and mitigate intra-class confusion by penalizing atypical visual samples within classes. Assessed across four benchmark datasets, SA-VAEGAN has established a new performance standard, underscoring its efficiency in addressing the domain shift challenge within TZSL tasks, and achieving high accuracy.},
  archive      = {J_PR},
  author       = {Yang Liu and Keda Tao and Tianhui Tian and Xinbo Gao and Jungong Han and Ling Shao},
  doi          = {10.1016/j.patcog.2024.110561},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110561},
  shortjournal = {Pattern Recognition},
  title        = {Transductive zero-shot learning with generative model-driven structure alignment},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthetic unknown class learning for learning unknowns.
<em>PR</em>, <em>153</em>, 110560. (<a
href="https://doi.org/10.1016/j.patcog.2024.110560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the open set recognition (OSR) problem, where the goal is to correctly classify samples of known classes while detecting unknown samples to reject. In the OSR problem, “unknown” is assumed to have infinite possibilities because we have no knowledge about unknowns until they emerge. Intuitively, the more an OSR system explores the possibilities of unknowns, the more likely it is to detect unknowns. Even though several generative OSR models have been proposed to explore more by generating synthetic samples and learning them as unknowns, the generated samples are limited to a small subspace of the known classes. Thus, this paper proposes a novel synthetic unknown class learning method that constantly generates unknown-like samples while maintaining diversity between the generated samples. By learning the unknown-like samples and known samples in an alternating manner, the proposed method can not only experience diverse synthetic unknowns but also reduce overgeneralization with respect to known classes. Experiments on several benchmark datasets show that the proposed method significantly outperforms other state-of-the-art approaches by generating diverse realistic unknown samples.},
  archive      = {J_PR},
  author       = {Jaeyeon Jang},
  doi          = {10.1016/j.patcog.2024.110560},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110560},
  shortjournal = {Pattern Recognition},
  title        = {Synthetic unknown class learning for learning unknowns},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Ta-adapter: Enhancing few-shot CLIP with task-aware
encoders. <em>PR</em>, <em>153</em>, 110559. (<a
href="https://doi.org/10.1016/j.patcog.2024.110559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive Language-Image Pre-training (CLIP) has shown impressive zero-shot transfer capabilities, but its potential for specific downstream tasks is not fully utilized. To further enhance CLIP’s few-shot capability for specific datasets, some subsequent works have been proposed, such as methods based on lightweight adapters and prompt learning. However, since CLIP is pretrained on a diverse collection of image and text pairs sourced from the internet, it is difficult to sufficiently tune models to specific datasets using only lightweight adaptions. In this paper, we argue that largely modifying the internal representations within CLIP’s encoders can yield better results on downstream datasets. In this work, we introduce Ta-Adapter, a method that equips both the visual and textual encoders of CLIP with task-specific prompts. These prompts are generated using a collaborative prompt learning approach, which allows the encoders to produce representations that are better aligned with specific downstream datasets. Then, we initialize an adapter module using the optimized features generated by the task-aware visual encoder for further feature alignment, and this module can also be further fine-tuned. Our extensive experiments on image classification datasets show that compared to the state-of-the-art few-shot methods Tip-Adapter-F and MaPLe, our model exhibits good performance and obtains an average absolute gain of 2.04% and 1.62% on 11 different image recognition datasets, respectively. In conclusion, this work presents a unique and effective approach to unlocking the full potential of CLIP’s few-shot learning capabilities.},
  archive      = {J_PR},
  author       = {Wenbo Zhang and Yifan Zhang and Yuyang Deng and Wenlong Zhang and Jianfeng Lin and Binqiang Huang and Jinlu Zhang and Wenhao Yu},
  doi          = {10.1016/j.patcog.2024.110559},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110559},
  shortjournal = {Pattern Recognition},
  title        = {Ta-adapter: Enhancing few-shot CLIP with task-aware encoders},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite convolution: A flexible operator for deep learning
on 3D point clouds. <em>PR</em>, <em>153</em>, 110557. (<a
href="https://doi.org/10.1016/j.patcog.2024.110557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks require specific layers to process point clouds, as the scattered and irregular location of 3D points prevents the use of conventional convolutional filters. We introduce the composite layer, a flexible and general alternative to the existing convolutional operators that process 3D point clouds. We design our composite layer to extract and compress the spatial information from the 3D coordinates of points and then combine this with the feature vectors. Compared to mainstream point-convolutional layers such as ConvPoint and KPConv, our composite layer guarantees greater flexibility in network design and provides an additional form of regularization. To demonstrate the generality of our composite layers, we define both a convolutional composite layer and an aggregate version that combines spatial information and features in a nonlinear manner, and we use these layers to implement CompositeNets. Our experiments on synthetic and real-world datasets show that, in both classification, segmentation, and anomaly detection, our CompositeNets outperform ConvPoint, which uses the same sequential architecture, and achieve similar results as KPConv, which has a deeper, residual architecture. Moreover, our CompositeNets achieve state-of-the-art performance in anomaly detection on point clouds. Our code is publicly available at https://github.com/sirolf-otrebla/CompositeNet .},
  archive      = {J_PR},
  author       = {Alberto Floris and Luca Frittoli and Diego Carrera and Giacomo Boracchi},
  doi          = {10.1016/j.patcog.2024.110557},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110557},
  shortjournal = {Pattern Recognition},
  title        = {Composite convolution: A flexible operator for deep learning on 3D point clouds},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSTrans: Correlation-guided self-activation transformer for
counting everything. <em>PR</em>, <em>153</em>, 110556. (<a
href="https://doi.org/10.1016/j.patcog.2024.110556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counting everything, also named few-shot counting, requires a model to be able to count objects with any novel (unseen) category giving few exemplar boxes. However, the existing few-shot counting methods are sub-optimal due to weak feature representation, such as the correlation between the exemplar patch and query feature, and contextual dependencies in density map prediction. In this paper, we propose a very simple but effective method, CSTrans, consisting of a Correlation-guided Self-Activation (CSA) module and a Local Dependency Transformer (LDT) module, to mitigate the above two issues, respectively. The CSA utilizes the correlation map to activate the semantic features and suppress the noisy influence of the query features, aiming at mining the potential relation while enriching correlation representation. Furthermore, the LDT incorporates a Transformer to explore local contextual dependencies and predict the density map. Our method achieves competitive performance on FSC-147 and CARPK datasets. We hope its simple implementation and superior performance can serve as a new and strong baseline for few-shot counting tasks and attract more interest in designing simple but effective models in future studies. Our code for CSTrans is available at https://github.com/gaobb/CSTrans .},
  archive      = {J_PR},
  author       = {Bin-Bin Gao and Zhongyi Huang},
  doi          = {10.1016/j.patcog.2024.110556},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110556},
  shortjournal = {Pattern Recognition},
  title        = {CSTrans: Correlation-guided self-activation transformer for counting everything},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAST: Cross-modal retrieval and visual conditioning for
image captioning. <em>PR</em>, <em>153</em>, 110555. (<a
href="https://doi.org/10.1016/j.patcog.2024.110555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning requires not only accurate recognition of objects and corresponding relationships, but also full comprehension of the scene information. However, existing models suffer from partial understanding and object hallucination. In this paper, a Cross-modal retrievAl and viSual condiTioning model (CAST) is proposed to address the above issues for image captioning with three key modules: an image–text retriever, an image &amp; memory comprehender and a dual attention decoder. Aiming at a comprehensive understanding, we propose to exploit cross-modal retrieval to mimic human cognition, i.e. , to trigger retrieval of contextual information (called episodic memory) about a specific event. Specifically, the image–text retriever searches the top n relevant sentences which serve as episodic memory for each input image. Then the image &amp; memory comprehender encodes an input image and enriches episodic memory by self-attention and relevance attention respectively, which can encourage CAST to comprehend the scene thoroughly and support decoding more effectively. Finally, such image representation and memory are integrated into our dual attention decoder, which performs visual conditioning by re-weighting image and text features to alleviate object hallucination. Extensive experiments are conducted on MS COCO and Flickr30k datasets, which demonstrate that our CAST achieves state-of-the-art performance. Our model also has a promising performance even in low-resource scenarios ( i.e. 0.1%, 0.5% and 1% of MS COCO training set).},
  archive      = {J_PR},
  author       = {Shan Cao and Gaoyun An and Yigang Cen and Zhaoqilin Yang and Weisi Lin},
  doi          = {10.1016/j.patcog.2024.110555},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110555},
  shortjournal = {Pattern Recognition},
  title        = {CAST: Cross-modal retrieval and visual conditioning for image captioning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain tumor segmentation in MRI with multi-modality spatial
information enhancement and boundary shape correction. <em>PR</em>,
<em>153</em>, 110553. (<a
href="https://doi.org/10.1016/j.patcog.2024.110553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation is currently of a priori guiding significance in medical research and clinical diagnosis. Brain tumor segmentation techniques can accurately partition different tumor areas on multi-modality images captured by magnetic resonance imaging (MRI). Due to the unpredictable pathological process of brain tumor generation and growth, brain tumor images often show irregular shapes and uneven internal gray levels. Existing neural network-based segmentation methods with an encoding/decoding structure can perform image segmentation to some extent. However, they ignore issues such as differences in multi-modality information, loss of spatial information, and under-utilization of boundary information, thereby limiting the further improvement of segmentation accuracy. This paper proposes a multimodal spatial information enhancement and boundary shape correction method consisting of a modality information extraction (MIE) module, a spatial information enhancement (SIE) module, and a boundary shape correction (BSC) module. The above three modules act on the input, backbone, and loss functions of deep convolutional networks (DCNN), respectively, and compose an end-to-end 3D brain tumor segmentation model. The three proposed modules can solve the low utilization rate of effective modality information, the insufficient spatial information acquisition ability, and the improper segmentation of key boundary positions can be solved. The proposed method was validated on BraTS2017, 2018, and 2019 datasets. Comparative experimental results confirmed the effectiveness and superiority of the proposed method over state-of-the-art segmentation methods.},
  archive      = {J_PR},
  author       = {Zhiqin Zhu and Ziyu Wang and Guanqiu Qi and Neal Mazur and Pan Yang and Yu Liu},
  doi          = {10.1016/j.patcog.2024.110553},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110553},
  shortjournal = {Pattern Recognition},
  title        = {Brain tumor segmentation in MRI with multi-modality spatial information enhancement and boundary shape correction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robformer: A robust decomposition transformer for long-term
time series forecasting. <em>PR</em>, <em>153</em>, 110552. (<a
href="https://doi.org/10.1016/j.patcog.2024.110552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based forecasting methods have been widely applied to forecast long-term multivariate time series, which achieves significant improvements on extending the forecasting time. However, their performance can degenerate terribly when abrupt trend shift and seasonal fluctuation arise in long-term time series. Hence, we identify two bottlenecks of previous Transformers architecture: (1) the robustless decomposition module and (2) trend shifting problem . These result in a different distribution between the trend prediction and ground truth in the long-term multivariate series forecasting. Towards these bottlenecks, we design Robformer as a novel decomposition-based Transformer, which consists of three new inner module to enhance the predictability of Transformers. Concretely, we renew the decomposition module and add a seasonal component adjustment module to tackle the unstationarized series. Further, we propose a novel inner trend forecasting architecture inspired by polynomial fitting method, which outperforms previous design in accuracy and robustness. Our empirical studies show that Robformer can achieve 17% and 10% relative improvements than state-of-the-art Autoformer and FEDformer baselines under the fair long-term multivariate setting on six benchmarks, covering five mainstream time series forecasting applications: energy, economics, traffic, weather, and disease. The code will be released upon publication.},
  archive      = {J_PR},
  author       = {Yang Yu and Ruizhe Ma and Zongmin Ma},
  doi          = {10.1016/j.patcog.2024.110552},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110552},
  shortjournal = {Pattern Recognition},
  title        = {Robformer: A robust decomposition transformer for long-term time series forecasting},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed statistical learning algorithm for nonlinear
regression with autoregressive errors. <em>PR</em>, <em>153</em>,
110551. (<a href="https://doi.org/10.1016/j.patcog.2024.110551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing size of modern data brings challenges to statistical learning, and substantial distributed algorithms have been proposed. However, most of them need the homogeneity assumption that the distribution of the local data is the same as that of the global data. This is seldom in practice, and the learning performance deteriorates seriously if this assumption is not satisfied. Moreover, they are only for independent data, and cannot incorporate the serial correlations between data. To solve these issues, we propose a novel distributed statistical learning framework for the nonlinear regression with autoregressive errors, which realizes communication-efficient distributed optimization, and overcomes the homogeneity assumption. The theoretical results also guarantee that the new distributed framework is equivalent to the global one. Numerical experiments also illustrate the good performance of the new method.},
  archive      = {J_PR},
  author       = {Shaomin Li and Xiaofei Sun and Kangning Wang},
  doi          = {10.1016/j.patcog.2024.110551},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110551},
  shortjournal = {Pattern Recognition},
  title        = {Distributed statistical learning algorithm for nonlinear regression with autoregressive errors},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video anomaly detection guided by clustering learning.
<em>PR</em>, <em>153</em>, 110550. (<a
href="https://doi.org/10.1016/j.patcog.2024.110550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fuzzy boundary between normal and abnormal video data, which cannot be well distinguished by most methods, anomaly detection in video requires better characterization of the data. First we give a convolution-enhanced self-attentive video auto-encoder based on the U-Net architecture, which can extract richer image features. Secondly we design a dual-scale feature clustering structure for this encoder, which simultaneously compresses the channel and spatial structure features of the image to represent the features to obtain good coding characteristics and expand the boundary between normal and abnormal data. We also verify that our approach is equivalent to a class of auto-encoders for memory-guided learning. Finally, in the reconstruction task, since video auto-encoders are capable of triggering temporal time leakage phenomena that can lead to network performance degradation, we propose an anomaly score computation paradigm for video auto-encoders that utilizes the average frame anomaly score of a video clip to compute the first frame anomaly score in that video clip. Extensive experiments on three benchmark datasets show that our method outperforms most existing methods on large datasets with complex patterns. The code will be published at the following link: Anomaly-detection-guided-by-clustering-learning},
  archive      = {J_PR},
  author       = {Shaoming Qiu and Jingfeng Ye and Jiancheng Zhao and Lei He and Liangyu Liu and Bicong E. and Xinchen Huang},
  doi          = {10.1016/j.patcog.2024.110550},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110550},
  shortjournal = {Pattern Recognition},
  title        = {Video anomaly detection guided by clustering learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework for implementing distances for
categorical variables. <em>PR</em>, <em>153</em>, 110547. (<a
href="https://doi.org/10.1016/j.patcog.2024.110547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The degree to which objects differ from each other with respect to observations on a set of variables, plays an important role in many statistical methods. Many data analysis methods require a quantification of differences in the observed values which we can call distances. An appropriate definition of a distance depends on the nature of the data and the problem at hand. For distances between numerical variables, there exist many definitions that depend on the size of the observed differences. For categorical data, the definition of a distance is more complex as there is no straightforward quantification of the size of the observed differences. In this paper, we introduce a flexible framework for efficiently computing distances between categorical variables, supporting existing and new formulations tailored to specific contexts. In supervised classification, it enhances performance by integrating relationships between response and predictor variables. This framework allows measuring differences among objects across diverse data types and domains.},
  archive      = {J_PR},
  author       = {Michel van de Velden and Alfonso Iodice D’Enza and Angelos Markos and Carlo Cavicchia},
  doi          = {10.1016/j.patcog.2024.110547},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110547},
  shortjournal = {Pattern Recognition},
  title        = {A general framework for implementing distances for categorical variables},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient training-from-scratch framework with BN-based
structural compressor. <em>PR</em>, <em>153</em>, 110546. (<a
href="https://doi.org/10.1016/j.patcog.2024.110546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is an effective way of compressing convolutional neural networks (CNNs) under constrained resources. The current pruning methods follow a progressive pretrain-prune-finetune pipeline, which is inefficient and computationally expensive. In this paper, we bypass the pretrain-prune-finetune pipeline and propose a novel and efficient model training framework based on online channel pruning, which automatically produces a compact well-performed sub-network in one training-from-scratch pass under a given budget condition. Specifically, we introduce a novel BN-based indicator and a sparsity regularization strategy in the early training stage to iteratively and greedily shrink the model layers, which encourages a high-quality architecture with low channel redundancy. To ensure training stability and promote the generalization ability of the resultant pruned network, we also skillfully incorporate a simple self-distillation framework into our training and pruning pipeline. Extensive experiments indicate that our method can effectively achieve competitive performance on the image classification task compared with the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Fuyi Hu and Jin Zhang and Song Gao and Yu Lin and Wei Zhou and Ruxin Wang},
  doi          = {10.1016/j.patcog.2024.110546},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110546},
  shortjournal = {Pattern Recognition},
  title        = {An efficient training-from-scratch framework with BN-based structural compressor},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DynamicKD: An effective knowledge distillation via dynamic
entropy correction-based distillation for gap optimizing. <em>PR</em>,
<em>153</em>, 110545. (<a
href="https://doi.org/10.1016/j.patcog.2024.110545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The knowledge distillation uses a high-performance teacher network to guide the student network. However, the performance gap between the teacher and student networks can affect the student’s training. This paper proposes a novel knowledge distillation algorithm based on dynamic entropy correction, which adjusts the student instead of the teacher to reduce the gap. Firstly, the effect of changing the output entropy (short for output information entropy) on the distillation loss in the student is analyzed in theory. This paper shows that correcting the output entropy can reduce the gap. Then, a knowledge distillation algorithm based on dynamic entropy correction is created, which can correct the output entropy in real-time with an entropy controller updated dynamically by the distillation loss. The proposed algorithm is validated on the CIFAR100, ImageNet, and PASCAL VOC 2007. The comparison with various state-of-the-art distillation algorithms shows impressive results, especially in the experiment on the CIFAR100 regarding teacher–student pair resnet32x4–resnet8x4. The proposed algorithm raises 2.64 points over the traditional distillation algorithm and 0.87 points over the state-of-the-art algorithm CRD in classification accuracy, demonstrating its effectiveness and efficiency.},
  archive      = {J_PR},
  author       = {Songling Zhu and Ronghua Shang and Bo Yuan and Weitong Zhang and Wenjie Li and Yangyang Li and Licheng Jiao},
  doi          = {10.1016/j.patcog.2024.110545},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110545},
  shortjournal = {Pattern Recognition},
  title        = {DynamicKD: An effective knowledge distillation via dynamic entropy correction-based distillation for gap optimizing},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse and robust support vector machine with capped squared
loss for large-scale pattern classification. <em>PR</em>, <em>153</em>,
110544. (<a href="https://doi.org/10.1016/j.patcog.2024.110544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM), being considered one of the most efficient tools for classification, has received widespread attention in various fields. However, its performance is hindered when dealing with large-scale pattern classification tasks due to high memory requirements and running very slow. To address this challenge, we construct a novel sparse and robust SVM based on our newly proposed capped squared loss (named as L c s l Lcsl -SVM). To solve L c s l Lcsl -SVM, we first focus on establishing optimality theory of L c s l Lcsl -SVM via our defined proximal stationary point, which is convenient for us to efficiently characterize the L c s l Lcsl support vectors of L c s l Lcsl -SVM. We subsequently demonstrate that the L c s l Lcsl support vectors comprise merely a minor fraction of entire training data. This observation leads us to introduce the concept of the working set. Furthermore, we design a novel subspace fast algorithm with working set (named as L c s l Lcsl -ADMM) for solving L c s l Lcsl -SVM, which is proven that L c s l Lcsl -ADMM has both global convergence and relatively low computational complexity. Finally, numerical experiments show that L c s l Lcsl -ADMM has excellent performances in terms of getting the best classification accuracy, using the shortest time and presenting the smallest numbers of support vectors when solving large-scale pattern classification problems.},
  archive      = {J_PR},
  author       = {Huajun Wang and Hongwei Zhang and Wenqian Li},
  doi          = {10.1016/j.patcog.2024.110544},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110544},
  shortjournal = {Pattern Recognition},
  title        = {Sparse and robust support vector machine with capped squared loss for large-scale pattern classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic heterogeneous federated learning with multi-level
prototypes. <em>PR</em>, <em>153</em>, 110542. (<a
href="https://doi.org/10.1016/j.patcog.2024.110542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning shows promise as a privacy-preserving collaborative learning technique. Existing research mainly focuses on skewing the class distribution across clients. However, most approaches suffer from catastrophic forgetting and classifier shift, mainly when the global distribution of all classes is extremely unbalanced and the data distribution of the client dynamically evolves over time. In this paper, we study the Dynamic Heterogeneous Federated Learning, which addresses the practical scenario where heterogeneous data distributions exist among different clients and dynamic tasks within the client. Accordingly, we propose a novel federated learning framework named Federated Multi-Level Prototypes and design federated multi-level regularizations. To mitigate classifier shift, we construct semantic prototypes to provide fruitful generalization knowledge. To maintain the model stability and consistency convergence, three regularizations are introduced as training losses, i.e., prototype-based regularization, semantic prototype-based regularization, and federated inter-task regularization. Extensive experiments show that the proposed method achieves advanced performance in various settings.},
  archive      = {J_PR},
  author       = {Shunxin Guo and Hongsong Wang and Xin Geng},
  doi          = {10.1016/j.patcog.2024.110542},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110542},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic heterogeneous federated learning with multi-level prototypes},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-decoupling the classification branch in object detectors
for few-class scenes. <em>PR</em>, <em>153</em>, 110541. (<a
href="https://doi.org/10.1016/j.patcog.2024.110541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-class object detection is a critical task in numerous scenes, such as autonomous driving and intelligent surveillance. The current researches mainly focus on the correlation or decoupling between classification and regression subtasks in object detection. However, they rarely take advantage of the potential of re-decoupling the classification subtask. In this paper, we propose to re-decouple the classification branch in object detection for few-class scenes by reducing multi-classification features to multiple binary-classification features. Since multi-classification losses cannot supervise the network to learn decoupled binary-classification features, we introduce a single-class loss to supervise decoupled multiple binary-classification branches. In particular, we propose a basic feature degradation head (FD-Head) structure that decouples the classification branches and applies binary-classification loss to encourage each branch to learn only the degraded single-class features. In addition, based on the mutual exclusion between classes, we propose a mutual exclusion constraint (FD-Head-M) module to constrain the scores of all classes, promoting the detector performance. Finally, we replace the original convolution with more powerful feature extraction modules to form the enhanced FD-Head (FD-Head-E). Notably, our method can be used as a universal module and embedded into the existing object detectors to boost their performance. When applying our method to typical object detectors, it experimentally achieves performance gains of 1.2–2.2%, 1.7–2.5% on the KITTI-3, SeaShips datasets respectively. When using ResNet50 as the backbone network, our method gains an accuracy of 45.9% on the MS COCO dataset.},
  archive      = {J_PR},
  author       = {Jie Hua and Zhongyuan Wang and Qin Zou and Jinsheng Xiao and Xin Tian and Yufei Zhang},
  doi          = {10.1016/j.patcog.2024.110541},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110541},
  shortjournal = {Pattern Recognition},
  title        = {Re-decoupling the classification branch in object detectors for few-class scenes},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dense center-direction regression for object counting and
localization with point supervision. <em>PR</em>, <em>153</em>, 110540.
(<a href="https://doi.org/10.1016/j.patcog.2024.110540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object counting and localization problems are commonly addressed with point-supervised learning, which allows the use of less labor-intensive point annotations. However, learning based on point annotations poses challenges due to the high imbalance between the sets of annotated and unannotated pixels, which is often treated with Gaussian smoothing of point annotations and focal loss. However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of the data only indirectly. In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a dense regression of directions pointing towards the nearest object centers, i.e. center-directions . This provides greater support for each center point arising from many surrounding pixels pointing towards the object center. We propose a formulation of center-directions that allows the problem to be split into the domain-specific dense regression of center-directions and the final localization task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely independent of the target domain. We demonstrate the performance of the proposed method on six different datasets for object counting and localization and show that it outperforms the existing state-of-the-art methods. The code is accessible on GitHub at https://github.com/vicoslab/CeDiRNet.git .},
  archive      = {J_PR},
  author       = {Domen Tabernik and Jon Muhovič and Danijel Skočaj},
  doi          = {10.1016/j.patcog.2024.110540},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110540},
  shortjournal = {Pattern Recognition},
  title        = {Dense center-direction regression for object counting and localization with point supervision},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust pedestrian detection via constructing versatile
pedestrian knowledge bank. <em>PR</em>, <em>153</em>, 110539. (<a
href="https://doi.org/10.1016/j.patcog.2024.110539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection is a crucial field of computer vision research which can be adopted in various real-world applications ( e.g., self-driving systems). However, despite noticeable evolution of pedestrian detection, pedestrian representations learned within a detection framework are usually limited to particular scene data in which they were trained. Therefore, in this paper, we propose a novel approach to construct versatile pedestrian knowledge bank containing representative pedestrian knowledge which can be applicable to various detection frameworks and adopted in diverse scenes. We extract generalized pedestrian knowledge from a large-scale pretrained model, and we curate them by quantizing most representative features and guiding them to be distinguishable from background scenes. Finally, we construct versatile pedestrian knowledge bank which is composed of such representations, and then we leverage it to complement and enhance pedestrian features within a pedestrian detection framework. Through comprehensive experiments, we validate the effectiveness of our method, demonstrating its versatility and outperforming state-of-the-art detection performances.},
  archive      = {J_PR},
  author       = {Sungjune Park and Hyunjun Kim and Yong Man Ro},
  doi          = {10.1016/j.patcog.2024.110539},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110539},
  shortjournal = {Pattern Recognition},
  title        = {Robust pedestrian detection via constructing versatile pedestrian knowledge bank},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph correlated discriminant embedding for multi-source
domain adaptation. <em>PR</em>, <em>153</em>, 110538. (<a
href="https://doi.org/10.1016/j.patcog.2024.110538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a main branch of domain adaptation (DA), multi-source DA (MSDA) has attracted increasing attention for exploiting information from multi-source domain data. However, how to effectively explore useful information from each source domain for target tasks is still a key problem. In this paper, to fully explore multiple information of different domain data, we propose a graph correlated discriminant embedding (GCDE) method for MSDA. In GCDE, the category-discriminative information, manifold structure, and correlation learning are fully considered. Specifically, GCDE encodes the within- and between- class information of each domain data, preserves the local and global structure information of the data, and extracts the maximization correlative features from different domains by designing a novel correlative learning scheme. We also extend GCDE to a nonlinear case and obtain kernel GCDE (KGCDE). We have conducted extensive experiments on four public data benchmarks to verify the performance of GCDE and KGCDE. The promising performance on the databases prove the efficiency of our methods with the comparison of the advanced approaches.},
  archive      = {J_PR},
  author       = {Wai Keung Wong and Yuwu Lu and Zhihui Lai and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110538},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110538},
  shortjournal = {Pattern Recognition},
  title        = {Graph correlated discriminant embedding for multi-source domain adaptation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-attention empowered graph convolutional network for
structure learning and node embedding. <em>PR</em>, <em>153</em>,
110537. (<a href="https://doi.org/10.1016/j.patcog.2024.110537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In representation learning on graph-structured data, many popular graph neural networks (GNNs) fail to capture long-range dependencies, leading to performance degradation. Furthermore, this weakness is magnified when the concerned graph is characterized by heterophily (low homophily). To solve this issue, this paper proposes a novel graph representation learning framework called the graph convolutional network with self-attention (GCN-SA). The proposed scheme exhibits an exceptional generalization capability in node-level representation learning. The proposed GCN-SA contains two enhancements corresponding to edges and node features. For edges, we utilize a self-attention mechanism to design a stable and effective graph-structure-learning module that can capture the internal correlation between any pair of nodes. This graph-structure-learning module can identify reliable neighbors for each node from the entire graph. Regarding the node features, we modify the transformer block to enable it to assist the GCN in integrating valuable information from the entire graph. These two enhancements work in distinct ways to help our GCN-SA capture long-range dependencies, enabling it to perform representation learning on graphs with varying levels of homophily. The experimental results on real-world benchmark datasets demonstrate the effectiveness of the proposed GCN-SA. Compared to other outstanding GNN counterparts, the proposed GCN-SA is competitive. The source code is available at https://github.com/mengyingjiang/GCN-SA.},
  archive      = {J_PR},
  author       = {Mengying Jiang and Guizhong Liu and Yuanchao Su and Xinliang Wu},
  doi          = {10.1016/j.patcog.2024.110537},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110537},
  shortjournal = {Pattern Recognition},
  title        = {Self-attention empowered graph convolutional network for structure learning and node embedding},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Decoupled and boosted learning for skeleton-based dynamic
hand gesture recognition. <em>PR</em>, <em>153</em>, 110536. (<a
href="https://doi.org/10.1016/j.patcog.2024.110536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cost-effective depth sensors, skeleton-based dynamic hand gesture recognition has made significant progress. Existing methods mostly utilize a single model to learn all spatial–temporal features. Meanwhile, they cannot effectively boost key features and make use of multi-scale features. In this paper, we propose a lightweight dual-stream framework, which consists of a temporal mutual boosted stream (TMB-Stream) and a spatial self-boosted stream (SSB-Stream). In the TMB-Stream, we design a hybrid attention module (HAM) to boost important motion features from temporal sequences, which is composed of a multi-scale multi-head attention module (MMAM) and a spatial–temporal attention module (STAM). In the SSB-Stream, we present a self-boosted learning manner to promote the performance of the spatial stream. Specifically, we design a multi-scale auto-encoder (MAE), which can use limited skeleton data to extract and boost spatial latent features by minimizing the gap between original and reconstructed skeleton images. In addition, we propose a multi-scale fusion module (MFM) to effectively fuse multi-scale features in stages. Experimental results show that our lightweight framework yields satisfactory performance on SHREC’17 Track and DHG-14/28 datasets, as well as very competitive performance on FPHA dataset.},
  archive      = {J_PR},
  author       = {Yangke Li and Guangshun Wei and Christian Desrosiers and Yuanfeng Zhou},
  doi          = {10.1016/j.patcog.2024.110536},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110536},
  shortjournal = {Pattern Recognition},
  title        = {Decoupled and boosted learning for skeleton-based dynamic hand gesture recognition},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional correlation-driven inter-frame interaction
transformer for referring video object segmentation. <em>PR</em>,
<em>153</em>, 110535. (<a
href="https://doi.org/10.1016/j.patcog.2024.110535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referring video object segmentation (RVOS) aims to segment the target object in a video sequence described by a language expression. Typical multimodal Transformer based RVOS approaches process video sequence in a frame-independent manner to reduce the high computational cost, which however restricts the performance due to the lack of inter-frame interaction for temporal coherence modeling and spatio-temporal representation learning of the referred object. Besides, the absence of sufficient cross-modal interactions results in weak correlation between the visual and linguistic features, which increases the difficulty of decoding the target information and limits the performance of the model. In this paper, we propose a bidirectional correlation-driven inter-frame interaction Transformer, dubbed BIFIT, to address these issues in RVOS. Specifically, we design a lightweight and plug-and-play inter-frame interaction module in the Transformer decoder to efficiently learn the spatio-temporal features of the referred object, so as to decode the object information in the video sequence more precisely and generate more accurate segmentation results. Moreover, a bidirectional multi-level vision-language interaction module is implemented before the multimodal Transformer to enhance the correlation between the linguistic and multi-level visual features, thus facilitating the language queries to decode more precise object information from visual features and ultimately improving the segmentation performance. Extensive experimental results on four benchmarks validate the superiority of our BIFIT over state-of-the-art methods and the effectiveness of our proposed modules. The code is available in https://github.com/LANMNG/BIFIT .},
  archive      = {J_PR},
  author       = {Meng Lan and Fu Rong and Zuchao Li and Wei Yu and Lefei Zhang},
  doi          = {10.1016/j.patcog.2024.110535},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110535},
  shortjournal = {Pattern Recognition},
  title        = {Bidirectional correlation-driven inter-frame interaction transformer for referring video object segmentation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PARDet: Dynamic point set alignment for rotated object
detection. <em>PR</em>, <em>153</em>, 110534. (<a
href="https://doi.org/10.1016/j.patcog.2024.110534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent mismatch between rotated objects and horizontal features, feature point misalignment has been a challenge in the task of Rotated Object Detection (ROD). Specifically, considering the pattern of convolution, foreground features are often mixed with background noise, which can confuse the model and affect the model from feature point alignment during the training phase. To mitigate this issue, previous methods concentrate on fixed positions derived from predicted boxes by additionally introducing a refinement stage. However, merely learning fixed position priors during training can result in suboptimal alignment and inefficiency during inference. This paper introduces a dynamic point alignment detector to concurrently address issues associated with feature misalignment and inference inefficiency. The method is made up of two components: the fine-grained points generator (FPG) captures key information, and the point alignment module (PAM) derives precise feature representations. Both modules empower the detector with the capability to dynamically perceive rotated objects, extracting more comprehensive and reasoned feature contents from the feature maps. In general, our method enables the model to independently identify and prioritize the valuable features during the training process. Subsequently, during the inference stage, the results can be directly predicted without additional alignment operations. The experimental results demonstrate that our method can achieve competitive and superior results with average precision (AP) values of 79.33%, 95.73%, and 63.37% on the DOTA, HRSC2016, and DIOR-R datasets, respectively. Codes will be publicly available at https://github.com/Xuyihaoby/PARDet .},
  archive      = {J_PR},
  author       = {Yihao Xu and Jifeng Shen and Ming Dai and Wankou Yang},
  doi          = {10.1016/j.patcog.2024.110534},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110534},
  shortjournal = {Pattern Recognition},
  title        = {PARDet: Dynamic point set alignment for rotated object detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified multi-level neighbor clustering for source-free
unsupervised domain adaptation. <em>PR</em>, <em>153</em>, 110533. (<a
href="https://doi.org/10.1016/j.patcog.2024.110533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-Free Unsupervised Domain Adaptation (SFUDA) transfers knowledge from the source domain to the target domain with using the source domain model instead of the source data, as the source data cannot be accessed in data privacy scenarios. Our method is based on a clustering assumption: although there is a domain shift, target data with similar semantic still form a cluster in the source feature space. We identify two levels of clustering. One is class-level neighbor clustering: data with the same label tend to form a large cluster. The other is instance-level neighbor clustering: data and its neighbors tend to share the same label. Previous methods only consider one level, and we consider that both are complementary. We propose a new SFUDA method called unified multi-level neighbor clustering to address class and instance consistency in a complementary way. Our proposed method achieves competitive results on three domain adaptation benchmark datasets.},
  archive      = {J_PR},
  author       = {Yuzhe Xiao and Guangyi Xiao and Hao Chen},
  doi          = {10.1016/j.patcog.2024.110533},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110533},
  shortjournal = {Pattern Recognition},
  title        = {Unified multi-level neighbor clustering for source-free unsupervised domain adaptation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancements in point cloud data augmentation for deep
learning: A survey. <em>PR</em>, <em>153</em>, 110532. (<a
href="https://doi.org/10.1016/j.patcog.2024.110532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has become one of the mainstream and effective methods for point cloud analysis tasks such as detection, segmentation and classification. To reduce overfitting during training DL models and improve model performance especially when the amount and/or diversity of training data are limited, augmentation is often crucial. Although various point cloud data augmentation methods have been widely used in different point cloud processing tasks, there are currently no published systematic surveys or reviews of these methods. Therefore, this article surveys these methods, categorizing them into a taxonomy framework that comprises basic and specialized point cloud data augmentation methods. Through a comprehensive evaluation of these augmentation methods, this article identifies their potentials and limitations, serving as a useful reference for choosing appropriate augmentation methods. In addition, potential directions for future research are recommended. This survey contributes to providing a holistic overview of the current state of point cloud data augmentation, promoting its wider application and development.},
  archive      = {J_PR},
  author       = {Qinfeng Zhu and Lei Fan and Ningxin Weng},
  doi          = {10.1016/j.patcog.2024.110532},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110532},
  shortjournal = {Pattern Recognition},
  title        = {Advancements in point cloud data augmentation for deep learning: A survey},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on handwritten mathematical expression recognition:
The rise of encoder-decoder and GNN models. <em>PR</em>, <em>153</em>,
110531. (<a href="https://doi.org/10.1016/j.patcog.2024.110531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of handwritten mathematical expressions (HMEs) has attracted growing interest due to steady progress in handwriting recognition techniques and the rapid emergence of pen- and touch-based devices. Math formula recognition may be understood as a generalization of text recognition: formulas represent mathematical statements using a two dimensional arrangement of symbols on writing lines that are organized hierarchically. This survey provides an overview of techniques published in the last decade, including those taking input from handwritten strokes (i.e., ‘online’, as captured by a pen/touch device), raster images (i.e., ‘offline,’ from pixels), or both. Traditionally, HMEs were recognized by performing four structural pattern recognition tasks in separate steps: (1) symbol segmentation, (2) symbol classification, (3) spatial relationship classification, and (4) structural analysis, which identifies the arrangement of symbols on writing lines (e.g., in a Symbol Layout Tree (SLT) or LaTeX string). Recently, encoder–decoder neural network models and Graph Neural Network (GNN) approaches have greatly increased HME recognition accuracy. These newer approaches perform all recognition tasks simultaneously, and utilize contextual features across tasks (e.g., using neural self-attention models). We also discuss evaluation techniques and benchmarks, and explore some implicit dependencies among the four key recognition tasks. Finally, we identify limitations of current systems, and present suggestions for future work, such as using two-dimensional language models rather than the one-dimensional models commonly used in encoder–decoder models.},
  archive      = {J_PR},
  author       = {Thanh-Nghia Truong and Cuong Tuan Nguyen and Richard Zanibbi and Harold Mouchère and Masaki Nakagawa},
  doi          = {10.1016/j.patcog.2024.110531},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110531},
  shortjournal = {Pattern Recognition},
  title        = {A survey on handwritten mathematical expression recognition: The rise of encoder-decoder and GNN models},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep and wide nonnegative matrix factorization with embedded
regularization. <em>PR</em>, <em>153</em>, 110530. (<a
href="https://doi.org/10.1016/j.patcog.2024.110530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end learning is an advanced framework in deep learning. It combines feature extraction, followed by pattern recognition (classification, clustering, etc.) in a unified learning structure. However, these deep networks face several challenges such as overfitting, vanishing gradient, computational complexity, information loss in layers, and weak robustness to noisy data/features. To address these challenges, this paper presents Deep and Wide Nonnegative Matrix Factorization (DWNMF) with embedded regularization for the feature extraction stage of the end-to-end models. DWNMF aims to identify more robust features while preventing overfitting via embedding regularization. For this purpose, DWNMF integrates input data with its noisy versions as diverse augmented channels. Then, the features in all channels are extracted in parallel using distinct network branches. The parameters of this model learn the intrinsic hierarchical features in the channels of complex data objects. Finally, the extracted features in different channels are aggregated in a single feature space to perform the classification task. To embed regularization in the DWNMF model, some NMF neurons in the layers are substituted by random neurons to increase the stability and robustness of the extracted features. Experimental results confirm that the DWNMF model extracts more robust features, prevents overfitting, and achieves better classification accuracy compared to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Hojjat Moayed and Eghbal G. Mansoori},
  doi          = {10.1016/j.patcog.2024.110530},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110530},
  shortjournal = {Pattern Recognition},
  title        = {Deep and wide nonnegative matrix factorization with embedded regularization},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene recovery: Combining visual enhancement and resolution
improvement. <em>PR</em>, <em>153</em>, 110529. (<a
href="https://doi.org/10.1016/j.patcog.2024.110529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visibility enhancement of outdoor images under complex imaging conditions has been a crucial task for computer vision and received growing attention. However, existing image enhancement methods could result in typical block-like artifacts or color distortion. The undesirable impurities might also be significantly magnified after the enhancement task, further reducing the image quality. For enhancing and super-resolving complex real-world degradation, we propose a simultaneous visual enhancement and resolution improvement (VERI) variational scene recovery model for jointly enhancing image visibility and improving the resolution of the degraded image. Particularly, we estimate the scattering light map for degradation images to achieve clean scene radiance and simultaneously seek a high-quality image through a deep super-resolution network. The semi-proximal alternating direction method of multipliers (sPADMM) algorithm is employed for efficiently solving the minimization problems in the proposed model. Extensive experiments illustrate the effectiveness and robustness of the proposed method in dealing with various scenes, such as haze, sandstorm, underwater or low illumination.},
  archive      = {J_PR},
  author       = {Hao Zhang and Te Qi and Tieyong Zeng},
  doi          = {10.1016/j.patcog.2024.110529},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110529},
  shortjournal = {Pattern Recognition},
  title        = {Scene recovery: Combining visual enhancement and resolution improvement},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Affine collaborative normalization: A shortcut for
adaptation in medical image analysis. <em>PR</em>, <em>153</em>, 110528.
(<a href="https://doi.org/10.1016/j.patcog.2024.110528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of “pretraining-then-finetuning” (PT-FT) has been extensively explored to enhance the performance of clinical applications with limited annotations. A major impediment to applying such workflow to various medical imaging modalities is the lack of parameter-free approaches boosting the transferability against notable domain shifts. The success of normalization techniques in domain adaptation provides a promising solution, while inaccessible source data in finetuning (FT) poses new challenges. In this paper, we revisit the Batch Normalization module in PT-FT and propose a unified framework for both fast transferability estimation and transferability-aware finetuning. We discovered that the vanilla FT suffers from the issue that feature patterns of corresponding channels could be misaligned across domains. Hence, we present an Affine Collaborative Normalization (AC-Norm) to dynamically recalibrate the channels in the target model according to the cross-domain channel-wise correlations without any source data and extra parameters. AC-Norm provides very competitive results compared to state-of-the-art FT methods in six classification/segmentation tasks. We also demonstrate the capability of AC-Norm in estimating the transferability of pretrained models in only single-step backpropagation. Our code is available at https://github.com/EndoluminalSurgicalVision-IMR/ACNorm .},
  archive      = {J_PR},
  author       = {Chuyan Zhang and Yuncheng Yang and Hao Zheng and Yawen Huang and Yefeng Zheng and Yun Gu},
  doi          = {10.1016/j.patcog.2024.110528},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110528},
  shortjournal = {Pattern Recognition},
  title        = {Affine collaborative normalization: A shortcut for adaptation in medical image analysis},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early event detection for facial expression based on
infinite mixture prototypes. <em>PR</em>, <em>153</em>, 110527. (<a
href="https://doi.org/10.1016/j.patcog.2024.110527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In early event detection, the ability to detect events at an earlier stage means to have more ample time for processing. The purpose of this study is to apply few-shot learning to early event detection based on infinite mixture prototypes. we present a design to distribute the early event detection issues across the loss function and the data task setting in few-shot learning, and the adoption of infinite mixture models helps to resolve the multi-model distribution of the pattern of early events. The proposed algorithm is evaluated on the CK＋database. The experimental results demonstrate not only feasible performance in terms of accuracy and detection time but also alignment with our hypothesis through the observation of generated clusters.},
  archive      = {J_PR},
  author       = {Zhi-Fang Yang and Dai-Yi Chiu},
  doi          = {10.1016/j.patcog.2024.110527},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110527},
  shortjournal = {Pattern Recognition},
  title        = {Early event detection for facial expression based on infinite mixture prototypes},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-aware hierarchical labeling for face forgery
detection. <em>PR</em>, <em>153</em>, 110526. (<a
href="https://doi.org/10.1016/j.patcog.2024.110526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an uncertainty-aware hierarchical labeling method for face forgery detection, which aims to simultaneously explore the traces of forgery contents from a hierarchical level. Unlike existing face forgery detection methods that usually focus on local regions or entire images, our proposed approach takes advantage of hierarchical labeling as auxiliary supervised signals to capture hybrid information from pixel-level, patch-level and image-level. Moreover, we utilize Transformer architecture to extract the patch-level information to build a bridge between the pixel-level and the image-level information. However, the face forgery ground truth always carries data uncertainty due to the existence of the blending step and the compression process during face image manipulation. Thus we introduce an uncertainty learning method to formulate and leverage the data uncertainty. Extensive experimental results on five public datasets demonstrate that our approach not only achieves very competitive performance but also improves the generalization ability.},
  archive      = {J_PR},
  author       = {Bingyao Yu and Wanhua Li and Xiu Li and Jie Zhou and Jiwen Lu},
  doi          = {10.1016/j.patcog.2024.110526},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110526},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty-aware hierarchical labeling for face forgery detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). PlaneAC: Line-guided planar 3D reconstruction based on
self-attention and convolution hybrid model. <em>PR</em>, <em>153</em>,
110519. (<a href="https://doi.org/10.1016/j.patcog.2024.110519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planar 3D reconstruction aims to simultaneously extract plane instances and reconstruct the local 3D model through the estimated plane parameters. Existing methods achieve promising results either through self-attention or convolution neural network (CNNs), but usually ignore the complementary properties of them. In this paper, we propose a line-guided planar 3D reconstruction method PlaneAC, which leverages the advantage of self-attention and CNNs to capture long-range dependencies and alleviate the computational burden. In addition, explicit connection between two adjacent attention layers is built for better leveraging the transferable knowledge and facilitating the information flow between tokenized feature from different layers. Therefore, the subsequent attention layer can directly interact with previous results. Finally, a line segment filtering method is presented to remove irrelevant guiding information from indistinctive line segments extracted from the image. Extensive experiments on ScanNet and NYUv2 public datasets demonstrate the preferable performance of our proposed method, and the results show that PlaneAC achieves a better trade-off between accuracy and computation cost compared with other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jiahui Zhang and Jinfu Yang and Fuji Fu and Jiaqi Ma},
  doi          = {10.1016/j.patcog.2024.110519},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110519},
  shortjournal = {Pattern Recognition},
  title        = {PlaneAC: Line-guided planar 3D reconstruction based on self-attention and convolution hybrid model},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised vanishing point detection with contrastive
learning. <em>PR</em>, <em>153</em>, 110518. (<a
href="https://doi.org/10.1016/j.patcog.2024.110518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vanishing point (VP) detection in road images plays an important role in driving scenes for advanced driver assistance systems (ADAS) and autonomous vehicles. Because it is still a challenging problem to obtain expensive annotated datasets in deep-learning-based supervised training, in this paper, we propose a semi-supervised VP detection method in road images. Our proposed model first extracts high-resolution VP heatmaps of the road images by fusing multi-level and global–local contrastive learning. Then, we apply the π π model as a semi-supervised learning framework and feed the whole global network with a small number of training samples to detect VPs. In the experiment, we used Kong’s dataset for the unstructured road test and the KITTI-VP dataset for the structured road test. Compared with the state-of-the-art methods, our model achieved high accuracy and robustness in road VP detection.},
  archive      = {J_PR},
  author       = {Yukun Wang and Shuo Gu and Yinbo Liu and Hui Kong},
  doi          = {10.1016/j.patcog.2024.110518},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110518},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised vanishing point detection with contrastive learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly privileged learning with knowledge extraction.
<em>PR</em>, <em>153</em>, 110517. (<a
href="https://doi.org/10.1016/j.patcog.2024.110517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning using privileged information (LUPI) has shown promise in improving supervised learning by embedding additional knowledge. However, its reliance on the assumption of readily available privileged information may not hold true in practical scenarios due to limitations in access or confidentiality. To address these challenges, this paper presents a novel weakly privileged learning (WPL) framework, integrating knowledge extraction methods within the LUPI context. An effective strategy is proposed to implement the WPL framework, where knowledge extraction techniques generate a weight matrix as weak privileged information. Extensive experiments employing various existing knowledge extraction techniques demonstrate that the proposed WPL outperforms traditional supervised learning and approaches the performance of standard privileged learning where privileged information is given in advance. This research establishes WPL as a promising learning paradigm, addressing limitations in privileged information availability and advancing the field of machine learning in practical settings.},
  archive      = {J_PR},
  author       = {Saiji Fu and Tianyi Dong and Zhaoxin Wang and Yingjie Tian},
  doi          = {10.1016/j.patcog.2024.110517},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110517},
  shortjournal = {Pattern Recognition},
  title        = {Weakly privileged learning with knowledge extraction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Irregular text block recognition via decoupling visual,
linguistic, and positional information. <em>PR</em>, <em>153</em>,
110516. (<a href="https://doi.org/10.1016/j.patcog.2024.110516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition has made great progress in regular formats, and the recent research has focused on irregular text recognition. In this work, we investigate a new challenge problem of recognizing a text block instance with irregular arrangement of characters, which is referred to as irregular text block recognition (ITBR). This problem is prevalent in daily scenarios, especially with the increasing use of rich text designs in signboards, logos, posters, and other mediums. The primary challenge arises from the weakened position clues and the highly complex reading order, which can often only be deciphered by a heavy reliance on understanding the intrinsic linguistic information. Hence, conventional recognition methods that employ inflexible character grouping rules, coupled with positional information, or constrained by vocabulary reliance, may struggle with the ITBR problem. To this end, we propose a progressive layout reasoning network (PLRN) to recognize the irregular text block by decoupling visual, linguistic, and positional information. PLRN comprises a character spotting module that recognizes the character set based solely on visual features with a new TopK-rank decoding mechanism, and a linkage reasoning module to interpret the character relationships within this set with a progressive refinement strategy. The linkages are initially reasoned by linguistic information and then progressively refined through the incorporation of proximity and tendency information, allowing for explicit decoupling and improved reasoning accuracy. To assess the effectiveness of the proposed method, we construct a new dataset called TextBlock600. This dataset consists of 600 images of irregular text blocks, each with complete sequence annotations. Experimental results demonstrate that PLRN shows promising performance in ITBR, opening up possibilities for further research in this field. Code and datasets will be available at https://github.com/eezyli/PLRN .},
  archive      = {J_PR},
  author       = {Ziyan Li and Lianwen Jin and Chengquan Zhang and Jiaxin Zhang and Zecheng Xie and Pengyuan Lyu and Kun Yao},
  doi          = {10.1016/j.patcog.2024.110516},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110516},
  shortjournal = {Pattern Recognition},
  title        = {Irregular text block recognition via decoupling visual, linguistic, and positional information},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RFL-CDNet: Towards accurate change detection via richer
feature learning. <em>PR</em>, <em>153</em>, 110515. (<a
href="https://doi.org/10.1016/j.patcog.2024.110515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change Detection is a crucial but extremely challenging task of remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods mainly focus on intricate feature extraction and multi-scale feature fusion, while ignoring the insufficient utilization of features in the intermediate stages, thus resulting in sub-optimal results. To this end, we propose a novel framework, named RFL-CDNet, that utilizes richer feature learning to boost change detection performance. Specifically, we first introduce deep multiple supervision to enhance intermediate representations, thus unleashing the potential of backbone feature extractor at each stage. Furthermore, we design the Coarse-To-Fine Guiding (C2FG) module and the Learnable Fusion (LF) module to further improve feature learning and obtain more discriminative feature representations. The C2FG module aims to seamlessly integrate the side prediction from previous coarse-scale into the current fine-scale prediction in a coarse-to-fine manner, while LF module assumes that the contribution of each stage and each spatial location is independent, thus designing a learnable module to fuse multiple predictions. Experiments on several benchmark datasets show that our proposed RFL-CDNet achieves state-of-the-art performance on WHU cultivated land dataset and CDD dataset, and the second best performance on WHU building dataset. The source code and models are publicly available at https://github.com/Hhaizee/RFL-CDNet .},
  archive      = {J_PR},
  author       = {Yuhang Gan and Wenjie Xuan and Hang Chen and Juhua Liu and Bo Du},
  doi          = {10.1016/j.patcog.2024.110515},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110515},
  shortjournal = {Pattern Recognition},
  title        = {RFL-CDNet: Towards accurate change detection via richer feature learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection by universum embedding. <em>PR</em>,
<em>153</em>, 110514. (<a
href="https://doi.org/10.1016/j.patcog.2024.110514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection in classification is an important task in machine learning. Inspired by the success of Universum support vector machine proposed by Weston et al. on improving the classification ability of classical support vector machine, this paper considers a special type of Universum and further lets it play its role in both useful feature identification and separating hyperplane construction, aiming to improve both the feature selection ability and classification performance of Universum support vector machine. By introducing this special Universum, a redundant feature can be identified by observing whether some Universum sample is useful. In fact, we prove that by observing the dual solution of the optimization problem, useful features can be selected from a set satisfying some properties. Due to the introduction of these extra Universum samples, it needs to cope with a large-scale optimization problem. To improve the training efficiency, we modify the sequential minimal optimization algorithm and further combine it with the coordinate descent technique to solve the proposed model. Experimental results on artificial datasets, benchmark datasets, and text classification datasets demonstrate that the proposed method improves the classification performance of support vector machine and Universum support vector machine, and also has good feature selection ability.},
  archive      = {J_PR},
  author       = {Chun-Na Li and Ling-Wei Huang and Yuan-Hai Shao and Tingting Guo and Yu Mao},
  doi          = {10.1016/j.patcog.2024.110514},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110514},
  shortjournal = {Pattern Recognition},
  title        = {Feature selection by universum embedding},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). GCNet: Probing self-similarity learning for generalized
counting network. <em>PR</em>, <em>153</em>, 110513. (<a
href="https://doi.org/10.1016/j.patcog.2024.110513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class-agnostic counting (CAC) problem has garnered significant attention recently due to its broad societal applications and formidable challenges. Existing approaches to counting objects of various categories typically rely on user-provided exemplars, which are challenging to obtain and limit their generality. In this paper, our goal is to empower the framework to recognize adaptive exemplars within entire images. To achieve this, we introduce a zero-shot Generalized Counting Network (GCNet), which utilizes a pseudo-Siamese structure to automatically and efficiently learn pseudo exemplar cues from inherent repetition patterns. In addition, a weakly-supervised scheme is presented to reduce the burden of laborious density maps required by all contemporary CAC models, allowing GCNet to be trained using count-level supervisory signals in an end-to-end manner. Without providing any spatial location hints, GCNet is capable of adaptively capturing them through a carefully-designed self-similarity learning strategy. Extensive experiments and ablation studies on the prevailing benchmark FSC147 for zero-shot CAC demonstrate the superiority of our GCNet. It performs on par with existing exemplar-dependent methods and shows stunning cross-dataset generality on crowd-specific datasets, e.g. , ShanghaiTech Part A, Part B and UCF_QNRF.},
  archive      = {J_PR},
  author       = {Mingjie Wang and Yande Li and Jun Zhou and Graham W. Taylor and Minglun Gong},
  doi          = {10.1016/j.patcog.2024.110513},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110513},
  shortjournal = {Pattern Recognition},
  title        = {GCNet: Probing self-similarity learning for generalized counting network},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust and sparse linear discriminant analysis for
image classification. <em>PR</em>, <em>153</em>, 110512. (<a
href="https://doi.org/10.1016/j.patcog.2024.110512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is a popular dimensionality reduction technique that has been widely used in pattern recognition. However, there exist a large number of redundant features and corrupted noise in real-world applications, which makes the performance of existing LDA methods degrade and thus leads to a decrease in classification accuracy. To address the above issues, we propose a novel robust and sparse LDA formulation dubbed RSLDA+. The key idea is introducing the mixed sparse regularization, i.e., ℓ 0 ℓ0 -norm plus ℓ 2 , 0 ℓ2,0 -norm, for feature representation and enforce ℓ 0 ℓ0 -norm for noise reduction. Furthermore, an optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed in combination with hard thresholding operators. Extensive experiments on six common image datasets verify that the proposed RSLDA+ outperforms state-of-the-art LDA variants in classification accuracy. In addition, the ablation, robustness, convergence, stability, and sparsity are analyzed in detail. The results suggest that the proposed RSLDA+ provides an effective and robust method for image classification.},
  archive      = {J_PR},
  author       = {Jingjing Liu and Manlong Feng and Xianchao Xiu and Wanquan Liu},
  doi          = {10.1016/j.patcog.2024.110512},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110512},
  shortjournal = {Pattern Recognition},
  title        = {Towards robust and sparse linear discriminant analysis for image classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory-adaptive vision-and-language navigation. <em>PR</em>,
<em>153</em>, 110511. (<a
href="https://doi.org/10.1016/j.patcog.2024.110511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-and-Language Navigation (VLN) requests an agent to navigate in 3D environments following given instructions, where history is critical for decision-making in dynamic navigation process. Particularly, a memory bank storing histories is widely used in existing methods to incorporate with multimodel representations in current scenes for better decision-making. However, by weighting each history with a simple scalar, those methods cannot purely utilize the informative cues that co-exist with detrimental contents in each history, thereby inevitably introducing noises into decision-making. To that end, we propose a novel Memory-Adaptive Model (MAM) that can dynamically restrain the detrimental contents in histories for retaining contents that benefit navigation only. Specifically, two key modules, Visual and Textual Adaptive Modules, are designed to restrain history noises based on scene-related vision and text, respectively. A Reliability Estimator Module is further introduced to refine above adaptation operations. Our experiments on the widely used RxR and R2R datasets show that MAM outperforms its baseline method by 4.0% / 2.5% and 2% / 1% on the validation unseen/test split, respectively, wrt the SR metric.},
  archive      = {J_PR},
  author       = {Keji He and Ya Jing and Yan Huang and Zhihe Lu and Dong An and Liang Wang},
  doi          = {10.1016/j.patcog.2024.110511},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110511},
  shortjournal = {Pattern Recognition},
  title        = {Memory-adaptive vision-and-language navigation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing performance of vision transformers on small
datasets through local inductive bias incorporation. <em>PR</em>,
<em>153</em>, 110510. (<a
href="https://doi.org/10.1016/j.patcog.2024.110510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers (ViTs) achieve remarkable performance on large datasets, but tend to perform worse than convolutional neural networks (CNNs) when trained from scratch on smaller datasets, possibly due to a lack of local inductive bias in the architecture. Recent studies have therefore added locality to the architecture and demonstrated that it can help ViTs achieve performance comparable to CNNs in the small-size dataset regime. Existing methods, however, are architecture-specific or have higher computational and memory costs. Thus, we propose a module called Local InFormation Enhancer (LIFE) that extracts patch-level local information and incorporates it into the embeddings used in the self-attention block of ViTs. Our proposed module is memory and computation efficient, as well as flexible enough to process auxiliary tokens such as the classification and distillation tokens. Empirical results show that the addition of the LIFE module improves the performance of ViTs on small image classification datasets. We further demonstrate how the effect can be extended to downstream tasks, such as object detection and semantic segmentation. In addition, we introduce a new visualization method, Dense Attention Roll-Out, specifically designed for dense prediction tasks, allowing the generation of class-specific attention maps utilizing the attention maps of all tokens. The code for this project is available on Github ( https://github.com/NeurAI-Lab/LIFE https://github.com/NeurAI-Lab/LIFE).},
  archive      = {J_PR},
  author       = {Ibrahim Batuhan Akkaya and Senthilkumar S. Kathiresan and Elahe Arani and Bahram Zonooz},
  doi          = {10.1016/j.patcog.2024.110510},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110510},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing performance of vision transformers on small datasets through local inductive bias incorporation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SIAM: A parameter-free, spatial intersection attention
module. <em>PR</em>, <em>153</em>, 110509. (<a
href="https://doi.org/10.1016/j.patcog.2024.110509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms have been shown to play a crucial role in enhancing visual perception tasks. However, in most existing approaches, channel and spatial attention maps are estimated separately without considering the varying importance of each other. This results in a coarse attention weight for objects of interest from a holistic 3-D perspective. To address this issue, we propose a novel Parameter-free Spatial Intersection Attention Module (SIAM), which estimates 3D attention maps with spatial intersection using a parameter-free way. Specifically, SIAM first generates two independent mean queries from two spatial axes and views input as keys. Then, by computing a dot product between these mean queries and keys, SIAM generates two cross-dimension (channel and spatial) attention maps from two spatial directions and combines them into 3-D attention maps. By doing so, the produced attention maps reason important areas with spatial intersection, which can capture location-aware information to facilitate difficult objects’ location in the images. We evaluate our method in image classification, object detection, and object segmentation tasks. Extensive experimental results consistently demonstrate our approach is superior to its counterparts.},
  archive      = {J_PR},
  author       = {Gaoge Han and Shaoli Huang and Fang Zhao and Jinglei Tang},
  doi          = {10.1016/j.patcog.2024.110509},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110509},
  shortjournal = {Pattern Recognition},
  title        = {SIAM: A parameter-free, spatial intersection attention module},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SaliencyCut: Augmenting plausible anomalies for anomaly
detection. <em>PR</em>, <em>153</em>, 110508. (<a
href="https://doi.org/10.1016/j.patcog.2024.110508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection under the open-set scenario is a challenging task that requires learning discriminative features to detect anomalies that were even unseen during training. As a cheap yet effective approach, data augmentation has been widely used to create pseudo anomalies for better training of such models. Recent wisdom of augmentation methods focuses on generating random pseudo instances that may lead to a mixture of augmented instances with seen anomalies, or out of the typical range of anomalies. To address this issue, we propose a novel saliency-guided data augmentation method, SaliencyCut, to produce pseudo but more common anomalies that tend to stay in the plausible range of anomalies. Furthermore, we deploy a two-head learning strategy consisting of normal and anomaly learning heads to learn the anomaly score of each sample. Theoretical analyses show that this mechanism offers a more tractable and tighter lower bound of the data log-likelihood. We then design a novel patch-wise residual module in the anomaly learning head to extract and assess anomaly features from each sample, facilitating the learning of discriminative representations of anomaly instances. Extensive experiments conducted on six real-world anomaly detection datasets demonstrate the superiority of our method to competing methods under various settings. Codes are available at: https://github.com/yjnanan/SaliencyCut .},
  archive      = {J_PR},
  author       = {Jianan Ye and Yijie Hu and Xi Yang and Qiu-Feng Wang and Chao Huang and Kaizhu Huang},
  doi          = {10.1016/j.patcog.2024.110508},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110508},
  shortjournal = {Pattern Recognition},
  title        = {SaliencyCut: Augmenting plausible anomalies for anomaly detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mirrored x-net: Joint classification and contrastive
learning for weakly supervised GA segmentation in SD-OCT. <em>PR</em>,
<em>153</em>, 110507. (<a
href="https://doi.org/10.1016/j.patcog.2024.110507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning achieves impressive performance in medical image segmentation, but the training phase requires a large amount of annotated data with precise clinical definitions. Weakly supervised lesion segmentation aims to produce pixel-level masks by learning discriminatory information from weak annotations. In this paper, Mirrored X-Net, a novel segmentation model only supervised by image-level category labels, is proposed to segment the Geographic Atrophy (GA) regions in en-face projection of Spectral-Domain Optical Coherence Tomography (SD-OCT) data. Characterized by the dimension-asymmetric information in SD-OCT images, a novel Anisotropic Downsampling (ADS) is proposed to augment feature shapes. To extract the regions of normal retina for both images with and without lesions, we propose a contrastive learning module to assimilate the deep representation of normal tissue in the SD-OCT images and improve the class-wise difference of deep representation. Based on the contrastive learning module, Anomalous Probability Map (APM) can be obtained to draw the distribution of difference with normal tissue in images. We jointly train the image classification and contrastive learning module, and the final GA segmentation is refined based on the en-face projection of APM. The experimental results on two independent GA datasets demonstrate that the proposed weakly supervised model can produce satisfactory results, and obtain even higher accuracy than fully supervised approaches. The source code is available at https://github.com/maxiao0234/Mirrored-X-Net-pytorch .},
  archive      = {J_PR},
  author       = {Zexuan Ji and Xiao Ma and Theodore Leng and Daniel L. Rubin and Qiang Chen},
  doi          = {10.1016/j.patcog.2024.110507},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110507},
  shortjournal = {Pattern Recognition},
  title        = {Mirrored X-net: Joint classification and contrastive learning for weakly supervised GA segmentation in SD-OCT},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rebalancing network with knowledge stability for class
incremental learning. <em>PR</em>, <em>153</em>, 110506. (<a
href="https://doi.org/10.1016/j.patcog.2024.110506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class incremental learning (CIL) has been proposed to solve the problem of learning to classify new classes while maintaining the performance on old classes. A typical strategy is to update the old classification model with entire new class data and a few old exemplars, which face serious performance degradation on old classes. The main reasons come down to class imbalance between old and new classes along with catastrophic forgetting towards old classes. Most existing CIL methods have proposed solving the above two issues in classification space, ignoring their adverse effects of overlapping between old and new classes and confusion among old classes in feature space. In this paper, we propose a rebalancing network with knowledge stability (RNKS), aiming to adequately retain the model performance on old classes in CIL by solving class imbalance and catastrophic forgetting in feature and classification space simultaneously. In detail, the proposed RNKS mainly consists of multi-proxies rebalancing (MPR) and hybrid knowledge distillation (HKD). MPR, focusing on class imbalance, employs multi-proxies metric learning to decrease the feature overlapping between old and new classes, together with balanced data sampling to correct the skewed decision boundary. HKD, coping with catastrophic forgetting, encourages the updated model to reproduce identical feature topologies and predictions of old classes as the old model via feature relation-based and response-based distillations. Experiments on CIFAR-100 and ILSVRC datasets demonstrate the effectiveness of this work against the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Jialun Song and Jian Chen and Lan Du},
  doi          = {10.1016/j.patcog.2024.110506},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110506},
  shortjournal = {Pattern Recognition},
  title        = {Rebalancing network with knowledge stability for class incremental learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ranking-based problem transformation method for weakly
supervised multi-label learning. <em>PR</em>, <em>153</em>, 110505. (<a
href="https://doi.org/10.1016/j.patcog.2024.110505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem transformation is a simple yet effective framework for multi-label learning, where the original multi-label problem can be transformed into a series of single-label subproblems. However, the existing problem transformation methods have difficulties in handling label defect issues in real applications, e.g. multi-label learning with missing labels, partial multi-label learning and noisy multi-label learning. To deal with these issues, we propose a novel problem transformation method named EPR (i.e., Ensemble of Pairwise Ranking learners) applicable to various multi-label tasks. In EPR, the weakly supervised multi-label problem is converted into an ensemble of supervised single-label patterns due to pairwise label ranking, which successfully enhances label correlation exploration and improves the utilization of instances with defect labels. Moreover, an ensemble pruning mechanism is presented to heuristically balance the model performance and efficiency. Extensive experiments demonstrate the effectiveness of EPR against state-of-the-art algorithms in diverse multi-label learning scenarios.},
  archive      = {J_PR},
  author       = {Jiaxuan Li and Xiaoyan Zhu and Weichu Zhang and Jiayin Wang},
  doi          = {10.1016/j.patcog.2024.110505},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110505},
  shortjournal = {Pattern Recognition},
  title        = {A ranking-based problem transformation method for weakly supervised multi-label learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust deep fuzzy k-means clustering for image data.
<em>PR</em>, <em>153</em>, 110504. (<a
href="https://doi.org/10.1016/j.patcog.2024.110504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image clustering is a difficult task with important application value in computer vision. The key to this task is the quality of images features. Most of current clustering methods encounter the challenge. That is, the process of feature learning and clustering operates independently. To address this problem, several researchers have been dedicated to performing feature learning and deep clustering together. However, the obtained features lack discriminability to address high-dimensional data successfully. To deal with this issue, we propose a novel model named as robust deep fuzzy K K -means clustering (RD-FKC), which efficiently projects image samples into a representative embedding space and precisely learns membership degrees into a combined framework. Specifically, RD-FKC introduces Laplacian regularization technique to preserve locality properties of data. Moreover, by using an adaptive loss function, the model becomes more robust to diverse types of outliers. Furthermore, to avoid the latent space being distorted and make the extracted features retain the original information as much as possible, the model introduces reconstruction error and adds regularization to network parameters. Finally, an effective algorithm is derived to solve the optimization model. Numerous experiments have been conducted, illustrating the advantages and superiority of RD-FKC over existing clustering approaches.},
  archive      = {J_PR},
  author       = {Xiaoling Wu and Yu-Feng Yu and Long Chen and Weiping Ding and Yingxu Wang},
  doi          = {10.1016/j.patcog.2024.110504},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110504},
  shortjournal = {Pattern Recognition},
  title        = {Robust deep fuzzy K-means clustering for image data},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Dynamic image super-resolution via progressive contrastive
self-distillation. <em>PR</em>, <em>153</em>, 110502. (<a
href="https://doi.org/10.1016/j.patcog.2024.110502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are highly successful for image super-resolution (SR). However, they often require sophisticated architectures with high memory cost and computational overhead, significantly restricting their practical deployments on resource-limited devices. In this paper, we propose a novel dynamic contrastive self-distillation (Dynamic-CSD) framework to simultaneously compress and accelerate various off-the-shelf SR models, and explore using the trained model for dynamic inference. In particular, to build a compact student network, a channel-splitting super-resolution network (CSSR-Net) can first be constructed from a target teacher network. Then, we propose a novel contrastive loss to improve the quality of SR images via explicit knowledge transfer. Furthermore, progressive CSD (Pro-CSD) is developed to extend the two-branch CSSR-Net into multi-branch, leading to a switchable model at runtime. Finally, a difficulty-aware branch selection strategy for dynamic inference is given. Extensive experiments demonstrate that the proposed Dynamic-CSD scheme effectively compresses and accelerates several standard SR models such as EDSR, RCAN and CARN.},
  archive      = {J_PR},
  author       = {Zhizhong Zhang and Yuan Xie and Chong Zhang and Yanbo Wang and Yanyun Qu and Shaohui Lin and Lizhuang Ma and Qi Tian},
  doi          = {10.1016/j.patcog.2024.110502},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110502},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic image super-resolution via progressive contrastive self-distillation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative data augmentation by conditional inpainting for
multi-class object detection in infrared images. <em>PR</em>,
<em>153</em>, 110501. (<a
href="https://doi.org/10.1016/j.patcog.2024.110501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class object detection in infrared images is important in military and civilian use. Deep learning methods can obtain high accuracy but require a large-scale dataset. We propose a generative data augmentation framework DOCI-GAN, for infrared multi-class object detection with limited data. Contributions of this paper are four-folds. Firstly, DOCI-GAN is designed as a conditional image inpainting framework, yielding paired infrared multi-class object image and annotation. Secondly, a text-to-image converter is formulated to transform text-format object annotations to bounding box mask images, leading the augmentation to be mask-image-to-raw-image translation. Thirdly, a multiscale morphological erosion-based loss is created to alleviate the intensity inconsistency between inpainted local backgrounds and global background. Finally, for generating diverse images, artificial multi-class object annotations are integrated with real ones during augmentation. Experimental results demonstrated that DOCI-GAN augments dataset with high-quality infrared multi-class object images, consequently improving the accuracy of object detection baselines.},
  archive      = {J_PR},
  author       = {Peng Wang and Zhe Ma and Bo Dong and Xiuhua Liu and Jishiyu Ding and Kewu Sun and Ying Chen},
  doi          = {10.1016/j.patcog.2024.110501},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110501},
  shortjournal = {Pattern Recognition},
  title        = {Generative data augmentation by conditional inpainting for multi-class object detection in infrared images},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual GroupGAN: An unsupervised four-competitor (2V2)
approach for video anomaly detection. <em>PR</em>, <em>153</em>, 110500.
(<a href="https://doi.org/10.1016/j.patcog.2024.110500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the issues of overgeneralization in reconstruction-based methods and noise sensitivity in prediction-based methods for video anomaly detection, this paper proposes a novel unsupervised video anomaly detection approach using dual GroupGAN, refers to a four-competitor (2V2), based on channel attention mechanism. Our appraoch incorporates a channel attention mechanism into two generators, namely the SE-U-Net and SE-VAE, which respectively serve as the prediction and reconstruction networks. The SE-U-Net captures essential spatio-temporal features and automatically calibrates the channel dimension, while the SE-VAE learns global features from associated video frames. A weighting strategy is used to fuse the anomaly scores of the two networks and balance their emphasis on spatio-temporal feature representation. To wrap up, the proposed prediction network (SE-U-Net) is resistant to overgeneralization and improves quality of the reconstruction network (SE-VAE) when using the prediction frame as the input of SE-VAE. Also, the SE-VAE enhances predicted future frames from normal events, thereby increasing the robustness of the SE-U-Net. Experimental results from UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets demonstrate the effectiveness of the proposed approach both qualitatively and quantitatively.},
  archive      = {J_PR},
  author       = {Zhe Sun and Panpan Wang and Wang Zheng and Meng Zhang},
  doi          = {10.1016/j.patcog.2024.110500},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110500},
  shortjournal = {Pattern Recognition},
  title        = {Dual GroupGAN: An unsupervised four-competitor (2V2) approach for video anomaly detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELMP-net: The successive application of a randomized local
transform for texture classification. <em>PR</em>, <em>153</em>, 110499.
(<a href="https://doi.org/10.1016/j.patcog.2024.110499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a method for texture classification based on the successive application of a local transform presented here for the first time. Such transform comprises two steps: (1) We built a two-layer mapping relating each pixel with its neighborhood, with the weights in the first layer randomly assigned; (2) We use the parameters learned by such mapping to transform the original image. Finally, we extract local descriptors at different stages of the successive application of this transform to compose the texture descriptors. The performance of our method is verified in the classification of benchmark texture databases and compared with state-of-the-art approaches. We also present an application for plant species identification. The results confirm our expectation that a model that is not based on the classical learning-based approach can still be competitive in texture analysis.},
  archive      = {J_PR},
  author       = {Joao B. Florindo and Andre R. Backes and Acacio Neckel},
  doi          = {10.1016/j.patcog.2024.110499},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110499},
  shortjournal = {Pattern Recognition},
  title        = {ELMP-net: The successive application of a randomized local transform for texture classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information filtering and interpolating for semi-supervised
graph domain adaptation. <em>PR</em>, <em>153</em>, 110498. (<a
href="https://doi.org/10.1016/j.patcog.2024.110498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph domain adaptation, which falls under the umbrella of graph transfer learning, involves transferring knowledge from a labeled source graph to improve prediction accuracy on an unlabeled target graph, where both graphs have identical label spaces but exhibit distribution discrepancies due to temporal data shifts or distinct data collection methods. This adaptation is complicated by the challenges of graph-specific domain discrepancies and cross-graph label scarcity. This paper proposes a semi-supervised G raph domain adaptation method via I nformation F iltering and I nterpolating (GIFI). Specifically, GIFI utilizes a parameterized graph reduction module and a variational information bottleneck to adequately filter out irrelevant information from the source and target graphs to eliminate distribution discrepancy. GIFI also introduces an interpolation-enhanced pseudo-labeling strategy for cross-graph semi-supervised learning, which can mitigate model over-fitting on domain-specific features and limited labeled nodes, thus improving the model’s adaptation and discriminative capability. Experimental results on various graph domain adaptation benchmarks demonstrate GIFI’s superior performance over state-of-the-art methods. Our code is available at https://github.com/joe817/GIFI .},
  archive      = {J_PR},
  author       = {Ziyue Qiao and Meng Xiao and Weiyu Guo and Xiao Luo and Hui Xiong},
  doi          = {10.1016/j.patcog.2024.110498},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110498},
  shortjournal = {Pattern Recognition},
  title        = {Information filtering and interpolating for semi-supervised graph domain adaptation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SurvivalLVQ: Interpretable supervised clustering and
prediction in survival analysis via learning vector quantization.
<em>PR</em>, <em>153</em>, 110497. (<a
href="https://doi.org/10.1016/j.patcog.2024.110497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying subgroups with similar survival outcomes is a pivotal challenge in survival analysis. Traditional clustering methods often neglect the outcome variable, potentially leading to inaccurate representation of risk profiles. To address this, we present SurvivalLVQ, a novel interpretable method that adapts Learning Vector Quantization (LVQ) to survival analysis. Unlike traditional classification uses of LVQ, SurvivalLVQ groups individuals by survival probabilities and assigns a unique survival curve to each cluster, representing the collective survival behavior within that group. Moreover, it can predict individual survival curves using weighted averages from nearby clusters. When tested on 76 benchmark datasets, it outperformed other clustering methods and showed competitive prediction performance. SurvivalLVQ bridges the gap between clustering techniques and outcome-oriented methods. Its strong clustering performance, coupled with competitive prediction capabilities and with easy to interpret outcomes, make it a promising tool for various applications within survival analysis.},
  archive      = {J_PR},
  author       = {Jasper de Boer and Klest Dedja and Celine Vens},
  doi          = {10.1016/j.patcog.2024.110497},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110497},
  shortjournal = {Pattern Recognition},
  title        = {SurvivalLVQ: Interpretable supervised clustering and prediction in survival analysis via learning vector quantization},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Boosting sharpness-aware training with dynamic
neighborhood. <em>PR</em>, <em>153</em>, 110496. (<a
href="https://doi.org/10.1016/j.patcog.2024.110496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning algorithms motivated by minimizing the sharpness of loss surface is a hot research topic in improving generalization. The existing methods usually solve a constrained min–max problem to minimize sharpness and find flat minima. However, most constraints (i.e., the neighborhood of the sharpness) are inappropriate, leading to sub-optimal results. This paper theoretically explores the optimal neighborhood from the view of Probably Approximately Correct-Bayesian (PAC-Bayesian) framework. A closed form of the optimal neighborhood is provided. This neighborhood is determined by the Hessian matrix and the scales of parameters. Then a generalization bound is derived that serves as a guiding principle in the design of the sharpness minimization algorithm. The Dynamic neighborhood-based Sharpness-Aware Minimization algorithm is proposed, which can adaptively adjust the neighborhood during the training process to gain better performance. Also, the algorithm is proved can convergent at the rate O ( log T / T ) O(logT/T) . Experimental results demonstrate that the proposed algorithm outperforms the other methods (e.g., accuracy ＋2.86% over baseline on CIFAR-100 for VGG-16).},
  archive      = {J_PR},
  author       = {Junhong Chen and Hong Li and C.L. Philip Chen},
  doi          = {10.1016/j.patcog.2024.110496},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110496},
  shortjournal = {Pattern Recognition},
  title        = {Boosting sharpness-aware training with dynamic neighborhood},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint magnetic resonance imaging artifacts and noise
reduction on discrete shape space of images. <em>PR</em>, <em>153</em>,
110495. (<a href="https://doi.org/10.1016/j.patcog.2024.110495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance (MR) images can be corrupted by artifacts and noise, potentially leading to misinterpretation of the images. In this paper, we propose a novel approach based on the discrete shape space of images (DSSI) to jointly reduce artifacts and noise in MR images. The proposed method restores MR images in multiple domains based on the distinct generation mechanisms of noise and artifacts. The images in multiple domains are analyzed in a non-Euclidean space. The DSSI is constructed as a Riemannian manifold to measure the intrinsic properties of images. Images are considered shapes from a geometric perspective, and the impact of similarity transformations (e.g., rotation, scaling, and translation) on image analysis is eliminated. The patch-based rank-ordered difference (PROD) detector is defined in k-space within the framework of DSSI to detect and remove sparse outliers that cause artifacts. In addition, a novel similarity function for images is defined using the DSSI and be used to design the improved filter. Finally, the convergence of the improved filter is theoretically analyzed, indicating that our method offers an effective estimator of the ideal image. The experimental results of various MR images demonstrate that the proposed approach outperforms classical and state-of-the-art methods for artifact correction and noise removal, both qualitatively and quantitatively.},
  archive      = {J_PR},
  author       = {Xiangyuan Liu and Zhongke Wu and Xingce Wang and Quansheng Liu and Jose M. Pozo and Alejandro F. Frangi},
  doi          = {10.1016/j.patcog.2024.110495},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110495},
  shortjournal = {Pattern Recognition},
  title        = {Joint magnetic resonance imaging artifacts and noise reduction on discrete shape space of images},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessment of volumetric dense tissue segmentation in
tomosynthesis using deep virtual clinical trials. <em>PR</em>,
<em>153</em>, 110494. (<a
href="https://doi.org/10.1016/j.patcog.2024.110494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of artificial intelligence (AI) in medical imaging requires careful evaluation of machine-learning algorithms. We propose the use of a “deep virtual clinical trial” (DeepVCT) method to effectively evaluate the performance of AI algorithms. In this paper, DeepVCTs have been proposed to elucidate limitations of AI applications and predictions of clinical outcomes, avoiding biases in study designs. The DeepVCT method was used to evaluate the performance of nnU-Net models in assessing volumetric breast density (VBD) from digital breast tomosynthesis (DBT) images. In total, 2010 anatomical breast models were simulated. Projections were simulated using the acquisition geometry of a clinical DBT system. The projections were reconstructed using 0.1, 0.2, and 0.5 mm plane spacing. nnU-Net models were developed using the center-most planes of the reconstructions with the respective ground-truth. The results show that the accuracy of the nnU-Net improves significantly with DBT images reconstructed with 0.1 mm plane spacing ( 78 . 4 × 205 . 3 × 40 . 1 78.4×205.3×40.1 mm 3 ). The segmentations resulted in Dice values up to 0.84 with area under the receiver operating characteristic curve of 0.92. The optimization of plane spacing for VBD assessment was used as an exemplar of a DeepVCT application, allowing better interpretation of the input parameters and outcomes of the nnU-Net. Thus, DeepVCTs can provide evidence to predict the efficacy of AI algorithms using large-scale simulation-based data.},
  archive      = {J_PR},
  author       = {B. Barufaldi and J.V. Gomes and T.M. Silva Filho and T.G. do Rêgo and Y. Malheiros and T.L. Vent and A. Gastounioti and A.D.A. Maidment},
  doi          = {10.1016/j.patcog.2024.110494},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110494},
  shortjournal = {Pattern Recognition},
  title        = {Assessment of volumetric dense tissue segmentation in tomosynthesis using deep virtual clinical trials},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental convolutional transformer for baggage threat
detection. <em>PR</em>, <em>153</em>, 110493. (<a
href="https://doi.org/10.1016/j.patcog.2024.110493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting cluttered and overlapping contraband items from baggage scans is one of the most challenging tasks, even for human experts. Recently, considerable literature has grown up around the theme of deep learning-based X-ray screening for localizing contraband data. However, the existing threat detection systems are still vulnerable to high occlusion, clutter, and concealment. Furthermore, they require exhaustive training routines on large-scale and well-annotated data in order to produce accurate results. To overcome the above-mentioned limitations, this paper presents a novel convolutional transformer system that recognizes different overlapping instances of prohibited objects in complex baggage X-ray scans via a distillation-driven incremental instance segmentation scheme. Furthermore, unlike its competitors, the proposed framework allows an incremental integration of new item instances while avoiding costly training routines. In addition to this, the proposed framework also outperforms state-of-the-art approaches by achieving a mean average precision score of 0.7896, 0.5974, and 0.7569 on publicly available GDXray, SIXray, and OPIXray datasets for detecting concealed and cluttered baggage threats.},
  archive      = {J_PR},
  author       = {Taimur Hassan and Bilal Hassan and Muhammad Owais and Divya Velayudhan and Jorge Dias and Mohammed Ghazal and Naoufel Werghi},
  doi          = {10.1016/j.patcog.2024.110493},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110493},
  shortjournal = {Pattern Recognition},
  title        = {Incremental convolutional transformer for baggage threat detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lit me up: A reference free adaptive low light image
enhancement for in-the-wild conditions. <em>PR</em>, <em>153</em>,
110490. (<a href="https://doi.org/10.1016/j.patcog.2024.110490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured with different devices in uneven conditions (e.g., invariable lighting, low lighting, weather changes, exposure time, etc.) often lead to low image visibility and poor color and contrast, affecting the performance of computer vision and pattern recognition applications. The pre-trained convolutional neural networks (CNNs) solely rely on the training data and lack adaptation due to the uncertainty in the lighting conditions. Moreover, capturing large-scale datasets to train CNNs also raises the computational complexity and overall cost. This work integrates the knowledge and data and proposes a two-stage Uneven-to-Enliven network (U2E-Net), which rapidly learns to see in uneven conditions. A multiple-layered Uneven network learns to distinguish the reflection and illumination in the input images, and an encoder–decoder-based Enliven-Net contextualizes the illumination information. A key component in such ill-posed problems is to obtain information from priors and pairs; however, we present the compelling idea of information trade-off followed by decomposition consistency, thereby progressively improving the visual quality with the subsequent enhancement operations. To this end, we proposed a two-faceted framework that can work without depending on the data type. A novel color and contrast preservation strategy (CPS) is proposed following the decomposition of input data. CPS is integrated within the network to extract contrast in the darkest background regions.},
  archive      = {J_PR},
  author       = {Rizwan Khan and Atif Mehmood and Farah Shahid and Zhonglong Zheng and Mostafa M. Ibrahim},
  doi          = {10.1016/j.patcog.2024.110490},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110490},
  shortjournal = {Pattern Recognition},
  title        = {Lit me up: A reference free adaptive low light image enhancement for in-the-wild conditions},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft independence guided filter pruning. <em>PR</em>,
<em>153</em>, 110488. (<a
href="https://doi.org/10.1016/j.patcog.2024.110488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Filter pruning (FP) is an effective method for reducing the computational costs of convolutional neural networks, and herein, the most critical task involves evaluating the significance of each convolutional filter and eliminating the less important ones while minimizing performance degradation. Most existing FP methods consider only local information, which may prevent them from accurately recognizing the most important filters. To address this limitation, we propose the soft filter independence (SFI) method, which leverages global information to identify the most important filters using their magnitude and correlation information in different functional layers. The SFI criterion measures the replaceability of filters from a global perspective in a network. Filters with low independence can be represented effectively by others, so their information can be accurately conveyed by other filters. In addition, we introduce a novel SFI-based asymptotic pruning ratio, which improves training and pruning stability. Compared to the most advanced FP methods, our method enables CNNs to achieve higher pruning rates and better classification performance.},
  archive      = {J_PR},
  author       = {Liu Yang and Shiqiao Gu and Chenyang Shen and Xile Zhao and Qinghua Hu},
  doi          = {10.1016/j.patcog.2024.110488},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110488},
  shortjournal = {Pattern Recognition},
  title        = {Soft independence guided filter pruning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Self-reconstruction network for fine-grained few-shot
classification. <em>PR</em>, <em>153</em>, 110485. (<a
href="https://doi.org/10.1016/j.patcog.2024.110485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based methods are one of the most common methods to solve the problem of few-shot image classification. However, traditional metric-based few-shot methods suffer from overfitting and local feature misalignment. The recently proposed feature reconstruction-based approach, which reconstructs query image features from the support set features of a given class and compares the distance between the original query features and the reconstructed query features as the classification criterion, effectively solves the feature misalignment problem. However, the issue of overfitting still has not been considered. To this end, we propose a self-reconstruction metric module for diversifying query features and a restrained cross-entropy loss for avoiding over-confident predictions. By introducing them, the proposed self-reconstruction network can effectively alleviate overfitting. Extensive experiments on five benchmark fine-grained datasets demonstrate that our proposed method achieves state-of-the-art performance on both 5-way 1-shot and 5-way 5-shot classification tasks. Code is available at https://github.com/liz-lut/SRM-main .},
  archive      = {J_PR},
  author       = {Xiaoxu Li and Zhen Li and Jiyang Xie and Xiaochen Yang and Jing-Hao Xue and Zhanyu Ma},
  doi          = {10.1016/j.patcog.2024.110485},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110485},
  shortjournal = {Pattern Recognition},
  title        = {Self-reconstruction network for fine-grained few-shot classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Feature decoupling and regeneration towards wifi-based
human activity recognition. <em>PR</em>, <em>153</em>, 110480. (<a
href="https://doi.org/10.1016/j.patcog.2024.110480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition using WiFi is vital for human–computer interaction, smart homes, smart spaces, and elderly care. WiFi signals are non-stationary, sensitive to the environment, and traditional pattern recognition-based HAR methods incur exorbitant training and deployment costs due to their reliance on the quality and quantity of training data. While there has been extensive research on enhancing HAR techniques, the efficiency of the system is still limited by the scarcity of training data and identity-action feature coupling. This research focuses on attaining gesture recognition across multiple users with a limited number of samples. Our findings show that the conventional data augmentation methods are incapable of facilitating the system in attaining sample diversity. As a result, we propose a training-free augmentation strategy as a means to provide adequate training data. Unlike the conventional data enhancement approach, this scheme aims to develop data processing methods that differentiate between various samples in practical applications. Consequently, it enhances data pertinent to specific HAR assignments effectively. To effectively extract action features in WiFi samples, an unsupervised c ross- u ser d omain s ample g eneration (CUDSG) model is proposed. This model generates virtual gesture samples for new user domains by decoupling and recombining gesture and identity features. This extends the sensing boundary of the system to new user domains without requiring a significant number of users to participate. Model performance was evaluated using various classifiers, such as SVM, KNN and CNN. The results demonstrate a significant improvement in average classification accuracy from 57.3% to 98.4%. This indicates that CUDSG is a highly effective tool for enhancing the performance of existing gesture recognition techniques.},
  archive      = {J_PR},
  author       = {Siyang Wang and Lin Wang and Wenyuan Liu},
  doi          = {10.1016/j.patcog.2024.110480},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110480},
  shortjournal = {Pattern Recognition},
  title        = {Feature decoupling and regeneration towards wifi-based human activity recognition},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PLoPS: Localization-aware person search with prototypical
normalization. <em>PR</em>, <em>153</em>, 110479. (<a
href="https://doi.org/10.1016/j.patcog.2024.110479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search involves localizing and re-identifying persons of interest captured by multiple, non-overlapping cameras. Recent approaches to person search are typically built on object detection frameworks to learn joint person representations for detection and re-identification. To this end, the features extracted from pedestrian proposals are projected onto a unit hypersphere using L2 normalization, and positive proposals that sufficiently overlap with the ground truth are equally incorporated for training by exploiting an external lookup table (LUT). We have found that (1) using the L2 normalization technique, without considering feature distributions, can degenerate the discriminative power of person representations, (2) positive proposals often depict distracting details, such as background clutter and person overlaps, and (3) person features in the LUT are not often updated during training. To address these limitations, we propose a novel framework for person search, dubbed PLoPS, using a prototypical normalization layer, ProtoNorm, that calibrates features while considering the long-tail distribution across person IDs. PLoPS also entails a localization-aware learning scheme that prioritizes better-aligned proposals w.r.t the ground truth. We further introduce a LUT calibration technique to continuously adjust the person features in the LUT. Experimental results and analysis on standard benchmarks demonstrate the effectiveness of PLoPS.},
  archive      = {J_PR},
  author       = {Sanghoon Lee and Youngmin Oh and Donghyeon Baek and Junghyup Lee and Bumsub Ham},
  doi          = {10.1016/j.patcog.2024.110479},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110479},
  shortjournal = {Pattern Recognition},
  title        = {PLoPS: Localization-aware person search with prototypical normalization},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth-aware guidance with self-estimated depth
representations of diffusion models. <em>PR</em>, <em>153</em>, 110474.
(<a href="https://doi.org/10.1016/j.patcog.2024.110474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have recently shown significant advancement in the generative models with their impressive fidelity and diversity. The success of these models can be often attributed to their use of sampling guidance techniques, such as classifier or classifier-free guidance, which provide effective mechanisms to trade-off between fidelity and diversity. However, these methods are not capable of guiding a generated image to be aware of its geometric configuration, e.g., depth, which hinders their application to downstream tasks such as scene understanding that require a certain level of depth awareness. To overcome this limitation, we propose a novel sampling guidance method for diffusion models that uses self-predicted depth information derived from the rich intermediate representations of diffusion models. Concretely, we first present a label-efficient depth estimation framework using internal representations of diffusion models. Subsequently, we propose the incorporation of two guidance techniques during the sampling phase. These methods involve using pseudo-labeling and depth-domain diffusion prior to self-condition the generated image using the estimated depth map. Experiments and comprehensive ablation studies demonstrate the effectiveness of our method in guiding the diffusion models toward the generation of geometrically plausible images. Our project page is available at https://ku-cvlab.github.io/DAG/ .},
  archive      = {J_PR},
  author       = {Gyeongnyeon Kim and Wooseok Jang and Gyuseong Lee and Susung Hong and Junyoung Seo and Seungryong Kim},
  doi          = {10.1016/j.patcog.2024.110474},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110474},
  shortjournal = {Pattern Recognition},
  title        = {Depth-aware guidance with self-estimated depth representations of diffusion models},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learners for few-shot weakly-supervised medical image
segmentation. <em>PR</em>, <em>153</em>, 110471. (<a
href="https://doi.org/10.1016/j.patcog.2024.110471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most uses of Meta-Learning in visual recognition are very often applied to image classification, with a relative lack of work in other tasks such as segmentation and detection. We propose a new generic Meta-Learning framework for few-shot weakly supervised segmentation in medical imaging domains. The proposed approach includes a meta-training phase that uses a meta-dataset. It is deployed on an out-of-distribution few-shot target task, where a single highly generalizable model, trained via a selective supervised loss function, is used as a predictor. The model can be trained in several distinct ways, such as second-order optimization, metric learning, and late fusion. Some relevant improvements of existing methods that are part of the proposed approach are presented. We conduct a comparative analysis of meta-learners from distinct paradigms adapted to few-shot image segmentation in different sparsely annotated radiological tasks. The imaging modalities include 2D chest, mammographic, and dental X-rays, as well as 2D slices of volumetric tomography and resonance images. Our experiments consider in total 9 meta-learners, 4 backbones, and multiple target organ segmentation tasks. We explore small-data scenarios in radiology with varying weak annotation styles and densities. Our analysis shows that metric-based meta-learning approaches achieve better segmentation results in tasks with smaller domain shifts compared to the meta-training datasets, while some gradient- and fusion-based meta-learners are more generalizable to larger domain shifts. Guidelines learned from the comparative performance assessment of the analyzed methods are summarized to support those readers interested in the field.},
  archive      = {J_PR},
  author       = {Hugo Oliveira and Pedro H.T. Gama and Isabelle Bloch and Roberto Marcondes Cesar Jr.},
  doi          = {10.1016/j.patcog.2024.110471},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110471},
  shortjournal = {Pattern Recognition},
  title        = {Meta-learners for few-shot weakly-supervised medical image segmentation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ScopeViT: Scale-aware vision transformer. <em>PR</em>,
<em>153</em>, 110470. (<a
href="https://doi.org/10.1016/j.patcog.2024.110470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale features are essential for various vision tasks, such as classification, detection, and segmentation. Although Vision Transformers (ViTs) show remarkable success in capturing global features within an image, how to leverage multi-scale features in Transformers is not well explored. This paper proposes a scale-aware vision Transformer called ScopeViT that efficiently captures multi-granularity representations. Two novel attention with lightweight computation are introduced: Multi-Scale Self-Attention (MSSA) and Global-Scale Dilated Attention (GSDA). MSSA embeds visual tokens with different receptive fields into distinct attention heads, allowing the model to perceive various scales across the network. GSDA enhances model understanding of the global context through token-dilation operation, which reduces the number of tokens involved in attention computations. This dual attention method enables ScopeViT to “see” various scales throughout the entire network and effectively learn inter-object relationships, reducing heavy quadratic computational complexity. Extensive experiments demonstrate that ScopeViT achieves competitive complexity/accuracy trade-offs compared to existing networks across a wide range of visual tasks. On the ImageNet-1K dataset, ScopeViT achieves a top-1 accuracy of 81.1%, using only 7.4M parameters and 2.0G FLOPs. Our approach outperforms Swin (ViT-based) by 1.9% accuracy while saving 42% of the parameters, outperforms MobileViTv2 (Hybrid-based) with a 0.7% accuracy gain while using 50% of the computations, and also beats ConvNeXt V2 (ConvNet-based) by 0.8% with fewer parameters.},
  archive      = {J_PR},
  author       = {Xuesong Nie and Haoyuan Jin and Yunfeng Yan and Xi Chen and Zhihang Zhu and Donglian Qi},
  doi          = {10.1016/j.patcog.2024.110470},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110470},
  shortjournal = {Pattern Recognition},
  title        = {ScopeViT: Scale-aware vision transformer},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Seizure detection via deterministic learning feature
extraction. <em>PR</em>, <em>153</em>, 110466. (<a
href="https://doi.org/10.1016/j.patcog.2024.110466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epileptic seizures have a significant impact on the well-being of a large number of individuals worldwide. Utilizing electroencephalographic (EEG) signals for automatic seizure detection proves to be a valuable solution. However, dealing with raw EEG signals is inherently complex, necessitating a preliminary step of feature extraction prior to detection. Traditional feature extraction methods often amalgamate various types of features for seizure detection, as each type typically captures specific properties. In contrast, this paper focuses on detecting seizures by analyzing the system dynamics. The proposed Deterministic Learning Feature Extraction (DLFE) method extracts a single type of nonlinear dynamical feature rooted in the EEG system dynamics. DLFE employs deterministic learning to discern the inherent system dynamics of the EEG under both seizure and normal conditions. Through the feature extraction process, the infinite-dimensional system dynamics are transformed into feature vectors, exhibiting distinct distributions in seizure and normal states. This disparity can be effectively utilized for classification using standard classifiers. The performance of the proposed seizure detection method was assessed using the CHB-MIT and Bonn datasets. The average classification accuracy was found to be 98.63% with a specificity of 99.19% and a sensitivity of 98.06% on CHB-MIT dataset. Compared with the latest similar methods, the accuracy, specificity and sensitivity are improved by 0.31%, 0.21% and 0.05% respectively. Moreover, the performance was achieved with the short-time interval EEG signals within a few channels. The average classification accuracy was found to be 99.90% with a 0.22% improvement on Bonn dataset, which indicates the good generalization performance.},
  archive      = {J_PR},
  author       = {Zirui Zhang and Weiming Wu and Chen Sun and Cong Wang},
  doi          = {10.1016/j.patcog.2024.110466},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110466},
  shortjournal = {Pattern Recognition},
  title        = {Seizure detection via deterministic learning feature extraction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IBVC: Interpolation-driven b-frame video compression.
<em>PR</em>, <em>153</em>, 110465. (<a
href="https://doi.org/10.1016/j.patcog.2024.110465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learned B-frame video compression aims to adopt bi-directional motion estimation and motion compensation (MEMC) coding for middle frame reconstruction. However, previous learned approaches often directly extend neural P-frame codecs to B-frame relying on bi-directional optical-flow estimation or video frame interpolation. They suffer from inaccurate quantized motions and inefficient motion compensation. To address these issues, we propose a simple yet effective structure called Interpolation-driven B-frame Video Compression (IBVC). Our approach only involves two major operations: video frame interpolation and artifact reduction compression. IBVC introduces a bit-rate free MEMC based on interpolation, which avoids optical-flow quantization and additional compression distortions. Later, to reduce duplicate bit-rate consumption and focus on unaligned artifacts, a residual guided masking encoder is deployed to adaptively select the meaningful contexts with interpolated multi-scale dependencies. In addition, a conditional spatio-temporal decoder is proposed to eliminate location errors and artifacts instead of using MEMC coding in other methods. The experimental results on B-frame coding demonstrate that IBVC has significant improvements compared to the relevant state-of-the-art methods. Meanwhile, our approach can save bit rates compared with the random access (RA) configuration of H.266 (VTM). The code will be available at https://github.com/ruhig6/IBVC .},
  archive      = {J_PR},
  author       = {Chenming Xu and Meiqin Liu and Chao Yao and Weisi Lin and Yao Zhao},
  doi          = {10.1016/j.patcog.2024.110465},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110465},
  shortjournal = {Pattern Recognition},
  title        = {IBVC: Interpolation-driven B-frame video compression},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orthogonal subspace exploration for matrix completion.
<em>PR</em>, <em>153</em>, 110456. (<a
href="https://doi.org/10.1016/j.patcog.2024.110456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix completion, aiming at restoring a low-rank matrix from observed entries, indicates the connection with the subspace clustering due to the low-rank property. However, it is expensive to incorporate subspace learning into the pervasive surrogate of matrix completion, the nuclear norm. In this paper, we design an orthogonal subspace exploration model for matrix completion, which can be easily integrated due to the succinct formulation. Then, we propose a non-convex surrogate with tractable solutions for low-rank matrix completion, so that the subspace exploration can be performed simultaneously. Compared with the existing surrogates ( e.g. , nuclear norm, Schatten- p p norm, max norm, etc. ), the proposed surrogate is differential such that the optimization is still simple even after the subspace exploration is incorporated. Although the surrogate is non-convex, a parameter-free algorithm that is proved to converge into the global optimum is developed. The optimization consists of closed-form solutions so that the orthogonal subspace exploration will not distinctly bring additional costs and the algorithm empirically converges within dozens of iterations. Experiments illustrate the efficiency and superiority of our model.},
  archive      = {J_PR},
  author       = {Hongyuan Zhang and Ziheng Jiao and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110456},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110456},
  shortjournal = {Pattern Recognition},
  title        = {Orthogonal subspace exploration for matrix completion},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid federated learning with brain-region attention
network for multi-center alzheimer’s disease detection. <em>PR</em>,
<em>153</em>, 110423. (<a
href="https://doi.org/10.1016/j.patcog.2024.110423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying reproducible and interpretable biomarkers for Alzheimer&#39;s disease (AD) detection remains a challenge. AD detection using multi-center datasets can expand the sample size to improve robustness but might lead to a data privacy problem. Moreover, due to the high cost of labeling data, a lot of unlabeled data in each center is not fully utilized. To address this, a hybrid FL (HFL) framework is proposed that not only uses unlabeled data to train deep learning networks, but also achieves data privacy protection. We propose a novel Brain-region Attention Network (BANet), which highlights important regions via attention to represent the region of interest (ROIs).Specifically, we use a brain template to extract ROI signals from the preprocessed structure magnetic resonance imaging (sMRI) data. In addition, we add a self-supervised loss to the current loss to guide the attention map generation to learn the representations from unlabeled data. Finally, we evaluate our method on a multi-center database which is constructed using five AD datasets. The experimental results show that the proposed method performs better than state-of-the-art methods, achieving mean accuracy rates of 85.69 %, 63.34 %, and 69.89 % on the AD vs. NC, MCI vs. NC, and AD vs. MCI respectively. The source code is available for reproducibility at: https://github.com/yuliangCarmelo/HFL .},
  archive      = {J_PR},
  author       = {Baiying Lei and Yu Liang and Jiayi Xie and You Wu and Enmin Liang and Yong Liu and Peng Yang and Tianfu Wang and ChuanMing Liu and Jichen Du and Xiaohua Xiao and Shuqiang Wang},
  doi          = {10.1016/j.patcog.2024.110423},
  journal      = {Pattern Recognition},
  month        = {9},
  pages        = {110423},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid federated learning with brain-region attention network for multi-center alzheimer&#39;s disease detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised pathological image segmentation via cross
distillation of multiple attentions and seg-CAM consistency.
<em>PR</em>, <em>152</em>, 110492. (<a
href="https://doi.org/10.1016/j.patcog.2024.110492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of pathological images is a crucial step for accurate cancer diagnosis. However, acquiring dense annotations of such images for training is labor-intensive and time-consuming. To address this issue, Semi-Supervised Learning (SSL) has the potential for reducing the annotation cost, but it is challenged by a large number of unlabeled training images. In this paper, we propose a novel SSL method based on Cross Distillation of Multiple Attentions and Seg-CAM Consistency (CDMA+) to effectively leverage unlabeled images. First, we propose a Multi-attention Tri-decoder Network (MTNet) that consists of a shared encoder and three decoders, with each decoder using a different attention mechanism that calibrates features in different aspects to generate diverse outputs. Second, we introduce Cross Decoder Knowledge Distillation (CDKD) between the three decoders, allowing them to learn from each other’s soft labels to mitigate the negative impact of incorrect pseudo labels during training. Subsequently, motivated by the observation that the Class Activation Maps (CAMs) derived from the classification task could provide a rough segmentation, we employ an auxiliary classification head and introduce a consistency constraint between the CAM and segmentation results, i.e. Seg-CAM consistency. Additionally, uncertainty minimization is applied to the average prediction of the three decoders, which further regularizes predictions on unlabeled images and encourages inter-decoder consistency. Our proposed CDMA+ was compared with eight state-of-the-art SSL methods on two public pathological image datasets, and the experimental results showed that our method outperforms the other approaches under different annotation ratios. The code is available at https://github.com/HiLab-git/CDMA .},
  archive      = {J_PR},
  author       = {Lanfeng Zhong and Xiangde Luo and Xin Liao and Shaoting Zhang and Guotai Wang},
  doi          = {10.1016/j.patcog.2024.110492},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110492},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised pathological image segmentation via cross distillation of multiple attentions and seg-CAM consistency},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UCTNet: Uncertainty-guided CNN-transformer hybrid networks
for medical image segmentation. <em>PR</em>, <em>152</em>, 110491. (<a
href="https://doi.org/10.1016/j.patcog.2024.110491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer, born for long-range dependency establishment, has been widely studied as a complementary of convolutional neural networks (CNNs) in medical image segmentation. However, existing CNN-Transformer hybrid approaches simply pursue implicit feature fusion without considering their underlying functional overlap. Medical images typically follow stable anatomical structures, making convolution capable of handling most segmentation targets. Without differentiation, enforcing transformers to operate self-attention for all image patches would result in severe redundancy, hindering global feature extraction. In this paper, we propose a simple yet effective hybrid network named UCTNet where transformers only focus on establishing global dependency for CNN’s unreliable regions predicted through uncertainty estimation. In this way, CNN and transformer are explicitly fused to minimize functional overlap. More importantly, with fewer regions to handle, UCTNet is of better convergence to learn more robust feature representations for hard examples. Extensive experiments on publicly-available datasets demonstrate the superiority of UCTNet against the state-of-the-art approaches, achieving 89.44%, 92.91%, and 91.15% in Dice similarity coefficient on Synapse, ACDC, and ISIC2018 respectively. Furthermore, such a CNN-Transformer hybrid strategy is highly extendable to other frameworks without introducing additional computational burdens. Code is available at https://github.com/innocence0206/UCTNet .},
  archive      = {J_PR},
  author       = {Xiayu Guo and Xian Lin and Xin Yang and Li Yu and Kwang-Ting Cheng and Zengqiang Yan},
  doi          = {10.1016/j.patcog.2024.110491},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110491},
  shortjournal = {Pattern Recognition},
  title        = {UCTNet: Uncertainty-guided CNN-transformer hybrid networks for medical image segmentation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving image segmentation with contextual and structural
similarity. <em>PR</em>, <em>152</em>, 110489. (<a
href="https://doi.org/10.1016/j.patcog.2024.110489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models for medical image segmentation are usually trained with voxel-wise losses, e.g., cross-entropy loss, focusing on unary supervision without considering inter-voxel relationships. This oversight potentially leads to semantically inconsistent predictions. Here, we propose a contextual similarity loss (CSL) and a structural similarity loss (SSL) to explicitly and efficiently incorporate inter-voxel relationships for improved performance. The CSL promotes consistency in predicted object categories for each image sub-region compared to ground truth. The SSL enforces compatibility between the predictions of voxel pairs by computing pair-wise distances between them, ensuring that voxels of the same class are close together whereas those from different classes are separated by a wide margin in the distribution space. The effectiveness of the CSL and SSL is evaluated using a clinical cone-beam computed tomography (CBCT) dataset of patients with various craniomaxillofacial (CMF) deformities and a public pancreas dataset. Experimental results show that the CSL and SSL outperform state-of-the-art regional loss functions in preserving segmentation semantics.},
  archive      = {J_PR},
  author       = {Xiaoyang Chen and Qin Liu and Hannah H. Deng and Tianshu Kuang and Henry Hung-Ying Lin and Deqiang Xiao and Jaime Gateno and James J. Xia and Pew-Thian Yap},
  doi          = {10.1016/j.patcog.2024.110489},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110489},
  shortjournal = {Pattern Recognition},
  title        = {Improving image segmentation with contextual and structural similarity},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Community detection in the stochastic block model by mixed
integer programming. <em>PR</em>, <em>152</em>, 110487. (<a
href="https://doi.org/10.1016/j.patcog.2024.110487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Degree-Corrected Stochastic Block Model (DCSBM) is a popular model to generate random graphs with community structure given an expected degree sequence. The standard approach of community detection based on the DCSBM is to search for the model parameters that are the most likely to have produced the observed network data through maximum likelihood estimation (MLE). Current techniques for the MLE problem are heuristics, and therefore do not guarantee convergence to the optimum. We present mathematical programming formulations and exact solution methods that can provably find the model parameters and community assignments of maximum likelihood given an observed graph. We compare these exact methods with classical heuristic algorithms based on expectation–maximization (EM). The solutions given by exact methods give us a principled way of measuring the experimental performance of classical heuristics and comparing different variations thereof.},
  archive      = {J_PR},
  author       = {Breno Serrano and Thibaut Vidal},
  doi          = {10.1016/j.patcog.2024.110487},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110487},
  shortjournal = {Pattern Recognition},
  title        = {Community detection in the stochastic block model by mixed integer programming},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MTS2Graph: Interpretable multivariate time series
classification with temporal evolving graphs. <em>PR</em>, <em>152</em>,
110486. (<a href="https://doi.org/10.1016/j.patcog.2024.110486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional time series classification approaches based on bags of patterns or shapelets face significant challenges in dealing with a vast amount of feature candidates from high-dimensional multivariate data. In contrast, deep neural networks can learn low-dimensional features efficiently, and in particular, convolutional neural networks have shown promising results in classifying multivariate time series data. A key factor in the success of deep neural networks is this astonishing expressive power. However, this power comes at the cost of complex, black-boxed models, conflicting with the goals of building reliable and human-understandable models. In this work 1 , we introduce a new interpretable framework for multivariate time series data that by extracting and clustering the input quantifies the contribution of time-varying input variables and each signal’s role to the classification. We construct a graph that captures the temporal relationship between the extracted patterns for each layer and propose an effective merging strategy to aggregate those graphs into one. Finally, a graph embedding algorithm generates new representations of the created interpretable time-series features. Our extensive experiments indicate the benefit of our time-aware graph-based representation in multivariate time series classification while enriching them with more interpretability.},
  archive      = {J_PR},
  author       = {Raneen Younis and Abdul Hakmeh and Zahra Ahmadi},
  doi          = {10.1016/j.patcog.2024.110486},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110486},
  shortjournal = {Pattern Recognition},
  title        = {MTS2Graph: Interpretable multivariate time series classification with temporal evolving graphs},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoReFace: Sample-guided contrastive regularization for deep
face recognition. <em>PR</em>, <em>152</em>, 110483. (<a
href="https://doi.org/10.1016/j.patcog.2024.110483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discriminability of the feature representation is crucial for face recognition. However, previous methods rely solely on the learnable weights of the classification layer, which represent the identities. This reliance could be problematic as the evaluation process depends on the similarity between pairs of face images and requires minimal identity information learned during training. As a result, there is an inconsistency between the training and evaluation processes, which can confuse the feature encoder and hinder the effectiveness of identity-based methods. To address this problem, we propose a novel approach namely Contrastive Regularization for Face Recognition (CoReFace), which applies sample-level regularization in feature representation learning. Specifically, we employ sample-guided contrastive learning to directly regularize the training based on the sample-sample relationship and thus align it with the evaluation process. To avoid image quality degradation, we augment the embeddings instead of the images in order to integrate contrastive learning into face recognition. Additionally, we introduce a new contrastive loss function for the regularization of representation distribution. This function incorporates an adaptive margin and a supervised contrastive mask to ensure stable loss values and prevent interference with the identity supervision signals. Finally, we explore new pair-coupling protocols in order to overcome the problem of semantically repetitive signals in contrastive learning. Extensive experiments demonstrate the efficacy and efficiency of our CoReFace approach, which achieves competitive results compared to state-of-the-art methods. Code could be found https://github.com/IsidoreSong/CoreFace here.},
  archive      = {J_PR},
  author       = {Youzhe Song and Feng Wang},
  doi          = {10.1016/j.patcog.2024.110483},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110483},
  shortjournal = {Pattern Recognition},
  title        = {CoReFace: Sample-guided contrastive regularization for deep face recognition},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video-based face outline recognition. <em>PR</em>,
<em>152</em>, 110482. (<a
href="https://doi.org/10.1016/j.patcog.2024.110482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel approach for individual recognition that uses the motion traits from face outline-anonymised videos as the id entity signatures; we call this type of signature a Temporal-iD . To extract a robust Temporal-iD, a highly lightweight transformer-based model, namely the Temporal-iD-ViT ( TiDViT ), is devised to capture and aggregate Temporal-iD features from videos. The TiDViT is equipped with a custom-designed multi-head temporal–spatial joint attention module that establishes interaction between the current frame input and the previous hidden state, thereby temporally aggregating the temporal features. The TiDViT processes the face video frame by frame instead of in a fixed batch-wise manner, which requires less computational memory. Moreover, the TiDViT can extract the temporal features of unconstrained face videos and considers ethical and privacy concerns. Extensive experimental results show that the proposed TiDViT model achieves a decent performance on this highly challenging task.},
  archive      = {J_PR},
  author       = {Xingbo Dong and Jiewen Yang and Andrew Beng Jin Teoh and Dahai Yu and Xiaomeng Li and Zhe Jin},
  doi          = {10.1016/j.patcog.2024.110482},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110482},
  shortjournal = {Pattern Recognition},
  title        = {Video-based face outline recognition},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-based person search via cross-modal alignment learning.
<em>PR</em>, <em>152</em>, 110481. (<a
href="https://doi.org/10.1016/j.patcog.2024.110481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to use text descriptions to search for corresponding person images. However, due to the obvious pattern differences in image and text modalities, it is still a challenging problem to align the two modalities. Most existing approaches only consider semantic alignment within a global context or partial parts, lacking consideration of how to match image and text in terms of differences in model information. Therefore, in this paper, we propose an efficient Modality-Aligned Person Search network (MAPS) to address this problem. First, we suppress image-specific information by image feature style normalization to achieve modality knowledge alignment and reduce information differences between text and image. Second, we design a multi-granularity modal feature fusion and optimization method to enrich the modal features. To address the problem of useless and redundant information in the multi-granularity fused features, we propose a Multi-granularity Feature Self-optimization Module (MFSM) to adaptively adjust the corresponding contributions of different granularities in the fused features of the two modalities. Finally, to address the problem of information inconsistency in the training and inference stages, we propose a Cross-instance Feature Alignment (CFA) to help the network enhance category-level generalization ability and improve retrieval performance. Extensive experiments demonstrate that our MAPS achieves state-of-the-art performance on all text-based person search datasets, and significantly outperforms other existing methods.},
  archive      = {J_PR},
  author       = {Xiao Ke and Hao Liu and Peirong Xu and Xinru Lin and Wenzhong Guo},
  doi          = {10.1016/j.patcog.2024.110481},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110481},
  shortjournal = {Pattern Recognition},
  title        = {Text-based person search via cross-modal alignment learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmented skeleton sequences with hypergraph network for
self-supervised group activity recognition. <em>PR</em>, <em>152</em>,
110478. (<a href="https://doi.org/10.1016/j.patcog.2024.110478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has been widely applied to self-supervised skeleton-based single-person action recognition. However, directly employing single-person contrastive learning techniques for multi-person skeleton-based Group Activity Recognition (GAR) suffers from some challenges. Firstly, single-person data augmentation strategies struggle to capture complex collaborations between actors in multi-person scenarios, resulting in poor generalization. Secondly, real-world uncertainties in the number of people make single-person methods fail to capture changing high-order actor relations. Finally, single-person methods treat each actor with equal importance for recognition, struggling to distinguish imbalanced contributions between individual and group activities. To this end, the coarse-to-fine A ugmented H ypergraph Net work (AHNet) is proposed for effective self-supervised GAR. Specifically, we introduce multi-person augmentation strategies to enhance the generalization of the model under complex actor collaboration scenarios. Moreover, a knowledge-masked hypergraph network is employed to enhance the adaptability of the model to capture varied high-order actor relations. Finally, coarse-to-fine contrast among key actors is conducted to mitigate the imbalanced contributions between individual and group levels. Extensive experiments on multiple datasets demonstrate that our AHNet achieves substantial improvements over state-of-the-art methods with various backbone architectures. Our code is available at https://github.com/WGQ109/AHNet .},
  archive      = {J_PR},
  author       = {Guoquan Wang and Mengyuan Liu and Hong Liu and Peini Guo and Ti Wang and Jingwen Guo and Ruijia Fan},
  doi          = {10.1016/j.patcog.2024.110478},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110478},
  shortjournal = {Pattern Recognition},
  title        = {Augmented skeleton sequences with hypergraph network for self-supervised group activity recognition},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal de-deviation for enhancing few-shot
classification. <em>PR</em>, <em>152</em>, 110475. (<a
href="https://doi.org/10.1016/j.patcog.2024.110475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning poses a critical challenge due to the deviation problem caused by the scarcity of available samples. In this work, we aim to address deviations in both feature representations and prototypes. To achieve this, we propose a cross-modal de-deviation framework that leverages class semantic information to provide robust prior knowledge for the samples. This framework begins with a visual-to-semantic autoencoder trained on the labeled samples to predict semantic features for the unlabeled samples. Then, we devise a binary linear programming model to match the initial prototypes with the cluster centers of the unlabeled samples. To circumvent potential mismatches between the cluster centers and the initial prototypes, we perform the label assignment process in the semantic space by transforming the cluster centers into semantic representations and utilizing the class ground truth semantic features as reference points. Moreover, we model a linear classifier with the concatenation of the refined prototypes and the class ground truth semantic features serving as the initial weights. Then we propose a novel optimization strategy based on the alternating least squares (ALS) model. From the ALS model, we can derive two closed-form solutions regarding to the features and weights, facilitating alternative optimization of them. Extensive experiments conducted on few-shot learning benchmarks demonstrate the competitive advantages of our CMDD method over the state-of-the-art approaches, confirming its effectiveness in reducing deviation. The code is available at: https://github.com/pmhDL/CMDD.git .},
  archive      = {J_PR},
  author       = {Mei-Hong Pan and Hong-Bin Shen},
  doi          = {10.1016/j.patcog.2024.110475},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110475},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal de-deviation for enhancing few-shot classification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Robust multiple subspaces transfer for heterogeneous domain
adaptation. <em>PR</em>, <em>152</em>, 110473. (<a
href="https://doi.org/10.1016/j.patcog.2024.110473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) aims to execute knowledge transfer from a source domain to a heterogeneous target domain. Previous works typically inject knowledge from the source and target domain into a common subspace. However, this may lead to the ineffectiveness of knowledge transfer due to the existence of heterogeneity. To overcome this drawback, in this paper, we propose a robust multiple subspaces transfer method for heterogeneous domain adaptation. Specifically, knowledge of two domains is projected into a union of multiple subspaces via a self-expressive model, in which joint distribution alignment and dynamic Laplacian regularization on self-repressive coefficients are included in the loss for characterizing transferability. Moreover, we provide a comprehensive analysis of stability, complexity, generalization, and convergence guarantee for the proposed method. Experiments on benchmark vision and Language datasets verify effectiveness of the proposed approach for heterogeneous domain adaptation.},
  archive      = {J_PR},
  author       = {Youfa Liu and Bo Du and Yongyong Chen and Lefei Zhang},
  doi          = {10.1016/j.patcog.2024.110473},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110473},
  shortjournal = {Pattern Recognition},
  title        = {Robust multiple subspaces transfer for heterogeneous domain adaptation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BSDP: Brain-inspired streaming dual-level perturbations for
online open world object detection. <em>PR</em>, <em>152</em>, 110472.
(<a href="https://doi.org/10.1016/j.patcog.2024.110472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans can easily distinguish the known and unknown categories and can recognize the unknown object by learning it once instead of repeating it many times without forgetting the learned object. Hence, we aim to make deep learning models simulate the way people learn. We refer to such a learning manner as O n L ine O pen W orld O bject D etection(OLOWOD). Existing OWOD approaches pay more attention to the identification of unknown categories, while the incremental learning part is also very important. Besides, some neuroscience research shows that specific noises allow the brain to form new connections and neural pathways which may improve learning speed and efficiency. In this paper, we take the dual-level information of old samples as perturbations on new samples to make the model good at learning new knowledge without forgetting the old knowledge. Therefore, we propose a simple plug-and-play method, called B rain-inspired S treaming D ual-level P erturbations(BSDP), to solve the OLOWOD problem. Specifically, (1) we first calculate the prototypes of previous categories and use the distance between samples and the prototypes as the sample selecting strategy to choose old samples for replay; (2) then take the prototypes as the streaming feature-level perturbations of new samples, so as to improve the plasticity of the model through revisiting the old knowledge; (3) and also use the distribution of the features of the old category samples to generate adversarial data in the form of streams as the data-level perturbations to enhance the robustness of the model to new categories. We empirically evaluate BSDP on PASCAL VOC and MS-COCO, and the excellent results demonstrate the promising performance of our proposed method and learning manner.},
  archive      = {J_PR},
  author       = {Yu Chen and Liyan Ma and Liping Jing and Jian Yu},
  doi          = {10.1016/j.patcog.2024.110472},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110472},
  shortjournal = {Pattern Recognition},
  title        = {BSDP: Brain-inspired streaming dual-level perturbations for online open world object detection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalization memorization machine with zero empirical risk
for classification. <em>PR</em>, <em>152</em>, 110469. (<a
href="https://doi.org/10.1016/j.patcog.2024.110469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying the training data correctly without over-fitting is one of the goals in machine learning. In this paper, we propose a general Generalization Memorization Machine (GMM) to obtain zero empirical risk with better generalization. The widely applied loss-based learning models can be extended by the GMM to improve their memorization and generalization abilities. Specifically, we propose two new models based on the GMM, called Hard Generalization Memorization Machine (HGMM) and Soft Generalization Memorization Machine (SGMM). Both HGMM and SGMM obtain zero empirical risks with well generalization, and the SGMM further improves the capacity and applicability of HGMM. The optimization problems in the proposed models are quadratic programming problems and could be solved efficiently. Additionally, the recently proposed generalization memorization kernel and the corresponding support vector machine are the special cases of our SGMM. Experimental results demonstrate the effectiveness of the proposed HGMM and SGMM both on memorization and generalization.},
  archive      = {J_PR},
  author       = {Zhen Wang and Lan Bai and Yuanhai Shao},
  doi          = {10.1016/j.patcog.2024.110469},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110469},
  shortjournal = {Pattern Recognition},
  title        = {Generalization memorization machine with zero empirical risk for classification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SceneFake: An initial dataset and benchmarks for scene fake
audio detection. <em>PR</em>, <em>152</em>, 110468. (<a
href="https://doi.org/10.1016/j.patcog.2024.110468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many datasets have been designed to further the development of fake audio detection. However, fake utterances in previous datasets are mostly generated by altering timbre, prosody, linguistic content or channel noise of original audio. These datasets leave out a scenario, in which the acoustic scene of an original audio is manipulated with a forged one. It will pose a major threat to our society if some people misuse the manipulated audio with malicious purpose. Therefore, this motivates us to fill in the gap. This paper proposes such a dataset for scene fake audio detection named SceneFake, where a manipulated audio is generated by only tampering with the acoustic scene of an real utterance by using speech enhancement technologies. Some scene fake audio detection benchmark results on the SceneFake dataset are reported in this paper. In addition, an analysis of fake attacks with different speech enhancement technologies and signal-to-noise ratios are presented in this paper. The results indicate that scene fake utterances cannot be reliably detected by baseline models trained on the ASVspoof 2019 dataset. Although these models perform well on the SceneFake training set and seen testing set, their performance is poor on the unseen test set. The dataset 2 and benchmark source codes 3 are publicly available.},
  archive      = {J_PR},
  author       = {Jiangyan Yi and Chenglong Wang and Jianhua Tao and Chu Yuan Zhang and Cunhang Fan and Zhengkun Tian and Haoxin Ma and Ruibo Fu},
  doi          = {10.1016/j.patcog.2024.110468},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110468},
  shortjournal = {Pattern Recognition},
  title        = {SceneFake: An initial dataset and benchmarks for scene fake audio detection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WRD-net: Water reflection detection using a parallel
attention transformer. <em>PR</em>, <em>152</em>, 110467. (<a
href="https://doi.org/10.1016/j.patcog.2024.110467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to symmetry detection, Water Reflection Detection (WRD) is less studied. We treat this topic as a Symmetry Axis Point Prediction task which outputs a set of points by implicitly learning Gaussian heat maps and explicitly learning numerical coordinates. We first collect a new data set, namely, the Water Reflection Scene Data Set (WRSD). Then, we introduce a novel Water Reflection Detection Network, i.e., WRD-Net. This network is built on top of a series of Parallel Attention Vision Transformer blocks with the Atrous Spatial Pyramid (ASP-PAViT) that we deliberately design. Each block captures both the local and global features at multiple scales. To our knowledge, neither the WRSD nor the WRD-Net has been used for water reflection detection before. To derive the axis of symmetry, we perform Principal Component Analysis (PCA) on the points predicted. Experimental results show that the WRD-Net outperforms its counterparts and achieves the true positive rate of 0.823 compared with the human annotation.},
  archive      = {J_PR},
  author       = {Huijie Dong and Hao Qi and Huiyu Zhou and Junyu Dong and Xinghui Dong},
  doi          = {10.1016/j.patcog.2024.110467},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110467},
  shortjournal = {Pattern Recognition},
  title        = {WRD-net: Water reflection detection using a parallel attention transformer},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast main density peak clustering within relevant regions
via a robust decision graph. <em>PR</em>, <em>152</em>, 110458. (<a
href="https://doi.org/10.1016/j.patcog.2024.110458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Density Peak Clustering (DPC) can easily locate cluster centers by detecting density peaks in its decision graph, its allocation strategy may unadvisedly associate irrelevant points, its decision graph may mislead the cluster center selection, and its high computational complexity O ( n 2 ) O(n2) shies itself away from large-scale data. Herein, a Fast Main Density Peak Clustering Within Relevant Regions Via A Robust Decision Graph (R-MDPC) is proposed. R-MDPC assigns points within the relevant regions to avoid the association of irrelevant points. With the removal of regional differences and the attenuation of satellite peaks, a robust decision graph is obtained. Moreover, based on the kNN distance of data points, R-MDPC is believed to be suitable for large-scale data. Experimental results demonstrated the high robustness of R-MDPC’s decision graph in identifying cluster centers, and its outstanding performance and fast running speed in recognizing complex-shaped clusters.},
  archive      = {J_PR},
  author       = {Junyi Guan and Sheng Li and Jinhui Zhu and Xiongxiong He and Jiajia Chen},
  doi          = {10.1016/j.patcog.2024.110458},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110458},
  shortjournal = {Pattern Recognition},
  title        = {Fast main density peak clustering within relevant regions via a robust decision graph},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-camera multi-object tracking on the move via
single-stage global association approach. <em>PR</em>, <em>152</em>,
110457. (<a href="https://doi.org/10.1016/j.patcog.2024.110457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of autonomous vehicles generates a tremendous demand for a low-cost solution based on a complete set of camera sensors to perceive the environment around the car. Towards this solution, it is essential for object detection and tracking to address new challenges in multi-camera settings. To address these challenges, this work introduces novel Single-Stage Global Association Tracking approaches to associate one or more detections from multi-cameras with tracked objects. These approaches aim to solve fragment-tracking issues caused by inconsistent 3D object detection. Moreover, our models also improve the detection accuracy of the standard vision-based 3D object detectors in the nuScenes detection challenge. The extensive experimental results on the nuScenes dataset demonstrate the benefits of the proposed method, which outperforms prior vision-based tracking methods in multi-camera settings.},
  archive      = {J_PR},
  author       = {Pha Nguyen and Kha Gia Quach and Chi Nhan Duong and Son Lam Phung and Ngan Le and Khoa Luu},
  doi          = {10.1016/j.patcog.2024.110457},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110457},
  shortjournal = {Pattern Recognition},
  title        = {Multi-camera multi-object tracking on the move via single-stage global association approach},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pairwise difference relational distillation for object
re-identification. <em>PR</em>, <em>152</em>, 110455. (<a
href="https://doi.org/10.1016/j.patcog.2024.110455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most relationship knowledge distillation methods individually optimize pairwise similarities to improve the accuracy performance of a lightweight student network. However, this optimization approach may not be optimal for object re-identification (Re-ID) which prioritizes ranking. This is because it does not guarantee consistent ranking results between a lightweight student network and a large teacher network. For that, we propose a novel method called pairwise difference relational distillation (PDRD) for object Re-ID. First, we theoretically prove that minimizing the difference relationship between pairwise similarities resulting from student and teacher networks ensures consistent ranking results between the two networks. Second, based on this theoretical foundation, we combine non-linear activation functions on pairwise similarity discrepancies to create a non-linear pairwise difference rational knowledge loss function, which enhances knowledge transfer. Extensive experiments on four public datasets demonstrate that our method achieves state-of-the-art performance. For example, on Market-1501, using ResNet18 as a lightweight student network, our method acquires a rank-1 identification rate of 93.62%.},
  archive      = {J_PR},
  author       = {Yi Xie and Hanxiao Wu and Yihong Lin and Jianqing Zhu and Huanqiang Zeng},
  doi          = {10.1016/j.patcog.2024.110455},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110455},
  shortjournal = {Pattern Recognition},
  title        = {Pairwise difference relational distillation for object re-identification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification of childhood obstructive sleep apnea based on
x-ray images analysis by quasi-conformal geometry. <em>PR</em>,
<em>152</em>, 110454. (<a
href="https://doi.org/10.1016/j.patcog.2024.110454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Craniofacial profile is one of the anatomical causes of obstructive sleep apnea (OSA). By medical research, cephalometry provides information on patients’ skeletal structures and soft tissues. In this work, a novel approach to cephalometric analysis using quasi-conformal geometry based local deformation information was proposed for OSA classification. Our study was a retrospective analysis based on 60 case-control pairs with accessible lateral cephalometry and polysomnography (PSG) data (mean age: 8.9 ± 2.3 years). By using the quasi-conformal geometry to study the local deformation around 15 landmark points, and combining the results with three linear distances between landmark points, a total of 1218 information features were obtained per subject. A L 2 L2 norm based classification model was built. Under experiments, our proposed model achieves high testing accuracy. The accurate classification tool provides us with a reliable and accurate screening tool for childhood OSA.},
  archive      = {J_PR},
  author       = {Hei-Long Chan and Hoi-Man Yuen and Chun-Ting Au and Kate Ching-Ching Chan and Albert Martin Li and Lok-Ming Lui},
  doi          = {10.1016/j.patcog.2024.110454},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110454},
  shortjournal = {Pattern Recognition},
  title        = {Classification of childhood obstructive sleep apnea based on X-ray images analysis by quasi-conformal geometry},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph embedding orthogonal decomposition: A synchronous
feature selection technique based on collaborative particle swarm
optimization. <em>PR</em>, <em>152</em>, 110453. (<a
href="https://doi.org/10.1016/j.patcog.2024.110453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unsupervised feature selection, the clustering label matrix has the ability to distinguish between projection clusters. However, the latent geometric structure of the clustering labels is often ignored. In addition, the optimal sub-feature selection performance of feature selection techniques relies greatly on the choice of balanced parameters, and the selection range of most technical parameters is limited and fixed. To solve the above-mentioned problems, this paper proposes a synchronous feature selection technique based on graph-embedded cluster label orthogonal decomposition and collaborative particle swarm optimization (GOD-cPSO). First, GOD-cPSO extends the feature selection framework of clustering label orthogonal decomposition by graph embedding to retain the latent geometric structure of clustering labels, thus maintaining the correlation between clustered sample labels. Then, the l 2,1-2 -norm with strong global convergence is extended to the graph embedding clustering label orthogonal decomposition framework. By imposing this non-convex constraint, GOD-cPSO can achieve low-dimensional sparse and low-redundant sub-features. In addition, the local structure preserving of low-dimensional manifolds is integrated into the graph-embedded clustering label orthogonal decomposition framework to obtain good cluster separation and effectively maintain the latent local structure of the data. Finally, to ensure the adaptive parameter selection over a large range, GOD-cPSO synchronously guides the graph-embedding clustering labeling orthogonal decomposition framework for feature selection through collaborative particle swarm optimization. GOD-cPSO has synchronous parameter optimization and feature selection and picks parameters in a larger range. Comprehensive numerical experiments are performed on nine datasets to test the validity of the GOD-cPSO. The experimental results demonstrate that the sub-features selected by the GOD-cPSO have stronger discriminative power and are superior to other techniques in the clustering assignments.},
  archive      = {J_PR},
  author       = {Jingyu Zhong and Ronghua Shang and Songhua Xu and Yangyang Li},
  doi          = {10.1016/j.patcog.2024.110453},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110453},
  shortjournal = {Pattern Recognition},
  title        = {Graph embedding orthogonal decomposition: A synchronous feature selection technique based on collaborative particle swarm optimization},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Zero-shot sketch-based image retrieval via adaptive
relation-aware metric learning. <em>PR</em>, <em>152</em>, 110452. (<a
href="https://doi.org/10.1016/j.patcog.2024.110452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieving natural images with the query sketches under the zero-shot scenario is known as zero-shot sketch-based image retrieval (ZS-SBIR). Most of the best-performing methods adapt the triplet loss to learn projections that map natural images and sketches to a latent embedding space. They nevertheless neglect the modality gap between the hand-drawn sketches and the photos and consider no difference between any two incorrect classes, which limits their performance in real use cases. Towards this end, we put forward a simple and effective model, which adopts relation-aware metric learning to suppress the modality gap between the sketches and the photos. We also propose an adaptive margin that utilizes each anchor in embedding space to improve clustering ability in metric learning. Extensive experiments on the Sketchy and TU-Berlin datasets show the dominant position of our proposed model over SOTA competitors.},
  archive      = {J_PR},
  author       = {Yang Liu and Yuhao Dang and Xinbo Gao and Jungong Han and Ling Shao},
  doi          = {10.1016/j.patcog.2024.110452},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110452},
  shortjournal = {Pattern Recognition},
  title        = {Zero-shot sketch-based image retrieval via adaptive relation-aware metric learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual balancing in state-object components for
compositional zero-shot learning. <em>PR</em>, <em>152</em>, 110451. (<a
href="https://doi.org/10.1016/j.patcog.2024.110451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional Zero-Shot Learning (CZSL) aims to recognize unseen compositions from seen states and objects. The disparity between the manually labeled semantic information and its actual visual features causes a significant imbalance of visual deviation in the distribution of various object classes and state classes, which is ignored by existing methods. To ameliorate these issues, we consider the CZSL task as an unbalanced multi-label classification task and propose a novel method called MU tual balancing in ST ate-object components ( MUST ) for CZSL, which provides a balancing inductive bias for the model. In particular, we split the classification of the composition classes into two consecutive processes to analyze the entanglement of the two components to get additional knowledge in advance, which reflects the degree of visual deviation between the two components. We use the knowledge gained to modify the model’s training process in order to generate more distinct class borders for classes with significant visual deviations. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art on MIT-States, UT-Zappos, and C-GQA when combined with the basic CZSL frameworks, and it can improve various CZSL frameworks. Our code is available at https://github.com/LanchJL/MUST .},
  archive      = {J_PR},
  author       = {Chenyi Jiang and Qiaolin Ye and Shidong Wang and Yuming Shen and Zheng Zhang and Haofeng Zhang},
  doi          = {10.1016/j.patcog.2024.110451},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110451},
  shortjournal = {Pattern Recognition},
  title        = {Mutual balancing in state-object components for compositional zero-shot learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable medical deep framework by logits-constraint
attention guiding graph-based multi-scale fusion for alzheimer’s disease
analysis. <em>PR</em>, <em>152</em>, 110450. (<a
href="https://doi.org/10.1016/j.patcog.2024.110450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning using structural MRI has been widely applied to early diagnosis study of Alzheimer’s disease. Among existing methods, attention-based 3D subject-level methods can not only provide diagnosis results but also interpret the significant brain regions, thereby attracting considerable attention. However, the performance of previous attention-based methods might be still restricted by: (i) the gap between attention scores and semantic significant regions; (ii) using only single-scale features or simply fusing multi-scale information by addition or concatenation for classification decision-making. To overcome these two issues, we propose an innovative dual-branch model called LA-GMF, which consists of two major modules: logits-constraint attention (LA) and graph-based multi-scale fusion (GMF). The LA module is designed to guide the model to focus on key areas to enhance the diagnostic performance of local lesions, by reducing the inconsistency between attention scores and class prediction probabilities. Meanwhile, by combining the graph neural network and the self-attention mechanism, the GMF module not only introduces the interaction between patches, but also explores the correlation and complementarity between features at different scales, thereby extracting feature representations more comprehensively. Experiments on the popular ADNI and AIBL datasets validate the potential of our model in boosting early AD diagnosis accuracy. Additionally, our interpretation experiments demonstrate the superior interpretability performance of the proposed method over recent state-of-the-art attention-based methods. Our source codes are released at: https://github.com/nollexu/LA-GMF .},
  archive      = {J_PR},
  author       = {Jinghao Xu and Chenxi Yuan and Xiaochuan Ma and Huifang Shang and Xiaoshuang Shi and Xiaofeng Zhu},
  doi          = {10.1016/j.patcog.2024.110450},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110450},
  shortjournal = {Pattern Recognition},
  title        = {Interpretable medical deep framework by logits-constraint attention guiding graph-based multi-scale fusion for alzheimer’s disease analysis},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-target label backdoor attacks on graph neural
networks. <em>PR</em>, <em>152</em>, 110449. (<a
href="https://doi.org/10.1016/j.patcog.2024.110449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks have been shown to have characteristics that make them susceptible to backdoor attacks, and many recent works have proposed feasible graph backdoor attack methods. However, existing graph backdoor attack methods only target one-to-one attack types and lack graph backdoor attack methods that can address one-to-many attack requirements. This paper is the first research work on one-to-many type graph backdoor attacks and proposes the backdoor attack method MLGB, which can achieve multi-target label attacks for GNN node classification tasks. We designed encoding mechanisms to allow MLGB to customize triggers for different target labels and ensure differentiation between triggers for different target labels through loss functions. Additionally, we designed an innovative poisoned node selection method to improve the efficiency of MLGB’s attacks further. Extensive experiments were conducted to validate MLGB’s effectiveness across multiple datasets and model architectures, demonstrating its robustness against graph backdoor attack defense mechanisms. Furthermore, ablation experiments and explainability analyses were conducted to provide deeper insights into MLGB. Our work reveals that graph neural networks are also vulnerable to one-to-many type backdoor attacks, which is important for practitioners to understand model risks comprehensively.},
  archive      = {J_PR},
  author       = {Kaiyang Wang and Huaxin Deng and Yijia Xu and Zhonglin Liu and Yong Fang},
  doi          = {10.1016/j.patcog.2024.110449},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110449},
  shortjournal = {Pattern Recognition},
  title        = {Multi-target label backdoor attacks on graph neural networks},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive domain-adaptive graph selective self-training
network for cross-network edge classification. <em>PR</em>,
<em>152</em>, 110448. (<a
href="https://doi.org/10.1016/j.patcog.2024.110448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of graph Neural Network (GNNs) can degrade significantly when trained on the graphs with noisy edges connecting nodes from different classes. To mitigate the negative effect of noisy edges, previous studies have largely focused on predicting the label agreement between node pairs within a single network. So far, predicting noisy edges across different networks remains largely underexplored. To bridge this gap, our work studies a novel problem of cross-network homophilous and heterophilous edge classification (CNHHEC), aiming to predict the label agreement of edges in an unlabeled target network by transferring the knowledge from a labeled source network. A novel Contrastive Domain-adaptive Graph Self-training Network (CDGSN) is proposed. Firstly, CDGSN learns node and edge embeddings end-to-end by a GNN encoder with adaptive edge weights during neighborhood aggregation. Secondly, to facilitate knowledge transfer across networks, CDGSN employs an adversarial domain adaptation module to align edge embeddings across networks, and also designs a novel contrastive domain adaptation module to conduct class-aware cross-network alignment of node embeddings. As a result, the intra-class domain divergence can be mitigated while the inter-class domain discrepancy can be enlarged to yield network-invariant and label-discriminative node and edge embeddings. Moreover, CDGSN designs a selective positive and negative pseudo-labeling strategy to assign positive (negative) pseudo-labels to the target nodes with extremely high (low) prediction confidence of belonging to each specific class. Such pseudo-labeled target nodes would be employed to iteratively re-train the model in a self-training manner, so as to obtain more reliable target pseudo-labels progressively to guide class-aware domain alignment. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed CDGSN on the CNHHEC problem. The performance of CDGSN is robust against various types of edge embeddings. When adopting three operators to construct edge embeddings, the proposed CDGSN can improve the state-of-the-art method for CNHHEC by an average of 0.5 %, 2.8 %, and 10.8 % in terms of AUC, and 1.3 %, 3.0 %, and 6.3 % in terms of AP, respectively.},
  archive      = {J_PR},
  author       = {Mengqiu Shao and Peng Xue and Xi Zhou and Xiao Shen},
  doi          = {10.1016/j.patcog.2024.110448},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110448},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive domain-adaptive graph selective self-training network for cross-network edge classification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label enhancement via manifold approximation and projection
with graph convolutional network. <em>PR</em>, <em>152</em>, 110447. (<a
href="https://doi.org/10.1016/j.patcog.2024.110447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label enhancement (LE) aims to enrich logical labels into their corresponding label distributions. But existing LE algorithms fail to fully leverage the structural information in the feature space to improve LE learning. To address this key issue, we first apply manifold learning to map the relatedness between low-dimensional feature samples to the label space. Based on the smoothness assumption of manifolds, the implicit correlation between low-dimensional feature and label spaces effectively promotes the LE process, enabling the learning model to accurately capture the mapping relationship between feature and label manifolds. This leads to an LE based on feature representation (LEFR) algorithm. We also propose an LE algorithm based on graph convolutional network (GCN), called LE-GCN. Inspired by the relationship between threshold connections and label connections, we extend GCN to the LE field for the first time to fully exploit the hidden relationships between nodes and labels. By enhancing node information with threshold connections and label connections, the label learning accuracy reaches a new level. Experiments on real-world datasets show that our LEFR and LE-GCN outperform several state-of-the-art LE algorithms.},
  archive      = {J_PR},
  author       = {Chao Tan and Sheng Chen and Xin Geng and Yunyao Zhou and Genlin Ji},
  doi          = {10.1016/j.patcog.2024.110447},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110447},
  shortjournal = {Pattern Recognition},
  title        = {Label enhancement via manifold approximation and projection with graph convolutional network},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DGFormer: Dynamic graph transformer for 3D human pose
estimation. <em>PR</em>, <em>152</em>, 110446. (<a
href="https://doi.org/10.1016/j.patcog.2024.110446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant progress for monocular 3D human pose estimation, it still faces challenges due to self-occlusions and depth ambiguities. To tackle those issues, we propose a novel Dynamic Graph Transformer (DGFormer) to exploit local and global relationships between skeleton joints for pose estimation. Specifically, the proposed DGFormer mainly consists of three core modules: Transformer Encoder (TE), immobile Graph Convolutional Network (GCN), and dynamic GCN. TE module leverages the self-attention mechanism to learn the complex global relationships among skeleton joints. The immobile GCN is responsible for capturing the local physical connections between human joints, while the dynamic GCN concentrates on learning the sparse dynamic K-nearest neighbor interactions according to different action poses. By building the adequately global long-range, local physical, and sparse dynamic dependencies of human joints, experiments on Human3.6M and MPI-INF-3DHP datasets demonstrate that our method can predict 3D pose with lower errors outperforming the recent state-of-the-art image-based performance. Furthermore, experiments on in-the-wild videos demonstrate the impressive generalization abilities of our method. Code will be available at: https://github.com/czmmmm/DGFormer .},
  archive      = {J_PR},
  author       = {Zhangmeng Chen and Ju Dai and Junxuan Bai and Junjun Pan},
  doi          = {10.1016/j.patcog.2024.110446},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110446},
  shortjournal = {Pattern Recognition},
  title        = {DGFormer: Dynamic graph transformer for 3D human pose estimation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFIFusion: An infrared and visible image enhanced fusion
network based on multi-level feature injection. <em>PR</em>,
<em>152</em>, 110445. (<a
href="https://doi.org/10.1016/j.patcog.2024.110445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to integrate complementary information from both types of images. Existing deep learning-based fusion methods rely solely on the final output of the feature extraction network, which may overlook valuable information presented in the middle layers of the network, ultimately reducing the richness of the fusion results, i.e. , detailed texture information might not be fully extracted and integrated into the fused image. This study proposes a multi-level feature injection method based on an image decomposition model for infrared and visible image fusion, termed as MFIFusion. On the one hand, we introduce an attention-guided multi-level feature injection module designed to mitigate information loss during the feature extraction stage of the image scale decomposition process. More specifically, the proposed method integrates multiple fusion branches in the encoder network and employs an attention mechanism to guide the feature fusion process. On the other hand, based on the characteristics that superficial features retain image detail information and profound features are more suitable for extracting semantic information from images, we use distinct fusion strategies in these two phases to adaptively control the intensity distribution of the salient targets and to preserve the texture information in the background region. Qualitative and quantitative results demonstrate that our proposed approach produces fused images that are more visually salient to the target and contain richly detailed textures.},
  archive      = {J_PR},
  author       = {Aimei Dong and Long Wang and Jian Liu and Guohua Lv and Guixin Zhao and Jinyong Cheng},
  doi          = {10.1016/j.patcog.2024.110445},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110445},
  shortjournal = {Pattern Recognition},
  title        = {MFIFusion: An infrared and visible image enhanced fusion network based on multi-level feature injection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). ACQ: Improving generative data-free quantization via
attention correction. <em>PR</em>, <em>152</em>, 110444. (<a
href="https://doi.org/10.1016/j.patcog.2024.110444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-free quantization aims to achieve model quantization without accessing any authentic sample. It is significant in an application-oriented context involving data privacy. Converting noise vectors into synthetic samples through a generator is a popular data-free quantization method, which is called generative data-free quantization. However, there is a difference in attention map between synthetic samples and authentic samples. This is always ignored and restricts the quantization performance. First, since synthetic samples of the same class are prone to have homogenous attention, the quantized network can only learn limited intra-class visual features. Second, synthetic samples in eval mode and training mode exhibit different attention. Hence, the statistical distribution matching tends to be inaccurate. ACQ is proposed in this paper to fix the attention of synthetic samples. Regarding intra-class attention homogeneity, we introduce an attention center matching loss aimed at achieving coarse-grained matching of attention of synthetic samples. Additionally, we have devised an adversarial loss based on pairs of samples with identical conditions. On one hand, this mechanism prevents the generator from mode collapse due to excessive attention on conditional information. On the other hand, it augments the separation between intra-class samples, thus further enhancing intra-class attention diversity. To improve the attention similarity of synthetic samples in different network modes, we introduce a consistency penalty to guarantee accurate statistical distribution matching. The experimental results demonstrate that ACQ effectively improves the attention problems of synthetic samples. Under various training settings, ACQ achieves the best quantization performance. For the 4-bit quantization of Resnet18 and Resnet50, ACQ reaches 67.55 % and 72.23 % accuracy, respectively.},
  archive      = {J_PR},
  author       = {Jixing Li and Xiaozhou Guo and Benzhe Dai and Guoliang Gong and Min Jin and Gang Chen and Wenyu Mao and Huaxiang Lu},
  doi          = {10.1016/j.patcog.2024.110444},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110444},
  shortjournal = {Pattern Recognition},
  title        = {ACQ: Improving generative data-free quantization via attention correction},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Automatic calculation of step size and inertia parameter
for convolutional dictionary learning. <em>PR</em>, <em>152</em>,
110443. (<a href="https://doi.org/10.1016/j.patcog.2024.110443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergent methods available for convolutional dictionary learning (CDL) are the proximal gradient method (PGM) and the inertial proximal gradient method (IPGM). However, it is not trivial and heuristic for IPGM to set the step size and inertia parameter, and IPGM produces local minima and oscillating solutions. To address these issues, in this paper, we introduce a dry friction, which has an oscillation-alleviating property. Specifically, the proposed IPGM with dry friction (IPGM-DF) generates the composite proximal mappings, whose construction and optimization solver are two challenges. An auxiliary function is designed to construct the composite proximal mappings, whose optimization problem is solved by the alternating direction method of multipliers. Fortunately, IPGM-DF obtains the formulas of the step size and inertia parameter. The finite convergence of IPGM-DF is proved. Experimental results of image reconstruction, separation, and fusion demonstrate the superiority of IPGM-DF over IPGM and the state-of-the-art methods. For image reconstruction, the objective function value of IPGM-DF is reduced by about 38.708% than that of IPGM. Throughout alleviating oscillations, IPGM-DF obtains a much lower value of the objective function than IPGM, which indicates that IPGM-DF jumps out of the local minima of IPGM. In addition, the average PSNR of IPGM-DF is about 3.5 dB larger than that of IPGM and the state-of-the-art methods. The code is available.},
  archive      = {J_PR},
  author       = {Jinjia Wang and Pengyu Li and Yali Zhang and Ze Li and Jingchen Xu and Qian Wang and Jing Li},
  doi          = {10.1016/j.patcog.2024.110443},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110443},
  shortjournal = {Pattern Recognition},
  title        = {Automatic calculation of step size and inertia parameter for convolutional dictionary learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). E2F-net: Eyes-to-face inpainting via StyleGAN latent space.
<em>PR</em>, <em>152</em>, 110442. (<a
href="https://doi.org/10.1016/j.patcog.2024.110442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face inpainting, the technique of restoring missing or damaged regions in facial images, is pivotal for applications like face recognition in occluded scenarios and image analysis with poor-quality captures. This process not only needs to produce realistic visuals but also preserve individual identity characteristics. The aim of this paper is to inpaint a face given periocular region (eyes-to-face) through a proposed new Generative Adversarial Network (GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach extracts identity and non-identity features from the periocular region using two dedicated encoders have been used. The extracted features are then mapped to the latent space of a pre-trained StyleGAN generator to benefit from its state-of-the-art performance and its rich, diverse and expressive latent space without any additional training. We further improve the StyleGAN&#39;s output to find the optimal code in the latent space using a new optimization for GAN inversion technique. Our E2F-Net requires a minimum training process reducing the computational complexity as a secondary benefit. Through extensive experiments, we show that our method successfully reconstructs the whole face with high quality, surpassing current techniques, despite significantly less training and supervision efforts. We have generated seven eyes-to-face datasets based on well-known public face datasets for training and verifying our proposed methods. The code and datasets are publicly available. 1},
  archive      = {J_PR},
  author       = {Ahmad Hassanpour and Fatemeh Jamalbafrani and Bian Yang and Kiran Raja and Raymond Veldhuis and Julian Fierrez},
  doi          = {10.1016/j.patcog.2024.110442},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110442},
  shortjournal = {Pattern Recognition},
  title        = {E2F-net: Eyes-to-face inpainting via StyleGAN latent space},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Appearance debiased gaze estimation via stochastic
subject-wise adversarial learning. <em>PR</em>, <em>152</em>, 110441.
(<a href="https://doi.org/10.1016/j.patcog.2024.110441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, appearance-based gaze estimation has been attracting attention in computer vision, and remarkable improvements have been achieved using various deep learning techniques. Despite such progress, most methods aim to infer gaze vectors from images directly, which causes overfitting to person-specific appearance factors. In this paper, we address these challenges and propose a novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE), which trains a network to generalize the appearance of subjects. We design a Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face identity classifier and a proposed adversarial loss. The proposed loss generalizes face appearance factors so that the identity classifier inferences a uniform probability distribution. In addition, the Fgen-Net is trained by a learning mechanism that optimizes the network by reselecting a subset of subjects at every training step to avoid overfitting. Our experimental results verify the robustness of the method in that it yields state-of-the-art performance, achieving 3.89°and 4.42°on the MPIIFaceGaze and EyeDiap datasets, respectively. Furthermore, we demonstrate the positive generalization effect by conducting further experiments using face images involving different styles generated from the generative model.},
  archive      = {J_PR},
  author       = {Suneung Kim and Woo-Jeoung Nam and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2024.110441},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110441},
  shortjournal = {Pattern Recognition},
  title        = {Appearance debiased gaze estimation via stochastic subject-wise adversarial learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyper-feature aggregation and relaxed distillation for class
incremental learning. <em>PR</em>, <em>152</em>, 110440. (<a
href="https://doi.org/10.1016/j.patcog.2024.110440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although neural networks have been used extensively in pattern recognition scenarios, the pre-acquisition of datasets is still challenging. In most pattern recognition areas, preparing a training dataset that covers all data domains is difficult. Incremental learning was proposed to update neural networks in an online manner, but the catastrophic forgetting issue still needs to be studied. Class-incremental learning is one of the most challenging incremental learning contexts; it trains a unified model to classify all incrementally arriving classes learned thus far equally. Prior studies on class-incremental learning favor model stability over plasticity to realize old knowledge reservation and prevent catastrophic forgetting. Consequently, the model’s plasticity is omitted, leading to difficult generalization on new data. We propose a novel distillation-based method named Hyper-feature Aggregation and Relaxed Distillation (HARD) to realize balanced optimization of old and new knowledge. The aggregation of features is proposed to capture the global semantics while maintaining the diversity of the feature distribution after promoting representations of exemplars to higher dimensions. The proposed algorithm also introduces a relaxed restriction in the hyper-feature space to conditions the hyper-feature space through a normalized comparison of the relation matrices. Following generalization on more classes, the model is encouraged to rebuild the feature distribution when meeting new classes and to fine-tune the feature space to realize more distinct interclass boundaries. Extensive experiments were conducted on two benchmark datasets, and consistent improvements under diverse experimental settings demonstrated the effectiveness of the proposed approach.},
  archive      = {J_PR},
  author       = {Ran Wu and Huanyu Liu and Zongcheng Yue and Jun-Bao Li and Chiu-Wing Sham},
  doi          = {10.1016/j.patcog.2024.110440},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110440},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-feature aggregation and relaxed distillation for class incremental learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual feature-reutilization inception network.
<em>PR</em>, <em>152</em>, 110439. (<a
href="https://doi.org/10.1016/j.patcog.2024.110439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Capturing feature information effectively is of great importance in the field of computer vision. With the development of convolutional neural networks, concepts like residual connection and multiple scales promote continual performance gains in diverse deep learning vision tasks. In this paper, novel residual feature-reutilization inception and split-residual feature-reutilization inception are proposed to improve performance on various vision tasks. It consists of four parallel branches, each with convolutional kernels of different sizes. These branches are interconnected by hierarchically organized channels, similar to residual connections, facilitating information exchange and rich dimensional variations at different levels. This structure enables the acquisition of features with varying granularity and effectively broadens the span of the receptive field in each network layer. Moreover, according to the network structure designed above, split-residual feature-reutilization inceptions can adjust the split ratio of the input information, thereby reducing the number of parameters and guaranteeing the model performance. Specifically, in image classification experiments based on popular vision datasets, such as CIFAR10 (97.94%), CIFAR100 (85.91%), Tiny Imagenet (70.54%) and ImageNet (80.83%), we obtain state-of-the-art results compared with other modern models under the premise that the models’ sizes are approximate and no additional data is used.},
  archive      = {J_PR},
  author       = {Yuanpeng He and Wenjie Song and Lijian Li and Tianxiang Zhan and Wenpin Jiao},
  doi          = {10.1016/j.patcog.2024.110439},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110439},
  shortjournal = {Pattern Recognition},
  title        = {Residual feature-reutilization inception network},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multi-text union for stable text-to-image synthesis
learning. <em>PR</em>, <em>152</em>, 110438. (<a
href="https://doi.org/10.1016/j.patcog.2024.110438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have significantly boosted the performance of text-to-image generation tasks in recent years. To train the generator, losses like reconstruction loss or adversarial loss between the generated image and ground truth image are widely adopted by recent works. These losses are all built over one assumption: the given text descriptions can describe the corresponding image perfectly. Unfortunately, this assumption is not satisfied in many cases, especially in datasets like COCO with complicated scenes due to the variance of annotator experience and focal point. This paper addresses this issue by proposing a multi-text-to-image training framework, which adaptively adjusts the weights of all the text descriptions corresponding to a specific image to generate the union description features. With the union description features, the generator can generate more visual-consistent images and mitigate the negative optimization caused by incomplete or inconsistent text descriptions. To better measure the similarity between generated images and multi-text descriptions, we also reformulate the process of multi-modal matching loss to better measure the similarity between image and multi-text descriptions. Extensive experiments on relevant benchmarks CUB and COCO prove the proposed method’s effectiveness and superiority compared to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yan Zhou and Jiechang Qian and Huaidong Zhang and Xuemiao Xu and Huajie Sun and Fanzhi Zeng and Yuexia Zhou},
  doi          = {10.1016/j.patcog.2024.110438},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110438},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive multi-text union for stable text-to-image synthesis learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distillation embedded absorbable pruning for fast object
re-identification. <em>PR</em>, <em>152</em>, 110437. (<a
href="https://doi.org/10.1016/j.patcog.2024.110437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining knowledge distillation (KD) and network pruning (NP) shows promise in learning a light network to accelerate object re-identification. However, KD requires an untrained student network to establish more critical connections in early epochs, but NP demands a well-trained student network to avoid destroying critical connections. This presents a dilemma, potentially leading to a collapse of the student network and harming object Re-ID performance. For that, we propose a distillation embedded absorbable pruning (DEAP) method. We design a pruner-convolution-pruner (PCP) unit to resolve the dilemma by loading NP’s sparse regularization on extra untrained pruners. Additionally, we propose an asymmetric relation knowledge distillation method. It readily transfers feature representation knowledge and asymmetric pairwise similarity knowledge without using additional adaptation modules. Finally, we apply re-parameterization to absorb pruners of PCP units to simplify student networks. Experiments demonstrate the superiority of DEAP, such as on the VeRi-776 dataset, with ResNet-101 as a teacher, DEAP saves 73.24% of model parameters and 71.98% of floating-point operations without sacrificing accuracy.},
  archive      = {J_PR},
  author       = {Yi Xie and Hanxiao Wu and Jianqing Zhu and Huanqiang Zeng},
  doi          = {10.1016/j.patcog.2024.110437},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110437},
  shortjournal = {Pattern Recognition},
  title        = {Distillation embedded absorbable pruning for fast object re-identification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards effective person search with deep learning: A survey
from systematic perspective. <em>PR</em>, <em>152</em>, 110434. (<a
href="https://doi.org/10.1016/j.patcog.2024.110434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search detects and retrieves simultaneously a query person across uncropped scene images captured by multiple non-overlapping cameras. In light of the deep learning advancement, person search has emerged as a promising research direction that demonstrates great potential for real-world applications. This paper presents a systematic survey of deep learning methods for person search. Different from existing categorizations, we propose a new taxonomy that dissects person search models into four major components i.e. , proposal prediction, feature representation learning, training objectives, and ranking optimization. The most representative works in each component are summarized with highlighted contributions to this field. An in-depth analysis is provided upon evaluation performances of state-of-the-art person search models together with a summary of benchmark datasets. Despite that significant progress has been made to date, practical and extendable person search remains an open task. We conclude with discussions on those under-explored yet challenging datasets and learning mechanisms for real-world demands to inspire future research directions.},
  archive      = {J_PR},
  author       = {Pengcheng Zhang and Xiaohan Yu and Chen Wang and Jin Zheng and Xin Ning and Xiao Bai},
  doi          = {10.1016/j.patcog.2024.110434},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110434},
  shortjournal = {Pattern Recognition},
  title        = {Towards effective person search with deep learning: A survey from systematic perspective},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frozen is better than learning: A new design of
prototype-based classifier for semantic segmentation. <em>PR</em>,
<em>152</em>, 110431. (<a
href="https://doi.org/10.1016/j.patcog.2024.110431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation models comprise an encoder to extract features and a classifier for prediction. However, the learning of the classifier suffers from the ambiguity which is caused by two factors: (1) the weights of a classifier for similar categories may have positive similarities lowing the performance for similar categories, named correlation ambiguity , and (2) the classifier is prone to predict the category with a larger ℓ 2 ℓ2 norm and vice versa, termed prior ambiguity . To comedy the issues, we propose Category-Basis Prototype (CBP), frozen and mutually orthogonalized prototypes with equal ℓ 2 ℓ2 norm . Orthogonalization prevents the prototypes from being similar to each other and the equality decouples the prediction from the ℓ 2 ℓ2 norm. To better shape the feature space, we propose Online Centroid Contrastive Loss (OCCL) equipped with centroid and category-level losses. Experiments show that our method yields compelling results over two widely applied benchmarks indicating the effectiveness of our methods.},
  archive      = {J_PR},
  author       = {Jialei Chen and Daisuke Deguchi and Chenkai Zhang and Xu Zheng and Hiroshi Murase},
  doi          = {10.1016/j.patcog.2024.110431},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110431},
  shortjournal = {Pattern Recognition},
  title        = {Frozen is better than learning: A new design of prototype-based classifier for semantic segmentation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross co-teaching for semi-supervised medical image
segmentation. <em>PR</em>, <em>152</em>, 110426. (<a
href="https://doi.org/10.1016/j.patcog.2024.110426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excellent performance has been achieved on semi-supervised medical image segmentation, but existing algorithms perform relatively poorly for objects with variable topologies and weak boundaries. In this paper, we propose a novel cross co-teaching framework, called Cross-structure-task Collaborative Teaching (CroCT), which not only can effectively handle variable topologies, but also strengthens the learning for weak boundaries of unlabeled data. Specifically, a new cross-structure-task collaborative teaching mechanism is developed based on our designed “E-Net” structure composed of a shared encoder and two decoder branches with distinct learning paradigms, which asks these two branches to regress topology-aware signed distance functions and densely-predicted segmentation masks for each other. Powered by the collaboration across different structural biases and sequence-related tasks, our CroCT can extract more discriminative yet complementary representations from abundant raw medical data to promote the consistency learning generalization, further boosting the performance for tackling highly diverse shapes and topological changes intra-/inter-slices. Besides, it guarantees the diversities from multi-levels, i.e., structure and task perspectives, to preclude prediction uncertainty. In addition, a novel adaptive boundary enhancing (ABE) module is proposed to introduce compact annularly enhanced boundary features into semi-supervised training, which significantly improves weak boundary perception ability for unlabeled data while facilitating collaborative teaching for efficiently propagating complementary knowledge across different branches. The extensive experiments on three challenging medical benchmarks, employing different labeled settings, demonstrate the superiority of our CroCT over recent state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Fan Zhang and Huiying Liu and Jinjiang Wang and Jun Lyu and Qing Cai and Huafeng Li and Junyu Dong and David Zhang},
  doi          = {10.1016/j.patcog.2024.110426},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110426},
  shortjournal = {Pattern Recognition},
  title        = {Cross co-teaching for semi-supervised medical image segmentation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R-FACE: Reference guided face component editing.
<em>PR</em>, <em>152</em>, 110425. (<a
href="https://doi.org/10.1016/j.patcog.2024.110425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent studies have made significant processes in face portrait editing, simple and accurate face component editing remains a challenge. Face components, such as eyes, nose, and mouth, have a shape style that is difficult to transfer. Existing methods either (1) manipulate pre-defined binary attribute labels, which is difficult to edit the shape of face components with observable changes, or (2) control the shape change by manually editing the intermediate representation (e.g., precise masks or sketches) of face components, which is time-consuming and requires painting skills. To break these limitations, we propose a simple and effective framework for diverse and controllable face component editing with geometric changes, which utilizes an inpainting model to learn the shape of face components from reference images without any manual annotations. In order to guide generated images to learn the shape style of reference face components, an example-guided attention module is designed to help the network focus on target face component regions. Moreover, a novel domain verification discriminator is introduced to pull the realism of the generated facial component close to the source face. Experimental results demonstrate that the proposed method outperforms conventional methods in image quality, editing accuracy, and diversity of results (see Video Demo ).},
  archive      = {J_PR},
  author       = {Qiyao Deng and Jie Cao and Yunfan Liu and Qi Li and Zhenan Sun},
  doi          = {10.1016/j.patcog.2024.110425},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110425},
  shortjournal = {Pattern Recognition},
  title        = {R-FACE: Reference guided face component editing},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Test-time adaptation for 6D pose tracking. <em>PR</em>,
<em>152</em>, 110390. (<a
href="https://doi.org/10.1016/j.patcog.2024.110390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a test-time adaptation for 6D object pose tracking that learns to adapt a pre-trained model to track the 6D pose of novel objects. We consider the problem of 6D object pose tracking as a 3D keypoint detection and matching task and present a model that extracts 3D keypoints. Given an RGB-D image and the mask of a target object for each frame, the proposed model consists of the self- and cross-attention modules to produce the features that aggregate the information within and across frames, respectively. By using the keypoints detected from the features for each frame, we estimate the pose changes between two frames, which enables 6D pose tracking when the 6D pose of a target object in the initial frame is given. Our model is first trained in a source domain, a category-level tracking dataset where the ground truth 6D pose of the object is available. To deploy this pre-trained model to track novel objects, we present a test-time adaptation strategy that trains the model to adapt to the target novel object by self-supervised learning. Given an RGB-D video sequence of the novel object, the proposed self-supervised losses encourage the model to estimate the 6D pose changes that can keep the photometric and geometric consistency of the object. We validate our method on the NOCS-REAL275 dataset and our collected dataset, and the results show the advantages of tracking novel objects. The collected dataset and visualisation of tracking results are available: https://qm-ipalab.github.io/TA-6DT/},
  archive      = {J_PR},
  author       = {Long Tian and Changjae Oh and Andrea Cavallaro},
  doi          = {10.1016/j.patcog.2024.110390},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110390},
  shortjournal = {Pattern Recognition},
  title        = {Test-time adaptation for 6D pose tracking},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible image denoising model with multi-layer conditional
feature modulation. <em>PR</em>, <em>152</em>, 110372. (<a
href="https://doi.org/10.1016/j.patcog.2024.110372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For flexible non-blind image denoising, existing deep networks usually concatenate noisy image and noise level map as the input for handling various noise levels with a single model. However, in this kind of solution, the noise variance (i.e., noise level) is only deployed to modulate the first layer of convolution feature with channel-wise shifting, which is limited in balancing noise removal and detail preservation. In this paper, we present a novel flexible image denoising network (CFMNet) by equipping a U-Net backbone with multi-layer conditional feature modulation (CFM) modules. In comparison to channel-wise shifting only in the first layer, CFMNet can make better use of noise level information by deploying multiple layers of CFM. Moreover, each CFM module takes convolutional features from both noisy image and noise level map as input for better trade-off between noise removal and detail preservation. Experimental results show that our CFMNet is effective in exploiting noise level information for flexible non-blind denoising, and performs favorably against the existing deep image denoising methods in terms of both quantitative metrics and visual quality.},
  archive      = {J_PR},
  author       = {Jiazhi Du and Xin Qiao and Zifei Yan and Hongzhi Zhang and Wangmeng Zuo},
  doi          = {10.1016/j.patcog.2024.110372},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110372},
  shortjournal = {Pattern Recognition},
  title        = {Flexible image denoising model with multi-layer conditional feature modulation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-environment self-adaptive network for aerial-view
geo-localization. <em>PR</em>, <em>152</em>, 110363. (<a
href="https://doi.org/10.1016/j.patcog.2024.110363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aerial-view geo-localization tends to determine an unknown position through matching the drone-view image with the geo-tagged satellite-view image. This task is mostly regarded as an image retrieval problem. The key underpinning this task is to design a series of deep neural networks to learn discriminative image descriptors. However, existing methods meet large performance drops under realistic weather, such as rain and fog, since they do not take the domain shift between the training data and multiple test environments into consideration. To minor this domain gap, we propose a Multiple-environment Self-adaptive Network (MuSe-Net) to dynamically adjust the domain shift caused by environmental changing. In particular, MuSe-Net employs a two-branch neural network containing one multiple-environment style extraction network and one self-adaptive feature extraction network. As the name implies, the multiple-environment style extraction network is to extract the environment-related style information, while the self-adaptive feature extraction network utilizes an adaptive modulation module to dynamically minimize the environment-related style gap. Extensive experiments on three widely-used benchmarks, i.e. , University-1652, SUES-200, and CVUSA, demonstrate that the proposed MuSe-Net achieves a competitive result for geo-localization in multiple environments. Furthermore, we observe that the proposed method also shows great potential to the unseen extreme weather, such as mixing the fog, rain and snow.},
  archive      = {J_PR},
  author       = {Tingyu Wang and Zhedong Zheng and Yaoqi Sun and Chenggang Yan and Yi Yang and Tat-Seng Chua},
  doi          = {10.1016/j.patcog.2024.110363},
  journal      = {Pattern Recognition},
  month        = {8},
  pages        = {110363},
  shortjournal = {Pattern Recognition},
  title        = {Multiple-environment self-adaptive network for aerial-view geo-localization},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal pattern mining for knowledge discovery in the early
prediction of septic shock. <em>PR</em>, <em>151</em>, 110436. (<a
href="https://doi.org/10.1016/j.patcog.2024.110436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal pattern mining can be employed to detect patterns and trends in a patient&#39;s health status as it evolves over time. However, these methods often produce an overwhelming number of patterns, impeding knowledge discovery and practical implementation in acute care settings. To address this, we propose a framework that focuses on identifying a concise set of relevant temporal patterns and static variables from electronic health records for the early prediction of septic shock. Sepsis is caused by an adverse immune response to infection that triggers widespread inflammation throughout the body, which can progress to septic shock and ultimately result in death if not treated promptly. The analysis of health state patterns in sepsis patients over time offers the potential to predict septic shock prior to its onset, enabling proactive healthcare interventions. Our framework incorporates a temporal pattern mining method and four feature selection techniques. We discover that selecting features based on a model-based wrapper approach yields the highest prediction performance among these techniques. On the other hand, the use of information value identifies more multi-state patterns with abnormal health states, providing healthcare providers with valuable indicators of patient deterioration.},
  archive      = {J_PR},
  author       = {Ruoting Li and Joseph K. Agor and Osman Y. Özaltın},
  doi          = {10.1016/j.patcog.2024.110436},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110436},
  shortjournal = {Pattern Recognition},
  title        = {Temporal pattern mining for knowledge discovery in the early prediction of septic shock},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UnitModule: A lightweight joint image enhancement module for
underwater object detection. <em>PR</em>, <em>151</em>, 110435. (<a
href="https://doi.org/10.1016/j.patcog.2024.110435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection faces the problem of underwater image degradation, which affects the performance of the detector. Underwater object detection methods based on noise reduction and image enhancement usually do not provide images preferred by the detector or require additional datasets. In this paper, we propose a plug-and-play U nderwater joi n t i mage enhancemen t Module (UnitModule) that provides the input image preferred by the detector. We design an unsupervised learning loss for the joint training of UnitModule with the detector without additional datasets to improve the interaction between UnitModule and the detector. Furthermore, a color cast predictor with the assisting color cast loss and a data augmentation called Underwater Color Random Transfer (UCRT) are designed to improve the performance of UnitModule on underwater images with different color casts. Extensive experiments are conducted on DUO for different object detection models, where UnitModule achieves the highest performance improvement of 2.6 AP for YOLOv5-S and gains the improvement of 3.3 AP on the brand-new test set ( URPC t e s t URPCtest ). And UnitModule significantly improves the performance of all object detection models we test, especially for models with a small number of parameters. In addition, UnitModule with a small number of parameters of 31K has little effect on the inference speed of the original object detection model. Our quantitative and visual analysis also demonstrates the effectiveness of UnitModule in enhancing the input image and improving the perception ability of the detector for object features. The code is available at https://github.com/LEFTeyex/UnitModule .},
  archive      = {J_PR},
  author       = {Zhuoyan Liu and Bo Wang and Ye Li and Jiaxian He and Yunfeng Li},
  doi          = {10.1016/j.patcog.2024.110435},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110435},
  shortjournal = {Pattern Recognition},
  title        = {UnitModule: A lightweight joint image enhancement module for underwater object detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A distortion model guided adversarial surrogate for
recaptured document detection. <em>PR</em>, <em>151</em>, 110433. (<a
href="https://doi.org/10.1016/j.patcog.2024.110433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the ever-growing need for e-business, document presentation attack detection (DPAD) is an important forensic task. With deep learning, the performances of DPAD methods have been improved significantly. However, the cross-domain performance under different types of document images is not yet satisfactory. In this work, we propose to study the document recapturing distortion model (DM), which is employed in guiding the training of an end-to-end surrogate distortion model. This surrogate model is incorporated into the DPAD scheme in an adversarial training fashion, yielding the proposed Adversarial Surrogate-based DPAD (AS-DPAD) scheme. The surrogate model actively generates adversarial samples with recapturing distortions that confuse the DPAD model. Meanwhile, the DPAD backbone learns a latent space that considers the distances between the generated and real recaptured samples to achieve a better generalization performance. A challenging cross-domain protocol is conducted in our experiment. The results confirm that our DM improves the efficacy of the trained surrogate distortion model and also validate that the proposed adversarial training strategy leads to a significant performance gain in the evaluation of document images with different contents.},
  archive      = {J_PR},
  author       = {Changsheng Chen and Xijin Li and Baoying Chen and Haodong Li},
  doi          = {10.1016/j.patcog.2024.110433},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110433},
  shortjournal = {Pattern Recognition},
  title        = {A distortion model guided adversarial surrogate for recaptured document detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving ellipse fitting via multi-scale smoothing and
key-point searching. <em>PR</em>, <em>151</em>, 110432. (<a
href="https://doi.org/10.1016/j.patcog.2024.110432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast and efficient fitting of accurate ellipses from data points has many applications in pattern recognition, machine vision, and robotics. However, the fitting accuracy may significantly degrade in the existence of outliers, such as the least-squares-based approaches. Despite robust methods attaining more accurate results than the least-squares manner under the contamination of outliers, they typically require the careful tuning of the hyper-parameters for good results. To mitigate the outlier disturbance, in this paper, we propose a conceptually simple yet quite useful preprocessing framework for high-precision ellipse fitting. Firstly, we leverage multi-scale operators to shrink the input image, by which a large number of outliers can be removed, followed by the smoothing of the sub-image to further improve the data quality. Then, we propose a key-point searching method to enhance the fitting precision via the analysis of the discrete pixel data in images. We prove that key-point-based ellipse fitting gives the upper bound of the approximation error generated by other sampled points with the same ellipse. Based on the key-point pairs inside and outside the ellipse, we further calculate their barycentric points and then perform fitting on these points to attain high-precision ellipses. We conduct extensive experiments on synthetic and real-world images to validate the proposed method and compare it with representative state-of-the-art approaches. Quantitative and qualitative results demonstrate that our method has more accurate and robust performance than competitors. Additionally, we employ the proposed method to compared approaches as a preprocessing step. Experiments demonstrate that our method is effective to significantly improve their fitting accuracy. Our source code is freely available at https://github.com/ChengQian09/MSKPF .},
  archive      = {J_PR},
  author       = {Xiao-Diao Chen and Cheng Qian and Mingyang Zhao and Jun-Hai Yong and Dong-Ming Yan},
  doi          = {10.1016/j.patcog.2024.110432},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110432},
  shortjournal = {Pattern Recognition},
  title        = {Improving ellipse fitting via multi-scale smoothing and key-point searching},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-incremental learning without forgetting based on
random vector functional link networks. <em>PR</em>, <em>151</em>,
110430. (<a href="https://doi.org/10.1016/j.patcog.2024.110430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental learning is a paradigm that extends knowledge by learning from new data, often used to add new classes to an existing model or to learn a new domain. It imposes strict limitations on the model’s access to data from previous tasks, making it similar to the human learning process. The main challenge of incremental learning is catastrophic forgetting, where previous knowledge is severely forgotten while learning new tasks. In this work, we propose a novel approach for domain-incremental learning. Inspired by the Normal Equation , we accumulate the Gram Matrix from each task’s hidden layer output to update a simplified RVFL model. This algorithm achieves performance comparable to joint training while strictly adhering to privacy restrictions. With issues such as forgetting, storage requirements and privacy protection be addressed, this algorithm has the potential to play a crucial role in the field of edge computing and other related fields.},
  archive      = {J_PR},
  author       = {Chong Liu and Yi Wang and Dong Li and Xizhao Wang},
  doi          = {10.1016/j.patcog.2024.110430},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110430},
  shortjournal = {Pattern Recognition},
  title        = {Domain-incremental learning without forgetting based on random vector functional link networks},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAIC-NET: Semi-supervised OCTA vessel segmentation with
self-supervised pretext task and dual consistency training. <em>PR</em>,
<em>151</em>, 110429. (<a
href="https://doi.org/10.1016/j.patcog.2024.110429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Coherence Tomography Angiography(OCTA) vessel segmentation is a challenging task. On the one hand, the complex structure of the capillary networks presents significant obstacles to achieving accurate vessel segmentation. On the other hand, current research on OCTA vessel segmentation heavily relies on high-quality manual annotations, especially in fully-supervised approaches. In contrast, pixel-level annotation of OCTA vessels is time-consuming and labor-intensive. To address these issues, we propose a semi-supervised method called HAIC-Net, which integrates self-supervised learning with a homologous augmented image classification pretext task and dual consistency training with data perturbation consistency and topological connectivity consistency. Firstly, we design a self-supervised homologous augmented image classification pretext task that directs the model’s attention to similar vascular features in homologous augmented images, thereby extracting rich vessel information from unlabeled images and reducing the dependence on manual annotations. Secondly, we introduce a dual-consistency structure with topological connectivity consistency to provide constraint from a topological perspective, which is consistent with the topological characteristics of the vascular network, to enhance the segmentation network’s sensitivity to vessel connectivity and decrease the topological errors in segmentation results. We conduct experiments on two publicly available datasets and one private dataset and validate the state-of-the-art performance of the proposed method. On the ROSE-1 dataset, our method achieves 0.9143 accuracy and 0.7658 dice coefficient, surpassing other current semi-supervised methods and approaching the performance of state-of-the-art fully supervised methods. The same result can also be observed on OCTA500 and our private dataset, demonstrating the effectiveness and superiority of our approach.},
  archive      = {J_PR},
  author       = {Hailan Shen and Zheng Tang and Yajing Li and Xuanchu Duan and Zailiang Chen},
  doi          = {10.1016/j.patcog.2024.110429},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110429},
  shortjournal = {Pattern Recognition},
  title        = {HAIC-NET: Semi-supervised OCTA vessel segmentation with self-supervised pretext task and dual consistency training},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LCSeg-net: A low-contrast images semantic segmentation model
with structural and frequency spectrum information. <em>PR</em>,
<em>151</em>, 110428. (<a
href="https://doi.org/10.1016/j.patcog.2024.110428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation in low-contrast images is a challenging problem due to the ambiguous boundary of the segmented target and indistinguishable noise. Current models generally segment the target with local features, which leads to the lack of structural information, which reduces the performance. And they directly fuse multi-scale features to keep details, while it may integrate noise into the fused features. These problems reduce the performance of segmentation in the low-contrast image. To solve the above issues, we propose an image semantic segmentation model called Low-Contrast Segmentation Net (LCSeg-Net). The model enhances the structural information with the global context and reduces the noise of the fused features through the adaptive fusion way. Meanwhile, to improve the robustness of LCSeg-Net, we augment the low-frequency spectrum of the fused feature in the training phase. Extensive experiments are conducted on the five public datasets. Results show the proposed model achieves the best comprehensive performance in the low-contrast image.},
  archive      = {J_PR},
  author       = {Haochen Yuan and Junjie Peng},
  doi          = {10.1016/j.patcog.2024.110428},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110428},
  shortjournal = {Pattern Recognition},
  title        = {LCSeg-net: A low-contrast images semantic segmentation model with structural and frequency spectrum information},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial–temporal hypergraph based on dual-stage attention
network for multi-view data lightweight action recognition. <em>PR</em>,
<em>151</em>, 110427. (<a
href="https://doi.org/10.1016/j.patcog.2024.110427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the problems of irrelevant frames and high model complexity in action recognition, we propose a Spatial–Temporal Hypergraph based on Dual-Stage Attention Network (STHG-DAN) for multi-view data lightweight action recognition. It includes two stages: Temporal Attention Mechanism based on Trainable Threshold (TAM-TT) and Hypergraph Convolution based on Dynamic Spatial–Temporal Attention Mechanism (HG-DSTAM). In the first stage, TAM-TT uses a learning threshold to extract keyframes from multi-view videos, with the multi-view data serving as a guarantee for providing more comprehensive information subsequently; In the second stage, HG-DSTAM divides the human joints into three parts: trunk, hand and leg to build spatial–temporal hypergraphs, extracts high-order features from spatial–temporal hypergraphs constructed of multi-view human body joints, inputs them into the dynamic spatial–temporal attention mechanism, and learns the intra frame correlation of multi-view data between the joint features of body parts, which can obtain the significant areas of action; We use multi-scale convolution operation and depth separable network, which can realize efficient action recognition with a few trainable parameters. We experiment on the NTU-RGB+D, NTU-RGB+D 120 and the imitating traffic police gesture dataset. The performance and accuracy of the model are better than the existing algorithms, effectively improving the machine and human body language interaction cognitive ability.},
  archive      = {J_PR},
  author       = {Zhixuan Wu and Nan Ma and Cheng Wang and Cheng Xu and Genbao Xu and Mingxing Li},
  doi          = {10.1016/j.patcog.2024.110427},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110427},
  shortjournal = {Pattern Recognition},
  title        = {Spatial–temporal hypergraph based on dual-stage attention network for multi-view data lightweight action recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning for medical image analysis: A survey.
<em>PR</em>, <em>151</em>, 110424. (<a
href="https://doi.org/10.1016/j.patcog.2024.110424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning in medical imaging often faces a fundamental dilemma, namely, the small sample size problem. Many recent studies suggest using multi-domain data pooled from different acquisition sites/centers to improve statistical power. However, medical images from different sites cannot be easily shared to build large datasets for model training due to privacy protection reasons. As a promising solution, federated learning, which enables collaborative training of machine learning models based on data from different sites without cross-site data sharing, has attracted considerable attention recently. In this paper, we conduct a comprehensive survey of the recent development of federated learning methods in medical image analysis. We have systematically gathered research papers on federated learning and its applications in medical image analysis published between 2017 and 2023. Our search and compilation were conducted using databases from IEEE Xplore, ACM Digital Library, Science Direct, Springer Link, Web of Science, Google Scholar, and PubMed. In this survey, we first introduce the background of federated learning for dealing with privacy protection and collaborative learning issues. We then present a comprehensive review of recent advances in federated learning methods for medical image analysis. Specifically, existing methods are categorized based on three critical aspects of a federated learning system, including client end, server end, and communication techniques. In each category, we summarize the existing federated learning methods according to specific research problems in medical image analysis and also provide insights into the motivations of different approaches. In addition, we provide a review of existing benchmark medical imaging datasets and software platforms for current federated learning research. We also conduct an experimental study to empirically evaluate typical federated learning methods for medical image analysis. This survey can help to better understand the current research status, challenges, and potential research opportunities in this promising research field.},
  archive      = {J_PR},
  author       = {Hao Guan and Pew-Thian Yap and Andrea Bozoki and Mingxia Liu},
  doi          = {10.1016/j.patcog.2024.110424},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110424},
  shortjournal = {Pattern Recognition},
  title        = {Federated learning for medical image analysis: A survey},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Dual teachers for self-knowledge distillation. <em>PR</em>,
<em>151</em>, 110422. (<a
href="https://doi.org/10.1016/j.patcog.2024.110422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an efficient self-knowledge distillation framework, Dual Teachers for Self-Knowledge Distillation (DTSKD), where the student receives self-supervisions by dual teachers from two substantially different fields, i.e., the past learning history and the current network structure. Specifically, DTSKD trains a considerably lightweight multi-branch network and acquires predictions from each, which are simultaneously supervised by a historical teacher from the previous epoch and a structural teacher under the current iteration. To our best knowledge, it is the first attempt to jointly conduct historical and structural self-knowledge distillation in a unified framework where they demonstrate complementary and mutual benefits. The Mixed Fusion Module (MFM) is further developed to bridge the semantic gap between deep stages and shallow branches by iteratively fusing multi-stage features based on the top-down topology. Extensive experiments prove the effectiveness of our proposed method, showing superior performance over related state-of-the-art self-distillation works on three datasets: CIFAR-100, ImageNet-2012, and PASCAL VOC.},
  archive      = {J_PR},
  author       = {Zheng Li and Xiang Li and Lingfeng Yang and Renjie Song and Jian Yang and Zhigeng Pan},
  doi          = {10.1016/j.patcog.2024.110422},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110422},
  shortjournal = {Pattern Recognition},
  title        = {Dual teachers for self-knowledge distillation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative regularized input manifold for multilayer
perceptron. <em>PR</em>, <em>151</em>, 110421. (<a
href="https://doi.org/10.1016/j.patcog.2024.110421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilayer perceptron (MLP) fails to discriminate the ambiguous inputs belonging to the overlapping regions of multiple classes, resulting in misclassification. To classify the input samples accurately according to their classes, removing the ambiguity that occurred due to the sharing of common input space is important. In this article, a novel neural network framework, called Discriminative Regularized Input Manifold MLP (DRIM-MLP) is proposed to reduce this ambiguity and improve the classification accuracy. The proposed framework consists of two different feed forward networks that are trained simultaneously: (i) DRIM and (ii) MLP. The proposed DRIM learns the input distribution during its training and the learnt information is incorporated during the training of MLP to reduce the ambiguity. Simultaneously, the class information learnt by MLP is incorporated into the DRIM to learn discriminative information of the input distribution. The hidden layer output of DRIM estimates (i) the input of DRIM using the weights of hidden and output layers of DRIM and (ii) the class output of MLP using the weights of hidden and output layers of MLP. Here, the estimated class output, learnt by the DRIM that contains information of the input distribution, is used as a regularizer for the MLP to minimize the difference between the estimated class output of DRIM and the estimated class output of MLP itself. The hidden layer of MLP also estimates: (i) the class output of MLP using the weights between hidden and output layers of MLP and (ii) the input of DRIM using the weights between hidden and output layers of DRIM. Here, the estimated input learnt by MLP that contains class information is used as a regularizer for DRIM to minimize the difference between the estimated input of DRIM itself and the estimated input of MLP. These two regularizers are respectively called the Input manifold Regularizer (ImR) and the Discriminative Regularizer (DiscR). The experimental results based on ten standard data sets strongly support the effectiveness of the proposed DRIM-MLP compared to conventional MLP, auto encoder based MLP (AE-MLP), denoising auto encoder based MLP (DAE-MLP), AE-MLP with KLD, DAE-MLP with KLD along with two recent works of state-of-the-art.},
  archive      = {J_PR},
  author       = {Rahul Mondal and Tandra Pal and Prasenjit Dey},
  doi          = {10.1016/j.patcog.2024.110421},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110421},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative regularized input manifold for multilayer perceptron},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view clustering via pseudo-label guide learning and
latent graph structure recovery. <em>PR</em>, <em>151</em>, 110420. (<a
href="https://doi.org/10.1016/j.patcog.2024.110420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering (MvC) accomplishes sample classification tasks by exploring information from different views. Currently, researchers have paid greater attention to graph-based MvC methods. However, most existing methods only consider the original graph structure and pay relatively little attention to the graph structure in the latent space. In addition, most methods need to pay more attention to the consistency of information on different labels. Otherwise, this may lead to the loss of label information. This paper presents a new multi-view clustering framework to address the above issues. The proposed method considers both the information in the latent space and the original data space, which firstly obtains the pseudo-label by latent representation learning and then lets the pseudo-label guide the learning of the complementary information between the raw data views. To ensure the integrity of the data structure, a latent graph structure recovery strategy is designed in the latent space. Finally, an enhanced label fusion strategy is designed to fusion the different types of labels, yielding an information-rich label matrix for clustering. Experimental results demonstrate the proposed method’s effectiveness compared to other advanced approaches.},
  archive      = {J_PR},
  author       = {Ronggang Cai and Hongmei Chen and Yong Mi and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.patcog.2024.110420},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110420},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view clustering via pseudo-label guide learning and latent graph structure recovery},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Structure-aware neural radiance fields without posed
camera. <em>PR</em>, <em>151</em>, 110419. (<a
href="https://doi.org/10.1016/j.patcog.2024.110419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The neural radiance fields (NeRF) for realistic novel view synthesis require camera poses to be pre-acquired by a structure-from-motion (SfM) approach. This two-stage strategy is not convenient to use and degrades the performance because the error in the pose extraction can propagate to the view synthesis. We integrate pose extraction and view synthesis into a jointly optimized process so that they can benefit from each other. For network training, only images are given without pre-known camera poses. The camera poses are obtained by the depth-consistent constraint in which the identical feature in different views has the same world coordinates transformed from the local camera coordinates according to the extracted poses. The depth-consistent constraint is jointly optimized with the pixel color constraint. The poses are represented by a CNN-based deep network, whose input is the related frames. This joint optimization enables NeRF to be aware of the scene’s structure, resulting in improved generalization performance. Experiments on three datasets demonstrate the effectiveness of camera pose estimation and novel view synthesis. Code is available at https://github.com/XTU-PR-LAB/SaNerf .},
  archive      = {J_PR},
  author       = {Shu Chen and Yang Zhang and Yaxin Xu and Beiji Zou},
  doi          = {10.1016/j.patcog.2024.110419},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110419},
  shortjournal = {Pattern Recognition},
  title        = {Structure-aware neural radiance fields without posed camera},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable AI in human motion: A comprehensive approach to
analysis, modeling, and generation. <em>PR</em>, <em>151</em>, 110418.
(<a href="https://doi.org/10.1016/j.patcog.2024.110418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive research has been conducted on analyzing human movements, driven by its diverse practical applications such as human–robot interaction, human learning, and clinical diagnosis. However, the current state-of-the-art still encounters scientific challenges when it comes to modeling human movements. There are two key aspects that need to be addressed. Firstly, new models should consider the stochastic nature of human movement and the physical structure of the human body to accurately predict the patterns in full-body motion descriptors over time. Secondly, while deep learning algorithms have been utilized, they lack explainability in terms of predicting body posture sequences, making it essential to improve their comprehensible representation of human movement. This paper aims to tackle these challenges by presenting three innovative methods for creating explainable representations of human movement. The study formulates human body movement as a state-space model based on the Gesture Operational Model (GOM). Model parameters are estimated through either one-shot training employing Kalman Filters or data-intensive training utilizing artificial neural networks. The trained models are utilized for analyzing the dexterity of expert professionals in full-body movements, enabling the identification of dynamic associations between body joints and gesture recognition. Additionally, these models are employed to generate artificial professional movements.},
  archive      = {J_PR},
  author       = {Brenda Elizabeth Olivas-Padilla and Sotiris Manitsaris and Alina Glushkova},
  doi          = {10.1016/j.patcog.2024.110418},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110418},
  shortjournal = {Pattern Recognition},
  title        = {Explainable AI in human motion: A comprehensive approach to analysis, modeling, and generation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LATFormer: Locality-aware point-view fusion transformer for
3D shape recognition. <em>PR</em>, <em>151</em>, 110413. (<a
href="https://doi.org/10.1016/j.patcog.2024.110413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D shape understanding has achieved significant progress due to the advances of deep learning models on various data formats like images, voxels, and point clouds. Among them, point clouds and multi-view images are two complementary modalities of 3D objects, and learning representations by fusing both of them has been proven to be fairly effective. While prior works typically focus on exploiting global features of the two modalities, herein we argue that more discriminative features can be derived by modeling “where to fuse”. To investigate this, we propose a novel Locality-Aware Point-View Fusion Transformer (LATFormer) for 3D shape retrieval and classification. The core component of LATFormer is a module named Locality-Aware Fusion (LAF) which integrates the local features of correlated regions across the two modalities based on the co-occurrence scores. We further propose to filter out scores with low values to obtain salient local co-occurring regions, which reduces redundancy for the fusion process. In our LATFormer, we utilize the LAF module to fuse the multi-scale features of the two modalities both bidirectionally and hierarchically to obtain more informative features. Comprehensive experiments on four popular 3D shape benchmarks covering 3D object retrieval and classification validate its effectiveness.},
  archive      = {J_PR},
  author       = {Xinwei He and Silin Cheng and Dingkang Liang and Song Bai and Xi Wang and Yingying Zhu},
  doi          = {10.1016/j.patcog.2024.110413},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110413},
  shortjournal = {Pattern Recognition},
  title        = {LATFormer: Locality-aware point-view fusion transformer for 3D shape recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). ISP-IRLNet: Joint optimization of interpretable sampler and
implicit regularization learning network for accerlerated MRI.
<em>PR</em>, <em>151</em>, 110412. (<a
href="https://doi.org/10.1016/j.patcog.2024.110412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed Sensing Magnetic Resonance Imaging (CS-MRI) was proposed to accelerate data acquisition and reconstruct MR images from under-sampled data in k -space. However, the traditional approaches design the sampling patterns separately from the reconstruction process, which often leads to suboptimal reconstruction performance. To address this issue, we propose a joint optimization model dubbed as ISP-IRLNet to yield optimal recovery including an interpretable sampler and a reconstructor using the implicit regularization learning network. In particular, we introduce a probabilistic distribution model of the sampling incorporated into the prior of k -space locations, which provides a robust CS-MRI sample pattern with an explicit expression. To estimate the approximating gradient in backward propagation for more stable training, we employ a differentiable binarization strategy. In addition, we unroll the ADMM optimization for solving the regularized reconstruction model to be a deep network ( i.e. , IRLNet) with a learnable implicit regularization sub-network to learn a regularizer or prior term implicitly. The experiments on our collected brain and public knee datasets demonstrate that our method provides an optimal sample pattern and achieves superior image reconstruction performance compared with existing methods, especially at high acceleration rates.},
  archive      = {J_PR},
  author       = {Xing Li and Yan Yang and Hairong Zheng and Zongben Xu},
  doi          = {10.1016/j.patcog.2024.110412},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110412},
  shortjournal = {Pattern Recognition},
  title        = {ISP-IRLNet: Joint optimization of interpretable sampler and implicit regularization learning network for accerlerated MRI},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label feature selection via latent representation
learning and dynamic graph constraints. <em>PR</em>, <em>151</em>,
110411. (<a href="https://doi.org/10.1016/j.patcog.2024.110411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective method to deal with the curse of dimensionality, multi-label feature selection aims to select the most representative subset of features by eliminating unfavorable features. Although great progress has been made in this field, how to mine adequate supervisory information from multi-label data remains a key challenge. Compared to the latent information of instances, the latent information of instance relevance contains both the basic information of instances and the latent relevance between instances. Base on this knowledge, we propose a novel multi-label feature selection method named LRDG that explores latent representation learning and dynamic graph constraints. Specifically, we introduce the latent representation of instance relevance as supervisory information for pseudo-label learning, and minimize information loss during pseudo-label learning by means of the label manifold, the non-negative constraints, and the minimization of the Frobenius norm between pseudo-labels and ground-truth labels. In addition, considering the shortcomings brought by traditional graph regularization, we propose to use the dynamic graph constructed from low-dimensional pseudo-labels to constrain feature weights. Extensive experiments on various multi-label datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/yunbao520/LRDG .},
  archive      = {J_PR},
  author       = {Yao Zhang and Wei Huo and Jun Tang},
  doi          = {10.1016/j.patcog.2024.110411},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110411},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection via latent representation learning and dynamic graph constraints},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UGNet: Uncertainty aware geometry enhanced networks for
stereo matching. <em>PR</em>, <em>151</em>, 110410. (<a
href="https://doi.org/10.1016/j.patcog.2024.110410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching is a fundamental research area in the field of computer vision. In recent years, iterative methods based on Gated Recurrent Units (GRUs) have showcased remarkable achievements in this domain. Despite their high accuracy, these methods suffer from notable limitations such as a reliance on a large number of iterations and a tendency to lose high-frequency details. To address these issues, we propose a novel uncertainty-aware framework that combines 3D convolution and GRU-based iterations, aiming to improve efficiency and accuracy. Specifically, we first introduce a probabilistic method to jointly train the disparity map and its corresponding uncertainty map using 3D convolutions. Next, leveraging the uncertainty map as a guide, we develop a novel uncertainty reweighting iterative module to assist in identifying errors in the coarse disparity and cost volume, thereby refining the disparity estimation process and significantly improving the iteration efficiency. Moreover, we introduce a high-resolution refinement module that utilizes Pixel Difference Convolution (PDC) to incorporate additional gradient information. This module can fine-tune the disparity estimation to enhance accuracy. Finally, our network is evaluated on multiple widely-used benchmark datasets. The results demonstrate its proficiency in predicting precise boundaries and effectively reduce iterations. Our model achieves comparable performance to other state-of-the-art methods, ranking 1st on KITTI 2015, and 2nd on KITTI 2012. These results validate its strong performance and generalizability.},
  archive      = {J_PR},
  author       = {Zhengkai Qi and Junkang Zhang and Faming Fang and Tingting Wang and Guixu Zhang},
  doi          = {10.1016/j.patcog.2024.110410},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110410},
  shortjournal = {Pattern Recognition},
  title        = {UGNet: Uncertainty aware geometry enhanced networks for stereo matching},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep joint semantic adaptation network for multi-source
unsupervised domain adaptation. <em>PR</em>, <em>151</em>, 110409. (<a
href="https://doi.org/10.1016/j.patcog.2024.110409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source Unsupervised Domain Adaptation (MUDA) transfers knowledge learned from multiple labeled source domains to an unlabeled target domain by minimizing the domain shift between multiple source domains and the target domain. Recent studies on MUDA have focused on aligning the distribution of each pair of source and target domains in separate feature spaces to reduce their domain shift. However, these approaches suffer from two main shortcomings. First, they usually focus on the global domain shift which lacks consideration of the joint distribution of category-corresponded subdomains. Second, out-of-distribution samples far from the sample center are hard to align by the global domain alignment. Therefore, we propose a novel Deep Joint Semantic Adaptive Network (DJSAN) for MUDA. Specifically, a new maximum mean discrepancy-based metric, Joint Semantic Maximum Mean Discrepancy (JSMMD), is proposed, which can uniformly optimize the cross-domain joint distribution of category-corresponded subdomains on multiple task-specific layers. Moreover, to deal with the out-of-distribution hard samples, we propose an across-domain data augmentation method called Source-Target Domain Mixing (STDMix) to enhance the robustness of the model, which synthesizes the source domain and target domain into a new domain at a fixed ratio and utilizes information entropy to provide reliable pseudo-labels for samples in the target domain. Experimental results on three public datasets, i.e., Office-31, Digits-five, and Office-Home, show that our proposed method achieves improvements of 0.3%, 1.8%, and 2.7% in average accuracy, respectively.},
  archive      = {J_PR},
  author       = {Zhiming Cheng and Shuai Wang and Defu Yang and Jie Qi and Mang Xiao and Chenggang Yan},
  doi          = {10.1016/j.patcog.2024.110409},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110409},
  shortjournal = {Pattern Recognition},
  title        = {Deep joint semantic adaptation network for multi-source unsupervised domain adaptation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rigid pairwise 3D point cloud registration: A survey.
<em>PR</em>, <em>151</em>, 110408. (<a
href="https://doi.org/10.1016/j.patcog.2024.110408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past years, 3D point cloud registration has attracted unprecedented attention. Researchers develop various approaches to tackle the challenging task, such as optimization-based and deep learning-based methods. To systematically sort out the relevant literature and follow the state-of-the-art solutions, this paper conducts a thorough survey. We propose a novel taxonomy dubbed Intermediates Based Taxon (IBTaxon) which effectively categorizes multifarious registration approaches by the introduced intermediate variables or the leveraged intermediate modules. We further delve into each of the categories and present a comprehensive technique review with a focus on the distinct insight behind each of the methods. Besides, the relevant datasets and evaluation metrics are also combed and reorganized. We conclude our paper by discussing the possible open research problems and presenting our visions for future research in the field of 3D point cloud registration.},
  archive      = {J_PR},
  author       = {Mengjin Lyu and Jie Yang and Zhiquan Qi and Ruijie Xu and Jiabin Liu},
  doi          = {10.1016/j.patcog.2024.110408},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110408},
  shortjournal = {Pattern Recognition},
  title        = {Rigid pairwise 3D point cloud registration: A survey},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overall positive prototype for few-shot open-set
recognition. <em>PR</em>, <em>151</em>, 110400. (<a
href="https://doi.org/10.1016/j.patcog.2024.110400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot open-set recognition (FSOR) is the task of recognizing samples in known classes with a limited number of annotated instances while also detecting samples that do not belong to any known class. This is a challenging problem because the models must learn to generalize from a small number of labeled samples and distinguish them from an unlimited number of potential negative examples. In this paper, we propose a novel approach called overall positive prototype to effectively improve performance. Conceptually, negative samples would distribute throughout the feature space and are hard to be described. From the opposite viewpoint, we propose to construct an overall positive prototype that acts as a cohesive representation for positive samples that distribute in a relatively smaller neighborhood. By measuring the distance between a query sample and the overall positive prototype, we can effectively classify it as either positive or negative. We show that this simple yet innovative approach provides the state-of-the-art FSOR performance in terms of accuracy and AUROC.},
  archive      = {J_PR},
  author       = {Liang-Yu Sun and Wei-Ta Chu},
  doi          = {10.1016/j.patcog.2024.110400},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110400},
  shortjournal = {Pattern Recognition},
  title        = {Overall positive prototype for few-shot open-set recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prompting large language model with context and pre-answer
for knowledge-based VQA. <em>PR</em>, <em>151</em>, 110399. (<a
href="https://doi.org/10.1016/j.patcog.2024.110399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing studies apply Large Language Model (LLM) to knowledge-based Visual Question Answering (VQA) with encouraging results. Due to the insufficient input information, the previous methods still have shortcomings in constructing the prompt for LLM, and cannot fully activate the capacity of LLM. In addition, previous works adopt GPT-3 for inference, which has expensive costs. In this paper, we propose PCPA: a framework that P rompts LLM with C ontext and P re- A nswer for VQA. Specifically, we adopt a vanilla VQA model to generate in-context examples and candidate answers, and add a pre-answer selection layer to generate pre-answers. We integrate in-context examples and pre-answers into the prompt to inspire the LLM. In addition, we choose LLaMA instead of GPT-3, which is an open and free model. We build a small dataset to fine-tune the LLM. Compared to existing baselines, the PCPA improves accuracy by more than 2.1 and 1.5 on OK-VQA and A-OKVQA, respectively.},
  archive      = {J_PR},
  author       = {Zhongjian Hu and Peng Yang and Yuanshuang Jiang and Zijian Bai},
  doi          = {10.1016/j.patcog.2024.110399},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110399},
  shortjournal = {Pattern Recognition},
  title        = {Prompting large language model with context and pre-answer for knowledge-based VQA},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task hierarchical convolutional network for
visual-semantic cross-modal retrieval. <em>PR</em>, <em>151</em>,
110398. (<a href="https://doi.org/10.1016/j.patcog.2024.110398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bridging visual and textual representations plays a central role in delving into multimedia data understanding. The main challenge arises from that images and texts exist in heterogeneous spaces, leading to the difficulty to preserve the semantic consistency between both modalities. To narrow the modality gap, most recent methods resort to extra object detectors or parsers to obtain the hierarchical representations. In this work, we address this problem by introducing our Multi-Task Hierarchical Convolutional Neural Network (MT-HCN). It is characterized by mining the hierarchical semantic information without the aid of any extra supervisions. Firstly, from the perspective of representing architecture, we leverage the intrinsic hierarchical structure of Convolutional Neural Networks (CNNs) to decompose the representations of both modalities into two semantically complementary levels, i.e. , exterior representations and concept representations. The former focuses on discovering the fine-grained low-level associations between both modalities, meanwhile the latter underlines capturing more high-level abstract semantics. Specifically, we present a Self-Supervised Clustering (SSC) loss to preserve more fine-grained semantic clues in exterior representations. It is constituted on the basis of viewing multiple image/text pairs with similar exterior as a category. In addition, a novel harmonious bidirectional triplet ranking (HBTR) loss is proposed, which mitigate the adverse effects brought about by the biased and noisy negative samples. Besides hardest negatives, it also imposes the constraints on the distance between the positive pairs and the centroid of negative pairs. Extensive experimental results on two popular cross-modal retrieval benchmarks demonstrate our proposed MT-HCN can achieve the competitive results compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zhong Ji and Zhigang Lin and Haoran Wang and Yanwei Pang and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110398},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110398},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task hierarchical convolutional network for visual-semantic cross-modal retrieval},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-path dehazing network with spatial-frequency feature
fusion. <em>PR</em>, <em>151</em>, 110397. (<a
href="https://doi.org/10.1016/j.patcog.2024.110397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid improvement of deep learning, significant progress has been made in image dehazing, leading to favorable outcomes in many methods. However, a common challenge arises as most of these methods struggle to restore intricate details with vibrant colors in complex haze. In response to this challenge, we present a novel dual-path dehazing network with spatial-frequency feature fusion (DDN-SFF) to remove heterogeneous haze. The proposed dual-path network consists of a spatial-domain vanilla path and a frequency-domain frequency-guided path, effectively harnessing spatial-frequency knowledge. To maximize the versatility of the learned features, we introduce a relaxation dense feature fusion (RDFF) module in the vanilla path. This module can skillfully re-exploit features from non-adjacent levels and concurrently generate new features. In the frequency-guided path, we integrate the discrete wavelet transform (DWT) and introduce a frequency attention (FA) mechanism for the flexible handling of specific channels. More precisely, we deploy a channel attention (CA) and a dense feature fusion (DFF) module for low-frequency channels, whereas a pixel attention (PA) and a residual dense block (RDB) module are implemented for high-frequency channels. In summary, the deep dual-path network fuses sub-bands with specific spatial-frequency features, effectively eliminating the haze and restoring intricate details along with rich textures. Extensive experimental results demonstrate the superior performance of the proposed DDN-SFF over state-of-the-art dehazing algorithms.},
  archive      = {J_PR},
  author       = {Li Wang and Hang Dong and Ruyu Li and Chao Zhu and Huibin Tao and Yu Guo and Fei Wang},
  doi          = {10.1016/j.patcog.2024.110397},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110397},
  shortjournal = {Pattern Recognition},
  title        = {Dual-path dehazing network with spatial-frequency feature fusion},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Spectral clustering with linear embedding: A discrete
clustering method for large-scale data. <em>PR</em>, <em>151</em>,
110396. (<a href="https://doi.org/10.1016/j.patcog.2024.110396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, spectral clustering has found widespread applications in various real-world scenarios, showcasing its effectiveness. Traditional spectral clustering typically follows a two-step procedure to address the optimization problem. However, this approach may result in substantial information loss and performance decline. Furthermore, the eigenvalue decomposition, a key step in spectral clustering, entails cubic computational complexity. This paper incorporates linear embedding into the objective function of spectral clustering and proposes a direct method to solve the indicator matrix. Moreover, our method achieves a linear time complexity with respect to the input data size. Our method, referred to as Spectral Clustering with Linear Embedding (SCLE), achieves a direct and efficient solution and naturally handles out-of-sample data. SCLE initiates the process with balanced and hierarchical K-means, effectively partitioning the input data into balanced clusters. After generating anchors, we compute a similarity matrix based on the distances between the input data points and the generated anchors. In contrast to the conventional two-step spectral clustering approach, we directly solve the cluster indicator matrix at a linear time complexity. Extensive experiments across multiple datasets underscore the efficiency and effectiveness of our proposed SCLE method.},
  archive      = {J_PR},
  author       = {Chenhui Gao and Wenzhi Chen and Feiping Nie and Weizhong Yu and Zonghui Wang},
  doi          = {10.1016/j.patcog.2024.110396},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110396},
  shortjournal = {Pattern Recognition},
  title        = {Spectral clustering with linear embedding: A discrete clustering method for large-scale data},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deterministic attribute selection for isolation forest.
<em>PR</em>, <em>151</em>, 110395. (<a
href="https://doi.org/10.1016/j.patcog.2024.110395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data mining techniques have been gained importance in recent years. In particular, anomaly detection algorithms, applied in key sectors of information technology, have been growing in popularity. One of the efficient and fast algorithms is Isolation Forest. The method consists of two separated stages: Forest formation and evaluation of elements. The first stage relies on forming a forest of isolation trees. Each tree is built in the same manner according to drawn samples and random divisions of data attributes. In this study, an innovative deterministic attribute selection method is proposed, maintaining its random value. New ideas based on imbalance, clustering, and a dispersion of values through non-linear transformation of elements are introduced and thoroughly analyzed. These novel anomaly detection approaches are applied to 25 real datasets, as well as our own artificially generated databases. The Area Under the ROC Curve and the Area Under the PR Curve are used as a measure of the outliers classification quality. The results of the numerical experiment have proven high efficiency and competitive evaluation speed of the proposals in comparison to other Isolation Forest-based approaches, as well as several other popular techniques.},
  archive      = {J_PR},
  author       = {Łukasz Gałka and Paweł Karczmarek},
  doi          = {10.1016/j.patcog.2024.110395},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110395},
  shortjournal = {Pattern Recognition},
  title        = {Deterministic attribute selection for isolation forest},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data filtering for efficient adversarial training.
<em>PR</em>, <em>151</em>, 110394. (<a
href="https://doi.org/10.1016/j.patcog.2024.110394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training has been considered to be one of the most effective strategies to defend against adversarial attacks. Most existing adversarial training methods have shown a trade-off between training cost and robustness. This paper explores a new optimization direction from training data to reduce the computational cost of adversarial training without scarifying robustness. First, we show that some adversarial examples are less important, meaning that removing them does not hurt the robustness. Second, we propose a method to identify insignificant adversarial examples at a minimal cost. Third, we demonstrate that our approach can be integrated with other adversarial training frameworks with few modifications. The experimental results show that combined with previous works, our approach not only reduces about 20% of computational cost on the CIFAR10 and CIFAR100 datasets but also improves about 1.5% natural accuracy. With less computational cost, it achieves 58.22%, 30.68%, and 41.92% robust accuracy on CIFAR10, CIFAR100, and ImageNet datasets respectively, which are higher than those of the original methods.},
  archive      = {J_PR},
  author       = {Erh-Chung Chen and Che-Rung Lee},
  doi          = {10.1016/j.patcog.2024.110394},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110394},
  shortjournal = {Pattern Recognition},
  title        = {Data filtering for efficient adversarial training},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise level estimation using locality preserving natural
image statistics. <em>PR</em>, <em>151</em>, 110393. (<a
href="https://doi.org/10.1016/j.patcog.2024.110393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural images are known to have certain regular statistical properties. These properties get modified under any artificial change or distortion in natural images. Most common form of image degradation occurs in the form of noise. The amount of degradation in noisy images is measured by estimating the noise level. Many image processing applications such as denoising, restoration, segmentation, compression etc. use noise level information as a prior; inaccurate estimate of which may impact their performance. In this article, we explore natural image statistics in locality preserving transform domain. This property groups structurally similar images/image patches when projected in the transform domain. Image patches corrupted with similar noise level get projected close by in the locality preserving domain and show consistent coefficient behaviour. In particular, we use Two Dimensional Orthogonal Locality Preserving Projection (2DOLPP) as the domain transformation technique. 2DOLPP basis, representing natural images, are learnt in advance from a set of clean images, thereby reducing the computational time significantly. Features based on natural image statistics are extracted from 2DOLPP domain representation of input image patches. Mapping from feature space to noise level is carried out using support vector regression. The proposed noise estimation approach is at par with or surpasses the state-of-the-art techniques with much less computational time. Performance of this approach is stable across a wide range of noise levels and independent of the image structure.},
  archive      = {J_PR},
  author       = {Gitam Shikkenawis and Suman K. Mitra and Ashutosh Saxena},
  doi          = {10.1016/j.patcog.2024.110393},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110393},
  shortjournal = {Pattern Recognition},
  title        = {Noise level estimation using locality preserving natural image statistics},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embrace sustainable AI: Dynamic data subset selection for
image classification. <em>PR</em>, <em>151</em>, 110392. (<a
href="https://doi.org/10.1016/j.patcog.2024.110392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.},
  archive      = {J_PR},
  author       = {Zimo Yin and Jian Pu and Ru Wan and Xiangyang Xue},
  doi          = {10.1016/j.patcog.2024.110392},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110392},
  shortjournal = {Pattern Recognition},
  title        = {Embrace sustainable AI: Dynamic data subset selection for image classification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bibimbap: Pre-trained models ensemble for domain
generalization. <em>PR</em>, <em>151</em>, 110391. (<a
href="https://doi.org/10.1016/j.patcog.2024.110391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a machine learning problem often challenged by differences in the distributions of training and real-world data. We propose a framework that addresses the problem of underfitting in the ensembling method using pre-trained models and improves the performance and robustness of deep learning models through ensemble diversity. For the naive weight ensembling framework, we discovered that the ensembled models could not lie in the same loss basin under extreme domain shift conditions, suggesting that a loss barrier may exist. We used a fine-tuning step after the weighted ensemble to address the underfitting problem caused by the loss barrier and stabilize the batch normalization running parameters. We also inferred through qualitative analysis that the diversity of ensemble models affects domain generalization. We validate our method on a large-scale image dataset (ImageNet-1K) and chemical molecule data, which is suitable for testing with domain shift problems due to its data-splitting method.},
  archive      = {J_PR},
  author       = {Jinho Kang and Taero Kim and Yewon Kim and Changdae Oh and Jiyoung Jung and Rakwoo Chang and Kyungwoo Song},
  doi          = {10.1016/j.patcog.2024.110391},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110391},
  shortjournal = {Pattern Recognition},
  title        = {Bibimbap: Pre-trained models ensemble for domain generalization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-free ensemble clustering with dynamic weighting
mechanism. <em>PR</em>, <em>151</em>, 110389. (<a
href="https://doi.org/10.1016/j.patcog.2024.110389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering (EC) gains more and more attention because it can improve the effectiveness and robustness of single clustering methods. A popular ensemble approach is to construct a co-association matrix which represents the possibility that the sample pair is divided into different clusters by base clusterings. Then, some single clustering methods could be performed on it. However, this approach separates the construction of the co-association matrix from the generation of clustering results. Besides, the importance of base clusterings and clusters are often regarded as the same but their performance or quality is different actually. Although some weighted ensemble clustering methods have been proposed, typically, the weights are calculated based on certain criteria and then fixed. To cope with these issues, we propose a Parameter-Free Ensemble Clustering (PFEC) with dynamic weighting mechanism, which is capable of dynamically adjusting the weights of base clusterings and clusters. Firstly, the self-weighted framework is applied in our method to weight base clusterings automatically. Furthermore, an additional weight is assigned to each cluster in the base clustering, which can also be dynamically adjusted. This can help alleviate the issue of imbalanced clusters as well. Finally, a structured affinity graph is learned from the results of base clusterings through rank constraint and no post-processing is required. The experimental results on synthetic and real datasets illustrate the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Fangyuan Xie and Feiping Nie and Weizhong Yu and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110389},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110389},
  shortjournal = {Pattern Recognition},
  title        = {Parameter-free ensemble clustering with dynamic weighting mechanism},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A randomized algorithm for clustering discrete sequences.
<em>PR</em>, <em>151</em>, 110388. (<a
href="https://doi.org/10.1016/j.patcog.2024.110388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis is one of the most important research issues in data mining and machine learning. To date, numerous clustering algorithms have been proposed to tackle the fixed-length vector data. In many real applications, we need to detect clusters from a set of discrete sequences in which each sequence is an ordered list of items. Due to the sequential and discrete nature, the discrete sequence clustering problem is more challenging and most of existing vector data clustering algorithms cannot be directly employed. In this paper, we present a stochastic algorithm for clustering discrete sequences. Our method first quickly generates a set of random partitions over the sequential data set and then merges these random clustering results via weighted graph construction and partition. We perform extensive empirical comparisons on real data sets to show that our method is comparable to those state-of-the-art clustering algorithms with respect to both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Mudi Jiang and Lianyu Hu and Xin Han and Yong Zhou and Zengyou He},
  doi          = {10.1016/j.patcog.2024.110388},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110388},
  shortjournal = {Pattern Recognition},
  title        = {A randomized algorithm for clustering discrete sequences},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AG-meta: Adaptive graph meta-learning via representation
consistency over local subgraphs. <em>PR</em>, <em>151</em>, 110387. (<a
href="https://doi.org/10.1016/j.patcog.2024.110387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph meta-learning has recently received significantly increased attention by virtue of its potential to extract common and transferable knowledge from learning different tasks on a graph. Existing methods for graph meta-learning usually leverage local subgraphs to transfer subgraph-specific information. However, they inherently face the challenge of imbalanced subgraphs due to inconsistent node density and different label distributions over local subgraphs. This paper proposes an adaptive graph meta-learning framework (AG-Meta) for learning the consistent and transferable representation of a graph in a way that can adapt to imbalanced subgraphs. Specifically, AG-Meta first learns the structural representation of subgraphs with various degrees using an Adaptive Graph Cascade Diffusion Network (AGCDN). AG-Meta then employs a prototype-consistency classifier to produce more accurate transferable inductive representations (also called prototypes) under few-shot settings with different label distributions of a subgraph. In the context of optimizing a model-agnostic meta-learner, a novel metric loss is finally introduced to achieve structural representation and prototype consistency. Extensive experiments are conducted to compare AG-Meta against baselines on five real-world networks, which validates that AG-Meta outperforms the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Yi Wang and Changqin Huang and Ming Li and Qionghao Huang and Xuemei Wu and Jia Wu},
  doi          = {10.1016/j.patcog.2024.110387},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110387},
  shortjournal = {Pattern Recognition},
  title        = {AG-meta: Adaptive graph meta-learning via representation consistency over local subgraphs},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient high-resolution template matching with vector
quantized nearest neighbour fields. <em>PR</em>, <em>151</em>, 110386.
(<a href="https://doi.org/10.1016/j.patcog.2024.110386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Template matching is a fundamental problem in computer vision with applications in fields including object detection, image registration, and object tracking. Current methods rely on nearest-neighbour (NN) matching, where the query feature space is converted to NN space by representing each query pixel with its NN in the template. NN-based methods have been shown to perform better in occlusions, appearance changes, and non-rigid transformations; however, they scale poorly with high-resolution data and high feature dimensions. We present an NN-based method that efficiently reduces the NN computations and introduces filtering in the NN fields (NNFs). A vector quantization step is introduced before the NN calculation to represent the template with k k features, and the filter response over the NNFs is used to compare the template and query distributions over the features. We show that state-of-the-art performance is achieved in low-resolution data, and our method outperforms previous methods at higher resolution.},
  archive      = {J_PR},
  author       = {Ankit Gupta and Ida-Maria Sintorn},
  doi          = {10.1016/j.patcog.2024.110386},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110386},
  shortjournal = {Pattern Recognition},
  title        = {Efficient high-resolution template matching with vector quantized nearest neighbour fields},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BALQUE: Batch active learning by querying unstable examples
with calibrated confidence. <em>PR</em>, <em>151</em>, 110385. (<a
href="https://doi.org/10.1016/j.patcog.2024.110385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active learning alleviates labeling costs by selecting and labeling the most informative examples from an unlabeled pool. However, most existing active learning approaches estimate informativeness with uncalibrated confidence, resulting in unreliable informativeness estimation. These approaches generally ignored two significant issues caused by uncalibrated confidence methods. Firstly, the average uncalibrated confidence generated by modern neural networks is usually higher than the accuracy. Secondly, examples located near the decision boundaries are unstable during prediction when the target model updates parameters in the last several epochs, even throughout the training process. This phenomenon, caused by the forgetting characteristic of neural networks, has a significant impact on some specific models that estimate the informativeness by predicted probability vectors or pseudo labels. To address these issues, in this paper, we propose a novel active learning approach to reliably estimate informativeness with calibrated confidence. Specifically, we integrate the intermediate predictions for each unlabeled example, generated by the target model during the training process, to generate calibrated confidence. The calibrated confidence can capture a tendentious label from an indecisive subset of the class space. We show that the calibrated confidence with tendentiousness can maintain the ability of correct predictions. The empirical results demonstrate that our approach outperforms the state-of-the-art active learning methods on image classification tasks.},
  archive      = {J_PR},
  author       = {Yincheng Han and Dajiang Liu and Jiaxing Shang and Linjiang Zheng and Jiang Zhong and Weiwei Cao and Hong Sun and Wu Xie},
  doi          = {10.1016/j.patcog.2024.110385},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110385},
  shortjournal = {Pattern Recognition},
  title        = {BALQUE: Batch active learning by querying unstable examples with calibrated confidence},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty-driven active developmental learning.
<em>PR</em>, <em>151</em>, 110384. (<a
href="https://doi.org/10.1016/j.patcog.2024.110384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing machine learning models can well handle common classes but struggle to detect unfamiliar or unknown classes due to environmental variations. To address this challenge, we propose a new task called active developmental learning (ADL), which empowers models to actively determine what to learn in the open world, thereby progressively enhancing the capability of detecting unfamiliar and unknown classes. Considering the uncertain essence of the task, we design an uncertainty-driven method for ADL (UADL) that measures and utilizes uncertainty to evaluate unfamiliar known classes and unknown classes separately, which consists of two stages: (1) unfamiliar detection of known classes and (2) unknown detection of novel classes. In the first stage, UADL identifies unfamiliar samples of known classes via known-class uncertainty calculated by GMMs on detectors’ heads. In the second stage, UADL identifies samples containing unknown classes via unknown-class uncertainty computed by class-specific GMMs in feature space. In both stages, uncertainty is used to select a minimal number of unlabeled samples for manual labeling, facilitating the model’s active self-development. Experiments on multiple object detection benchmark datasets demonstrate the feasibility and significant performance of UADL and show its effectiveness against the ADL task compared to other state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Qinghua Hu and Luona Ji and Yu Wang and Shuai Zhao and Zhibin Lin},
  doi          = {10.1016/j.patcog.2024.110384},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110384},
  shortjournal = {Pattern Recognition},
  title        = {Uncertainty-driven active developmental learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introspective GAN: Learning to grow a GAN for incremental
generation and classification. <em>PR</em>, <em>151</em>, 110383. (<a
href="https://doi.org/10.1016/j.patcog.2024.110383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lifelong learning, the ability to continually learn new concepts throughout our life, is a hallmark of human intelligence. Generally, humans learn a new concept by knowing what it looks like and what makes it different from the others , which are correlated. Those two ways can be characterized by generation and classification in machine learning respectively. In this paper, we carefully design a dynamically growing GAN called Introspective GAN (IntroGAN) that can perform incremental generation and classification simultaneously with the guidance of prototypes, inspired by their roles of efficient information organization in human visual learning and excellent performance in other fields like zero-shot/few-shot/incremental learning. Specifically, we incorporate prototype-based classification which is robust to feature change in incremental learning and GAN as a generative memory to alleviate forgetting into a unified end-to-end framework. A comprehensive benchmark on the joint incremental generation and classification task is proposed and our method demonstrates promising results. Additionally, we conduct comprehensive analyses over the properties of IntroGAN and verify that generation and classification can be mutually beneficial in incremental scenarios, which is an inspiring area to be further exploited. The code is available at https://github.com/TonyPod/IntroGAN .},
  archive      = {J_PR},
  author       = {Chen He and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1016/j.patcog.2024.110383},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110383},
  shortjournal = {Pattern Recognition},
  title        = {Introspective GAN: Learning to grow a GAN for incremental generation and classification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive and fuzzy locality discriminant analysis for
dimensionality reduction. <em>PR</em>, <em>151</em>, 110382. (<a
href="https://doi.org/10.1016/j.patcog.2024.110382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) uses labeled samples for acquiring a discriminant projection direction, by which data of different categories are separated into distinct groups in a lower-dimensional subspace. In response to the issue that LDA lacks robustness to non-Gaussian data with rich local information, improvements on LDA explore the subspace manifold structure by building a fully connected similarity graph. However, these methods are vulnerable to the interference of noisy and redundant information. In this paper, we propose a new local LDA method, named adaptive and fuzzy locality discriminant analysis (AFLDA), which aims at extracting precise and compact features. Firstly, an adaptive and fuzzy k-means strategy is adopted, where the membership between data and corresponding subclass centers for each class is learned to establish a hybrid bipartite graph and capture local information. Secondly, we design a discrete and probability constraint imposed on the membership matrix to explore the intricate structure of multimodal data. Moreover, the subblock partition for each class makes data accommodate to multimodal subclasses. Thirdly, the maximum total scatter regularization term is introduced, which amply disperses the data to enhance the recognition of local structure and avoid trivial solutions. Finally, to eliminate the interference of noisy and redundant information, AFLDA learns an optimized subspace, where cluster centers and the membership matrix are updated alternately. Promising results in experiments illustrate the efficacy of the model.},
  archive      = {J_PR},
  author       = {Jingyu Wang and Hengheng Yin and Feiping Nie and Xuelong Li},
  doi          = {10.1016/j.patcog.2024.110382},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110382},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive and fuzzy locality discriminant analysis for dimensionality reduction},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep anomaly detection on set data: Survey and comparison.
<em>PR</em>, <em>151</em>, 110381. (<a
href="https://doi.org/10.1016/j.patcog.2024.110381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalous samples in set data is a problem attracting increased interest due to novel modalities, such as point-cloud data produced by lidars. Novel methods including those based on deep neural networks are often tuned for a single purpose prohibiting intuition of how relevant they are for another purpose or application domains. The aim of this survey is to: (i) review elementary concepts of anomaly detection of set data, (ii) identify the building blocks of deep anomaly detectors, and (iii) analyze the impact of these blocks on performance. The impact is studied in a large experimental comparison on a variety of benchmark datasets. The results reveal that the main factor determining the performance is the type of anomalies in the dataset. While deep methods embedding the whole set to a single fixed vector perform well on point cloud data, the methods embedding each feature vector independently are better for datasets from multi-instance learning. Moreover, sophisticated methods utilizing transformer blocks are frequently inferior to simple models with properly optimized hyperparameters. An independent factor in performance is the cardinality of sets, the proper treatment of which remains an open problem, as the existing analytical solution was found to be inaccurate.},
  archive      = {J_PR},
  author       = {Michaela Mašková and Matěj Zorek and Tomáš Pevný and Václav Šmídl},
  doi          = {10.1016/j.patcog.2024.110381},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110381},
  shortjournal = {Pattern Recognition},
  title        = {Deep anomaly detection on set data: Survey and comparison},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Query-centric distance modulator for few-shot
classification. <em>PR</em>, <em>151</em>, 110380. (<a
href="https://doi.org/10.1016/j.patcog.2024.110380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification (FSC) is a highly challenging task, as only a small number of labeled samples are available when identifying new categories. Distance metric learning-based methods have emerged as a prominent approach to FSC, which typically use a distance function to measure the difference between query and support samples for identifying the class membership of the query sample. However, these methods simply treat each channel difference between query and support features equally when computing the class scores. Since different channels in the learned feature seek for different patterns, these distance metrics fail to consider that different channels are of different importance to FSC, and thus cannot accurately measure the similarity between samples. To address this issue, we propose a Q uery- C entric D istance M odulator ( QCDM ) to generate query-related weights for each channel difference adaptively. Specifically, since the distribution of difference between a query sample and all support samples in a particular channel can reflect the importance of this channel to the classification of the query sample, we take this difference vector as input and generate a query-specific channel weight through a meta-network. QCDM can guide FSC models to focus on discriminative channel differences and achieve better generalization. QCDM is a plug-and-play module that can be seamlessly integrated with existing distance metric learning-based FSC methods. Extensive experimental results indicate that our method can effectively improve the performance of distance metric learning-based FSC methods. The source code is available in https://github.com/Wu-Wenxiao/QCDM .},
  archive      = {J_PR},
  author       = {Wenxiao Wu and Yuanjie Shao and Changxin Gao and Jing-Hao Xue and Nong Sang},
  doi          = {10.1016/j.patcog.2024.110380},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110380},
  shortjournal = {Pattern Recognition},
  title        = {Query-centric distance modulator for few-shot classification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum to “FGPNet: A weakly supervised fine-grained 3D
point clouds classification network” [pattern recognition 139 (2023)
109509]. <em>PR</em>, <em>151</em>, 110379. (<a
href="https://doi.org/10.1016/j.patcog.2024.110379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Huihui Shao and Jing Bai and Rusong Wu and Jinzhe Jiang and Hongbo Liang},
  doi          = {10.1016/j.patcog.2024.110379},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110379},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “FGPNet: A weakly supervised fine-grained 3D point clouds classification network” [Pattern recognition 139 (2023) 109509]},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural knitworks: Patched neural implicit representation
networks. <em>PR</em>, <em>151</em>, 110378. (<a
href="https://doi.org/10.1016/j.patcog.2024.110378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing images as output of a neural network has been shown to introduce a powerful prior for image inverse tasks, capable of producing solutions of reasonable quality in a fully internal learning context, where no external datasets are involved. Two potential technical approaches involve fitting a coordinate-based Multilayer Perceptron (MLP), or a Convolutional Neural Network to produce the result image as output. The aim of this work is to evaluate the two counterparts, as well as a new framework proposed here, named Neural Knitwork, which maps pixel coordinates to local texture patches rather than singular pixel values. The utility of the proposed technique is demonstrated on the tasks of image inpainting, super-resolution, and denoising. It is shown that the Neural Knitwork can outperform the standard coordinate-based MLP baseline for the tasks of inpainting and denoising, and perform comparably for the super-resolution task.},
  archive      = {J_PR},
  author       = {Mikolaj Czerkawski and Javier Cardona and Robert Atkinson and Craig Michie and Ivan Andonovic and Carmine Clemente and Christos Tachtatzis},
  doi          = {10.1016/j.patcog.2024.110378},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110378},
  shortjournal = {Pattern Recognition},
  title        = {Neural knitworks: Patched neural implicit representation networks},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoupled representation for multi-view learning.
<em>PR</em>, <em>151</em>, 110377. (<a
href="https://doi.org/10.1016/j.patcog.2024.110377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning multi-view data is a central topic for advanced deep model applications. Existing efforts mainly focus on exploring shared information to maximize the consensus among all the views. However, after reasonably discarding superfluous task-irrelevant noise, the view-specific information is equally essential to downstream tasks. In this paper, we propose to decouple the multi-view representation learning into the shared and specific information extractions with parallel branches, and seamlessly adopt feature fusion in end-to-end models. The common feature is obtained based on the view-agnostic contrastive learning and view-discriminative training to minimize the discrepancy within the views. Simultaneously, the specific feature is learned with orthogonality constraints to minimize the view-level correlation. Besides, the semantic information in the features is reserved with supervised training. After disentangling the representations, we fuse the mutually complementary common and specific features for downstream tasks. Particularly, we provide a theoretical explanation for our method from an information bottleneck perspective. Compared with state-of-the-art multi-view models on benchmark datasets, we empirically demonstrate the advantage of our method in several downstream tasks, such as ordinary classification and few-shot learning. In addition, extensive experiments validate the robustness and transferability of our approach, when applying the learned representation on the source dataset to several target datasets.},
  archive      = {J_PR},
  author       = {Shiding Sun and Bo Wang and Yingjie Tian},
  doi          = {10.1016/j.patcog.2024.110377},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110377},
  shortjournal = {Pattern Recognition},
  title        = {Decoupled representation for multi-view learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Global and local multi-modal feature mutual learning for
retinal vessel segmentation. <em>PR</em>, <em>151</em>, 110376. (<a
href="https://doi.org/10.1016/j.patcog.2024.110376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on optical coherence tomography angiography (OCTA) images has received extensive attention in recent years since it provides more detailed information about retinal structures. The automatic segmentation of retinal vessel (RV) has become one of the key issues in the quantification of retinal indicators. To this end, there are various methods proposed with cutting-edge designs and techniques in the literature. However, most of them only learn features from single-modal data, despite the potential relation between data from different modalities. Clinically, 2D projection maps are more convenient for doctors to observe. Nevertheless, 3D volumes preserve the intrinsic retinal structure. We thus propose a novel multi-modal feature mutual learning framework that contains local mutual learning and global mutual learning capturing 2D and 3D information. In the framework, the 3D model and 2D model learn collaboratively and teach each other throughout the training process. Experimental results show that our method outperforms previous deep-learning methods in RV segmentation. The generalization experiments on the ROSE dataset demonstrate the portability and scalability of the proposed framework.},
  archive      = {J_PR},
  author       = {Xin Zhao and Jing Zhang and Qiaozhe Li and Tengfei Zhao and Yi Li and Zifeng Wu},
  doi          = {10.1016/j.patcog.2024.110376},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110376},
  shortjournal = {Pattern Recognition},
  title        = {Global and local multi-modal feature mutual learning for retinal vessel segmentation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical image segmentation based on dynamic positioning and
region-aware attention. <em>PR</em>, <em>151</em>, 110375. (<a
href="https://doi.org/10.1016/j.patcog.2024.110375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has already proven its ability to model long-distance dependencies. However, medical images have strong local structures. Directly using Transformer to extract features would not only contain redundant information increasing the computational effort, but also be detrimental to extracting local details. Given these issues, we propose a network based on dynamic positioning and region-aware attention, which adopts a two-stage feature extraction strategy. In the shallow layer, we design Dynamic Positioning Attention (DPA). It will localize to the key feature information and construct a variable window for it, then perform attention calculation. DPA improves the learning ability of local details, reduces the amount of computation. At the deep level, Bi-Level Routing Attention (BRA) is used to discard irrelevant key–value pairs, achieve content-aware sparse attention for the deep dispersed semantic information, and improve computational efficiency. After several experiments, the results show that our method achieves advanced performance on different types of datasets.},
  archive      = {J_PR},
  author       = {Zhongmiao Huang and Shuli Cheng and Liejun Wang},
  doi          = {10.1016/j.patcog.2024.110375},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110375},
  shortjournal = {Pattern Recognition},
  title        = {Medical image segmentation based on dynamic positioning and region-aware attention},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-lingual few-shot sign language recognition.
<em>PR</em>, <em>151</em>, 110374. (<a
href="https://doi.org/10.1016/j.patcog.2024.110374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are over 150 sign languages worldwide, each with numerous local variants and thousands of signs. However, collecting annotated data for each sign language to train a model is a laborious and expert-dependent task. To address this issue, this paper introduces the problem of few-shot sign language recognition (FSSLR) in a cross-lingual setting. The central motivation is to be able to recognize a novel sign, even if it belongs to a sign language unseen during training, based on a small set of examples. To tackle this problem, we propose a novel embedding-based framework that first extracts a spatio-temporal visual representation based on video and hand features, as well as hand landmark estimates. To establish a comprehensive test bed, we propose three meta-learning FSSLR benchmarks that span multiple languages, and extensively evaluate the proposed framework. The experimental results demonstrate the effectiveness and superiority of the proposed approach for few-shot sign language recognition in both monolingual and cross-lingual settings.},
  archive      = {J_PR},
  author       = {Yunus Can Bilge and Nazli Ikizler-Cinbis and Ramazan Gokberk Cinbis},
  doi          = {10.1016/j.patcog.2024.110374},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110374},
  shortjournal = {Pattern Recognition},
  title        = {Cross-lingual few-shot sign language recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach for handwritten chinese text recognition
unifying character segmentation and recognition. <em>PR</em>,
<em>151</em>, 110373. (<a
href="https://doi.org/10.1016/j.patcog.2024.110373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text line recognition methods can be categorized into explicit segmentation based and implicit segmentation based ones. Explicit segmentation based methods require character-level annotation during training, while implicit segmentation based methods, trained on line-level annotated data, face alignment drift challenges. Though some methods have been proposed to address these challenges using weakly supervised object detection, they often rely on cumbersome pseudo-box generation processes and complex decoding. In this paper, we propose a unified framework to overcome these challenges, achieving high accuracy in text recognition and character segmentation. To eliminate the need of character-level annotated real text line data in training, we introduce a novel training paradigm that utilizes character-level annotated synthetic data and line-level annotated real data jointly. For synthetic data, candidate characters are explicitly aligned with labeled characters to generate hard labels for supervising model training. For real data, implicit alignment is produced by Connectionist Temporal Classification (CTC) mapping to provide soft labels for weakly-supervised model training. And for inference, we propose two decoding strategies leveraging the advantages of Non-Maximum Suppression (NMS) and CTC decoding. Extensive experiments on benchmark datasets demonstrate the superior performance of our method in text recognition and character localization, even with minimal amounts of character-level annotated line data.},
  archive      = {J_PR},
  author       = {Ming-Ming Yu and Heng Zhang and Fei Yin and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2024.110373},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110373},
  shortjournal = {Pattern Recognition},
  title        = {An approach for handwritten chinese text recognition unifying character segmentation and recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motion-guided and occlusion-aware multi-object tracking with
hierarchical matching. <em>PR</em>, <em>151</em>, 110369. (<a
href="https://doi.org/10.1016/j.patcog.2024.110369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of multi-target tracking, the widely embraced tracking-by-detection paradigm has rapidly progressed with the refinement of detectors and matching techniques. However, the paradigm of joint detection and tracking is relatively limited, and it is difficult to model complex scenes, such as the complexities introduced by camera motion and occlusion. In this work, a hierarchical joint detection and tracking framework is proposed, namely MSPNet. From a temporal concern, a motion-guided feature aggregation module is proposed to address the complexities of multi-frame variations. From a spatial concern, an occlusion-aware head and hierarchical spatial association are proposed to handle the challenges of occlusion. Extensive experiments on MOT challenging benchmarks demonstrate that the MSPNet can effectively reduce false negatives and improve the accuracy of tracking while outperforming a wide range of existing methods.},
  archive      = {J_PR},
  author       = {Yujin Zheng and Hang Qi and Lei Li and Shan Li and Yan Huang and Chu He and Dingwen Wang},
  doi          = {10.1016/j.patcog.2024.110369},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110369},
  shortjournal = {Pattern Recognition},
  title        = {Motion-guided and occlusion-aware multi-object tracking with hierarchical matching},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Coordinating explicit and implicit knowledge for
knowledge-based VQA. <em>PR</em>, <em>151</em>, 110368. (<a
href="https://doi.org/10.1016/j.patcog.2024.110368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained models often generate plausible looking statements that are factually incorrect because of the inaccurate implicit knowledge contained in the model’s parameters. Related methods retrieve explicit knowledge from the external knowledge source to help improve the prediction performance and reliability. However, these methods often use weak training signals for the retriever, and require the model to make each prediction based on the retrieved knowledge, even when the retrieved knowledge is not reliable or the model can produce better prediction only using its implicit knowledge. Therefore, it is necessary to enable the pre-trained model to actively select more beneficial knowledge for producing better prediction. This work proposes a novel method to help the model to C oordinate E xplicit and I mplicit K nowledge (CEIK) for the knowledge-based visual question answering (VQA) task, which is an important direction of pre-trained models. Furthermore, a better training signal is proposed for the retriever according to whether the retrieved knowledge can correct the prediction. Experimental results demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Qunbo Wang and Jing Liu and Wenjun Wu},
  doi          = {10.1016/j.patcog.2024.110368},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110368},
  shortjournal = {Pattern Recognition},
  title        = {Coordinating explicit and implicit knowledge for knowledge-based VQA},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relation fusion propagation network for transductive
few-shot learning. <em>PR</em>, <em>151</em>, 110367. (<a
href="https://doi.org/10.1016/j.patcog.2024.110367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous graph-based meta-learning approaches have explored pairwise feature similarity to learn instance-level relations of samples, however, the gap between the sample relations in feature and label spaces is often ignored. It is empirically observed that instances with different labels may display considerable similarity in visual characteristics, making it challenging to distinguish between them in the feature space. To this end, we propose a dual-branch Relation Fusion Propagation Network (RFPN) for transductive few-shot learning, which explicitly models both feature and label relations across support-query pairs. Specifically, we design a Relation Fusion Block (RFB) to fuse instance-level and class-level relations, thus obtaining more robust fusion-level relations to guide feature propagation. In addition to the feature propagation branch, we encode the label relations and construct a label shortcut branch for label propagation. To alleviate the error accumulation during propagation, which is caused by uncertain pseudo-labels for query samples, we propose a Label Shortcut Mechanism (LSM) to progressively update the sample relations with the initial labels. Our full method is plug-and-play and can be easily applied in existing graph-based approaches for transductive few-shot learning. Extensive experiments demonstrate that our proposed RFPN yields significant improvements over the baselines and achieves promising performance on four popular few-shot classification benchmarks.},
  archive      = {J_PR},
  author       = {Yixiang Huang and Hongyu Hao and Weichao Ge and Yang Cao and Ming Wu and Chuang Zhang and Jun Guo},
  doi          = {10.1016/j.patcog.2024.110367},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110367},
  shortjournal = {Pattern Recognition},
  title        = {Relation fusion propagation network for transductive few-shot learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of spectral clustering based on graph theory.
<em>PR</em>, <em>151</em>, 110366. (<a
href="https://doi.org/10.1016/j.patcog.2024.110366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering converts the data clustering problem to the graph cut problem. It is based on graph theory. Due to the reliable theoretical basis and good clustering performance, spectral clustering has been successfully applied in many fields. Although spectral clustering has many advantages, it faces the challenges of high time and space complexity when dealing with large scale complex data. Firstly, this paper introduces the basic concept of graph theory, reviews the properties of Laplacian matrix and the traditional graph cuts method. Then, it focuses on four aspects of the realization process of spectral clustering, including the construction of similarity matrix, the establishment of Laplacian matrix, the selection of eigenvectors and the determination of the number of clusters. In addition, some successful applications of spectral clustering are summarized. In each aspect, the shortcomings of spectral clustering and some representative improved algorithms are emphatically analyzed. Finally, the paper comprehensively analyzes some research on spectral clustering that has not yet been in-depth, and gives prospects on some valuable research directions.},
  archive      = {J_PR},
  author       = {Ling Ding and Chao Li and Di Jin and Shifei Ding},
  doi          = {10.1016/j.patcog.2024.110366},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110366},
  shortjournal = {Pattern Recognition},
  title        = {Survey of spectral clustering based on graph theory},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning spatial-spectral dual adaptive graph embedding for
multispectral and hyperspectral image fusion. <em>PR</em>, <em>151</em>,
110365. (<a href="https://doi.org/10.1016/j.patcog.2024.110365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusion of high spatial resolution multispectral (HR MS) and low spatial resolution hyperspectral (LR HS) images has become a significant way to produce high spatial resolution hyperspectral (HR HS) images. Though many methods have exploited the spatial nonlocal similarity (SNS) and spectral band correlation (SBC) in the HR HS image, it is difficult to model the priors explicitly because the HR HS image is unavailable in real scenes. As the low-dimensional degradation versions, HR MS and LR HS images inherit the SNS and SBC in the HR HS image, respectively. But these methods seldom consider the inheritance of SNS and SBC between the two source images and the HR HS image. In this paper, we propose a spatial–spectral dual adaptive graph embedding model (SDAGE) to exploit the SNS and SBC in HR MS and LR HS images for the regularization of their fusion. Specifically, spatial and spectral graphs are constructed adaptively to describe the SNS in the HR MS image and the SBC in the LR HS image. Then, the two graphs are embedded into the features for the reconstruction of the HR HS image. In this way, it is explicit to ensure the consistency of SNS and SBC between the source images and the HR HS image. Experiments on three benchmark datasets demonstrate the effectiveness of our SDAGE method. The code can be downloaded from https://github.com/RSMagneto/SDAGE .},
  archive      = {J_PR},
  author       = {Xuquan Wang and Feng Zhang and Kai Zhang and Weijie Wang and Xiong Dun and Jiande Sun},
  doi          = {10.1016/j.patcog.2024.110365},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110365},
  shortjournal = {Pattern Recognition},
  title        = {Learning spatial-spectral dual adaptive graph embedding for multispectral and hyperspectral image fusion},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph convolutional network with elastic topology.
<em>PR</em>, <em>151</em>, 110364. (<a
href="https://doi.org/10.1016/j.patcog.2024.110364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) has drawn widespread attention in data mining on graphs due to its outstanding performance and rigor theoretical guarantee. However, some recent studies have revealed that GCN-based methods may mine latent information insufficiently owing to the underutilization of the feature space. Besides, the unlearnable topology also significantly imperils the performance of GCN-based methods. In this paper, we conduct experiments to investigate these issues, finding that GCN does not fully consider the potential structure in the feature space, and a fixed topology deteriorates the robustness of GCN. Thus, it is desired to distill node features and establish a learnable graph. Motivated by this goal, we propose a framework dubbed G raph C onvolutional N etwork with e lastic t opology (GCNet 1 ). With the analysis of the optimization for the proposed flexible Laplacian embedding, GCNet is naturally constructed by alternative graph convolutional layers and adaptive topology learning layers. GCNet aims to deeply explore the feature space and employ the mined information to construct a learnable topology, which leads to a more robust graph representation. In addition, a set-level orthogonal loss is utilized to meet the orthogonal constraint required by the flexible Laplacian embedding and promote better class separability. Moreover, comprehensive experiments indicate that GCNet achieves remarkable performance and generalization on several real-world datasets.},
  archive      = {J_PR},
  author       = {Zhihao Wu and Zhaoliang Chen and Shide Du and Sujia Huang and Shiping Wang},
  doi          = {10.1016/j.patcog.2024.110364},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110364},
  shortjournal = {Pattern Recognition},
  title        = {Graph convolutional network with elastic topology},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera-aware cluster-instance joint online learning for
unsupervised person re-identification. <em>PR</em>, <em>151</em>,
110359. (<a href="https://doi.org/10.1016/j.patcog.2024.110359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised person re-identification (re-ID) aims at learning discriminative feature representations for person retrieval without any annotations. Pseudo-label-based methods that iteratively perform pseudo-label generation and model training are currently the most popular approach to achieve this goal. However, distribution variations among cameras inevitably introduce noise in the generated pseudo-labels. Moreover, they are often assigned offline using relatively simple clustering criteria, which further accumulates the noise and limits the potential improvement in model performance. To address these issues, we propose a novel camera-aware cluster-instance joint online learning (CCIOL) framework that leverages the online inter-camera K-reciprocal nearest neighbors (OICKRNs) mined for each sample at every iteration to soften the traditional hard pseudo-labels at the cluster-level and generate multi-labels at the instance-level. Additionally, contrastive learning losses at two levels are employed to rectify the erroneous closeness between samples and promote intra-class aggregation and inter-class separation. Extensive experimental results on Market1501 and MSMT17 demonstrate the competitiveness of the proposed method compared to state-of-the-art unsupervised re-ID approaches.},
  archive      = {J_PR},
  author       = {Zhaoru Chen and Zheyi Fan and Yiyu Chen and Yixuan Zhu},
  doi          = {10.1016/j.patcog.2024.110359},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110359},
  shortjournal = {Pattern Recognition},
  title        = {Camera-aware cluster-instance joint online learning for unsupervised person re-identification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting single-step adversarial training for robustness
and generalization. <em>PR</em>, <em>151</em>, 110356. (<a
href="https://doi.org/10.1016/j.patcog.2024.110356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, single-step adversarial training has received high attention because it shows robustness and efficiency. However, a phenomenon referred to as “catastrophic overfitting” has been observed, which is prevalent in single-step defenses and may frustrate attempts to use FGSM adversarial training. To address this issue, we propose a novel method, S table and E fficient A dversarial T raining ( SEAT ). SEAT mitigates catastrophic overfitting by harnessing on local properties that differentiate a robust model from one prone to catastrophic overfitting. The proposed SEAT is underpinned by robust theoretical justifications, in that minimizing the SEAT loss is demonstrated to promote a smoother empirical risk, consequently enhancing robustness. Experimental results demonstrate that the proposed method successfully mitigates catastrophic overfitting, yielding superior performance amongst efficient defenses. Our single-step method can reach 51% robust accuracy for CIFAR-10 with l ∞ l∞ perturbations of radius 8/255 under a strong PGD-50 attack, matching the performance of a 10-step iterative method at merely 3% computational cost.},
  archive      = {J_PR},
  author       = {Zhuorong Li and Daiwei Yu and Minghui Wu and Sixian Chan and Hongchuan Yu and Zhike Han},
  doi          = {10.1016/j.patcog.2024.110356},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110356},
  shortjournal = {Pattern Recognition},
  title        = {Revisiting single-step adversarial training for robustness and generalization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-view learning via m-estimator joint sparse
representation. <em>PR</em>, <em>151</em>, 110355. (<a
href="https://doi.org/10.1016/j.patcog.2024.110355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multi-view learning has achieved extraordinary success in many research areas such as pattern recognition and data mining. Most existing multi-view methods mainly focus on exploring the correlation information between different views and their performance may severely degrade in the presence of heavy noises and outliers. In this paper, we put forward a robust multi-view joint sparse representation (RMJSR) method for multi-view learning. Firstly, we design a novel multi-view Cauchy estimator based loss function originating from robust statistics to address complex noises and outliers in reality. Based on this, we leverage the ℓ 1 , q ℓ1,q norm to enhance our model by encouraging the learned representation of multiple views to share the same sparsity pattern. Secondly, to explore the optimal solution for the RMJSR model, we devise an effective optimization algorithm based on the half-quadratic (HQ) theory and the alternating direction method of multipliers (ADMM) framework. Thirdly, we provide the theoretical guarantee for revealing the theoretical condition for the success of the proposed method. Further, we have also provided extensive analysis of the proposed method, including the optimality condition, convergence analysis, and complexity analysis. Extensive experimental results validate the effectiveness and robustness of the proposed method in comparison with state-of-the-art competitors. The source code is available at https://github.com/Huyutao7/RMJSRC .},
  archive      = {J_PR},
  author       = {Yutao Hu and Yulong Wang and Han Li and Hong Chen},
  doi          = {10.1016/j.patcog.2024.110355},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110355},
  shortjournal = {Pattern Recognition},
  title        = {Robust multi-view learning via M-estimator joint sparse representation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FApSH: An effective and robust local feature descriptor for
3D registration and object recognition. <em>PR</em>, <em>151</em>,
110354. (<a href="https://doi.org/10.1016/j.patcog.2024.110354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) local feature descriptor plays an important role in 3D computer vision because it is widely used to build point-to-point correspondences in many 3D vision applications. However, existing descriptors are difficult to have both high descriptiveness and strong robustness to various nuisances (e.g., noise and occlusion). To address this problem, a descriptor named Fully Attribute-pairs Statistical Histogram (FApSH) is proposed. FApSH is constructed on a local reference axis (LRA), and fully encodes the relevancy information of five attributes at each neighbor point by ten attribute-pair statistics. In this process, a priori distribution-based partition strategy is proposed for evenly distributing the attribute values of all neighbor points, and a radial distance-based histogram assignment method is proposed to improve the robustness to noise and outliers. The proposed methods are rigorously evaluated on six benchmark datasets with different application scenarios and nuisances. The results show that FApSH has high descriptiveness and strong robustness. It obviously outperforms the existing handcrafted descriptors, and is comparable to some superior learning-based descriptors. The results also show that the proposed priori distribution-based partition strategy significantly reduces the length and also improves the descriptiveness of FApSH, and the proposed radial distance-based histogram assignment method improves the robustness of FApSH on various datasets.},
  archive      = {J_PR},
  author       = {Bao Zhao and Zihan Wang and Xiaobo Chen and Xianyong Fang and Zhaohong Jia},
  doi          = {10.1016/j.patcog.2024.110354},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110354},
  shortjournal = {Pattern Recognition},
  title        = {FApSH: An effective and robust local feature descriptor for 3D registration and object recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dog identification based on textural features and spatial
relation of noseprint. <em>PR</em>, <em>151</em>, 110353. (<a
href="https://doi.org/10.1016/j.patcog.2024.110353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes dog identification technology based on dog noseprints, which are equivalent to human fingerprints and possess unique characteristics. The aim is to utilize this technology for identifying and managing stray animals. The study presents three processing stages. In the first stage, YOLOv3 detects the dog&#39;s nose and nostril regions. The second stage involves enhancing the image&#39;s contrast and the contour of the scaly block using the multi-scale line detector. Finally, in the third stage, the shape and spatial features of the scaly block are extracted and utilized for dog identification. The study included a dataset of 276 dogs from multiple animal families and public shelters in Taiwan. The dataset was randomly divided into three groups to determine the optimal parameters for matching the dog noseprint via experimentation. Each dog identification group achieved an accuracy (ACC) exceeding 97.83 %, demonstrating that the proposed parameter-matching method offers high stability. Furthermore, in an additional experimental dataset consisting of dog noseprint images used for dog identification, the proposed method achieved an ACC exceeding 90.22 % in the Top 1 and 94.57 % in the Top 3. The ACC results across different groups consistently demonstrate the proposed method&#39;s ability to achieve high accuracy in dog identification. The source code and trained models are publicly available at: https://github.com/Chuen-HorngLin/Dog-Identification-Noseprint .},
  archive      = {J_PR},
  author       = {Yung-Kuan Chan and Chuen-Horng Lin and Ching-Lin Wang and Keng-Chang Tu and Shu-Chun Yang and Meng-Hsiun Tsai and Shyr-Shen Yu},
  doi          = {10.1016/j.patcog.2024.110353},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110353},
  shortjournal = {Pattern Recognition},
  title        = {Dog identification based on textural features and spatial relation of noseprint},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive weighted dictionary representation using anchor
graph for subspace clustering. <em>PR</em>, <em>151</em>, 110350. (<a
href="https://doi.org/10.1016/j.patcog.2024.110350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Samples are commonly represented as sparse vectors in many dictionary representation algorithms. However, this method may result in loss of discriminatory information. Moreover, a redundant dictionary can increase the computational complexity of the algorithm. To tackle these challenges, we propose a novel method named Adaptive Weighted Dictionary Representation using Anchor Graph for Subspace Clustering (AWDR). First, AWDR constructs an anchor graph that encodes the classification information and establishes accurate connectivity components between anchors and clusters, thereby fully utilizing the discriminative information of the original samples. In addition, AWDR learns a complete-dictionary in the subspace to eliminate the noise and out-of-sample effects of the original sample space, while also improving computational efficiency. Finally, AWDR computes the coefficients for the samples in an adaptively weighted manner to find discriminative representation of the samples from the dictionary. Extensive experiments on real-world datasets demonstrate that our method is effective and efficient compared to the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wenyi Feng and Zhe Wang and Ting Xiao and Mengping Yang},
  doi          = {10.1016/j.patcog.2024.110350},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110350},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive weighted dictionary representation using anchor graph for subspace clustering},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A thorough experimental comparison of multilabel methods for
classification performance. <em>PR</em>, <em>151</em>, 110342. (<a
href="https://doi.org/10.1016/j.patcog.2024.110342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel classification as a data mining task has recently attracted increasing interest from researchers. Many current data mining applications address problems with instances that belong to more than one class. These problems require the development of new, efficient methods. Advantageously using the correlation among different labels can provide better performance than methods that manage each label separately. In recent decades, many methods have been developed to deal with multilabel datasets, which makes it difficult to decide which method is the most appropriate for a given task. In this paper, we present the most comprehensive comparison carried out so far. We compare a total of 62 different methods and several configurations of each one for a total of 197 trained models. We also use a large set of problems comprising 65 datasets. In addition, we studied the efficiency of the methods considering six different classification performance metrics. Our results show that, although there are methods that repeatedly appear among the top-performing models, the best methods are closely related to the metric used for evaluating the performance. We also analyzed different aspects of the behavior of the methods.},
  archive      = {J_PR},
  author       = {Nicolás E. García-Pedrajas and José M. Cuevas-Muñoz and Gonzalo Cerruela-García and Aida de Haro-García},
  doi          = {10.1016/j.patcog.2024.110342},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110342},
  shortjournal = {Pattern Recognition},
  title        = {A thorough experimental comparison of multilabel methods for classification performance},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global feature-based multimodal semantic segmentation.
<em>PR</em>, <em>151</em>, 110340. (<a
href="https://doi.org/10.1016/j.patcog.2024.110340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating complementary modality into RGB branch can significantly improve the effectiveness of semantic segmentation. However, fusion between the two modalities faces huge challenge due to the difference of their optical dimensions. Existed fusion methods can&#39;t keep a balance between performance and efficiency in aggregating detailed features. To address this problem, we propose a global feature-based network (GFBN) for semantic segmentation that establishes mapping function and extraction relationship among the multi-modalities. The GFBN contains three important modules, which are used for feature correction, fusion and edge enhancement. Firstly, the cross-attention rectification module (CARM) adaptively extracts mapping relationships and rectifies the RGB and complementary features. Secondly, the cross-field fusion module (CFM) integrates long-range rectified features of two branches to obtain an optimal fusion feature. Finally, the boundary guidance module (BGM) sharpens the boundary information of the fused features to effectively improve the segmentation accuracy of object boundaries. We make the experiments of GFBN on the challenging MCubeS and ZJU-RGB-Ps datasets. The results show that GFBN outperforms state-of-the-art methods by at least 0.64 % and 0.7 % on mean intersection over union (mIoU), respectively. It demonstrates the performance and efficiency of our proposed method. The code corresponding to our method can be found at the following link: https://github.com/Sci-Epiphany/GFBNext .},
  archive      = {J_PR},
  author       = {Suining Gao and Xiubin Yang and Li Jiang and Zongqiang Fu and Jiamin Du},
  doi          = {10.1016/j.patcog.2024.110340},
  journal      = {Pattern Recognition},
  month        = {7},
  pages        = {110340},
  shortjournal = {Pattern Recognition},
  title        = {Global feature-based multimodal semantic segmentation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wavelet-based auto-encoder for simultaneous haze and rain
removal from images. <em>PR</em>, <em>150</em>, 110370. (<a
href="https://doi.org/10.1016/j.patcog.2024.110370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise introduced due to weather can reduce the efficiency of computer vision applications as the visibility of the objects in images is greatly affected. Haze and rain are the most common weather conditions seen in nature. However, most of the algorithms found in the literature apply rain and haze removal approaches separately. To this end, in this paper, we propose a novel Wavelet-based deep Auto-encoder, called WAE, for simultaneously removing the haze and rain effects from images. The proposed network uses wavelet transformation and inverse wavelet transformation as an alternative to down-sampling and up-sampling operations, respectively, in order to add sparsity to the network. By training the model on both spatial and frequency domains, it learns non-stationary features that are found to be useful to remove haze and rain effects from images. The proposed model is tested on several rain and haze-affected image datasets, and it performs well in terms of standard evaluation metrics like structural similarity index measure and peak signal-to-noise ratio. The code can be found at : https://github.com/asfakali/WAE.git .},
  archive      = {J_PR},
  author       = {Asfak Ali and Ram Sarkar and Sheli Sinha Chaudhuri},
  doi          = {10.1016/j.patcog.2024.110370},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110370},
  shortjournal = {Pattern Recognition},
  title        = {Wavelet-based auto-encoder for simultaneous haze and rain removal from images},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-granularity cross transformer network for person
re-identification. <em>PR</em>, <em>150</em>, 110362. (<a
href="https://doi.org/10.1016/j.patcog.2024.110362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) aims to retrieve the same person in the gallery. Transformers have been introduced to the Re-ID task due to their excellent ability to model long-range dependency. However, due to the properties of the global attention mechanism, they are less effective in capturing the discriminative local semantics of pedestrians compared to convolutional operations. To address this issue, we present a Multi-granularity Cross Transformer Network (MCTN) that progressively learns salient features of different local structures in a global context. Specifically, we first utilize a Multi-granularity Convolutional Layer (MCL) to investigate salient pedestrian features at various granularities. On this basis, we propose a Pyramidal Cross Transformer learning layer (PCT), which contains a pyramidal division of pedestrian image feature maps, differentiated feature extraction of different parts of pedestrians, and cross attention to exploring the local–global relationship of the feature map. It allows effective mining of local information in the global structure from a coarse-to-fine perspective. Furthermore, to enhance the interaction between low-level detailed features and high-level semantic features, a Hierarchical Aggregation Strategy (HAS) is introduced to fuse features learned by cross attention learning at different stages. Pedestrian features learned in shallow layers will serve as global priors for semantics learning in deep layers. We evaluate our method on four large-scale Re-ID datasets, and the experimental results reveal that the proposed method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yanping Li and Duoqian Miao and Hongyun Zhang and Jie Zhou and Cairong Zhao},
  doi          = {10.1016/j.patcog.2024.110362},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110362},
  shortjournal = {Pattern Recognition},
  title        = {Multi-granularity cross transformer network for person re-identification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing identification for person search with multi-scale
multi-grained representation learning. <em>PR</em>, <em>150</em>,
110361. (<a href="https://doi.org/10.1016/j.patcog.2024.110361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Search aims to simultaneously address Person Detection and Person Re-ID. There are various challenges in person search such as significant scale variations, occlusions, and partial instances. In this paper, we propose a Multi-Scale Multi-Grained (MSMG) sequential network for end-to-end person search, intended to alleviate these issues. To generate re-id representations robust to scale changes, MSMG leverages multi-scale RoI features and aggregates them with a proposed Multi-Scale feature Aggregation Encoder (MSAE). In this way, the aggregated multi-scale re-id features are enriched with more semantic information and detailed information, thereby being more discriminative for identification. Moreover, to produce re-id representations more robust to occlusions and partial instances, MSMG introduces a Multi-Grained feature Learning Decoder (MGLD) focused on multi-grained feature learning. MGLD adaptively decodes multi-grained re-id representations with more accurate semantic information through a regional deformable cross-attention module. Finally, the multi-scale multi-grained re-id representation substantially improves the identification accuracy under challenging cases. Through comprehensive experiments, we demonstrate that our method achieves state-of-the-art performance on two benchmark datasets. On the challenging PRW benchmark, MSMG obtains the best-reported results with a mean average precision (mAP) score of 61.3%.},
  archive      = {J_PR},
  author       = {Zhixiong Han and Bingpeng Ma},
  doi          = {10.1016/j.patcog.2024.110361},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110361},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing identification for person search with multi-scale multi-grained representation learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapt only once: Fast unsupervised person re-identification
via relevance-aware guidance. <em>PR</em>, <em>150</em>, 110360. (<a
href="https://doi.org/10.1016/j.patcog.2024.110360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive person re-identification (UDA person reID) defines a task where labels in target domains are totally unknown while source domains are fully labeled. Assigning reliable labels quickly is a critical issue for UDA person reID that could be applied in the real-world scenarios. Recent studies focus on obtaining pseudo labels by clustering algorithms and then training the reID model with these labels. However, the main limitation of these methods is the high time complexity, which is caused by the calculation of all pair-wise similarities and multiple iterations in the clustering algorithm to obtain reliable results. When the data is very large or the feature dimensions are very high, the memory and time cost requirements of the clustering algorithm can increase rapidly. In this paper, we provide a fast unsupervised domain adaptive person reID framework (FUReID), which calculates the relevance between unlabeled samples only once to adapt to the new scenarios without any iterations in the stage of label generation. Especially, instead of pursuing accurate labels, FUReID considers constructing a lightweight paradigm to generate coarse labels and then refine these labels during the training stage. Therefore, FUReID designs a prototype-guided labeling method that only relies on calculating the relevance between the prototype vectors and the samples, and assigning coarse labels with noise. Then, to alleviate the issue of noise, FUReID designs a label-flexible training network with an adaptive selection strategy to refine those coarse labels progressively. For several widely-used person reID datasets, our method achieves 81.7%, 26.2%, and 87.7% in mAP on Market1501, MSMT17 and PersonX, respectively. Code is available at https://github.com/AILab90/FUReID .},
  archive      = {J_PR},
  author       = {Jinjia Peng and Jiazuo Yu and Chengjun Wang and Huibing Wang and Xianping Fu},
  doi          = {10.1016/j.patcog.2024.110360},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110360},
  shortjournal = {Pattern Recognition},
  title        = {Adapt only once: Fast unsupervised person re-identification via relevance-aware guidance},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised imbalanced multi-label classification with
label propagation. <em>PR</em>, <em>150</em>, 110358. (<a
href="https://doi.org/10.1016/j.patcog.2024.110358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label learning tasks usually encounter the problem of the class-imbalance, where samples and their corresponding labels are non-uniformly distributed over multi-label data space. It has attracted increasing attention during the past decade, however, there is a lack of methods capable of handling the imbalanced problem in a semi-supervised setting. This study proposes a label propagation technique to settle the semi-supervised imbalanced multi-label issue. Specially, we first utilize a collaborative manner to exploit the correlations from labels and instances, and learn a label regularization matrix to overcome the imbalanced problem in the labeled instance. After that, we extend to semi-supervised learning and explore to represent the similarity of instances with weighted graphs on labeled and unlabeled data. Then, the data distribution information and label correlations are fully utilized to design the loss function under the consistency assumption manner. At last, we present an iterative scheme to settle the optimization issue, thereby achieving label propagation to address the imbalanced challenge. Experiments on a variety of multi-label data sets show the favorable performance of the proposed method against related comparing approaches. Notably, the proposed method is also validated to be robust with a limited number of training instances.},
  archive      = {J_PR},
  author       = {Guodong Du and Jia Zhang and Ning Zhang and Hanrui Wu and Peiliang Wu and Shaozi Li},
  doi          = {10.1016/j.patcog.2024.110358},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110358},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised imbalanced multi-label classification with label propagation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient image analysis with triple attention vision
transformer. <em>PR</em>, <em>150</em>, 110357. (<a
href="https://doi.org/10.1016/j.patcog.2024.110357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces TrpViT, a novel triple attention vision transformer that efficiently captures both local and global features. The proposed architecture tackles global information acquisition by employing three complementary attention mechanisms in a unique attention block: Window, Dilated, and Channel attention. This attention block extracts spatially local features while expanding the receptive field to capture richer global context. By integrating this attention block with convolution, a new C-C-T-T architecture is formed. We rigorously evaluate TrpViT, demonstrating state-of-the-art performance on various computer vision tasks, including image classification, 2D and 3D object detection, instance segmentation, and low-level image colorization. Notably, TrpViT achieves strong accuracy across all parameter scales, highlighting its computational efficiency and effectiveness.},
  archive      = {J_PR},
  author       = {Gehui Li and Tongtong Zhao},
  doi          = {10.1016/j.patcog.2024.110357},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110357},
  shortjournal = {Pattern Recognition},
  title        = {Efficient image analysis with triple attention vision transformer},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prior based pyramid residual clique network for human body
image super-resolution. <em>PR</em>, <em>150</em>, 110352. (<a
href="https://doi.org/10.1016/j.patcog.2024.110352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research in the analysis of human images, such as human parsing and pose estimation, usually requires input images to have a sufficiently high resolution. However, small images of people are commonly encountered in our daily lives, particularly in surveillance applications. This paper aims to ultra-resolve a tiny person image to its high-resolution counterpart by learning effective feature representations and exploiting useful human body prior knowledge. First, we propose the Residual Clique Block (RCB) to fully exploit compact feature representations for image Super-Resolution (SR). Second, a series of RCBs are cascaded in a coarse-to-fine manner to construct the Pyramid Residual Clique Network (PRCN), which simultaneously reconstructs multiple SR results (e.g. 2 × × , 4 × × , and 8 × × ) in one feed-forward pass. Third, we utilize the human parsing map as the shape prior, and the high-frequency sub-bands of Uniform Discrete Curvelet Transform (UDCT) as the texture prior to enhance the details of reconstructed human body image. Experimental results demonstrate that our proposed method achieves state-of-the-art performance with superior visual quality and PSNR/SSIM scores. Moreover, we show that our results can considerably enhance the performance of human parsing and pose estimation tasks.},
  archive      = {J_PR},
  author       = {Simiao Wang and Yu Sang and Yunan Liu and Chunpeng Wang and Mingyu Lu and Jinguang Sun},
  doi          = {10.1016/j.patcog.2024.110352},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110352},
  shortjournal = {Pattern Recognition},
  title        = {Prior based pyramid residual clique network for human body image super-resolution},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated convolutional neural networks for joint
super-resolution and classification of radar images. <em>PR</em>,
<em>150</em>, 110351. (<a
href="https://doi.org/10.1016/j.patcog.2024.110351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have been widely used for two-dimensional (2D) and three-dimensional (3D) computer vision problems, such as object detection, super-resolution (SR) and classification to name a few. Radar images suffer from poor resolution as compared to optical images, hence developing a high-accuracy model to solve computer vision problems, such as a classifier, is a challenge. This is because of the lack of high-frequency details in the input images which makes it difficult for the classifier model to generate accurate predictions. Ways of addressing this challenge include training the learning model with a large dataset or using a more complicated model, such as deeper layer architecture. However, employing such approaches might result in the overfitting of the model, where the model might not generalize well on previously unseen data. Also, generating a large dataset for training the model is a challenging task, especially in the case of radar images. An alternate solution for achieving high accuracy in radar classification problems is provided in this paper wherein a CNN-enabled super-resolution (SR) model is integrated with the classifier model. The SR model is trained to generate high-resolution (HR) millimeter-wave (mmW) images from any input low-resolution (LR) mmW images. These resolved images from the SR model will be used by the classifier model to classify the input images into appropriate classes, consisting of threat and non-threat objects. The training data for the dual CNN layers are generated using a numerical model of a near-field coded-aperture computational imaging (CI) system. This trained dual CNN model is tested with simulated data generated from the CI numerical model wherein a high classification accuracy of 95% and a fast inference time of 0.193 s are obtained, making it suitable for real-time automated threat classification applications. For fair comparison, the developed CNN model is also validated with experimentally generated reconstruction data, in which case, a classification accuracy of 94% is obtained.},
  archive      = {J_PR},
  author       = {Rahul Sharma and Bhabesh Deka and Vincent Fusco and Okan Yurduseven},
  doi          = {10.1016/j.patcog.2024.110351},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110351},
  shortjournal = {Pattern Recognition},
  title        = {Integrated convolutional neural networks for joint super-resolution and classification of radar images},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A patch distribution-based active learning method for
multiple instance alzheimer’s disease diagnosis. <em>PR</em>,
<em>150</em>, 110341. (<a
href="https://doi.org/10.1016/j.patcog.2024.110341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical data, particularly the complex brain imaging structures, acquisition presents significant difficulties and high diagnostic expenses, resulting in a scarcity of the trainable samples in the real-world scenarios. To overcome this limitation, we present an active learning-based sampling strategy that selects the most informative samples from the unlabeled candidate sample pool for expert annotation, leading to high classification performance with a reduced number of training samples. This study adopts a patch-level perspective and introduces a multi-instance learning framework for Alzheimer&#39;s Disease diagnosis. Initially, a patch pre-selection module is designed to identify pathology-prone regions while excluding background areas and irrelevant information. Subsequently, an inner-patch local attention mechanism block and an outer-patch global attention mechanism block are developed to enhance the extraction of discriminative local and global information by the network model. Finally, an active learning sampling strategy is devised to minimize the costs associated with data acquisition and expert annotation in medical domain. The effectiveness of the proposed network framework and active learning strategy was validated through four sets of control experiments on the ADNI dataset.},
  archive      = {J_PR},
  author       = {Tianxiang Wang and Qun Dai},
  doi          = {10.1016/j.patcog.2024.110341},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110341},
  shortjournal = {Pattern Recognition},
  title        = {A patch distribution-based active learning method for multiple instance alzheimer&#39;s disease diagnosis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating foreground–background feature distillation and
contrastive feature learning for ultra-fine-grained visual
classification. <em>PR</em>, <em>150</em>, 110339. (<a
href="https://doi.org/10.1016/j.patcog.2024.110339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pattern recognition, ultra-fine-grained visual classification (ultra-FGVC) has emerged as a paramount challenge, focusing on sub-category distinction within fine-grained objects. The near-indistinguishable similarities among such objects, combined with the dearth of sample data, intensify this challenge. In response, our FDCL-DA method is introduced, which integrates Foreground–background feature Distillation (FD) and Contrastive feature Learning (CL) with Dual Augmentation (DA). This method uses two different data augmentation techniques, standard and auxiliary augmentation, to enhance model performance and generalization ability. The FD module reduces superfluous features and augments the contrast between the principal entity and its backdrop, while the CL focuses on creating unique data imprints by reducing intra-class resemblances and enhancing inter-class disparities. Integrating this method with different architectures, such as ResNet-50, Vision Transformer, and Swin-Transformer (Swin-T), significantly improves these backbone networks, especially when used with Swin-T, leading to promising results on eight popular datasets for ultra-FGVC tasks. 1},
  archive      = {J_PR},
  author       = {Qiupu Chen and Lin Jiao and Fenmei Wang and Jianming Du and Haiyun Liu and Xue Wang and Rujing Wang},
  doi          = {10.1016/j.patcog.2024.110339},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110339},
  shortjournal = {Pattern Recognition},
  title        = {Integrating foreground–background feature distillation and contrastive feature learning for ultra-fine-grained visual classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Discovering causally invariant features for
out-of-distribution generalization. <em>PR</em>, <em>150</em>, 110338.
(<a href="https://doi.org/10.1016/j.patcog.2024.110338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) generalization aims to generalize a model trained on source domains to unseen target domains. Recently, causality-based generalization methods have focused on learning invariant causal relationships around the label variable, as causal mechanisms are robust across different domains. However, these methods would yield an inaccurate causal variable set due to the lack of heterogeneous domain data or a prior causal structure, which severely weakens their generalization capacity. To address this problem, we propose a Causally Invariant Features Discovery (CIFD) framework, which combines causal structure discovery and causal effect estimation for selecting a high-quality causal variable set and realizing better OOD generalization. Specifically, CIFD first identifies all potential causal variables by learning a double-layer-based local causal structure around the label variable. Secondly, CIFD uses a double-layer causal effect estimator for estimating the causality of potential causal variables and obtaining true causal variables. The comprehensive experiments on both regression and classification tasks clearly demonstrate the superiority of our framework over the state-of-art methods.},
  archive      = {J_PR},
  author       = {Yujie Wang and Kui Yu and Guodu Xiang and Fuyuan Cao and Jiye Liang},
  doi          = {10.1016/j.patcog.2024.110338},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110338},
  shortjournal = {Pattern Recognition},
  title        = {Discovering causally invariant features for out-of-distribution generalization},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FLODCAST: Flow and depth forecasting via multimodal
recurrent architectures. <em>PR</em>, <em>150</em>, 110337. (<a
href="https://doi.org/10.1016/j.patcog.2024.110337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting motion and spatial positions of objects is of fundamental importance, especially in safety-critical settings such as autonomous driving. In this work, we address the issue by forecasting two different modalities that carry complementary information, namely optical flow and depth. To this end we propose FLODCAST a flow and depth forecasting model that leverages a multitask recurrent architecture, trained to jointly forecast both modalities at once. We stress the importance of training using flows and depth maps together, demonstrating that both tasks improve when the model is informed of the other modality. We train the proposed model to also perform predictions for several timesteps in the future. This provides better supervision and leads to more precise predictions, retaining the capability of the model to yield outputs autoregressively for any future time horizon. We test our model on the challenging Cityscapes dataset, obtaining state of the art results for both flow and depth forecasting. Thanks to the high quality of the generated flows, we also report benefits on the downstream task of segmentation forecasting, injecting our predictions in a flow-based mask-warping framework.},
  archive      = {J_PR},
  author       = {Andrea Ciamarra and Federico Becattini and Lorenzo Seidenari and Alberto Del Bimbo},
  doi          = {10.1016/j.patcog.2024.110337},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110337},
  shortjournal = {Pattern Recognition},
  title        = {FLODCAST: Flow and depth forecasting via multimodal recurrent architectures},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RASNet: Renal automatic segmentation using an improved u-net
with multi-scale perception and attention unit. <em>PR</em>,
<em>150</em>, 110336. (<a
href="https://doi.org/10.1016/j.patcog.2024.110336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the treatment of renal disease, the application of radioactive equipment has become one of the important methods. Accurate segmentation of renal contour plays an important role in clinical diagnosis. However, manual renal contour drawing is not only inefficient but also prone to inaccurate outlining results due to different manual proficiency and fatigue caused by long-term work. There is little research on automatic renal segmentation with renal dynamic imaging. To address this issue, an improved model based on a deep neural network called Renal Automatic Segmentation Network (RASNet) is proposed, to aid in the automatic segmentation of renal contours. Besides, a multi-scale spatial perception module and a decoding module with attention connection are introduced to enrich the semantic information and further improve the accuracy of network segmentation. Extensive experiments were conducted on a renal dynamic medical image database established in this paper. Analysis results show the superiority of the proposed RASNet to several existing segmentation frameworks.},
  archive      = {J_PR},
  author       = {Gaoyu Cao and Zhanquan Sun and Chaoli Wang and Hongquan Geng and Hongliang Fu and Zhong Yin and Minlan Pan},
  doi          = {10.1016/j.patcog.2024.110336},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110336},
  shortjournal = {Pattern Recognition},
  title        = {RASNet: Renal automatic segmentation using an improved U-net with multi-scale perception and attention unit},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OLCH: Online label consistent hashing for streaming
cross-modal retrieval. <em>PR</em>, <em>150</em>, 110335. (<a
href="https://doi.org/10.1016/j.patcog.2024.110335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing has received growing interest to facilitating efficient retrieval across large-scale multi-modal data, and existing methods still face three challenges: 1) Most offline learning works are unsuitable for processing and training with streaming multi-modal data. 2) Current online learning methods rarely consider the potential interdependency between the label categories. 3) Existing supervised methods often utilize pairwise label similarities or adopt relaxation scheme to learn hash codes, which, respectively, require much computation time or accumulate large quantization loss during the learning process. To alleviate these challenges, this paper presents an efficient Online Label Consistent Hashing (OLCH) approach for streaming cross-modal retrieval. The proposed approach first exploits the relative similarity of semantic labels and utilizes the multi-class classification to derive the common semantic vector. Then, an online semantic representation learning framework is adaptively designed to preserve the semantic similarity across different modalities, and a mini-batch online gradient descent approach associated with forward–backward splitting is developed to discriminatively optimize the hash functions. Accordingly, the hash codes are incrementally learned with high discriminative capability, while avoiding high computation complexity to process the streaming data. Extensive experiments highlight the superiority of the proposed approach and show its very competitive performance in comparison with the state-of-the-arts.},
  archive      = {J_PR},
  author       = {Shu-Juan Peng and Jinhan Yi and Xin Liu and Yiu-ming Cheung and Zhen Cui and Taihao Li},
  doi          = {10.1016/j.patcog.2024.110335},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110335},
  shortjournal = {Pattern Recognition},
  title        = {OLCH: Online label consistent hashing for streaming cross-modal retrieval},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferable graph auto-encoders for cross-network node
classification. <em>PR</em>, <em>150</em>, 110334. (<a
href="https://doi.org/10.1016/j.patcog.2024.110334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Node classification is a popular and challenging task in graph neural networks, and existing approaches are mainly developed for a single network. With the advances in domain adaptation, researchers tend to leverage knowledge extracted from a fully-labeled source network to further improve the node classification performance in an unlabeled target network. This learning paradigm refers to cross-network node classification, which is the topic we studied in this paper. Specifically, we propose a novel model named Transferable Graph Auto-Encoders (TGAE), which first encodes the initial network data into latent representations and then decodes the learned features to preserve graph information. In the encoding phase, TGAE adopts the attentional mechanism to fuse the local and global information of nodes to discover latent node representations. To obtain transferable features between the source and target networks, TGAE aligns their distributions based on the learned representations by reducing marginal and conditional distribution differences. In the decoding phase, the latent representations are subjected to pairwise and reconstruction constraints, thus preserving structural proximity and graph topology information to learn discriminative features. Besides, a node classifier is trained to enhance the discriminant of the node representations further. Experimental results on several real-world datasets demonstrate that the proposed model achieves state-of-the-art performance in cross-network node classification tasks compared with existing methods.},
  archive      = {J_PR},
  author       = {Hanrui Wu and Lei Tian and Yanxin Wu and Jia Zhang and Michael K. Ng and Jinyi Long},
  doi          = {10.1016/j.patcog.2024.110334},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110334},
  shortjournal = {Pattern Recognition},
  title        = {Transferable graph auto-encoders for cross-network node classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving self-supervised action recognition from extremely
augmented skeleton sequences. <em>PR</em>, <em>150</em>, 110333. (<a
href="https://doi.org/10.1016/j.patcog.2024.110333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised contrastive learning has been widely applied to skeleton-based action recognition due to its ability to learn discriminative features. However, directly applying the existing contrastive learning framework for 3D skeleton learning is limited by the well-designed augmentations and the simple multi-stream decision-level fusion. To deal with these drawbacks, we propose a three-stream contrastive learning framework utilizing abundant information mining for self-supervised action representation (3s-AimCLR++). For single-stream contrastive learning, extreme augmentation is first proposed to generate more movement patterns, which can introduce more movement patterns to improve the universality of the learned representations. Since directly using extreme augmentation can barely boost the performance due to the drastic changes in original identity, the Distributional Divergence Minimization (DDM) loss is proposed to utilize the extreme augmentation more gently. Moreover, the Single-Stream Nearest Neighbors Mining (SNNM) is proposed to expand positive samples to make the learning process more reasonable. For multi-stream, existing methods simply ensemble the results. Yet, considering the complementarity of information between different streams, we propose Multi-Stream Aggregation and Interaction (MSAI) strategy to better fuse multi-stream information. Extensive experiments on NTU-60, NTU-120, and PKU-MMD datasets have verified that our 3s-AimCLR++ can significantly perform favorably against state-of-the-art methods under a variety of evaluation protocols. The code and models are available at https://github.com/Levigty/AimCLR-v2 .},
  archive      = {J_PR},
  author       = {Tianyu Guo and Mengyuan Liu and Hong Liu and Guoquan Wang and Wenhao Li},
  doi          = {10.1016/j.patcog.2024.110333},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110333},
  shortjournal = {Pattern Recognition},
  title        = {Improving self-supervised action recognition from extremely augmented skeleton sequences},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FSDA: Frequency re-scaling in data augmentation for
corruption-robust image classification. <em>PR</em>, <em>150</em>,
110332. (<a href="https://doi.org/10.1016/j.patcog.2024.110332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern convolutional neural networks (CNNs) are used in various applications, including computer vision, speech recognition, and robotics. However, practical usage in various applications requires large-scale datasets, and real-world data contains various corruptions that degrade the model’s performance owing to the inconsistencies in the training and testing distributions. In this study, we propose Frequency re-Scaling Data Augmentation (FSDA) to improve the classification performance, robustness against corruption, and localizability of classifiers trained on various image classification datasets. Our method consists of two processes: mask generation process (MGP) and pattern re-scaling process (PSP). MGP clusters the frequency domain spectra to produce similar frequency patterns, and then PSP scales frequency by learning rescaling parameters from frequency patterns. Because the CNN classifies images by focusing on their structural features highlighted with FSDA, CNN trained with the proposed method has more robustness against corruption than that with the other data augmentations (DAs). Our technique outperforms the existing DAs on four public image classification datasets, including the CIFAR-10/100, STL-10, and ImageNet. Particularly, our strategy increases the robustness of the classifier against the different corruption errors by an average of 5.04% over the baseline.},
  archive      = {J_PR},
  author       = {Ju-Hyeon Nam and Sang-Chul Lee},
  doi          = {10.1016/j.patcog.2024.110332},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110332},
  shortjournal = {Pattern Recognition},
  title        = {FSDA: Frequency re-scaling in data augmentation for corruption-robust image classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain-driven facial image reconstruction via StyleGAN
inversion with improved identity consistency. <em>PR</em>, <em>150</em>,
110331. (<a href="https://doi.org/10.1016/j.patcog.2024.110331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of visual stimuli from fMRI data represents a major technological and scientific challenge at the forefront of contemporary neuroscience research. Deep learning techniques have played a critical role in advancing decoding models for visual stimulus reconstruction from fMRI data. Particularly, the use of advanced GANs has resulted in significant improvements in the quality of image generation, providing a powerful tool for addressing the challenges of this complex task. However, none of these studies have taken into account the inherent characteristics of the stimulus contents themselves; This, in turn, leads to unsatisfactory outcomes, as demonstrated by the inconsistent identity between reconstructed faces and ground truth in the decoding of facial images. In order to tackle this challenge, we introduce a new framework aimed at enhancing the accuracy of reconstructing facial images from fMRI data. Our key innovation involves extracting and disentangling multi-level visual information from brain signals in the latent space and optimizing high-level features for facial identity control using identity loss. Specifically, our framework uses StyleGAN inversion to extract hierarchical latent codes from images, which are then bridged to fMRI data through transformation blocks. Additionally, we introduce a multi-stage refinement method to enhance the accuracy of reconstructed faces, which involves progressively updating fMRI latent codes with custom loss functions designed for both feature- and image-wise optimization. Our experimental results demonstrate that our proposed framework effectively achieves two critical objectives: (1) accurate facial image reconstruction from fMRI data and (2) preservation of identity characteristics with a high level of consistency.},
  archive      = {J_PR},
  author       = {Ziqi Ren and Jie Li and Lukun Wu and Xuetong Xue and Xin Li and Fan Yang and Zhicheng Jiao and Xinbo Gao},
  doi          = {10.1016/j.patcog.2024.110331},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110331},
  shortjournal = {Pattern Recognition},
  title        = {Brain-driven facial image reconstruction via StyleGAN inversion with improved identity consistency},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPONet: A two-stream gated progressive optimization network
for salient object detection. <em>PR</em>, <em>150</em>, 110330. (<a
href="https://doi.org/10.1016/j.patcog.2024.110330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The salient object detection task is to locate and detect salient regions in images, which is widely applied in various fields. In this paper, we propose a gated progressive optimization network (GPONet) for salient object detection. Firstly, to extract salient regions more accurately, we design a multi-level feature fusion module with a gating mechanism named gate fusion network (GFN). GFN focuses on the semantic information of high-level features as well as the detailed information of low-level features, enabling purposeful delivery of high-level features to low-level features. The gate fusion unit (GFU) is also able to maintain valid information and suppress redundant information in the fusion process. Secondly, while some existing methods have shown that the additional edge supervision can facilitate salient object detection, edge pixels are often much less common than non-edge pixels, leading to the challenge of class imbalance. To overcome this issue, we introduce detail labels that provide additional internal details as a supplementary supervisory signal. Combining these labels with proposed detail perception loss (DPL) enables our network to learn edge information of salient objects more effectively. To complement each other and guide information exchange between the two branches, we propose a cross guide module (CGM) to control the information flow transfer between them. Finally, we develop a simple and efficient attention fusion strategy to merge the prediction maps of the two branches to generate the final salient prediction map. Extensive experimental results validate that our method reaches optimal or comparable performance on several mainstream datasets. The code of GPONet is available from https://github.com/antonie-z/GPONet .},
  archive      = {J_PR},
  author       = {Yugen Yi and Ningyi Zhang and Wei Zhou and Yanjiao Shi and Gengsheng Xie and Jianzhong Wang},
  doi          = {10.1016/j.patcog.2024.110330},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110330},
  shortjournal = {Pattern Recognition},
  title        = {GPONet: A two-stream gated progressive optimization network for salient object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSCA: A dual semantic correlation alignment method for
domain adaptation object detection. <em>PR</em>, <em>150</em>, 110329.
(<a href="https://doi.org/10.1016/j.patcog.2024.110329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In self-driving cars, adverse weather (e.g., fog, rain, snow, and cloud) or occlusion scenarios result in domain shift being unavoidable in object detection. Researchers have recently proposed Domain Adaptive Object Detection (DAOD), i.e., aligning the source and target domains at the image and instance levels distribution by utilizing the Unsupervised Domain Adaptation (UDA) method. However, the semantic correlation information is ignored leading to the effect of aligning not good, and low detection accuracy of objects in adverse weather or occlusion scenarios. Here, we propose a Dual Semantic Correlation Alignment (DSCA) method for DAOD to address the problem. The core idea behind DSCA is to make full use of semantic correlation information including context correlation semantic information and class correlation semantic information to align object semantic information in source and target domains, which supplement and enhance the missing information for target domains. It consists of a two-level semantic alignment: (1) context correlation semantic alignment is developed to obtain the context correlation semantic information of the object to align context semantic information at the image level; (2) class correlation semantic alignment is proposed to obtain the class correlation semantic information of the object to align class semantic information at the instance level. The two-level semantic alignment can effectively decrease negative transfer and complete object information to improve the detection accuracy of objects in different domains. Experiments on four challenging benchmarks show that our proposed DSCA method outperforms state-of-the-art DAOD methods.},
  archive      = {J_PR},
  author       = {Yinsai Guo and Hang Yu and Shaorong Xie and Liyan Ma and Xinzhi Cao and Xiangfeng Luo},
  doi          = {10.1016/j.patcog.2024.110329},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110329},
  shortjournal = {Pattern Recognition},
  title        = {DSCA: A dual semantic correlation alignment method for domain adaptation object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separate first, then segment: An integrity segmentation
network for salient object detection. <em>PR</em>, <em>150</em>, 110328.
(<a href="https://doi.org/10.1016/j.patcog.2024.110328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods aggregate multi-level features or introduce auxiliary information to get more refined saliency maps. However, little attention is paid to how to obtain complete salient objects in cluttered background. To address this problem, we propose an integrity segmentation network (ISNet) with a novel detection paradigm that first separates the targets completely and then segment them finely. Specifically, the ISNet architecture consists of a target separation (TS) branch and an object segmentation (OS) branch, trained using a hierarchical difference-aware (HDA) loss. The TS branch equipped with a fractal structure is utilized to produce saliency features with expanded boundary (SF w/ EB), which can enlarge the difference of border details to separate the target from background completely. Compared with the edge and skeleton information, the SF w/ EB contains a more complete structure, which can supplement the defect of salient objects. The OS branch is leveraged to generate complementary features, which gradually integrates the SF w/ EB and aggregated features to segment complete saliency maps. Moreover, we propose the HDA loss to further improve the structural integrity of prediction, which hierarchically assigns weight to pixels according to their differences. Hard pixels will be given more attention to discriminate the similar parts between foreground and background. Comprehensive experimental results on five datasets show that the proposed ISNet outperforms the state-of-the-art methods both quantitatively and qualitatively. Concretely, compared with three typical models, the average gain percentage reaches 2.6% in terms of F β Fβ , S m Sm and MAE on two large complex datasets. The improvements demonstrate that the proposed ISNet are beneficial for improving the integrity of prediction. Besides, the ISNet is efficient and runs at a real-time speed of 39.5 FPS when processing an image with size of 320 × 320. Furthermore, the proposed model has better generalization, which can also be applied to other vision tasks to handle complex scenes. Codes are available at https://github.com/lesonly/ISNet .},
  archive      = {J_PR},
  author       = {Ge Zhu and Jinbao Li and Yahong Guo},
  doi          = {10.1016/j.patcog.2024.110328},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110328},
  shortjournal = {Pattern Recognition},
  title        = {Separate first, then segment: An integrity segmentation network for salient object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised multi-task learning for medical image
analysis. <em>PR</em>, <em>150</em>, 110327. (<a
href="https://doi.org/10.1016/j.patcog.2024.110327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is crucial for preliminary screening and diagnostic assistance based on medical image analysis. However, limited annotated data and complex anatomical structures challenge existing models as they struggle to capture anatomical context information effectively. In response, we propose a novel self-supervised multi-task learning framework (SSMT), which integrates two key modules: a discriminative-based module and a generative-based module. These modules collaborate through multiple proxy tasks, encouraging models to learn global discriminative representations and local fine-grained representations. Additionally, we introduce an efficient uniformity regularization to further enhance the learned representations. To demonstrate the effectiveness of SSMT, we conduct extensive experiments on six public Chest X-ray image datasets. Our results highlight that SSMT not only outperforms existing state-of-the-art methods but also achieves comparable performance to the supervised model in challenging downstream tasks. The ablation study demonstrates collaboration between the key components of SSMT, showcasing its potential for advancing medical image analysis.},
  archive      = {J_PR},
  author       = {Huihui Yu and Qun Dai},
  doi          = {10.1016/j.patcog.2024.110327},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110327},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised multi-task learning for medical image analysis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A categorical interpretation of state merging algorithms for
DFA inference. <em>PR</em>, <em>150</em>, 110326. (<a
href="https://doi.org/10.1016/j.patcog.2024.110326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use Category Theory to interpret the family of algorithms for inference of DFAs that work by merging states. This interpretation allows us to characterize the structure of the search space and to define criteria for the convergence of these algorithms to the correct DFA. We also prove that the well-known EDSM algorithm does not identify DFAs in the limit.},
  archive      = {J_PR},
  author       = {Juan Miguel Vilar},
  doi          = {10.1016/j.patcog.2024.110326},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110326},
  shortjournal = {Pattern Recognition},
  title        = {A categorical interpretation of state merging algorithms for DFA inference},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certainty weighted voting-based noise correction for
crowdsourcing. <em>PR</em>, <em>150</em>, 110325. (<a
href="https://doi.org/10.1016/j.patcog.2024.110325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowdsourcing scenarios, we can obtain each instance’s multiple noisy label set from different workers and then use a ground truth inference algorithm to infer its integrated label. Despite the effectiveness of ground truth inference algorithms, there is still a certain level of noise in integrated labels. To reduce the impact of noise, many noise correction algorithms have been proposed in recent years. To the best of our knowledge, almost all these algorithms assume that workers have the same labeling certainty on different classes and instances. However, it is rarely true in reality due to the differences in workers’ individual preferences and cognitive abilities. In this paper, we argue that the labeling certainty of a worker should be class-dependent and instance-dependent. Based on this premise, we propose a certainty weighted voting-based noise correction (CWVNC) algorithm. At first, we use the consistency between worker-labeled labels and integrated labels on different classes to estimate the class-dependent certainty. Then, we train a probability-based classifier on the instances labeled by each worker separately and use it to estimate the instance-dependent certainty. Finally, we correct the integrated label of each instance by weighted voting based on class-dependent certainty and instance-dependent certainty. When the proposed algorithm CWVNC is examined, the average noise ratio of CWVNC on 34 simulated datasets is equal to 15.08%, and on two real-world datasets “Income” and “Music_genre” the noise ratio is equal to 25.77% and 26.94%, respectively. The results show that CWVNC significantly outperforms all other state-of-the-art noise correction algorithms used for comparison.},
  archive      = {J_PR},
  author       = {Huiru Li and Liangxiao Jiang and Chaoqun Li},
  doi          = {10.1016/j.patcog.2024.110325},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110325},
  shortjournal = {Pattern Recognition},
  title        = {Certainty weighted voting-based noise correction for crowdsourcing},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised local rotation-stable descriptors for 3D
ultrasound registration using translation equivariant FCN. <em>PR</em>,
<em>150</em>, 110324. (<a
href="https://doi.org/10.1016/j.patcog.2024.110324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotation-stable descriptors are crucial for feature matching in medical image registration. Most existing descriptors rely on hand-crafted models to achieve rotation stability, which are susceptible to complex noise and fail to efficiently extract batches of three-dimensional features, particularly for ultrasound volume. In this study, a translation equivariant design was performed based on the fully convolutional network to extract descriptors at different positions in batches by removing position bias errors, thereby improving the descriptor extraction efficiency. Descriptor rotation consistency is used for self-supervised training to avoid the need for data annotation. Before matching, the image ROI is restructured to adjust the input size of the network, further improving the descriptor extraction efficiency. Then, the multi-consistencies filter based on the correlation among descriptors, spatial positions, and texture features is designed to preserve stable matched pairs for accurate and robust registration results. Classification experimental results based on rotation stability show that the descriptors extracted by the proposed method have high classification accuracy, particularly under interference, such as noise, blur, and artifacts. Experimental results of clinical ultrasound image registration show that the proposed method has a lower registration error of 3.59 ± 1.15 mm compared with other methods. In addition, the descriptor extraction network proposed in this study has low training costs and high processing speed, further revealing the potential of the proposed method in clinical applications.},
  archive      = {J_PR},
  author       = {Yifan Wang and Tianyu Fu and Xinyu Chen and Jingfan Fan and Deqiang Xiao and Hong Song and Ping Liang and Jian Yang},
  doi          = {10.1016/j.patcog.2024.110324},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110324},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised local rotation-stable descriptors for 3D ultrasound registration using translation equivariant FCN},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IDC-net: Breast cancer classification network based on
BI-RADS 4. <em>PR</em>, <em>150</em>, 110323. (<a
href="https://doi.org/10.1016/j.patcog.2024.110323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the diagnosis of breast cancer, the 3 sub-categories 4a-4c of BI-RADS 4 are of great significance to doctors. However, low resolution of ultrasound image and high similarity between different category images pose great challenges to this task, which requires the network to be more capable of extracting image features. Therefore, in response to the efficient classification of BI-RADS 4a-4c in breast ultrasound images, we developed a lightweight classification network IDC Net, a neural network model combining the advantages of convolutional neural network(CNN) and CapsNet. In this model: Firstly, we proposed ID-Net based on CNN architecture and mainly constructed by ID block and DD block, which ensure the ID-Net deep and wide enough to extract sufficient local semantic information of image, and at the same time being lightweight. Secondly, we use the CapsNet to learn the position and posture information between the global features of the image, which makes up for the defects of CNN. Finally, two parallel paths of IDC Net and CapsNet are fused to enhance IDC Net&#39;s capability of feature extraction. To verify our method, experiments have been conducted on the breast ultrasound dataset of Yunnan cancer hospital and two public datasets. The classification results of our method have been compared with those obtained by five existing approaches. The experimental results show that the proposed method IDC Net has the highest Accuracy (98.54 %), Precision (98.54 %) and F1 score (98.54 %).},
  archive      = {J_PR},
  author       = {Sanli Yi and Ziyan Chen and Furong She and Tianwei Wang and Xuelian Yang and Dong Chen and Xiaomao Luo},
  doi          = {10.1016/j.patcog.2024.110323},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110323},
  shortjournal = {Pattern Recognition},
  title        = {IDC-net: Breast cancer classification network based on BI-RADS 4},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample diversity selection strategy based on label
distribution morphology for active label distribution learning.
<em>PR</em>, <em>150</em>, 110322. (<a
href="https://doi.org/10.1016/j.patcog.2024.110322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Labeling a sample in label distribution learning is highly expensive because it involves several labels at the same time and also requires assigning an exact value as the significance of each label. Therefore, active learning, which lowers the labeling cost by actively querying the labels of the most useful data, becomes especially critical for label distribution learning. Most of the known active query algorithms are for multi-label learning, and applying them directly to label distribution learning will lose some key supervisory information and thus fail to yield satisfactory experimental results. In this paper, we propose a sample diversity selection strategy based on the label distribution morphology, which can select diverse samples with different distribution morphologies from a large number of unlabeled samples for active querying. First, we use the feature space to construct a dissimilarity matrix that describes the pairwise dissimilarity among the unlabeled samples in order to pick a subset of samples that are representative of the unlabeled dataset. Second, using the information about the label distribution morphologies provided by the predicted labels of the unlabeled samples, we design a diversity loss score for each unlabeled sample. This score reflects the degree of difference between the sample and the labeled training sample . Finally, we use a convex optimization method to select valuable samples that are diverse from the labeled samples and represent the distribution of the unlabeled samples. The results of the comparison experiments demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Weiwei Li and Wei Qian and Lei Chen and Xiuyi Jia},
  doi          = {10.1016/j.patcog.2024.110322},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110322},
  shortjournal = {Pattern Recognition},
  title        = {Sample diversity selection strategy based on label distribution morphology for active label distribution learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Learning with incomplete labels of multisource datasets for
ECG classification. <em>PR</em>, <em>150</em>, 110321. (<a
href="https://doi.org/10.1016/j.patcog.2024.110321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shortage of annotated ECG data presents a significant impediment, hampering the overall generalization capabilities of machine learning models tailored for automated ECG classification. The collective integration of multisource datasets presents a potential remedy for this challenge. However, it is crucial to underscore that the mere addition of supplementary data does not automatically guarantee performance enhancement, given the unresolved challenges associated with multisource data. In this research, we address one such challenge, namely, the issue of incomplete labels arising from the diversity of annotations within multi-source ECG datasets. First, we identified three distinct types of label missing: dataset-related label missing, supertype missing, and subtype missing. To address the supertype missing effectively, we introduce a novel approach known as offline category mapping which leverages the hierarchical relationships inherent within the categories to recover the missing supertype labels. Additionally, two complementary strategies, referred to as prediction masking and online category mapping, are proposed to mitigating the adverse effects of subtype and dataset-related label missing on model optimization. These strategies enhance the model&#39;s ability to identify missing subtypes under conditions of weak supervision. These pioneering methodologies are integrated into a deep learning-based framework designed for multilabel ECG classification. The performance of our proposed framework is rigorously evaluated using realistic multi-source datasets obtained from the PhysioNet/CinC challenge 2020/2021. The proposed learning framework exhibits a notable improvement in macro-average precision, surpassing the corresponding baseline model by more than 25 % on the test datasets. As a result, this research study makes a substantial contribution to the field of ECG classification by addressing the critical issue of incomplete labels in multisource datasets, ultimately enhancing the generalization capabilities of machine learning models in this domain.},
  archive      = {J_PR},
  author       = {Qince Li and Yang Liu and Ze Zhang and Jun Liu and Yongfeng Yuan and Kuanquan Wang and Runnan He},
  doi          = {10.1016/j.patcog.2024.110321},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110321},
  shortjournal = {Pattern Recognition},
  title        = {Learning with incomplete labels of multisource datasets for ECG classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving generalized zero-shot learning via cluster-based
semantic disentangling representation. <em>PR</em>, <em>150</em>,
110320. (<a href="https://doi.org/10.1016/j.patcog.2024.110320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized Zero-Shot Learning (GZSL) aims to recognize both seen and unseen classes by training only the seen classes, in which the instances of unseen classes tend to be biased towards the seen class. In this paper, we propose a Cluster-based Semantic Disentangling Representation (CSDR) method to improve GZSL by alleviating the problems of domain shift and semantic gap . First, we cluster the seen data into multiple clusters , where the samples in each cluster belong to several original seen categories, so as to facilitate fine-grained semantic disentangling of visual feature vectors. Then, we introduce representation random swapping and contrastive learning based on the clustering results to realize the disentangling semantic representations of semantic-unspecific, class-shared, and class-unique. The fine-grained semantic disentangling representations show high intra-class similarity and inter-class discriminability, which improve the performance of GZSL by alleviating the problem of domain shift. Finally, we construct the visual-semantic embedding space by the variational auto-encoder and alignment module, which can bridge the semantic gap by generating strongly discriminative unseen class samples. Extensive experimental results on four public data sets prove that our method significantly outperforms state-of-the-art methods in generalized and conventional settings.},
  archive      = {J_PR},
  author       = {Yi Gao and Wentao Feng and Rong Xiao and Lihuo He and Zhenan He and Jiancheng Lv and Chenwei Tang},
  doi          = {10.1016/j.patcog.2024.110320},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110320},
  shortjournal = {Pattern Recognition},
  title        = {Improving generalized zero-shot learning via cluster-based semantic disentangling representation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task meta label correction for time series prediction.
<em>PR</em>, <em>150</em>, 110319. (<a
href="https://doi.org/10.1016/j.patcog.2024.110319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification faces two unavoidable problems. One is partial feature information and the other is poor label quality, which may affect model performance. To address the above issues, we create a label correction method to time series data with meta-learning under a multi-task framework. There are three main contributions. First, we train the label correction model with a two-branch neural network in the outer loop. While in the model-agnostic inner loop, we use pre-existing classification models in a multi-task way and jointly update the meta-knowledge so as to help us achieve adaptive labeling on complex time series. Second, we devise new data visualization methods for both image patterns of the historical data and data in the prediction horizon. Finally, we test our method with various financial datasets, including XOM, S&amp;P500, and SZ50. Results show that our method is more effective and accurate than some existing label correction techniques.},
  archive      = {J_PR},
  author       = {Luxuan Yang and Ting Gao and Wei Wei and Min Dai and Cheng Fang and Jinqiao Duan},
  doi          = {10.1016/j.patcog.2024.110319},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110319},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task meta label correction for time series prediction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCsearcher: Reaction center identification in retrosynthesis
via deep q-learning. <em>PR</em>, <em>150</em>, 110318. (<a
href="https://doi.org/10.1016/j.patcog.2024.110318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reaction center consists of atoms in the product with local properties that differ from those in the reactants. Previous studies focused on identifying the reaction center using semi-templated retrosynthesis methods, which are limited to single reaction center identification. In reality, however, many reaction centers involve multiple bonds or atoms, referred to as multiple reaction centers. This paper introduces RCsearcher, a unified framework that exploits the strengths of graph neural networks and deep reinforcement learning for identifying both single and multiple reaction centers. The key insight of the framework is that the single or multiple reaction center must be a node-induced subgraph of the molecular product graph. At each step, RCsearcher selects a node in the molecular product graph and adds it to the explored node-induced subgraph as an action. Comprehensive experiments demonstrate that RCsearcher consistently outperforms other baselines, and is able to identify reaction center patterns not present in the training set. Ablation experiments confirm the effectiveness of individual components of RCsearcher, including the beam search and the one-hop constraint of the action space.},
  archive      = {J_PR},
  author       = {Zixun Lan and Zuo Zeng and Binjie Hong and Zhenfu Liu and Fei Ma},
  doi          = {10.1016/j.patcog.2024.110318},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110318},
  shortjournal = {Pattern Recognition},
  title        = {RCsearcher: Reaction center identification in retrosynthesis via deep Q-learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Orientation-aware leg movement learning for action-driven
human motion prediction. <em>PR</em>, <em>150</em>, 110317. (<a
href="https://doi.org/10.1016/j.patcog.2024.110317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of action-driven human motion prediction aims to forecast future human motion based on the observed sequence while respecting the given action label. It requires modeling not only the stochasticity within human motion but the smooth yet realistic transition between multiple action labels. However, the fact that most datasets do not contain such transition data complicates this task. Existing work tackles this issue by learning a smoothness prior to simply promote smooth transitions, yet doing so can result in unnatural transitions especially when the history and predicted motions differ significantly in orientations. In this paper, we argue that valid human motion transitions should incorporate realistic leg movements to handle orientation changes, and cast it as an action-conditioned in-betweening (ACB) learning task to encourage transition naturalness. Because modeling all possible transitions is virtually unreasonable, our ACB is only performed on very few selected action classes with active gait motions, such as “ Walk ” or “ Run ”. Specifically, we follow a two-stage forecasting strategy by first employing the motion diffusion model to generate the target motion with a specified future action, and then producing the in-betweening to smoothly connect the observation and prediction to eventually address motion prediction. Our method is completely free from the labeled motion transition data during training. To show the robustness of our approach, we generalize our trained in-betweening learning model on one dataset to two unseen large-scale motion datasets to produce natural transitions. Extensive experimental evaluations on three benchmark datasets demonstrate that our method yields the state-of-the-art performance in terms of visual quality, prediction accuracy, and action faithfulness.},
  archive      = {J_PR},
  author       = {Chunzhi Gu and Chao Zhang and Shigeru Kuriyama},
  doi          = {10.1016/j.patcog.2024.110317},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110317},
  shortjournal = {Pattern Recognition},
  title        = {Orientation-aware leg movement learning for action-driven human motion prediction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematics-aware spatial-temporal feature transform for 3D
human pose estimation. <em>PR</em>, <em>150</em>, 110316. (<a
href="https://doi.org/10.1016/j.patcog.2024.110316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose estimation plays an important role in various human-machine interactive applications, but how to effectively extract and represent the kinematical features of human body structure in video has always been a challenge. This paper presents some inspiring observations on the human body properties that hold heuristic patterns of human poses: 1) There is distinct temporal coherence in any kind of human pose; 2) there exist evident spatial and temporal correlations among local joints even though the human is doing complex actions. According to the observed patterns, a locally structured feature encoder and a spatial–temporal feature transform are proposed for kinematics-aware feature extraction and enhancement. Unlike existing works directly projecting every bone joint to pose features without distinction, the proposed locally-structured feature encoder maps the local connection property of human body structure to kinematical features which are neural embeddings extracted from both local and global groups of human bone joints. Since the local and global bone-joint groups are pre-defined according to human body kinematics, the kinematical features are able to represent body kinematics. The kinematical features are then transformed by the proposed spatial–temporal feature transform to enhance the spatial and temporal correlations among human bone joints. The overall framework well promotes the representation of human body kinematics for 3D pose estimation. Extensive experimental results on commonly used datasets show that the mean per joint position error (MPJPE) is significantly reduced when compared with state-of-the-art methods under the same experimental condition. The improvement is expected to promote machines to better understand human poses for building superior human-centered automation systems.},
  archive      = {J_PR},
  author       = {Songlin Du and Zhiwei Yuan and Takeshi Ikenaga},
  doi          = {10.1016/j.patcog.2024.110316},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110316},
  shortjournal = {Pattern Recognition},
  title        = {Kinematics-aware spatial-temporal feature transform for 3D human pose estimation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MHSAN: Multi-view hierarchical self-attention network for 3D
shape recognition. <em>PR</em>, <em>150</em>, 110315. (<a
href="https://doi.org/10.1016/j.patcog.2024.110315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view learning has demonstrated promising performance for 3D shape recognition. However, existing multi-view methods usually focus on fusing multiple views and ignore the structural and discriminative information carried by 2D views. In this paper, we propose a multi-view hierarchical self-attention network (MHSAN) to explore the geometric and discriminative information from complex 2D views. Specifically, MHSAN consists of two self-attention networks. First, a global self-attention network is adopted to exploit the structure information by embedding position information of views. Then, the discriminative self-attention network learns discriminative information from the views with high classification scores. Through the proposed MHSAN, the geometric and discriminative information is condensed as the novel representation of 3D shapes. To validate the effectiveness of our proposed method, extensive experiments have been conducted on three 3D shape benchmarks. Experimental results demonstrate that our method is generally superior to the state-of-the-art methods in 3D shape classification and retrieval tasks.},
  archive      = {J_PR},
  author       = {Jiangzhong Cao and Lianggeng Yu and Bingo Wing-Kuen Ling and Zijie Yao and Qingyun Dai},
  doi          = {10.1016/j.patcog.2024.110315},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110315},
  shortjournal = {Pattern Recognition},
  title        = {MHSAN: Multi-view hierarchical self-attention network for 3D shape recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reading order detection in visually-rich documents with
multi-modal layout-aware relation prediction. <em>PR</em>, <em>150</em>,
110314. (<a href="https://doi.org/10.1016/j.patcog.2024.110314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reading order detection aims to arrange the text logically, which is essential in understanding visual documents. Current methods mostly model the problem as a sequence generation task, which use insufficient modalities information ignore the various reading habits under different document layouts, leading to the lack of robustness for some complex scenarios. To address these challenges, we present a novel approach with the Multi-Modal Layout-Aware Relation Prediction. It employs a straightforward yet highly effective task formulation for predicting the order relation between text instances. Our model leverages visual, semantic, and positional features, with the positional features being adaptively generated through a layout-aware position embedding module. Then, different modality features are enhanced via a two-staged position-guided multi-modal fusion module. Additionally, we introduce two novel loss functions, Degree Loss and Cycle Loss, to effectively impose network constraints at multiple levels. Our experimental results, conducted on three real-world datasets, demonstrate that our proposed method achieves a new state-of-the-art level of performance.},
  archive      = {J_PR},
  author       = {Liang Qiao and Can Li and Zhanzhan Cheng and Yunlu Xu and Yi Niu and Xi Li},
  doi          = {10.1016/j.patcog.2024.110314},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110314},
  shortjournal = {Pattern Recognition},
  title        = {Reading order detection in visually-rich documents with multi-modal layout-aware relation prediction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). HDR light field imaging of dynamic scenes: A learning-based
method and a benchmark dataset. <em>PR</em>, <em>150</em>, 110313. (<a
href="https://doi.org/10.1016/j.patcog.2024.110313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) imaging is an effective way to enable immersive applications. However, limited by the potential well capacity of the image sensor, the acquired LF images suffer from low dynamic range and are thus prone to under-exposure or over-exposure. High dynamic range (HDR) LF imaging is an efficacious avenue to improve the LF imaging&#39;s dynamic range. Unfortunately, for dynamic scenes, existing methods are inclined to produce ghosting artifacts and lose details in the saturated regions, while potentially damaging the parallax structure of generated HDR LF images. To address the above challenges, in this paper, we propose a new ghost-free HDR LF imaging method based on a deformable aggregation and angular embedding network. Specifically, considering the four-dimensional geometric structure of the LF image, a deformable alignment module is first designed to handle dynamic regions in the spatial domain, and then the aligned spatial features are fully fused through an aggregation operation. Subsequently, an angular embedding module is constructed to explore angular information to enhance the aggregated spatial features. Based on this, the above two modules are cascaded in a multi-scale manner to achieve multi-level feature extraction and enhance the feature representation ability. Finally, a decoder is leveraged to recover the ghost-free HDR LF image from the enhanced multi-scale features. For performance evaluation, this paper establishes a large-scale benchmark dataset with multi-exposure inputs and ground truth images. Extensive experimental results show that the proposed method generates visually pleasing HDR LF images while preserving accurate angular consistency. Moreover, the proposed method surpasses the state-of-the-art methods in both quantitative and qualitative comparisons. The code and dataset will be available at https://github.com/YeyaoChen/HDRLFI .},
  archive      = {J_PR},
  author       = {Yeyao Chen and Gangyi Jiang and Mei Yu and Chongchong Jin and Haiyong Xu and Yo-Sung Ho},
  doi          = {10.1016/j.patcog.2024.110313},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110313},
  shortjournal = {Pattern Recognition},
  title        = {HDR light field imaging of dynamic scenes: A learning-based method and a benchmark dataset},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep-NFA: A deep a contrario framework for tiny object
detection. <em>PR</em>, <em>150</em>, 110312. (<a
href="https://doi.org/10.1016/j.patcog.2024.110312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of tiny objects is a challenging task in computer vision . Conventional object detection methods have difficulties in finding the balance between high detection rate and low false alarm rate . In the literature, some methods have addressed this issue by enhancing the feature map responses for small objects, but without guaranteeing robustness with respect to the number of false alarms induced by background elements. To tackle this problem, we introduce an a contrario decision criterion into the learning process to take into account the unexpectedness of tiny objects. This statistic criterion enhances the feature map responses while controlling the number of false alarms (NFA) and can be integrated as an add-on into any semantic segmentation neural network . Our add-on NFA module not only allows us to obtain competitive results for small target, road crack and ship detection tasks respectively, but also leads to more robust and interpretable results.},
  archive      = {J_PR},
  author       = {Alina Ciocarlan and Sylvie Le Hégarat-Mascle and Sidonie Lefebvre and Arnaud Woiselle},
  doi          = {10.1016/j.patcog.2024.110312},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110312},
  shortjournal = {Pattern Recognition},
  title        = {Deep-NFA: A deep a contrario framework for tiny object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pose-robust personalized facial expression recognition
through unsupervised multi-source domain adaptation. <em>PR</em>,
<em>150</em>, 110311. (<a
href="https://doi.org/10.1016/j.patcog.2024.110311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-robust personalized facial expression recognition is rather challenging, as facial expressions are subject-related and pose-dependent. Multi-source domain adaptation tries to leverage knowledge from multiple source domains to boost the performance of the target domain. For pose-robust personalized facial expression recognition, the images of the source domain are from multiple sources since the images are under different poses. Thus, in this paper, we propose a novel unsupervised multi-source domain adaptation framework for pose-robust personalized facial expression recognition. The proposed framework consists of five components: a source encoder, a target encoder, an expression classifier , a view discriminator , and a domain discriminator . The source encoder and target encoder learn facial representations from facial images in the training and testing sets, respectively. The expression classifier recognizes expressions from the learned representations. The view discriminator classifies poses. The domain discriminator distinguishes the learned representations of the source domain from those of the target domain. The source encoder works cooperatively with the expression classifier and adversarially with the view discriminator. The target encoder aims to learn domain robust representations and fool the domain discriminator, while the domain discriminator tries to distinguish between the source and target domains. Through adversarial learning, the distribution of the learned representations from the source domain converges to that from the target domain. Thus, the feature representation extracted by the target encoder is pose-invariant and target subject-specific. Experimental results demonstrate the superiority of the proposed method compared to related works.},
  archive      = {J_PR},
  author       = {Shangfei Wang and Yanan Chang and Qiong Li and Can Wang and Guoming Li and Meng Mao},
  doi          = {10.1016/j.patcog.2024.110311},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110311},
  shortjournal = {Pattern Recognition},
  title        = {Pose-robust personalized facial expression recognition through unsupervised multi-source domain adaptation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularization and optimization in model-based clustering.
<em>PR</em>, <em>150</em>, 110310. (<a
href="https://doi.org/10.1016/j.patcog.2024.110310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their conceptual simplicity, k-means algorithm variants have been extensively used for unsupervised cluster analysis. However, one main shortcoming of these algorithms is that they essentially fit a mixture of identical spherical Gaussians to data that vastly deviates from such a distribution. In comparison, general Gaussian Mixture Models (GMMs) can fit richer structures but require estimating a quadratic number of parameters per cluster to represent the covariance matrices. This poses two main issues: (i) the underlying optimization problems are challenging due to their larger number of local minima, and (ii) their solutions can overfit the data. In this work, we design search strategies that circumvent both issues. We develop more effective optimization algorithms for general GMMs, and we combine these algorithms with regularization strategies that avoid overfitting. Through extensive computational analyses, we observe that optimization or regularization in isolation does not substantially improve cluster recovery. However, combining these techniques permits a completely new level of performance previously unachieved by k-means algorithm variants, unraveling vastly different cluster structures. These results shed new light on the current status quo between GMM and k-means methods and suggest the more frequent use of general GMMs for data exploration. To facilitate such applications, we provide open-source code as well as Julia packages ( UnsupervisedClustering.jl and RegularizedCovarianceMatrices.jl ) implementing the proposed techniques.},
  archive      = {J_PR},
  author       = {Raphael Araujo Sampaio and Joaquim Dias Garcia and Marcus Poggi and Thibaut Vidal},
  doi          = {10.1016/j.patcog.2024.110310},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110310},
  shortjournal = {Pattern Recognition},
  title        = {Regularization and optimization in model-based clustering},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable AI for time series via virtual inspection
layers. <em>PR</em>, <em>150</em>, 110309. (<a
href="https://doi.org/10.1016/j.patcog.2024.110309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of eXplainable Artificial Intelligence (XAI) has witnessed significant advancements in recent years. However, the majority of progress has been concentrated in the domains of computer vision and natural language processing . For time series data , where the input itself is often not interpretable, dedicated XAI research is scarce. In this work, we put forward a virtual inspection layer for transforming the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods. In this way, we extend the applicability of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. In this work, we focus on the Fourier Transform which, is prominently applied in the preprocessing of time series, with Layer-wise Relevance Propagation (LRP) and refer to our method as DFT-LRP . We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and medical data. We showcase how DFT-LRP reveals differences in the classification strategies of models trained in different domains (e.g., time vs. frequency domain) or helps to discover how models act on spurious correlations in the data.},
  archive      = {J_PR},
  author       = {Johanna Vielhaben and Sebastian Lapuschkin and Grégoire Montavon and Wojciech Samek},
  doi          = {10.1016/j.patcog.2024.110309},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110309},
  shortjournal = {Pattern Recognition},
  title        = {Explainable AI for time series via virtual inspection layers},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust spectral embedded bilateral orthogonal concept
factorization for clustering. <em>PR</em>, <em>150</em>, 110308. (<a
href="https://doi.org/10.1016/j.patcog.2024.110308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept factorization (CF), unlike nonnegative matrix factorization (NMF), can handle data with negative values by approximating the original data with two low-dimensional nonnegative matrices and itself. Nevertheless, existing CF-based methods continue to suffer from the two issues specified as follows: (1) Their effectiveness is reduced by the high degree of factorization freedom and the two-stage mismatch between factorization and category acquisition, and (2) their robustness drops significantly when dealing with complex noise. In response to the aforementioned issues, we propose a robust spectral-embedded bilateral orthogonal concept factorization (RSOCF) model for clustering. It constrains the factor matrices as orthogonal matrices to decrease the freedom and obtain samples’ categories directly after factorization, which can significantly improve clustering effectiveness. Moreover, correntropy is introduced into RSOCF to improve its robustness to complex noise. To optimize the non-convex RSOCF model, a half-quadratic-based algorithm is devised. Numerous experiments demonstrate that RSOCF surpasses other state-of-the-art methods in terms of clustering effectiveness and robustness.},
  archive      = {J_PR},
  author       = {Ben Yang and Jinghan Wu and Yu Zhou and Xuetao Zhang and Zhiping Lin and Feiping Nie and Badong Chen},
  doi          = {10.1016/j.patcog.2024.110308},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110308},
  shortjournal = {Pattern Recognition},
  title        = {Robust spectral embedded bilateral orthogonal concept factorization for clustering},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kernel correlation–dissimilarity for multiple kernel k-means
clustering. <em>PR</em>, <em>150</em>, 110307. (<a
href="https://doi.org/10.1016/j.patcog.2024.110307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to extract non-linear information and achieve optimal clustering by optimizing base kernel matrices. Current methods enhance information diversity and reduce redundancy by exploiting interdependencies among multiple kernels based on correlations or dissimilarities. Nevertheless, relying solely on a single metric, such as correlation or dissimilarity, to define kernel relationships introduces bias and incomplete characterization. Consequently, this limitation hinders efficient information extraction, ultimately compromising clustering performance. To tackle this challenge, we introduce a novel method that systematically integrates both kernel correlation and dissimilarity. Our approach comprehensively captures kernel relationships, facilitating more efficient classification information extraction and improving clustering performance. By emphasizing the coherence between kernel correlation and dissimilarity, our method offers a more objective and transparent strategy for extracting non-linear information and significantly improving clustering precision, supported by theoretical rationale. We assess the performance of our algorithm on 13 challenging benchmark datasets, demonstrating its superiority over contemporary state-of-the-art MKKM techniques.},
  archive      = {J_PR},
  author       = {Rina Su and Yu Guo and Caiying Wu and Qiyu Jin and Tieyong Zeng},
  doi          = {10.1016/j.patcog.2024.110307},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110307},
  shortjournal = {Pattern Recognition},
  title        = {Kernel correlation–dissimilarity for multiple kernel k-means clustering},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised incremental estimation of gaussian mixture
models with 1D split moves. <em>PR</em>, <em>150</em>, 110306. (<a
href="https://doi.org/10.1016/j.patcog.2024.110306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new type of split rule for incremental estimation of Gaussian Mixture Models with model selection. Split-based methods typically start with a mixture composed of a single component representing all data, and successively split and optimize components for a given model selection criterion. These algorithms are typically faster than alternatives, but depend critically on the component splitting method, since a good split rule promotes a faster convergence of the mixture optimization phase. We propose a new efficient and robust split rule that projects mixture components onto a 1D subspace and fits a two-component model to the projected data with the Expectation Maximization algorithm. The proposed approach is fast and robust to parameter tuning, being the ideal choice for applications that favor speed while still maintaining an acceptable accuracy. We illustrate the validity of the method through a series of experiments on synthetic and real datasets comparing the proposed method to alternatives of the state-of-the-art in terms of efficiency, accuracy, and sensitivity to parameter tuning.},
  archive      = {J_PR},
  author       = {Nicola Greggio and Alexandre Bernardino},
  doi          = {10.1016/j.patcog.2024.110306},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110306},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised incremental estimation of gaussian mixture models with 1D split moves},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transformer-CNN parallel network for image guided depth
completion. <em>PR</em>, <em>150</em>, 110305. (<a
href="https://doi.org/10.1016/j.patcog.2024.110305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image guided depth completion aims to predict a dense depth map from sparse depth measurements and the corresponding single color image. However, most state-of-the-art methods only rely on convolutional neural network (CNN) or transformer. In this paper, we propose a transformer-CNN parallel network (TCPNet) to integrate the advantages of CNN in local detail recovery and transformer in long-range semantic modeling. Specifically, our CNN branch adopts dense connection to strengthen feature propagation. Since the common transformer computes self-attention based on all the tokens in the window, no matter if they are relevant or not, this will inevitably introduce interferences and noises. To improve the self-attention accuracy, we propose a correlation-based transformer to only allow nearest neighbor tokens to participate in the self-attention computation. We also design a multi-scale conditional random field (CRF) module to implement multi-scale high-dimensional filtering for depth refinement. The comprehensive experimental results on KITTI and NYUv2 demonstrate that our method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Tao Li and Xiucheng Dong and Jie Lin and Yonghong Peng},
  doi          = {10.1016/j.patcog.2024.110305},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110305},
  shortjournal = {Pattern Recognition},
  title        = {A transformer-CNN parallel network for image guided depth completion},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional feature learning network for RGB-d salient
object detection. <em>PR</em>, <em>150</em>, 110304. (<a
href="https://doi.org/10.1016/j.patcog.2024.110304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-D salient object detection aims to perform the pixel-wise localization of salient objects from both RGB and depth images, whose challenge mainly comes from how to learn complementary features from each modality. Existing works often use increasingly large models for performance enhancement, which need large memory and time consumption in practice. In this paper, we propose a simple yet effective B idirectional F eature L earning Net work (BFLNet) for RGB-D salient object detection under limited memory and time conditions. To achieve accurate performance with lightweight backbone networks, an effective B idirectional F eature F usion (BFF) module is designed to merge features from both RGB and depth streams, in which the cross-modal fusions and cross-scale fusions are jointly conducted to fuse the immediate features in multiple scales and multiple modals. What is more, a simple D ual C onsistency L oss (DCL) function is designed to prompt cross-modal fusion by keeping the consistency between cross-modal target predictions. Extensive experiments on four benchmark datasets demonstrate that our method has achieved the state-of-the-art performance with high efficiency in RGB-D salient object detection. Code will be available at https://github.com/nightsky-nostar/BFLNet .},
  archive      = {J_PR},
  author       = {Ye Niu and Sanping Zhou and Yonghao Dong and Le Wang and Jinjun Wang and Nanning Zheng},
  doi          = {10.1016/j.patcog.2024.110304},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110304},
  shortjournal = {Pattern Recognition},
  title        = {Bidirectional feature learning network for RGB-D salient object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive assessment of triclustering algorithms for
three-way temporal data analysis. <em>PR</em>, <em>150</em>, 110303. (<a
href="https://doi.org/10.1016/j.patcog.2024.110303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of temporal data has gained increasing attention in recent years, aiming to identify patterns and trends that change over time. Temporal triclustering is a promising approach for this purpose, as it allows for the simultaneous clustering of three dimensions of data: objects, attributes, and time. In this work, we present a comparative study and experimental evaluation of state-of-the-art temporal triclustering algorithms. Our study provides a comprehensive quantitative assessment of several triclustering algorithms to unravel their strengths and limitations. To this end, we consider synthetic data with varying sizes and regularities, where true solutions are planted with different coherence and quality criteria, in order to assess the algorithms’ performance in datasets with diverse characteristics and assess their capacity to retrieve specific types of hidden patterns. This provides a more comprehensive evaluation of the algorithms and allows for a better understanding of their capabilities and limitations. This study is the first to compare state-of-the-art triclustering algorithms inherently prepared to deal with temporal data and provides new benchmark results for the Temporal Triclustering task. Our results on the algorithms’ performance can guide practitioners in selecting the most appropriate algorithm for their specific application.},
  archive      = {J_PR},
  author       = {Diogo F. Soares and Rui Henriques and Sara C. Madeira},
  doi          = {10.1016/j.patcog.2024.110303},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110303},
  shortjournal = {Pattern Recognition},
  title        = {Comprehensive assessment of triclustering algorithms for three-way temporal data analysis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Cross-frame feature-saliency mutual reinforcing for weakly
supervised video salient object detection. <em>PR</em>, <em>150</em>,
110302. (<a href="https://doi.org/10.1016/j.patcog.2024.110302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scribble annotations have recently become popular in video salient object detection . Previous methods only focus on utilizing shallow feature consistency for more integral predictions. However, there is potential for consistency between cross-frame deep features to be used to help regularize saliency predictions better. Besides, we have observed that leveraging saliency predictions as pseudo-supervision signals yields notable improvements in extracting both intra-frame and cross-frame deep features. This, in turn, leads to more precise and detailed object structural information. Thus, we propose a cross-frame feature-saliency mutual reinforcing training process to assist scribble annotations for integral video saliency predictions. Specifically, we design a cross-frame feature regularization head, which leverages intra-frame and cross-frame deep feature consistency to regularize saliency predictions as auxiliary supervision. Then, to help obtain more accurate feature consistency, we design a cross-frame saliency regularization head, where predicted saliency values are used as pseudo-supervision signals to acquire better feature consistency. In this way, our cross-frame feature and saliency regularization heads can benefit from each other to help the network learn more accurately. Extensive experiments show that our method can achieve better performances than the previous best methods. The project is available at https://github.com/muchengxue0911/CFMR.},
  archive      = {J_PR},
  author       = {Jian Wang and Siyue Yu and Bingfeng Zhang and Xinqiao Zhao and Ángel F. García-Fernández and Eng Gee Lim and Jimin Xiao},
  doi          = {10.1016/j.patcog.2024.110302},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110302},
  shortjournal = {Pattern Recognition},
  title        = {Cross-frame feature-saliency mutual reinforcing for weakly supervised video salient object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging spatio-temporal features using graph neural
networks for human activity recognition. <em>PR</em>, <em>150</em>,
110301. (<a href="https://doi.org/10.1016/j.patcog.2024.110301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised human activity recognition (HAR) algorithms working on motion capture (mocap) data often use spatial information and neglect the activity-specific information contained in the temporal sequences. In this work, we propose a new unsupervised algorithm for HAR from mocap data to leverage both spatial and temporal information embedded in activity sequences. For this, we employ a shallow graph neural network (GNN) comprising a graph convolutional network and a gated recurrent unit to aggregate the spatial and temporal features of the mocap sequences, respectively. Moreover, we encode the transformations of the human body through log-regularized kernel covariance descriptors linked to the trajectory movement maps of mocap frames. These descriptors are then fused with the GNN features for downstream activity recognition tasks. Finally, HAR is performed by a new unsupervised algorithm using a neighborhood Laplacian regularizer and a normalized dictionary learning approach . The generalizability of the proposed model is validated by training the GNN on a public dataset and testing on the other datasets. The performance of the proposed model is evaluated using six publicly available human mocap datasets. Compared to existing approaches, the proposed model improves activity recognition consistently by 12%–30% across different datasets.},
  archive      = {J_PR},
  author       = {M.S. Subodh Raj and Sudhish N. George and Kiran Raja},
  doi          = {10.1016/j.patcog.2024.110301},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110301},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging spatio-temporal features using graph neural networks for human activity recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local kernels based graph learning for multiple kernel
clustering. <em>PR</em>, <em>150</em>, 110300. (<a
href="https://doi.org/10.1016/j.patcog.2024.110300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple kernel clustering (MKC) has been extensively studied in recent years. The focus of MKC is how to explore the information of base kernels. Although existing methods have promising leaning abilities, they ignore the intrinsic local structure contained in base kernels, which may negatively affect their performances. To address the above problem, a novel method, termed as consensus graph learning based on local kernels (CGLLK), is introduced. CGLLK is based on the partitions extracted by kernel k-means. Specifically, we first design a simple yet effective scheme to construct the local kernels of base kernels and then a consensus graph is applied to capture the complementary information contained in the extracted partitions of local kernels. CGLLK also considers the prior knowledge existing in base kernels. Since the partitions of local kernels and the learning stage of the consensus graph contain useful information for each other, the two processes are optimized jointly. Extensive experiments on some popular datasets are carried out to verify the effectiveness of the proposed method. Experimental results illustrate that CGLLK is much more competitive than the state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Zheng Liu and Shiluo Huang and Wei Jin and Ying Mu},
  doi          = {10.1016/j.patcog.2024.110300},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110300},
  shortjournal = {Pattern Recognition},
  title        = {Local kernels based graph learning for multiple kernel clustering},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast multi-network k-dependence bayesian classifier for
continuous features. <em>PR</em>, <em>150</em>, 110299. (<a
href="https://doi.org/10.1016/j.patcog.2024.110299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the Bayesian network classifiers widely used in the classification is K-dependence Bayesian (KDB). However, most of the KDB classifiers build a single network on a class variable without considering dependencies between features in each class. Moreover, many KDB classifiers need the discretization process to handle continuous features. This paper aims to propose a fast Multi-Network K-Dependence Bayesian (MNKDB) classifier for continuous features. According to this aim, we propose a non-parametric approach that efficiently identifies dependencies between continuous features in each class with a low computational cost and without discretizing continuous features. The results indicate that the MNKDB classifier is more accurate than the state-of-the-art KDB classifiers, especially for datasets with more than three classes. The MNKDB classifier not only decreases the classification time but also deals with continuous variables without discretizing them. The results for K=2 show that the MNKDB classifier is 36.5, 31.8, and 14.2 times faster and 4.13%, 5.15%, and 5.48% more accurate than the state-of-the-art FKDB (Flexible KDB), KMM-KDB (Kernel Mixture Model based on KDB), and SKDB (Scalable KDB) classifiers, respectively.},
  archive      = {J_PR},
  author       = {Imaneh Khodayari-Samghabadi and Leyli Mohammad-Khanli and Jafar Tanha},
  doi          = {10.1016/j.patcog.2024.110299},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110299},
  shortjournal = {Pattern Recognition},
  title        = {A fast multi-network K-dependence bayesian classifier for continuous features},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast and explainable clustering based on sorting.
<em>PR</em>, <em>150</em>, 110298. (<a
href="https://doi.org/10.1016/j.patcog.2024.110298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a fast and explainable clustering method called CLASSIX. It consists of two phases, namely a greedy aggregation phase of the sorted data into groups of nearby data points, followed by the merging of groups into clusters. The algorithm is controlled by two scalar parameters, namely a distance parameter for the aggregation and another parameter controlling the minimal cluster size. Extensive experiments are conducted to give a comprehensive evaluation of the clustering performance on synthetic and real-world datasets, with various cluster shapes and low to high feature dimensionality. Our experiments demonstrate that CLASSIX competes with state-of-the-art clustering algorithms. The algorithm has linear space complexity and achieves near linear time complexity on a wide range of problems. Its inherent simplicity allows for the generation of intuitive explanations of the computed clusters.},
  archive      = {J_PR},
  author       = {Xinye Chen and Stefan Güttel},
  doi          = {10.1016/j.patcog.2024.110298},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110298},
  shortjournal = {Pattern Recognition},
  title        = {Fast and explainable clustering based on sorting},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local topology similarity guided probabilistic sampling for
mismatch removal. <em>PR</em>, <em>150</em>, 110293. (<a
href="https://doi.org/10.1016/j.patcog.2024.110293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature point matching between two images is a fundamental and important process in machine vision. In many cases, mismatches are inevitable, and removing mismatches is an indispensable task. The existing methods attempt to find comprehensive constraints or sampling model to achieve better performance, which results in the increasingly complexity and may cause the weakness of the generality and scalability. To address this issue, a method called Local Topology similarity guided probabilistic Sampling consensus (LTS) is proposed. It constructs a topological network, then quantifies the mismatch probability in a concise approach based on comparing the topological relationship with neighbourhoods. Then, it detects and removes the mismatches by sampling guided by the mismatch probability. Compared with the state-of-the-art methods, LTS has an excellent performance in accuracy and robustness.},
  archive      = {J_PR},
  author       = {Zaixing He and Chentao Shen and Xinyue Zhao},
  doi          = {10.1016/j.patcog.2024.110293},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110293},
  shortjournal = {Pattern Recognition},
  title        = {Local topology similarity guided probabilistic sampling for mismatch removal},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCPNet: Multi-space color correction and features prior
fusion for single-image dehazing in non-homogeneous haze scenarios.
<em>PR</em>, <em>150</em>, 110290. (<a
href="https://doi.org/10.1016/j.patcog.2024.110290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When capturing images in non-homogeneous haze conditions, the degree of haze impact varies across the scene, and the information in certain parts of the scene are lost, making the scene nearly invisible. However, most existing dehazing convolutional neural networks (CNNs) are designed for homogeneous haze and do not consider the challenge of feature extraction arising from non-homogeneous haze distribution. Moreover, these networks primarily rely on the RGB color space for feature extraction, often fail to extract color and detail feature effectively, resulting in color distortion and loss of details in the output. To tackle this problem, we introduce image priors in the YCbCr and HSV color spaces, proposing a novel Multiple Color Space Prior Network (MCPNet) to enhance the dehazing performance specifically for non-homogeneous hazy images, while simultaneously correcting the color, preserving the visual quality of the output. Leveraging image priors, we designed two parallel sub-networks to extract color and detail features from the YCbCr and HSV color spaces. Moreover, to capitalize on these features and incorporate them effectively into the dehazed image, we introduce a Comprehensive Fusion Module (CFM). This module judiciously takes into account both the fusion of multiscale features and the interrelation among channels to optimize feature fusion. By employing a dual network architecture coupled with the CFM, our model proficiently amalgamates and exploits the mined features, accurately restoring the color and detail information of the image, especially for images containing non-homogeneous haze. Extensive experimental highlight the effectiveness of our model in addressing homogeneous and non-homogeneous hazy images, concurrently preserving the visual appeal of the dehazed outcomes. When compared with other SOTA models, our MCPNet demonstrates superior results in dehazing.},
  archive      = {J_PR},
  author       = {Zhiyu Lyu and Yan Chen and Yimin Hou},
  doi          = {10.1016/j.patcog.2024.110290},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110290},
  shortjournal = {Pattern Recognition},
  title        = {MCPNet: Multi-space color correction and features prior fusion for single-image dehazing in non-homogeneous haze scenarios},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinematics modeling network for video-based human pose
estimation. <em>PR</em>, <em>150</em>, 110287. (<a
href="https://doi.org/10.1016/j.patcog.2024.110287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating human poses from videos is critical in human–computer interaction. Joints cooperate rather than move independently during human movement. There are both spatial and temporal correlations between joints. Despite the positive results of previous approaches, most of them focus on modeling the spatial correlation between joints while only straightforwardly integrating features along the temporal dimension, which ignores the temporal correlation between joints. In this work, we propose a plug-and-play kinematics modeling module (KMM) to explicitly model temporal correlations between joints across different frames by calculating their temporal similarity. In this way, KMM can capture motion cues of the current joint relative to all joints in different time. Besides, we formulate video-based human pose estimation as a Markov Decision Process and design a novel kinematics modeling network (KIMNet) to simulate the Markov Chain , allowing KIMNet to locate joints recursively. Our approach achieves state-of-the-art results on two challenging benchmarks. In particular, KIMNet shows robustness to the occlusion. Code will be released at https://github.com/YHDang/KIMNet .},
  archive      = {J_PR},
  author       = {Yonghao Dang and Jianqin Yin and Shaojie Zhang and Jiping Liu and Yanzhu Hu},
  doi          = {10.1016/j.patcog.2024.110287},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110287},
  shortjournal = {Pattern Recognition},
  title        = {Kinematics modeling network for video-based human pose estimation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale continual learning for ancient chinese character
recognition. <em>PR</em>, <em>150</em>, 110283. (<a
href="https://doi.org/10.1016/j.patcog.2024.110283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancient Chinese character recognition is a challenging problem in the field of pattern recognition. It is difficult to collect all character classes during the training stage due to the numerous classes of ancient Chinese characters and the likelihood of discovering new characters over time. A solution to address this problem is continual learning. However, most continual learning methods are not well-suited for large-scale applications, making them insufficient for solving the problem of ancient Chinese character recognition. Although saving raw data for old classes is a good approach for continual learning to address large-scale problems, it is often infeasible due to the lack of data accessibility in reality. To solve these problems, we propose a large-scale continual learning framework based on the convolutional prototype network (CPN), which does not save raw data for old classes. In this paper, several basic strategies have been proposed for the initial training stage to enhance the feature extraction ability and robustness of the network, which can improve the performance of the model in continual learning. In addition, we propose two practical methods in varying feature space (parameters of feature extractor are changeable) and fixed feature space (parameters of feature extractor are fixed), which enable the model to carry out large-scale continual learning. The proposed method does not save the raw data of old classes and enables simultaneous classification of all existing classes without knowing the incremental batch number. Experiments on the CASIA-AHCDB dataset with 5000 character classes demonstrate the effectiveness and superiority of the proposed method.},
  archive      = {J_PR},
  author       = {Yue Xu and Xu-Yao Zhang and Zhaoxiang Zhang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2024.110283},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110283},
  shortjournal = {Pattern Recognition},
  title        = {Large-scale continual learning for ancient chinese character recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-labeling based modified gated graph network for
few-shot learning. <em>PR</em>, <em>150</em>, 110264. (<a
href="https://doi.org/10.1016/j.patcog.2024.110264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate determination of similarity between samples is fundamental and critical for graph network based few-shot learning tasks. Previous approaches typically employ convolutional neural networks to obtain relations between nodes. However, these networks are not adept at handling node features in vector form. To overcome this limitation, we proposed a modified gated graph network (MGGN) that uniquely integrates graph networks and modified gated recurrent units (M-GRU) for few-shot classification. The introduced M-GRU mitigates the loss of label information from the initial graph and reduces computational complexity . The MGGN contains two modules that alternately update node and edge features. The node update module leverages a gating mechanism to integrate edge features into node update weights, fostering a learnable node aggregation process. The edge update component perceives the trend in edge feature changes and establishes long-term dependencies. Experimental results on two benchmark datasets demonstrate that our MGGN achieves comparable performance to state-of-the-art methods. The code is available at https://github.com/zpx16900/MGGN .},
  archive      = {J_PR},
  author       = {Peixiao Zheng and Xin Guo and Enqing Chen and Lin Qi and Ling Guan},
  doi          = {10.1016/j.patcog.2024.110264},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110264},
  shortjournal = {Pattern Recognition},
  title        = {Edge-labeling based modified gated graph network for few-shot learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal progressive inward-outward aggregation
network for skeleton-based action recognition. <em>PR</em>,
<em>150</em>, 110262. (<a
href="https://doi.org/10.1016/j.patcog.2024.110262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works have realized that spatio-temporal entanglement features can not be ignored in skeleton-based motion recognition tasks, then they have not broken away from the barriers of traditional GCN (The entanglement feature is still modeled by the extended single-frame adjacency matrix). We introduce a new joint-correlations determination mechanism that uses a non-linear transformation of the distance between joints in multiple frames to construct the connection relationship. The proposed method results in improved accuracy while significantly reducing the number of parameters. Meanwhile, recent works have alleviated the problem of most actions being only related to the dynamic characteristics of local joints by aggregating features of different parts of the human body in parallel, while interacting with different features still remains at a lower level of concatenation or addition. We propose a progressive inward-outward structure (PIS) that allows joint features corresponding to the action to be extracted while taking into account the lightweight link between this part of the joints and the rest. Integrating the above two designs, we propose a Spatiotemporal Progressive Inward-Outward Aggregation Network (SPIANet) to model the complex spatiotemporal entanglement between joints in the process of human motion , which is validated on three public datasets (NTU-RGB+D60, NTU-RGB+D120, and UESTC varying-view) and outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xinpeng Yin and Jianqi Zhong and Deliang Lian and Wenming Cao},
  doi          = {10.1016/j.patcog.2024.110262},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110262},
  shortjournal = {Pattern Recognition},
  title        = {Spatiotemporal progressive inward-outward aggregation network for skeleton-based action recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EmoComicNet: A multi-task model for comic emotion
recognition. <em>PR</em>, <em>150</em>, 110261. (<a
href="https://doi.org/10.1016/j.patcog.2024.110261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emotion and sentiment associated with comic scenes can provide potential information for inferring the context of comic stories, which is an essential pre-requisite for developing comics’ automatic content understanding tools. Here, we address this open area of comic research by exploiting the multi-modal nature of comics. The general assumptions for multi-modal sentiment analysis methods are that both image and text modalities are always present at the test phase. However, this assumption is not always satisfied for comics since comic characters’ facial expressions, gestures, etc., are not always clearly visible. Also, the dialogues between comic characters are often challenging to comprehend the underlying context. To deal with these constraints of comic emotion analysis , we propose a multi-task-based framework, namely EmoComicNet , to fuse multi-modal information (i.e., both image and text) if it is available. However, the proposed EmoComicNet is designed to perform even when any modality is weak or completely missing. The proposed method potentially improves the overall performance. Besides, EmoComicNet can also deal with the problem of weak or absent modality during the training phase.},
  archive      = {J_PR},
  author       = {Arpita Dutta and Samit Biswas and Amit Kumar Das},
  doi          = {10.1016/j.patcog.2024.110261},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110261},
  shortjournal = {Pattern Recognition},
  title        = {EmoComicNet: A multi-task model for comic emotion recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAFED: Towards robust ensemble models via latent feature
diversification. <em>PR</em>, <em>150</em>, 110225. (<a
href="https://doi.org/10.1016/j.patcog.2023.110225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples pose a significant challenge to the security of deep neural networks (DNNs). In order to defend against malicious attacks , adversarial training forces DNNs to learn more robust features by suppressing generalizable but non-robust features, which boosts the robustness while suffering from significant accuracy drops on clean images. Ensemble training, on the other hand, trains multiple sub-models to predict data for improved robustness and still achieves desirable accuracy on clean data. Despite these efforts, previous ensemble methods are still susceptible to attacks and fail to increase model diversity as the size of the ensemble group increases. In this work, we revisit the model diversity from the perspective of data and discover that high similarity between training batches decreases feature diversity and weakens ensemble robustness. To this end, we propose La tent Fe ature D iversification (LAFED) , which reconstructs training sets with diverse features during the optimization, enhancing the overall robustness of an ensemble. For each sub-model, LAFED treats the vulnerability extracted from other sub-models as raw data, which is then combined with round-changed weights with a stochastic manner in the latent space. This results in the formation of new features, remarkably reducing the similarity of learned representations between the sub-models. Furthermore, LAFED enhances feature diversity within the ensemble model by utilizing hierarchical smoothed labels. Extensive experiments illustrate that LAFED significantly improves diversity among sub-models and enhances robustness against adversarial attacks compared to current methods. The code is publicly available at https://github.com/zhuangwz/LAFED .},
  archive      = {J_PR},
  author       = {Wenzi Zhuang and Lifeng Huang and Chengying Gao and Ning Liu},
  doi          = {10.1016/j.patcog.2023.110225},
  journal      = {Pattern Recognition},
  month        = {6},
  pages        = {110225},
  shortjournal = {Pattern Recognition},
  title        = {LAFED: Towards robust ensemble models via latent feature diversification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source domain adaptation with mixture of joint
distributions. <em>PR</em>, <em>149</em>, 110295. (<a
href="https://doi.org/10.1016/j.patcog.2024.110295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Multi-Source Domain Adaptation (MSDA) is to train a model ( e.g. , neural network) with minimal target loss, utilizing training data from multiple source domains (source joint distributions) and a target domain (target joint distribution). The challenge in this problem is that the multiple source joint distributions are different from the target joint distribution. In this paper, we develop a theory that shows a neural network’s target loss is upper bounded by both its source mixture loss ( i.e. , the loss concerning the source mixture joint distribution) and the Pearson χ 2 χ2 divergence between the source mixture joint distribution and the target joint distribution. Here, the source mixture joint distribution is the mixture of multiple source joint distributions with mixing weights. Accordingly, we propose an algorithm that optimizes both the mixing weights and the neural network to minimize the estimated source mixture loss and the estimated Pearson χ 2 χ2 divergence. To estimate the Pearson χ 2 χ2 divergence, we rewrite it as the maximal value of a quadratic functional, exploit a linear-in-parameter function as the functional’s input, and solve the resultant optimization problem with an analytic solution . This analytic solution allows us to explicitly express the estimated divergence as a loss of the mixing weights and the network’s feature extractor. Finally, we conduct experiments on popular image classification datasets, and the results show that our algorithm statistically outperforms the comparison algorithms. PyTorch code is available at https://github.com/sentaochen/Mixture-of-Joint-Distributions .},
  archive      = {J_PR},
  author       = {Sentao Chen},
  doi          = {10.1016/j.patcog.2024.110295},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110295},
  shortjournal = {Pattern Recognition},
  title        = {Multi-source domain adaptation with mixture of joint distributions},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive modality-complement aggregative multitransformer
for domain multi-modal neural machine translation. <em>PR</em>,
<em>149</em>, 110294. (<a
href="https://doi.org/10.1016/j.patcog.2024.110294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific Multi-modal Neural Machine Translation (DMNMT) aims to translate domain-specific sentences from a source language to a target language by incorporating text-related visual information. Generally, domain-specific text-image data often complement each other and have the potential to collaboratively enhance the representation of domain-specific information. Unfortunately, there is a considerable modality gap between image and text in data format and semantic expression, which leads to distinctive challenges in domain-text translation tasks. Narrowing the modality gap and improving domain-aware representation are two critical challenges in DMNMT. To this end, this paper proposes a progressive modality-complement aggregative MultiTransformer, which aims to simultaneously narrow the modality gap and capture domain-specific multi-modal representation. We first adopt a bidirectional progressive cross-modal interactive strategy to effectively narrow the text-to-text, text-to-visual, and visual-to-text semantics in the multi-modal representation space by integrating visual and text information layer-by-layer. Subsequently, we introduce a modality-complement MultiTransformer based on progressive cross-modal interaction to extract the domain-related multi-modal representation, thereby enhancing machine translation performance. Experiment results on the Fashion-MMT and Multi-30k datasets are conducted, and the results show that the proposed approach outperforms the compared state-of-the-art (SOTA) methods on the En-Zh task in E-commerce domain, En-De, En-Fr and En-Cs tasks of Multi-30k in general domain. The in-depth analysis confirms the validity of the proposed modality-complement MultiTransformer and bidirectional progressive cross-modal interactive strategy for DMNMT.},
  archive      = {J_PR},
  author       = {Junjun Guo and Zhenyu Hou and Yantuan Xian and Zhengtao Yu},
  doi          = {10.1016/j.patcog.2024.110294},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110294},
  shortjournal = {Pattern Recognition},
  title        = {Progressive modality-complement aggregative multitransformer for domain multi-modal neural machine translation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hypergraph modeling and hypergraph multi-view attention
neural network for link prediction. <em>PR</em>, <em>149</em>, 110292.
(<a href="https://doi.org/10.1016/j.patcog.2024.110292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypergraph neural networks are widely used in link prediction because of their ability to learn the high-order structure relationship. However, most existing hypergraph modeling relies on the attribute information of nodes. And as for the link prediction, missing links are not utilized when training link predictors, so conventional transductive hypergraph learning are generally not consistent with link prediction tasks. To address these limitations, we propose the N etwork S tructure L inear R epresentation (NSLR) method to model hypergraph for general networks without node attribute information and the inductive hypergraph learning method H ypergraph M ulti-view A ttention N eural N etwork (HMANN) that learns the rich high-order structure information from node-level and hyperedge-level. Also, this paper put forwards a novel NSLR-HMANN link prediction algorithm based on NSLR and HMANN methods. Extensive comparison and ablation experiments show that the NSLR-HMANN link prediction algorithm achieves state-of-the-art performance on link prediction and has better performance on robustness.},
  archive      = {J_PR},
  author       = {Lang Chai and Lilan Tu and Xianjia Wang and Qingqing Su},
  doi          = {10.1016/j.patcog.2024.110292},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110292},
  shortjournal = {Pattern Recognition},
  title        = {Hypergraph modeling and hypergraph multi-view attention neural network for link prediction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual residual attention network for image denoising.
<em>PR</em>, <em>149</em>, 110291. (<a
href="https://doi.org/10.1016/j.patcog.2024.110291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image denoising , deep convolutional neural networks (CNNs) can obtain favorable performance on removing spatially invariant noise. However, many of these networks cannot perform well on removing the real noise (i.e. spatially variant noise) that is generated during image acquisition or transmission, which severely impedes their application in practical image denoising tasks. In this paper, we propose a novel Dual-branch Residual Attention Network (DRANet) for image denoising, which has both the merits of a wide model architecture and the attention-guided feature learning . The proposed DRANet includes two different parallel branches, which can capture complementary features to enhance the learning ability of the model. We designed a new residual attention block (RAB) and a novel hybrid dilated residual attention block (HDRAB) for the upper and lower branches, respectively. The RAB and HDRAB can capture rich local features through multiple skip connections between different convolutional layers , and the unimportant features can be dropped. Meanwhile, the long skip connections in each branch and the global feature fusion between the two parallel branches can effectively capture the global features as well. Extensive experiments demonstrate that compared with other state-of-the-art denoising methods, our DRANet can produce competitive denoising performance both on the synthetic and real-world noise removal. The code for DRANet is accessible at https://github.com/WenCongWu/DRANet .},
  archive      = {J_PR},
  author       = {Wencong Wu and Shijie Liu and Yuelong Xia and Yungang Zhang},
  doi          = {10.1016/j.patcog.2024.110291},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110291},
  shortjournal = {Pattern Recognition},
  title        = {Dual residual attention network for image denoising},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single image super-resolution based on trainable feature
matching attention network. <em>PR</em>, <em>149</em>, 110289. (<a
href="https://doi.org/10.1016/j.patcog.2024.110289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have been widely employed for image Super-Resolution (SR) in recent years. Various techniques enhance SR performance by altering CNN structures or incorporating improved self-attention mechanisms. Interestingly, these advancements share a common trait. Instead of explicitly learning high-frequency details, they learn an implicit feature processing mode that utilizes weighted sums of a feature map’s own elements for reconstruction, akin to convolution and non-local. In contrast, early dictionary-based approaches learn feature decompositions explicitly to match and rebuild Low-Resolution (LR) features. Building on this analysis, we introduce Trainable Feature Matching (TFM) to amalgamate this explicit feature learning into CNNs, augmenting their representation capabilities. Within TFM, trainable feature sets are integrated to explicitly learn features from training images through feature matching. Furthermore, we integrate non-local and channel attention into our proposed Trainable Feature Matching Attention Network (TFMAN) to further enhance SR performance. To alleviate the computational demands of non-local operations, we propose a streamlined variant called Same-size-divided Region-level Non-Local (SRNL). SRNL conducts non-local computations in parallel on blocks uniformly divided from the input feature map . The efficacy of TFM and SRNL is validated through ablation studies and module explorations. We employ a recurrent convolutional network as the backbone of our TFMAN to optimize parameter utilization. Comprehensive experiments on benchmark datasets demonstrate that TFMAN achieves superior results in most comparisons while using fewer parameters. The code is available at https://github.com/qizhou000/tfman .},
  archive      = {J_PR},
  author       = {Qizhou Chen and Qing Shao},
  doi          = {10.1016/j.patcog.2024.110289},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110289},
  shortjournal = {Pattern Recognition},
  title        = {Single image super-resolution based on trainable feature matching attention network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced visible–infrared person re-identification based on
cross-attention multiscale residual vision transformer. <em>PR</em>,
<em>149</em>, 110288. (<a
href="https://doi.org/10.1016/j.patcog.2024.110288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible–infrared (VI) person re-identification (Re-ID) is a critical identification task that involves retrieving and matching images of an individual using both infrared and visible imaging modalities . To improve the performance, researchers have developed methods to obtain implicit feature information; however, this degrades with fewer discriminative features . To address this issue, we propose a weighted fused cross-attention multi-scale residual vision transformer (WF-CAMReViT) approach to re-identify the appropriate person from visible–infrared modality images by integrating the cross-attention multi-scale residual vision transformer architecture with Opposition-based Dove Swarm Optimization (ODSO). The proposed framework aims to bridge the domain gap between the visible and infrared modalities and significantly improve the re-identification performance. RGB (visible) and infrared (IR) images of persons are gathered from standard datasets, subjected to a cross-attention multi-scale residual vision transformer network to collect features, and then fuse using minimal weight. We also propose Opposition-based DSO to find the minimal weight. The weighted fused features are then subjected to the final decoder layer of CAMReViT to perceive the characteristics of each modality. In this study, model-aware enhancement (MAE) loss is develop to improve the modality information capacity of modality-shared features. Then, the experimental results on the SYSU-MM01 and RegDB datasets are compared with state-of-the-art transformer-based visible–infrared person Re-ID tasks to verify the efficacy of the proposed model.},
  archive      = {J_PR},
  author       = {Prodip Kumar Sarker and Qingjie Zhao},
  doi          = {10.1016/j.patcog.2024.110288},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110288},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced visible–infrared person re-identification based on cross-attention multiscale residual vision transformer},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPG-LSD: A high-quality line segment detector based on
multi-scale perceptual grouping. <em>PR</em>, <em>149</em>, 110286. (<a
href="https://doi.org/10.1016/j.patcog.2024.110286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods that rely on a single Gaussian scale to detect line segments could yield poor line continuity and inferior orientation and position accuracy due to insufficient suppression of quantization noise inherent in digital images. To address this fundamental issue, a novel multi-scale perceptual grouping-based line segment detector (MPG-LSD) is proposed in this paper. Our multi-scale perceptual grouping is developed to identify and aggregate collinear pixels that have similar gradient orientations for producing line segment candidates, not only over the input image but also across a set of Gaussian scales. Multi-scale line segment refinement and validation are then further developed and implemented to produce the final detection result, which delivers high quality in terms of line continuity, orientation and position accuracy. To enrich the evaluation of line segment detection performance, a new dataset consisting of high-resolution and natural noise-corrupted images with line segment annotations is constructed. Extensive experimental results show that our proposed MPG-LSD can outperform the current state-of-the-arts by a large margin.},
  archive      = {J_PR},
  author       = {Zikai Wang and Baojiang Zhong and Xueyuan Chen and Hangjia Zheng},
  doi          = {10.1016/j.patcog.2024.110286},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110286},
  shortjournal = {Pattern Recognition},
  title        = {MPG-LSD: A high-quality line segment detector based on multi-scale perceptual grouping},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion-competition framework of local topology and global
texture for head pose estimation. <em>PR</em>, <em>149</em>, 110285. (<a
href="https://doi.org/10.1016/j.patcog.2024.110285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB image and point cloud involve texture and geometric structure, which are widely used for head pose estimation. However, images lack of spatial information, and the quality of point cloud is easily affected by sensor noise. In this paper, a novel fusion-competition framework (FCF) is proposed to overcome the limitations of a single modality. The global texture information is extracted from image and the local topology information is extracted from point cloud to project heterogeneous data into a common feature subspace. The projected texture feature weighted by the channel attention mechanism is embedded into each local point cloud region with different topological features for fusion. The scoring mechanism creates competition among the regions involving local-global fused features to predict final pose with the highest score. According to the evaluation results on the public and our constructed datasets, the FCF improves the estimation accuracy and stability by an average of 13.6 % and 12.7 %, which is compared to nine state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Dongsheng Ma and Tianyu Fu and Yifei Yang and Kaibin Cao and Jingfan Fan and Deqiang Xiao and Hong Song and Ying Gu and Jian Yang},
  doi          = {10.1016/j.patcog.2024.110285},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110285},
  shortjournal = {Pattern Recognition},
  title        = {Fusion-competition framework of local topology and global texture for head pose estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). PVConvNet: Pixel-voxel sparse convolution for multimodal 3D
object detection. <em>PR</em>, <em>149</em>, 110284. (<a
href="https://doi.org/10.1016/j.patcog.2024.110284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current LiDAR-only 3D detection methods inevitably suffer from the sparsity of point clouds and insufficient semantic information. To alleviate this difficulty, recent proposals densify LiDAR points by depth completion and then perform feature fusion with image pixels at the data-level or result-level. However, these methods often suffer from poor fusion effects and insufficient use of image information for voxel feature-level fusion. Meanwhile, noises brought by inaccurate depth completion significantly degrade detection accuracy. In this paper, we propose PVConvNet, a unified framework for multi-modal feature fusion that cleverly combines LiDAR points, virtual points and image pixels. Firstly, we develop an efficient Pixel-Voxel Sparse Convolution (PVConv) to perform voxel-wise feature-level fusion of point clouds and images. Secondly, we design a Noise-Resistant Dilated Sparse Convolution (NRDConv) to encode the voxel features of virtual points, which effectively reduces the impact of noise. Finally, we propose a unified RoI pooling strategy, namely Multimodal Voxel-RoI Pooling, for improving proposal refinement accuracy. We evaluate PVConvNet on the widely used KITTI dataset and the more challenging nuScenes dataset. Experimental results show that our method outperforms state-of-the-art multi-modal based methods, achieving a moderate 3D AP of 86.92% on the KITTI test set.},
  archive      = {J_PR},
  author       = {Huaijin Liu and Jixiang Du and Yong Zhang and Hongbo Zhang and Jiandian Zeng},
  doi          = {10.1016/j.patcog.2024.110284},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110284},
  shortjournal = {Pattern Recognition},
  title        = {PVConvNet: Pixel-voxel sparse convolution for multimodal 3D object detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal brain tumor segmentation via disentangled
representation learning and region-aware contrastive learning.
<em>PR</em>, <em>149</em>, 110282. (<a
href="https://doi.org/10.1016/j.patcog.2024.110282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are threatening the life and health of people in the world. Automatic brain tumor segmentation using multiple MR images is challenging in medical image analysis. It is known that accurate segmentation relies on effective feature learning . Existing methods address the multi-modal MR brain tumor segmentation by explicitly learning a shared feature representation. However, these methods fail to capture the relationship between MR modalities and the feature correlation between different target tumor regions. In this paper, I propose a multi-modal brain tumor segmentation network via disentangled representation learning and region-aware contrastive learning . Specifically, a feature fusion module is first designed to learn the valuable multi-modal feature representation. Subsequently, a novel disentangled representation learning is proposed to decouple the fused feature representation into multiple factors corresponding to the target tumor regions. Furthermore, contrastive learning is presented to help the network extract tumor region-related feature representations. Finally, the segmentation results are obtained using the segmentation decoders. Quantitative and qualitative experiments conducted on the public datasets, BraTS 2018 and BraTS 2019, justify the importance of the proposed strategies, and the proposed approach can achieve better performance than other state-of-the-art approaches. In addition, the proposed strategies can be extended to other deep neural networks .},
  archive      = {J_PR},
  author       = {Tongxue Zhou},
  doi          = {10.1016/j.patcog.2024.110282},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110282},
  shortjournal = {Pattern Recognition},
  title        = {Multi-modal brain tumor segmentation via disentangled representation learning and region-aware contrastive learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards robust neural networks via orthogonal diversity.
<em>PR</em>, <em>149</em>, 110281. (<a
href="https://doi.org/10.1016/j.patcog.2024.110281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are vulnerable to invisible perturbations on the images generated by adversarial attacks , which raises researches on the adversarial robustness of DNNs. A series of methods represented by the adversarial training and its variants have proven as one of the most effective techniques in enhancing the DNN robustness. Generally, adversarial training focuses on enriching the training data by involving perturbed data. Such data augmentation effect of the involved perturbed data in adversarial training does not contribute to the robustness of DNN itself and usually suffers from clean accuracy drop. Towards the robustness of DNN itself, we in this paper propose a novel defense that aims at augmenting the model in order to learn features that are adaptive to diverse inputs, including adversarial examples . More specifically, to augment the model, multiple paths are embedded into the network, and an orthogonality constraint is imposed on these paths to guarantee the diversity among them. A margin-maximization loss is then designed to further boost such DIversity via Orthogonality (DIO). In this way, the proposed DIO augments the model and enhances the robustness of DNN itself as the learned features can be corrected by these mutually-orthogonal paths. Extensive empirical results on various data sets, structures and attacks verify the stronger adversarial robustness of the proposed DIO utilizing model augmentation. Besides, DIO can also be flexibly combined with different data augmentation techniques ( e.g. , TRADES and DDPM), further promoting robustness gains.},
  archive      = {J_PR},
  author       = {Kun Fang and Qinghua Tao and Yingwen Wu and Tao Li and Jia Cai and Feipeng Cai and Xiaolin Huang and Jie Yang},
  doi          = {10.1016/j.patcog.2024.110281},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110281},
  shortjournal = {Pattern Recognition},
  title        = {Towards robust neural networks via orthogonal diversity},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised domain generalization with evolving
intermediate domain. <em>PR</em>, <em>149</em>, 110280. (<a
href="https://doi.org/10.1016/j.patcog.2024.110280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain Generalization (DG) aims to generalize a model trained on multiple source domains to an unseen target domain. The source domains always require precise annotations, which can be cumbersome or even infeasible to obtain in practice due to the vast amount of data involved. Web data, namely web-crawled images, offers an opportunity to access large amounts of unlabeled images with rich style information, which can be leveraged to improve DG. From this perspective, we introduce a novel paradigm of DG, termed as Semi-Supervised Domain Generalization (SSDG) , to explore how the labeled and unlabeled source domains can interact, and establish two settings, including the close-set and open-set SSDG. The close-set SSDG is based on existing public DG datasets, while the open-set SSDG, built on the newly-collected web-crawled datasets, presents a novel yet realistic challenge that pushes the limits of current technologies. A natural approach of SSDG is to transfer knowledge from labeled data to unlabeled data via pseudo labeling, and train the model on both labeled and pseudo-labeled data for generalization. Since there are conflicting goals between domain-oriented pseudo labeling and out-of-domain generalization, we develop a pseudo labeling phase and a generalization phase independently for SSDG. Unfortunately, due to the large domain gap, the pseudo labels provided in the pseudo labeling phase inevitably contain noise, which has negative affect on the subsequent generalization phase. Therefore, to improve the quality of pseudo labels and further enhance generalizability , we propose a cyclic learning framework to encourage a positive feedback between these two phases, utilizing an evolving intermediate domain that bridges the labeled and unlabeled domains in a curriculum learning manner. Extensive experiments are conducted to validate the effectiveness of our method. It is worth highlighting that web-crawled images can promote domain generalization as demonstrated by the experimental results.},
  archive      = {J_PR},
  author       = {Luojun Lin and Han Xie and Zhishu Sun and Weijie Chen and Wenxi Liu and Yuanlong Yu and Lei Zhang},
  doi          = {10.1016/j.patcog.2024.110280},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110280},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised domain generalization with evolving intermediate domain},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SEMv2: Table separation line detection based on instance
segmentation. <em>PR</em>, <em>149</em>, 110279. (<a
href="https://doi.org/10.1016/j.patcog.2024.110279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Table structure recognition is an indispensable element for enabling machines to comprehend tables. Its primary purpose is to identify the internal structure of a table. Nevertheless, due to the complexity and diversity of their structure and style, it is highly challenging to parse the tabular data into a structured format that machines can comprehend. In this work, we adhere to the principle of the split-and-merge based methods and propose an accurate table structure recognizer, termed SEMv2 (SEM: S plit, E mbed and M erge). Unlike the previous works in the “split” stage, we aim to address the table separation line instance-level discrimination problem and introduce a table separation line detection strategy based on conditional convolution. Specifically, we design the “split” in a top-down manner that detects the table separation line instance first and then dynamically predicts the table separation line mask for each instance. The final table separation line shape can be accurately obtained by processing the table separation line mask in a row-wise/column-wise manner. To comprehensively evaluate the SEMv2, we also present a more challenging dataset for table structure recognition, dubbed iFLYTAB, which encompasses multiple style tables in various scenarios such as photos, scanned documents, etc. Extensive experiments on publicly available datasets (e.g. SciTSR, PubTabNet and iFLYTAB) demonstrate the efficacy of our proposed approach. The code and iFLYTAB dataset are available at https://github.com/ZZR8066/SEMv2},
  archive      = {J_PR},
  author       = {Zhenrong Zhang and Pengfei Hu and Jiefeng Ma and Jun Du and Jianshu Zhang and Baocai Yin and Bing Yin and Cong Liu},
  doi          = {10.1016/j.patcog.2024.110279},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110279},
  shortjournal = {Pattern Recognition},
  title        = {SEMv2: Table separation line detection based on instance segmentation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unbiased and augmentation-free self-supervised graph
representation learning. <em>PR</em>, <em>149</em>, 110274. (<a
href="https://doi.org/10.1016/j.patcog.2024.110274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) is a promising self-supervised method for learning node representations that combines graph convolutional networks (GCN) and contrastive learning. However, existing GCL methods heavily rely on graph structure data and augmentation schemes to learn invariant representations between different augmentation views. This can be problematic as the performance of GCNs may deteriorate when noisy connections are present in the original graph structure. Additionally, there is limited knowledge on how to significantly augment graphs without altering their labels. To address these issues, we propose a novel method called Unbiased and Augmentation-Free Self-Supervised Graph Contrastive Learning (USAF-GCL). We design graph learners and post-processing schemes to improve the structure of the original graph. Instead of using augmentation schemes, we generate contrastive views using global and local semantics. To ensure consistency between embedding similarity and original feature similarity, we introduce pseudo-homology to maximize the mutual information between predicted and true labels. Furthermore, we theoretically demonstrate that pseudo-homology maximization can enhance the upper bound of mutual information between predicted and true labels. USAF-GCL offers several advantages over existing GCL methods. Firstly, it uses an unbiased graph structure to reduce the impact of noise on model performance. Secondly, it saves computational resources by eliminating complex data expansion. Lastly, it integrates structural information, neighborhood information, and the consistency of embeddings and features in graph representation learning , effectively improving model performance. Extensive experiments on eight benchmark datasets confirm the remarkable effectiveness and efficiency of USAF-GCL.},
  archive      = {J_PR},
  author       = {Ruyue Liu and Rong Yin and Yong Liu and Weiping Wang},
  doi          = {10.1016/j.patcog.2024.110274},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110274},
  shortjournal = {Pattern Recognition},
  title        = {Unbiased and augmentation-free self-supervised graph representation learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual learning for cross-modal image-text retrieval
based on domain-selective attention. <em>PR</em>, <em>149</em>, 110273.
(<a href="https://doi.org/10.1016/j.patcog.2024.110273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal image-text retrieval (CMITR) has been a high-value research topic for more than a decade. In most of the previous studies, the data for all tasks are trained as a single set. However, in reality, a more likely scenario is that the dataset has multiple tasks and trains them in sequence. The consequence is the limited ability to memorize the old task once a new task arrives; in other words, catastrophic forgetting. To solve this issue, this paper proposes a novel continual learning for cross-modal image-text retrieval (CLCMR) method to alleviate catastrophic forgetting. We construct a multilayer domain-selective attention (MDSA) based network to obtain knowledge from task-relevant and domain-specific attention levels. Moreover, a memory factor has been designed to achieve weight regularization , and a novel memory loss function is utilized to constrain MDSA. The extensive experimental results from multiple datasets (Wikipedia, Pascal Sentence, and PKU XMedianet datasets) demonstrate that CLCMR can effectively alleviate catastrophic forgetting and achieve a superior continual learning ability compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Rui Yang and Shuang Wang and Yu Gu and Jihui Wang and Yingzhi Sun and Huan Zhang and Yu Liao and Licheng Jiao},
  doi          = {10.1016/j.patcog.2024.110273},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110273},
  shortjournal = {Pattern Recognition},
  title        = {Continual learning for cross-modal image-text retrieval based on domain-selective attention},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Learning self-target knowledge for few-shot segmentation.
<em>PR</em>, <em>149</em>, 110266. (<a
href="https://doi.org/10.1016/j.patcog.2024.110266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation uses a few annotated data of a specific class in the support set to segment the target of the same class in the query set. Most existing approaches fail to perform well when there are significant intra-class variances. This paper alleviates the problem by concentrating on mining the query image and using the support set as supplementary information. First, it proposes a Query Prototype Generation Module to generate a query foreground prototype from the query features. Specifically, we use both prototype-level and pixel-level similarity matching to generate two complementary initial prototypes, which we then integrate to create a discriminative query foreground prototype. Second, we propose a Support Auxiliary Refinement Module to further guide the final precise prediction of the query image by leveraging the target category information of the support set through step-by-step mining. Specifically, we generate a query-support mixture prototype based on the support prototype representation obtained using the attention mechanism . Then we generate a support supplement prototype to complement the missing information by encoding over the foreground regions that the query-support mixture prototype fails to segment out. Extensive experiments on PASCAL- 5 i 5i and COCO- 2 0 i 20i demonstrate that our model outperforms the prior works of few-shot segmentation.},
  archive      = {J_PR},
  author       = {Yadang Chen and Sihan Chen and Zhi-Xin Yang and Enhua Wu},
  doi          = {10.1016/j.patcog.2024.110266},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110266},
  shortjournal = {Pattern Recognition},
  title        = {Learning self-target knowledge for few-shot segmentation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). FET-FGVC: Feature-enhanced transformer for fine-grained
visual classification. <em>PR</em>, <em>149</em>, 110265. (<a
href="https://doi.org/10.1016/j.patcog.2024.110265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of Fine-grained visual classification (FGVC) comes from the small variations between classes and the large variations within classes. Inspired by the fact that identifying bird species focuses not only on the global features of the subject area but also on the subtle details of the local area, we propose a feature-enhanced Transformer to improve the performance of FGVC. Our proposed method consists of a Dynamic Swin Transformer backbone for extracting comprehensive global image features through continuous attention aggregation, a GCN-based local branch for separating and enhancing local features in different regions, and a pairwise feature interaction (PFI) module for enhancing global features through interactions between image pairs. We conducted extensive experiments on five FGVC datasets to demonstrate the superiority of our method. By fusing the enhanced global and local features, our method achieves the best accuracy compared to existing methods. Our method has an advantage in terms of computational efficiency.},
  archive      = {J_PR},
  author       = {Huazhen Chen and Haimiao Zhang and Chang Liu and Jianpeng An and Zhongke Gao and Jun Qiu},
  doi          = {10.1016/j.patcog.2024.110265},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110265},
  shortjournal = {Pattern Recognition},
  title        = {FET-FGVC: Feature-enhanced transformer for fine-grained visual classification},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the representation and methodology for wide and short
range head pose estimation. <em>PR</em>, <em>149</em>, 110263. (<a
href="https://doi.org/10.1016/j.patcog.2024.110263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation (HPE) is a problem of interest in computer vision to improve the performance of face processing tasks in semi-frontal or profile settings. Recent applications require the analysis of faces in the full 360° rotation range. Traditional approaches to solve the semi-frontal and profile cases are not directly amenable for the full rotation case. In this paper we analyze the methodology for short- and wide-range HPE and discuss which representations and metrics are adequate for each case. We show that the popular Euler angles representation is a good choice for short-range HPE, but not at extreme rotations. However, the Euler angles’ gimbal lock problem prevents them from being used as a valid metric in any setting. We also revisit the current cross-data set evaluation methodology and note that the lack of alignment between the reference systems of the training and test data sets negatively biases the results of all articles in the literature. We introduce a procedure to quantify this misalignment and a new methodology for cross-data set HPE that establishes new, more accurate, SOTA for the 300W-LP/Biwi benchmark. We also propose a generalization of the geodesic angular distance metric that enables the construction of a loss that controls the contribution of each training sample to the optimization of the model. Finally, we introduce a wide range HPE benchmark based on the CMU Panoptic data set. code: https://github.com/pcr-upm/opal23_headpose},
  archive      = {J_PR},
  author       = {Alejandro Cobo and Roberto Valle and José M. Buenaposada and Luis Baumela},
  doi          = {10.1016/j.patcog.2024.110263},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110263},
  shortjournal = {Pattern Recognition},
  title        = {On the representation and methodology for wide and short range head pose estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale hypergraph-based feature alignment network for
cell localization. <em>PR</em>, <em>149</em>, 110260. (<a
href="https://doi.org/10.1016/j.patcog.2024.110260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell localization in medical image analysis is a challenging task due to the significant variation in cell shape, size and color. Existing localization methods continue to tackle these challenges separately, frequently facing complications where these difficulties intersect and adversely impact model performance. In this paper, these challenges are first reframed as issues of feature misalignment between cell images and location maps, which are then collectively addressed. Specifically, we propose a feature alignment model based on a multi-scale hypergraph attention network . The model considers local regions in the feature map as nodes and utilizes a learnable similarity metric to construct hypergraphs at various scales. We then utilize a hypergraph convolutional network to aggregate the features associated with the nodes and achieve feature alignment between the cell images and location maps. Furthermore, we introduce a stepwise adaptive fusion module to fuse features at different levels effectively and adaptively. The comprehensive experimental results demonstrate the effectiveness of our proposed multi-scale hypergraph attention module in addressing the issue of feature misalignment, and our model achieves state-of-the-art performance across various cell localization datasets.},
  archive      = {J_PR},
  author       = {Bo Li and Yong Zhang and Chengyang Zhang and Xinglin Piao and Yongli Hu and Baocai Yin},
  doi          = {10.1016/j.patcog.2024.110260},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110260},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale hypergraph-based feature alignment network for cell localization},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Promote knowledge mining towards open-world semi-supervised
learning. <em>PR</em>, <em>149</em>, 110259. (<a
href="https://doi.org/10.1016/j.patcog.2024.110259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models often rely on a large number of labeled data to achieve good performance . However, labeling such a large number of data requires exhaustive labor efforts. In recent years, a pivotal research direction is to generalize deep learning models to learn from not only unlabeled data of seen classes but also data of novel classes which are not predefined, known as open-world semi-supervised learning (open-world SSL). Existing works tackled this challenging task by manually designing different optimizations for labeled/unlabeled data and seen/novel classes. In this paper, we propose a simple unified framework that can be applied to all images and all classes in the same form. In this framework, we exploit the Sinkhorn–Knopp algorithm to overcome the overconfidence issue of pseudo labels on seen classes and thus lead to a more balanced distribution of seen and novel classes. To reduce the intra-class variance and avoid model collapse, we take as input two different views of an image and regard one’s prediction as the other’s pseudo label. However, in a unified framework, the model converges much faster on the seen classes than those novel classes. To balance them and encourage knowledge transfer from seen classes to novel classes, we further propose mixing up any two training images during our unified optimization. Extensive experiments on three benchmarks ( i.e. , CIFAR-10, CIFAR-100, and ImageNet-100) show that our unified framework achieved comparable performance with existing state-of-the-art methods. Our code is available on https://github.com/happytianhao/OWSSL .},
  archive      = {J_PR},
  author       = {Tianhao Zhao and Yutian Lin and Yu Wu and Bo Du},
  doi          = {10.1016/j.patcog.2024.110259},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110259},
  shortjournal = {Pattern Recognition},
  title        = {Promote knowledge mining towards open-world semi-supervised learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning adversarial semantic embeddings for zero-shot
recognition in open worlds. <em>PR</em>, <em>149</em>, 110258. (<a
href="https://doi.org/10.1016/j.patcog.2024.110258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-Shot Learning (ZSL) focuses on classifying samples of unseen classes with only their side semantic information presented during training. It cannot handle real-life, open-world scenarios where there are test samples of unknown classes for which neither samples (e.g., images) nor their side semantic information is known during training. Open-Set Recognition (OSR) is dedicated to addressing the unknown class issue, but existing OSR methods are not designed to model the semantic information of the unseen classes. To tackle this combined ZSL and OSR problem, we consider the case of “Zero-Shot Open-Set Recognition” (ZS-OSR), where a model is trained under the ZSL setting but it is required to accurately classify samples from the unseen classes while being able to reject samples from the unknown classes during inference. We perform large experiments on combining existing state-of-the-art ZSL and OSR models for the ZS-OSR task on four widely used datasets adapted from the ZSL task, and reveal that ZS-OSR is a non-trivial task as the simply combined solutions perform badly in distinguishing the unseen-class and unknown-class samples. We further introduce a novel approach specifically designed for ZS-OSR, in which our model learns to generate adversarial semantic embeddings of the unknown classes to train an unknowns-informed ZS-OSR classifier . Extensive empirical results show that our method 1) substantially outperforms the combined solutions in detecting the unknown classes while retaining the classification accuracy on the unseen classes and 2) achieves similar superiority under generalized ZS-OSR settings. Our code is available at https://github.com/lhrst/ASE .},
  archive      = {J_PR},
  author       = {Tianqi Li and Guansong Pang and Xiao Bai and Jin Zheng and Lei Zhou and Xin Ning},
  doi          = {10.1016/j.patcog.2024.110258},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110258},
  shortjournal = {Pattern Recognition},
  title        = {Learning adversarial semantic embeddings for zero-shot recognition in open worlds},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Superpixel-based multi-scale multi-instance learning for
hyperspectral image classification. <em>PR</em>, <em>149</em>, 110257.
(<a href="https://doi.org/10.1016/j.patcog.2024.110257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Superpixels can define meaningful local regions within a hyperspectral image (HSI) and have become the building blocks of various HSI classification methods. The superpixels in HSIs consist of multiple spectral pixels, sharing a similar structure with the data in multi-instance learning (MIL). However, the potential of MIL methods in the field of HSI classification has been rarely explored. In this paper, we propose the superpixel-based multi-scale multi-instance learning (MSMIL) framework, enhancing the superpixel representation with MIL for the first time. Segmenting the HSIs with superpixels, MSMIL converts the HSI classification into MIL problems and extracts superpixel representations via the MIL method, namely multi-instance factor analysis (MIFA). Compared with the existing methods focusing exclusively on the local information , MIFA utilizes the deviations from an overall generative model to describe the superpixels, retaining both the local and the global information. Moreover, MSMIL introduces multi-scale superpixels and a spectral–spatial decision fusion strategy for further refinement, where the results of multi-scale superpixel maps are weighted according to prediction certainty and spatial consistency. The proposed method is evaluated on four benchmark datasets and achieves competitive results. For example, MSMIL outperforms the comparison methods with a margin of 5% overall accuracy on the Indian Pines dataset, when 2% pixels are selected as the training set.},
  archive      = {J_PR},
  author       = {Shiluo Huang and Zheng Liu and Wei Jin and Ying Mu},
  doi          = {10.1016/j.patcog.2024.110257},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110257},
  shortjournal = {Pattern Recognition},
  title        = {Superpixel-based multi-scale multi-instance learning for hyperspectral image classification},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust circle detector with regionalized radius aid.
<em>PR</em>, <em>149</em>, 110256. (<a
href="https://doi.org/10.1016/j.patcog.2024.110256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of circles in geometric shapes is highly valued in computer vision and pattern recognition. Conventionally, the least-squares fitting is sensitive to occlusion or noise and prone to false circles. Therefore, this paper proposes a novel algorithm for robust circle detection using the least-squares fitting method combined with regionalized radius aid on the arc and chord lengths. To reduce edge noise impact, we present an edge pruning method to prune non-curve edge branch ports. Furthermore, we extract arcs based on the inflection points and sharp corners of the approximate line segment. Next, curves that belong to the arcs obtain circle parameters according to the regionalized radius aided the least-squares method. Then, valid circles are obtained by considering two distance deviations to verify the candidate circles. Finally, valid arcs that belong to the same circle are combined and refitted, wherein the most representative arc is used as the basis for the refitting of all arcs. All experiments are conducted on real images from four publicly diverse datasets (one of them is the one we built). The detection results are compared with those of representative state-of-the-art algorithms, and the proposed algorithm has several advantages based on the comparison results: more robust to noise, effective rejection of false circles, and better performance.},
  archive      = {J_PR},
  author       = {Xianguang Xu and Ronggang Yang and Naige Wang},
  doi          = {10.1016/j.patcog.2024.110256},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110256},
  shortjournal = {Pattern Recognition},
  title        = {A robust circle detector with regionalized radius aid},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic model based on gaussian random fields to
characterize the morphology of granular objects. <em>PR</em>,
<em>149</em>, 110255. (<a
href="https://doi.org/10.1016/j.patcog.2024.110255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The geometrical modeling of granular objects is a complex challenge that exists in many scientific fields, such as the modeling of granular materials or rocks and coarse aggregates with applications in civil, mechanical, and chemical engineering . In this paper, a model called SPHERE (Stochastic Process for Highly Effective Radial Expansion) is proposed, which is based on the deformation of an ellipsoid mesh using multiple 3D Gaussian random fields. The model is designed to be flexible (full control over 2D and 3D morphological properties of granular objects), ultra-fast (over 1000 aggregates in less than 5 s), and independent of the mesh and base shape used (as long as it is a star-shaped object). The flexibility of the model and its ability to reflect real data is illustrated using images of latex nanoparticle aggregates. Using 2D measurements on images from a morphogranulometer, a method based on the SPHERE model is proposed to estimate the 3D morphological properties of aggregates. A multiscale optimization process is applied, in particular using a partial reconstruction of 2D shapes from elliptic Fourier descriptors , in order to best reproduce the shape, angularity and texture of the aggregates using the SPHERE model. Validation of the method on 3D printed data shows relative errors of less than 3% for all measured 2D and 3D morphological characteristics, and validation on a population of synthetic objects shows relative errors of less than 6%. The results are compared and discussed with those obtained using other models based on overlapping spheres and show consistency with previous work. Finally, suggestions for improvement are given.},
  archive      = {J_PR},
  author       = {L. Théodon and C. Coufort-Saudejaud and J. Debayle},
  doi          = {10.1016/j.patcog.2024.110255},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110255},
  shortjournal = {Pattern Recognition},
  title        = {A stochastic model based on gaussian random fields to characterize the morphology of granular objects},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HRNet: 3D object detection network for point cloud with
hierarchical refinement. <em>PR</em>, <em>149</em>, 110254. (<a
href="https://doi.org/10.1016/j.patcog.2024.110254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D object detection from LiDAR point clouds has advanced rapidly. Although the second stage can improve the detection performance significantly, prior works concern little about the essential differences among different stages for the performance enhancement. To address this, this paper proposes a Hierarchical Refinement Network (HRNet) with two novel strategies. Firstly, we build the detection head on multi-scale voxel features to optimize the regression branch progressively with an effective Scale-aware Attentive Propagation (SAP) module. Then, we propose a Dynamic Sample Selection (DSS) module for the recalculation of the IoU during each stage to obtain more balanced positive and negative sample selections. Experiments over benchmark datasets show the effectiveness of our HRNet, particularly for car detection in the sparse point clouds.},
  archive      = {J_PR},
  author       = {Bin Lu and Yang Sun and Zhenyu Yang and Ran Song and Haiyan Jiang and Yonghuai Liu},
  doi          = {10.1016/j.patcog.2024.110254},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110254},
  shortjournal = {Pattern Recognition},
  title        = {HRNet: 3D object detection network for point cloud with hierarchical refinement},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor recovery based on bivariate equivalent
minimax-concave penalty. <em>PR</em>, <em>149</em>, 110253. (<a
href="https://doi.org/10.1016/j.patcog.2024.110253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In tensor recovery problem, larger singular values often correspond to the primary information of the image, such as contours, sharp edges and smooth areas. The minimum–maximum concave penalty (MCP) function has been effective in preserving larger singular values and achieving good tensor recovery results. However, in the process of solving this problem, singular values may vary during iterations, and the fixed parameters of the MCP function may not sufficiently preserve all the larger singular values, which may hinder the attainment of optimal results in tensor recovery. To overcome this challenge, we propose the Bivariate Equivalent Minimax-Concave Penalty (BEMCP) theorem, which allows the parameters to adapt to the changes in singular values and more comprehensively preserve the larger singular values. For low-rank tensor completion and tensor robust principal component analysis problems, we propose BEMCP-based models. Finally, experiments with various real-world data demonstrate that the proposed methods outperform state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Hongbing Zhang and Hongtao Fan and Yajing Li},
  doi          = {10.1016/j.patcog.2024.110253},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110253},
  shortjournal = {Pattern Recognition},
  title        = {Tensor recovery based on bivariate equivalent minimax-concave penalty},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdvOps: Decoupling adversarial examples. <em>PR</em>,
<em>149</em>, 110252. (<a
href="https://doi.org/10.1016/j.patcog.2024.110252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial examples have a simple additive structure that the clean sample is added with delicate devised noise. Inspired by such an observation, we find that the prediction of the network on adversarial examples can also be decoupled into a simple additive structure, which is the sum of clean samples and adversarial perturbations in terms of the model prediction (called the decoupling principle). Thus, our findings can be served as a useful tool to gain insight into the underlying relationship between the inputs and the outputs of the model. However, although the adversarial examples generated by existing attack methods can satisfy the decoupling principle, the proportion is small. In this paper, we formulate the above issues as an optimization problem with multi-constrains, and we propose a generative model to generate adversarial examples that satisfy the decoupling principle and simultaneously obtain high attack performance. Specifically, we first adopt the adversarial loss to ensure the attack performance. Then, we devise a decouple loss to guarantee the decoupling principle. Moreover, we treat the Euclidean distances of perturbation as regularization terms to maintain visual quality. Extensive experiments against various networks on ImageNet and CIFAR10 show that the proposed method performs better than comparison methods in the comprehensive metric. Furthermore, transferability results suggested that adversarial examples that satisfy the decoupling principle show better transferability.},
  archive      = {J_PR},
  author       = {Donghua Wang and Wen Yao and Tingsong Jiang and Xiaoqian Chen},
  doi          = {10.1016/j.patcog.2024.110252},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110252},
  shortjournal = {Pattern Recognition},
  title        = {AdvOps: Decoupling adversarial examples},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CR-CAM: Generating explanations for deep neural networks by
contrasting and ranking features. <em>PR</em>, <em>149</em>, 110251. (<a
href="https://doi.org/10.1016/j.patcog.2024.110251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class activation mapping (CAM) algorithm is a visual interpretation algorithm that identifies the most discriminative regions for the target class in a classification task . However, existing CAM algorithms do not consider the differences between different categories and regions that are irrelevant to the target class during the feature extraction process. This results in interference from similar categories and irrelevant regions on the edges of the target class, leading to distorted saliency maps. To address this issue, we propose the Contrast-Ranking Class Activation Mapping (CR-CAM), including Inter-class Mapping Contract Block (IMCB) and Ranking Block. To gradually eliminate the interference regions that are irrelevant to the target class, IMCB is designed to compare distances between features in manifold space in order to generate more accurate saliency maps. To account for the similarity between different categories, ranking blocks adopt a comparative approach to measure the distances of feature mappings in the manifold space, thereby reducing the weights of surrounding regions. CR-CAM can be used on both CNN and GCN without modifying the algorithm to generate the class activation map. Experiments on both ImageNet and NTU RGB+D 60 datasets show highly competitive performance. In particular, CR-CAM outperforms alternative approaches by an average decrease of 0.0811 in Ave Drop, while exhibiting an average improvement of 0.011 in Ave Inc for CNN.},
  archive      = {J_PR},
  author       = {Yanshan Li and Huajie Liang and Hongfang Zheng and Rui Yu},
  doi          = {10.1016/j.patcog.2024.110251},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110251},
  shortjournal = {Pattern Recognition},
  title        = {CR-CAM: Generating explanations for deep neural networks by contrasting and ranking features},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring low-resource medical image classification with
weakly supervised prompt learning. <em>PR</em>, <em>149</em>, 110250.
(<a href="https://doi.org/10.1016/j.patcog.2024.110250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most advances in medical image recognition supporting clinical auxiliary diagnosis meet challenges due to the low-resource situation in the medical field, where annotations are highly expensive and professional. This low-resource problem can be alleviated by leveraging the transferable representations of large-scale pre-trained vision-language models like CLIP. After being pre-trained using large-scale unlabeled medical images and texts (such as medical reports), the vision-language models can learn transferable representations and support flexible downstream clinical tasks such as medical image classification via relevant medical text prompts. However, existing pre-trained vision-language models require domain experts (clinicians) to carefully design the medical text prompts based on different datasets when applied to specific medical image tasks, which is extremely time-consuming and greatly increases the burden on clinicians. To address this problem, we propose a weakly supervised prompt learning method M e d P r o m p t MedPrompt for automatically generating medical prompts, which includes an unsupervised pre-trained vision-language model and a weakly supervised prompt learning model. The unsupervised pre-trained vision-language model adopts large-scale medical images and texts for pre-training, utilizing the natural correlation between medical images and corresponding medical texts without manual annotations. The weakly supervised prompt learning model only utilizes the classes of images in the dataset to guide the learning of the specific class vector in the prompt, while the learning of other context vectors in the prompt does not require any manual annotations for guidance. To the best of our knowledge, this is the first model to automatically generate medical prompts. With the assistance of these prompts, the pre-trained vision-language model can be freed from the strong expert dependency of manual annotation and manual prompt design, thus achieving end-to-end, low-cost medical image classification . Experimental results show that the model using our automatically generated prompts outperforms all its hand-crafted prompts counterparts in full-shot learning on all four datasets, and achieves superior accuracy on zero-shot image classification and few-shot learning in three of the four medical benchmark datasets and comparable accuracy in the remaining one. In addition, the proposed prompt generator is lightweight and therefore has the potential to be embedded into any network architecture .},
  archive      = {J_PR},
  author       = {Fudan Zheng and Jindong Cao and Weijiang Yu and Zhiguang Chen and Nong Xiao and Yutong Lu},
  doi          = {10.1016/j.patcog.2024.110250},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110250},
  shortjournal = {Pattern Recognition},
  title        = {Exploring low-resource medical image classification with weakly supervised prompt learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCGTracker: Spatio-temporal correlation and graph neural
networks for multiple object tracking. <em>PR</em>, <em>149</em>,
110249. (<a href="https://doi.org/10.1016/j.patcog.2023.110249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple Object Tracking is an important vitally important fundamental task in computer vision . Visual tracking becomes challenging when objects move in groups and are obscured from each other. There are two mainstream solution strategies for these group models. One is to transform the data association problem into a graph matching problem for solving, while the other is to apply the social power model as an advanced constraint for group tracking. In the former case, the solving difficulty geometric growth as the number of tracked objects increases, and the computing efficiency for real-time tracking demand cannot be met. The latter strategy tends to set up fixed-size groups or offline training rules, resulting in a lack of flexibility that limits their scenario generalization. According to the shortcomings of existing methods, this paper proposes a novel multiple object tracking method with spatio-temporal correlation and graph neural networks . Firstly, the relational features of the historical trajectories are extracted through the spatio-temporal relationship learning module, which models the spatio-temporal correlations of the objects and dynamically constructs the group structure online. Then, the graph neural network is combined with appearance and motion information, and the similarity between each detection and tracklet is used as a weight in node feature aggregation to make powerful distinctions between node features. Meanwhile, the spatio-temporal correlation method is also used to solve target loss issues caused by occlusion. Even collocated with linearly assigned data association method, good tracking results are still achieved, with a low computational complexity . Experiments on three challenging public datasets, namely MOT16, MOT17, and MOT20, validated the accuracy and efficiency of the proposed tracking method.},
  archive      = {J_PR},
  author       = {Yajuan Zhang and Yongquan Liang and Jiaxu Leng and Zhihui Wang},
  doi          = {10.1016/j.patcog.2023.110249},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110249},
  shortjournal = {Pattern Recognition},
  title        = {SCGTracker: Spatio-temporal correlation and graph neural networks for multiple object tracking},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient aggregation based fine-grained image retrieval: A
unified viewpoint for CNN and transformer. <em>PR</em>, <em>149</em>,
110248. (<a href="https://doi.org/10.1016/j.patcog.2023.110248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gradients of CNN are traditionally utilized for optimization and visualization. In this paper, we find that a discriminative representation hides in the gradients of convolution filters . Based on this, we propose a corresponding feature extraction and aggregation method for fine-grained image retrieval (FGIR). Firstly, we propose a metric to evaluate manually-designed loss functions and design a loss function originating from Grad-CAM in the testing phase based on it to extract the gradients of the convolution filters. Secondly, we take the gradients as the new features and design a succinct approach to aggregate them into a compact vector, which is named as Convolution Filters Gradient Aggregation (CFGA) feature. CFGA features can be extracted from pre-trained and fine-tuned CNN models. Extensive experiments are conducted on FGIR to verify the effectiveness of our proposed CFGA approach, compared with five supervised state-of-the-art methods and two unsupervised methods on two standard fine-grained retrieval datasets. Moreover, we generalize the CFGA method designed for CNN to Swin Transformer , and propose the Transformer parameter gradients aggregation (TPGA) method, which proves the applicability of the core idea of CFGA/TPGA to mainstream feature extraction models. We achieve state-of-the-art FGIR performance on CUB-200-2011 dataset and CARS196 dataset.},
  archive      = {J_PR},
  author       = {Han Yu and Huibin Lu and Min Zhao and Zhuoyi Li and Guanghua Gu},
  doi          = {10.1016/j.patcog.2023.110248},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110248},
  shortjournal = {Pattern Recognition},
  title        = {Gradient aggregation based fine-grained image retrieval: A unified viewpoint for CNN and transformer},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Pan-sharpening via intrinsic decomposition knowledge
distillation. <em>PR</em>, <em>149</em>, 110247. (<a
href="https://doi.org/10.1016/j.patcog.2023.110247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep-learning-based pan-sharpening strategies mainly involve the fusion of panchromatic and multispectral (MS) information at both the pixel and feature levels. In this paper, we hypothesize that the MS image can be expressed as the multiplication of reflectance and illumination components, and that the reflection components of low-resolution (LR) MS and high-resolution (HR) MS images are invariant. Here, the spectral reflection component can effectively describe the spectral response of an object, while the illumination component can effectively describe its texture. Based on this hypothesis, we propose a novel and concise pan-sharpening framework called intrinsic decomposition knowledge distillation . Specifically, the teacher network decomposes the HR MS image into reflectance and illumination components, which are then combined in the student network with the reflectance component and the enhanced illumination component from LR MS to reconstruct the pan-sharpened image. To approximate the component distributions from the teacher network, we introduce a novel three-stage knowledge distillation strategy that can transfer knowledge about the relationships between components and constrain the student network. Our quantitative and qualitative comparisons demonstrate the reasonableness of our hypothesis and the effectiveness of our proposed method in significantly improving perception quality.},
  archive      = {J_PR},
  author       = {Jiaming Wang and Qiang Zhou and Xiao Huang and Ruiqian Zhang and Xitong Chen and Tao Lu},
  doi          = {10.1016/j.patcog.2023.110247},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110247},
  shortjournal = {Pattern Recognition},
  title        = {Pan-sharpening via intrinsic decomposition knowledge distillation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source-free domain adaptation with unrestricted source
hypothesis. <em>PR</em>, <em>149</em>, 110246. (<a
href="https://doi.org/10.1016/j.patcog.2023.110246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to bridge the distribution discrepancy across different domains and improve the generalization ability of learning models on the target domain. The existing domain adaptation approaches align the distribution shift via adversarial training on the source and target data. In practice, however, the source data is usually unavailable due to the privacy factor. In this work, we mainly focus on the source-free domain adaptation setting, in which we are only accessible to the model trained on the source data and the unlabeled target data. To this end, we propose the Source-Free Adversarial Domain Adaptation (SFADA) approach to align the distribution of the target domain data in the absence of source domain data . In particular, we develop an effective metric to measure the domain discrepancy by introducing the proxy data of the source domain. To generate the proxy data, our approach retrieves target data which lie over the intersection of the supports of the source and target domains. We also derive the learning bound of the source-free domain adaptation theoretically and show that our proposed SFADA approach is capable of reducing the bound effectively. Additionally, instead of modifying the source model in previous source-free approaches, our SFADA does not require training the source model with specific restrictions (i.e., normalizing the classifier weight) for practice and privacy-related concerns. State-of-the-art results are achieved for different standard domain adaptation benchmarks. The code can be available from https://github.com/tiggers23/SFADA-main .},
  archive      = {J_PR},
  author       = {Jiujun He and Liang Wu and Chaofan Tao and Fengmao Lv},
  doi          = {10.1016/j.patcog.2023.110246},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110246},
  shortjournal = {Pattern Recognition},
  title        = {Source-free domain adaptation with unrestricted source hypothesis},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Content temporal relation network for temporal action
proposal generation. <em>PR</em>, <em>149</em>, 110245. (<a
href="https://doi.org/10.1016/j.patcog.2023.110245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action proposal generation is an essential step for untrimmed video analysis and gains much attention from academia. However, most of the prior works predict the confidence score of each proposal separately and neglect the relations between proposals, limiting their performance. In this work, we design a novel Content Temporal Relation Network (CTRNet) to generate temporal action proposals by exploring the content and temporal semantic relations between proposals simultaneously. Specifically, we design a proposal feature map generation layer to convert the temporal semantic relations of proposals into spatial relations . Based on the proposal feature map, we propose a content-temporal relation module, which applies a novel adaptive-dilated convolution to model the temporal semantic relations between proposals and designs a content-adaptive convolution operation to explore the content semantic relation between proposals. Considering the temporal and content semantic relations between proposals, CTRNet has learned discriminative proposal features to improve performance. Extensive experiments are performed on two mainstream temporal action detection datasets, and CTRNet significantly outperforms the previous state-of-the-art methods. The codes are available at https://github.com/YanZhang-bit/CTRNet .},
  archive      = {J_PR},
  author       = {Ming-Gang Gan and Yan Zhang},
  doi          = {10.1016/j.patcog.2023.110245},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110245},
  shortjournal = {Pattern Recognition},
  title        = {Content temporal relation network for temporal action proposal generation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class-aware mask-guided feature refinement for scene text
recognition. <em>PR</em>, <em>149</em>, 110244. (<a
href="https://doi.org/10.1016/j.patcog.2023.110244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition is a rapidly developing field that faces numerous challenges due to the complexity and diversity of scene text, including complex backgrounds, diverse fonts, flexible arrangements, and accidental occlusions. In this paper, we propose a novel approach called Class-Aware Mask-guided feature refinement (CAM) to address these challenges. Our approach introduces canonical class-aware glyph masks generated from a standard font to effectively suppress background and text style noise, thereby enhancing feature discrimination. Additionally, we design a feature alignment and fusion module to incorporate the canonical mask guidance for further feature refinement for text recognition. By enhancing the alignment between the canonical mask feature and the text feature, the module ensures more effective fusion, ultimately leading to improved recognition performance. We first evaluate CAM on six standard text recognition benchmarks to demonstrate its effectiveness. Furthermore, CAM exhibits superiority over the state-of-the-art method by an average performance gain of 4.1% across six more challenging datasets, despite utilizing a smaller model size. Our study highlights the importance of incorporating canonical mask guidance and aligned feature refinement techniques for robust scene text recognition. Code will be available at https://github.com/MelosY/CAM .},
  archive      = {J_PR},
  author       = {Mingkun Yang and Biao Yang and Minghui Liao and Yingying Zhu and Xiang Bai},
  doi          = {10.1016/j.patcog.2023.110244},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110244},
  shortjournal = {Pattern Recognition},
  title        = {Class-aware mask-guided feature refinement for scene text recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICausalOSR: Invertible causal disentanglement for open-set
recognition. <em>PR</em>, <em>149</em>, 110243. (<a
href="https://doi.org/10.1016/j.patcog.2023.110243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite recent developments that allowed neural networks to achieve impressive performance in a wide range of recognition, these models are intrinsically challenged by real-world applications. Open-set recognition is introduced to facilitate the development of recognition systems towards real-world applications, for this it has to deal with the issue caused by discriminative features . Such features arise from closed-set training and tend to partition the full input space into the closed set of target classes. To reduce the issue, we present an invertible model, iCausalOSR, to learn invertible causal disentanglement to reveal the essence of classes for open-set recognition. The invertible model consists of an encoder and class functions, wherein the class functions are responsible to model the known classes, and the encoder is responsible for progressive signal separation and contraction. A contrast strategy is designed to couple the encoder and class functions to learn invertible causal disentanglement. The dual properties of the model, causal disentanglement and invertibility , constitute the key elements in revealing the class essence. Experiments on widely-used standard datasets in open-set recognition demonstrate the superior performance of our model.},
  archive      = {J_PR},
  author       = {Fenglei Yang and Baomin Li and Jingling Han},
  doi          = {10.1016/j.patcog.2023.110243},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110243},
  shortjournal = {Pattern Recognition},
  title        = {ICausalOSR: Invertible causal disentanglement for open-set recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale architectures matter: Examining the adversarial
robustness of flow-based lossless compression. <em>PR</em>,
<em>149</em>, 110242. (<a
href="https://doi.org/10.1016/j.patcog.2023.110242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exact likelihood estimation on entropy coding makes flow-based models appealing for lossless image compression . However, the high fidelity storage cost is affected by the lossless compression ratio. The trade-off between efficiency and robustness of flow-based deep lossless compression models has not been fully explored. This paper characterizes the trade-off theoretically and empirically, revealing that flow-based models are susceptible to adversarial examples resulting in a significant change in compression ratio. The fragile robustness of flow-based models is due to their intrinsic multi-scale architectures lacking the Lipschitz property. Based on this insight, a stronger white-box attack, Auto-Weighted Projected Gradient Descent (AW-PGD), is developed to generate more universal adversarial examples. Additionally, a novel flow-based lossless compression model , Robust Integer Discrete Flow (R-IDF), is proposed to achieve comparable robustness to adversarial training without sacrificing compression efficiency . Experiments demonstrate that the PGD algorithm falls into local extreme values when attacking the compression model, but the proposed attack and defense methods effectively improve the invulnerability of the flow-based compression model.},
  archive      = {J_PR},
  author       = {Yichong Xia and Bin Chen and Yan Feng and Tianshuo Ge and Yujun Huang and Haoqian Wang and Yaowei Wang},
  doi          = {10.1016/j.patcog.2023.110242},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110242},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale architectures matter: Examining the adversarial robustness of flow-based lossless compression},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust low tubal rank tensor recovery via L2E criterion.
<em>PR</em>, <em>149</em>, 110241. (<a
href="https://doi.org/10.1016/j.patcog.2023.110241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor analysis has received enormous attention as the increasing prevalence of high-dimensional data in science research and engineering application . Tensor recovery is an important and meaningful problem for tensor analysis. It aims to complete a tensor from an observed subset of its entries disturbed by noise. However, classical methods either develop on the second-order statistics or Lasso-type penalty, leading to them not effectively dealing with gross or dense noise effectively. To address such issues, we propose a robust tensor recovery model for simultaneously completing a low tubal rank tensor with complex noise and missing data. Based on tensor–tensor product (t-product), we first develop a tensor factor Frobenius norm to exploit the low tubal rank property which is closely related to tensor nuclear norm and tubal rank. By utilizing the robust L 2 L2 criterion, we derive the nonconvex objective function to accommodate the low tubal rank tensor. An implementable alternating minimization algorithm has been developed to estimate the low tubal rank tensor. It is worth noting that our method is able to jointly estimate the precision parameter to capture the hidden complex noise pattern. Furthermore, some convergence properties of the proposed algorithm are presented. A series of numerical experiments are conducted on both synthetic and real-world data to demonstrate the effectiveness and robustness of the proposed approach in comparison with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Zihao Song and Xiangjian Xu and Heng Lian and Weihua Zhao},
  doi          = {10.1016/j.patcog.2023.110241},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110241},
  shortjournal = {Pattern Recognition},
  title        = {Robust low tubal rank tensor recovery via L2E criterion},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network characteristics adaption and hierarchical feature
exploration for robust object recognition. <em>PR</em>, <em>149</em>,
110240. (<a href="https://doi.org/10.1016/j.patcog.2023.110240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in deep networks have achieved appealing performances on object recognition tasks , due to their robust feature learning abilities. Besides the generated deep features, other network characteristics, e.g., inter-layer weight matrix and their back-propagated derivatives, may behave complementarily in feature learning in terms of generalization and robustness performances. However, characteristics adaptivity to different databases is not well studied. Meanwhile, current algorithms are apt to explore the most salient features for better generalization performance , while the hierarchically-salient features that may be beneficial for network robustness are not fully explored. Thus, we propose an attention module to make network characteristics adaptive to different training tasks, which can be further combined with the dynamic dropout algorithm to suppress salient neurons to explore more SndMS (Second Most Salient) features for robust recognition. The proposed algorithm has two main merits. First, the complementarity of network characteristics is taken into account when conducting training on different databases; Second, with the exploration of more SndMS neurons for hierarchically-salient feature representation learning, the network robustness against adversarial perturbations or fine-grained differences can be enhanced. The extensive experiments on seven public databases show that the proposed attention-based dropout largely improves the network robustness, without compromising the generalization performance, compared with related variants and state-of-the-art (SOTA) algorithms. Algorithm codes are available at https://github.com/lingjivoo/ACAD .},
  archive      = {J_PR},
  author       = {Weicheng Xie and Cheng Luo and Gui Wang and Linlin Shen and Zhihui Lai and Siyang Song},
  doi          = {10.1016/j.patcog.2023.110240},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110240},
  shortjournal = {Pattern Recognition},
  title        = {Network characteristics adaption and hierarchical feature exploration for robust object recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label contrastive hashing. <em>PR</em>, <em>149</em>,
110239. (<a href="https://doi.org/10.1016/j.patcog.2023.110239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint learning of image representation and hash encoding represents a dominant solution to approximate nearest neighbor search for large-scale image retrieval . Despite significant advances in deep learning to hash in multi-label setting, optimization of semantic similarity-preserving representations with minimal quantization error remains challenging. Motivated by the recent success of contrastive representation learning in various vision tasks, this article introduces a Multi-label Contrastive Hashing (MCH) method for large-scale multi-label image retrieval. We extend the image similarity modeling in existing supervised contrastive loss from binary to multi-level structure, by which multi-level semantic similarity between multi-label images can be well modeled and captured in learning to hash. Despite the appealing properties of contrastive learning , directly adapting it into multi-label hashing is non-trivial, as the quantization loss may restricts the optimization of the multi-level contrastive loss, degrading the multi-level similarity-preserving hashing. To this end, we design a curriculum strategy to adaptively adjust the weight of quantization loss by leveraging the historical quantization deviations during training, such that the multi-level semantic similarity can be well preserved with progressively reduced quantization deviation. We conduct extensive experiments on three benchmark datasets including MirFlickr25k, NUS-WIDE, and IAPRTC-12. The results indicate the effectiveness of our approach, outperforming several state-of-the-art solutions for hashing-based multi-label image retrieval.},
  archive      = {J_PR},
  author       = {Zeqiang Wei and Kai Jin and Zheng Zhang and Xiuzhuang Zhou},
  doi          = {10.1016/j.patcog.2023.110239},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110239},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label contrastive hashing},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive instance similarity embedding for online continual
learning. <em>PR</em>, <em>149</em>, 110238. (<a
href="https://doi.org/10.1016/j.patcog.2023.110238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the online continual learning (CL) paradigm, where the learner must continually learn a sequence of tasks. In this setting, improving the learning ability of the model and mitigating catastrophic forgetting are two pivotal factors. Note that most existing approaches for online continual learning are based on the experience replay strategy. In this type of method, a memory buffer is applied to store a subset of previous tasks to prevent catastrophic forgetting. The samples from between the current task and the memory buffer are jointly trained to update the network parameters. Consider that most methods only generate the feature embeddings via a shared feature extractor and then train the network via cross-entropy loss. We argue that such methods fail to explore the feature embedding in its entirety and neglect the similar relations between samples, thus leading to lower discriminant performance, especially in an online learning setting. To this end, we propose the Adaptive Instance Similarity Embedding for Online Continual Learning (AISEOCL) framework, which further takes all the sample relations in a given batch into account. In detail, firstly, the experience replay strategy is used to avoid catastrophic forgetting. Then, during training, we apply the adaptive similar embedding to obtain additional valuable similar information from the current training samples composed of the current and previous tasks. Since not all samples are equally important to make a prediction, we further weigh the importance of each instance accordingly resorting to the attention mechanism. Importantly, we further impose a similarity distillation loss on the distributions of the similarity relationship between current and previous models. Such operation can transfer the similarity relationship between different samples from the old model to the current model to alleviate catastrophic forgetting. With this strategy, AISEOCL can further improve the learning ability of the model while enhancing the discriminant power, which is also beneficial to stably resist forgetting. The experiments on several existing benchmarks validate the effectiveness of our proposed approach.},
  archive      = {J_PR},
  author       = {Ya-nan Han and Jian-wei Liu},
  doi          = {10.1016/j.patcog.2023.110238},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110238},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive instance similarity embedding for online continual learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised spatial self-similarity difference-based change
detection method for multi-source heterogeneous images. <em>PR</em>,
<em>149</em>, 110237. (<a
href="https://doi.org/10.1016/j.patcog.2023.110237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source heterogeneous change detection has been widely used in dynamic disaster monitoring, land cover updating, etc. Various methods have been proposed to make heterogeneous data comparable. However, heterogeneous images are difficult to compare directly and may be affected by noise. Most existing methods obtain change information through mapping and regression, lacking the utilisation of image spatial information and a comprehensive portrayal of the changes, which may affect change detection results. To address these challenges, we propose an unsupervised spatial self-similarity difference-based change detection (USSD) method for multi-source heterogeneous images to evaluate the similarity of spatial relationships in heterogeneous images. First, the images are divided into image blocks to construct spatial self-difference images between individual image blocks aiming to make the data comparable. Second, the change information is portrayed in terms of both the magnitude differences and similarity differences to obtain a more comprehensive spatial self-difference change magnitude map. Then, the spatial neighbourhood information of the spatial self-difference change magnitude map is considered to avoid noise. Experimental results on six open datasets indicate that the overall accuracy of the USSD method was approximately 85%–95%. This method improves the change magnitude map discrimination, better detects the change region, and avoids noise in synthetic aperture radar images.},
  archive      = {J_PR},
  author       = {Linye Zhu and Wenbin Sun and Deqin Fan and Huaqiao Xing and Xiaoqi Liu},
  doi          = {10.1016/j.patcog.2023.110237},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110237},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised spatial self-similarity difference-based change detection method for multi-source heterogeneous images},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A max-relevance-min-divergence criterion for data
discretization with applications on naive bayes. <em>PR</em>,
<em>149</em>, 110236. (<a
href="https://doi.org/10.1016/j.patcog.2023.110236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many classification models , data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance . As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme . The proposed MDmD criterion is technically appealing, but it is difficult to reliably estimate the high-order joint distributions of attributes and the classification variable. We hence further propose a more practical solution, Max-Relevance-Min-Divergence (MRmD) discretization scheme, where each attribute is discretized separately, by simultaneously maximizing the discriminant information and the generalization ability of the discretized data. The proposed MRmD is compared with the state-of-the-art discretization algorithms under the naive Bayes classification framework on 45 benchmark datasets. It significantly outperforms all the compared methods on most of the datasets.},
  archive      = {J_PR},
  author       = {Shihe Wang and Jianfeng Ren and Ruibin Bai and Yuan Yao and Xudong Jiang},
  doi          = {10.1016/j.patcog.2023.110236},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110236},
  shortjournal = {Pattern Recognition},
  title        = {A max-relevance-min-divergence criterion for data discretization with applications on naive bayes},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Closed-loop unified knowledge distillation for dense object
detection. <em>PR</em>, <em>149</em>, 110235. (<a
href="https://doi.org/10.1016/j.patcog.2023.110235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of knowledge distillation methods for object detection are feature-based and have achieved competitive results. However, only distillating in feature imitation part does not take full advantage of more sophisticated detection head design for object detection, especially dense object detection. In this paper, a triple parallel distillation (TPD) is proposed which can efficiently transfer all the output response in detection head from teacher to student. Moreover, to overcome the drawback of simply combining the feature-based with the response-based distillation with limited effect enhancement. A hierarchical re-weighting attention distillation (HRAD) is proposed to make student learn more than the teacher in feature information, as well as reciprocal feedback between the classification-IoU joint representation of detection head and the attention-based feature. By jointly interacting the benefits of TPD and HRAD, a closed-loop unified knowledge distillation for dense object detection is proposed, which makes the feature-based and response-based distillation unified and complementary. Experiments on different benchmark datasets have shown that the proposed work is able to outperform other state-of-the-art distillation methods for dense object detection on both accuracy and robustness.},
  archive      = {J_PR},
  author       = {Yaoye Song and Peng Zhang and Wei Huang and Yufei Zha and Tao You and Yanning Zhang},
  doi          = {10.1016/j.patcog.2023.110235},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110235},
  shortjournal = {Pattern Recognition},
  title        = {Closed-loop unified knowledge distillation for dense object detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards fair and personalized federated recommendation.
<em>PR</em>, <em>149</em>, 110234. (<a
href="https://doi.org/10.1016/j.patcog.2023.110234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems have gained immense popularity in recent years for predicting users’ interests by learning embeddings. The majority of existing recommendation approaches, represented by graph neural network-based recommendation algorithms , rely on centralized storage of user-item graphs for model learning, which raises privacy issues in the process of collecting and sharing user data. Federated recommendation can mitigate privacy concerns by preventing the server from collecting sensitive data from clients while there still exist unfairness and personalization issues. To address these challenges, we propose a novel framework named F air and P ersonalized F ederated R ecommendation (FPFR). On the client-side, the soft attention mechanism is designed to learn the representation of user/item by combining interaction and attribute information, and the filter network is combined to better characterize user preferences. On the server-side, we cluster users into different groups and learn personalized models for each user. Then, we select representative users from each group to participate in the global model parameters update. Finally, the fairness of federated recommendation is implemented by adding the fairness constraint to recommendation loss. We conduct experiments on five real-world recommendation datasets, and the results demonstrate that the proposed FPFR not only balances group fairness and recommendation accuracy but also improves personalization.},
  archive      = {J_PR},
  author       = {Shanfeng Wang and Hao Tao and Jianzhao Li and Xinyuan Ji and Yuan Gao and Maoguo Gong},
  doi          = {10.1016/j.patcog.2023.110234},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110234},
  shortjournal = {Pattern Recognition},
  title        = {Towards fair and personalized federated recommendation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DA-tran: Multiphase liver tumor segmentation with a
domain-adaptive transformer network. <em>PR</em>, <em>149</em>, 110233.
(<a href="https://doi.org/10.1016/j.patcog.2023.110233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate liver tumor segmentation from multiphase CT images is a prerequisite for data-driven tumor analysis. This study presents a domain-adaptive transformer (DA-Tran) network to segment liver tumors from each CT phase. First, a DA module is designed to produce domain-adapted feature maps from noncontrast-enhanced (NC)-phase, arterial (ART)-phase, portal venous (PV)-phase, and delay-phase (DP) images. Then, these domain-adapted feature maps are integrated using 3D transformer blocks to catch patch-structured similarity information and global context attention. Finally, the attention fusion decoder (AFD) integrates features from different branches to generate a more refined prediction. Extensive experimental results demonstrate that DA-Tran achieves state-of-the-art tumor segmentation results, i.e., a Dice similarity coefficient (DSC) of 87.00% and a 95% Hausdorff distance (HD95) of 5.10 mm on a clinical dataset (DB1). Additionally, DA-Tran consistently outperforms other cutting-edge methods on another multiphase liver tumor dataset (DB2). The DA module and transformer blocks can boost the co-segmentation performance and make DA-Tran an ideal solution for multiphase liver tumor segmentation.},
  archive      = {J_PR},
  author       = {Yangfan Ni and Geng Chen and Zhan Feng and Heng Cui and Dimitris Metaxas and Shaoting Zhang and Wentao Zhu},
  doi          = {10.1016/j.patcog.2023.110233},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110233},
  shortjournal = {Pattern Recognition},
  title        = {DA-tran: Multiphase liver tumor segmentation with a domain-adaptive transformer network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A diagnostic report supervised deep learning model training
strategy for diagnosis of COVID-19. <em>PR</em>, <em>149</em>, 110232.
(<a href="https://doi.org/10.1016/j.patcog.2023.110232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 is a highly contagious infectious disease that necessitates timely assessment and effective diagnosis, although it is no longer a health emergency. Most existing computer-aided diagnosis systems for COVID-19 can achieve high accuracy, but they show insufficient generalization performance and weak interpretability. To address these issues, we propose diagnostic report supervised contrastive learning (DRSCL), a model training strategy that incorporates textual information from medical reports into model pretraining and then only transfers the pretrained image encoder into model inference. Due to the issue of data recurrence in medical diagnosis reports, which is common in the medical domain and can cause nonconvergence of the pretraining stage of DRSCL, we improve the loss function calculation of contrastive learning by integrating an operation to merge identical text or image features. In addition, for the fine-tuning and inference stage of DRSCL, we design a hierarchical fine-tuning strategy to better evaluate the pretraining performance and importance of each module. In case study, we build a medical image-text pair dataset of lung diseases for pretraining, with samples collected from hospitals in East China, and then conduct the fine-tuning and inference operation of DRSCL with a publicly available SARS-CoV-2 dataset. The comparative experimental results show that DRSCL helps all involved image encoders obtain better classification accuracy and superior generalization performance in the given COVID-19-related diagnostic application. This finding indicates that DRSCL enhances deep models to learn more deep information with supervision of medical textual information. Furthermore, we adopt the Grad-CAM method to visualize pretrained models, and the results demonstrate that the DRSCL strategy is advantageous for improving model interpretability.},
  archive      = {J_PR},
  author       = {Shiqi Deng and Xing Zhang and Shancheng Jiang},
  doi          = {10.1016/j.patcog.2023.110232},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110232},
  shortjournal = {Pattern Recognition},
  title        = {A diagnostic report supervised deep learning model training strategy for diagnosis of COVID-19},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TriSig: Evaluating the statistical significance of
triclusters. <em>PR</em>, <em>149</em>, 110231. (<a
href="https://doi.org/10.1016/j.patcog.2023.110231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor data analysis allows researchers to uncover novel patterns and relationships that cannot be obtained from tabular data alone. The information inferred from multi-way patterns can offer valuable insights into disease progression, bioproduction processes, behavioral responses, weather fluctuations, or social dynamics. However, spurious patterns often hamper this process. This work aims at proposing a statistical frame to assess the probability of patterns in tensor data to deviate from null expectations, extending well-established principles for assessing the statistical significance of patterns in tabular data. A principled discussion on binomial testing to mitigate false positive discoveries is entailed at the light of: variable dependencies, temporal associations and misalignments, and multi-hypothesis correction. Results gathered from the application of triclustering algorithms over distinct real-world case studies in biotechnological domains confer validity to the proposed statistical frame while revealing vulnerabilities of reference triclustering searches. The proposed assessment can be incorporated into existing triclustering algorithms to minimize spurious occurrences, rank patterns, and further prune the search space, reducing their computational complexity.},
  archive      = {J_PR},
  author       = {Leonardo Alexandre and Rafael S. Costa and Rui Henriques},
  doi          = {10.1016/j.patcog.2023.110231},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110231},
  shortjournal = {Pattern Recognition},
  title        = {TriSig: Evaluating the statistical significance of triclusters},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedNN: Federated learning on concept drift data using weight
and adaptive group normalizations. <em>PR</em>, <em>149</em>, 110230.
(<a href="https://doi.org/10.1016/j.patcog.2023.110230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) allows a global model to be trained without sharing private raw data. The major challenge in FL is client-wise data heterogeneity leading to different model convergence speed and accuracy. Despite the recent progress of FL, most methods verify their accuracy on prior probability shift (label distribution skew) dataset, while the concept drift problem (i.e., where each client has distinct styles of input while sharing the same labels) has not been explored. In real scenarios, concept drift is of paramount concern in FL since the client’s data is collected under extremely different conditions making FL optimization more challenging. Significant differences in inputs among clients exacerbate the heterogeneity of clients’ parameters compared to prior probability shift, ultimately resulting in failures for previous FL approaches . To address the challenge of concept drift, we use Weight Normalization (WN) and Adaptive Group Normalization (AGN) to alleviate conflicts during global model updates. WN re-parameterizes weights to have zero mean and unit variance while AGN adaptively selects the optimal mean and standard deviation for feature normalization based on the dataset. These two components significantly contribute to having consistent activations after global model updates reducing heterogeneity in concept drift data. Comprehensive experiments on seven datasets (with concept drift) demonstrate that our method outperforms five state-of-the-art FL methods and shows faster convergence speed compared to the previous FL methods.},
  archive      = {J_PR},
  author       = {Myeongkyun Kang and Soopil Kim and Kyong Hwan Jin and Ehsan Adeli and Kilian M. Pohl and Sang Hyun Park},
  doi          = {10.1016/j.patcog.2023.110230},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110230},
  shortjournal = {Pattern Recognition},
  title        = {FedNN: Federated learning on concept drift data using weight and adaptive group normalizations},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging tensor kernels to reduce objective function
mismatch in deep clustering. <em>PR</em>, <em>149</em>, 110229. (<a
href="https://doi.org/10.1016/j.patcog.2023.110229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective Function Mismatch (OFM) occurs when the optimization of one objective has a negative impact on the optimization of another objective. In this work we study OFM in deep clustering, and find that the popular autoencoder-based approach to deep clustering can lead to both reduced clustering performance, and a significant amount of OFM between the reconstruction and clustering objectives. To reduce the mismatch, while maintaining the structure-preserving property of an auxiliary objective, we propose a set of new auxiliary objectives for deep clustering, referred to as the Unsupervised Companion Objectives (UCOs). The UCOs rely on a kernel function to formulate a clustering objective on intermediate representations in the network. Generally, intermediate representations can include other dimensions, for instance spatial or temporal, in addition to the feature dimension. We therefore argue that the naïve approach of vectorizing and applying a vector kernel is suboptimal for such representations, as it ignores the information contained in the other dimensions. To address this drawback, we equip the UCOs with structure-exploiting tensor kernels, designed for tensors of arbitrary rank. The UCOs can thus be adapted to a broad class of network architectures . We also propose a novel, regression-based measure of OFM, allowing us to accurately quantify the amount of OFM observed during training. Our experiments show that the OFM between the UCOs and the main clustering objective is lower, compared to a similar autoencoder-based model. Further, we illustrate that the UCOs improve the clustering performance of the model, in contrast to the autoencoder-based approach. The code for our experiments is available at https://github.com/danieltrosten/tk-uco .},
  archive      = {J_PR},
  author       = {Daniel J. Trosten and Sigurd Løkse and Robert Jenssen and Michael Kampffmeyer},
  doi          = {10.1016/j.patcog.2023.110229},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110229},
  shortjournal = {Pattern Recognition},
  title        = {Leveraging tensor kernels to reduce objective function mismatch in deep clustering},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adaptive subspace representation from a geometric
intuition. <em>PR</em>, <em>149</em>, 110228. (<a
href="https://doi.org/10.1016/j.patcog.2023.110228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the unsupervised subspace learning task from a geometric intuition, which offers a direct understanding of the data representation . First, we consider all subspaces of the Euclidean space as a series of graded Grassmannian manifolds, and represent them by orbits of rotation group action. Then, we reformulate the unsupervised subspace learning by a least square problem with respect to rotation and projection operator. Second, we introduce a low-rank regularization to obtain a low-dimensional and robust subspace representation. Then, the model is translated into a minimization problem on the Grassmannian manifold. By dividing the model into two subproblems of solving the optimal rotation and subspace dimension, we design an alternating iteration strategy, where the locally geodesic structure of the rotation group and the unconstrained quadratic 0-1 programming are utilized. Finally, we apply the proposed method to the image classification problem on five benchmark datasets and compare it with nine state-of-the-art methods. Numerical results show that our proposed method has better feature representation ability and almost achieves the best performance in terms of classification accuracy and robustness.},
  archive      = {J_PR},
  author       = {Lipeng Cai and Jun Shi and Shaoyi Du and Yue Gao and Shihui Ying},
  doi          = {10.1016/j.patcog.2023.110228},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110228},
  shortjournal = {Pattern Recognition},
  title        = {Self-adaptive subspace representation from a geometric intuition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence-based dynamic cross-modal memory network for
image aesthetic assessment. <em>PR</em>, <em>149</em>, 110227. (<a
href="https://doi.org/10.1016/j.patcog.2023.110227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image aesthetic assessment (IAA) aims to design algorithms that can make human-like aesthetic decisions. Due to its high subjectivity and complexity, visual information alone is limited to fully predict the aesthetic quality of an image. More and more researchers try to use complementary information from user comments. However, user comments are not always available due to various technical and practical reasons. Therefore, it is necessary to find a way to reconstruct the missing textual information for aesthetic prediction with visual information only. This paper solves this problem by proposing a Confidence-based Dynamic Cross-modal Memory Network (CDCM-Net). Specifically, the proposed CDCM-Net consists of two key components: Visual and Textual Memory (VTM) network and Confidence-based Dynamical Multi-modal Fusion module (CDMF). VTM is based on the key–value memory network. It consists of a visual key memory and a textual value memory. The visual key memory learns the visual information. While the textual value memory learns to remember the textual feature and align them with the corresponding visual features. During inference, textual information can be reconstructed using only visual features. Furthermore, a CDMF module is introduced to perform trustworthy fusion. CDMF evaluates modality-level informativeness and then dynamically integrates reliable information. Extensive experiments are performed to demonstrate the superiority of the proposed method.},
  archive      = {J_PR},
  author       = {Xiaodan Zhang and Yuan Xiao and Jinye Peng and Xinbo Gao and Bo Hu},
  doi          = {10.1016/j.patcog.2023.110227},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110227},
  shortjournal = {Pattern Recognition},
  title        = {Confidence-based dynamic cross-modal memory network for image aesthetic assessment},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASFFuse: Infrared and visible image fusion model based on
adaptive selection feature maps. <em>PR</em>, <em>149</em>, 110226. (<a
href="https://doi.org/10.1016/j.patcog.2023.110226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers continuously modify deep learning network architecture for improved fusion results. However, little attention is given to the influence of noise feature maps generated during the convolution process on the fusion outcomes. Here, we aim to minimize the influence of noisy feature maps on fusion results and propose a fusion model, the infrared and visible image fusion model based on adaptive selection feature maps (ASFFuse). We propose an adaptive selection feature maps module (ASFM). ASFM measures the amount of information contained in each feature map and filters out feature maps that contain more noise information. Additionally, we introduce a feature enhancement module (FEM) to enrich the fusion image with more source image information . For unsupervised training of the proposed model, we propose a texture loss function based on contrast learning. This loss function preserves the texture information of the image in a better way and makes the fusion image have a better visual effect. Our ASFFuse model has been shown to outperform state-of-the-art models in both quantitative and qualitative evaluations in extensive experiments on the TNO and RoadScene datasets. The code is available at https://github.com/LKZ1584905069/ASFFuse .},
  archive      = {J_PR},
  author       = {Kuizhuang Liu and Min Li and Enguang Zuo and Chen Chen and Cheng Chen and Bo Wang and Yunling Wang and Xiaoyi Lv},
  doi          = {10.1016/j.patcog.2023.110226},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110226},
  shortjournal = {Pattern Recognition},
  title        = {ASFFuse: Infrared and visible image fusion model based on adaptive selection feature maps},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian process classification bandits. <em>PR</em>,
<em>149</em>, 110224. (<a
href="https://doi.org/10.1016/j.patcog.2023.110224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification bandits are multi-armed bandit problems whose task is to classify a given set of arms into either positive or negative class depending on whether the rate of the arms with the expected reward of at least h h is not less than w w for given thresholds h h and w w . We study a special classification bandit problem in which arms correspond to points x x in d d -dimensional real space with expected rewards f ( x ) f(x) which are generated according to a Gaussian process prior. We develop a framework algorithm for the problem using various arm selection policies and propose policies called FCB (Farthest Confidence Bound) and FTSV (Farthest Thompson Sampling Value). We show a smaller sample complexity upper bound for FCB than that for the existing algorithm of the level set estimation, in which whether f ( x ) f(x) is at least h h or not must be decided for every arm’s x x . Arm selection policies depending on an estimated rate of arms with mean rewards of at least h h are also proposed and shown to improve empirical sample complexity. According to our experimental results, the rate-estimation versions of FCB and FTSV, together with that of the popular active learning policy which selects the point with the maximum variance, outperform other policies for synthetic functions, and the rate-estimation version of FTSV is also the best performer for our real-world dataset.},
  archive      = {J_PR},
  author       = {Tatsuya Hayashi and Naoki Ito and Koji Tabata and Atsuyoshi Nakamura and Katsumasa Fujita and Yoshinori Harada and Tamiki Komatsuzaki},
  doi          = {10.1016/j.patcog.2023.110224},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110224},
  shortjournal = {Pattern Recognition},
  title        = {Gaussian process classification bandits},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic perceptive infrared and visible image fusion
transformer. <em>PR</em>, <em>149</em>, 110223. (<a
href="https://doi.org/10.1016/j.patcog.2023.110223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based fusion mechanisms have achieved sophisticated performance in the field of image fusion. However, most existing approaches focus on learning global and local features but seldom consider to modeling semantic information, which might result in inadequate source information preservation. In this work, we propose a semantic perceptive infrared and visible image fusion Transformer (SePT). The proposed SePT extracts local feature through convolutional neural network (CNN) based module and learns long-range dependency through Transformer based modules, and meanwhile designs two semantic modeling modules based on Transformer architecture to manage high-level semantic information. One semantic modeling module maps the shallow features of source images into deep semantic, the other learns the deep semantic information in different receptive fields. The final fused results are recovered from the combination of local feature, long-range dependency and semantic feature . Extensive comparison experiments demonstrate the superiority of SePT compare to other advanced fusion approaches.},
  archive      = {J_PR},
  author       = {Xin Yang and Hongtao Huo and Chang Li and Xiaowen Liu and Wenxi Wang and Cheng Wang},
  doi          = {10.1016/j.patcog.2023.110223},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110223},
  shortjournal = {Pattern Recognition},
  title        = {Semantic perceptive infrared and visible image fusion transformer},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gated cross-domain collaborative network for underwater
object detection. <em>PR</em>, <em>149</em>, 110222. (<a
href="https://doi.org/10.1016/j.patcog.2023.110222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater object detection (UOD) plays a significant role in aquaculture and marine environmental protection. Considering the challenges posed by low contrast and low-light conditions in underwater environments, several underwater image enhancement (UIE) methods have been proposed to improve the quality of underwater images. However, only using the enhanced images does not improve the performance of UOD, since it may unavoidably remove or alter critical patterns and details of underwater objects. In contrast, we believe that exploring the complementary information from the two domains is beneficial for UOD. The raw image preserves the natural characteristics of the scene and texture information of the objects, while the enhanced image improves the visibility of underwater objects. Based on this perspective, we propose a Gated Cross-domain Collaborative Network (GCC-Net) to address the challenges of poor visibility and low contrast in underwater environments, which comprises three dedicated components. Firstly, a real-time UIE method is employed to generate enhanced images, which can improve the visibility of objects in low-contrast areas. Secondly, a cross-domain feature interaction module is introduced to facilitate the interaction and mine complementary information between raw and enhanced image features . Thirdly, to prevent the contamination of unreliable generated results, a gated feature fusion module is proposed to adaptively control the fusion ratio of cross-domain information. Our method presents a new UOD paradigm from the perspective of cross-domain information interaction and fusion. Experimental results demonstrate that the proposed GCC-Net achieves state-of-the-art performance on four underwater datasets.},
  archive      = {J_PR},
  author       = {Linhui Dai and Hong Liu and Pinhao Song and Mengyuan Liu},
  doi          = {10.1016/j.patcog.2023.110222},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110222},
  shortjournal = {Pattern Recognition},
  title        = {A gated cross-domain collaborative network for underwater object detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class correlation correction for unbiased scene graph
generation. <em>PR</em>, <em>149</em>, 110221. (<a
href="https://doi.org/10.1016/j.patcog.2023.110221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-tail distribution in the scene graph generation (SGG) task has spurred immense interest in unbiased SGG. However, current state-of-the-art debiasing techniques extract statistics from the independent category, while ignoring the correlation between categories. To address this issue, we propose a simple but effective method, class correlation correction, to aggregate dependency knowledge among various classes. Specifically, given biased predictions, two kinds of debiasing transformations are developed employing the class correlation aware label to recover the unbiased estimates. We also propose to retrain SGG models with the biasing transformations adapted to the biased data distribution . The proposed debiasing method is evaluated using several biased datasets that are constructed from CIFAR-10 and Fashion-MNIST, as well as the commonly used SGG dataset and Caltech 101 dataset. Multiple evaluation metrics are used to assess the debiasing performance, and the results of extensive experiments show the effectiveness of the proposed method. The Pytorch® implementations can be downloaded from an open-source GitHub project https://github.com/Dlut-lab-zmn/class-correlation-correction .},
  archive      = {J_PR},
  author       = {Mengnan Zhao and Yuqiu Kong and Lihe Zhang and Baocai Yin},
  doi          = {10.1016/j.patcog.2023.110221},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110221},
  shortjournal = {Pattern Recognition},
  title        = {Class correlation correction for unbiased scene graph generation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A tree-based model with branch parallel decoding for
handwritten mathematical expression recognition. <em>PR</em>,
<em>149</em>, 110220. (<a
href="https://doi.org/10.1016/j.patcog.2023.110220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten mathematical expression recognition (HMER) is a challenging task in the field of computer vision due to the complex two-dimensional spatial structure and diverse handwriting styles of mathematical expressions (MEs). Recent mainstream approach treats MEs as objects with tree structures, modeled by sequence decoders or tree decoders. These decoders recognize the symbols and relationships between symbols in MEs in depth-first order, resulting in long decoding steps that can harm their performance, particularly for MEs with complex structures. In this paper, we propose a novel tree-based model with branch parallel decoding for HMER, which parses the structures of ME trees by explicitly predicting the relationships between symbols. In addition, a query constructing module is proposed to assist the decoder in decoding the branches of ME trees in parallel, thus reducing the number of decoding time steps and alleviating the problem of long sequence attention decoding. As a result, our model outperforms existing models on three widely-used benchmarks and demonstrates significant improvements in HMER performance.},
  archive      = {J_PR},
  author       = {Zhe Li and Wentao Yang and Hengnian Qi and Lianwen Jin and Yichao Huang and Kai Ding},
  doi          = {10.1016/j.patcog.2023.110220},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110220},
  shortjournal = {Pattern Recognition},
  title        = {A tree-based model with branch parallel decoding for handwritten mathematical expression recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view robust regression for feature extraction.
<em>PR</em>, <em>149</em>, 110219. (<a
href="https://doi.org/10.1016/j.patcog.2023.110219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Multi-view Discriminant Analysis (MVDA) has been proposed and achieves good performance in multi-view recognition tasks. However, as an extension of LDA , this method still suffers from the small-class problem and has the sensitivity to outliers. In order to address these drawbacks and achieve better performance on multi-view recognition tasks, we proposed Multi-view Robust Regression (MVRR) for multi-view feature extraction. MVRR is a regression based method that imposes L 2 , 1 L2,1 norm as the metric of the loss function and the regularization term to improve robustness and obtain jointly sparse projection matrices for effective feature extraction. Moreover, we incorporate an orthogonal matrix to regress the extracted features to their scaled label to avoid the small-class problem. Therefore, MVRR guarantees the projection matrix to break through the restriction of the number of class for solving the small-class problem. We also propose an iterative algorithm to compute the optimal solution of MVRR and the convergence of MVRR is proved. Experiments are conducted on four databases to verify the performance of MVRR and the result illustrates that MVRR is robust on multi-view feature extraction.},
  archive      = {J_PR},
  author       = {Zhihui Lai and Foping Chen and Jiajun Wen},
  doi          = {10.1016/j.patcog.2023.110219},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110219},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view robust regression for feature extraction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent linear discriminant analysis for feature extraction
via isometric structural learning. <em>PR</em>, <em>149</em>, 110218.
(<a href="https://doi.org/10.1016/j.patcog.2023.110218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is one of the most successful feature extraction methods, which projects high-dimensional data to a low-dimensional space with discriminative features . However, there are problems in the existing LDAs: (1) the effect of hidden data is not exploited in LDA, (2) the LDAs cannot preserve the local isometric structure, (3) there is no consideration for structural consistency that unifies the supervised global and unsupervised local information . In this paper, we propose a brand-new LDA method, namely, L atent L inear D iscriminant A nalysis with I sometric S tructural L earning ( L 2 L2 DA-ISL). We formulate LDA to a latent representation framework that considers both the discriminability from observed data and hidden data. Then, we propose isometric structural learning to capture the intrinsic local structural information. Lastly, we establish the concept of structural consistency in LDA framework. Extensive experiments and comparisons show that L 2 L2 DA-ISL achieves a promising performance with structural consistency and stronger robustness in feature extraction.},
  archive      = {J_PR},
  author       = {Jianhang Zhou and Qi Zhang and Shaoning Zeng and Bob Zhang and Leyuan Fang},
  doi          = {10.1016/j.patcog.2023.110218},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110218},
  shortjournal = {Pattern Recognition},
  title        = {Latent linear discriminant analysis for feature extraction via isometric structural learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bimodal SegNet: Fused instance segmentation using events and
RGB frames. <em>PR</em>, <em>149</em>, 110215. (<a
href="https://doi.org/10.1016/j.patcog.2023.110215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object segmentation enhances robotic grasping by aiding object identification. Complex environments and dynamic conditions pose challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Bimodal SegNet that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders — one for RGB signal input and another for Event signal input, in addition to an Atrous Pyramidal Feature Amplification module. Encoders capture and fuse the rich contextual information from different resolutions via a Cross-Domain Contextual Attention layer while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The results show a 4%–6% MIOU score improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The source code , dataset and model are publicly available at: https://github.com/sanket0707/Bimodal-SegNet .},
  archive      = {J_PR},
  author       = {Sanket Kachole and Xiaoqian Huang and Fariborz Baghaei Naeini and Rajkumar Muthusamy and Dimitrios Makris and Yahya Zweiri},
  doi          = {10.1016/j.patcog.2023.110215},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110215},
  shortjournal = {Pattern Recognition},
  title        = {Bimodal SegNet: Fused instance segmentation using events and RGB frames},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GaFL: Geometric-aware feature learning for universal 3D
models recognition. <em>PR</em>, <em>149</em>, 110214. (<a
href="https://doi.org/10.1016/j.patcog.2023.110214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods of 3D model recognition mainly depend on deep learning algorithms . However, the extracted deep feature lacks the geometric information of the models. Thus, these methods fail to identify rigid and non-rigid 3D models simultaneously, i.e., universal 3D models recognition. In this work, we propose a novel method, G eometric- a ware F eature L earning (GaFL), to further investigate the combination mechanism of geometric feature and deep learning in universal 3D models recognition. In GaFL , we design the Layer-Projected and Ray-Projected feature extraction policies to obtain depth values, which contain rich geometric information. Furthermore, the sphere convolution is proposed to guarantee the continuity and integrity of the ray feature when feeding into a deep network and the feature inactivation fusion module is designed to achieve the complementarity between layer and ray feature. Finally, the final merged feature vector contains enough geometric information as well as high-level semantic information, which are critical to universal 3D models recognition. In the experiments, GaFL achieves 95.0% and 95.2% classification accuracy in the rigid 3D models dataset ModelNet40 and the non-rigid 3D models dataset SHREC16, respectively, indicating that GaFL is powerful in universal 3D models recognition. Moreover, its significant advantage over state-of-the-art methods has also been validated on three other datasets, i.e., ShapeNet Core55, ScanObjectNN and SHREC15.},
  archive      = {J_PR},
  author       = {Yan Zhou and Huajie Sun and Huaidong Zhang and Xuemiao Xu and Chang’an Yi and Dewang Ye and Yuexia Zhou and Xiangyu Liu},
  doi          = {10.1016/j.patcog.2023.110214},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110214},
  shortjournal = {Pattern Recognition},
  title        = {GaFL: Geometric-aware feature learning for universal 3D models recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TFS-ViT: Token-level feature stylization for domain
generalization. <em>PR</em>, <em>149</em>, 110213. (<a
href="https://doi.org/10.1016/j.patcog.2023.110213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard deep learning models such as convolutional neural networks (CNNs) lack the ability of generalizing to domains which have not been seen during training. This problem is mainly due to the common but often wrong assumption of such models that the source and target data come from the same i.i.d. distribution. Recently, Vision Transformers (ViTs) have shown outstanding performance for a broad range of computer vision tasks . However, very few studies have investigated their ability to generalize to new domains. This paper presents a first Token-level Feature Stylization (TFS-ViT) approach for domain generalization, which improves the performance of ViTs to unseen data by synthesizing new domains. Our approach transforms token features by mixing the normalization statistics of images from different domains. We further improve this approach with a novel strategy for attention-aware stylization, which uses the attention maps of class (CLS) tokens to compute and mix normalization statistics of tokens corresponding to different image regions. The proposed method is flexible to the choice of backbone model and can be easily applied to any ViT-based architecture with a negligible increase in computational complexity . Comprehensive experiments show that our approach is able to achieve state-of-the-art performance on five challenging benchmarks for domain generalization, and demonstrate its ability to deal with different types of domain shifts. The implementation is available at this repository .},
  archive      = {J_PR},
  author       = {Mehrdad Noori and Milad Cheraghalikhani and Ali Bahri and Gustavo A. Vargas Hakim and David Osowiechi and Ismail Ben Ayed and Christian Desrosiers},
  doi          = {10.1016/j.patcog.2023.110213},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110213},
  shortjournal = {Pattern Recognition},
  title        = {TFS-ViT: Token-level feature stylization for domain generalization},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multimodal multiscale dynamic graph convolution networks
for stock price prediction. <em>PR</em>, <em>149</em>, 110211. (<a
href="https://doi.org/10.1016/j.patcog.2023.110211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting directional future stock price movements is very challenging due to the complex, stochastic, and evolving nature of the financial markets. Existing literature either neglects other timely and granular alternative data, such as media text data, or fails to extract and distill predictive multimodal features from the data. Moreover, the time-varying cross-sectional relations beyond sequential dependencies of stock prices are informative for forecasting price fluctuations, for which the modelling flexibility, however, is not adequate in most of the previous studies. In this paper, we propose a novel M ultiscal e Mu l tim o dal Dy namic G raph C onvolution N etwork (Melody-GCN) to address these issues in stock price prediction. It contains three core modules: (1) multimodal fusing-diffusing blocks that effectively integrate and align the numerical and textual features; (2) a multiscale architecture that extracts and refines temporal features via a fine-to-coarse descending path and a coarse-to-fine ascending path progressively; and (3) dynamic spatio-temporal graph convolutional layers that learn the complex and evolving stock relations not only in between industries and individual companies but also across time horizons. Extensive experimental results and trading simulations on two real-world datasets demonstrate the superior performance of our proposed approach beyond other state-of-the-art models.},
  archive      = {J_PR},
  author       = {Ruirui Liu and Haoxian Liu and Huichou Huang and Bo Song and Qingyao Wu},
  doi          = {10.1016/j.patcog.2023.110211},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110211},
  shortjournal = {Pattern Recognition},
  title        = {Multimodal multiscale dynamic graph convolution networks for stock price prediction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A contrastive variational graph auto-encoder for node
clustering. <em>PR</em>, <em>149</em>, 110209. (<a
href="https://doi.org/10.1016/j.patcog.2023.110209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational Graph Auto-Encoders (VGAEs) have been widely used to solve the node clustering task . However, the state-of-the-art methods have numerous challenges. First, existing VGAEs do not account for the discrepancy between the inference and generative models after incorporating the clustering inductive bias. Second, current models are prone to degenerate solutions that make the latent codes match the prior independently of the input signal (i.e., Posterior Collapse). Third, existing VGAEs overlook the effect of the noisy clustering assignments (i.e., Feature Randomness) and the impact of the strong trade-off between clustering and reconstruction (i.e., Feature Drift). To address these problems, we formulate a variational lower bound in a contrastive setting. Our lower bound is a tighter approximation of the log-likelihood function than the corresponding Evidence Lower BOund (ELBO). Thanks to a newly identified term, our lower bound can escape Posterior Collapse and has more flexibility to account for the difference between the inference and generative models . Additionally, our solution has two mechanisms to control the trade-off between Feature Randomness and Feature Drift. Extensive experiments show that the proposed method achieves state-of-the-art clustering results on several datasets. We provide strong evidence that this improvement is attributed to four aspects: integrating contrastive learning and alleviating Feature Randomness, Feature Drift, and Posterior Collapse.},
  archive      = {J_PR},
  author       = {Nairouz Mrabah and Mohamed Bouguessa and Riadh Ksantini},
  doi          = {10.1016/j.patcog.2023.110209},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110209},
  shortjournal = {Pattern Recognition},
  title        = {A contrastive variational graph auto-encoder for node clustering},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end model for multi-view scene text recognition.
<em>PR</em>, <em>149</em>, 110206. (<a
href="https://doi.org/10.1016/j.patcog.2023.110206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing applications of surveillance and monitoring such as person re-identification, vehicle re-identification and sports events tracking, the necessity of text detection and end-to-end recognition is also growing. Although the past deep learning-based models have addressed several challenges such as arbitrary-shaped text, multiple scripts, and variations in the geometric structure of characters, the scope of the models is limited to a single view. This paper presents an end-to-end model for text recognition through refining the multi-views of the same scene, which is called E2EMVSTR (End-to-End Model for Multi-View Scene Text Recognition). Considering the common characteristics shared in multi-view texts, we propose a cycle consistency pairwise similarity-based deep learning model to find texts more efficiently in three input views. Further, the extracted texts are supplied to a Siamese network and semi-supervised attention embedding combinational network for obtaining recognition results. The proposed model combines natural language processing and genetic algorithm models to restore missing character information and correct wrong recognition results. In experiments on our multi-view dataset and several benchmark datasets, the proposed method is proven effective compared to the state-of-the-art methods. The dataset and codes will be made available to the public upon acceptance.},
  archive      = {J_PR},
  author       = {Ayan Banerjee and Palaiahnakote Shivakumara and Saumik Bhattacharya and Umapada Pal and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2023.110206},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110206},
  shortjournal = {Pattern Recognition},
  title        = {An end-to-end model for multi-view scene text recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). U-transformer-based multi-levels refinement for weakly
supervised action segmentation. <em>PR</em>, <em>149</em>, 110199. (<a
href="https://doi.org/10.1016/j.patcog.2023.110199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action segmentation is a research hotspot in human action analysis, which aims to split videos into segments of different actions. Recent algorithms have achieved great success in modeling based on temporal convolution, but these methods weight local or global timing information through additional modules, ignoring the existing long-term and short-term information connections between actions. This paper proposes a U-Transformer structure based on multi-level refinement, introduces neighborhood attention to learn the neighborhood information of adjacent frames, and aggregates video frame features to effectively process long-term sequence information. Then a loss optimization strategy is proposed to smooth the original classification effect and generate a more accurate calibration sequence by introducing a pairing similarity optimization method based on deep feature learning . In addition, we propose a timestamp supervised training method to generate complete information for actions based on pseudo-label predictions for action boundary predictions. Experiments on three challenging action segmentation datasets, 50Salads, GTEA, and Breakfast, show that our model performs state-of-the-art models, and our weakly supervised model also performs comparably to fully supervised performance.},
  archive      = {J_PR},
  author       = {Xiao Ke and Xin Miao and Wenzhong Guo},
  doi          = {10.1016/j.patcog.2023.110199},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110199},
  shortjournal = {Pattern Recognition},
  title        = {U-transformer-based multi-levels refinement for weakly supervised action segmentation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). PDRLRR: A novel low-rank representation with projection
distance regularization via manifold optimization for clustering.
<em>PR</em>, <em>149</em>, 110198. (<a
href="https://doi.org/10.1016/j.patcog.2023.110198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-rank representation (LRR) method has attracted widespread attention due to its excellent performance in pattern recognition and machine learning . LRR-based variants have been proposed to solve the three existing problems in LRR: (1) the projection matrix is permanently fixed when dimensionality reduction techniques are adopted; (2) LRR fails to capture the local geometric structure; and (3) the solution deviates from the real low-rank solution. To address these problems, this paper proposes a low-rank representation with projection distance regularization (PDRLRR) via manifold optimization for clustering. In detail, we first introduce a low-dimensional projection matrix and a projection distance regularization term to fit the projected data automatically and capture the local structure of the data, respectively. Consequently, the projection matrix and representation matrix are obtained jointly. Then, we obtain a more accurate low-rank solution by minimizing the Schatten- p p norm instead of the nuclear norm . Next, the projection matrix is optimized through a generalized Stiefel manifold . Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Haoran Chen and Xu Chen and Hongwei Tao and Zuhe Li and Boyue Wang},
  doi          = {10.1016/j.patcog.2023.110198},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110198},
  shortjournal = {Pattern Recognition},
  title        = {PDRLRR: A novel low-rank representation with projection distance regularization via manifold optimization for clustering},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SED: Searching enhanced decoder with switchable skip
connection for semantic segmentation. <em>PR</em>, <em>149</em>, 110196.
(<a href="https://doi.org/10.1016/j.patcog.2023.110196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has shown excellent performance. However, existing semantic segmentation models rely heavily on pre-training on Image-Net or COCO and mainly focus on the designing of decoders. Directly training the encoder–decoder architecture search models from scratch to SOTA for semantic segmentation requires even thousands GPU days, which greatly limits the application of NAS. To address this issue, we propose a novel neural architecture Search framework for Enhanced Decoder (SED). Utilizing the pre-trained hand-designing backbone and the searching space composed of light-weight cells, SED searches for a decoder which can perform high-quality segmentation. Furthermore, we attach switchable skip connection operations to search space , expanding the diversity of possible network structure. The parameters of backbone and operations selected in searching phrase are copied to retraining process. As a result, searching, pruning and retraining can be done in just 1 day. The experimental results show that the SED proposed in this paper only needs 1/4 of the parameters and calculation in contrast to hand-designing decoder, and obtains higher segmentation accuracy on Cityscapes. Transferring the same decoder architecture to other datasets, such as: Pascal VOC 2012, Camvid, ADE20K proves the robustness of SED.},
  archive      = {J_PR},
  author       = {Xian Zhang and Zhibin Quan and Qiang Li and Dejun Zhu and Wankou Yang},
  doi          = {10.1016/j.patcog.2023.110196},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110196},
  shortjournal = {Pattern Recognition},
  title        = {SED: Searching enhanced decoder with switchable skip connection for semantic segmentation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSText v2: A comprehensive video text spotting dataset for
dense and small text. <em>PR</em>, <em>149</em>, 110177. (<a
href="https://doi.org/10.1016/j.patcog.2023.110177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, video text detection, tracking, and recognition in natural scenes are becoming very popular in the computer vision community. However, most existing algorithms and benchmarks focus on common text cases ( e.g., normal size, density) and single scenario, while ignoring extreme video text challenges, i.e., dense and small text in various scenarios. In this paper, we establish a video text reading benchmark, named DSText V2, which focuses on D ense and S mall text reading challenges in the video with various scenarios. Compared with the previous datasets, the proposed dataset mainly include three new challenges: (1) Dense video texts, a new challenge for video text spotters to track and read. (2) High-proportioned small texts, coupled with the blurriness and distortion in the video, will bring further challenges. (3) Various new scenarios, e.g., ‘Game’, ‘Sports’, etc. The proposed DSText V2 includes 140 video clips from 7 open scenarios, supporting three tasks, i.e., video text detection (Task 1), video text tracking (Task 2), and end-to-end video text spotting (Task 3). In this article, we describe detailed statistical information of the dataset, tasks, evaluation protocols, and the results summaries. Most importantly, a thorough investigation and analysis targeting three unique challenges derived from our dataset are provided, aiming to provide new insights. Moreover, we hope the benchmark will promise video text research in the community. DSText v2 is built upon DSText v1, which was previously introduced to organize the ICDAR 2023 competition for dense and small video text. The dataset website can be found at RRC or Zenodo .},
  archive      = {J_PR},
  author       = {Weijia Wu and Yiming Zhang and Yefei He and Luoming Zhang and Zhenyu Lou and Hong Zhou and Xiang Bai},
  doi          = {10.1016/j.patcog.2023.110177},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110177},
  shortjournal = {Pattern Recognition},
  title        = {DSText v2: A comprehensive video text spotting dataset for dense and small text},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D human pose estimation with single image and inertial
measurement unit (IMU) sequence. <em>PR</em>, <em>149</em>, 110175. (<a
href="https://doi.org/10.1016/j.patcog.2023.110175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional human pose estimation plays an important role in the field of computer vision , such as in healthcare, sports, activity recognition, motion capture, and augmented reality . However, monocular image or video based methods are sensitive to occlusions, while multi-view methods usually require enormous computation resources. Currently, inertial measurement unit (IMU)-based methods have begun to overcome the occlusion problem and can potentially achieve real-time inference. Yet, they still suffer from insufficient precision and scale drift error over time. In this paper, we propose a novel, efficient framework to fuse a single image with temporal sequence from IMU sensors to estimate human poses and reconstruct human shapes. Our method achieves 46 mm Mean Per Joint Positional Error (MPJPE) on the Total Capture dataset with 30 frames time segment, and surpasses state-of-the-art pure IMU-based methods. Moreover, in comparison with other vision-based methods, the proposed method shows great advantage in reducing computing floating point operations per second (FLOPS) quota while still achieving competitive estimation precision. Our method achieves 74 FPS on an IPhone 12 for offline processing. In addition, our method can easily be generalized for outdoor cases.},
  archive      = {J_PR},
  author       = {Liujun Liu and Jiewen Yang and Ye Lin and Peixuan Zhang and Lihua Zhang},
  doi          = {10.1016/j.patcog.2023.110175},
  journal      = {Pattern Recognition},
  month        = {5},
  pages        = {110175},
  shortjournal = {Pattern Recognition},
  title        = {3D human pose estimation with single image and inertial measurement unit (IMU) sequence},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Corrigendum to “GITGAN: Generative inter-subject transfer
for EEG motor imagery analysis” [pattern recognition 146 (2024) 110015].
<em>PR</em>, <em>148</em>, 110217. (<a
href="https://doi.org/10.1016/j.patcog.2023.110217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Kang Yin and Elissa Yanting Lim and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2023.110217},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110217},
  shortjournal = {Pattern Recognition},
  title        = {Corrigendum to “GITGAN: Generative inter-subject transfer for EEG motor imagery analysis” [Pattern recognition 146 (2024) 110015]},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic semantic structure distillation for low-resolution
fine-grained recognition. <em>PR</em>, <em>148</em>, 110216. (<a
href="https://doi.org/10.1016/j.patcog.2023.110216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-resolution images are ubiquitous in real applications such as surveillance and mobile photography. However, existing fine-grained approaches usually suffer catastrophic failures when dealing with low-resolution inputs. This is because their learning strategy inherently depends on the semantic structure of the pre-trained model, resulting in poor robustness and generalization. To mitigate this limitation, we propose a dynamic semantic structure distillation learning framework. Our method first facilitates knowledge distillation of diverse semantic structures by perturbing the composition of semantic components and then utilizes a decoupled distillation objective to prevent the loss of primary semantic part relation knowledge. We evaluate our proposed approach on two knowledge distillation tasks: high-to-low resolution and large-to-small model. The experimental results show that our proposed approach significantly outperforms existing methods in low-resolution fine-grained image classification tasks. This indicates that it can effectively distill knowledge from high-resolution teacher models to low-resolution student models. Furthermore, we demonstrate the effectiveness of our approach in general image classification and standard knowledge distillation tasks.},
  archive      = {J_PR},
  author       = {Mingjiang Liang and Shaoli Huang and Wei Liu},
  doi          = {10.1016/j.patcog.2023.110216},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110216},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic semantic structure distillation for low-resolution fine-grained recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mathematical formula detection in document images: A new
dataset and a new approach. <em>PR</em>, <em>148</em>, 110212. (<a
href="https://doi.org/10.1016/j.patcog.2023.110212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of existing mathematical formula detectors focus on detecting formula entities through object detection or instance segmentation techniques . However, these methods often fail to convey complete messages due to the absence of the contextual and layout information of mathematical formulas. For a more comprehensive understanding of mathematical formulas in document images , it is preferable to detect logical formula blocks that include one or multiple formula entities arranged in their natural reading order. These logical formula blocks enable the transmission of complete contextual messages of mathematical formulas and aid in the reconstruction of layout information of the document images, resulting in a more accurate mathematical formula detection. In this paper, we present a novel perspective on mathematical formula detection by framing it as a joint task of formula entity detection and formula relation extraction for identifying logical formula blocks. To this end, we introduce a new, large-scale dataset, called ArxivFormula, that includes well-annotated formula entity bounding boxes and formula relationships. We also propose a new approach, called FormulaDet, to address these two sub-tasks simultaneously. FormulaDet first employs a dynamic convolution-based formula entity detector, named DynFormula, to detect formula entities. It then uses a multi-modal transformer-based relation extraction method, named RelFormer, to group these detected formula entities into logical formula blocks. Extensive experiments on standard benchmarks in this field and the proposed dataset demonstrate that our FormulaDet can achieve significantly improved performance on formula entity detection and formula relation extraction compared to previous state-of-the-art methods. The joint detection and relation extraction approach provides a more thorough understanding of mathematical formulas in document images and effectively supports downstream tasks such as document layout analysis and scientific document digitization. The ArxivFormula dataset is publicly available at https://github.com/microsoft/ArxivFormula .},
  archive      = {J_PR},
  author       = {Kai Hu and Zhuoyao Zhong and Lei Sun and Qiang Huo},
  doi          = {10.1016/j.patcog.2023.110212},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110212},
  shortjournal = {Pattern Recognition},
  title        = {Mathematical formula detection in document images: A new dataset and a new approach},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNN-LoFI: A novel graph neural network through localized
feature-based histogram intersection. <em>PR</em>, <em>148</em>, 110210.
(<a href="https://doi.org/10.1016/j.patcog.2023.110210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks are increasingly becoming the framework of choice for graph-based machine learning . In this paper, we propose a new graph neural network architecture that substitutes classical message passing with an analysis of the local distribution of node features. To this end, we extract the distribution of features in the egonet for each local neighbourhood and compare them against a set of learned label distributions by taking the histogram intersection kernel. The similarity information is then propagated to other nodes in the network, effectively creating a message passing-like mechanism where the message is determined by the ensemble of the features. We perform an ablation study to evaluate the network’s performance under different choices of its hyper-parameters. Finally, we test our model on standard graph classification and regression benchmarks, and we find that it outperforms widely used alternative approaches, including both graph kernels and graph neural networks.},
  archive      = {J_PR},
  author       = {Alessandro Bicciato and Luca Cosmo and Giorgia Minello and Luca Rossi and Andrea Torsello},
  doi          = {10.1016/j.patcog.2023.110210},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110210},
  shortjournal = {Pattern Recognition},
  title        = {GNN-LoFI: A novel graph neural network through localized feature-based histogram intersection},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SideNet: Learning representations from interactive side
information for zero-shot chinese character recognition. <em>PR</em>,
<em>148</em>, 110208. (<a
href="https://doi.org/10.1016/j.patcog.2023.110208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing methods for zero-shot Chinese character recognition usually exploit a single type of side information such as radicals, glyphs, or strokes to establish a mapping with the input characters for the recognition of unseen categories. However, these approaches have two limitations. Firstly, the mappings are inefficient owing to their complexity. Some existing methods design radical-level mappings using a non-differentiable dictionary-matching strategy, whereas others construct sophisticated embeddings to map seen and unseen characters into a unified latent space. Although the latter approach is straightforward, it lacks a learnable scheme for explicit structure construction. Secondly, the complementarity within multiple types of side information has not been effectively explored. For example, the radicals provide structural knowledge at an abstract level, whereas glyphs offer detailed information on their figurative counterparts. To this end, we propose a new method called SideNet that jointly learns character-level representations assisted by two types of interactive side information: radicals and glyphs. SideNet contains a structural conversion module that extracts radical knowledge via dimensional decomposition, and a spatial conversion module that encodes the radical counting map to produce an interactive outcome between radicals and glyph. Finally, we propose a new classifier that integrates the converted features by a similarity-guided fusion mechanism. To the best of our knowledge, this study represents the first attempt to integrate these two types of side information and explore a joint representation for zero-shot learning. Experiments show that SideNet consistently outperforms existing methods by a significant margin in diverse scenarios, including handwriting, printed art, natural scenes, and ancient Chinese characters, which demonstrates the potential of joint learning with multiple types of side information.},
  archive      = {J_PR},
  author       = {Ziyan Li and Yuhao Huang and Dezhi Peng and Mengchao He and Lianwen Jin},
  doi          = {10.1016/j.patcog.2023.110208},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110208},
  shortjournal = {Pattern Recognition},
  title        = {SideNet: Learning representations from interactive side information for zero-shot chinese character recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-negative tucker decomposition with graph regularization
and smooth constraint for clustering. <em>PR</em>, <em>148</em>, 110207.
(<a href="https://doi.org/10.1016/j.patcog.2023.110207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative Tucker decomposition (NTD) and its graph regularized extensions are the most popular techniques for representing high-dimensional non-negative data, which are typically found in a low-dimensional sub-manifold of ambient space, from a geometric perspective. Therefore, the performance of the graph-based NTD methods relies heavily on the low-dimensional representation of the original data. However, most existing approaches treat the last factor matrix in NTD as a low-dimensional representation of the original data. This treatment leads to the loss of the original data’s multi-linear structure in the low-dimensional subspace. To remedy this defect, we propose a novel graph regularized L p Lp smooth NTD (GSNTD) method for high-dimensional data representation by incorporating graph regularization and an L p Lp smoothing constraint into NTD. The new graph regularization term constructed by the product of the core tensor and the last factor matrix in NTD, and it is used to uncover hidden semantics while maintaining the intrinsic multi-linear geometric structure of the data. The addition of the L p Lp smoothing constraint to NTD may produce a more accurate and smoother solution to the optimization problem . The update rules and the convergence of the GSNTD method are proposed. In addition, a randomized variant of the GSNTD algorithm based on fiber sampling is proposed. Finally, the experimental results on four standard image databases show that the proposed method and its randomized variant have better performance than some other state-of-the-art graph-based regularization methods for image clustering.},
  archive      = {J_PR},
  author       = {Qilong Liu and Linzhang Lu and Zhen Chen},
  doi          = {10.1016/j.patcog.2023.110207},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110207},
  shortjournal = {Pattern Recognition},
  title        = {Non-negative tucker decomposition with graph regularization and smooth constraint for clustering},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From heavy rain removal to detail restoration: A faster and
better network. <em>PR</em>, <em>148</em>, 110205. (<a
href="https://doi.org/10.1016/j.patcog.2023.110205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The profound accumulation of precipitation during intense rainfall events can markedly degrade the quality of images, leading to the erosion of textural details. Despite the improvements observed in existing learning-based methods specialized for heavy rain removal, it is discerned that a significant proportion of these methods tend to overlook the precise reconstruction of the intricate details. In this work, we introduce a simple dual-stage progressive enhancement network, denoted as DPENet, aiming to achieve effective deraining while preserving the structural accuracy of rain-free images. This approach comprises two key modules, a rain streaks removal network (R 2 2 Net) focusing on accurate rain removal, and a details reconstruction network (DRNet) designed to recover the textural details of rain-free images. Firstly, we introduce a dilated dense residual block (DDRB) within R 2 2 Net, enabling the aggregation of high-level and low-level features. Secondly, an enhanced residual pixel-wise attention block (ERPAB) is integrated into DRNet to facilitate the incorporation of contextual information. To further enhance the fidelity of our approach, we employ a comprehensive loss function that accentuates both the marginal and regional accuracy of rain-free images. Extensive experiments conducted on publicly available benchmarks demonstrates the noteworthy efficiency and effectiveness of our proposed DPENet. The source code and pre-trained models are currently available at https://github.com/chdwyb/DPENet .},
  archive      = {J_PR},
  author       = {Yuanbo Wen and Tao Gao and Jing Zhang and Kaihao Zhang and Ting Chen},
  doi          = {10.1016/j.patcog.2023.110205},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110205},
  shortjournal = {Pattern Recognition},
  title        = {From heavy rain removal to detail restoration: A faster and better network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the effectiveness of data augmentation from
similarity and diversity: An empirical study. <em>PR</em>, <em>148</em>,
110204. (<a href="https://doi.org/10.1016/j.patcog.2023.110204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation has emerged as a widely adopted technique for improving the generalization capabilities of deep neural networks . However, evaluating the effectiveness of data augmentation methods solely based on model training is computationally demanding and lacks interpretability . Moreover, the absence of quantitative standards hinders our understanding of the underlying mechanisms of data augmentation approaches and the development of novel techniques. To this end, we propose interpretable quantitative measures that decompose the effectiveness of data augmentation methods into two key dimensions: similarity and diversity. The proposed similarity measure describes the overall similarity between the original and augmented datasets, while the diversity measure quantifies the divergence in inherent complexity between the original and augmented datasets in terms of categories. Importantly, our proposed measures are model training-agnostic, ensuring efficiency in their calculation. Through experiments on several benchmark datasets, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, we demonstrate the efficacy of our measures in evaluating the effectiveness of various data augmentation methods. Furthermore, although the proposed measures are straightforward, they have the potential to guide the design and parameter tuning of data augmentation techniques and enable the validation of data augmentation methods’ efficacy before embarking on large-scale model training.},
  archive      = {J_PR},
  author       = {Suorong Yang and Suhan Guo and Jian Zhao and Furao Shen},
  doi          = {10.1016/j.patcog.2023.110204},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110204},
  shortjournal = {Pattern Recognition},
  title        = {Investigating the effectiveness of data augmentation from similarity and diversity: An empirical study},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prototype guided pseudo labeling and perturbation-based
active learning for domain adaptive semantic segmentation. <em>PR</em>,
<em>148</em>, 110203. (<a
href="https://doi.org/10.1016/j.patcog.2023.110203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work aims at active domain adaptation to transfer knowledge from a fully-labeled source domain to an entirely unlabeled target domain. During the active learning period, some pixels in the target domain are selected and annotated as active labels through several selection rounds. Such active labels can improve the target domain model performance greatly. However, existing approaches solely rely on pseudo labels, highly-confident classifier predictions on target images, to train the initial target domain model, resulting in a sub-optimal solution for model training. This initial model will be used for active label selection. Meanwhile, previous methods use entropy-based measurement to select pixels for annotation, which fails to detect high-confidence errors in earlier selection rounds due to the absence of target information . To address these issues, we propose a prototype-guided pseudo-label generating approach that leverages the relationships between source prototypes and target features. It generates target pseudo labels based on diverse source prototypes, thereby alleviating the issue of classifier predictions. Furthermore, perturbation-based uncertainty measurement, calculating the discrepancy between the target image and the augmented one, is introduced to find the areas with unstable predictions. Extensive experiments demonstrate that our approach outperforms state-of-the-art active domain adaptation methods on two benchmarks, GTAV → → Cityscapes, and SYNTHIA → → Cityscapes. Comparable performance is also achieved when compared to fully-supervised methods.},
  archive      = {J_PR},
  author       = {Junkun Peng and Mingjie Sun and Eng Gee Lim and Qiufeng Wang and Jimin Xiao},
  doi          = {10.1016/j.patcog.2023.110203},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110203},
  shortjournal = {Pattern Recognition},
  title        = {Prototype guided pseudo labeling and perturbation-based active learning for domain adaptive semantic segmentation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learnable support selection scheme for boosting few-shot
segmentation. <em>PR</em>, <em>148</em>, 110202. (<a
href="https://doi.org/10.1016/j.patcog.2023.110202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upon reevaluating recent studies of Few-Shot Segmentation (FSS), a key observation is that the random selection of support images is not always the optimal. In this situation, the support images cannot provide the useful guidance for the segmentation task . Therefore, we argue that a similarity-based support selection scheme, which selects support images according to the similarity between the query and candidate support images, is able to boost the performance of an FSS network . To this end, we propose a Siamese Support Selection Network (SSSN) which can be end-to-end trained along with an FSS network. We also leverage the joint utilization of a Convolutional Neural Network (CNN) and a Transformer network on top of a new feature fusion method to further improve the performance. To our knowledge, none of the similarity-based support selection scheme and the dual-stream network have been utilized for the FSS task before. Experimental results show that our FSS approach outperforms its counterparts on three data sets. In particular, the SSSN is able to boost the performance of an FSS network. We believe that these promising results should be due to the ability of the SSSN to select the top similar support images, which are useful for the FSS task. 1},
  archive      = {J_PR},
  author       = {Wenxuan Shao and Hao Qi and Xinghui Dong},
  doi          = {10.1016/j.patcog.2023.110202},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110202},
  shortjournal = {Pattern Recognition},
  title        = {A learnable support selection scheme for boosting few-shot segmentation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LC-MSM: Language-conditioned masked segmentation model for
unsupervised domain adaptation. <em>PR</em>, <em>148</em>, 110201. (<a
href="https://doi.org/10.1016/j.patcog.2023.110201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) is an important research topic in semantic segmentation tasks, wherein pixel-wise annotations are often difficult to collect in a test environment due to their high labeling costs. Previous UDA-based studies trained their segmentation networks using labeled synthetic data and unlabeled realistic data as source and target domains, respectively. However, they often fail to distinguish semantically similar classes, such as person vs. rider and road vs. sidewalk , because these classes are prone to confusion in domain-shifted environments. In this paper, we introduce a Language-Conditioned Masked Segmentation Model (LC-MSM), which is a new framework for the joint learning of context relations and domain-agnostic information for domain-adaptive semantic segmentation. Specifically, we reconstruct semantic labels with masked image conditions on the generalized text embeddings of the corresponding semantic class from OpenCLIP, which contains domain-invariant knowledge from large-scale data. To this end, we correlate the generalized text embeddings onto the per-pixel image feature of a masked image that learned the spatial context to further append domain-agnostic language information to the semantic decoder. This facilitates the generalization of our model to the target domain via the learning of context information within individual training instances, while considering cross-domain representations spanning the entire dataset. LC-MSM achieves an unprecedented UDA performance of 71.8 and 62.8 mIoU on GTA → → Cityscapes and SYNTHIA → → Cityscapes, respectively, which corresponds to an improvement of +3.5 and +1.9 percent points over the baseline method .},
  archive      = {J_PR},
  author       = {Young-Eun Kim and Yu-Won Lee and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2023.110201},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110201},
  shortjournal = {Pattern Recognition},
  title        = {LC-MSM: Language-conditioned masked segmentation model for unsupervised domain adaptation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IIOF: Intra- and inter-feature orthogonal fusion of local
and global features for music emotion recognition. <em>PR</em>,
<em>148</em>, 110200. (<a
href="https://doi.org/10.1016/j.patcog.2023.110200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose intra- and inter-feature orthogonal fusion (IIOF) of local and global features obtained from MS-SincResNet or MS-SSincResNet (a variant of MS-SincResNet) for music emotion recognition (MER). Given a raw waveform of music signal, MS-SincResNet/MS-SSincResNet is first used to learn several 2D representations having different receptive fields and obtain embeddings with time-frequency information from different layers. Then, local and global features are extracted from these embeddings. IIOF consisting of intra-feature OF and inter-feature OF is further employed to integrate both local and global features to obtain a discriminative descriptor for MER. The intra-feature OF is used to enhance the diversity of the global feature, and the inter-feature OF is utilized to reduce redundancies and produce complementary information between local and global features. The experimental results have demonstrated that the representation discriminability can be enhanced by IIOF considering the feature orthogonality . Furthermore, extensive experimental results have shown that the proposed method outperforms other state-of-the-art methods in terms of regression and classification tasks on the well-known MER datasets, including the DEAM dataset and the PMEmo dataset. The codes are available at https://github.com/PeiChunChang/MS-SSincResNet_with_IIOF .},
  archive      = {J_PR},
  author       = {Pei-Chun Chang and Yong-Sheng Chen and Chang-Hsing Lee},
  doi          = {10.1016/j.patcog.2023.110200},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110200},
  shortjournal = {Pattern Recognition},
  title        = {IIOF: Intra- and inter-feature orthogonal fusion of local and global features for music emotion recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Devil in the details: Delving into accurate quality scoring
for DensePose. <em>PR</em>, <em>148</em>, 110197. (<a
href="https://doi.org/10.1016/j.patcog.2023.110197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to score the quality of the network output is an essential but long-neglected problem in DensePose, which dramatically limits the potential of the existing methods. To fill the blank in the quality estimation of DensePose, we conduct rigorous experiments to clarify the key factors that accurately reflect the quality of DensePose results. We find that the accurate results already exist in the candidate pool but are mistakenly removed due to the inappropriate quality scores. To solve this problem, we proposed DensePose Scoring RCNN (DS RCNN), a simple and comprehensive quality estimation framework to learn the calibrated quality score and select high-quality results from the pool. DS RCNN introduces a quality scoring module (QSM) and a quality perception module (QPM) into the existing high-performance pipeline. The QSM scores the quality of DensePose results by fusing diverse quality information, and the QPM enhances the ability of quality perception by extracting instance-aware quality features guided by the predicted IUV maps. Benefiting from the superiority of QSM and QPM, DS RCNN outperforms baselines by up to 4.8 AP on the DensePose-COCO dataset.},
  archive      = {J_PR},
  author       = {Junyao Sun and Qiong Liu},
  doi          = {10.1016/j.patcog.2023.110197},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110197},
  shortjournal = {Pattern Recognition},
  title        = {Devil in the details: Delving into accurate quality scoring for DensePose},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSIR: Spatial shuffle multi-head self-attention for single
image super-resolution. <em>PR</em>, <em>148</em>, 110195. (<a
href="https://doi.org/10.1016/j.patcog.2023.110195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from the development of deep convolutional neural networks , CNN-based single-image super-resolution methods have achieved remarkable reconstruction results. However, the limited perceptual field of the convolutional kernel and the use of static weights in the inference process limit the performance of CNN-based methods. Recently, a few vision transformer-based image super-resolution methods have achieved excellent performance compared to CNN-based methods. These methods contain many parameters and require vast amounts of GPU memory for training. In this paper, we propose a spatial shuffle multi-head self-attention for single-image super-resolution that can significantly model long-range pixel dependencies without additional computational consumption. A local perception module is also proposed to combine convolutional neural networks’ local connectivity and translational invariance . Reconstruction results on five popular benchmarks show that the proposed method outperforms existing methods in both reconstruction accuracy and visual performance. The proposed method matches the performance of transformed-based methods but requires an inferior number of transformer blocks, which reduces the number of parameters by 40%, GPU memory by 30%, and inference time by 30% compared to transformer-based methods.},
  archive      = {J_PR},
  author       = {Liangliang Zhao and Junyu Gao and Donghu Deng and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.110195},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110195},
  shortjournal = {Pattern Recognition},
  title        = {SSIR: Spatial shuffle multi-head self-attention for single image super-resolution},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSPNet: Scale and spatial priors guided generalizable and
interpretable pedestrian attribute recognition. <em>PR</em>,
<em>148</em>, 110194. (<a
href="https://doi.org/10.1016/j.patcog.2023.110194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global feature based Pedestrian Attribute Recognition (PAR) models are often poorly localized when using Grad-CAM for attribute response analysis, which has a significant impact on the interpretability , generalizability and performance. Previous researches have attempted to improve generalization and interpretation through meticulous model design, yet they often have neglected or underutilized effective prior information crucial for PAR. To this end, a novel Scale and Spatial Priors Guided Network (SSPNet) is proposed for PAR, which is mainly composed of the Adaptive Feature Scale Selection (AFSS) and Prior Location Extraction (PLE) modules. The AFSS module learns to provide reasonable scale prior information for different attribute groups, allowing the model to focus on different levels of feature maps with varying semantic granularity . The PLE module reveals potential attribute spatial prior information, which avoids unnecessary attention on irrelevant areas and lowers the risk of model over-fitting. More specifically, the scale prior in AFSS is adaptively learned from different layers of feature pyramid with maximum accuracy, while the spatial priors in PLE can be revealed from part feature with different granularity (such as image blocks, human pose keypoint and sparse sampling points). Besides, a novel IoU based attribute localization metric is proposed for Weakly-supervised Pedestrian Attribute Localization (WPAL) based on the improved Grad-CAM for attribute response mask. The experimental results on the intra-dataset and cross-dataset evaluations demonstrate the effectiveness of our proposed method in terms of mean accuracy (mA). Furthermore, it also achieves superior performance on the PCS dataset for attribute localization in terms of IoU. Code will be released at https://github.com/guotengg/SSPNet .},
  archive      = {J_PR},
  author       = {Jifeng Shen and Teng Guo and Xin Zuo and Heng Fan and Wankou Yang},
  doi          = {10.1016/j.patcog.2023.110194},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110194},
  shortjournal = {Pattern Recognition},
  title        = {SSPNet: Scale and spatial priors guided generalizable and interpretable pedestrian attribute recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep federated learning hybrid optimization model based on
encrypted aligned data. <em>PR</em>, <em>148</em>, 110193. (<a
href="https://doi.org/10.1016/j.patcog.2023.110193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning can achieve multi-party data-collaborative applications while safeguarding personal privacy. However, the process often leads to a decline in the quality of sample data due to a substantial amount of missing encrypted aligned data, and there is a lack of research on how to improve the model learning effect by increasing the number of samples of encrypted aligned data in federated learning. Therefore, this paper integrates the functional characteristics of deep learning models and proposes a Variational AutoEncoder Gaussian Mixture Model Clustering Vertical Federated Learning Model (VAEGMMC-VFL), which leverages the feature extraction capability of the autoencoder and the clustering and pattern discovery capabilities of Gaussian mixture clustering on diverse datasets to further explore a large number of potentially usable samples. Firstly, the Variational AutoEncoder is used to achieve dimensionality reduction and sample feature reconstruction of high-dimensional data samples. Subsequently, Gaussian mixture clustering is further employed to partition the dataset into multiple potential Gaussian-distributed clusters and filter the sample data using thresholding. Additionally, the paper introduces a labeled sample attribute value finding algorithm to fill in attribute values for encrypted unaligned samples that meet the requirements, allowing for the full recovery of encrypted unaligned data. In the experimental section, the paper selects four sets of datasets from different industries and compares the proposed method with three federated learning clustering methods in terms of clustering loss, reconstruction loss, and other metrics. Tests on precision, accuracy, recall, ROC curve, and F1-score indicate that the proposed method outperforms similar approaches.},
  archive      = {J_PR},
  author       = {Zhongnan Zhao and Xiaoliang Liang and Hai Huang and Kun Wang},
  doi          = {10.1016/j.patcog.2023.110193},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110193},
  shortjournal = {Pattern Recognition},
  title        = {Deep federated learning hybrid optimization model based on encrypted aligned data},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A full-scale hierarchical encoder-decoder network with
cascading edge-prior for infrared and visible image fusion. <em>PR</em>,
<em>148</em>, 110192. (<a
href="https://doi.org/10.1016/j.patcog.2023.110192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing deep learning-based infrared and visible image fusion methods always fail to consider the full-scale long-range correlation and the prior knowledge, resulting in the fused images with low-contrast salient objects and blurred edge details. To overcome these drawbacks, a full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion is proposed. First, a top-down encoder extracts the hierarchical representations from source image. Then, to inject edge priors into the network and capture the progressive semantic correlations, a triple fusion mechanism is proposed including edge image fusion based on maximum fusion strategy, single-scale shallow layer fusion and full-scale semantic layer fusion based on dual-attention fusion (DAF) strategy. The fused full-scale semantic features (F2SF) are obtained by capturing the long-range affinities of the full-scale. At the same time, a cascading edge-prior branch (CEPB) is designed to embed the fused edge knowledge into fused single-scale shallow features, jointly guiding the decoder to focus on abundant details layer-by-layer on the basis of F2SF, thus recovering the edge and texture details of the fused image well. Finally, a novel loss function consisting of SSIM, intensity and edge loss is constructed to further maintain the network with better edge representation and reconstruction capability. Compared with existing state-of-the-art fusion methods, the proposed method has better performance in terms of both visual evaluation and objective evaluation on public datasets. The source code is available at https://github.com/lxq-jnu/FSFusion .},
  archive      = {J_PR},
  author       = {Xiaoqing Luo and Juan Wang and Zhancheng Zhang and Xiao-jun Wu},
  doi          = {10.1016/j.patcog.2023.110192},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110192},
  shortjournal = {Pattern Recognition},
  title        = {A full-scale hierarchical encoder-decoder network with cascading edge-prior for infrared and visible image fusion},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bounded exponential loss function based AdaBoost ensemble of
OCSVMs. <em>PR</em>, <em>148</em>, 110191. (<a
href="https://doi.org/10.1016/j.patcog.2023.110191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a commonly used ensemble method , AdaBoost has drawn much consideration in the field of machine learning . However, AdaBoost is highly sensitive to outliers. The performance of AdaBoost may be greatly deteriorated when the training samples are polluted by outliers. For binary and multi-class classifications, there have emerged many approaches to improving the robustness of AdaBoost against outliers. Unfortunately, there are too few researches on enhancing the robustness of AdaBoost against outliers in the case of one-class classification. In this study, the exponential loss function of AdaBoost is replaced by a more robust one to improve the anti-outlier ability of the conventional AdaBoost based ensemble of one-class support vector machines (OCSVMs). Furthermore, based on the redesigned loss function, the update formulae for the weights of base classifiers and the probability distribution of training samples are reformulated towards the AdaBoost ensemble of OCSVMs. The empirical error upper bound is derived from the theoretical viewpoint. Experimental outcomes upon the artificial and benchmark data sets show that the presented ensemble approach is more robust against outliers than its related methods.},
  archive      = {J_PR},
  author       = {Hong-Jie Xing and Wei-Tao Liu and Xi-Zhao Wang},
  doi          = {10.1016/j.patcog.2023.110191},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110191},
  shortjournal = {Pattern Recognition},
  title        = {Bounded exponential loss function based AdaBoost ensemble of OCSVMs},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSVT: Token sparsification vision transformer for robust
RGB-d salient object detection. <em>PR</em>, <em>148</em>, 110190. (<a
href="https://doi.org/10.1016/j.patcog.2023.110190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual transformer-based salient object detection (SOD) models have attracted increasing research attention. However, the existing transformer-based RGB-D SOD models usually operate on the full token sequences of RGB-D images and use an equal tokenization process to treat appearance and depth modalities, which leads to limited feature richness and inefficiency. To address these limitations, we present a novel token sparsification vision transformer architecture for RGB-D SOD, named TSVT, that explicitly extracts global-local multi-modality features with sparse tokens. The TSVT is an asymmetric encoder–decoder architecture with a dynamic sparse token encoder that adaptively selects and operates on sparse tokens, along with an multiple cascade aggregation decoder (MCAD) that predicts saliency results. Furthermore, we deeply investigate the differences and similarities between the appearance and depth modalities and develop an interactive diversity fusion module (IDFM) to integrate each pair of multi-modality tokens in different stages. Finally, to comprehensively evaluate the performance of the proposed model, we conduct extensive experiments on seven standard RGB-D SOD benchmarks in terms of five evaluation metrics . The experimental results reveal that the proposed model is more robust and effective than fifteen existing RGB-D SOD models. Moreover, the complexity of our model with the sparsification module is more than two times lower than that of the variant model without the dynamic sparse token module (DSTM).},
  archive      = {J_PR},
  author       = {Lina Gao and Bing Liu and Ping Fu and Mingzhu Xu},
  doi          = {10.1016/j.patcog.2023.110190},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110190},
  shortjournal = {Pattern Recognition},
  title        = {TSVT: Token sparsification vision transformer for robust RGB-D salient object detection},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-margin multiple kernel ℓp-SVDD using frank–wolfe
algorithm for novelty detection. <em>PR</em>, <em>148</em>, 110189. (<a
href="https://doi.org/10.1016/j.patcog.2023.110189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a variable ℓ p ≥ 1 ℓp≥1 -norm penalty on the slacks, the recently introduced ℓ p ℓp -norm Support Vector Data Description ( ℓ p ℓp -SVDD) method has improved the performance in novelty detection over the baseline approach, sometimes remarkably. This work extends this modelling formalism in multiple aspects. First, a large-margin extension of the ℓ p ℓp -SVDD method is formulated to enhance generalisation capability by maximising the margin between the positive and negative samples. Second, based on the Frank–Wolfe algorithm, an efficient yet effective method with predictable accuracy is presented to optimise the convex objective function in the proposed method. Finally, it is illustrated that the proposed approach can effectively benefit from a multiple kernel learning scheme to achieve state-of-the-art performance. The proposed method is theoretically analysed using Rademacher complexities to link its classification error probability to the margin and experimentally evaluated on several datasets to demonstrate its merits against existing methods.},
  archive      = {J_PR},
  author       = {Shervin Rahimzadeh Arashloo},
  doi          = {10.1016/j.patcog.2023.110189},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110189},
  shortjournal = {Pattern Recognition},
  title        = {Large-margin multiple kernel ℓp-SVDD using Frank–Wolfe algorithm for novelty detection},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-grained clip focus for skeleton-based action
recognition. <em>PR</em>, <em>148</em>, 110188. (<a
href="https://doi.org/10.1016/j.patcog.2023.110188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint-level and part-level information are crucial for modeling actions with different granularity . In addition, the relevant information on different joints between consecutive frames is very useful for skeleton-based action recognition. To effectively capture the action information, a new multi-grained clip focus network (MGCF-Net) is proposed. Firstly, the skeleton sequence is divided into multiple clips, each containing several consecutive frames. According to the structure of the human body, each clip is divided into several tuples. Then an intra-clip attention module is proposed to capture intra-clip action information. Specifically, multi-head self-attention is divided into two parts, obtaining relevant information at the joint and part levels, and integrating the information captured from these two parts to obtain multi-grained contextual features. In addition, an inter-clip focus module is used to capture the key information of several consecutive sub-actions, which will help to distinguish similar actions. On two large-scale benchmarks for skeleton-based action recognition, our method achieves the most advanced performance, and its effectiveness has been verified.},
  archive      = {J_PR},
  author       = {Helei Qiu and Biao Hou},
  doi          = {10.1016/j.patcog.2023.110188},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110188},
  shortjournal = {Pattern Recognition},
  title        = {Multi-grained clip focus for skeleton-based action recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised feature selection by learning exponential
weights. <em>PR</em>, <em>148</em>, 110183. (<a
href="https://doi.org/10.1016/j.patcog.2023.110183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection has gained considerable attention for extracting valuable features from unlabeled datasets. Existing approaches typically rely on sparse mapping matrices to preserve local neighborhood structures. However, this strategy favors large-weight features, potentially overlooking smaller yet valuable ones and distorting data distribution and feature structure. Besides, some methods focus on local structure information, failing to explore global information. To address these limitations, we introduce an exponential weighting mechanism to induce a rational feature distribution and explore data structure in the feature subspace. Specifically, we propose a unified framework incorporating local structure learning and exponentially weighted sparse regression for optimal feature combinations, preserving global and local information . Experimental results demonstrate the superiority of our approach over existing unsupervised feature selection methods.},
  archive      = {J_PR},
  author       = {Chenchen Wang and Jun Wang and Zhichen Gu and Jin-Mao Wei and Jian Liu},
  doi          = {10.1016/j.patcog.2023.110183},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110183},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised feature selection by learning exponential weights},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving augmentation consistency for graph contrastive
learning. <em>PR</em>, <em>148</em>, 110182. (<a
href="https://doi.org/10.1016/j.patcog.2023.110182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) enhances unsupervised graph representation by generating different contrastive views, in which properties of augmented nodes are required to be aligned with their anchors. However, we find that in some existing GCL methods, it is hard to inherit semantic and structural properties of graphs from anchor views due to inconsistent augmentation schemes, which may hurt node consistency in augmented views. In this paper, we present ConGCL to improve node consistency and enhance node classification . Specifically, we first consider context entailment, which integrates the semantic and structural properties to better mine the underlying consistency relationships of nodes. Beneficial from this, we then design a novel consistency improvement loss to maintain augmentation consistency agreement among positive node pairs under stochastic augmentation schemes. To investigate the effectiveness of ConGCL on improving augmentation consistency and enhancing node classification, we conduct empirical study and extensive experiments on benchmark datasets. The code is available at: https://github.com/brysonwx/ConGCL .},
  archive      = {J_PR},
  author       = {Weixin Bu and Xiaofeng Cao and Yizhen Zheng and Shirui Pan},
  doi          = {10.1016/j.patcog.2023.110182},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110182},
  shortjournal = {Pattern Recognition},
  title        = {Improving augmentation consistency for graph contrastive learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Japanese historical character recognition by focusing on
character parts. <em>PR</em>, <em>148</em>, 110181. (<a
href="https://doi.org/10.1016/j.patcog.2023.110181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Japanese historical documents provide valuable information. Character recognition is a critical technology for the digitalization of historical documents. Sample imbalance is a significant obstacle in recognizing Japanese historical characters, kuzushiji . Thousands of kuzushiji only have less than a few samples. Thus, recognition performance deteriorates greatly in kuzushiji with a few samples. In this study, we propose a framework for transferring knowledge of character parts from font to kuzushiji. The pretraining learns character parts from synthesized font images. However, fine-tuning to kuzushiji is more complex. We propose calculating a mean squared error loss between feature vectors of kuzushiji and font images, resulting in consistent feature vectors in kuzushiji and font. Consequently, we can perform zero-shot recognition for kuzushiji using the font images of zero-sampled kuzushiji. The experimental results show that the proposed method recognized zero-sampled kuzushiji at approximately 48% accuracy. Consequently, we significantly expand the number of recognizable kuzushiji.},
  archive      = {J_PR},
  author       = {Takuru Ishikawa and Tomo Miyazaki and Shinichiro Omachi},
  doi          = {10.1016/j.patcog.2023.110181},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110181},
  shortjournal = {Pattern Recognition},
  title        = {Japanese historical character recognition by focusing on character parts},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Oracle character recognition using unsupervised
discriminative consistency network. <em>PR</em>, <em>148</em>, 110180.
(<a href="https://doi.org/10.1016/j.patcog.2023.110180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ancient history relies on the study of ancient characters. However, real-world scanned oracle characters are difficult to collect and annotate, posing a major obstacle for oracle character recognition (OrCR). Besides, serious abrasion and inter-class similarity also make OrCR more challenging. In this paper, we propose a novel unsupervised domain adaptation method for OrCR, which enables to transfer knowledge from labeled handprinted oracle characters to unlabeled scanned data. We leverage pseudo-labeling to incorporate the semantic information into adaptation and constrain augmentation consistency to make the predictions of scanned samples consistent under different perturbations, leading to the model robustness to abrasion, stain and distortion. Simultaneously, an unsupervised transition loss is proposed to learn more discriminative features on the scanned domain by optimizing both between-class and within-class transition probability. Extensive experiments show that our approach achieves state-of-the-art result on Oracle-241 dataset and substantially outperforms the recently proposed structure-texture separation network by 15.1%.},
  archive      = {J_PR},
  author       = {Mei Wang and Weihong Deng and Sen Su},
  doi          = {10.1016/j.patcog.2023.110180},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110180},
  shortjournal = {Pattern Recognition},
  title        = {Oracle character recognition using unsupervised discriminative consistency network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep asymmetric nonnegative matrix factorization for graph
clustering. <em>PR</em>, <em>148</em>, 110179. (<a
href="https://doi.org/10.1016/j.patcog.2023.110179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering is a fundamental technique in machine learning that has widespread applications in various fields. Deep Nonnegative Matrix Factorization (DNMF) was recently emerged to cope with the extraction of several layers of features, and it has been demonstrated to achieve remarkable results on unsupervised tasks. While DNMF has been applied for analyzing graphs, the effectiveness of the current DNMF approaches for graph clustering is generally unsatisfactory: these methods are intrinsically data representation models, and their objective functions do not capture cluster structures, also ignores direction which is crucial in the directed graph clustering problems . To overcome these downsides, this paper proposes a graph-specific DNMF model based on the Asymmetric NMF which can handle undirected and directed graphs. Inspired by hierarchical graph clustering and graph summarization approaches, the Deep Asymmetric Nonnegative Matrix Factorization (DAsNMF) is introduced for the directed graph clustering problem. In a pseudo-hierarchical clustering setting, DAsNMF decomposes the input graph to extract low-level to high-level node representations and graph representations (summarized graphs). In addition, the asymmetric cosine and PageRank-based similarities are imposed on the proposed model to preserve the local and global graph structures. The learning process is formulated as a unified optimization problem to jointly train representation learning model and clustering model. The extensive experimental studies validate the effectiveness of the proposed method on directed graphs.},
  archive      = {J_PR},
  author       = {Akram Hajiveiseh and Seyed Amjad Seyedi and Fardin Akhlaghian Tab},
  doi          = {10.1016/j.patcog.2023.110179},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110179},
  shortjournal = {Pattern Recognition},
  title        = {Deep asymmetric nonnegative matrix factorization for graph clustering},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable multidisease diagnosis and label noise
detection based on a matching network and self-paced learning.
<em>PR</em>, <em>148</em>, 110178. (<a
href="https://doi.org/10.1016/j.patcog.2023.110178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the extensive use of information systems in hospitals, a large quantity of electronic medical record data has been accumulated, which makes it possible to train clinical decision support systems based on the data. However, electronic medical records are written by doctors of different levels, which easily introduces label noise into the datasets. The lack of interpretability of current auxiliary diagnosis methods is another problem. To address these challenges, we introduce a matching network based on medical guidelines and build an auxiliary diagnosis model based on self-paced learning. The matching network based on guidelines can provide medical knowledge beyond medical records and a certain degree of interpretability. Additionally, self-paced learning can help the model identify the label noise and prevent the model from being misled. The experiments show that our method outperforms the baselines in a Chinese medical multi-disease diagnosis dataset and the MIMIC-III dataset and has good performance in the label noise detection task.},
  archive      = {J_PR},
  author       = {Jiawei Long and Jiangtao Ren},
  doi          = {10.1016/j.patcog.2023.110178},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110178},
  shortjournal = {Pattern Recognition},
  title        = {Interpretable multidisease diagnosis and label noise detection based on a matching network and self-paced learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NODE-ImgNet: A PDE-informed effective and robust model for
image denoising. <em>PR</em>, <em>148</em>, 110176. (<a
href="https://doi.org/10.1016/j.patcog.2023.110176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the traditional partial differential equation (PDE) approach for image denoising , we propose a novel neural network architecture , referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising , our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise , as well as real-noisy images, and demonstrates superiority in learning from small image datasets.},
  archive      = {J_PR},
  author       = {Xinheng Xie and Yue Wu and Hao Ni and Cuiyu He},
  doi          = {10.1016/j.patcog.2023.110176},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110176},
  shortjournal = {Pattern Recognition},
  title        = {NODE-ImgNet: A PDE-informed effective and robust model for image denoising},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confusable facial expression recognition with geometry-aware
conditional network. <em>PR</em>, <em>148</em>, 110174. (<a
href="https://doi.org/10.1016/j.patcog.2023.110174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many facial expression recognition (FER) methods have now achieved satisfactory results. However, some facial expressions have similar muscle deformation, making them easy to confuse. These confusable facial expressions are a key challenge to accurately recognizing facial expressions. In addition, most current FER methods rely on the convolution operation , but convolution is a building block that processes one local neighborhood at a time; thus, it fails to capture the geometric patterns that are important for facial muscle deformation. To address this issue, considering the problems of pose variations and insufficient training data , this paper proposes a geometry-aware conditional network (GACN) that captures long-range dependencies for simultaneous pose-invariant facial expression editing and geometry-aware FER. Specifically, the GACN can complete a pose-invariant image editing task with long-range dependency by introducing conditional self-attention operations to a generative adversarial network . Moreover, the GACN presents non-local operations as building blocks of the classifier to capture the texture and geometry patterns simultaneously. Finally, these two tasks can further boost each other’s performances through our GACN, and confusable facial expressions can be effectively distinguished. And we overcome the effect of pose variations while expanding and enriching the training set. Our proposed algorithm is evaluated on both the in-the-lab and in-the-wild datasets and outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Tong Liu and Jing Li and Jia Wu and Bo Du and Jun Wan and Jun Chang},
  doi          = {10.1016/j.patcog.2023.110174},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110174},
  shortjournal = {Pattern Recognition},
  title        = {Confusable facial expression recognition with geometry-aware conditional network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint recognition of basic and compound facial expressions
by mining latent soft labels. <em>PR</em>, <em>148</em>, 110173. (<a
href="https://doi.org/10.1016/j.patcog.2023.110173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous works on facial expression recognition focus on basic emotions, while ignoring more complex compound expressions. However, both basic and compound emotions appear in the real-world environment. In this work, we aim to jointly recognize basic and compound expressions. Aiming at the Basic-Compound Facial Expression Recognition (BC-FER) task, we illustrate that traditional hard label training is not ideal due to great label dependencies. Therefore, we propose an expression soft label mining (ESLM) method to improve the performance. On the one hand, an iterated soft label mining (ISLM) algorithm assisted by teacher–student network is proposed to make the network generate soft targets automatically for learning. On the other hand, to explicitly leverage prior knowledge of label correlations, we propose an expression correlation score learning (ECSL) loss to regularize the predicted distributions. Extensive experimental results on CFEE, RAF-DB, and EmotioNet show that our method achieves state-of-the-art performance on BC-FER task.},
  archive      = {J_PR},
  author       = {Jing Jiang and Mei Wang and Bo Xiao and Jiani Hu and Weihong Deng},
  doi          = {10.1016/j.patcog.2023.110173},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110173},
  shortjournal = {Pattern Recognition},
  title        = {Joint recognition of basic and compound facial expressions by mining latent soft labels},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GhostFormer: Efficiently amalgamated CNN-transformer
architecture for object detection. <em>PR</em>, <em>148</em>, 110172.
(<a href="https://doi.org/10.1016/j.patcog.2023.110172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lightweight network model has gradually evolved into an important research direction in object detection. Network lightweight design has a variety of research methods, such as quantization, knowledge distillation , and neural architecture search . However, these methods either fail to break through the performance bottleneck of the model itself or require massive training costs. In order to solve these problems, a new object detection model based on CNN-Transformer hybrid feature extraction network called GhostFormer is proposed from the perspective of lightweight network structure design. GhostFormer makes full use of the advantages of local modeling of CNN and global modeling of Transformer, not only effectively reducing the complexity of the convolution model but also breaking through the limitation of Transformer’s lack of inductive bias. Finally, better transfer results are obtained in downstream tasks. Experiments show that the model is less than half as computationally expensive as YOLOv7 on the Pascal VOC dataset, with only about 3 % mAP@0.5 loss, and 9.7% mAP@0.5:0.95 improvement on the MS COCO dataset compared with GhostNet.},
  archive      = {J_PR},
  author       = {Xin Xie and Dengquan Wu and Mingye Xie and Zixi Li},
  doi          = {10.1016/j.patcog.2023.110172},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110172},
  shortjournal = {Pattern Recognition},
  title        = {GhostFormer: Efficiently amalgamated CNN-transformer architecture for object detection},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AttenGait: Gait recognition with attention and rich
modalities. <em>PR</em>, <em>148</em>, 110171. (<a
href="https://doi.org/10.1016/j.patcog.2023.110171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current gait recognition systems employ different types of manual attention mechanisms , like horizontal cropping of the input data to guide the training process and extract useful gait signatures for people identification. Typically, these techniques are applied using silhouettes as input, which limits the learning capabilities of the models. Thus, due to the limited information provided by silhouettes, state-of-the-art gait recognition approaches must use very simple and manually designed mechanisms, in contrast to approaches proposed for other topics such as action recognition. To tackle this problem, we propose AttenGait, a novel model for gait recognition equipped with trainable attention mechanisms that automatically discover interesting areas of the input data. AttenGait can be used with any kind of informative modalities, such as optical flow, obtaining state-of-the-art results thanks to the richer information contained in those modalities. We evaluate AttenGait on two public datasets for gait recognition: CASIA-B and GREW; improving the previous state-of-the-art results on them, obtaining 95.8% and 70.7% average accuracy, respectively. Code will be available at https://github.com/fmcp/attengait .},
  archive      = {J_PR},
  author       = {Francisco M. Castro and Rubén Delgado-Escaño and Ruber Hernández-García and Manuel J. Marín-Jiménez and Nicolás Guil},
  doi          = {10.1016/j.patcog.2023.110171},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110171},
  shortjournal = {Pattern Recognition},
  title        = {AttenGait: Gait recognition with attention and rich modalities},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VPCFormer: A transformer-based multi-view finger vein
recognition model and a new benchmark. <em>PR</em>, <em>148</em>,
110170. (<a href="https://doi.org/10.1016/j.patcog.2023.110170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past decade, finger vein authentication garners significant interest. However, most existing databases and algorithms predominantly focused on single-view finger vein recognition . The current projection of vein patterns actually maps a 3D network topology into a 2D plane, which inevitably leads to 3D feature loss and topological ambiguity in 2D images. Additionally, single-view based methods are sensitive to finger rotation and translation in practical applications. So far, there are currently few dedicated studies and public databases on multi-view finger vein recognition. To address these issues, we first establish a benchmark for future research by constructing the multi-view finger vein database, named Tsinghua Multi-View Finger Vein-3 Views (THUMVFV-3V) Database , which is collected over two sessions. THUMVFV-3V provides three types of Regions of Interest (ROIs) and includes unified preprocessing operations, catering to the majority of existing methods. Furthermore, we propose a novel Transformer-based model named Vein Pattern Constrained Transformer (VPCFormer) for multi-view finger vein recognition, primarily composed of multiple Vein Pattern Constrained Encoders (VPC-Encoders) and Neighborhood-Perspective Modules (NPMs). Specifically, the VPC-Encoder incorporates a novel Vein Pattern Attention Module (VPAM) and an Integrative Feed-Forward Network (IFFN). Motivated by the fact that the strong correlations veins exhibit across different views, we devise the VPAM. Assisted by a vein mask, VPAM is meticulously designed to exclusively extract intra- and inter-view dependencies between vein patterns. Further, we propose IFFN to efficiently aggregate the preceding attention and contextual information of VPAM. In addition, the NPM is utilized to capture the correlations within a single view, enhancing the final multi-view finger vein representation. Extensive experiments demonstrate the superiority of our VPCFormer. The THUMVFV-3V database is available at https://github.com/Pengyang233/THUMVFV-3V-Database .},
  archive      = {J_PR},
  author       = {Pengyang Zhao and Yizhuo Song and Siqi Wang and Jing-Hao Xue and Shuping Zhao and Qingmin Liao and Wenming Yang},
  doi          = {10.1016/j.patcog.2023.110170},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110170},
  shortjournal = {Pattern Recognition},
  title        = {VPCFormer: A transformer-based multi-view finger vein recognition model and a new benchmark},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MI3C: Mining intra- and inter-image context for person
search. <em>PR</em>, <em>148</em>, 110169. (<a
href="https://doi.org/10.1016/j.patcog.2023.110169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search aims to localize the queried person from a gallery of uncropped, realistic images. Unlike re-identification (Re-ID), person search deals with the entire scene image containing rich and diverse visual context information. However, existing works mainly focus on the person’s appearance while ignoring other essential intra- and inter-image context information. To comprehensively leverage the intra- and inter-image context, we propose a unified framework termed MI 3 3 C including the Intra-image Multi-View Context network (IMVC) and the Inter-image Group Context Ranking algorithm (IGCR). Concretely, the IMVC integrates the features from the scene, surrounding, instance, and part views collaboratively to generate the final ID feature for person search. Furthermore, the IGCR algorithm employs group matching results between query and gallery image pairs to measure the holistic image matching similarity, which is adopted as part of the sorting metric to yield a more robust ranking among the whole gallery. Extensive experiments on two popular person search benchmarks demonstrate that by mining intra- and inter-image context, our method outperforms previous state-of-the-art methods by conspicuous margins. Specifically, we achieve 96.7% mAP and 97.1% top-1 accuracy on the CUHK-SYSU dataset, 55.6% mAP, and 90.8% top-1 accuracy on the PRW dataset.},
  archive      = {J_PR},
  author       = {Zongheng Tang and Yulu Gao and Tianrui Hui and Fengguang Peng and Si Liu},
  doi          = {10.1016/j.patcog.2023.110169},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110169},
  shortjournal = {Pattern Recognition},
  title        = {MI3C: Mining intra- and inter-image context for person search},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICLR: Instance credibility-based label refinement for label
noisy person re-identification. <em>PR</em>, <em>148</em>, 110168. (<a
href="https://doi.org/10.1016/j.patcog.2023.110168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) has demonstrated remarkable performance when trained on accurately annotated data. However, in practical applications, the presence of annotation errors is unavoidable, which can undermine the accuracy and robustness of the Re-ID model training. To address the adverse impacts of label noise, especially in scenarios with limited training samples for each identity (ID), a common approach is to utilize all the available sample labels. Unfortunately, these labels contain incorrect labels, leading to the model being influenced by noise and compromising its performance. In this paper, we propose an Instance Credibility-based Label Refinement and Re-weighting (ICLR) framework to exploit partially credible labels to refine and re-weight incredible labels effectively. Specifically, the Label-Incredibility Optimization (LIO) module is proposed to optimize incredible labels before model training, which partitions the samples into credible and incredible samples and propagates credible labels to others. Furthermore, we design an Incredible Instance Re-weight (I 2 2 R) strategy, aiming to emphasize instances that contribute more significantly and dynamically adjust the weight of each instance. The proposed method seamlessly reinforces accuracy without requiring additional information or discarding any samples. Extensive experimental results conducted on Market-1501 and Duke-MTMC datasets demonstrate the effectiveness of our proposed method, leading to a substantial improvement in performance under both random noise and pattern noise settings. Code will be available at https://github.com/whut16/ReID-Label-Noise .},
  archive      = {J_PR},
  author       = {Xian Zhong and Xiyu Han and Xuemei Jia and Wenxin Huang and Wenxuan Liu and Shuaipeng Su and Xiaohan Yu and Mang Ye},
  doi          = {10.1016/j.patcog.2023.110168},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110168},
  shortjournal = {Pattern Recognition},
  title        = {ICLR: Instance credibility-based label refinement for label noisy person re-identification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection via gating highway connection for retinal
fundus images. <em>PR</em>, <em>148</em>, 110167. (<a
href="https://doi.org/10.1016/j.patcog.2023.110167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the labels for medical images are challenging to collect in real scenarios, especially for rare diseases, fully supervised methods cannot achieve robust performance for clinical anomaly detection . Recent research tried to tackle this problem by training the anomaly detection framework using only normal data. Reconstruction-based methods, e.g., auto-encoder, achieved impressive performances in the anomaly detection task. However, most existing methods adopted the straightforward backbone architecture ( i.e., encoder-and-decoder) for image reconstruction. The design of a skip connection, which can directly transfer information between the encoder and decoder, is rarely used. Since the existing U-Net has demonstrated the effectiveness of skip connections for image reconstruction tasks, in this paper, we first use the dynamic gating strategy to achieve the usage of skip connections in existing reconstruction-based anomaly detection methods and then propose a novel gating highway connection module to adaptively integrate skip connections into the framework and boost its anomaly detection performance, namely GatingAno. Furthermore, we formulate an auxiliary task, namely histograms of oriented gradients (HOG) prediction, to encourage the framework to exploit contextual information from fundus images in a self-driven manner, which increases the robustness of feature representation extracted from the healthy samples. Last but not least, to improve the model generalization for anomalous data, we introduce an adversarial strategy for the training of our multi-task framework. Experimental results on the publicly available datasets, i.e., IDRiD and ADAM, validate the superiority of our method for detecting abnormalities in retinal fundus images. The source code is available at https://github.com/WentianZhang-ML/GatingAno .},
  archive      = {J_PR},
  author       = {Wentian Zhang and Haozhe Liu and Jinheng Xie and Yawen Huang and Yu Zhang and Yuexiang Li and Raghavendra Ramachandra and Yefeng Zheng},
  doi          = {10.1016/j.patcog.2023.110167},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110167},
  shortjournal = {Pattern Recognition},
  title        = {Anomaly detection via gating highway connection for retinal fundus images},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IME: Efficient list-based method for incremental mining of
maximal erasable patterns. <em>PR</em>, <em>148</em>, 110166. (<a
href="https://doi.org/10.1016/j.patcog.2023.110166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasable pattern mining can help factories facing a financial crisis increase productivity by identifying and eliminating unprofitable products. The Flag-GenMax-EI algorithm extracts Maximal Erasable Itemsets (MEIs); however, it does not support dynamic data. In practice, many applications create databases incrementally. Using the Flag-GenMax-EI algorithm to mine maximal erasable patterns from incremental databases is clearly very costly because it must be run each time. In this paper, an efficient method called IME is proposed for incremental mining of maximal erasable patterns. IMEI-List and IMEP-List are two new data structures introduced by the proposed method. These lists allow the algorithm to update all tree nodes without rescanning the updated database (original database + new database) and recreating the nodes. This is the first study of incremental mining of maximal erasable patterns. Extensive experimental results on dense and sparse incremental data show that the proposed algorithm improves scalability. It extracts MEIs much faster than the Flag-GenMax-EI algorithm in different modes of database update.},
  archive      = {J_PR},
  author       = {Razieh Davashi},
  doi          = {10.1016/j.patcog.2023.110166},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110166},
  shortjournal = {Pattern Recognition},
  title        = {IME: Efficient list-based method for incremental mining of maximal erasable patterns},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DGD-cGAN: A dual generator for image dewatering and
restoration. <em>PR</em>, <em>148</em>, 110159. (<a
href="https://doi.org/10.1016/j.patcog.2023.110159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images are usually covered with a blue–greenish colour cast, making them distorted, blurry or low in contrast. This phenomenon occurs due to the light attenuation given by the scattering and absorption in the water column. In this paper we present an image dewatering approach motivated upon the observation that the image formation model can be used to drive the learning process by constraining the loss function and making used of paired data. To this end, we employ a conditional generative adversarial network (cGAN) with two generators. Our Dual Generator Dewatering cGAN (DGD-cGAN) removes the haze and colour cast induced by the water column and restores the true colours of underwater scenes whereby the effects of various attenuation and scattering phenomena that occur in underwater images are tackled by the two generators. The first generator takes as input the underwater image and predicts the dewatered scene, while the second generator learns the underwater image formation process by implementing a custom loss function based upon the transmission and the veiling light components of the image formation model. Extensive experiments show that DGD-cGAN consistently delivers a margin of improvement as compared with state-of-the-art methods on several widely available datasets.},
  archive      = {J_PR},
  author       = {Salma Gonzalez-Sabbagh and Antonio Robles-Kelly and Shang Gao},
  doi          = {10.1016/j.patcog.2023.110159},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110159},
  shortjournal = {Pattern Recognition},
  title        = {DGD-cGAN: A dual generator for image dewatering and restoration},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-abstraction and perturbing support pair network for
few-shot fine-grained image classification. <em>PR</em>, <em>148</em>,
110158. (<a href="https://doi.org/10.1016/j.patcog.2023.110158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of few-shot fine-grained image classification (FSFGIC) is to distinguish subordinate-level categories with subtle visual differences such as the species of bird and models of car with only a few samples. In this work, we argue that a designed network that has the ability to better distinguish feature descriptors of different categories will effectively improve the performance of FSFGIC. We propose a re-abstraction and perturbing support pair network (RaPSPNet) for FSFGIC. Specifically, we first design a feature re-abstraction embedding (FRaE) module which can not only effectively amplify the difference between the feature information from different categories but also better extract the feature information from images. Furthermore, a novel perturbing support pair (PSP) based similarity measure module is designed which evaluates the relationships of feature information among a query image and two different categories of support images (a support pair) at the same time for guiding the designed FRaE module to find salient feature information from the same category of query and support images and find distinguishable feature information from the different categories of query and support images. Extensive experiments on FSFGIC tasks demonstrate the superiority of the proposed methods over state-of-the-art benchmarks.},
  archive      = {J_PR},
  author       = {Weichuan Zhang and Yali Zhao and Yongsheng Gao and Changming Sun},
  doi          = {10.1016/j.patcog.2023.110158},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110158},
  shortjournal = {Pattern Recognition},
  title        = {Re-abstraction and perturbing support pair network for few-shot fine-grained image classification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual subspace manifold learning based on GCN for
intensity-invariant facial expression recognition. <em>PR</em>,
<em>148</em>, 110157. (<a
href="https://doi.org/10.1016/j.patcog.2023.110157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is one of the most important computer vision tasks for understanding human inner emotions. However, the poor generation ability of the FER model limits its applicability due to tremendous intraclass variation. Especially for expressions of varying intensities, the appearance differences among weak expressions are subtle, which makes FER tasks challenging. In response to these issues, this paper presents a dual subspace manifold learning method based on a graph convolutional network (GCN) for intensity-invariant FER tasks. Our method treats the target task as a node classification problem and learns the manifold representation using two subspace analysis methods: locality preserving projection (LPP) and peak-piloted locality preserving projection (PLPP). Inspired by the classic LPP, which maintains local similarity among data, this paper introduces a novel PLPP that maintains the locality between peak expressions and non-peak expressions to enhance the representation of weak expressions. This paper also reports two subspace fusion methods, one based on a weighted adjacency matrix and another on a self-attention mechanism, that combine the LPP and PLPP to further improve FER performance. The second method achieves a recognition accuracy of 93.83% on the CK+, 74.86% on the Oulu-CASIA and 75.37% on the MMI for weak expressions, outperforming state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jingying Chen and Jinxin Shi and Ruyi Xu},
  doi          = {10.1016/j.patcog.2023.110157},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110157},
  shortjournal = {Pattern Recognition},
  title        = {Dual subspace manifold learning based on GCN for intensity-invariant facial expression recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised adversarial variational learning.
<em>PR</em>, <em>148</em>, 110156. (<a
href="https://doi.org/10.1016/j.patcog.2023.110156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A natural approach for representation learning is to combine the inference mechanisms of VAEs and the generative abilities of GANs, within a new model, namely VAEGAN. Most existing VAEGAN models would jointly train the generator and inference modules, which has limitations when learning representations generated by a pre-trained GAN model without data. In this paper, we develop a novel hybrid model, called the Self-Supervised Adversarial Variational Learning (SS-AVL) which introduces a two-step optimization procedure training separately the generator and the inference model. The primary advantage of SS-AVL over existing VAEGAN models is that SS-AVL optimizes the inference models in a self-supervised learning manner where the samples used for training the inference models are drawn from the generator distribution instead of using real samples. This can allow SS-AVL to learn representations from arbitrary GAN models without using real data. Additionally, we employ information maximization into the context of increasing the maximum likelihood , which encourages SS-AVL to learn meaningful latent representations. We perform extensive experiments to demonstrate the effectiveness of the proposed SS-AVL model.},
  archive      = {J_PR},
  author       = {Fei Ye and Adrian. G. Bors},
  doi          = {10.1016/j.patcog.2023.110156},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110156},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised adversarial variational learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Offline handwritten mathematical expression recognition with
graph encoder and transformer decoder. <em>PR</em>, <em>148</em>,
110155. (<a href="https://doi.org/10.1016/j.patcog.2023.110155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten mathematical expression recognition (HMER) has attracted extensive attention. Despite the significant progress achieved in recent years attributed to the development of deep learning approaches , HMER remains a challenge due to the complex spatial structure and variable writing styles. Encoder–decoder models with attention mechanism , which treats HMER as an image-to-sequence (i.e. LaTeX) generation task, have boosted the accuracy, but suffer from low interpretability in that the symbols are not segmented explicitly. Symbol segmentation is desired for facilitating post-processing and human interaction in real applications. In this paper, we formulate the mathematical expression as a graph and propose a Graph-Encoder-Transformer-Decoder (GETD) approach for HMER. For constructing the graph from input image, candidate symbols are first detected using an object detector and represented as the nodes of a graph, called symbol graph, and the edges of the graph encodes the between-symbol relationship. The spatial information is aggregated in a graph neural network (GNN), and a Transformer-based decoder is used to identify the symbol classes and structure from the graph. Experiments on public datasets demonstrate that our GETD model achieves competitive expression recognition performance while offering good interpretability compared with previous methods.},
  archive      = {J_PR},
  author       = {Jia-Man Tang and Hong-Yu Guo and Jin-Wen Wu and Fei Yin and Lin-Lin Huang},
  doi          = {10.1016/j.patcog.2023.110155},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110155},
  shortjournal = {Pattern Recognition},
  title        = {Offline handwritten mathematical expression recognition with graph encoder and transformer decoder},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative multi-label feature selection with adaptive
graph diffusion. <em>PR</em>, <em>148</em>, 110154. (<a
href="https://doi.org/10.1016/j.patcog.2023.110154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection can alleviate the problem of the curse of dimensionality by selecting more discriminative features , which plays an important role in multi-label learning. Recently, embedded feature selection methods have received increasing attentions. However, most existing methods learn the low-dimensional embeddings under the guidance of the local structure between the original instance pairs, thereby ignoring the high-order structure between instances and being sensitive to noise in the original features. To address these issues, we propose a feature selection method named discriminative multi-label feature selection with adaptive graph diffusion (MFS-AGD). Specifically, we first construct a graph embedding learning framework equipped with adaptive graph diffusion to uncover a latent subspace that preserves the higher-order structure information between four tuples. Then, the Hilbert–Schmidt independence criterion (HSIC) is incorporated into the embedding learning framework to ensure the maximum dependency between the latent representation and labels. Benefiting from the interactive optimization of the feature selection matrix, latent representation and similarity graph , the selected features can accurately explore the higher-order structural and supervised information of data. By further considering the correlation between labels, MFS-AG is extended to a more discriminative version,i.e., LMFS-AG. Extensive experimental results on various benchmark data sets validate the advantages of the proposed MFS-AGD and LMFS-AGD methods.},
  archive      = {J_PR},
  author       = {Jiajun Ma and Fei Xu and Xiaofeng Rong},
  doi          = {10.1016/j.patcog.2023.110154},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110154},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative multi-label feature selection with adaptive graph diffusion},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph meets probabilistic generation model: A new
perspective for graph disentanglement. <em>PR</em>, <em>148</em>,
110153. (<a href="https://doi.org/10.1016/j.patcog.2023.110153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different from the existing graph disentanglement neural networks , we interpret the graph entanglement under a probabilistic generation framework in this paper. With this foundation, a M ixed P robabilistic Generation Model induced G raph D isentanglement Network (MPGD) is proposed. Considering the disentangled components corresponding to different factors as obeying specific distributions, a generalized probabilistic aggregation scheme among components is deduced theoretically. As a key part of the mixed probabilistic generative model , we provide a solution for estimating the mixture probabilities using self-attention and an in-depth analysis of its close connection with the classical EM parameter estimation method. Meanwhile, a way of probabilistic aggregation is formulated to obtain the node representation in embedding space. In addition, the prior mixture probabilities are formulated as an auxiliary factor-aware representation to facilitate the twin-branch prediction. A variety of experiments show that MPGD achieves more competitive performance than some existing state-of-the-art methods while having ideal disentangling effects. The code implementation is available in https://github.com/GiorgioPeng/MPGD .},
  archive      = {J_PR},
  author       = {Zouzhang Peng and Shuai Zheng and Zhenfeng Zhu and Zhizhe Liu and Jian Cheng and Honghui Dong and Yao Zhao},
  doi          = {10.1016/j.patcog.2023.110153},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110153},
  shortjournal = {Pattern Recognition},
  title        = {Graph meets probabilistic generation model: A new perspective for graph disentanglement},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLOPX: Anchor-free multi-task learning network for panoptic
driving perception. <em>PR</em>, <em>148</em>, 110152. (<a
href="https://doi.org/10.1016/j.patcog.2023.110152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoptic driving perception encompasses traffic object detection, drivable area segmentation, and lane detection . Existing methods typically utilize anchor-based multi-task learning networks to complete this task. While these methods yield promising results, they suffer from the inherent limitations of anchor-based detectors. In this paper, we propose YOLOPX, a simple and efficient anchor-free multi-task learning network for panoptic driving perception. To the best of our knowledge, this is the first work to employ the anchor-free detection head in panoptic driving perception. This anchor-free manner simplifies training by avoiding anchor-related heuristic tuning, and enhances the adaptability and scalability of our multi-task learning network. In addition, YOLOPX incorporates a novel lane detection head that combines multi-scale high-resolution features and long-distance contextual dependencies to improve segmentation performance . Beyond structure optimization, we propose optimization improvements to enhance network training, enabling our multi-task learning network to achieve optimal performance through simple end-to-end training. Experimental results on the challenging BDD100K dataset demonstrate the state-of-the-art (SOTA) performance of YOLOPX: it achieves 93.7% recall and 83.3% mAP50 on traffic object detection, 93.2% mIoU on drivable area segmentation, and 88.6% accuracy and 27.2% IoU on lane detection. Moreover, YOLOPX has faster inference speed compared to the lightweight network YOLOP. Consequently, YOLOPX is a powerful solution for panoptic driving perception problems. The code is available at https://github.com/jiaoZ7688/YOLOPX .},
  archive      = {J_PR},
  author       = {Jiao Zhan and Yarong Luo and Chi Guo and Yejun Wu and Jiawei Meng and Jingnan Liu},
  doi          = {10.1016/j.patcog.2023.110152},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110152},
  shortjournal = {Pattern Recognition},
  title        = {YOLOPX: Anchor-free multi-task learning network for panoptic driving perception},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PA-pose: Partial point cloud fusion based on reliable
alignment for 6D pose tracking. <em>PR</em>, <em>148</em>, 110151. (<a
href="https://doi.org/10.1016/j.patcog.2023.110151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based 6-DOF (6D) pose tracking, serving as a basis for most real-time applications such as augmented reality and robot manipulation, receives attention transiting from 2D to 3D vision, with the popularity of depth sensors. However, the irregular nature of 3D point clouds challenges this task, especially since the lack of explicit alignments hinders the interaction and fusion between the observed point clouds. Therefore, this paper proposes a novel approach named PA-Pose to achieve 6D pose tracking in point clouds. It takes the forward-predicted dense correspondences within an overlap as reliable alignments, to guide the feature fusion of the partial-to-partial point clouds. Then, the relative transformation pose of adjacent observations is continuously regressed from the point-wisely fused features by confidence scoring, avoiding non-differentiable pose fitting. In addition, a shifted point convolution (SPConv) operation is introduced in the fusion process, to further promote the local context interaction of the observed point cloud pair in the expanded alignment field. Extensive experiments on two benchmark datasets (YCB-Video and YCBInEOAT) demonstrate that our method achieves state-of-the-art performance. Even though only 3D point clouds are taken as input, our PA-Pose is still competitive with those methods fully utilizing RGB-D information in the single view. Finally, experiments in the real scene for tracking industrial objects also validates the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Zhenyu Liu and Qide Wang and Daxin Liu and Jianrong Tan},
  doi          = {10.1016/j.patcog.2023.110151},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110151},
  shortjournal = {Pattern Recognition},
  title        = {PA-pose: Partial point cloud fusion based on reliable alignment for 6D pose tracking},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonmonotone variable projection algorithms for matrix
decomposition with missing data. <em>PR</em>, <em>148</em>, 110150. (<a
href="https://doi.org/10.1016/j.patcog.2023.110150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates algorithms for matrix factorization when some or many components are missing, a problem that arises frequently in computer vision and pattern recognition. We demonstrate that the Jacobian used in the damped Wiberg (DW) method is exactly the same as that of Kaufman’s simplified variable projection (VP) algorithm. Our analysis provides a novel perspective on the efficiency of VP algorithms by improving the strong convexity of the approximate function. To enhance numerical stability , we set a lower bound on the damping parameter instead of adding a null space like the DW algorithm. Another challenge of low-rank matrix decomposition with missing data is the existence of many sharp local minima , which are often distributed in narrow valleys of the landscape of objection functions. Falling into such minima tends to result in poor reconstruction results. To address this issue, we design a non-monotonic VP algorithm, which can facilitate the algorithm to escape from sharp minima and converge to flatter minima. Numerical experiments confirm the effectiveness and efficiency of the proposed nonmonotone VP algorithm.},
  archive      = {J_PR},
  author       = {Xiang-xiang Su and Min Gan and Guang-yong Chen and Lin Yang and Jun-wei Jin},
  doi          = {10.1016/j.patcog.2023.110150},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110150},
  shortjournal = {Pattern Recognition},
  title        = {Nonmonotone variable projection algorithms for matrix decomposition with missing data},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coresets for fast causal discovery with the additive noise
model. <em>PR</em>, <em>148</em>, 110149. (<a
href="https://doi.org/10.1016/j.patcog.2023.110149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery reveals the true causal relationships behind data and discovering causal relationships from observed data is a particularly challenging problem, especially in large-scale datasets. The functional causal model is an effective method for causal discovery, but its time efficiency cannot be guaranteed. How to efficiently apply it to massive data still needs to be solved. In this paper, we propose a coreset construction for the additive noise model to accelerate causal discovery. According to the asymmetry characteristic of causality, samples were assigned different weights to construct the coreset. With the constructed coreset, we propose a Fast causal discovery algorithm based on the Additive Noise Model (FANM) to improve the time efficiency of the functional causal model while ensuring the result performance of causal discovery. Experiments on synthetic data and real-world data show that our proposed algorithm is much more time-efficient than the methods based on the functional causal model, and the runtime of FANM remains consistent as sample size increases while maintaining or exceeding the accuracy of the original nonlinear additive noise model.},
  archive      = {J_PR},
  author       = {Boxiang Zhao and Shuliang Wang and Lianhua Chi and Hanning Yuan and Ye Yuan and Qi Li and Jing Geng and Shao-Liang Zhang},
  doi          = {10.1016/j.patcog.2023.110149},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110149},
  shortjournal = {Pattern Recognition},
  title        = {Coresets for fast causal discovery with the additive noise model},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved channel attention methods via hierarchical pooling
and reducing information loss. <em>PR</em>, <em>148</em>, 110148. (<a
href="https://doi.org/10.1016/j.patcog.2023.110148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel attention has been demonstrated to improve performance of convolutional neural networks . Most existing channel attention methods lower channel dimension for reducing computational complexity . However, the dimension reduction causes information loss, thus resulting in performance loss . To alleviate the paradox of complexity and performance trade-off, we propose two novel channel attention methods named Grouping-Shuffle-Aggregation Channel Attention (GSACA) method and Mixed Encoding Channel Attention (MECA) method, respectively. Our GSACA method partitions channel variables into several groups and performs the independent matrix multiplication without the dimension reduction to each group. Our GSACA method enables interaction between all groups using a ”channel shuffle” operator. After these, our GSACA method performs the independent matrix multiplication each group again and aggregates all channel correlations. Our MECA method encodes channel information through dual path architectures to benefit from both path topology where one uses the multilayer perception with dimension reduction to encode channel information and the other uses channel information encoding method without dimension reduction. Furthermore, a novel pooling operator named hierarchical pooling is presented and applied to our GSACA and MECA methods. The experimental results showed that our GSACA method almost consistently outperformed most existing channel attention methods and that our MECA method consistently outperformed the existing channel attention methods.},
  archive      = {J_PR},
  author       = {Meng Zhu and Weidong Min and Junwei Han and Qing Han and Shimiao Cui},
  doi          = {10.1016/j.patcog.2023.110148},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110148},
  shortjournal = {Pattern Recognition},
  title        = {Improved channel attention methods via hierarchical pooling and reducing information loss},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From patch, sample to domain: Capture geometric structures
for few-shot learning. <em>PR</em>, <em>148</em>, 110147. (<a
href="https://doi.org/10.1016/j.patcog.2023.110147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel concepts with only few samples by using prior knowledge learned from the seen concepts. In this paper, we address the problem of few-shot learning under domain shifts. Traditional few-shot learning methods are not directly applicable to cross-domain scenarios due to the large discrepancy of feature distributions across domains. To this end, we propose a novel Hierarchical Optimal Transport network with Attention (HOTA) for cross-domain few-shot learning. The underlying idea is to learn the transferable and discriminative embeddings by taking advantage of the hierarchical geometric structures among image data , ranging from patch, sample to domain. The HOTA framework utilizes a hierarchical optimal transport network to smooth the domain shifts by domain alignment while enhancing the discrimination and the transferability of the embeddings by aligning the patches of images. To further enhance the transferability, HOTA conducts a mix-up data augmentation based on cross-domain attention to capture the relationships of samples in different domains. The extensive experiments on a variety of few-shot benchmark scenarios demonstrate that HOTA outperforms the state-of-the-art methods under both supervised and unsupervised conditions.},
  archive      = {J_PR},
  author       = {Qiaonan Li and Guihua Wen and Pei Yang},
  doi          = {10.1016/j.patcog.2023.110147},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110147},
  shortjournal = {Pattern Recognition},
  title        = {From patch, sample to domain: Capture geometric structures for few-shot learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrasting augmented features for domain adaptation with
limited target domain data. <em>PR</em>, <em>148</em>, 110145. (<a
href="https://doi.org/10.1016/j.patcog.2023.110145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation aims to alleviate distribution gaps between source and target domains. However, when the available target domain data are scarce for training, learning generalizable representations for domain adaptation is challenging. We propose a novel approach, dubbed Contrasting Augmented Features (CAF), to tackle the challenge of insufficient target domain data for domain adaptation, by generating and contrasting augmented features. We introduce a semantic feature generator to generate augmented features by replacing the instance-level feature statistics of one domain with another domain. With the augmented features, we further design the reweighted instance contrastive loss and category contrastive loss to improve feature discrimination and align feature distributions of source and target domains. CAF can be applied to few-shot domain adaptation and unsupervised domain adaptation with limited unlabeled target domain data. Despite its simplicity, extensive experiments show promising results for both applications. In addition, experiments demonstrate that CAF is more robust to the number of target domain data and also effective in vanilla unsupervised domain adaptation setting with full target domain data.},
  archive      = {J_PR},
  author       = {Xi Yu and Xiang Gu and Jian Sun},
  doi          = {10.1016/j.patcog.2023.110145},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110145},
  shortjournal = {Pattern Recognition},
  title        = {Contrasting augmented features for domain adaptation with limited target domain data},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic graph contrastive learning via maximize temporal
consistency. <em>PR</em>, <em>148</em>, 110144. (<a
href="https://doi.org/10.1016/j.patcog.2023.110144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph contrastive learning (GCL) is one of the most powerful self-supervised representation learning frameworks. Existing GCL methods have achieved impressive performance. However, it is still challenging to capture the evolution of nodes or edges, where the interaction of nodes or edges is stable in a short time but changeable at long time intervals. Therefore, it is crucial to capture the temporal consistency in dynamic graph. In this paper, we propose a novel Dy namic G raph C ontrastive L earning framework, DyGCL, which learns node representation by maximizing the temporal consistency in a short time and discriminating the non-consistency in a long term. More specifically, DyGCL consists of two parts: GCL Trainer and Auxiliary Trainer. GCL Trainer focus on distinguishing temporal consistency and non-consistency. And the Auxiliary Trainer aims to improve the generalization ability with less labeled data as auxiliary supervision. Finally, we demonstrate the effectiveness and superiority of DyGCL by applying it to three datasets.},
  archive      = {J_PR},
  author       = {Peng Bao and Jianian Li and Rong Yan and Zhongyi Liu},
  doi          = {10.1016/j.patcog.2023.110144},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110144},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic graph contrastive learning via maximize temporal consistency},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A coarse-to-fine pattern parser for mitigating the issue of
drastic imbalance in pixel distribution. <em>PR</em>, <em>148</em>,
110143. (<a href="https://doi.org/10.1016/j.patcog.2023.110143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significance of minute semantic components such as eyes and eyebrows tends to be overshadowed by larger components like skin and background, leading to inadequate graph attention from the model. Recent cropping-and-segmenting strategies comprise distinct stages and do not involve extensive interactions, thereby preventing joint optimization for collaborative perception. To mitigate this flaw, a coarse-to-fine pattern parsing network (CtFPPN) is proposed based on the capsule network (CapsNet). The CtFPPN incorporates a coarse-grained parser module, which generates binary coarse-scaled masks for larger components, and a fine-grained parser module, which performs fine-scaled parsing for smaller components using the coarse contexts as references. To establish a connection between two parsers, the discretization attention fragmentation mechanism (DAFM) is customized. The fine-grained parser has the option to aggregate projections from non-spatially-fixed collections of the coarse-grained parser, thereby embodying the principle of &quot;coarse-to-fine&quot; of CtFPPN. Under the premise of the existence of imbalanced pixel distribution, quantitative and ablation experiments of face and human parsing demonstrate the superiority of CtFPPN over the state-of-the-arts. Notably, CtFPPN excels in mitigating the pixel imbalance issues and accurately defining fine-scaled semantic boundaries of minuscule components.},
  archive      = {J_PR},
  author       = {Zhongqi Lin and Xudong Jiang and Zengwei Zheng},
  doi          = {10.1016/j.patcog.2023.110143},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110143},
  shortjournal = {Pattern Recognition},
  title        = {A coarse-to-fine pattern parser for mitigating the issue of drastic imbalance in pixel distribution},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global routing between capsules. <em>PR</em>, <em>148</em>,
110142. (<a href="https://doi.org/10.1016/j.patcog.2023.110142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current convolutional neural networks (CNNs) lack viewpoint equivariance. Hence, they perform poorly when dealing with viewpoints unseen during training procedures. CNN achieves invariance via pooling operations in image classification tasks. However, the pooling operation does not necessarily improve viewpoint generalization, rather relying on more data to achieve viewpoint equivariance. Capsule network (CapsNet) is proposed to tackle this issue, but it is inefficient and inaccurate when applied to complex datasets. We propose a novel CapsNet architecture called Global Routing CapsNet (GR-CapsNet) to solve this problem. Specifically, colored background in the input image can generate invalid background voting capsules to reduce the performance of CapsNet. Therefore, we first construct a dynamic linear unit (DLU), which avoids the generation of invalid background voting capsules. Then we present two extra learnable units: frequency domain unit (FDU) and spatial unit (SPU). The former is used to capture finer features in the frequency domain and aims to improve classification performance on complex datasets. The latter is applied to construct the spatial relationship between the voting capsules and component capsules and aims to enhance robustness to affine transformation. Finally, we propose a global routing mechanism to simplify the routing process for CapsNet, which obtains more feature information to improve the performance of CapsNet. Extensive experiments on nine datasets show that our method obtains better robustness and generalization and achieves SOTA performance compared to other related methods. And it has fewer the number of parameters and GPU memory consumption than these related methods. The source code is available on https://github.com/cwpl/GR-CapsNet .},
  archive      = {J_PR},
  author       = {Ran Chen and Hao Shen and Zhong-Qiu Zhao and Yi Yang and Zhao Zhang},
  doi          = {10.1016/j.patcog.2023.110142},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110142},
  shortjournal = {Pattern Recognition},
  title        = {Global routing between capsules},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Random epipolar constraint loss functions for supervised
optical flow estimation. <em>PR</em>, <em>148</em>, 110141. (<a
href="https://doi.org/10.1016/j.patcog.2023.110141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of supervised models estimate optical flow through minimizing the numerical difference between the predicted flow and the ground truth, resulting in the loss of positional and geometric characteristics of the calculated flow fields. In addition, these models require a large number of parameters and high computational cost when computing optical flow. To address these issues, this paper presents a novel loss function and a lightweight framework for optical flow estimation . The proposed loss function, called the random epipolar constraint loss function (RECLoss), incorporates epipolar geometry into supervised optimization to transform the numerical difference into geometry constraint. The RECLoss can make the optical flow estimation models more effective and enhance their generalization abilities . Moreover, the design of RECLoss is more interpretable and the estimated optical flow fields from RECLoss have clearly defined mathematical meanings. A lightweight recurrent neural network for optical flow estimation (LRFlow) that balances computational cost and estimation accuracy is also proposed. The LRFlow, containing only 3.0M parameters, consists of a feature extractor, a correlation matching module, and an iterative update unit. The proposed lightweight network achieves state-of-the-art results compared to all other lightweight networks on the challenging MPI-Sintel and KITTI2015 datasets. The effectiveness of RECLoss in improving the accuracy of LRFlow and other state-of-the-art methods such as RAFT and GMA has been validated through extensive experiments. The source code of the project are available at https://github.com/Eryo-iPython/RECLoss .},
  archive      = {J_PR},
  author       = {Zhengyuan Fan and Zemin Cai},
  doi          = {10.1016/j.patcog.2023.110141},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110141},
  shortjournal = {Pattern Recognition},
  title        = {Random epipolar constraint loss functions for supervised optical flow estimation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid ARMA-GARCH-neural networks for intraday strategy
exploration in high-frequency trading. <em>PR</em>, <em>148</em>,
110139. (<a href="https://doi.org/10.1016/j.patcog.2023.110139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The frequency of armed conflicts increased during the last 20 years. The problems of the emergence of military disputes, not only concern social parameters, but also economic and financial dimensions. This study examines the potential impact of global geopolitical events on the stock market prices of the Dow Jones U.S. Aerospace &amp; Defense Index and Foreign Exchange (FOREX) markets movements. We analyse whether defence stocks and exchange rate perform similarly during military incidents or geopolitical crises. We built an Autoregressive Moving Average Model with a Generalized Autoregressive Conditional Heteroskedasticity process (ARMA-GARCH) with the machine learning methods of Neural Networks, Deep Recurrent Convolutional Neural Networks, Deep Neural Decision Trees, Quantum Neural Networks, and Quantum Recurrent Neural Networks , aimed at detecting intraday patterns for forecasting defence stock market and FOREX markets disturbances in a market microstructure framework. The empirical results provide preliminary findings on the foreseeability of market disturbances and small differences are observed before and during geopolitical events. Additionally, we confirm the effectiveness of the hybrid model ARMA-GARCH with the machine learning approaches, being ARMA-GARCH-Quantum Recurrent Neural Network the technique that achieves the best accuracy results. Our work has a large potential impact on investment market agents and portfolio managers, as shocks from geopolitical events could provide a new methodology to support the decision-making process for trading in High-Frequency Trading.},
  archive      = {J_PR},
  author       = {David Alaminos and M. Belén Salas and Antonio Partal-Ureña},
  doi          = {10.1016/j.patcog.2023.110139},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110139},
  shortjournal = {Pattern Recognition},
  title        = {Hybrid ARMA-GARCH-neural networks for intraday strategy exploration in high-frequency trading},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VisClust: A visual clustering algorithm based on orthogonal
projections. <em>PR</em>, <em>148</em>, 110136. (<a
href="https://doi.org/10.1016/j.patcog.2023.110136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel clustering algorithm , visClust , that is based on lower dimensional data representations and visual interpretation. Thereto, we design a transformation that allows the data to be represented by a binary integer array enabling the use of image processing methods to select a partition. Qualitative and quantitative analyses measured in accuracy and an adjusted Rand-Index show that the algorithm performs well while requiring low runtime as well and RAM . We compare the results to 6 state-of-the-art algorithms with available code, confirming the quality of visClust by superior performance in most experiments. Moreover, the algorithm asks for just one obligatory input parameter while allowing optimization via optional parameters. The code is made available on GitHub and straightforward to use.},
  archive      = {J_PR},
  author       = {Anna Breger and Clemens Karner and Martin Ehler},
  doi          = {10.1016/j.patcog.2023.110136},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110136},
  shortjournal = {Pattern Recognition},
  title        = {VisClust: A visual clustering algorithm based on orthogonal projections},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Linearized alternating direction method of multipliers for
elastic-net support vector machines. <em>PR</em>, <em>148</em>, 110134.
(<a href="https://doi.org/10.1016/j.patcog.2023.110134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many high-dimensional datasets, the phenomenon that features are relevant often occurs. Elastic-net regularization is widely used in support vector machines (SVMs) because it can automatically perform feature selection and encourage highly correlated features to be selected or removed together. Recently, some effective algorithms have been proposed to solve the elastic-net SVMs with different convex loss functions, such as hinge, squared hinge, huberized hinge, pinball and huberized pinball. In this paper, we develop a linearized alternating direction method of multipliers (LADMM) algorithm to solve above elastic-net SVMs. In addition, our algorithm can be applied to solve some new elastic-net SVMs such as elastic-net least squares SVM . Compared with some existing algorithms, our algorithm has comparable or better performances in terms of computational cost and accuracy. Under mild conditions , we prove the convergence and derive convergence rate of our algorithm. Furthermore, numerical experiments on synthetic and real datasets demonstrate the feasibility and validity of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Rongmei Liang and Xiaofei Wu and Zhimin Zhang},
  doi          = {10.1016/j.patcog.2023.110134},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110134},
  shortjournal = {Pattern Recognition},
  title        = {Linearized alternating direction method of multipliers for elastic-net support vector machines},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based stroke relation encoding for online
handwriting and sketches. <em>PR</em>, <em>148</em>, 110131. (<a
href="https://doi.org/10.1016/j.patcog.2023.110131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke classification for online handwriting and sketches, aimed at grouping strokes into different semantic categories, has drawn considerable attention due to its wide applications. This task is challenging since individual strokes look similar and are easily confused with each other. The key is to consider both the individual strokes and the contextual information jointly for making prediction. Previous methods are insufficient in modeling and exploiting complex contextual information of strokes. To overcome this limitation, we propose a Transformer-based model for Online Handwriting and Sketches (T-OHS), with novel relation encoding schemes to take advantage of temporal and spatial information in stroke sequence. Particularly, we introduce a coarse-to-fine hierarchical encoding approach based on the polar coordinate system for precisely modeling spatial relations between strokes. Experiments on three types of handwriting data, including online handwritten documents, diagrams, and sketches, demonstrate that our method is universal and provides state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Jing-Yu Liu and Yan-Ming Zhang and Fei Yin and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2023.110131},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110131},
  shortjournal = {Pattern Recognition},
  title        = {Transformer-based stroke relation encoding for online handwriting and sketches},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure correspondence searching of CAD model using local
feature-based description and indexing. <em>PR</em>, <em>148</em>,
110126. (<a href="https://doi.org/10.1016/j.patcog.2023.110126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CAD model retrieval has played a significant role in various applications, including product development and knowledge mining. However, most existing retrieval methods compare 3D shape similarity from a global perspective, while detecting similar structures automatically for CAD models remains a challenging problem. Consequently, this study proposes a structure correspondence searching framework for CAD models to address the issues. According to the boundary representation (B-rep) information, the proposed method first segments a CAD model into a set of local features denoted as structural cells. Then, the descriptor of each structural cell is extracted using a weighted shape distribution vector and neighbor set. In order to speed up the matching of structural cells, an indexing and filtering mechanism is constructed based on the shape clustering and topological analysis. The matched structural cells determine the boundary of similar structures. Finally, similarity measurement is conducted to generate a ranking list by analyzing the quality of the matched structural cells. The rationality and efficiency of the proposed approach are demonstrated via an analysis of experimental results.},
  archive      = {J_PR},
  author       = {Baoning Ji and Jie Zhang and Yuan Li and Wenbin Tang},
  doi          = {10.1016/j.patcog.2023.110126},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110126},
  shortjournal = {Pattern Recognition},
  title        = {Structure correspondence searching of CAD model using local feature-based description and indexing},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental feature selection for dynamic incomplete data
using sub-tolerance relations. <em>PR</em>, <em>148</em>, 110125. (<a
href="https://doi.org/10.1016/j.patcog.2023.110125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tolerance Rough Set (TRS) theory is commonly employed for feature selection with incomplete data. However, TRS has limitations such as ignoring uncertainty, which often leads to the inclusion of redundant features and diminished classification accuracy . To address these limitations, we propose an extension called Subrelation Tolerance Class (STC). STC decomposes the tolerance relation into two subrelations, enabling a two-stage certainty measurement. This approach progressively filters out certain regions, thereby reducing computational space requirements, and introduces a new significance measure that considers both certain and uncertain information. Leveraging STC and our proposed measure, we develop an incremental feature selection algorithm capable of handling incomplete streaming data. We conduct experiments on real-world datasets and compare the performance with existing algorithms to validate the superiority of our method. The experimental results show that our algorithm reduces the execution time by over 89.78% compared to the baselines while maintaining the classification accuracy .},
  archive      = {J_PR},
  author       = {Jie Zhao and Yun Ling and Faliang Huang and Jiahai Wang and Eric W.K. See-To},
  doi          = {10.1016/j.patcog.2023.110125},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110125},
  shortjournal = {Pattern Recognition},
  title        = {Incremental feature selection for dynamic incomplete data using sub-tolerance relations},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Kinship similarity for open sets. <em>PR</em>, <em>148</em>,
110123. (<a href="https://doi.org/10.1016/j.patcog.2023.110123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of image-based kinship recognition methods is to determine the genetic relationship between people from their face images. Kinship recognition and related methods have many applications in computer vision . Most of the methods are very meaningful but are focused on closed sets. However, for real-life scenarios, kinship recognition is an open set problem. In contrast to previous methods, this paper considers kinship recognition for open sets. Further, the aim is to determine family relationships and their corresponding degrees of kinship. Our method is pairwise-based and is able to exploit mutual information from positive pairs. Large scale experiments and ablation studies show that our method (1) reaches SOTA performance on the FIW dataset, (2) is able to properly separate kinship categories using pairwise similarity and (3) generates uniform similarity distributions.},
  archive      = {J_PR},
  author       = {Wei Wang and Shaodi You and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.patcog.2023.110123},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110123},
  shortjournal = {Pattern Recognition},
  title        = {Kinship similarity for open sets},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reducing communication in federated learning via efficient
client sampling. <em>PR</em>, <em>148</em>, 110122. (<a
href="https://doi.org/10.1016/j.patcog.2023.110122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) ameliorates privacy concerns in settings where a central server coordinates learning from data distributed across many clients; rather than sharing the data, the clients train locally and report the models they learn to the server. Aggregation of local models requires communicating massive amounts of information between the clients and the server, consuming network bandwidth . We propose a novel framework for updating the global model in communication-constrained FL systems by requesting input only from the clients with informative updates, and estimating the local updates that are not communicated. Specifically, describing the progression of the model’s weights by an Ornstein–Uhlenbeck process allows us to develop sampling strategy for selecting a subset of clients with significant weight updates; model updates of the clients not selected for communication are replaced by their estimates. We test this policy on realistic federated benchmark datasets and show that the proposed framework provides up to 50% reduction in communication while maintaining competitive or achieving superior performance compared to baselines. The proposed method represents a new line of strategies for communication-efficient FL that is orthogonal to the existing user-driven techniques, such as compression, thus complementing rather than aiming to replace those existing methods.},
  archive      = {J_PR},
  author       = {Mónica Ribero and Haris Vikalo},
  doi          = {10.1016/j.patcog.2023.110122},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110122},
  shortjournal = {Pattern Recognition},
  title        = {Reducing communication in federated learning via efficient client sampling},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised video distortion correction algorithm based
on iterative optimization. <em>PR</em>, <em>148</em>, 110114. (<a
href="https://doi.org/10.1016/j.patcog.2023.110114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wide-angle video frames typically contain more information, but they also exhibit distortion that degrades the visual quality, especially at the edges. To eliminate this distortion from videos, we propose a self-supervised iterative optimization method in this paper. Specifically, we construct a motion parameter estimation model utilizing two consecutive distorted frames, where motion parameters comprise affine transform and distortion parameters. We apply the Gauss–Newton algorithm to minimize the sum-of-squares error between frames and update parameters. Treating inter-frame motion as undistort-affine-distort transformations, frame alignment is achieved by continuously adjusting transform parameters. Ultimately, frames are corrected using the converged parameters. We generated a synthetic dataset with various distortion parameters for evaluation. Experiments demonstrate superior performance versus state-of-the-art methods on synthetic and real wide-angle videos. Our algorithm also achieves higher parameter estimation accuracy without sacrificing efficiency.},
  archive      = {J_PR},
  author       = {Zhihao Ren and Ya Su},
  doi          = {10.1016/j.patcog.2023.110114},
  journal      = {Pattern Recognition},
  month        = {4},
  pages        = {110114},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised video distortion correction algorithm based on iterative optimization},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ECLAD: Extracting concepts with local aggregated
descriptors. <em>PR</em>, <em>147</em>, 110146. (<a
href="https://doi.org/10.1016/j.patcog.2023.110146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are increasingly being used in critical systems , where robustness and alignment are crucial. In this context, the field of explainable artificial intelligence has proposed the generation of high-level explanations of the prediction process of CNNs through concept extraction. While these methods can detect whether or not a concept is present in an image, they are unable to determine its location. What is more, a fair comparison of such approaches is difficult due to a lack of proper validation procedures. To address these issues, we propose a novel method for automatic concept extraction and localization based on representations obtained through pixel-wise aggregations of CNN activation maps. Further, we introduce a process for the quantitative comparison and validation of concept-extraction techniques based on synthetic datasets with pixel-wise annotations of their main components, mitigating possible confirmation biases induced by human visual inspection . Extensive experimentation on both synthetic and real-world datasets demonstrates that our method outperforms state-of-the-art alternatives.},
  archive      = {J_PR},
  author       = {Andrés Felipe Posada-Moreno and Nikita Surya and Sebastian Trimpe},
  doi          = {10.1016/j.patcog.2023.110146},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110146},
  shortjournal = {Pattern Recognition},
  title        = {ECLAD: Extracting concepts with local aggregated descriptors},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CS-net: Conv-simpleformer network for agricultural image
segmentation. <em>PR</em>, <em>147</em>, 110140. (<a
href="https://doi.org/10.1016/j.patcog.2023.110140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agricultural image segmentation needs to catch up to the development speed of deep learning, and the explosive computational overhead and limited high-quality labeled datasets are the main reasons preventing the application of Transformers to agricultural image segmentation. This study proposes a Simple-Attention Block (SIAB) using channel-by-channel and spatial convolutional computation, whose computational complexity is linearly correlated with the input image size. Then, we design a Simpleformer by cascading SIAB and FFN to reshape the Transformer architecture. Further, the fusion of CNN and Simpleformer constructs a dataset quality-independent agricultural image segmentation model (CS-Net). Finally, we evaluate CS-Net on four datasets, and compared with the state-of-the-art models, CS-Net has more advantageous inference speed and segmentation accuracy, which pushes the development of Transformers in the field of agricultural image processing. Additionally, we explore the reasons for the Transformers’ performance collapse for agricultural applications, providing research scholars with a theoretical foundation for related issues.},
  archive      = {J_PR},
  author       = {Lei Liu and Guorun Li and Yuefeng Du and Xiaoyu Li and Xiuheng Wu and Zhi Qiao and Tianyi Wang},
  doi          = {10.1016/j.patcog.2023.110140},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110140},
  shortjournal = {Pattern Recognition},
  title        = {CS-net: Conv-simpleformer network for agricultural image segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConvGeN: A convex space learning approach for
deep-generative oversampling and imbalanced classification of small
tabular datasets. <em>PR</em>, <em>147</em>, 110138. (<a
href="https://doi.org/10.1016/j.patcog.2023.110138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oversampling is commonly used to improve classifier performance for small tabular imbalanced datasets. State-of-the-art linear interpolation approaches can be used to generate synthetic samples from the convex space of the minority class. Generative networks are common deep learning approaches for synthetic sample generation. However, their scope on synthetic tabular data generation in the context of imbalanced classification is not adequately explored. In this article, we show that existing deep generative models perform poorly compared to linear interpolation-based approaches for imbalanced classification problems on small tabular datasets. To overcome this, we propose a deep generative model, ConvGeN that combines the idea of convex space learning with deep generative models . ConvGeN learns coefficients for the convex combinations of the minority class samples, such that the synthetic data is distinct enough from the majority class . Our benchmarking experiments demonstrate that our proposed model ConvGeN improves imbalanced classification on such small datasets, as compared to existing deep generative models, while being on par with the existing linear interpolation approaches. Moreover, we discuss how our model can be used for synthetic tabular data generation in general, even outside the scope of data imbalance, and thus improves the overall applicability of convex space learning.},
  archive      = {J_PR},
  author       = {Kristian Schultz and Saptarshi Bej and Waldemar Hahn and Markus Wolfien and Prashant Srivastava and Olaf Wolkenhauer},
  doi          = {10.1016/j.patcog.2023.110138},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110138},
  shortjournal = {Pattern Recognition},
  title        = {ConvGeN: A convex space learning approach for deep-generative oversampling and imbalanced classification of small tabular datasets},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). H-CapsNet: A capsule network for hierarchical image
classification. <em>PR</em>, <em>147</em>, 110135. (<a
href="https://doi.org/10.1016/j.patcog.2023.110135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present H-CapsNet, a capsule network for hierarchical image classification . Our network makes use of the natural capacity of CapsNets (capsule networks) to capture hierarchical relationships. Thus, our network is such that each multi-layer capsule network accounts for each of the class hierarchies using dedicated capsules. Further, we make use of a modified hinge loss that enforces consistency amongst the hierarchies involved. We also present a strategy to dynamically adjust the training parameters to achieve a better balance between the class hierarchies under consideration. We have performed experiments using several widely available datasets and compared them against several alternatives. In our experiments, H-CapsNet delivers a margin of improvement over competing hierarchical classification networks elsewhere in the literature.},
  archive      = {J_PR},
  author       = {Khondaker Tasrif Noor and Antonio Robles-Kelly},
  doi          = {10.1016/j.patcog.2023.110135},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110135},
  shortjournal = {Pattern Recognition},
  title        = {H-CapsNet: A capsule network for hierarchical image classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint feature generation and open-set prototype learning for
generalized zero-shot open-set classification. <em>PR</em>,
<em>147</em>, 110133. (<a
href="https://doi.org/10.1016/j.patcog.2023.110133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In generalized zero-shot classification, test samples can belong to either seen or unseen classes. However, in real-world situations, there may be many open-set samples in the test set where neither visual nor semantic representations of the classes are provided. The new problem is defined as generalized zero-shot open-set classification (GZSOSC). The purpose is to tell whether an instance belongs to which seen or unseen classes, or to reject an instance if it belongs to the open-set classes. To address this problem, we propose a novel method called Joint Feature Generation and Open-Set Prototype Learning (JFGOPL) for GZSOSC tasks . JFGOPL is presented to combine GAN training with open-set prototype learning, where the former generates high-quality unseen and open-set samples and the latter learns some open-set prototypes. Specifically, a novel GAN training strategy is proposed, where an intra-class compactness loss and an inter-class dispersion loss are proposed to ensure the discrimination of the generated samples and to make the learned embedding network less susceptible to the domain shift problem. Furthermore, open-set prototypes are derived by projecting confident open-set samples into the semantic space using the updated embedding network. Experiments on widely used benchmarks demonstrate the superiority of JFGOPL over existing methods for tackling the challenging GZSOSC problem.},
  archive      = {J_PR},
  author       = {Xiao Li and Min Fang and Zhibo Zhai},
  doi          = {10.1016/j.patcog.2023.110133},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110133},
  shortjournal = {Pattern Recognition},
  title        = {Joint feature generation and open-set prototype learning for generalized zero-shot open-set classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). HairManip: High quality hair manipulation via hair element
disentangling. <em>PR</em>, <em>147</em>, 110132. (<a
href="https://doi.org/10.1016/j.patcog.2023.110132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hair editing is challenging due to the complexity and variety of hair materials and shapes. Existing methods employ reference images or user-painted masks to edit hair and have achieved promising results. However, discrepancies in color and shape between the source and target hair can occasionally result in unrealistic results. Therefore, we propose a new hair editing method named HairManip, which decouples the hair information from the input source image into shape and color components. We then train hairstyle and hair color editing sub-networks to handle this complex information independently. To further enhance editing efficiency and accuracy, we introduce a latent code preprocessing module that effectively extracts meaningful features from hair regions, thereby improving the model’s editing capabilities. The experimental results demonstrate that our method achieves significant results in editing accuracy and authenticity, thanks to the carefully designed network structure and loss functions. Code can be found at https://github.com/Zlin0530/HairManip .},
  archive      = {J_PR},
  author       = {Huihuang Zhao and Lin Zhang and Paul L. Rosin and Yu-Kun Lai and Yaonan Wang},
  doi          = {10.1016/j.patcog.2023.110132},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110132},
  shortjournal = {Pattern Recognition},
  title        = {HairManip: High quality hair manipulation via hair element disentangling},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological safeguard for evasion attack interpreting the
neural networks’ behavior. <em>PR</em>, <em>147</em>, 110130. (<a
href="https://doi.org/10.1016/j.patcog.2023.110130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but raising new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model’s decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a novel detector of evasion attacks is developed. It focuses on the information on the activations of the neurons given by the model when an input sample is injected. Moreover, it pays attention to the topology of the targeted deep learning model to analyze the activations according to which neurons are connecting. This approach is motivated from the observation that the literature shows that the targeted model’s topology contains essential information about if the evasion attack occurs. For this purpose, a huge data preprocessing is required to introduce all this information in the detector, which uses the Graph Convolutional Neural Network (GCN) technology. Thus, it understands the topology of the target model, obtaining promising results and improving the outcomes presented in the literature related to similar defenses.},
  archive      = {J_PR},
  author       = {Xabier Echeberria-Barrio and Amaia Gil-Lerchundi and Iñigo Mendialdua and Raul Orduna-Urrutia},
  doi          = {10.1016/j.patcog.2023.110130},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110130},
  shortjournal = {Pattern Recognition},
  title        = {Topological safeguard for evasion attack interpreting the neural networks’ behavior},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disturbance rejection with compensation on features.
<em>PR</em>, <em>147</em>, 110129. (<a
href="https://doi.org/10.1016/j.patcog.2023.110129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pattern recognition tasks, the information from system input is modeled through a series of nonlinear operations, which include but not limited to feature extraction, regression, and classification. Both theoretically and practically, these operations are inevitably subject to internal modeling error and external disturbance , resulting at a performance challenge. Those state-of-the-art methods, e.g. Convolutional Neural Network and Transformer, still display significant instabilities and failures under practical applications, so comes a lack of generalization. Consequently, the more robust pattern recognition methods and related theories still merit a further study. This paper firstly reviews those state-of-the-art technologies in the field. The bottleneck of performances in those latest researches is associated with a lack of disturbance estimation and corresponding compensation. Therefore, the implications of disturbance rejection in pattern recognition field are further discussed from a control point of view. Then, the open problems are summarized. Ultimately, a discussion of the potential solutions, which is related to the application of compensation on features, is given to highlight the future study. Through the systematic review in this paper, the disturbance rejection in pattern recognition is developed into a control problem. Hopefully, more effective control technologies for the compensation on features can be used to improve the robustness of pattern recognition theoretically and practically.},
  archive      = {J_PR},
  author       = {Xiaobo Hu and Jianbo Su and Jun Zhang},
  doi          = {10.1016/j.patcog.2023.110129},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110129},
  shortjournal = {Pattern Recognition},
  title        = {Disturbance rejection with compensation on features},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified and efficient semi-supervised learning framework
for stereo matching. <em>PR</em>, <em>147</em>, 110128. (<a
href="https://doi.org/10.1016/j.patcog.2023.110128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, stereo matching algorithms have made tremendous progress in terms of both accuracy and efficiency. However, it remains a great challenge to train a practical model due to the scarcity of ground truth disparity. In this paper, we propose a Semi-supervised Stereo Matching Framework (SSMF), i.e., a continuous self-training pipeline involving both teacher model and student model. The proposed framework combines the consistency regularization with the entropy minimization to effectively utilize the unlabeled data in large quantities. To enhance the quality of pseudo disparities: (1) we introduce an additional branch in stereo models to regress confidence map and perform adaptive selection; (2) we inject strong data augmentations on unlabeled data to achieve the prediction consistency; (3) we propose a novel loss function to utilize pseudo disparities of different confidence levels. Comprehensive experimental results show that the proposed framework enables to largely improve the disparity accuracy and robustness. Moreover, it also demonstrates competitive performance in cross-domain scenarios. Among all published methods as of August 2023, it achieves 1st on KITTI 2012 benchmark and 4th on KITTI 2015 benchmark. Code is available at https://github.com/Twil-7/semi-supervised-stereo-matching .},
  archive      = {J_PR},
  author       = {Fudong Xu and Lin Wang and Huibin Li},
  doi          = {10.1016/j.patcog.2023.110128},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110128},
  shortjournal = {Pattern Recognition},
  title        = {A unified and efficient semi-supervised learning framework for stereo matching},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight unsupervised adversarial detector based on
autoencoder and isolation forest. <em>PR</em>, <em>147</em>, 110127. (<a
href="https://doi.org/10.1016/j.patcog.2023.110127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep neural networks (DNNs) have performed well on many perceptual tasks, they are vulnerable to adversarial examples that are generated by adding slight but maliciously crafted perturbations to benign images. Adversarial detection is an important technique for identifying adversarial examples before they are entered into target DNNs. Previous studies that were performed to detect adversarial examples either targeted specific attacks or required expensive computation. Designing a lightweight unsupervised detector is still a challenging problem. In this paper, we propose an A uto E ncoder-based A dversarial E xamples ( AEAE ) detector that can guard DNN models by detecting adversarial examples with low computation in an unsupervised manner . The AEAE includes only a shallow autoencoder that performs two roles. First, a well-trained autoencoder has learned the manifold of benign examples. This autoencoder can produce a large reconstruction error for adversarial images with large perturbations, so we can detect significantly perturbed adversarial examples based on the reconstruction error. Second, the autoencoder can filter out small noises and change the DNN’s prediction on adversarial examples with small perturbations. It helps to detect slightly perturbed adversarial examples based on the prediction distance. To cover these two cases, we utilize the reconstruction error and prediction distance from benign images to construct a two-tuple feature set and train an adversarial detector using the isolation forest algorithm. We show empirically that AEAE is an unsupervised and inexpensive detector against most state-of-the-art attacks. Through the detection in these two cases, there is nowhere to hide adversarial examples.},
  archive      = {J_PR},
  author       = {Hui Liu and Bo Zhao and Jiabao Guo and Kehuan Zhang and Peng Liu},
  doi          = {10.1016/j.patcog.2023.110127},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110127},
  shortjournal = {Pattern Recognition},
  title        = {A lightweight unsupervised adversarial detector based on autoencoder and isolation forest},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When IC meets text: Towards a rich annotated integrated
circuit text dataset. <em>PR</em>, <em>147</em>, 110124. (<a
href="https://doi.org/10.1016/j.patcog.2023.110124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated Optical Inspection (AOI) is a process that uses cameras to autonomously scan printed circuit boards for quality control . Text is often printed on chip components, and it is crucial that this text is correctly recognized during AOI, as it contains valuable information. In this paper, we introduce ICText , the largest dataset for text detection and recognition on integrated circuits . Uniquely, it includes labels for character quality attributes such as low contrast, blurry, and broken. While loss-reweighting and Curriculum Learning (CL) have been proposed to improve object detector performance by balancing positive and negative samples and gradually training the model from easy to hard samples, these methods have had limited success with one-stage object detectors commonly used in industry. To address this, we propose Attribute-Guided Curriculum Learning (AGCL), which leverages the labeled character quality attributes in ICText . Our extensive experiments demonstrate that AGCL can be applied to different detectors in a plug-and-play fashion to achieve higher Average Precision (AP), significantly outperforming existing methods on ICText without any additional computational overhead during inference. Furthermore, we show that AGCL is also effective on the generic object detection dataset Pascal VOC. Our code and dataset will be publicly available at https://github.com/chunchet-ng/ICText-AGCL .},
  archive      = {J_PR},
  author       = {Chun Chet Ng and Che-Tsung Lin and Zhi Qin Tan and Xinyu Wang and Jie Long Kew and Chee Seng Chan and Christopher Zach},
  doi          = {10.1016/j.patcog.2023.110124},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110124},
  shortjournal = {Pattern Recognition},
  title        = {When IC meets text: Towards a rich annotated integrated circuit text dataset},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable and accurate subsequence transform for time series
classification. <em>PR</em>, <em>147</em>, 110121. (<a
href="https://doi.org/10.1016/j.patcog.2023.110121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification using phase-independent subsequences called shapelets is one of the best approaches in the state of the art. This approach is especially characterized by its interpretable property and its fast prediction time. However, given a dataset of n n time series of length at most m m , learning shapelets requires a computation time of O ( n 2 m 4 ) O(n2m4) which is too high for practical datasets. In this paper, we exploit the fact that shapelets are shared by the members of the same class to propose the SAST (Scalable and Accurate Subsequence Transform) algorithm which has a time complexity of O ( n m 3 ) O(nm3) . SAST is accurate, interpretable and does not learn redundant shapelets. The experiments we conducted on the UCR archive datasets showed that SAST is more accurate than the state of the art Shapelet Transform algorithm while being significantly more scalable.},
  archive      = {J_PR},
  author       = {Michael Franklin Mbouopda and Engelbert Mephu Nguifo},
  doi          = {10.1016/j.patcog.2023.110121},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110121},
  shortjournal = {Pattern Recognition},
  title        = {Scalable and accurate subsequence transform for time series classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance-based weighted transformer network for image
completion. <em>PR</em>, <em>147</em>, 110120. (<a
href="https://doi.org/10.1016/j.patcog.2023.110120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of image generation has been effectively modeled as a problem of structure priors or transformation. However, existing models have unsatisfactory performance in understanding the global input image structures because of particular inherent features (for example, local inductive prior). Recent studies have shown that self-attention is an efficient modeling technique for image completion problems. In this paper, we propose a new architecture that relies on Distance-based Weighted Transformer (DWT) to better understand the relationships between an image’s components. In our model, we leverage the strengths of both Convolutional Neural Networks (CNNs) and DWT blocks to enhance the image completion process. Specifically, CNNs are used to augment the local texture information of coarse priors and DWT blocks are used to recover certain coarse textures and coherent visual structures. Unlike current approaches that generally use CNNs to create feature maps, we use the DWT to encode global dependencies and compute distance-based weighted feature maps, which substantially minimizes the problem of visual ambiguities. Meanwhile, to better produce repeated textures, we introduce Residual Fast Fourier Convolution (Res-FFC) blocks to combine the encoder’s skip features with the coarse features provided by our generator. Furthermore, a simple yet effective technique is proposed to normalize the non-zero values of convolutions, and fine-tune the network layers for regularization of the gradient norms to provide an efficient training stabilizer. Extensive quantitative and qualitative experiments on three challenging datasets demonstrate the superiority of our proposed model compared to existing approaches.},
  archive      = {J_PR},
  author       = {Pourya Shamsolmoali and Masoumeh Zareapoor and Huiyu Zhou and Xuelong Li and Yue Lu},
  doi          = {10.1016/j.patcog.2023.110120},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110120},
  shortjournal = {Pattern Recognition},
  title        = {Distance-based weighted transformer network for image completion},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial and focused training of abnormal videos for
weakly-supervised anomaly detection. <em>PR</em>, <em>147</em>, 110119.
(<a href="https://doi.org/10.1016/j.patcog.2023.110119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the sparsity and scarcity of abnormal events, intra-video and inter-video data imbalance problems are fundamental issues for the weakly supervised video anomaly detection (WS-VAD) task. Many previous works have made great progress in the intra-video data imbalance problem while lacking attention to the inter-video case. However, we find that when reducing the number of abnormal videos used for training, the performance of some existing state-of-the-art WS-VAD methods will be decreased. To alleviate this problem, we propose a novel solution by adversarial and focused training (AFT) of abnormal videos. Specifically, our solution consists of two modules. One is a data-based adversarial training (AT) module that performs data augmentation through latent space-based adversarial sample generation of abnormal videos, and the other is a model-based focused training (FT) module that focuses on the cost-sensitive loss of abnormal videos. Once the whole pipeline has been trained, a score-level late fusion strategy is employed to combine the abnormal scores of both adversarial training and focused training modules in the testing phase. The effectiveness of the proposed approach is demonstrated on UCF-Crime, ShanghaiTech, XD-Violence, and UCSD Peds datasets in both the inter-video data imbalanced experimental setting and the original experimental setting. The source code is available at: https://github.com/Destind/AFT_codes .},
  archive      = {J_PR},
  author       = {Ping He and Fan Zhang and Gang Li and Huibin Li},
  doi          = {10.1016/j.patcog.2023.110119},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110119},
  shortjournal = {Pattern Recognition},
  title        = {Adversarial and focused training of abnormal videos for weakly-supervised anomaly detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PWDformer: Deformable transformer for long-term series
forecasting. <em>PR</em>, <em>147</em>, 110118. (<a
href="https://doi.org/10.1016/j.patcog.2023.110118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term forecasting is of paramount importance in numerous scenarios, including predicting future energy, water, and food consumption. For instance, extreme weather events and natural disasters can profoundly impact infrastructure operations and pose severe safety concerns. Traditional CNN-based models often struggle to capture long-distance dependencies effectively. In contrast, Transformers-based models have shown significant promise in long-term forecasting . This paper investigates the long-term forecasting problem and identifies a common limitation in existing Transformer-based models: they tend to reduce computational complexity at the expense of time information aggregation capability. Moreover, the order of time series plays a crucial role in accurate predictions, but current Transformer-based models lack sensitivity to time series order, rendering them unreasonable. To address these issues, we propose a novel Deformable-Local (DL) aggregation mechanism. This mechanism enhances the model’s ability to aggregate time information and allows the model to adaptively adjust the size of the time aggregation window. Consequently, the model can discern more complex time patterns, leading to more accurate predictions. Additionally, our model incorporates a Frequency Selection module to reinforce effective features and reduce noise. Furthermore, we introduce Position Weights to mitigate the order-insensitivity problem present in existing methods. In extensive evaluations of long-term forecasting tasks, we conducted benchmark tests on six datasets covering various practical applications, including energy, traffic, economics, weather, and disease. Our method achieved state-of-the-art (SOTA) results, demonstrating significant improvements. For instance, on the ETT dataset, our model achieved an average MSE improvement of approximately 19% and an average MAE improvement of around 27% . Remarkably, for predicted lengths of 96 and 192, we achieved outstanding MSE and MAE improvements of 32.1% and 30.9% , respectively.},
  archive      = {J_PR},
  author       = {Zheng Wang and Haowei Ran and Jinchang Ren and Meijun Sun},
  doi          = {10.1016/j.patcog.2023.110118},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110118},
  shortjournal = {Pattern Recognition},
  title        = {PWDformer: Deformable transformer for long-term series forecasting},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSA-GCN: Multiscale adaptive graph convolution network for
gait emotion recognition. <em>PR</em>, <em>147</em>, 110117. (<a
href="https://doi.org/10.1016/j.patcog.2023.110117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait emotion recognition plays a crucial role in the intelligent system. Most existing approaches identify emotions by focusing on local actions over time. However, some valuable observational facts that the effective distances of different emotions in the time domain are different, and the local actions during walking are quite similar, are put aside in those methods. And this ignorance often ends up impairing performance of emotion recognition. To address the issues, a novel model, named MSA-GCN ( M ulti S cale A daptive G raph C onvolution N etwork), is proposed to utilize the valuable observational knowledge for improving emotion recognition performance. In the proposed model, an adaptive spatio-temporal graph convolution is designed to dynamically select convolution kernels to learn the spatio-temporal features of different emotions. Moreover, a Cross-Scale Mapping Interaction mechanism (CSMI) is proposed to construct an adaptive adjacency matrix for high-quality aggregation of the multiscale information. Extensive experimental results on public datasets indicate that, compared with the state-of-the-art methods, the proposed approach achieves better performance in terms of emotion recognition accuracy , and shows the proposed approach is promising.},
  archive      = {J_PR},
  author       = {Yunfei Yin and Li Jing and Faliang Huang and Guangchao Yang and Zhuowei Wang},
  doi          = {10.1016/j.patcog.2023.110117},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110117},
  shortjournal = {Pattern Recognition},
  title        = {MSA-GCN: Multiscale adaptive graph convolution network for gait emotion recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JoyPose: Jointly learning evolutionary data augmentation and
anatomy-aware global–local representation for 3D human pose estimation.
<em>PR</em>, <em>147</em>, 110116. (<a
href="https://doi.org/10.1016/j.patcog.2023.110116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based 3D human pose estimation is an important yet challenging task for many human-involved pattern recognition systems . Existing deep learning-based 3D human pose estimation methods are faced with the problems of lacking large-scale training data and lacking effective solutions to represent the complicated human body structure. To this end, this paper proposes a jo intl y learning framework entitled JoyPose that simultaneously leverages both human pose data augmentation and human pose estimation. In particular, JoyPose consists of an evolutionary data augmentation module and an anatomy-aware global–local pose feature representation module for 3D human pose estimation. The evolution for data augmentation is guided by a reinforcement learning strategy in a probabilistic way according to pose estimation loss. The anatomy-aware global–local pose feature representation module separately captures global features and local features according to anatomical and kinematic patterns observed from pose estimation errors across different human joints . The performance of the final human pose estimation is leveraged by both data augmentation and anatomy-aware global–local feature representation. Extensive experiments on three real-world datasets demonstrate the superiority and robustness against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Songlin Du and Zhiwei Yuan and Peifu Lai and Takeshi Ikenaga},
  doi          = {10.1016/j.patcog.2023.110116},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110116},
  shortjournal = {Pattern Recognition},
  title        = {JoyPose: Jointly learning evolutionary data augmentation and anatomy-aware global–local representation for 3D human pose estimation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UniG-encoder: A universal feature encoder for graph and
hypergraph node classification. <em>PR</em>, <em>147</em>, 110115. (<a
href="https://doi.org/10.1016/j.patcog.2023.110115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the decent performance and fruitful applications of Graph Neural Networks (GNNs), Hypergraph Neural Networks (HGNNs), and their well-designed variants, on some commonly used benchmark graphs and hypergraphs, they are outperformed by even a simple Multi-Layer Perceptron . This observation motivates a reexamination of the design paradigm of the current GNNs and HGNNs and poses challenges of extracting graph features effectively. In this work, a universal feature encoder for both graph and hypergraph representation learning is designed, called UniG-Encoder. The architecture starts with a forward transformation of the topological relationships of connected nodes into edge or hyperedge features via a normalized projection matrix . The resulting edge/hyperedge features, together with the original node features, are fed into a neural network. The encoded node embeddings are then derived from the reversed transformation, described by the transpose of the projection matrix, of the network’s output, which can be further used for tasks such as node classification . The designed projection matrix, encoding the graph features, is intuitive and interpretable. The proposed architecture, in contrast to the traditional spectral-based and/or message passing approaches , simultaneously and comprehensively exploits the node features and graph/hypergraph topologies in an efficient and unified manner, covering both heterophilic and homophilic graphs. Furthermore, a variant version, UniG-Encoder II, is devised to leverage multi-hop node information. Extensive experiments are conducted and demonstrate the superior performance of the proposed framework on twelve representative hypergraph datasets and six real-world graph datasets, compared to the state-of-the-art methods. Our implementation is available online at https://github.com/MinhZou/UniG-Encoder .},
  archive      = {J_PR},
  author       = {Minhao Zou and Zhongxue Gan and Yutong Wang and Junheng Zhang and Dongyan Sui and Chun Guan and Siyang Leng},
  doi          = {10.1016/j.patcog.2023.110115},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110115},
  shortjournal = {Pattern Recognition},
  title        = {UniG-encoder: A universal feature encoder for graph and hypergraph node classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning conditional variational autoencoders with missing
covariates. <em>PR</em>, <em>147</em>, 110113. (<a
href="https://doi.org/10.1016/j.patcog.2023.110113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional variational autoencoders (CVAEs) are versatile deep latent variable models that extend the standard VAE framework by conditioning the generative model with auxiliary covariates. The original CVAE model assumes that the data samples are independent, whereas more recent conditional VAE models, such as the Gaussian process (GP) prior VAEs, can account for complex correlation structures across all data samples. While several methods have been proposed to learn standard VAEs from partially observed datasets, these methods fall short for conditional VAEs. In this work, we propose a method to learn conditional VAEs from datasets in which auxiliary covariates can contain missing values as well. The proposed method augments the conditional VAEs with a prior distribution for the missing covariates and estimates their posterior using amortised variational inference. At training time, our method accounts for the uncertainty associated with the missing covariates while simultaneously maximising the evidence lower bound. We develop computationally efficient methods to learn CVAEs and GP prior VAEs that are compatible with mini-batching. Our experiments on simulated datasets as well as on real-world biomedical datasets show that the proposed method outperforms previous methods in learning conditional VAEs from non-temporal, temporal, and longitudinal datasets.},
  archive      = {J_PR},
  author       = {Siddharth Ramchandran and Gleb Tikhonov and Otto Lönnroth and Pekka Tiikkainen and Harri Lähdesmäki},
  doi          = {10.1016/j.patcog.2023.110113},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110113},
  shortjournal = {Pattern Recognition},
  title        = {Learning conditional variational autoencoders with missing covariates},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion method based on spiking neural convolutional
network for edge detection. <em>PR</em>, <em>147</em>, 110112. (<a
href="https://doi.org/10.1016/j.patcog.2023.110112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {NSNP-type neuron is a new type of neuron model inspired by nonlinear spiking mechanisms in nonlinear spiking neural P systems. In order to address the loss problem of edge detail information in edge detection methods based on deep learning , we propose a feature fusion method based on NSNP-type neurons. The architecture of this feature fusion method consists of two modules: feature extraction module and feature fusion module. In particular, the feature fusion module is composed of convolutional blocks constructed by NSNP-type neurons for multi-level feature fusions, and CoT blocks with Transformer style is introduced to extract rich contextual information from low-level features and high-level features. To fuse multi-level features and preserve contextual information, we design a new loss function that not only preserves feature prediction loss and fusion loss, but also considers contour-related and texture-related information. The proposed method is evaluated on BSDS500 and NYUDv2 data sets and compare it with 9 baseline methods and 12 CNN-based methods, and we achieve ODS of 0.808 and OIS of 0.827 on BSDS500. The comparison results demonstrate the advantages of the proposed method for edge detection. The source code is available at https://github.com/xhuph66/FF-CNSNP-master .},
  archive      = {J_PR},
  author       = {Ronghao Xian and Xin Xiong and Hong Peng and Jun Wang and Antonio Ramírez de Arellano Marrero and Qian Yang},
  doi          = {10.1016/j.patcog.2023.110112},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110112},
  shortjournal = {Pattern Recognition},
  title        = {Feature fusion method based on spiking neural convolutional network for edge detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Vital information is only worth one thumbnail: Towards
efficient human pose estimation. <em>PR</em>, <em>147</em>, 110111. (<a
href="https://doi.org/10.1016/j.patcog.2023.110111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pursuit of impressive performance, existing DCNN-based approaches of human pose estimation usually use massive networks and large-size images to train a deep model. When applying these deep based methods in real-time systems, current works try to compress the deep network by reducing the number of layers and channels, but such approaches are complex and poorly generalized since they require elaborate design of small-scale network structures. Based on the fact that large-size images contain redundant information, in this paper, we explore the influence of image-size on system complexity and propose a novel framework called ThumbPose to accelerate and compress deep models by inferring on thumbnail representations in the task of human pose estimation. In our framework, we first propose a style supervised online downscaler to reduce an input image into a thumbnail image . Furthermore, a training strategy of dual-branch auto-encoding is designed to obtain effective and accurate thumbnail representation in a knowledge distillation manner, which is further used to maintain the performance of thumbnail images as the original-size input images. For heat-map based human pose estimation, ThumbPose is an orthogonal and implementation-friendly method, that can not only compress and accelerate the inference network but also obtain an image downscaler in a supervised manner that can be used in other high-level tasks ( e.g. detection, segmentation, etc. in practical applications). Extensive experiments on MS COCO dataset demonstrate the effectiveness of our proposed method, and ThumbPose achieves superior performance (＋ 1.3% AP and ＋ 0.7% AR) with negligible additional cost ( &lt; &amp;lt; 0.2 GFLOPs) compared to previous state-of-the-art methods when using small-size images as inputs. Moreover, experiments on MPII show that our model achieves higher accuracy (＋ 0.2% Mean@0.5) with minimal computation (2.5 GFLOPs) compared to superior lightweight models obtained by the network compression methods .},
  archive      = {J_PR},
  author       = {Zian Zhang and Yongqiang Zhang and Yin Zhang and Rui Tian and Mingli Ding},
  doi          = {10.1016/j.patcog.2023.110111},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110111},
  shortjournal = {Pattern Recognition},
  title        = {Vital information is only worth one thumbnail: Towards efficient human pose estimation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyRSM++: Hybrid relation guided temporal set matching for
few-shot action recognition. <em>PR</em>, <em>147</em>, 110110. (<a
href="https://doi.org/10.1016/j.patcog.2023.110110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot action recognition is a challenging but practical problem aiming to learn a model that can be easily adapted to identify new action categories with only a few labeled samples. However, existing attempts still suffer from two drawbacks: (i) learning individual features without considering the entire task may result in limited representation capability, and (ii) existing alignment strategies are sensitive to noises and misaligned instances. To handle the two limitations, we propose a novel Hybrid Relation guided temporal Set Matching (HyRSM++) approach for few-shot action recognition. The core idea of HyRSM++ is to integrate all videos within the task to learn discriminative representations and involve a robust matching technique. To be specific, HyRSM++ consists of two key components, a hybrid relation module and a temporal set matching metric. Given the basic representations from the feature extractor, the hybrid relation module is introduced to fully exploit associated relations within and cross videos in an episodic task and thus can learn task-specific embeddings. Subsequently, in the temporal set matching metric, we carry out the distance measure between query and support videos from a set matching perspective and design a bidirectional Mean Hausdorff Metric to improve the resilience to misaligned instances. Furthermore, we extend the proposed HyRSM++ to deal with the more challenging semi-supervised few-shot action recognition and unsupervised few-shot action recognition tasks. Experimental results on multiple benchmarks demonstrate that our method consistently outperforms existing methods and achieves state-of-the-art performance under various few-shot settings. The source code is available at https://github.com/alibaba-mmai-research/HyRSMPlusPlus .},
  archive      = {J_PR},
  author       = {Xiang Wang and Shiwei Zhang and Zhiwu Qing and Zhengrong Zuo and Changxin Gao and Rong Jin and Nong Sang},
  doi          = {10.1016/j.patcog.2023.110110},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110110},
  shortjournal = {Pattern Recognition},
  title        = {HyRSM++: Hybrid relation guided temporal set matching for few-shot action recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention cycle-consistent universal network for more
universal domain adaptation. <em>PR</em>, <em>147</em>, 110109. (<a
href="https://doi.org/10.1016/j.patcog.2023.110109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Universal Domain Adaptation (UniDA) approaches can handle various domain adaptation (DA) tasks, which need no prior information about the category overlap across target and source domains. However, traditional UniDA scenario cannot fully cover every DA scenario, e.g. , Multi-Source DA is absent. Therefore, aiming to simultaneously handle more DA scenarios in nature, we propose the More Universal Domain Adaptation (MUniDA) task. There are three challenges in MUniDA: (i) Category shift between source and target domains; (ii) Domain shift, especially the domain shift among multiple modalities in the source, which is ignored by the current UniDA approaches; (iii) How to recognize common categories across domains? We propose a more universally applicable DA approach that can tackle above challenges without any modification called A ttention Cycle -consistent U niversal N etwork (A-CycleUN). We show through extensive experiments on several benchmarks that A-CycleUN works stably and outperforms baselines across different MUniDA settings.},
  archive      = {J_PR},
  author       = {Ziyun Cai and Yawen Huang and Tengfei Zhang and Xiao-Yuan Jing and Yefeng Zheng and Ling Shao},
  doi          = {10.1016/j.patcog.2023.110109},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110109},
  shortjournal = {Pattern Recognition},
  title        = {Attention cycle-consistent universal network for more universal domain adaptation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoCNet++: Triangle-based descriptor for accurate and robust
point cloud registration. <em>PR</em>, <em>147</em>, 110108. (<a
href="https://doi.org/10.1016/j.patcog.2023.110108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces RoCNet++, a point cloud registration method with two main contributions, one concerning the design of a robust descriptor and another concerning the estimation of the rigid transformation. First, to robustly capture the local geometric properties of the surface, i.e., each point is characterized by all the triangles formed by itself and its nearest neighbours in the 3D point cloud. The idea is to assist the learning of the descriptor by introducing a priori information about interesting geometric properties such as the invariance of triangle angles under rigid transformations. This local triangle-based descriptor is integrated into the recently developed RoCNet architecture for estimating the correspondences between source and target point clouds. We then introduce the Farthest Sampling-guided Registration (FSR), which relies on successive farthest point samplings to estimate the global rigid transformation between 3D point clouds. The new proposed architecture RoCNet++ has been evaluated in different configurations: clean, noisy and partial data on both synthetic and real databases such as ModelNet40, KITTI, and 3DMatch. RoCNet++ shows improved performances on these benchmark datasets in favourable and unfavourable conditions. Furthermore, both the local triangle-based descriptor and the Farthest Sampling-guided Registration (FSR) can be used in other registration algorithms.},
  archive      = {J_PR},
  author       = {Karim Slimani and Catherine Achard and Brahim Tamadazte},
  doi          = {10.1016/j.patcog.2023.110108},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110108},
  shortjournal = {Pattern Recognition},
  title        = {RoCNet++: Triangle-based descriptor for accurate and robust point cloud registration},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DBN-mix: Training dual branch network using bilateral mixup
augmentation for long-tailed visual recognition. <em>PR</em>,
<em>147</em>, 110107. (<a
href="https://doi.org/10.1016/j.patcog.2023.110107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing interest in the challenging visual perception task of learning from long-tailed class distributions. The extreme class imbalance in the training dataset biases the model to prefer recognizing majority class data over minority class data. Furthermore, the lack of diversity in minority class samples makes it difficult to find a good representation. In this paper, we propose an effective data augmentation method, referred to as bilateral mixup augmentation , which can improve the performance of long-tailed visual recognition. The bilateral mixup augmentation combines two samples generated by a uniform sampler and a re-balanced sampler and augments the training dataset to enhance the representation learning for minority classes. We also reduce the classifier bias using class-wise temperature scaling, which scales the logits differently per class in the training phase. We apply both ideas to the dual-branch network (DBN) framework, presenting a new model, named dual-branch network with bilateral mixup (DBN-Mix) . Experiments on popular long-tailed visual recognition datasets show that DBN-Mix improves performance significantly over baseline and that the proposed method achieves state-of-the-art performance in some categories of benchmarks.},
  archive      = {J_PR},
  author       = {Jae Soon Baik and In Young Yoon and Jun Won Choi},
  doi          = {10.1016/j.patcog.2023.110107},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110107},
  shortjournal = {Pattern Recognition},
  title        = {DBN-mix: Training dual branch network using bilateral mixup augmentation for long-tailed visual recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). F-SCP: An automatic prompt generation method for specific
classes based on visual language pre-training models. <em>PR</em>,
<em>147</em>, 110096. (<a
href="https://doi.org/10.1016/j.patcog.2023.110096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The zero-shot classification performance of large-scale vision-language pre-training models (e.g., CLIP, BLIP and ALIGN) can be enhanced by incorporating a prompt (e.g., “a photo of a [CLASS]”) before the class words. Modifying the prompt slightly can have significant effect on the classification outcomes of these models. Thus, it is crucial to include an appropriate prompt tailored to the classes. However, manual prompt design is labor-intensive and necessitates domain-specific expertise. The CoOp (Context Optimization) converts hand-crafted prompt templates into learnable word vectors to automatically generate prompts, resulting in substantial improvements for CLIP. However, CoOp exhibited significant variation in classification performance across different classes. Although CoOp-CSC (Class-Specific Context) has a separate prompt for each class, only shows some advantages on fine-grained datasets. In this paper, we propose a novel automatic prompt generation method called F-SCP (Filter-based Specific Class Prompt), which distinguishes itself from the CoOp-UC (Unified Context) model and the CoOp-CSC model. Our approach focuses on prompt generation for low-accuracy classes and similar classes. We add the Filter and SCP modules to the prompt generation architecture. The Filter module selects the poorly classified classes, and then reproduce the prompts through the SCP (Specific Class Prompt) module to replace the prompts of specific classes. Experimental results on six multi-domain datasets shows the superiority of our approach over the state-of-the-art methods. Particularly, the improvement in accuracy for the specific classes mentioned above is significant. For instance, compared with CoOp-UC on the OxfordPets dataset, the low-accuracy classes, such as, Class21 and Class26, are improved by 18% and 12%, respectively.},
  archive      = {J_PR},
  author       = {Baihong Han and Xiaoyan Jiang and Zhijun Fang and Hamido Fujita and Yongbin Gao},
  doi          = {10.1016/j.patcog.2023.110096},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110096},
  shortjournal = {Pattern Recognition},
  title        = {F-SCP: An automatic prompt generation method for specific classes based on visual language pre-training models},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KGSR: A kernel guided network for real-world blind
super-resolution. <em>PR</em>, <em>147</em>, 110095. (<a
href="https://doi.org/10.1016/j.patcog.2023.110095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based methods have emerged as dominant players in the field of super-resolution (SR), owing to their exceptional reconstruction performance. The primary driver of their effectiveness lies in their utilization of extensive sets of paired low-resolution and high-resolution images for training deep learning models . This training enables the models to effectively replicate the intricate mapping relationship between low-resolution and high-resolution images. Nevertheless, at present, acquiring a sufficient quantity of such image pairs that satisfy the requirements remains a formidable obstacle. Therefore, in order to break the restriction of limited training sets, self-supervised learning has been introduced to train a model for each low-quality image, without requiring pairwise ground-truths. However, they generally presuppose the generation of low-resolution (LR) images from their high-resolution (HR) counterparts using a pre-defined kernel, such as Bicubic downscaling. Such an assumption is seldom valid for real-world LR images, where degradation processes in practical applications are diverse, intricate, and often undisclosed. Therefore, when the presumed downscaling kernel does not match the actual one, the outcomes of state-of-the-art approaches degrade substantially. In this paper, we introduce KGSR, a kernel-guided network for addressing real-world blind SR, effectively avoiding requiring large training image pairs and transforming the blind image super-resolution problem into a supervised learning and non-blind scenario. Specifically, KGSR trains two networks, namely Upscaling and Downscaling, utilizing only patches extracted from the input test image. On one hand, owing to the cross-scale recurrence property of the SR kernel within a single image, the Downscaling network acquires knowledge of the image-specific degradation process through a generative adversarial network . Consequently, the Downscaling network is capable of generating a downsampled version of the LR test image even when the acquisition process is unknown or less than ideal. Additionally, we employ a dedicated discriminator to compel the Downscaling network to prioritize the characterization of kernel orientations. Conversely, a precise blur kernel has the potential to yield superior performance. Guided by the accurate image-specific SR kernel acquired from the Downscaling network and the downsampled LR input, the Upscaling network is capable of producing a high-quality HR image from the LR input. Within the Upscaling network, we additionally introduce an effective module for harnessing the acquired image-specific SR kernel. KGSR operates as a fully unsupervised approach, yet it can concurrently produce both the image-specific SR kernel and high-quality HR images. Comprehensive experiments conducted on standard benchmarks validate the efficacy of the proposed approach compared to state-of-the-art methodologies. Moreover, the suggested method can deliver visually appealing SR outcomes while exhibiting shorter processing times when applied to real-world LR images.},
  archive      = {J_PR},
  author       = {Qingsen Yan and Axi Niu and Chaoqun Wang and Wei Dong and Marcin Woźniak and Yanning Zhang},
  doi          = {10.1016/j.patcog.2023.110095},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110095},
  shortjournal = {Pattern Recognition},
  title        = {KGSR: A kernel guided network for real-world blind super-resolution},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OAMatcher: An overlapping areas-based network with label
credibility for robust and accurate feature matching. <em>PR</em>,
<em>147</em>, 110094. (<a
href="https://doi.org/10.1016/j.patcog.2023.110094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature matching involves establishing accurate pixel-wise correspondences between an image pair, which is a critical component in several visual applications (e.g., visual localization). Recently, detector-free techniques have realized excellent performance in this task. However, existing methods tend to focus on the entire image without prioritizing overlapping regions, resulting in undesirable interference from non-overlapping areas during the descriptors enhancement process. Moreover, these approaches neglect unreliable ground-truth matching labels triggered by measurement noise in datasets, leading to sub-optimal network optimization. In this study, we develop a novel overlapping areas-based network OAMatcher to resolve these issues. For the first issue, OAMatcher employs an overlapping regions perception block (ORPB) that captures the overlapping areas of image pairs to filter out plentiful mismatches and circumvent interference from non-overlapping regions during descriptors enhancement process. Specifically, the ORPB first enhances the descriptors of all keypoints to mimic the human behaviour of scrutinizing entire images back and forth at the start of feature matching. Subsequently, the ORPB introduces an overlapping regions extraction block (OREB) that captures the keypoints within overlapping zones to mimic the humans behaviour of shifting the focus from the whole images to co-visible areas. After OREB, ORPB performs descriptors enhancement exclusively among the keypoints within these co-visible regions, ensuring minimal disturbances from non-overlapping areas. In addition, the ORPB confines the predicted matches strictly to co-visible regions, thus efficiently filtering out a significant number of mismatches in non-overlapping zones. For the second issue, OAMatcher proposes a labels weighting algorithm (LWA) that predicts the label credibility for ground-truth matching labels. LWA assigns low credibility to unreliable labels and utilizes the credibility to weight loss, effectively diminishing the influence of unreliable labels. Extensive experiments show that OAMatcher delivers excellent results for homography estimation, pose estimation, and visual localization tasks.},
  archive      = {J_PR},
  author       = {Kun Dai and Tao Xie and Ke Wang and Zhiqiang Jiang and Ruifeng Li and Lijun Zhao},
  doi          = {10.1016/j.patcog.2023.110094},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110094},
  shortjournal = {Pattern Recognition},
  title        = {OAMatcher: An overlapping areas-based network with label credibility for robust and accurate feature matching},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual deformable convolution for better image
de-weathering. <em>PR</em>, <em>147</em>, 110093. (<a
href="https://doi.org/10.1016/j.patcog.2023.110093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse weather conditions pose great challenges to computer vision tasks like detection, segmentation, tracing et, al. in the wild. Image de-weathering aiming at removing weather degradations from images/videos has hence accumulated huge popularity as a significant component of image restoration. A large number of SOTA de-weathering methods are based on the autoencoder architecture for its excellent generalization and high computational efficiency. However, for most of these models, parts of high-frequency information are inevitably lost in the downsampling process in the encoders, while degraded features are unable to be effectively inhibited in the upsampling modules in the decoders, largely limiting the restoration performance. In this paper, we propose a multi-patch skip-forward structure for the encoder to deliver fine-grain features from shallow layers to deep layers, and provide more detailed semantics for feature embedding. For the decoding part, the Residual Deformable Convolutional module is developed to dynamically recover the degradation with spatial attention , achieving high-quality pixel-wise reconstruction. Extensive experiments show that our model outperforms many recently proposed state-of-the-art works on both specific-task de-weathering, such as de-raining, de-snowing, and all-task de-weathering. The source code is available at https://github.com/IntelligentDrivingCoding/DeformDeweatherNet .},
  archive      = {J_PR},
  author       = {Huikai Liu and Ao Zhang and Wenqian Zhu and Bin Fu and Bingjian Ding and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2023.110093},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110093},
  shortjournal = {Pattern Recognition},
  title        = {Residual deformable convolution for better image de-weathering},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Region-adaptive and context-complementary cross modulation
for RGB-t semantic segmentation. <em>PR</em>, <em>147</em>, 110092. (<a
href="https://doi.org/10.1016/j.patcog.2023.110092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Thermal (RGB-T) semantic segmentation is an emerging task aiming to improve the robustness of segmentation methods under extreme imaging conditions with the aid of thermal infrared modality. Foreground–background distinguishment and complementary information mining are two key challenges of this task. Recent methods use naive channel attention and cross-attention to tackle these challenges, but they still struggle with a sub-optimal solution where salient foreground features and noisy background ones might be equally modulated without distinction. The quadratic computational overhead of cross-attention also blocks its application on high-resolution features. Moreover, lacking complementary information mining in the encoding phase hinders the comprehensive scene encoding as well. To alleviate these limitations, we propose a cross modulation process with two collaborative components. The first Region-Adaptive Channel Modulation (RACM) module conducts channel attention at a fine-grained region level where foreground and background regions can be modulated differently in each channel. The second Context-Complementary Spatial Modulation (CCSM) module mines and transfers complementary information between the two modalities early in the encoding phase. Experiments show that our method achieves state-of-the-art performances on current RGB-T segmentation benchmarks.},
  archive      = {J_PR},
  author       = {Fengguang Peng and Zihan Ding and Ziming Chen and Gang Wang and Tianrui Hui and Si Liu and Hang Shi},
  doi          = {10.1016/j.patcog.2023.110092},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110092},
  shortjournal = {Pattern Recognition},
  title        = {Region-adaptive and context-complementary cross modulation for RGB-T semantic segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning a target-dependent classifier for cross-domain
semantic segmentation: Fine-tuning versus meta-learning. <em>PR</em>,
<em>147</em>, 110091. (<a
href="https://doi.org/10.1016/j.patcog.2023.110091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently proposed domain adaptation arts have dominated the field of cross-domain semantic segmentation by operating domain manifolds alignment and learning an optimal joint hypothesis (joint-domain classifier) for both source and target domains. However, a joint-domain classifier can still violate the cluster assumption in the target domain in case domain manifolds are not fully aligned after domain adaptation. In this work, we raise the intractability of perfect domain alignment and turn to exploit a novel hypothesis: a target-dependent classifier, to efficiently adapt to the target domain clusters even given a certain degree of domain misalignment. Specifically, we first propose an unsupervised fine-tuning strategy, which optimizes the joint hypothesis of vanilla domain adaptation into a target-dependent hypothesis to better fit with the target domain clusters. Second, we connect the “learning to learn” concept of meta-learning with pixel-wise domain adaptation, which serves as a reliable hypothesis initialization, providing an alternative solution to learning a more generalized target-dependent classifier. The proposed learning method is general to conventional domain adaptation models. In experiments, we recycle the pre-trained conventional DA models and learn target-dependent classifiers with the proposed method. Experimental results on synthetic-to-real adaptation and cross-city adaptation benchmarks demonstrate that the target-dependent classifier leads over state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Haitao Tian and Shiru Qu and Pierre Payeur},
  doi          = {10.1016/j.patcog.2023.110091},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110091},
  shortjournal = {Pattern Recognition},
  title        = {Learning a target-dependent classifier for cross-domain semantic segmentation: Fine-tuning versus meta-learning},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gait feature learning via spatio-temporal two-branch
networks. <em>PR</em>, <em>147</em>, 110090. (<a
href="https://doi.org/10.1016/j.patcog.2023.110090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition has become a mainstream technology for identification due to its ability to capture gait features over long distances without subject cooperation and resistance to camouflage. However, current gait recognition methods face challenges as they use a single network to extract both temporal and spatial features from gait sequences. This approach imposes a heavy burden on the network, resulting in reduced extraction efficiency. To solve this problem, we propose a two-branch network to extract the spatio-temporal features of gait sequences. One branch primarily focuses on spatial feature extraction, while the other concentrates on temporal feature extraction. This design can make one branch focus on a specific task, leading to significant performance improvements . For temporal feature extraction, we propose the Global Temporal Information Extraction Network (GTIEN). GTIEN extracts temporal features of gait sequences by sequentially exploring the relationship between adjacent gait silhouettes from pixel and block levels. For spatial feature extraction, we introduce the Selective Horizontal Pyramid Convolution Network (SHPCN). SHPCN explores the multi-granularity features of gait silhouettes from global and local perspectives and assigns them appropriate weights according to their importance. By reasonably combining the temporal features extracted from GTIEN and spatial features extracted from SHPCN, we can effectively learn the spatial–temporal information of the gait sequences. Extensive experiments on CASIA-B and OUMVLP demonstrate that our method has better performance than some state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yifan Chen and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.110090},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110090},
  shortjournal = {Pattern Recognition},
  title        = {Gait feature learning via spatio-temporal two-branch networks},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). View-coherent correlation consistency for semi-supervised
semantic segmentation. <em>PR</em>, <em>147</em>, 110089. (<a
href="https://doi.org/10.1016/j.patcog.2023.110089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised semantic segmentation needs rich and robust supervision for unlabeled data . However, promoting or punishing feature similarities with vanilla contrastive learning can be unreliable for semi-supervised semantic segmentation: pixel pairs are assigned as either positive or negative based on noisy pseudo labels, and both reliable and wrongly-assigned pairs receive uniform penalties. To address this issue, we propose correlation consistency learning, which leverages rich pairwise relationships in self-correlation matrices and matches them to the similarities between soft pseudo labels to provide robust supervision. Unlike vanilla contrastive learning, our approach prioritizes pairs with highly confident pseudo labels and applies weaker penalties for pairs that are less confident. We also introduce a strong semi-supervised learning pipeline that applies data augmentation in a view-coherent manner: even under complex augmentation strategies, for each pixel, a match can be found in different augmentation views. The novelties of the proposed method are the correlation consistency loss and the view-coherent data augmentation, and their combination gives us the view-coherent correlation consistency (VC 3 3 ) system, which achieves state-of-the-art results in several semi-supervised settings on two datasets.},
  archive      = {J_PR},
  author       = {Yunzhong Hou and Stephen Gould and Liang Zheng},
  doi          = {10.1016/j.patcog.2023.110089},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110089},
  shortjournal = {Pattern Recognition},
  title        = {View-coherent correlation consistency for semi-supervised semantic segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cascaded framework with cross-modality transfer learning
for whole heart segmentation. <em>PR</em>, <em>147</em>, 110088. (<a
href="https://doi.org/10.1016/j.patcog.2023.110088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic and accurate segmentation of the whole heart structure from 3D cardiac images plays an important role in helping physicians diagnose and treat cardiovascular disease. However, the time-consuming and laborious manual labeling of the heart images results in the inefficiency of utilizing the existing CT or MRI for training the deep learning network, which decrease the accuracy of whole heart segmentation. However, multi-modality data contains multi-level information of cardiac images due to different imaging mechanisms, which is beneficial to improve the segmentation accuracy . Therefore, this paper proposes a cascaded framework with cross-modality transfer learning for whole heart segmentation (CM-TranCaF), which consists of three key modules: modality transfer network (MTN), U-shaped multi-attention network (MAUNet) and spatial configuration network (SCN). In MTN, MRI images are transferred from MRI domain to CT domain, to increase the data volume by adopting the idea of adversarial training . The MAUNet is designed based on UNet, while the attention gates (AGs) are integrated into the skip connection to reduce the weight of background pixels. Moreover, to solve the problem of boundary blur, the position attention block (PAB) is also integrated into the bottom layer to aggregate similar features. Finally, the SCN is used to finetune the segmentation results by utilizing the anatomical information between different cardiac substructures. By evaluating the proposed method on the dataset of the MM-WHS challenge, CM-TranCaF achieves a Dice score of 91.1% on the testing dataset . The extensive experimental results prove the effectiveness of the proposed method compared to other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yi Ding and Dan Mu and Jiaqi Zhang and Zhen Qin and Li You and Zhiguang Qin and Yingkun Guo},
  doi          = {10.1016/j.patcog.2023.110088},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110088},
  shortjournal = {Pattern Recognition},
  title        = {A cascaded framework with cross-modality transfer learning for whole heart segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal human action localization in indoor
surveillances. <em>PR</em>, <em>147</em>, 110087. (<a
href="https://doi.org/10.1016/j.patcog.2023.110087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal action localization is a crucial and challenging task in the field of video understanding . Existing benchmarks for spatio-temporal action detection are limited by factors such as incomplete annotations, high-level non-universal actions, and uncommon scenarios. To address these limitations and facilitate research in real-world security applications, we introduce a novel human-centric dataset for spatio-temporal localization of atomic actions in indoor surveillance settings, termed as HIA (Human-centric Indoor Actions). The HIA dataset is constructed by selecting 30 atomic action classes, compiling 100 surveillance videos , and annotating 219,225 frames with 370,937 bounding boxes . The primary characteristics of HIA include (1) accurate spatio-temporal annotations for atomic actions, (2) human-centric annotations at the frame level, (3) temporal linking of persons across discontinuous tracks, and (4) utilization of indoor surveillance videos. Our HIA, with its realistic settings in indoor surveillance scenes and comprehensive annotations, presents a valuable and novel challenge to the spatio-temporal action localization domain. To establish a benchmark, we evaluate various methods and provide an in-depth analysis of the HIA dataset. The HIA dataset will be made available soon, and we anticipate that it will serve as a standard and practical benchmark for the research community.},
  archive      = {J_PR},
  author       = {Zihao Liu and Danfeng Yan and Yuanqiang Cai and Yan Song},
  doi          = {10.1016/j.patcog.2023.110087},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110087},
  shortjournal = {Pattern Recognition},
  title        = {Spatio-temporal human action localization in indoor surveillances},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint representation learning for text and 3D point cloud.
<em>PR</em>, <em>147</em>, 110086. (<a
href="https://doi.org/10.1016/j.patcog.2023.110086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in vision-language pre-training ( e.g. , CLIP) have enabled 2D vision models to benefit from language supervision. However, the joint representation learning of 3D point cloud with text remains under-explored due to challenges in acquiring 3D-Text data pairs. Prior works propose to project point clouds into 2D depth maps and directly use CLIP, while they sacrifice 3D structural information, limiting its applicability. In this paper, we put forward Text4Point, a novel framework to construct language-guided 3D models for dense prediction tasks. Text4Point utilizes 2D images as a bridge to connect the point cloud and language modalities. It follows a pre-training and fine-tuning paradigm. During pre-training, we leverage dense contrastive learning to align the image and point cloud representations using the readily available RGB-D data. Together with the well-aligned image and text features achieved by CLIP, the point cloud features are implicitly aligned with the text embeddings. Further, we propose a Text Querying Module to integrate language information into 3D representation learning by querying text embeddings with point cloud features. For fine-tuning, the model learns 3D representations under informative language guidance without 2D images. Extensive experiments demonstrate consistent improvement on various dense prediction tasks with Text4Point.},
  archive      = {J_PR},
  author       = {Rui Huang and Xuran Pan and Henry Zheng and Haojun Jiang and Zhifeng Xie and Cheng Wu and Shiji Song and Gao Huang},
  doi          = {10.1016/j.patcog.2023.110086},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110086},
  shortjournal = {Pattern Recognition},
  title        = {Joint representation learning for text and 3D point cloud},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Feature specific progressive improvement for salient object
detection. <em>PR</em>, <em>147</em>, 110085. (<a
href="https://doi.org/10.1016/j.patcog.2023.110085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from deep learning , Salient Object Detection (SOD) has made much progress. However, most existing methods adopt the same strategy to extract salient cues from different feature levels without fully considering their differences in the feature extraction stage and/or suffer from the accumulation of noise and dilution of spatial details in the feature fusion stage. These two problems hinder the further improvement in performance. In this paper, we propose an effective SOD model, PiNet, which can address the above problems via two novel mechanisms in the network: level-specific feature extraction and progressive refinement of saliency. We have designed the customized feature extraction components for each level of features—enabling us to extract better saliency cues from multi-level features. The saliency feature refinement in the branches follows a coarse-to-fine process, where the refined features progressively contain more location cues, internal and boundary details. Through short connections, the extracted saliency cues in different branches are selectively transmitted and integrated, which well mitigates the accumulation of noisy information and the dilution of detailed information. By using four different backbones, we verify our model has good adaptability and can make accurate saliency predictions under different pretrained models. Extensive experiments on five public datasets demonstrate that PiNet outperforms 19 state-of-the-art (SOTA) methods in SOD, with its small model size (56.1 MB) and fast inference speed (47 FPS).},
  archive      = {J_PR},
  author       = {Xianheng Wang and Zhaobin Liu and Veronica Liesaputra and Zhiyi Huang},
  doi          = {10.1016/j.patcog.2023.110085},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110085},
  shortjournal = {Pattern Recognition},
  title        = {Feature specific progressive improvement for salient object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPCCT: Multimodal vision-language learning paradigm with
context-based compact transformer. <em>PR</em>, <em>147</em>, 110084.
(<a href="https://doi.org/10.1016/j.patcog.2023.110084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer and its variants have become the preferred option for multimodal vision-language paradigms. However, they struggle with tasks that demand high-dependency modeling and reasoning, like visual question answering (VQA) and visual grounding (VG). For this, we propose a general scheme called MPCCT, which: (1) incorporates designed textual global-context information to facilitate precise computation of dependency relationships between language tokens in the language encoder; (2) dynamically modulates and filters image features using optimized textual global-context information, combined with designed spatial context information, to further enhance the dependency modeling of image tokens and the model’s reasoning ability; (3) reasonably align the language sequence containing textual global-context information with the image sequence information modulated by spatial position information . To validate MPCCT, we conducted extensive experiments on five benchmark datasets in VQA and VG, achieving new SOTA performance on multiple benchmarks, especially 73.71% on VQA-v2 and 99.15% on CLEVR. The code is available at https://github.com/RainyMoo/myvqa .},
  archive      = {J_PR},
  author       = {Chongqing Chen and Dezhi Han and Chin-Chen Chang},
  doi          = {10.1016/j.patcog.2023.110084},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110084},
  shortjournal = {Pattern Recognition},
  title        = {MPCCT: Multimodal vision-language learning paradigm with context-based compact transformer},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Separated fan-beam projection with gaussian convolution for
invariant and robust butterfly image retrieval. <em>PR</em>,
<em>147</em>, 110083. (<a
href="https://doi.org/10.1016/j.patcog.2023.110083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Butterfly image retrieval is a challenging issue requiring the feature representation not only to be sensitive to the subtle inter-class difference but also to remain robust to large intra-class variations. Fan-beam projection is a mathematical tool originally applied to computed tomographic (CT) reconstruction of objects. In this paper, we introduce it for the first time into object recognition field. Separated fan-beam projection followed by Gaussian convolutions of different widths are designed to extract multiscale invariant features, patch-projection angles (PPA) and texture-projection angles (TPA), for separately depicting the patch patterns and texture properties of butterfly images. The PPA and TPA are then treated as heterogeneous co-occurrence patterns to be fused by a 2D histograms as final feature representation. We present a comprehensive experimental evaluation including image retrieval at species and subspecies levels, complementarity to deep-learning features, invariance and robustness. All the results consistently show the superior performance of the proposed method over the state-of-the arts.},
  archive      = {J_PR},
  author       = {Xin Chen and Bin Wang and Yongsheng Gao},
  doi          = {10.1016/j.patcog.2023.110083},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110083},
  shortjournal = {Pattern Recognition},
  title        = {Separated fan-beam projection with gaussian convolution for invariant and robust butterfly image retrieval.},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geometric-inspired graph-based incomplete multi-view
clustering. <em>PR</em>, <em>147</em>, 110082. (<a
href="https://doi.org/10.1016/j.patcog.2023.110082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering methods group data into different clusters by discovering the consensus in heterogeneous sources, which however becomes difficult when partial views of real-world data are missing. Consequently, reducing the impact of missing views and leveraging available views are the key concerns for the Incomplete Multi-view Clustering (IMvC) problem. In this research, we take an innovative, geometry-based perspective to investigate the IMvC problem under a commonly-used weight aggregation framework. We conduct a geometric analysis to understand how missing views shift the aggregation solution from the one achieved with full views, subsequently impacting the clustering result . Drawing from our analysis, we introduce a weight reallocation approach that minimizes the shift and approximates the full-view solution by reallocating the factual weight of each available view. Furthermore, we address the IMvC problem by using our reallocation method on a graph aggregation algorithm to obtain reliable clusters. Our extensive experiments demonstrate that our proposed approach outperforms previous IMvC methods, reporting superior results on four datasets with three metrics. Especially, on the Caltech101-7 dataset with 40 percent missing data, our method achieves an accuracy of 0.686, which significantly outperforms the results of other comparison methods that are no larger than 0.662. Further, our method can be used as a flexible plugin to improve other weight aggregation algorithms. The source code of this work is publicly available at https://github.com/bjlfzs/Geometric-Inspired-Graph-based-Incomplete-Multi-view-Clustering .},
  archive      = {J_PR},
  author       = {Zequn Yang and Han Zhang and Yake Wei and Zheng Wang and Feiping Nie and Di Hu},
  doi          = {10.1016/j.patcog.2023.110082},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110082},
  shortjournal = {Pattern Recognition},
  title        = {Geometric-inspired graph-based incomplete multi-view clustering},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning implicit labeling-importance and label correlation
for multi-label feature selection with streaming labels. <em>PR</em>,
<em>147</em>, 110081. (<a
href="https://doi.org/10.1016/j.patcog.2023.110081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection plays an increasingly important role in alleviating the high dimensionality of multi-label learning tasks. Most extant methods posit that the learning task is performed in an environment where the label space is statically known. In reality, however, the environment is open and the labels may arrive dynamically, which is coined as streaming labels. Streaming labels-based multi-label feature selection suffers from many challenges derived from label space: (1) The label space expands dynamically; (2) Newly arrived labels exhibit complex relationships, often involving label correlation and labeling-importance. To cope with this challenge, in this paper, an intuitive yet effective algorithm named LLSL, i.e. learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels, is proposed. To be specific, the implicit labeling-importance with respect to streaming labels is firstly formalized by conducting the nearest neighbor reconstruction on feature space . Secondly, label correlation is seamlessly integrated into the objective function of feature relevance by designing the feature relevance influence factor. Based on the above, we build a feature conversion, which can realize the fusion of label-specific features for each streaming label. Finally, extensive experiments conducted on fifteen benchmark datasets provide clear evidence that LLSL has superior performance compared to three established streaming label-based MFS algorithms and seven static label space-based MFS algorithms.},
  archive      = {J_PR},
  author       = {Jinghua Liu and Wei Wei and Yaojin Lin and Lijie Yang and Hongbo Zhang},
  doi          = {10.1016/j.patcog.2023.110081},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110081},
  shortjournal = {Pattern Recognition},
  title        = {Learning implicit labeling-importance and label correlation for multi-label feature selection with streaming labels},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A linear transportation lp distance for pattern recognition.
<em>PR</em>, <em>147</em>, 110080. (<a
href="https://doi.org/10.1016/j.patcog.2023.110080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transportation L p Lp distance, denoted TL p TLp , has been proposed as a generalisation of Wasserstein W p Wp distances motivated by the property that it can be applied directly to colour or multi-channelled images, as well as multivariate time-series without normalisation or mass constraints . Both TL p TLp and W p Wp assign a cost based on the transport distance (i.e. the “Lagrangian” model), the key difference between the distances is that TL p TLp interprets the signal as a function whilst W p Wp interprets the signal as a measure. Both distances are powerful tools in modelling data with spatial or temporal perturbations. However, their computational cost can make them infeasible to apply to even moderate pattern recognition tasks. The linear Wasserstein distance was proposed as a method for projecting signals into a Euclidean space where the Euclidean distance is approximately the Wasserstein distance (more formally, this is a projection on to the tangent manifold). We propose linear versions of the TL p TLp distance ( L TL p LTLp ) and we show significant improvement over the linear W p Wp distance on signal processing tasks , whilst being several orders of magnitude faster to compute than the TL p TLp distance.},
  archive      = {J_PR},
  author       = {Oliver M. Crook and Mihai Cucuringu and Tim Hurst and Carola-Bibiane Schönlieb and Matthew Thorpe and Konstantinos C. Zygalakis},
  doi          = {10.1016/j.patcog.2023.110080},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110080},
  shortjournal = {Pattern Recognition},
  title        = {A linear transportation lp distance for pattern recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep self-enhancement hashing for robust multi-label
cross-modal retrieval. <em>PR</em>, <em>147</em>, 110079. (<a
href="https://doi.org/10.1016/j.patcog.2023.110079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of cross-modal hashing is to map data from several modalities into a compact Hamming space for efficient and accurate retrieval. Despite the satisfactory performance, existing approaches are reliant on the closed-world assumption. When confronted with real-world retrieval tasks involving out-of-distribution (OOD) semantic data, the similarity relationships of known data retained in hash codes tend to be disrupted by these unknown ones, resulting in retrieval performance degradation . To this end, we present a deep self-enhancing hashing (DSEH) method, simultaneously learning multi-level similarity-preserved hash codes of the known multi-label cross-modal data and robustness to OOD instances. Specifically, we propose to construct pseudo-OOD samples in the feature space using random linear combinations to explore OOD semantics, during the training process. Meanwhile, a prototype-based generative model is incorporated to aggregate batch data to enhance the data representation’s differences in known and unknown semantics. Furthermore, we describe a bounded cosine quadrupled loss with distance bound to preserve the multi-level similarity of multi-label data and control the maximum distance between known data and the minimum distance between known and pseudo-OOD data for learning OOD robustness. Extensive experiments show that the DSEH achieves state-of-the-art performance on closed-world tasks and good performance on simulated real-world tasks.},
  archive      = {J_PR},
  author       = {Ge Song and Hanwen Su and Kai Huang and Fengyi Song and Ming Yang},
  doi          = {10.1016/j.patcog.2023.110079},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110079},
  shortjournal = {Pattern Recognition},
  title        = {Deep self-enhancement hashing for robust multi-label cross-modal retrieval},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deepfake detection via inter-frame inconsistency
recomposition and enhancement. <em>PR</em>, <em>147</em>, 110077. (<a
href="https://doi.org/10.1016/j.patcog.2023.110077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the remarkable progress in face manipulation technology, malicious applications of these technologies may pose a great threat to the social stability. Therefore, it is essential to carry out the research of deepfake detection. In this paper, we assumed that the illumination on frames that skip a certain space is basically consistent in real videos, but tends to be inconsistent in fake videos. From this point, a network which contains a learnable Image Decomposition Module (IDM) and multi-level feature enhancement is proposed. IDM decomposes frames into illumination and reflection, and frame recomposition is followed to highlight the frame-level illumination inconsistency. Multi-level feature enhancement is proposed to enhance the illumination inconsistency at feature level. In addition, considering the computational complexity and human vision perception mechanism, we train the network in logarithm domain. Experimental results show that the proposed method is effective and superior compared with other state-of-the-art deepfake detection methods on mainstream deepfake datasets.},
  archive      = {J_PR},
  author       = {Chuntao Zhu and Bolin Zhang and Qilin Yin and Chengxi Yin and Wei Lu},
  doi          = {10.1016/j.patcog.2023.110077},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110077},
  shortjournal = {Pattern Recognition},
  title        = {Deepfake detection via inter-frame inconsistency recomposition and enhancement},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting sublimated deep features for image retrieval.
<em>PR</em>, <em>147</em>, 110076. (<a
href="https://doi.org/10.1016/j.patcog.2023.110076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques can be used to describe image content, which is a good way to reduce the semantic gap between low-level and high-level features. However, convolutional neural networks (CNNs) exhibit texture bias and largely ignore global object shape. This does not conform to human perception and can fail to gain the advantages of both low-level features and deep features. To address this problem, in this work, the main focus was shifted from using simple deep features to the use of sublimated deep features , which incorporate global object shape and color features. Along these lines, in this work, a novel image retrieval method named the sublimated deep feature histogram (SDFH) was proposed. The main highlights are: 1) An effect orientation feature was introduced, namely, orientation-selective feature , to mimic the orientation-selection mechanism. This provides a good representation of global object shape and reduces the side-effects of texture bias. 2) A new concept was developed, namely color perceptual feature , to address the shortcoming of deep features—that they discard color features. This includes color cues in the deep features and provides a more discriminating representation. 3) The orientation selective and color perception mechanisms were effectively mimicked to provide a compact yet efficient representation, and propose an effective transfer learning method called gain whitening learning . By carrying out comparative experiments, it was demonstrated that sublimated deep features can provide highly competitive retrieval performance (in terms of mean average precision) using a pre-trained CNN model applied to well-known benchmark datasets. Our results provide new insights into image retrieval based on the mechanisms of the primary visual cortex (V1). Furthermore, the method is more in line with human perception than other methods.},
  archive      = {J_PR},
  author       = {Guang-Hai Liu and Zuo-Yong Li and Jing-Yu Yang and David Zhang},
  doi          = {10.1016/j.patcog.2023.110076},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110076},
  shortjournal = {Pattern Recognition},
  title        = {Exploiting sublimated deep features for image retrieval},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time pattern reconstruction for classification of
irregularly sampled time series. <em>PR</em>, <em>147</em>, 110075. (<a
href="https://doi.org/10.1016/j.patcog.2023.110075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irregularly Sampled Time Series (ISTS) include partially observed feature vectors caused by the lack of temporal alignment across dimensions and the presence of variable time intervals. Especially in medical applications, because patients’ examinations depend on their health status, observations in this event-based medical time series are nonuniformly distributed. When using deep learning models to classify ISTS, most work defines the problem that needs to be solved as alignment-caused data missing or nonuniformity-caused dependency change. However, they only modeled relationships between observed values, ignoring the fact that time is the independent variable for a time series. In this paper, we emphasize that irregularity is active, time-depended, and class-associated and is reflected in the Time Pattern (TP). To this end, this paper focused on the TP of ISTS for the first time, proposing a Time Pattern Reconstruction (TPR) method. It first encodes time information by the time encoding mechanism, then imputes values from time codes by the continuous-discrete Kalman network, selects key time points by the conditional masking mechanism, and finally classifies ISTS based on the reconstructed TP. Experiments on four real-world medical datasets and three other datasets show that TPR outperforms all baselines. We also show that TP can reveal biomarkers and key time points for diseases.},
  archive      = {J_PR},
  author       = {Chenxi Sun and Hongyan Li and Moxian Song and Derun Cai and Baofeng Zhang and Shenda Hong},
  doi          = {10.1016/j.patcog.2023.110075},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110075},
  shortjournal = {Pattern Recognition},
  title        = {Time pattern reconstruction for classification of irregularly sampled time series},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TMNet: Triple-modal interaction encoder and multi-scale
fusion decoder network for v-d-t salient object detection. <em>PR</em>,
<em>147</em>, 110074. (<a
href="https://doi.org/10.1016/j.patcog.2023.110074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection methods based on two-modal images have achieved remarkable success with the aid of image acquisition equipment. However, environmental factors often interfere with the Depth and Thermal maps, rendering them ineffective in providing object information . To address this weakness, we utilize the VDT dataset, which includes Visible, Depth, and Thermal images , and propose a triple-modal interaction encoder and multi-scale fusion decoder network (TMNet) to highlight the salient regions . The triple-modal interaction encoder comprises the separation context-aware feature module, channel-wise fusion module, and triple-modal refinement and fusion module, enabling us to fully explore and utilize the complementarity between Visible, Depth, and Thermal information. The multi-scale fusion decoder involves the semantic-aware localizing module and contour-aware refinement module to extract and fuse the location and boundary information, yielding a high-quality saliency map. Extensive experiments on the public VDT-2048 dataset demonstrate that our TMNet outperforms existing state-of-the-art methods in terms of all evaluation metrics .},
  archive      = {J_PR},
  author       = {Bin Wan and Chengtao lv and Xiaofei Zhou and Yaoqi Sun and Zunjie Zhu and Hongkui Wang and Chenggang Yan},
  doi          = {10.1016/j.patcog.2023.110074},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110074},
  shortjournal = {Pattern Recognition},
  title        = {TMNet: Triple-modal interaction encoder and multi-scale fusion decoder network for V-D-T salient object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Residual shape adaptive dense-nested unet: Redesign the long
lateral skip connections for metal surface tiny defect inspection.
<em>PR</em>, <em>147</em>, 110073. (<a
href="https://doi.org/10.1016/j.patcog.2023.110073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art metal surface defect inspection methods have two problems: (1) they are sensitive to tiny defects because of their extreme small sizes, and (2) they cannot accurately locate the random appeared defects whose semantic relationship with the background context is weak. To solve these problems, Residual Shape Adaptive Dense-nested Unet, a pixel-based defect inspection method is proposed, to obtain the exact shape and location of the defect, by (1) assembling different depth Unet branches with dense skip connections as the feature extractor to combine multi-semantic level visual features; (2) adding Residual Shape Adaptive modules on the dense skip connections to help the model locate the defect regions; and (3) introducing the multi-branch training method which enables model pruning to reduce redundant parameters and accelerate the inspection speed. Experiments are conducted and demonstrated that the Residual Shape Adaptive Dense-nested Unet achieves the best performance among the state-of-the-art defect inspection methods.},
  archive      = {J_PR},
  author       = {Benyi Yang and Zhenyu Liu and Guifang Duan and Jianrong Tan},
  doi          = {10.1016/j.patcog.2023.110073},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110073},
  shortjournal = {Pattern Recognition},
  title        = {Residual shape adaptive dense-nested unet: Redesign the long lateral skip connections for metal surface tiny defect inspection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A general elevating framework for label noise filters.
<em>PR</em>, <em>147</em>, 110072. (<a
href="https://doi.org/10.1016/j.patcog.2023.110072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real applications, label noise has a great influence on data modeling . As one kind of label noise treatment method, noise filter has attracted extensive attention recently. The existing filters perform well in dealing with label noise completely at random (NCAR) but poorly in dealing with the label noise in the form of clusters (LLC). Besides, the existing filters may over remove the samples located at the classification boundaries, thereby affecting the generalization performance of classifiers . To fill these gaps, we propose a general elevating framework for label noise filters. The core idea of the framework is to improve the filtering performance of the existing filters by sample reduction. Specifically, since most of the existing filters are based on classifier prediction, and the complexity of samples at the boundary will affect the prediction performance of classifiers. Therefore, reducing the complexity of the boundary sample is very helpful to improve the performance of the filters. To this end, we propose a sample reduction method, which can not only reduce the complexity of the sample at the boundary but also convert LLC to NCAR, to get some representative samples . Next, the filter based on classifier prediction is employed to recognize the noisy representative samples. Finally, the noisy labeled samples in the given data set are found according to the identified noisy representatives. Furthermore, through empirical analysis, we found that compared with some classical metrics for evaluating the performance of noise filters, classification accuracy is more suitable to measure the performance of filters. Exhaustive experiments testify the validity of the framework, and the experimental results demonstrate that the performance of our framework is especially outstanding for LLC treatment.},
  archive      = {J_PR},
  author       = {Qingqiang Chen and Gaoxia Jiang and Fuyuan Cao and Changqian Men and Wenjian Wang},
  doi          = {10.1016/j.patcog.2023.110072},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110072},
  shortjournal = {Pattern Recognition},
  title        = {A general elevating framework for label noise filters},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic attention augmented graph network for video accident
anticipation. <em>PR</em>, <em>147</em>, 110071. (<a
href="https://doi.org/10.1016/j.patcog.2023.110071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accident anticipation (or the prediction of abnormal events in general) aims to forecast accidents before they occur by assessing risks based on the preceding frames in videos. The risk assessment heavily relies on understanding the semantics of the scene context and predicting the interactions among the involved subjects. Indeed, the comprehensive utilization of spatial relationships among the subjects of immediate interest in a single frame and temporal dependencies across consecutive frames is crucial for video accident anticipation. To address this challenge, we propose a novel approach called Dynamic Attention Augmented Graph Network (DAA-GNN), which leverages underlying spatial cues and models’ relationships among detected subjects of immediate interest. Specifically, our approach employs a graph neural network that is enhanced by global context clues, allowing effective message propagation and the discovery of interactions among the subjects of interest in the scene. The DAA-GNN includes a temporal attention module designed to identify long-term dependencies along the temporal axis, contributing to an end-to-end deep network solution for accurate accident anticipation. We extensively evaluate our method on the publicly available Dashcam Accident Dataset (DAD) and Epic Fail (EF) datasets, by conducting comprehensive experiments to assess its performance. The results unequivocally demonstrate that our method outperforms the state-of-the-art accident anticipation methods. Our source code and datasets are available at https://github.com/ZxyLinkstart/DAA-GNN .},
  archive      = {J_PR},
  author       = {Wenfeng Song and Shuai Li and Tao Chang and Ke Xie and Aimin Hao and Hong Qin},
  doi          = {10.1016/j.patcog.2023.110071},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110071},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic attention augmented graph network for video accident anticipation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regional context-based recalibration network for cataract
recognition in AS-OCT. <em>PR</em>, <em>147</em>, 110069. (<a
href="https://doi.org/10.1016/j.patcog.2023.110069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have been widely applied to cataract recognition tasks and have achieved promising results. However, most existing methods focused on designing data-driven CNN architectures , and failed to exploit asymmetric opacity distribution prior of cataract, which is significant for cataract diagnosis. To this end, this paper proposes a regional context-based recalibration (RCR) module, which fully leverages the clinical prior to recalibrate the feature maps with regional pooling, region-based context integration, and integrated context fusion. We stack these RCR modules to form an RCRNet based on anterior segment optical coherence tomography (AS-OCT) images for cataract recognition. Experiments on the AS-OCT-NC2 dataset and two publicly available medical datasets demonstrate that RCRNet achieves a better trade-off between performance and efficiency than state-of-the-art channel attention-based networks. We also explain the inherent behavior of RCRNet with the aid of the visual analysis. In addition, this paper is the first to study the effects of two performance evaluation methods on AS-OCT image-based cataract classification results : the single-image level and the single-eye level, suggesting that adopting the single-eye level to evaluate cataract classification performance according to clinical diagnosis requirement.},
  archive      = {J_PR},
  author       = {Xiaoqing Zhang and Zunjie Xiao and Bing Yang and Xiao Wu and Risa Higashita and Jiang Liu},
  doi          = {10.1016/j.patcog.2023.110069},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110069},
  shortjournal = {Pattern Recognition},
  title        = {Regional context-based recalibration network for cataract recognition in AS-OCT},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional reiterative high-fidelity GAN inversion for
image editing. <em>PR</em>, <em>147</em>, 110068. (<a
href="https://doi.org/10.1016/j.patcog.2023.110068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work introduces a conditional reiteration mechanism for High-Fidelity GAN (Generative Adversarial Networks) inversion (HFGI), preserving image-specific details (like background, appearance, etc.) for both normal and out-of-domain images (e.g. heavy makeup faces). The HFGI encoder’s single-stage conditional latent maps result in blurry regions in restored images and loss of detailed information during editing. To address this, we proposed a reiterative conditional latents method that restores image-specific details sharply. The process involves two stages of iterations, reconstructing the image in the first stage, and refining image-specific details using conditional latent codes in the second stage. Our model successfully inverts out-of-domain images while preserving all details and supports InterfaceGAN, GANspace, and StyleClip for editing. We compare our approach with state-of-the-art GAN inversion methods on FFHQ (Flickr-Faces-HQ) Dataset, demonstrating significant improvements in inversion and editing quality.},
  archive      = {J_PR},
  author       = {Vedant Vasant Dere and Amita Shinde and Prachi Vast},
  doi          = {10.1016/j.patcog.2023.110068},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110068},
  shortjournal = {Pattern Recognition},
  title        = {Conditional reiterative high-fidelity GAN inversion for image editing},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoMP-transformer: Rotational bounding box with multi-level
feature pyramid transformer for object detection. <em>PR</em>,
<em>147</em>, 110067. (<a
href="https://doi.org/10.1016/j.patcog.2023.110067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes rotational bounding box with a multi-level feature pyramid transformer (RoMP-Transformer)—a fast and accurate one-stage deep neural network for object detection. The proposed RoMP-Transformer exhibits three characteristics. First, a rotational bounding box is utilized to minimize the effect of the background during the construction of feature maps, enhancing the robustness of the RoMP-Transformer. Second, the RoMP-Transformer employs a multi-level feature pyramid transformer by combining a multi-level feature pyramid network with a pyramid vision-transformer, effectively extracting high-quality features and achieving high accuracy. Third, the RoMP-Transformer executes bounding box optimization by minimizing the optimal intersection of union (IoU) loss by considering both the modified SKEW IoU and distance IoU. The modified SKEW IoU significantly accelerates the calculation, and the fused IoU calculation method improves prediction accuracy. Further, Bayesian optimization and weight lightening with half-tensor are performed to optimize the performance of the RoMP-Transformer for real-time applications. Experiments on three image sets—one on power transmission facilities, MSRA-TD500, and DOTA-v1.0—demonstrate that the proposed RoMP-Transformer outperforms other state-of-the-art neural networks in object detection in terms of accuracy, robustness, and calculation speed. Systematic analysis also reveals that the methods utilized by the RoMP-Transformer optimize object detection performance. The proposed architecture is expected to inspire further study of deep neural networks for object detection in real-world applications.},
  archive      = {J_PR},
  author       = {Joonhyeok Moon and Munsu Jeon and Siheon Jeong and Ki-Yong Oh},
  doi          = {10.1016/j.patcog.2023.110067},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110067},
  shortjournal = {Pattern Recognition},
  title        = {RoMP-transformer: Rotational bounding box with multi-level feature pyramid transformer for object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential architecture-agnostic black-box attack design and
analysis. <em>PR</em>, <em>147</em>, 110066. (<a
href="https://doi.org/10.1016/j.patcog.2023.110066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although adversarial machine learning attacks on image recognition models have been heavily investigated, the rising popularity of vision transformers revitalized the research on this topic. Due to the fundamental architectural differences between CNNs , which still dominate the image recognition applications, and transformers, the state-of-the-art attacks designed for CNNs are not effective against transformers, and vice versa. Such lack of transferability in attacks and the growing architectural heterogeneity in practice make the black-box attack design increasingly challenging. However, skillful attackers can handle the increasing uncertainty in target model architecture following two main approaches: designing transferable attacks that are robust to the architectural uncertainty in target model, and identifying the target architecture for attack selection. In this work, following the latter approach we propose a novel architecture-agnostic black-box attack design and analyze its performance. Experiments show that the proposed method, with a reasonable query overhead, outperforms the recent robust attack designs that follow the former approach. Different from the existing methods, the proposed method optimizes a trade-off between prior information about the target model and number of queries.},
  archive      = {J_PR},
  author       = {Furkan Mumcu and Yasin Yilmaz},
  doi          = {10.1016/j.patcog.2023.110066},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110066},
  shortjournal = {Pattern Recognition},
  title        = {Sequential architecture-agnostic black-box attack design and analysis},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NCL++: Nested collaborative learning for long-tailed visual
recognition. <em>PR</em>, <em>147</em>, 110064. (<a
href="https://doi.org/10.1016/j.patcog.2023.110064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed visual recognition has received increasing attention in recent years. Due to the extremely imbalanced data distribution in long-tailed learning, the learning process shows great uncertainties. For example, the predictions of different experts on the same image vary remarkably despite the same training settings. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL++) which tackles the long-tailed learning problem by a collaborative learning. To be specific, the collaborative learning consists of two folds, namely inter-expert collaborative learning (InterCL) and intra-expert collaborative learning (IntraCL). InterCL learns multiple experts collaboratively and concurrently, aiming to transfer the knowledge among different experts. IntraCL is similar to InterCL, but it aims to conduct the collaborative learning on multiple augmented copies of the same image within the single expert. To achieve the collaborative learning in long-tailed learning, the balanced online distillation is proposed to force the consistent predictions among different experts and augmented copies, which reduces the learning uncertainties. Moreover, in order to improve the meticulous distinguishing ability on the confusing categories, we further propose a Hard Category Mining (HCM), which selects the negative categories with high predicted scores as the hard categories. Then, the collaborative learning is formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether with using a single model or an ensemble. The code will be publicly released.},
  archive      = {J_PR},
  author       = {Zichang Tan and Jun Li and Jinhao Du and Jun Wan and Zhen Lei and Guodong Guo},
  doi          = {10.1016/j.patcog.2023.110064},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110064},
  shortjournal = {Pattern Recognition},
  title        = {NCL++: Nested collaborative learning for long-tailed visual recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCapsNet: Deep capsule network for human activity and gait
recognition with smartphone sensors. <em>PR</em>, <em>147</em>, 110054.
(<a href="https://doi.org/10.1016/j.patcog.2023.110054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep neural networks are used to recognize human activity/gait through mobile sensors which have attracted a great attention. Although the existing deep neural networks that perform automatic feature extraction have achieved desirable performance, their classification accuracy needs to be improved. In this paper, a deep neural network that combines a set of convolutional layers and capsule network is proposed. The proposed architecture named DCapsNet is suited to automatically extract the activity or gait features through built in sensors and classify them. The convolutional layers of the DCapsNet are more suitable for processing temporal sequences and provide scalar outputs but not the equivariance. The capsule network (CapsNet) is then trained by a dynamic routing algorithm to capture the equivariance having a magnitude and orientation, which increases the efficiency of the model classification. The performance of the proposed model is evaluated on four public datasets: two HAR datasets (UCI-HAR and WISDM) and two gait datasets (WhuGAIT). The recognition accuracy of the proposed model for the UCI-HAR and WISDM datasets are 97.92 % and 99.30 %, respectively, and for the WhuGAIT Dataset #1 and Dataset #2 are 94.75 % and 97.16 %, respectively. Experimental results show that the proposed model achieves the highest recognition accuracy over the reported results of the state-of-the-art models.},
  archive      = {J_PR},
  author       = {Ahmadreza Sezavar and Randa Atta and Mohammed Ghanbari},
  doi          = {10.1016/j.patcog.2023.110054},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110054},
  shortjournal = {Pattern Recognition},
  title        = {DCapsNet: Deep capsule network for human activity and gait recognition with smartphone sensors},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint discriminative representation learning for end-to-end
person search. <em>PR</em>, <em>147</em>, 110053. (<a
href="https://doi.org/10.1016/j.patcog.2023.110053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search simultaneously detects and retrieves a query person from uncropped scene images. Existing methods are either two-step or end-to-end. The former employs two standalone models for the two sub-tasks, while the latter conducts person search with a unified model. Despite encouraging progress, most existing end-to-end methods focus on balancing the model between detection and retrieval sub-tasks, while ignoring to enhance the learned representation for retrieval, which leads to inferior accuracy to two-step approaches. To that end, we propose a novel hierarchical framework that jointly optimizes instance-aware and part-aware embedding to enable discriminative representation learning . Specifically, we develop a region-of-interest cosegment (ROICoseg) module that captures part-aware information without requiring extra annotations to enable fine-grained discriminative representation. On top of that, a C ontextual I nstance B atch S ampling (CIBS) method is introduced to effectively employ contextual information for constructing training batches, thus facilitating effective instance-aware representation learning. We further introduce the first cross-door person search dataset (CDPS) that retrieves a target person in outdoor cameras with an indoor captured image or vice versa. Extensive experiments show that our proposed model achieves competitive performance on CUHK-SYSU and outperforms state-of-the-art end-to-end methods on the more challenging PRW and CDPS. 1},
  archive      = {J_PR},
  author       = {Pengcheng Zhang and Xiaohan Yu and Xiao Bai and Chen Wang and Jin Zheng and Xin Ning},
  doi          = {10.1016/j.patcog.2023.110053},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110053},
  shortjournal = {Pattern Recognition},
  title        = {Joint discriminative representation learning for end-to-end person search},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural architecture search: A contemporary literature review
for computer vision applications. <em>PR</em>, <em>147</em>, 110052. (<a
href="https://doi.org/10.1016/j.patcog.2023.110052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks have received considerable attention in recent years. As the complexity of network architecture increases in relation to the task complexity, it becomes harder to manually craft an optimal neural network architecture and train it to convergence. As such, Neural Architecture Search (NAS) is becoming far more prevalent within computer vision research, especially when the construction of efficient, smaller network architectures is becoming an increasingly important area of research, for which NAS is well suited. However, despite their promise, contemporary and end-to-end NAS pipeline require vast computational training resources. In this paper, we present a comprehensive overview of contemporary NAS approaches with respect to image classification , object detection, and image segmentation . We adopt consistent terminology to overcome contradictions common within existing NAS literature. Furthermore, we identify and compare current performance limitations in addition to highlighting directions for future NAS research.},
  archive      = {J_PR},
  author       = {Matt Poyser and Toby P. Breckon},
  doi          = {10.1016/j.patcog.2023.110052},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110052},
  shortjournal = {Pattern Recognition},
  title        = {Neural architecture search: A contemporary literature review for computer vision applications},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coupled discriminative manifold alignment for low-resolution
face recognition. <em>PR</em>, <em>147</em>, 110049. (<a
href="https://doi.org/10.1016/j.patcog.2023.110049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, due to a long distance between the monitored population and monitoring equipment, the face images or human pose captured by the cameras often incur low-resolution (LR), small size, and poor quality, which leads to extreme difficulty in directly matching an LR face with the high-resolution (HR) ones in the gallery. In this paper, we propose a novel coupled discriminative manifold alignment (CDMA) method for LR face recognition. Specifically, in the training stage the principal component analysis (PCA) is first used to reduce the dimensional gap between LR and HR facial features . Next the LR face images and the corresponding HR face images are converted into a common shared feature subspace by learning two linear mappings in a supervised manner, where the neighborhood samples within the same class and from different classes are jointly exploited to align the manifold structures of LR and HR faces. In the test stage, for a given LR face in the probe set , two learned coupled mappings (CMs) are applied to match the HR images in the gallery set through the correlative metric. Thorough experimental results on three representative face databases verify the effectiveness of the proposed method in comparing with other state-of-the-art competitors. In particular, the proposed method is capable of yielding more competitive recognition performance than other predecessors when lower dimensional feature subspaces are applied to match the expected HR faces.},
  archive      = {J_PR},
  author       = {Kaibing Zhang and Dongdong Zheng and Jie Li and Xinbo Gao and Jian Lu},
  doi          = {10.1016/j.patcog.2023.110049},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110049},
  shortjournal = {Pattern Recognition},
  title        = {Coupled discriminative manifold alignment for low-resolution face recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A temporal densely connected recurrent network for
event-based human pose estimation. <em>PR</em>, <em>147</em>, 110048.
(<a href="https://doi.org/10.1016/j.patcog.2023.110048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event camera is an emerging bio-inspired vision sensors that report per-pixel brightness changes asynchronously. It holds noticeable advantage of high dynamic range, high speed response, and low power budget that enable it to best capture local motions in uncontrolled environments. This motivates us to unlock the potential of event cameras for human pose estimation, as the human pose estimation with event cameras is rarely explored. Due to the novel paradigm shift from conventional frame-based cameras, however, event signals in a time interval contain very limited information, as event cameras can only capture the moving body parts and ignores those static body parts, resulting in some parts to be incomplete or even disappeared in the time interval. This paper proposes a novel densely connected recurrent architecture to address the problem of incomplete information . By this recurrent architecture, we can explicitly model not only the sequential but also non-sequential geometric consistency across time steps to accumulate information from previous frames to recover the entire human bodies, achieving a stable and accurate human pose estimation from event data. Moreover, to better evaluate our model, we collect a large-scale multimodal event-based dataset that comes with human pose annotations, which is by far the most challenging one to the best of our knowledge. The experimental results on two public datasets and our own dataset demonstrate the effectiveness and strength of our approach. Code is available online for facilitating the future research.},
  archive      = {J_PR},
  author       = {Zhanpeng Shao and Xueping Wang and Wen Zhou and Wuzhen Wang and Jianyu Yang and Youfu Li},
  doi          = {10.1016/j.patcog.2023.110048},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110048},
  shortjournal = {Pattern Recognition},
  title        = {A temporal densely connected recurrent network for event-based human pose estimation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep intra-image contrastive learning for weakly supervised
one-step person search. <em>PR</em>, <em>147</em>, 110047. (<a
href="https://doi.org/10.1016/j.patcog.2023.110047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised person search aims to perform joint pedestrian detection and re-identification (re-id) with only bounding-box annotations. Recently, the idea of contrastive learning is initially applied to weakly supervised person search, where two common contrast strategies are memory-based contrast and intra-image contrast. We argue that current intra-image contrast is shallow, which suffers from spatial-level and occlusion-level variance. In this paper, we present a novel deep intra-image contrastive learning using a Siamese network . Two key modules are spatial-invariant contrast (SIC) and occlusion-invariant contrast (OIC). SIC performs many-to-one contrasts between two branches of Siamese network and dense prediction contrasts in one branch of Siamese network. With these many-to-one and dense contrasts, SIC tends to learn discriminative scale-invariant and location-invariant features to solve spatial-level variance. OIC enhances feature consistency with the masking strategy to learn occlusion-invariant features. Extensive experiments are performed on two person search datasets. Our method achieves a state-of-the-art performance among weakly supervised one-step person search approaches.},
  archive      = {J_PR},
  author       = {Jiabei Wang and Yanwei Pang and Jiale Cao and Hanqing Sun and Zhuang Shao and Xuelong Li},
  doi          = {10.1016/j.patcog.2023.110047},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110047},
  shortjournal = {Pattern Recognition},
  title        = {Deep intra-image contrastive learning for weakly supervised one-step person search},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative features enhancement for low-altitude UAV
object detection. <em>PR</em>, <em>147</em>, 110041. (<a
href="https://doi.org/10.1016/j.patcog.2023.110041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection is a pivotal task in low-altitude UAV application. Here the small scale objects are dominant due to shooting distance and angle and insufficient feature information due to the data from real world scenes. Although general detector has made great progress, it is not suitable for small scale object detection directly. Dense detector has potential because of the pixel-by-pixel detection but the resolving power of complex background and objects especially small scale objects is still insufficient. We propose a Feature Guided Enhancement module by designing two non-linear learning operators to guide more discriminative features when training. Further, a Scale-Aware Weighted loss function is proposed to dynamically weight the loss of various scale objects by statistical computing and highlight the contribution of small scale objects. Experimental results show that our method can effectively improve FCOS and ATSS, and our models obtain better performance by 1.5% and 0.6% AP respectively on VisDrone 2018 dataset.},
  archive      = {J_PR},
  author       = {Shuqin Huang and Shasha Ren and Wei Wu and Qiong Liu},
  doi          = {10.1016/j.patcog.2023.110041},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110041},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative features enhancement for low-altitude UAV object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness in face presentation attack detection. <em>PR</em>,
<em>147</em>, 110002. (<a
href="https://doi.org/10.1016/j.patcog.2023.110002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition (FR) algorithms have been proven to exhibit discriminatory behaviors against certain demographic and non-demographic groups, raising ethical and legal concerns regarding their deployment in real-world scenarios. Despite the growing number of fairness studies in FR, the fairness of face presentation attack detection (PAD) has been overlooked, mainly due to the lack of appropriately annotated data. To avoid and mitigate the potential negative impact of such behavior, it is essential to assess the fairness in face PAD and develop fair PAD models. To enable fairness analysis in face PAD, we present a Combined Attribute Annotated PAD Dataset (CAAD-PAD), offering seven human-annotated attribute labels. Then, we comprehensively analyze the fairness of PAD and its relation to the nature of the training data and the Operational Decision Threshold Assignment (ODTA) through a set of face PAD solutions. Additionally, we propose a novel metric, the Accuracy Balanced Fairness (ABF), that jointly represents both the PAD fairness and the absolute PAD performance. The experimental results pointed out that female and faces with occluding features (e.g. eyeglasses, beard, etc.) are relatively less protected than male and non-occlusion groups by all PAD solutions. To alleviate this observed unfairness, we propose a plug-and-play data augmentation method, FairSWAP, to disrupt the identity/semantic information and encourage models to mine the attack clues. The extensive experimental results indicate that FairSWAP leads to better-performing and fairer face PADs in 10 out of 12 investigated cases.},
  archive      = {J_PR},
  author       = {Meiling Fang and Wufei Yang and Arjan Kuijper and Vitomir S̆truc and Naser Damer},
  doi          = {10.1016/j.patcog.2023.110002},
  journal      = {Pattern Recognition},
  month        = {3},
  pages        = {110002},
  shortjournal = {Pattern Recognition},
  title        = {Fairness in face presentation attack detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient long-short temporal attention network for
unsupervised video object segmentation. <em>PR</em>, <em>146</em>,
110078. (<a href="https://doi.org/10.1016/j.patcog.2023.110078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Video Object Segmentation (VOS) aims at identifying the contours of primary foreground objects in videos without any prior knowledge. However, previous methods do not fully use spatial–temporal context and fail to tackle this challenging task in real-time. This motivates us to develop an efficient L ong- S hort T emporal A ttention network (termed LSTA ) for unsupervised VOS task from a holistic view. Specifically, LSTA consists of two dominant modules, i.e., Long Temporal Memory and Short Temporal Attention. The former captures the long-term global pixel relations of the past frames and the current frame, which models constantly present objects by encoding appearance pattern. Meanwhile, the latter reveals the short-term local pixel relations of one nearby frame and the current frame, which models moving objects by encoding motion pattern. To speedup the inference, the efficient projection and the locality-based sliding window are adopted to achieve nearly linear time complexity for the two light modules, respectively. Extensive empirical studies on several benchmarks have demonstrated promising performances of the proposed method with high efficiency.},
  archive      = {J_PR},
  author       = {Ping Li and Yu Zhang and Li Yuan and Huaxin Xiao and Binbin Lin and Xianghua Xu},
  doi          = {10.1016/j.patcog.2023.110078},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110078},
  shortjournal = {Pattern Recognition},
  title        = {Efficient long-short temporal attention network for unsupervised video object segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “L1-norm discriminant analysis via bhattacharyya
error bounds under laplace distributions” [pattern recognition 141
(2023) 109609]. <em>PR</em>, <em>146</em>, 110070. (<a
href="https://doi.org/10.1016/j.patcog.2023.110070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  author       = {Zhizheng Liang and Lei Zhang},
  doi          = {10.1016/j.patcog.2023.110070},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110070},
  shortjournal = {Pattern Recognition},
  title        = {Erratum to ‘L1-norm discriminant analysis via bhattacharyya error bounds under laplace distributions’ [Pattern recognition 141 (2023) 109609]},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep image clustering with contrastive learning and
multi-scale graph convolutional networks. <em>PR</em>, <em>146</em>,
110065. (<a href="https://doi.org/10.1016/j.patcog.2023.110065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering has shown its promising capability in joint representation learning and clustering via deep neural networks . Despite the significant progress, the existing deep clustering works mostly utilize some distribution-based clustering loss, lacking the ability to unify representation learning and multi-scale structure learning . To address this, this paper presents a new deep clustering approach termed I mage c luster i ng with c ontrastive le arning and multi-scale G raph C onvolutional N etworks (IcicleGCN), which bridges the gap between convolutional neural network (CNN) and graph convolutional network (GCN) as well as the gap between contrastive learning and multi-scale structure learning for the deep clustering task . Our framework consists of four main modules, namely, the CNN-based backbone, the Instance Similarity Module (ISM), the Joint Cluster Structure Learning and Instance reconstruction Module (JC-SLIM), and the Multi-scale GCN module (M-GCN). Specifically, the backbone network with two weight-sharing views is utilized to learn the representations for the two augmented samples (from each image). The learned representations are then fed to ISM and JC-SLIM for joint instance-level and cluster-level contrastive learning, respectively, during which an auto-encoder in JC-SLIM is also pretrained to serve as a bridge to the M-GCN module. Further, to enforce multi-scale neighborhood structure learning, two streams of GCNs and the auto-encoder are simultaneously trained via (i) the layer-wise interaction with representation fusion and (ii) the joint self-adaptive learning. Experiments on multiple image datasets demonstrate the superior clustering performance of IcicleGCN over the state-of-the-art. The code is available at https://github.com/xuyuankun631/IcicleGCN .},
  archive      = {J_PR},
  author       = {Yuankun Xu and Dong Huang and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1016/j.patcog.2023.110065},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110065},
  shortjournal = {Pattern Recognition},
  title        = {Deep image clustering with contrastive learning and multi-scale graph convolutional networks},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GNaN: A natural neighbor search algorithm based on universal
gravitation. <em>PR</em>, <em>146</em>, 110063. (<a
href="https://doi.org/10.1016/j.patcog.2023.110063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The natural neighbor (NaN) method and its search algorithm (NaN-Searching) are widely used in many fields, including pattern recognition and image processing . NaN-Searching fundamentally overcomes the problem of the conventional nearest neighbor algorithm in selecting parameters for datasets with arbitrary shapes and achieves good results. However, this algorithm uses the conventional distance metric as the neighbor judgment criterion, which cannot accurately reflect the overall structure of the dataset in the process of neighbor search. Inspired by Newton’s law of universal gravitation, we propose a NaN search algorithm based on universal gravitation (GNaN-Searching). Our algorithm calculates gravitation using the structural features of data points in the dataset, it utilizes the gravitation between data as the neighbor judgment criterion, and inherits the no-parameter and dynamic neighborhood characteristics of the NaN search algorithm. Experimental results show that the natural neighborhood graph obtained by our method has a high performance in the representation of manifold data. We also applied the new method to clustering and outlier detection and achieved satisfactory results.},
  archive      = {J_PR},
  author       = {Juntao Yang and Lijun Yang and Jinghui Zhang and Qiwen Liang and Wentong Wang and Dongming Tang and Tao Liu},
  doi          = {10.1016/j.patcog.2023.110063},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110063},
  shortjournal = {Pattern Recognition},
  title        = {GNaN: A natural neighbor search algorithm based on universal gravitation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Coordinate descent optimized trace difference model for
joint clustering and feature extraction. <em>PR</em>, <em>146</em>,
110062. (<a href="https://doi.org/10.1016/j.patcog.2023.110062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Joint clustering and dimensionality reduction methods are a promising solution to clustering due to its scalability to high-dimensional data. Some methods leverage trace ratio criterion and attain clusters by borrowing the K-means algorithm. However, trace ratio criterion has no close-formed solution for the discriminative projection matrix and the K-means algorithm has a limited capacity to handle the many-cluster problem. In this paper, Coordinate Descent Optimized Trace Difference model (CDOTD) is proposed for joint clustering and feature extraction. Formulating the objective function as a direct trace difference criterion containing a balance parameter, CDOTD harmonizes between-cluster scatter maximization and within-cluster scatter minimization by the balance parameter. Using the direct trace difference criterion, CDOTD can straightforward solve for the discriminative projection matrix and avoid obtaining a poor discriminative projection matrix in the iterative manner when a bad cluster start is given. CDOTD uses the coordinate descent method for clustering optimization, improving the ability to address the many-cluster problem. Extensive experiments show that CDOTD has achieved significant performance improvements compared to previous trace ratio criterion related joint clustering and feature extraction methods, and also outperformed other clustering methods in most cases.},
  archive      = {J_PR},
  author       = {Quan Wang and Fei Wang and Zhongheng Li and Zheng Wang and Feiping Nie},
  doi          = {10.1016/j.patcog.2023.110062},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110062},
  shortjournal = {Pattern Recognition},
  title        = {Coordinate descent optimized trace difference model for joint clustering and feature extraction},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Objformer: Boosting 3D object detection via instance-wise
interaction. <em>PR</em>, <em>146</em>, 110061. (<a
href="https://doi.org/10.1016/j.patcog.2023.110061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning on point clouds drives 3D object detection . Despite rapid progress, point-based methods still suffer from the problems such as incompletion and occlusion, which are caused by the material properties of objects and cluttered scenes. These difficult targets increase the difficulty of identification or even lead to misidentification, severely weakening the performance of point-based methods on 3D object detection . To alleviate the above problems, we propose the Objformer to boost point-based 3D object detection via instance-wise interaction. We design an instance feature encoder to encode clean instance features, which contain key geometric priors and holistic semantic information. Further, an instance interaction module is devised to aggregate the complementary features across instances with label-guided interaction, boosting the performance of the 3D object detection. Experiments show that Objformer outperforms previous point-based state-of-the-arts on two popular benchmarks, ScanNet V2 and SUN RGB-D. Especially, our single-modal Objformer even outperforms the competing advanced multi-modal fusion method on both SUN RGB-D and ScanNet V2.},
  archive      = {J_PR},
  author       = {Manli Tao and Chaoyang Zhao and Ming Tang and Jinqiao Wang},
  doi          = {10.1016/j.patcog.2023.110061},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110061},
  shortjournal = {Pattern Recognition},
  title        = {Objformer: Boosting 3D object detection via instance-wise interaction},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Pseudo-label estimation via unsupervised identity link
prediction for one-shot person re-identification. <em>PR</em>,
<em>146</em>, 110060. (<a
href="https://doi.org/10.1016/j.patcog.2023.110060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an unsupervised identity link prediction (ILP) method for label estimation in one-shot person Re-ID. ILP aims to relax the constraints of labeled samples and group a set of unlabeled pedestrians by their potential identities. The main idea is that the category relationships between pedestrians (nodes) can be inferred from their local context in the feature space . Specifically, an Identity Link Subgraph (ILS) describes the link relationship between nodes and their nearest neighbors, which is constructed by a two-step procedure. Meanwhile, a Dynamic Penalty Module (DPM) is introduced at each ILS construction step to infer which linkage between pairs in the ILS should be pruned to assign higher-quality classification labels. To fully use the accurate identity information in initial labeled samples, we jointly use identity pseudo-labels (which are estimated by adopting the Nearest Neighbors classifier) with classification pseudo-labels for model training. Moreover, we design a Dual-Branch Fusion network (DBF-Net) to optimize the CNN model simultaneously through all (pseudo-)labeled samples. Results on multiple datasets prove that DBF-Net outperforms traditional one-shot Re-ID methods by a large margin.},
  archive      = {J_PR},
  author       = {Yulin Zhang and Bo Ma and Meng Li and Ying Liu and Feng Chen and Junyu Hou},
  doi          = {10.1016/j.patcog.2023.110060},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110060},
  shortjournal = {Pattern Recognition},
  title        = {Pseudo-label estimation via unsupervised identity link prediction for one-shot person re-identification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-order relational generative adversarial network for
video super-resolution. <em>PR</em>, <em>146</em>, 110059. (<a
href="https://doi.org/10.1016/j.patcog.2023.110059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video super-resolution can reconstruct a sequence of high-resolution frames with temporally consistent contents from their corresponding low-resolution sequences. The key challenge for this task is how to effectively utilize both inter-frame temporal relations and intra-frame spatial relations. The existing methods for super-resolving the videos commonly estimate optical flows to align the features of multiple frames based on temporal correlations. However, motion estimation is often error-prone and hence largely hinders the recovery of plausible details. Moreover, high-order contextual dependencies in the feature space are rarely exploited for further enhancing the spatio-temporal information fusion. To this end, we propose a novel generative adversarial network to super-resolve low-resolution videos, which makes full use of patch embeddings and is effective in exploring high-order spatio-temporal relations of the feature patches. Specifically, a motion-aware relation module is designed to handle the alignment between neighboring frames and reference ones. Depending on a patch-matching strategy for adaptive selection of multiple most similar patches, the cross-scale graph is constructed to reliably aggregate these patches using a feature pyramid. Based on the structure of multi-scale graph, a context-aware relation module is developed to capture high-order dependencies among resulting warped patches for better leveraging long-range complementary contexts. To further enhance reconstruction ability, the temporal position information of video sequences is also encoded into this module. Dual discriminators with cycle consistent constraints are adopted to provide more informative feedback to the generator while maintaining the global coherence. Extensive experiments have demonstrated the effectiveness of the proposed method in terms of quantitative and qualitative evaluation metrics.},
  archive      = {J_PR},
  author       = {Rui Chen and Yang Mu and Yan Zhang},
  doi          = {10.1016/j.patcog.2023.110059},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110059},
  shortjournal = {Pattern Recognition},
  title        = {High-order relational generative adversarial network for video super-resolution},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical image-to-image translation with nested
distributions modeling. <em>PR</em>, <em>146</em>, 110058. (<a
href="https://doi.org/10.1016/j.patcog.2023.110058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired image-to-image translation among category domains has achieved remarkable success in past decades. Recent studies mainly focus on two challenges. For one thing, such translation is inherently multi-modal (i.e. many-to-many mapping) due to variations of domain-specific information (e.g., the domain of house cat contains multiple sub-modes), which is usually addressed by predefined distribution sampling. For another, most existing multi-modal approaches have limits in handling more than two domains with one model, i.e. they have to independently build two distributions to capture variations for every pair of domains. To address these problems, we propose a Hierarchical Image-to-image Translation (HIT) method which jointly formulates the multi-domain and multi-modal problem in a semantic hierarchy structure by modeling a common and nested distribution space. Specifically, domains have inclusion relationships under a particular hierarchy structure. With the assumption of Gaussian prior for domains, distributions of domains at lower levels capture the local variations of their ancestors at higher levels, leading to the so-called nested distributions. To this end, we propose a nested distribution loss in light of the distribution divergence measurement and information entropy theory to characterize the aforementioned inclusion relations among domain distributions. Experiments on ImageNet, ShapeNet, and CelebA datasets validate the promising results of our HIT against state-of-the-arts, and as additional benefits of nested modeling, one can even control the uncertainty of multi-modal translations at different hierarchy levels.},
  archive      = {J_PR},
  author       = {Shishi Qiao and Ruiping Wang and Shiguang Shan and Xilin Chen},
  doi          = {10.1016/j.patcog.2023.110058},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110058},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical image-to-image translation with nested distributions modeling},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introducing instance label correlation in multiple instance
learning. Application to cancer detection on histopathological images.
<em>PR</em>, <em>146</em>, 110057. (<a
href="https://doi.org/10.1016/j.patcog.2023.110057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, the weakly supervised paradigm of multiple instance learning (MIL) has become very popular in many different areas. A paradigmatic example is computational pathology, where the lack of patch-level labels for whole-slide images prevents the application of supervised models. Probabilistic MIL methods based on Gaussian Processes (GPs) have obtained promising results due to their excellent uncertainty estimation capabilities. However, these are general-purpose MIL methods that do not take into account one important fact: in (histopathological) images, the labels of neighboring patches are expected to be correlated. In this work, we extend a state-of-the-art GP-based MIL method, which is called VGPMIL-PR, to exploit such correlation. To do so, we develop a novel coupling term inspired by the statistical physics Ising model . We use variational inference to estimate all the model parameters. Interestingly, the VGPMIL-PR formulation is recovered when the weight that regulates the strength of the Ising term vanishes. The performance of the proposed method is assessed in two real-world problems of prostate cancer detection. We show that our model achieves better results than other state-of-the-art probabilistic MIL methods. We also provide different visualizations and analysis to gain insights into the influence of the novel Ising term. These insights are expected to facilitate the application of the proposed model to other research areas.},
  archive      = {J_PR},
  author       = {Pablo Morales-Álvarez and Arne Schmidt and José Miguel Hernández-Lobato and Rafael Molina},
  doi          = {10.1016/j.patcog.2023.110057},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110057},
  shortjournal = {Pattern Recognition},
  title        = {Introducing instance label correlation in multiple instance learning. application to cancer detection on histopathological images},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted subspace anomaly detection in high-dimensional
space. <em>PR</em>, <em>146</em>, 110056. (<a
href="https://doi.org/10.1016/j.patcog.2023.110056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection aims at finding anomalies deviating from the normal data patterns. Virtually all anomaly detection methods create a model of the normal patterns before finding anomalies. In high-dimensional scenarios, due to the curse of dimensionality, it is difficult to construct the model of normal patterns in the full dimensional space. Subspace methods assuming that data can be characterized by low-dimensional manifolds have attracted a great deal of research. However, in unsupervised setting, unlabeled data is composed of both the normal and the abnormal data. The existence of anomalies might affect the establishment of the underlying normal subspaces. The undetermined number of the underlying subspaces also brings difficulties in subspace selection. To tackle the aforementioned problems, we come up with a weighted subspace anomaly detection (WSAD) method. We utilize correntropy to construct an objective function to mitigate the influence of the anomalies, which can be regarded as a weighting method for different data. Besides, we introduce an auxiliary variable with block sparsity regularization to achieve adaptive subspace selection, which can be regarded as a weighting method for different subspaces. After the normal underlying subspaces being established, we define the outlier scores by considering the deviation from the underlying subspaces, the local outlier score within subspaces, and the subspace scale. We use the half-quadratic theory to transform the optimization problem defined in WSAD, and apply alternating optimization to solve the transformed problem. Theoretically, we prove the convergence of the optimization algorithm . Experimentally, we demonstrate the effectiveness of the proposed method on both synthetic data and real datasets.},
  archive      = {J_PR},
  author       = {Jiankai Tu and Huan Liu and Chunguang Li},
  doi          = {10.1016/j.patcog.2023.110056},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110056},
  shortjournal = {Pattern Recognition},
  title        = {Weighted subspace anomaly detection in high-dimensional space},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open set transfer learning through distribution driven
active learning. <em>PR</em>, <em>146</em>, 110055. (<a
href="https://doi.org/10.1016/j.patcog.2023.110055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation enables effective transfer between source and target domains with different distributions. The latest research focuses on open set domain adaptation; that is, the target domain contains unknown categories that do not exist in the source domain. The existing open set domain adaptation cannot realize the fine-grained recognition of unknown categories. In this paper, we propose an uncertainty analysis evidence model and design a distribution driven active transfer learning (DATL) algorithm. DATL realizes fine-grained recognition of unknown categories with no requirements on the source domain to contain the unknown categories. To explore unknown distributions, the uncertainty analysis evidence model was adopted to divide the high uncertainty space. To select critical instances, a cluster-diversity query strategy was proposed to identify new categories. To enrich the label categories of the source domain, a global dynamic alignment strategy was designed to avoid negative transfers. Comparative experiments with state-of-the-art methods on the standard Office-31/Office-Home/Office-Caltech10 benchmarks showed that the DATL algorithm: (1) outperformed its competitors; (2) realized accurate identification of unknown subcategories from a fine-grained perspective; and (3) achieved outstanding performance even with a very high degree of openness.},
  archive      = {J_PR},
  author       = {Min Wang and Ting Wen and Xiao-Yu Jiang and An-An Zhang},
  doi          = {10.1016/j.patcog.2023.110055},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110055},
  shortjournal = {Pattern Recognition},
  title        = {Open set transfer learning through distribution driven active learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase randomization: A data augmentation for domain
adaptation in human action recognition. <em>PR</em>, <em>146</em>,
110051. (<a href="https://doi.org/10.1016/j.patcog.2023.110051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition models often suffer from achieving both accurate recognition and subject independence when the amount of training data is limited. In this paper, we propose a data-efficient domain adaptation approach to learning a subject-agnostic action recognition classifier . The core component of our approach is a novel data augmentation called Phase Randomization . On the basis of the observation that individual body size is highly correlated with the amplitude component of the motion sequence, we disentangle the individuality and action features by using contrastive self-supervised learning with data augmentation that randomizes only the phase component of the motion sequence. This enables us to estimate the subject label of each motion sequence and to train a subject-agnostic action recognition classifier by performing adversarial learning with the estimated subject labels. We empirically demonstrate the superiority of our method on two different action recognition tasks (skeleton-based action recognition and sensor-based activity recognition).},
  archive      = {J_PR},
  author       = {Yu Mitsuzumi and Go Irie and Akisato Kimura and Atsushi Nakazawa},
  doi          = {10.1016/j.patcog.2023.110051},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110051},
  shortjournal = {Pattern Recognition},
  title        = {Phase randomization: A data augmentation for domain adaptation in human action recognition},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharing-net: Lightweight feedforward network for
skeleton-based action recognition based on information sharing
mechanism. <em>PR</em>, <em>146</em>, 110050. (<a
href="https://doi.org/10.1016/j.patcog.2023.110050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of metaverse, augmented reality and human–robot teleoperation , action recognition plays an increasingly important role. In this work, we propose a Lightweight Feedforward Cross-channel Information Sharing Network (Sharing-Net) for action recognition. A multi-feature input module is constructed, which includes Cartesian Motion features, Global Joint Distances (GJD), and Global Joint Angles (GJA). The three types of features can tackle the problems of velocity differentiation, viewpoint diversification and object-distance variation, respectively. In order to take full use of the restricted parameters caused by the lightweight structure to enhance the accuracy under the premise of guaranteeing high speed, a multi-feature cross-channel information sharing mechanism is proposed. Dynamic nonlinear composite mapping between feature channels uses cross-channel residual blocks to share data information and establish coupling relationships. Extensive experiments on 3 public datasets and a self-built dataset verify the effectiveness of proposed methods. Compared with the state-of-the-art (SOAT) methods, Sharing-Net achieves the best accuracy with high speed on JHMDB and SHREC and performs superior balance of accuracy and computational cost on NTU RGB+D.},
  archive      = {J_PR},
  author       = {Yinan Zhao and Qing Gao and Zhaojie Ju and Jian Zhou and Yulan Guo},
  doi          = {10.1016/j.patcog.2023.110050},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110050},
  shortjournal = {Pattern Recognition},
  title        = {Sharing-net: Lightweight feedforward network for skeleton-based action recognition based on information sharing mechanism},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional feature generation for transductive open-set
recognition via dual-space consistent sampling. <em>PR</em>,
<em>146</em>, 110046. (<a
href="https://doi.org/10.1016/j.patcog.2023.110046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set recognition (OSR) aims to simultaneously detect unknown-class samples and classify known-class samples. Most of the existing OSR methods are inductive methods, which generally suffer from the domain shift problem that the learned model from the known-class domain might be unsuitable for the unknown-class domain. Addressing this problem, inspired by the success of transductive learning for alleviating the domain shift problem in many other visual tasks, we propose an Iterative Transductive OSR framework, called IT-OSR, which implements three explored modules iteratively, including a reliability sampling module, a feature generation module, and a baseline update module. Specifically at the initialization stage , a baseline method , which could be an arbitrary inductive OSR method, is used for assigning pseudo labels to the test samples. At the iteration stage, based on the consistency of the assigned pseudo labels between the output/logit space and the latent feature space of the baseline method, a dual-space consistent sampling approach is presented in the reliability sampling module for sampling some reliable ones from the test samples. Then in the feature generation module, a conditional dual-adversarial generative network is designed to generate discriminative features of both known and unknown classes. This generative network employs two discriminators for implementing fake/real and known/unknown-class discriminations respectively. And it is trained by jointly utilizing the test samples with their pseudo labels selected in the reliability sampling module and the labeled training samples . Finally in the baseline update module, the above baseline method is updated/re-trained for sample re-prediction by jointly utilizing the generated features, the selected test samples with pseudo labels, and the training samples. Extensive experimental results on both the standard-dataset and the cross-dataset settings demonstrate that the derived transductive methods, by introducing two typical inductive OSR methods into the proposed IT-OSR framework, achieve better performances than 19 state-of-the-art methods in most cases.},
  archive      = {J_PR},
  author       = {Jiayin Sun and Qiulei Dong},
  doi          = {10.1016/j.patcog.2023.110046},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110046},
  shortjournal = {Pattern Recognition},
  title        = {Conditional feature generation for transductive open-set recognition via dual-space consistent sampling},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel attention-driven framework for unsupervised
pedestrian re-identification with clustering optimization. <em>PR</em>,
<em>146</em>, 110045. (<a
href="https://doi.org/10.1016/j.patcog.2023.110045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised pedestrian re-identification (re-ID) is not merely a visual recognition task; it represents a significant sub-field within the domain of pattern recognition. Despite the remarkable success of Convolutional Neural Networks (CNN) in re-ID, they still face challenges in handling variations in pose, occlusion, and lighting conditions . To effectively tackle these challenges, it is imperative to prioritize implementing efficient sampling strategies. We propose a Novel Attention-Driven Framework for Unsupervised Pedestrian re-ID with Clustering Optimization (AFC) to address the above issues. First, we introduce a new attention mechanism that enhances multi-scale spatial attention and reduces the number of trainable parameters. Then, we employed a straightforward and effective method of group sampling. In addition, we apply a clustering consensus approach to estimate pseudo-label similarity in continuous training and use temporal propagation and ensembles to improve pseudo-labels. Extensive experiments on Market-1501, duketmc-reID and MSMT17 datasets show that our method achieves significant performance improvement in unsupervised pedestrian re-ID, which provides important theoretical and practical value for the research on deep fusion of pattern recognition field with pedestrian re-ID and promotes the further development of the related fields.},
  archive      = {J_PR},
  author       = {Xuan Wang and Zhaojie Sun and Abdellah Chehri and Gwanggil Jeon and Yongchao Song},
  doi          = {10.1016/j.patcog.2023.110045},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110045},
  shortjournal = {Pattern Recognition},
  title        = {A novel attention-driven framework for unsupervised pedestrian re-identification with clustering optimization},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DataMap: Dataset transferability map for medical image
classification. <em>PR</em>, <em>146</em>, 110044. (<a
href="https://doi.org/10.1016/j.patcog.2023.110044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-based models especially Convolutional Neural Network (CNN) models have recently achieved great success in medical image classifications . It is usually time-consuming and labor-intensive to train a practical classification model due to the requirement of large data volume. Since medical images are more difficult to acquire and label for model training, many scholars have applied transfer learning by pre-training a model on a larger dataset and then fine-tuning it on the target dataset to obtain better classification results . However, such approach, relying on individual expertise to select related datasets, is subjective and inconsistent in performance. In this paper, we propose a simple yet effective method for measuring the transferability between different datasets, and build a Dataset Map (DataMap) that can be used as a tool to find the most relevant datasets for transfer learning on target dataset. Recent studies show that the convolutional kernels in CNN models have different function roles. Therefore, we adopt the similarity between the convolution kernels to measure the transferability between datasets. Firstly, the gradient attribution is adopted to attribute the task related convolution kernels from last few convolution layers of the same pre-trained model architecture trained with different datasets. Then, the similarity between attributed convolutional kernels is calculated to denote the transferability between different datasets. Finally, we build a DataMap with 20 medical image datasets. Extensive experimental tests on 3 mainstream CNN architectures show that the proposed method can effectively measure the transferability between different datasets. With the guidance of the DataMap, the transfer learning can achieve the best performance on various training tasks, and the accuracy of the CNN classifier can be improved by 1% to 5% through pre-training.},
  archive      = {J_PR},
  author       = {Xiangtong Du and Zhidong Liu and Zunlei Feng and Hai Deng},
  doi          = {10.1016/j.patcog.2023.110044},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110044},
  shortjournal = {Pattern Recognition},
  title        = {DataMap: Dataset transferability map for medical image classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frequency-aware feature aggregation network with dual-task
consistency for RGB-t salient object detection. <em>PR</em>,
<em>146</em>, 110043. (<a
href="https://doi.org/10.1016/j.patcog.2023.110043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Thermal salient object detection (SOD) aims to merge two spectral images to segment visually appealing objects. Current methods primarily extract salient object information in the pixel perspective. However, biological and psychological research indicates notable frequency sensitivity of the human visual system (HVS). The high-frequency (HF) and low-frequency (LF) information in images are processed by different neural channels, which has been overlooked in SOD. In this study, we argue that the objective of RGB-T SOD is not only to enhance feature representation in the pixel-aware but also to emulate human visual mechanisms. To our best knowledge, we explore RGB-T SOD from the frequency perspective for the first time. Specifically, we first present a frequency-aware multi-spectral feature aggregation module (FMFA) to exploit the separability and complementarity of frequency-aware features, generating and making full use of LF and HF cues. FMFA improves the feature representation of RGB-T from the frequency perspective and provides stronger frequency cues for boundary auxiliary tasks. Then, we develop an HF-guided signed distance map prediction module (HF-SDM) with dual-task consistency to effectively alleviate the coarse mask caused by blur boundary. HF-SDM employs the geometric relationship of objects which boosts the interaction between salient regions and boundaries. As a result, the model can gain sharper boundaries for salient objects. Finally, we propose a frequency-aware feature aggregation network (FFANet) incorporated with dual-task learning. Extensive experiments on RGB-T SOD datasets demonstrate that our proposed method outperforms other state-of-the-art methods. Ablation studies and visualizations further verify the effectiveness and interpretability of our method.},
  archive      = {J_PR},
  author       = {Heng Zhou and Chunna Tian and Zhenxi Zhang and Chengyang Li and Yongqiang Xie and Zhongbo Li},
  doi          = {10.1016/j.patcog.2023.110043},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110043},
  shortjournal = {Pattern Recognition},
  title        = {Frequency-aware feature aggregation network with dual-task consistency for RGB-T salient object detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage fine-grained image classification model based on
multi-granularity feature fusion. <em>PR</em>, <em>146</em>, 110042. (<a
href="https://doi.org/10.1016/j.patcog.2023.110042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) is a difficult task due to the challenges of discriminative feature learning. Most existing methods directly use the final output of the network which always contains the global feature with high-level semantic information. However, the differences between fine-grained images are reflected in subtle local regions which often appear in the front of the network. When the texture of the background and object are similar or the proportion of the background is too large, the prediction will be greatly affected. In order to solve the above problems, this paper proposes multi-granularity feature fusion module (MGFF) and two-stage classification based on Vision-Transformer (ViT). The former comprehensively represents images by fusing features of different granularities, thus avoiding the limitations of single-scale features. The latter leverages the ViT model to separate the object from the background at a very small cost, thereby improving the accuracy of the prediction. We conduct comprehensive experiments and achieves the best performance in two fine-grained tasks on CUB-200-2011 and NA-Birds.},
  archive      = {J_PR},
  author       = {Yang Xu and Shanshan Wu and Biqi Wang and Ming Yang and Zebin Wu and Yazhou Yao and Zhihui Wei},
  doi          = {10.1016/j.patcog.2023.110042},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110042},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage fine-grained image classification model based on multi-granularity feature fusion},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HIE-EDT: Hierarchical interval estimation-based evidential
decision tree. <em>PR</em>, <em>146</em>, 110040. (<a
href="https://doi.org/10.1016/j.patcog.2023.110040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision tree algorithm , because of its strong interpretability and high algorithm efficiency, is widely used in the field of pattern recognition and classification. When the number of data samples is small and there is uncertainty in the data, it is difficult for the traditional decision tree algorithm to fully mine the effective information in the data. In this paper, we use the Dempster–Shafer framework to model data uncertainty and propose a hierarchical interval estimation method to improve decision tree algorithms. The proposed method constructs intervals through two methods of attribute boundary and mean square error estimation, which not only utilizes the characteristics of intervals to model the inaccuracy of data, but also constrains intervals from two aspects, narrowing the representation range of available information. By comparing with the classic decision tree algorithm and the decision tree algorithm based on single interval estimation, the proposed method can perform classification tasks robustly and accurately in different types of data under seven data sets.},
  archive      = {J_PR},
  author       = {Bingjie Gao and Qianli Zhou and Yong Deng},
  doi          = {10.1016/j.patcog.2023.110040},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110040},
  shortjournal = {Pattern Recognition},
  title        = {HIE-EDT: Hierarchical interval estimation-based evidential decision tree},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSL-net: Sparse semantic learning for identifying reliable
correspondences. <em>PR</em>, <em>146</em>, 110039. (<a
href="https://doi.org/10.1016/j.patcog.2023.110039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature matching aims to identify reliable correspondences between two sets of given initial feature points, which is of considerable importance to photogrammetry and computer vision . In this study, we propose an innovative sparse semantic learning-based network, named SSL-Net, for feature matching. Specifically, SSL-Net includes a novel sparsity constraint (SC) block, which builds a sparse graph for sparse semantic learning. The SC block adopts a region-to-whole learning strategy to measure the confidence of nodes in the sparse graph. It helps the sparse graph preserve the semantic information of positive influence while rejecting unnecessary ones, thereby suppressing the negative influence of incorrect correspondences. In addition, SSL-Net also includes a channel-spatial attention feature gathering block, which gathers features along the spatial direction and channel dimension of correspondences. To mitigate the existence of label ambiguity, we incorporate the accommodation factor into the loss function of SSL-Net for feature matching. As a result, our network outperforms the state-of-the-art method by a considerable margin. Notably, SSL-Net achieves a 9.05% improvement under an error threshold of 5 ° 5° over the state-of-the-art method for the relative pose estimation task on the YFCC100M dataset. Our code will be available at https://github.com/guobaoxiao/SSL-Net .},
  archive      = {J_PR},
  author       = {Shunxing Chen and Guobao Xiao and Ziwei Shi and Junwen Guo and Jiayi Ma},
  doi          = {10.1016/j.patcog.2023.110039},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110039},
  shortjournal = {Pattern Recognition},
  title        = {SSL-net: Sparse semantic learning for identifying reliable correspondences},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient search of comprehensively robust neural
architectures via multi-fidelity evaluation. <em>PR</em>, <em>146</em>,
110038. (<a href="https://doi.org/10.1016/j.patcog.2023.110038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) has emerged as one successful technique to find robust deep neural network (DNN) architectures. However, most existing robustness evaluations in NAS only consider l ∞ l∞ norm-based adversarial noises. In order to improve the robustness of DNN models against multiple types of noises, it is necessary to consider a comprehensive evaluation in NAS for robust architectures. But with the increasing number of types of robustness evaluations, it also becomes more time-consuming to find comprehensively robust architectures. To alleviate this problem, we propose a novel efficient search of comprehensively robust neural architectures via multi-fidelity evaluation (ES-CRNA-ME). Specifically, we first search for comprehensively robust architectures under multiple types of evaluations using the weight-sharing-based NAS method, including different l p lp norm attacks, semantic adversarial attacks , and composite adversarial attacks. In addition, we reduce the number of robustness evaluations by the correlation analysis, which can incorporate similar evaluations and decrease the evaluation cost. Finally, we propose a multi-fidelity online surrogate during optimization to further decrease the search cost. On the basis of the surrogate constructed by low-fidelity data, the online high-fidelity data is utilized to finetune the surrogate. Experiments on CIFAR10 and CIFAR100 datasets show the effectiveness of our proposed method.},
  archive      = {J_PR},
  author       = {Jialiang Sun and Wen Yao and Tingsong Jiang and Xiaoqian Chen},
  doi          = {10.1016/j.patcog.2023.110038},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110038},
  shortjournal = {Pattern Recognition},
  title        = {Efficient search of comprehensively robust neural architectures via multi-fidelity evaluation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A topic modeling and image classification framework: The
generalized dirichlet variational autoencoder. <em>PR</em>,
<em>146</em>, 110037. (<a
href="https://doi.org/10.1016/j.patcog.2023.110037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Dirichlet allocation model (LDA) has been widely used in topic modeling . Recent works have shown the effectiveness of integrating neural network mechanisms with this generative model for learning text representation. However, one of the significant setbacks of LDA is that it is based on a Dirichlet prior that has a restrictive covariance structure. All its variables are considered to be negatively correlated, which makes the model restrictive. In a practical sense, topics can be positively or negatively correlated. To address this problem, we proposed a generalized Dirichlet variational autoencoder (GD-VAE) for topic modeling. The Generalized Dirichlet (GD) distribution has a more general covariance structure than the Dirichlet distribution because it takes into account both positively and negatively correlated topics in the corpus. Our proposed model leverages rejection sampling variational inference using a reparameterization trick for effective training. GD-VAE compares favorably to recent works on topic models on several benchmark corpora. Experiments show that accounting for topics’ positive and negative correlations results in better performance. We further validate the superiority of our proposed framework on two image data sets. GD-VAE demonstrates its significance as an integral part of a classification architecture. For reproducibility and further research purposes, code for this work can be found at https://github.com/hormone03/GD-VAE .},
  archive      = {J_PR},
  author       = {Akinlolu Oluwabusayo Ojo and Nizar Bouguila},
  doi          = {10.1016/j.patcog.2023.110037},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110037},
  shortjournal = {Pattern Recognition},
  title        = {A topic modeling and image classification framework: The generalized dirichlet variational autoencoder},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Beyond k-means++: Towards better cluster exploration with
geometrical information. <em>PR</em>, <em>146</em>, 110036. (<a
href="https://doi.org/10.1016/j.patcog.2023.110036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although k k -means and its variants are known for their remarkable efficiency, they suffer from a strong dependence on the prior knowledge of K K and the assumption of a circle-like pattern, which can result in the algorithms dividing the input space instead of discovering non-predetermined data patterns. Thus, we propose beyond k k -means++ that infers and utilizes explicit clusters by emphasizing local geometrical information for better cluster exploration. To avoid the K K dependence, a novel framework of iterative division and aggregation (IDA) over k k -means++ is presented. It begins with any K ≥ 1 K≥1 , then increases and reduces K K along with the procedure of clusters’ division and aggregation, respectively. To break through the circle-like pattern limitation, we introduce a reasonability checking strategy (RCS) for cluster division. Given local geometrical information, RCS achieves arbitrary cluster shape support by rejecting edge patterns with distinguished convergence direction and merging adjacent clusters with pseudo-edge patterns. Furthermore, we design an edge shrinkage strategy (ESS). Taking edge patterns as the cluster prototype , it benefits accuracy by effectively avoiding representability reduction due to irregular distribution. To compensate for the loss of efficiency, a near maximin and random sampling algorithm is suggested for large-scale data with high dimensionality . Experimental results confirm that beyond k k -means++ is featured by handling arbitrary cluster shapes with remarkable accuracy.},
  archive      = {J_PR},
  author       = {Yuan Ping and Huina Li and Bin Hao and Chun Guo and Baocang Wang},
  doi          = {10.1016/j.patcog.2023.110036},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110036},
  shortjournal = {Pattern Recognition},
  title        = {Beyond k-means++: Towards better cluster exploration with geometrical information},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Traffic sign attack via pinpoint region probability
estimation network. <em>PR</em>, <em>146</em>, 110035. (<a
href="https://doi.org/10.1016/j.patcog.2023.110035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work show that Deep Neural Networks (DNNs) have created great performance in many tasks, but they are vulnerable to adversarial examples which trigger Artificial Intelligence (AI) security risks. Especially in the autonomous driving field, attacking a traffic sign classification network results in a serious consequence. Most existing researches prefer to digital level attacks focusing on smaller or more imperceptible adversarial noise. Given that attacks available to real-world implementation usually emerge in more security-critical scenarios, we propose an adaptively adversarial example generation algorithm for physical attacks in the real-world setting. Taking account of the traffic sign classification, our approach is divided into two steps. The first step is to generate a probability map which precisely predicts the probability of being attacked for each pixel in the input image through the proposed Pinpoint Region Probability Estimation Network (PRPEN) and meanwhile, try to reduce the size of the highlighted area in the map. It can also be regarded as a classification problem in which every pixel has two classes, suitable for attacking or not, including the restrictions on the number of items in the suitable sort. The second one is to remake a mask depending on the probability map and optimize adversarial patches only on what mask decides. Experimental results show that our method achieves almost 100% misclassification rate in several widely used networks with even smaller patches. We also find how to effectively disguise as a target class to mislead the DNN classifiers and improve AI security.},
  archive      = {J_PR},
  author       = {Yue Wang and Minjie Liu and Yanli Ren and Xinpeng Zhang and Guorui Feng},
  doi          = {10.1016/j.patcog.2023.110035},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110035},
  shortjournal = {Pattern Recognition},
  title        = {Traffic sign attack via pinpoint region probability estimation network},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial networks via a composite annealing of
noise and diffusion. <em>PR</em>, <em>146</em>, 110034. (<a
href="https://doi.org/10.1016/j.patcog.2023.110034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network (GAN) is a framework for generating fake data using a set of real examples. However, GAN is unstable in the training stage. In order to stabilize GANs, the noise injection has been used to enlarge the overlap of the real and fake distributions at the cost of increasing variance. The diffusion process (or data smoothing in its spatial domain) removes fine details in order to capture the structure and important patterns in data but it suppresses the capability of GANs to learn high-frequency information in the training procedure. Based on these observations, we propose a data representation for the GAN training, called noisy scale-space (NSS), that recursively applies the smoothing with a balanced noise to data in order to replace the high-frequency information by random data, leading to a coarse-to-fine training of GANs. We experiment with NSS using DCGAN and StyleGAN2 based on benchmark datasets in which the NSS-based GANs outperforms the state-of-the-arts in most cases.},
  archive      = {J_PR},
  author       = {Kensuke Nakamura and Simon Korman and Byung-Woo Hong},
  doi          = {10.1016/j.patcog.2023.110034},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110034},
  shortjournal = {Pattern Recognition},
  title        = {Generative adversarial networks via a composite annealing of noise and diffusion},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature incremental learning with causality. <em>PR</em>,
<em>146</em>, 110033. (<a
href="https://doi.org/10.1016/j.patcog.2023.110033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the emerging of new data collection ways, the features are incremental and accumulated gradually. Due to the expansion of feature spaces , it is more common that there are unknown biases between the distribution of training and testing datasets . It is known as the unknown data selection bias, which belongs to the learning scenario with non-i.i.d samples. The performance of traditional approaches, which need the i.i.d. assumption, will be aggravated seriously. How to design an algorithm to address the problem of data selection bias in this feature incremental scenario is crucial but rarely studied. In this paper, we propose a feature incremental classification algorithm with causality. Firstly, we embed the confounding variable balance algorithm in causal learning into the prediction modeling and utilize the logical regression algorithm with balancing regular terms as a baseline. Then, to satisfy the special requirement of feature increment, we design a new regularizer, which maintains the consistency of the regression coefficients between the data in the current and previous stages. It retains the correlation between the old features and labels. Finally, we propose the Multiple Balancing Logistic Regression model (MBRLR) to jointly optimize the balancing regularizer and weighted logistic regression model with multiple feature sets. We also present theoretical results to show that our proposed algorithm can make precise and stable predictions. Besides, the numerical results also demonstrate that our MBRLR algorithm is superior to other methods.},
  archive      = {J_PR},
  author       = {Haotian Ni and Shilin Gu and Ruidong Fan and Chenping Hou},
  doi          = {10.1016/j.patcog.2023.110033},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110033},
  shortjournal = {Pattern Recognition},
  title        = {Feature incremental learning with causality},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive clustering with a graph consistency constraint.
<em>PR</em>, <em>146</em>, 110032. (<a
href="https://doi.org/10.1016/j.patcog.2023.110032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with classical contrastive learning methods, the performance of contrastive clustering is more easily affected by the quality of positive and negative samples, due to the fact that the clustering assumption requires neighbors of points as their positives. In order to reduce the effect of the uncertainty of positives and negatives on contrastive clustering, we propose a new contrastive clustering algorithm with graph consistency constraint. In this algorithm, a loss of graph consistency is proposed to reduce the impact of false negative samples by comparing the neighbor distribution of positive samples. Furthermore, an incremental training method is designed to control the quality of the selected positives. Extensive experiments show that our algorithm outperforms other deep clustering methods on wide-used benchmark data sets.},
  archive      = {J_PR},
  author       = {Yunxiao Zhao and Liang Bai},
  doi          = {10.1016/j.patcog.2023.110032},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110032},
  shortjournal = {Pattern Recognition},
  title        = {Contrastive clustering with a graph consistency constraint},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive local principal component analysis improves the
clustering of high-dimensional data. <em>PR</em>, <em>146</em>, 110030.
(<a href="https://doi.org/10.1016/j.patcog.2023.110030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In local Principal Component Analysis (PCA), a distribution is approximated by multiple units, each representing a local region by a hyper-ellipsoid obtained through PCA. We present an extension for local PCA which adaptively adjusts both the learning rate of each unit and the potential function which guides the competition between the local units. Our local PCA method is an online neural network method where unit centers and shapes are modified after the presentation of each data point. For several benchmark distributions, we demonstrate that our method improves the overall quality of clustering, especially for high-dimensional distributions where many conventional methods do not perform satisfactorily. Our online method is also well suited for the processing of streaming data: The two adaptive mechanisms lead to a quick reorganization of the clustering when the underlying distribution changes.},
  archive      = {J_PR},
  author       = {Nico Migenda and Ralf Möller and Wolfram Schenck},
  doi          = {10.1016/j.patcog.2023.110030},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110030},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive local principal component analysis improves the clustering of high-dimensional data},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization via inter-domain alignment and
intra-domain expansion. <em>PR</em>, <em>146</em>, 110029. (<a
href="https://doi.org/10.1016/j.patcog.2023.110029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of traditional deep learning models tends to drop dramatically during being deployed in real-world scenarios when the distribution shift between the seen training and unseen test data occurs. Domain Generalization methods are designed to achieve generalizability to deal with the above issue. Since the features extracted by softmax cross-entropy loss are not adequately domain-invariant, previous works in Domain Generalization have attempted to overcome this problem by employing contrastive-based losses which pull positive pairs ( i.e. , samples with the same class label) from different domains closer. Unfortunately, these approaches tend to produce an extremely small feature space , which is not robust facing unseen domain and easily overfits to source domains. To address the aforementioned issue, we propose a novel loss named IAIE Loss to simultaneously perform I nter-domain A lignment and I ntra-domain E xpansion for positive pairs, which facilitates the model to extract domain-invariant features and mitigates overfitting. Specifically, we design two sets of positive samples named “easy positive samples” and “hard positive samples”. IAIE Loss pulls the hard positive pairs closer (alignment) while pushing the easy positive pairs apart (expansion). The state-of-the-art results on multiple DG benchmark datasets verify the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Jiajun Hu and Lei Qi and Jian Zhang and Yinghuan Shi},
  doi          = {10.1016/j.patcog.2023.110029},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110029},
  shortjournal = {Pattern Recognition},
  title        = {Domain generalization via inter-domain alignment and intra-domain expansion},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep orientated distance-transform network for
geometric-aware centerline detection. <em>PR</em>, <em>146</em>, 110028.
(<a href="https://doi.org/10.1016/j.patcog.2023.110028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of structure centerlines from imaging data plays a crucial role in the understanding, application and further analysis of many diverse problems, such as road mapping, crack detection, medical imaging and biometric identification. In each of these cases, pixel-wise segmentation is not sufficient to understand and quantify overall graph structure and connectivity without further processing that can lead to compound error. We thus require a method for automatic extraction of graph representations of patterning. In this paper, we propose a novel Deep Orientated Distance-transform Network (DODN), which predicts the centerline map and an orientated distance map, comprising orientation and distance in relation to the centerline and allowing exploitation of its geometric properties. This is refined by jointly modeling the relationship between neighboring pixels and connectivity to further enhance the estimated centerline and produce a graph of the structure. The proposed approach is evaluated on a diverse range of problems, including crack detection, road mapping and superficial vein centerline detection from infrared/ color images, improving over the state-of-the-art by 2.1%, 10.9% and 17.3%/ 4.6% respectively in terms of quality, demonstrating its generalizability and performance in a wide range of mapping problems.},
  archive      = {J_PR},
  author       = {Zheheng Jiang and Hossein Rahmani and Plamen Angelov and Ritesh Vyas and Huiyu Zhou and Sue Black and Bryan Williams},
  doi          = {10.1016/j.patcog.2023.110028},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110028},
  shortjournal = {Pattern Recognition},
  title        = {Deep orientated distance-transform network for geometric-aware centerline detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A segmentation method based on the deep fuzzy segmentation
model in combined with SCANDLE clustering. <em>PR</em>, <em>146</em>,
110027. (<a href="https://doi.org/10.1016/j.patcog.2023.110027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the low clustering accuracy of the fuzzy clustering segmentation algorithm for analyzing high spatial resolution remote sensing images (HSRRSIs), a deep fuzzy segmentation model (DFSM)combined with Spectral Clustering with Adaptive Neighbors for Deep Learning (SCANDLE) clustering is proposed. The DFSM is used to over-segment the image, and the automatic coding structure is used to adaptively fuse the image features, minimizing the internal compactness and maximizing the external separability of the clustering, yielding better results. Meanwhile, the SCANDLE clustering model is used to cluster the over-segmentation results, and the matrix construction algorithm for adaptive neighborhood allocation is used to map the frame of the connected layer and optimally combine the over-segmentation images to realize the final segmentation results. The new method can accurately segment HSRRSIs with good segmentation performance.},
  archive      = {J_PR},
  author       = {Zenan Yang and Haipeng Niu and Xiaoxuan Wang and Liangxin Fan},
  doi          = {10.1016/j.patcog.2023.110027},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110027},
  shortjournal = {Pattern Recognition},
  title        = {A segmentation method based on the deep fuzzy segmentation model in combined with SCANDLE clustering},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Customized meta-dataset for automatic classifier accuracy
evaluation. <em>PR</em>, <em>146</em>, 110026. (<a
href="https://doi.org/10.1016/j.patcog.2023.110026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic classifier accuracy evaluation (ACAEval) on unlabeled test sets is critical for unseen real-world environments. The use of dataset-level regression on synthesized meta-datasets (comprised of many sample sets) has shown promising results for ACAEval. However, the existing meta-dataset for ACAEval is created using simple image transformations such as rotation and background substitution, which can make it difficult to ensure a reasonable distribution shift between the sample set and the test set. When the distribution shift is large, it becomes challenging to estimate the classifier accuracy on the test set using those sample sets. To ensure more robust ACAEval, this paper attempts to customize a meta-dataset in which each sample set has a reasonable distribution shift to the test set. An intra-class cycle-consistent adversarial learning (ICAL) method is introduced to transfer the style of a labeled training set to the style of the test set, by jointly considering the domain shift issue, the label flipping issue (the semantic information may be changed after style transformation), and the diversity of multiple sample sets in the meta-dataset. Experiments validate that under the same experimental setup, our method outperforms the existing ACAEval methods by a good margin, and achieves state-of-the-art performance on several standard benchmark datasets, including digit classification and natural image classification .},
  archive      = {J_PR},
  author       = {Yan Huang and Zhang Zhang and Yan Huang and Qiang Wu and Han Huang and Yi Zhong and Liang Wang},
  doi          = {10.1016/j.patcog.2023.110026},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110026},
  shortjournal = {Pattern Recognition},
  title        = {Customized meta-dataset for automatic classifier accuracy evaluation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-regularization self-supervised retinex approach to
low-light image enhancement with parameterized illumination estimation.
<em>PR</em>, <em>146</em>, 110025. (<a
href="https://doi.org/10.1016/j.patcog.2023.110025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current Retinex-based low-light image enhancement (LLIE) methods, fine-tuning regularization parameters for Retinex decomposition and illumination estimation can be cumbersome. To address this, we present a novel non-regularization self-supervised Retinex approach for illumination estimation. Our contributions are twofold: First, we introduce a self-supervised method that incorporates edge-aware smoothness properties in bilateral learning, eliminating the need for regularization terms and simplifying parameter adjustments. Second, to enforce smoothness constraints on the estimated bilateral grid, we propose a bilateral grid parameterization network. This network employs a generative encoder to parameterize the bilateral grid of illumination and a trainable slicing layer guided by a map, reconstructing the grid into an illumination map. Despite the absence of regularization terms , our model excels in generating piece-wise smooth illumination, resulting in enhanced naturalness and improved contrast in images. Our model offers exceptional flexibility by eliminating the need for additional regularization terms and parameter fine-tuning. Moreover, it does not depend on external datasets for training, overcoming dataset collection challenges. Extensive experiments, comparing our model with eight state-of-the-art methods across five public available datasets, unequivocally demonstrate our model’s state-of-the-art performance based on key metrics such as NIQE, NIQMC, and CPCQI. These results reaffirm the effectiveness of our approach in low-light image enhancement. Code will be available at: https://github.com/zhaozunjin/NeurBR .},
  archive      = {J_PR},
  author       = {Zunjin Zhao and Hexiu Lin and Daming Shi and Guoqing Zhou},
  doi          = {10.1016/j.patcog.2023.110025},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110025},
  shortjournal = {Pattern Recognition},
  title        = {A non-regularization self-supervised retinex approach to low-light image enhancement with parameterized illumination estimation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). L0 regularized logistic regression for large-scale data.
<em>PR</em>, <em>146</em>, 110024. (<a
href="https://doi.org/10.1016/j.patcog.2023.110024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate L 0 L0 -regularized logistic regression models , and design two fast and efficient algorithms for high-dimensional correlated data and massive data, respectively. Our first algorithm, the Variable Sorted Active Set (VSAS) algorithm, is based on the local quadratic approximation of the KKT conditions for L 0 L0 -penalized maximum log-likelihood function in high-dimensional correlated data. We establish an L ∞ L∞ error upper bound for the estimator obtained by the VSAS algorithm and prove its optimal convergence rate. Moreover, when the target signal exceeds the detectable level, the estimator obtained by the VSAS algorithm can achieve the oracle estimator with high probability. Our second algorithm, Communication Effective Variable Sorted Active Set (CEVSAS), aims to solve high-dimensional and large-sample L 0 L0 -regularized logistic regression models by reduce computational and communication costs, while maintaining estimation efficiency. Finally, simulations and real data demonstrate the effectiveness of our proposed VSAS and CEVSAS algorithms.},
  archive      = {J_PR},
  author       = {Hao Ming and Hu Yang},
  doi          = {10.1016/j.patcog.2023.110024},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110024},
  shortjournal = {Pattern Recognition},
  title        = {L0 regularized logistic regression for large-scale data},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Inter-domain mixup for semi-supervised domain adaptation.
<em>PR</em>, <em>146</em>, 110023. (<a
href="https://doi.org/10.1016/j.patcog.2023.110023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised domain adaptation (SSDA) aims to bridge source and target domain distributions, with a small number of target labels available, achieving better classification performance than unsupervised domain adaptation (UDA). However, existing SSDA work fails to make full use of label information from both source and target domains for feature alignment across domains, resulting in label mismatch in the label space during model testing. This paper presents a novel SSDA approach, Inter-domain Mixup with Neighborhood Expansion (IDMNE), to tackle this issue. Firstly, we introduce a cross-domain feature alignment strategy, Inter-domain Mixup, that incorporates label information into model adaptation. Specifically, we employ sample-level and manifold-level data mixing to generate compatible training samples . These newly established samples, combined with reliable and actual label information, display diversity and compatibility across domains, while such extra supervision thus facilitates cross-domain feature alignment and mitigates label mismatch. Additionally, we utilize Neighborhood Expansion to leverage high-confidence pseudo-labeled samples in the target domain, diversifying the label information of the target domain and thereby further increasing the performance of the adaptation model. Accordingly, the proposed approach outperforms existing state-of-the-art methods, achieving significant accuracy improvements on popular SSDA benchmarks, including DomainNet, Office-Home, and Office-31.},
  archive      = {J_PR},
  author       = {Jichang Li and Guanbin Li and Yizhou Yu},
  doi          = {10.1016/j.patcog.2023.110023},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110023},
  shortjournal = {Pattern Recognition},
  title        = {Inter-domain mixup for semi-supervised domain adaptation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised class-conditional image synthesis with
semantics-guided adaptive feature transforms. <em>PR</em>, <em>146</em>,
110022. (<a href="https://doi.org/10.1016/j.patcog.2023.110022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GANs) have become the mainstream models for class-conditional synthesis of high-fidelity images. To reduce the demand for labeled data, we propose a class-conditional GAN with Semantic-guided Adaptive Feature Transforms, which is referred to as SAFT-GAN for semi-supervised image synthesis . Instead of simply incorporating a classifier to infer the class labels of unlabeled data , the key idea behind SAFT-GAN is to incorporate class-semantic guidance in real-fake discrimination. More specifically, we adopt a two-head architecture for a discriminator : A label-embedded head identifies real and fake instances, conditioned on class label. To focus more on class-related regions, we exploit class-aware attention information to regularize this head via regional feature transforms. On the other hand, to make better use of unlabeled data, we design a label-free head, on which channel-adaptive feature transforms are imposed to fuse the discriminator and classifier features, such that the class semantics of synthesized images can be improved. Extensive experiments are performed to demonstrate how class-conditional image synthesis can benefit from the proposed feature transforms, and also demonstrate the superiority of SAFT-GAN.},
  archive      = {J_PR},
  author       = {Xiaoyang Huo and Yunfei Zhang and Si Wu},
  doi          = {10.1016/j.patcog.2023.110022},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110022},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised class-conditional image synthesis with semantics-guided adaptive feature transforms},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel disentangling network for human–object interaction
detection. <em>PR</em>, <em>146</em>, 110021. (<a
href="https://doi.org/10.1016/j.patcog.2023.110021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–object interaction (HOI) detection aims to localize and classify triplets of human, object and interaction from a given image. Earlier two-stage methods suffer both from mutually independent training processes and the interference of redundant negative human–object pairs. Prevailing one-stage transformer-based methods are free from the above problems by tackling HOI in an end-to-end manner. However, one-stage transformer-based methods carry the unnecessary entanglements of the query for different tasks, i.e., human–object detection and interaction classification, and thus bring in poor performance. In this paper, we propose a new transformer-based approach that parallelly disentangles human–object detection and interaction classification in a triplet-wise manner. To make each query focus on one specific task clearly, we exhaustively disentangle HOI by parallelly expanding the naive query in vanilla transformer as triple explicit queries. Then, we introduce a semantic communication layer to preserve the consistent semantic association of each HOI through mixing the feature representations of each query triplet of the correspondence constraint. Extensive experiments demonstrate that our proposed framework outperforms the existing methods and achieves the state-of-the-art performance, with significant reduction in parameters and FLOPs.},
  archive      = {J_PR},
  author       = {Yamin Cheng and Hancong Duan and Chen Wang and Zhijun Chen},
  doi          = {10.1016/j.patcog.2023.110021},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110021},
  shortjournal = {Pattern Recognition},
  title        = {Parallel disentangling network for human–object interaction detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised medical image segmentation via hard
positives oriented contrastive learning. <em>PR</em>, <em>146</em>,
110020. (<a href="https://doi.org/10.1016/j.patcog.2023.110020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) has been a popular technique to resolve the annotation scarcity problem in pattern recognition and medical image segmentation , which usually focuses on two critical issues: 1) learning a well-structured categorizable embedding space, and 2) establishing a robust mapping from the embedding space to the pixel space. In this paper, to resolve the first issue, we propose a h ard p ositives oriented c ontrastive (HPC) learning strategy to pre-train an encoder-decoder-based segmentation model. Different from vanilla contrastive learning tending to focus only on hard negatives, our HPC learning strategy additionally concentrates on hard positives (i.e., samples with the same category but dissimilar feature representations to the anchor), which are considered to play an even more crucial role in delivering discriminative knowledge for semi-supervised medical image segmentation. Specifically, the HPC is constructed from two levels, including an unsupervised image-level HPC (IHPC) and a supervised pixel-level HPC (PHPC), empowering the embedding space learned by the encoder with both local and global senses. Particularly, the PHPC learning strategy is implemented in a region-based manner, saving memory usage while delivering more multi-granularity information. In response to the second issue, we insert several feature swap (FS) modules into the pre-trained decoder. These FS modules aim to perturb the mapping from the intermediate embedding space towards the pixel space, trying to encourage more robust segmentation predictions. Experiments on two public clinical datasets demonstrate that our proposed framework surpasses the state-of-the-art methods by a large margin. Source codes are available at https://github.com/PerPerZXY/BHPC .},
  archive      = {J_PR},
  author       = {Cheng Tang and Xinyi Zeng and Luping Zhou and Qizheng Zhou and Peng Wang and Xi Wu and Hongping Ren and Jiliu Zhou and Yan Wang},
  doi          = {10.1016/j.patcog.2023.110020},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110020},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised medical image segmentation via hard positives oriented contrastive learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coarse-to-fine online latent representations matching for
one-stage domain adaptive semantic segmentation. <em>PR</em>,
<em>146</em>, 110019. (<a
href="https://doi.org/10.1016/j.patcog.2023.110019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive semantic segmentation is meaningful since collecting numerous labeled samples in different domains is expensive and time-consuming. Recent domain adaptation methods yield not so efficient performance compared with supervised learning. With the hypothesis that semantic feature can be shared across domains, this paper proposes a coarse-to-fine online matching architecture (COM) for one-stage domain adaptation. We consider subsequent learning stages progressively refining the task in the latent feature space , i.e. the finer set at each component is hierarchically derived from the coarser set of the previous components, including cross-domain global prototypes, categories and instances matching and anchor-points contrastive learning , which further achieve self-supervised learning with region-level pseudo label generated only in a single training step. Beforehand, feature refinement are performed to realize edge perception and inter-feature augmentation. Then, coarse-to-fine network fuses global and local consistency matching via specific distribution alignment between the source and target domain. Finally, the adversarial structure controls the uncertainty of generator prediction through the maximization of classification results and minimization of two classifiers discrepancy. This proposed method is evaluated in two unsupervised domain adaptation tasks, i.e. GTA5 → → Cityscapes and SYNTHIA → → Cityscapes. Extensive experiments verify the effectiveness of our proposed COM model and demonstrate its superiority over several state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Zihao Dong and Sijie Niu and Xizhan Gao and Xiuli Shao},
  doi          = {10.1016/j.patcog.2023.110019},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110019},
  shortjournal = {Pattern Recognition},
  title        = {Coarse-to-fine online latent representations matching for one-stage domain adaptive semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convolutional neural networks rarely learn shape for
semantic segmentation. <em>PR</em>, <em>146</em>, 110018. (<a
href="https://doi.org/10.1016/j.patcog.2023.110018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape learning, or the ability to leverage shape information, could be a desirable property of convolutional neural networks (CNNs) when target objects have specific shapes. While some research on the topic is emerging, there is no systematic study to conclusively determine whether and under what circumstances CNNs learn shape. Here, we present such a study in the context of segmentation networks where shapes are particularly important. We define shape and propose a new behavioral metric to measure the extent to which a CNN utilizes shape information. We then execute a set of experiments with synthetic and real-world data to progressively uncover under which circumstances CNNs learn shape and what can be done to encourage such behavior. We conclude that (i) CNNs do not learn shape in typical settings but rather rely on other features available to identify the objects of interest, (ii) CNNs can learn shape, but only if the shape is the only feature available to identify the object, (iii) sufficiently large receptive field size relative to the size of target objects is necessary for shape learning; (iv) a limited set of augmentations can encourage shape learning; (v) learning shape is indeed useful in the presence of out-of-distribution data.},
  archive      = {J_PR},
  author       = {Yixin Zhang and Maciej A. Mazurowski},
  doi          = {10.1016/j.patcog.2023.110018},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110018},
  shortjournal = {Pattern Recognition},
  title        = {Convolutional neural networks rarely learn shape for semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FoodMask: Real-time food instance counting, segmentation and
recognition. <em>PR</em>, <em>146</em>, 110017. (<a
href="https://doi.org/10.1016/j.patcog.2023.110017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food computing has long been studied and deployed to several applications. Understanding a food image at the instance level, including recognition, counting and segmentation, is essential to quantifying nutrition and calorie consumption. Nevertheless, existing techniques are limited to either category-specific instance detection, which does not reflect precisely the instance size at the pixel level , or category-agnostic instance segmentation , which is insufficient for dish recognition. This paper presents a compact and fast multi-task network, namely FoodMask, for clustering-based food instance counting, segmentation and recognition. The network learns a semantic space simultaneously encoding food category distribution and instance height at pixel basis . While the former value addresses instance recognition, the latter value provides prior knowledge for instance extraction. Besides, we integrate into the semantic space a pathway for class-specific counting. With these three outputs, we propose a clustering algorithm to segment and recognize food instances at a real-time speed. Empirical studies are made on three large-scale food datasets, including Mixed Dishes, UECFoodPixComp and FoodSeg103, which cover Western, Chinese, Japanese and Indian cuisines. The proposed networks outperform benchmarks in both terms of instance map quality and speed efficiency.},
  archive      = {J_PR},
  author       = {Huu-Thanh Nguyen and Yu Cao and Chong-Wah Ngo and Wing-Kwong Chan},
  doi          = {10.1016/j.patcog.2023.110017},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110017},
  shortjournal = {Pattern Recognition},
  title        = {FoodMask: Real-time food instance counting, segmentation and recognition},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-distillation and self-supervision for partial label
learning. <em>PR</em>, <em>146</em>, 110016. (<a
href="https://doi.org/10.1016/j.patcog.2023.110016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a main branch of weakly supervised learning paradigm, partial label learning (PLL) copes with the situation where each sample corresponds to ambiguous candidate labels containing the unknown true label. The primary difficulty of PLL lies in label ambiguities, most existing researches focus on individual instance knowledge while ignore the importance of cross-sample knowledge. To circumvent this difficulty, an innovative multi-task framework is proposed in this work to integrate self-supervision and self-distillation to tackle PLL problem. Specifically, in the self-distillation task, cross-sample knowledge in the same batch is utilized to refine ensembled soft targets to supervise the distillation operation without using multiple networks. The auxiliary self-supervised task of recognizing rotation transformations of images provides more supervisory signal for feature learning . Overall, training supervision is constructed not only from the input data itself but also from other instances within the same batch. Empirical results on benchmark datasets reveal that this method is effective in learning from partially labeled data.},
  archive      = {J_PR},
  author       = {Xiaotong Yu and Shiding Sun and Yingjie Tian},
  doi          = {10.1016/j.patcog.2023.110016},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110016},
  shortjournal = {Pattern Recognition},
  title        = {Self-distillation and self-supervision for partial label learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). GITGAN: Generative inter-subject transfer for EEG motor
imagery analysis. <em>PR</em>, <em>146</em>, 110015. (<a
href="https://doi.org/10.1016/j.patcog.2023.110015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) plays a crucial role in achieving subject-independent performance in Brain-Computer Interface (BCI). However, previous studies have primarily focused on developing intricate network architecture designs, neglecting the impact of source data quality and the challenges posed by the out-of-distribution target data problem. To address these limitations, we argue that a target data-centered space, augmented by a carefully selected set of high-quality source data, can significantly enhance DA. In this study, we present an unsupervised end-to-end subject adaptation approach called GITGAN, a generative inter-subject transfer for electroencephalography motor imagery analysis. We also propose a practical and effective method for selecting source data, which further enhances performance. Our approach is non-intrusive, as it does not modify the target data distribution , thus preserving its integrity for further numerical and visual analysis. Our extensive experiments with two different datasets demonstrate not only the superiority of our approach compared to existing methods, but also its phenomenal potential for practical BCI applications. The findings of this study provide valuable insights into the potential of BCI and illustrate the importance of considering source data quality in DA. The implementation is available at https://github.com/Kang1121/GITGAN .},
  archive      = {J_PR},
  author       = {Kang Yin and Elissa Yanting Lim and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2023.110015},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110015},
  shortjournal = {Pattern Recognition},
  title        = {GITGAN: Generative inter-subject transfer for EEG motor imagery analysis},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CS-GAC: Compressively sensed geodesic active contours.
<em>PR</em>, <em>146</em>, 110007. (<a
href="https://doi.org/10.1016/j.patcog.2023.110007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an edge based compressively sensed (CS) geodesic active contour (GAC) model, termed CS-GAC, to ensure faithful edge detection and accurate object segmentation. The motivation behind this paper is that edge information driving the contour evolution can be iteratively obtained by incomplete CS measurements. In each iteration, the CS-GAC is a three-step process including edge detection, active contouring and sparse reconstruction. Instead of working on the final reconstructed images themselves, the evolution of the CS-GAC is driven by a few CS measurements and guided by updatable edge information. The edge information is generated by a complex shearlet transform (CST) based edge map. In the framework, reconstruction and edge detection work alternately. The iterative update property that takes advantages of both edge sparsity and edge detection can largely improve the evolution precision. Numerical experiments show that the CS-GAC can obtain challenging segmentation results in comparisons with the state of the art methods , and has competitive prospects.},
  archive      = {J_PR},
  author       = {Hao Shan},
  doi          = {10.1016/j.patcog.2023.110007},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110007},
  shortjournal = {Pattern Recognition},
  title        = {CS-GAC: Compressively sensed geodesic active contours},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted side-window based gradient guided image filtering.
<em>PR</em>, <em>146</em>, 110006. (<a
href="https://doi.org/10.1016/j.patcog.2023.110006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image filtering under guidance image, known as guided filtering (GF), has been successfully applied to a variety of applications. Existing GF methods utilize either conventional full window-based framework (FWF) or simple uniformly weighted aggregation strategy (UWA); thereby they suffer from edge-blurring. In this paper, based upon gradient guided filtering (GGF), a weighted side-window based gradient guided filtering (WSGGF) is proposed to address the aforementioned problem. First, both regression and adaptive regularization terms in GGF are improved upon eight side windows by introducing side window-based framework (SWF). L 1 L1 norm is adopted to choose the results calculated in side windows. Second, UWA strategy in GGF is replaced by a refined variance-based weighted average (VWA) aggregation. In VWA, the value of each weight is chosen inversely proportional to the corresponding estimator. We show that with these improvements our method can well retain the edge sharpness and is robust to visual artifacts. To cut down the time consumption, a fast version of WSGGF (FWSGGF) is further proposed by incorporating a simple but effective down-sampling strategy, which is about four times faster while maintaining the superior performance. By comparing with the state-of-the-art (SOTA) methods on edge-aware smoothing, detail enhancement, high dynamic range image (HDR) compression, image luminance adjustment, depth map upsampling and single image haze removal, the effectiveness and flexibility of our proposed methods are verified. The source code is available at: https://github.com/weimin581/WSGGF},
  archive      = {J_PR},
  author       = {Weimin Yuan and Cai Meng and Xiangzhi Bai},
  doi          = {10.1016/j.patcog.2023.110006},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110006},
  shortjournal = {Pattern Recognition},
  title        = {Weighted side-window based gradient guided image filtering},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sparse local binary pattern extraction algorithm applied
to event sensor data for object classification. <em>PR</em>,
<em>146</em>, 110004. (<a
href="https://doi.org/10.1016/j.patcog.2023.110004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, new sensors with active pixels were brought to market. These sensors export local variations of light intensity in the form of asynchronous events with low latency. Since the data output format is a stream of addressable events and not a complete image of light intensities, new algorithms are required for known problems in the field of Computer Vision , such as segmentation, VO, SLAM, object, and scene recognition. There are some proposed methodologies for object recognition using conventional methods, convolutional neural networks , and third-generation neural networks based on spikes. However, convolutional neural networks and spike neural networks require specific hardware for processing, hard to miniaturize. Also, several traditional Computer Vision operators and feature descriptors have been neglected in the context of event sensors and could contribute to lighter methodologies in object recognition. This paper proposes an algorithm for local binary pattern extraction in sparse structures, typically found in this context. This paper also proposes two methodologies using local binary patterns to captures with event-based sensors for object recognition. The first methodology exploits the known motion performed by the sensor, while the second is motion agnostic. It is demonstrated experimentally that the LBP operator is a fast and light alternative that enables variable reduction using PCA in some cases. The experiments also show that it is possible to reduce the final feature vector for classification by up to 99 , 73 % 99,73% when compared to conventional methods considered state-of-the-art while maintaining comparable accuracy.},
  archive      = {J_PR},
  author       = {Fernando Azevedo Fardo and Paulo Sérgio Silva Rodrigues},
  doi          = {10.1016/j.patcog.2023.110004},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110004},
  shortjournal = {Pattern Recognition},
  title        = {A sparse local binary pattern extraction algorithm applied to event sensor data for object classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monte carlo DropBlock for modeling uncertainty in object
detection. <em>PR</em>, <em>146</em>, 110003. (<a
href="https://doi.org/10.1016/j.patcog.2023.110003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancements made in deep learning , computer vision problems have seen a great improvement in performance. However, in many real-world applications such as autonomous driving vehicles, the risk associated with incorrect predictions of objects or segmentation of images is very high. Standard deep learning models for object detection and segmentation such as YOLO models are often overconfident in their predictions and do not take into account the uncertainty in predictions on out-of-distribution data. In this work, we propose an efficient and effective approach, Monte-Carlo DropBlock (MC-DropBlock), to model uncertainty in YOLO and convolutional vision Transformers for object detection. The proposed approach applies drop-block during training time and testing time on the convolutional layer of the deep learning models such as YOLO and convolutional transformer. We theoretically show that this leads to a Bayesian convolutional neural network capable of capturing the epistemic uncertainty in the model. Additionally, we capture the aleatoric uncertainty in the data using a Gaussian likelihood. We demonstrate the effectiveness of the proposed approach on modeling uncertainty in object detection and segmentation tasks using out-of-distribution experiments. Experimental results show that MC-DropBlock improves the generalization, calibration, and uncertainty modeling capabilities of YOLO models and convolutional Transformer models for object detection and segmentation.},
  archive      = {J_PR},
  author       = {Sai Harsha Yelleni and Deepshikha Kumari and Srijith P.K. and Krishna Mohan C.},
  doi          = {10.1016/j.patcog.2023.110003},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110003},
  shortjournal = {Pattern Recognition},
  title        = {Monte carlo DropBlock for modeling uncertainty in object detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-light image enhancement using gamma correction prior in
mixed color spaces. <em>PR</em>, <em>146</em>, 110001. (<a
href="https://doi.org/10.1016/j.patcog.2023.110001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient and fast low-light image enhancement method using an atmospheric scattering model based on an inverted low-light image. The transmission map is derived as a function of two saturations of the original image in the two color spaces. Due to the difficulty in estimating the saturation of the original image, the transmission map is converted into a function of the average and maximum values of the original image. These two values are estimated from a given low-light image using the gamma correction prior. In addition, a pixel-adaptive gamma value determination algorithm is proposed to prevent under- or over-enhancement. The proposed algorithm is fast because it does not require the training or refinement process. The simulation results show that the proposed low-light enhancement scheme outperforms state-of-the-art approaches regarding both computational simplicity and enhancement efficiency. The code is available on https://github.com/TripleJ2543 .},
  archive      = {J_PR},
  author       = {Jong Ju Jeon and Jun Young Park and Il Kyu Eom},
  doi          = {10.1016/j.patcog.2023.110001},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110001},
  shortjournal = {Pattern Recognition},
  title        = {Low-light image enhancement using gamma correction prior in mixed color spaces},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic gradient reactivation for backward compatible person
re-identification. <em>PR</em>, <em>146</em>, 110000. (<a
href="https://doi.org/10.1016/j.patcog.2023.110000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the backward compatible problem for person re-identification (Re-ID), which aims to constrain the features of an updated new model to be comparable with the existing features from the old model in galleries. Most of the existing works adopt distillation-based methods, which focus on pushing new features to imitate the distribution of the old ones. However, the distillation-based methods are intrinsically sub-optimal since it forces the new feature space to imitate the inferior old feature space. To address this issue, we propose the Ranking-based Backward Compatible Learning (RBCL), which directly optimizes the ranking metric between new features and old features. Different from previous methods, RBCL only pushes the new features to find best-ranking positions in the old feature space instead of strictly alignment, and is in line with the ultimate goal of backward retrieval. However, the sharp sigmoid function used to make the ranking metric differentiable also incurs the gradient vanish issue, therefore stems the ranking refinement during the later period of training. To address this issue, we propose the Dynamic Gradient Reactivation (DGR), which can reactivate the suppressed gradients by adding dynamically computed constants during the forward step. To further help target the best-ranking positions, we include the Neighbor Context Agents (NCAs) to approximate the entire old feature space during training. Unlike previous works that mainly test on the in-domain settings, we make the early attempt to introduce the cross-domain settings (including both supervised and unsupervised) for the backward compatible person Re-ID task, which are more challenging yet meaningful. The experimental results on all five settings show that the proposed RBCL outperforms previous state-of-the-art methods by large margins.},
  archive      = {J_PR},
  author       = {Xiao Pan and Hao Luo and Weihua Chen and Fan Wang and Hao Li and Wei Jiang and Jianming Zhang and Jianyang Gu and Peike Li},
  doi          = {10.1016/j.patcog.2023.110000},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {110000},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic gradient reactivation for backward compatible person re-identification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MF-net: Multi-frequency intrusion detection network for
internet traffic data. <em>PR</em>, <em>146</em>, 109999. (<a
href="https://doi.org/10.1016/j.patcog.2023.109999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of Internet technology renders intrusion detection an important research topic in the field of pattern recognition. Considering that traffic data relate to not only temporal information, but also attack frequency , this paper presents a novel deep learning framework termed the multi-frequency intrusion detection network (MF-Net). MF-Net regards the pattern of Internet traffic as a superposition of sequential data with various frequencies, and is able to recognize the multi-frequency nature of network traffic data. The core of MF-Net is the multi-frequency LSTM (MF-LSTM) and multi-frequency transformer (MF-Transformer) module, both of which consist of high-frequency and low-frequency layers. In comparison with other state-of-the-art approaches on 4 public datasets, namely UNSW-NB15, KDD Cup 99, NSL-KDD and CICIDS 2017, as well as an IPv6 traffic dataset we created, MF-Net has shown better result in both binary and multi-class classification, which demonstrates the superiority of MF-Net over other compared approaches on network traffic intrusion detection.},
  archive      = {J_PR},
  author       = {Zhaoxu Ding and Guoqiang Zhong and Xianping Qin and Qingyang Li and Zhenlin Fan and Zhaoyang Deng and Xiao Ling and Wei Xiang},
  doi          = {10.1016/j.patcog.2023.109999},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109999},
  shortjournal = {Pattern Recognition},
  title        = {MF-net: Multi-frequency intrusion detection network for internet traffic data},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning disentangled representations for controllable human
motion prediction. <em>PR</em>, <em>146</em>, 109998. (<a
href="https://doi.org/10.1016/j.patcog.2023.109998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative model-based motion prediction techniques have recently realized predicting controlled human motions , such as predicting multiple upper human body motions with similar lower-body motions. However, to achieve this, the state-of-the-art methods require either subsequently learning mapping functions to seek similar motions or training the model repetitively to enable control over the desired portion of body. In this paper, we propose a novel framework to learn disentangled representations for controllable human motion prediction. Our task is to predict multiple future human motions based on the past observed sequence, with the control of partial-body movements. Our network involves a conditional variational auto-encoder (CVAE) architecture to model full-body human motion, and an extra CVAE path to learn only the corresponding partial-body (e.g., lower-body) motion. Specifically, the inductive bias imposed by the extra CVAE path encourages two latent variables in two paths to respectively govern separate representations for each partial-body motion. With a single training, our model is able to provide two types of controls for the generated human motions: (i) strictly controlling one portion of human body and (ii) adaptively controlling the other portion, by sampling from a pair of latent spaces. Additionally, we extend and adapt a sampling strategy to our trained model to diversify the controllable predictions. Our framework also potentially allows new forms of control by flexibly customizing the input for the extra CVAE path. Extensive experimental results and ablation studies demonstrate that our approach is capable of predicting state-of-the-art controllable human motions both qualitatively and quantitatively.},
  archive      = {J_PR},
  author       = {Chunzhi Gu and Jun Yu and Chao Zhang},
  doi          = {10.1016/j.patcog.2023.109998},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109998},
  shortjournal = {Pattern Recognition},
  title        = {Learning disentangled representations for controllable human motion prediction},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). MFAN: Mixing feature attention network for trajectory
prediction. <em>PR</em>, <em>146</em>, 109997. (<a
href="https://doi.org/10.1016/j.patcog.2023.109997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate trajectory prediction of surrounding agents is essential for autonomous vehicles, where the key challenge is to understand the complex interactions among agents. Previous works treat all interacted features between agents equally in modeling interaction, while neglecting their different importance to the interaction, thus inevitably limiting the interaction modeling ability. Besides, existing methods suffer from significant performance degradation when domain shifts, resulting in severely deviant prediction from reality. To address these issues, we propose a novel prediction framework, dubbed Mixing Feature Attention Network (MFAN). Specifically, the proposed mixing feature attention is a parallel design to adaptively determine the importance of different interacted features and simultaneously capture the global interaction feature to improve interaction modeling. Meanwhile, the spatial global interaction is modeled from a spatial edge-featured graph input to capture the enhanced spatial interaction. The temporal motion pattern is modeled from a temporal edge-featured graph input to enhance the domain adaption. Finally, we estimate the parameters of bivariant Gaussian distribution for trajectory prediction. Experimental results show that our method achieves superior performance in trajectory prediction while maintaining low computational complexity and performs accurate prediction even when domain shifts.},
  archive      = {J_PR},
  author       = {Jingzhong Li and Lin Yang and Yuxuan Chen and Yue Jin},
  doi          = {10.1016/j.patcog.2023.109997},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109997},
  shortjournal = {Pattern Recognition},
  title        = {MFAN: Mixing feature attention network for trajectory prediction},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast anchor graph preserving projections. <em>PR</em>,
<em>146</em>, 109996. (<a
href="https://doi.org/10.1016/j.patcog.2023.109996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing graph-based dimensionality reduction algorithms need to learn an adjacency matrix or construct it in advance, therefore the time complexity of the graph-based dimensionality reduction algorithms is not less than O ( n 2 d ) O(n2d) , where n n denotes the number of samples, d d denotes the number of dimensions. Moreover, the existing dimensionality reduction algorithms do not consider the cluster information in the original space, resulting in the weakening or even loss of valuable information after dimensionality reduction. To address the above problems, we propose Fast Anchor Graph Preserving Projections (FAGPP), which learns the projection matrix , the anchors and the membership matrix at the same time. Especially, FAGPP has a built-in Principal Component Analysis (PCA) item, which makes our model not only deal with the cluster information of data, but also deal with the global information of data. The time complexity of FAGPP is O ( n m d ) O(nmd) , where m m denotes the number of the anchors and m m is much less than n n . We propose a novel iterative algorithm to solve the proposed model and the convergence of the algorithm is proved theoretically. The experimental results on a large number of high-dimensional benchmark image data sets demonstrate the efficiency of FAGPP. The data sets and the source code are available from https://github.com/511lab/FAGPP .},
  archive      = {J_PR},
  author       = {Jikui Wang and Yiwen Wu and Bing Li and Zhenguo Yang and Feiping Nie},
  doi          = {10.1016/j.patcog.2023.109996},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109996},
  shortjournal = {Pattern Recognition},
  title        = {Fast anchor graph preserving projections},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative contrastive learning for hypergraph node
classification. <em>PR</em>, <em>146</em>, 109995. (<a
href="https://doi.org/10.1016/j.patcog.2023.109995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plenty of models have been presented to handle the hypergraph node classification . However, very few of these methods consider contrastive learning , which is popular due to its great power to represent instances. This paper makes an attempt to leverage contrastive learning to hypergraph representation learning . Specifically, we propose a novel method called Collaborative Contrastive Learning (CCL), which incorporates a generated standard graph with the hypergraph. The main technical contribution here is that we develop a collaborative contrastive schema, which performs contrast between the node views obtained from the standard graph and hypergraph in each network layer, thus making the contrast collaborative. To be precise, in the first layer, the view from the standard graph is used to augment that from the hypergraph. Then, in the next layer, the augmented features are adopted to train a new representation to augment the view from the standard graph conversely. With this setting, the learning procedure is alternated between the standard graph and hypergraph. As a result, the learning on the standard graph and hypergraph is collaborative and leads to the final informative node representation. Experimental results on several widely used datasets validate the effectiveness of the proposed model.},
  archive      = {J_PR},
  author       = {Hanrui Wu and Nuosi Li and Jia Zhang and Sentao Chen and Michael K. Ng and Jinyi Long},
  doi          = {10.1016/j.patcog.2023.109995},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109995},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative contrastive learning for hypergraph node classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIC-CNN: Rotation-invariant coordinate convolutional neural
network. <em>PR</em>, <em>146</em>, 109994. (<a
href="https://doi.org/10.1016/j.patcog.2023.109994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of rotation invariance in traditional convolution operations , even acting a slight rotation on the input can severely degrade the performance of Convolutional Neural Networks (CNNs). To address this, we propose a Rotation-Invariant Coordinate Convolution (RIC-C), which achieves natural invariance to arbitrary rotations around the input center without additional trainable parameters or data augmentation . We first evaluate the rotational invariance of RIC-C using the MNIST dataset and compare its performance with most previous rotation-invariant CNN models . RIC-C achieves state-of-the-art classification on the MNIST-rot test set without data augmentation and with lower computational costs. Then, the interchangeability of RIC-C with traditional convolution operations is demonstrated by seamlessly integrating it into common CNN models like VGG, ResNet , and DenseNet . We conduct remote sensing image classification on the NWPU VHR-10, MTARSI and AID datasets and patch matching experiments on the UBC benchmark dataset, showing that RIC-C significantly enhances the performance of CNN models across different applications, especially when training data is limited. Our codes can be downloaded from https://github.com/HanlinMo/Rotation-Invariant-Coordinate-Convolutional-Neural-Network.git .},
  archive      = {J_PR},
  author       = {Hanlin Mo and Guoying Zhao},
  doi          = {10.1016/j.patcog.2023.109994},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109994},
  shortjournal = {Pattern Recognition},
  title        = {RIC-CNN: Rotation-invariant coordinate convolutional neural network},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning foreground information bottleneck for few-shot
semantic segmentation. <em>PR</em>, <em>146</em>, 109993. (<a
href="https://doi.org/10.1016/j.patcog.2023.109993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation aims to segment unseen classes with only a few annotated samples, which has great values for the real-world application in the wild. However, since the target class is treated as the background in the training, the network tends to extract much irrelevant nuisance factors, which results in the feature undermining problem for the target class. Consequently, it is difficult to produce an accurate segmentation map . To address this problem, in this paper, we apply the information bottleneck theory to few-shot semantic segmentation and propose the Foreground Information Bottleneck (FIB) module. Based on the support information, FIB module filters out the irrelevant information and promotes the foreground-related feature paradigms. Meanwhile, to solve the intractable mutual information and enable the end-to-end optimization of FIB module, we derive the Foreground Information Bottleneck Loss (FIBLoss) according to the inherent attribute of few-shot segmentation. Moreover, since there exists severe noise interference in the wild, we design a Target Information Refinement (TIR) block to further exploit discriminative cues of foreground. TIR block calculates the pairwise interaction and exploits the detailed information of the foreground object , which is beneficial to the feature refinement. Extensive experiments on two challenging datasets reflect the proposed FIB module significantly improves the performance of few-shot segmentation and delivers the state-of-the-art results.},
  archive      = {J_PR},
  author       = {Yutao Hu and Xin Huang and Xiaoyan Luo and Jungong Han and Xianbin Cao and Jun Zhang},
  doi          = {10.1016/j.patcog.2023.109993},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109993},
  shortjournal = {Pattern Recognition},
  title        = {Learning foreground information bottleneck for few-shot semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matrix randomized autoencoder. <em>PR</em>, <em>146</em>,
109992. (<a href="https://doi.org/10.1016/j.patcog.2023.109992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized autoencoder (RAE) has attracted much attention due to its strong capability of representation with fast learning speed. However, the mainstream RAEs are still designed for scalar/vector data, which inevitably destroys the structure information of tensor data. To alleviate this deficiency, a novel convolutions based matrix randomized autoencoder (MRAE) is developed for two-dimensional (2D) data in this paper, including a one-side MRAE (OMRAE) exploiting the row or column information and a double-side MRAE (DMRAE) that simultaneously extracts the row and column information by 2 parallel OMRAEs. To reduce meaningless encoded features, the within-class scatter matrix (WSI) and within-class interaction distance (WID) constraints are added into OMRAE resulting WSI-OMRAE and WID-OMRAE, respectively. To demonstrate the superiority, stacked MRAEs are embedded into hierarchical regularized least squares for one-class classification and comparisons with several state-of-the-art methods are provided. The source code would be available at https://github.com/ML-HDU/MRAE .},
  archive      = {J_PR},
  author       = {Shichen Zhang and Tianlei Wang and Jiuwen Cao and Wandong Zhang and Badong Chen},
  doi          = {10.1016/j.patcog.2023.109992},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109992},
  shortjournal = {Pattern Recognition},
  title        = {Matrix randomized autoencoder},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards self-explainable graph convolutional neural network
with frequency adaptive inception. <em>PR</em>, <em>146</em>, 109991.
(<a href="https://doi.org/10.1016/j.patcog.2023.109991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional neural networks (GCNs) have demonstrated powerful representing ability of irregular data, e.g., skeletal data and graph-structured data, providing the effective mechanism to fuse the neighbor nodes. However, inheriting from the deep learning , GCN also lacks interpretability , which hinders its application to scenarios that have high demand for transparency. Although, there have been many efforts on the interpretability of deep learning , they mainly concentrate on i.i.d data that is hard to be deployed to GCNs, which involve not only the node feature, but also the graph structure. There are few works that attempt to explain it with post-hoc manner, which can be biased, resulting in mis-representation of the true explanation. Therefore, in this paper, we propose a framework, namely ExpFiGCN, that reveals explainability of the GCNs from the perspective of graph structure and mathematical analysis. Specifically, ExpFiGCN can find the most intrinsically relevant node to the central node and obtain the informative and discriminative signals while performing denoising . For the graph structure, we find K K -nearest nodes; for the mathematical analysis, every channel of a node and its neighborhoods contribute dynamically to the final channel signal, which can capture the inherent difference of different channels and neighbor nodes. Meanwhile, it can enhance the representation ability of nodes and ameliorate the over-smoothing problem. On the other hand, our model can dynamically adjust the importance of neighborhoods to the central vertex. We empirically validate the effectiveness of the proposed framework ExpFiGCN on various benchmark datasets. Experimental results show that our method achieves substantial improvements and outperforms the state-of-the-art performance strikingly.},
  archive      = {J_PR},
  author       = {Feifei Wei and Kuizhi Mei},
  doi          = {10.1016/j.patcog.2023.109991},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109991},
  shortjournal = {Pattern Recognition},
  title        = {Towards self-explainable graph convolutional neural network with frequency adaptive inception},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregated-attention deformable convolutional network for
few-shot SAR jamming recognition. <em>PR</em>, <em>146</em>, 109990. (<a
href="https://doi.org/10.1016/j.patcog.2023.109990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work simultaneously addresses the challenges of unseen classes and low-data problems on synthetic aperture radar jamming recognition (SAR-JR). Currently, very few studies have tackled both challenges. Inspired by the success of few-shot learning, which learns a robust model from a few instances, we formulate SAR-JR as a few-shot task in a metric-learning framework to alleviate the above challenges. Against the jamming features with significant dispersion and complex geometric transformations , as well as feature obscuration in time–frequency images (TF), we propose an aggregated-attention deformable convolutional network (A 2 2 -DCNet) framework consisting of an aggregated-attention deformable convolutional module (A 2 2 -DC-Module) and a prototype classification module based on polynomial loss (PolyLoss-PC-Module). The former learns informative and refined embeddings from the TF images, while the latter performs the SAR-JR in an embedding space by calculating distances to prototypes of each class. Specifically, the modulated deformable convolution of the A 2 2 -DC-Module can capture long-range spatial contextual information from a global perspective, while the aggregated attention is designed to refine the representations of obscured features in the TF images. To further optimize the framework, we introduce a novel PolyLoss and customize the optimal form for our model to learn an embedding space with robust inter-class separability. Finally, to realize few-shot SAR-JR tasks, we simulate a novel dataset called JamSet . Extensive experimental results on our dataset have demonstrated substantial improvement of our proposed A 2 2 -DCNet method over the benchmarks.},
  archive      = {J_PR},
  author       = {Jinbiao Du and Weiwei Fan and Chen Gong and Jun Liu and Feng Zhou},
  doi          = {10.1016/j.patcog.2023.109990},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109990},
  shortjournal = {Pattern Recognition},
  title        = {Aggregated-attention deformable convolutional network for few-shot SAR jamming recognition},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDB: Gated convolutions-based document binarization.
<em>PR</em>, <em>146</em>, 109989. (<a
href="https://doi.org/10.1016/j.patcog.2023.109989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document binarization is a crucial pre-processing step for various document analysis tasks. However, existing methods fail to accurately capture stroke edges, primarily due to the inherent limitations of vanilla convolutions and the absence of adequate boundary-related supervision during stroke edge extraction . In this paper, we formulate text extraction as the learning of gating values and propose an end-to-end network architecture based on gated convolutions, named GDB, to address the problem of imprecise stroke edge extraction. The gated convolutions enable the selective extraction of stroke feature with different attention. Our proposed framework comprises two stages. Firstly, a coarse sub-network with an extra edge branch is trained to enhance the precision of feature maps by incorporating a priori mask and edge information . Secondly, a refinement sub-network is cascaded to enhance the output of the first stage using gated convolutions based on the sharp edges. To effectively incorporate global information, GDB also integrates a parallelized multi-scale operation that combines local and global features. We conduct comprehensive experiments on ten Document Image Binarization Contest (DIBCO) datasets from 2009 to 2019 and Document Deblurring Datasets. Experimental results show that our proposed methods outperform the state-of-the-art methods across all metrics on average. Extensive ablation studys demonstrate the efficacy of key components. Available codes: https://github.com/Royalvice/GDB .},
  archive      = {J_PR},
  author       = {Zongyuan Yang and Baolin Liu and Yongping Xiong and Guibin Wu},
  doi          = {10.1016/j.patcog.2023.109989},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109989},
  shortjournal = {Pattern Recognition},
  title        = {GDB: Gated convolutions-based document binarization},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Controllable style transfer via test-time training of
implicit neural representation. <em>PR</em>, <em>146</em>, 109988. (<a
href="https://doi.org/10.1016/j.patcog.2023.109988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing CNN-based style transfer frameworks have suffered from inaccurate control of pixel-wise stylization. As the CNN operation is designed based on kernel-wise operation, such a design unavoidably makes pixels affect each other. To mitigate this problem, we propose a controllable style transfer framework that leverages Implicit Neural Representation to encode each pixel respectively and optimize each style and content pair via test-time training. Unlike previous CNN-based style transfer frameworks, this formulation naturally enables accurate pixel-wise stylization control. In addition, to give explicit controllability on the degree of stylization, we define two vectors that represent the content and style respectively, enabling control by interpolating these vectors. We further demonstrate that, after being test-time trained once, our framework can show a various range of applications by precisely controlling the stylized images pixel-wise and freely adjusting image resolution and degree of stylization without further optimization or training.},
  archive      = {J_PR},
  author       = {Sunwoo Kim and Youngjo Min and Younghun Jung and Seungryong Kim},
  doi          = {10.1016/j.patcog.2023.109988},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109988},
  shortjournal = {Pattern Recognition},
  title        = {Controllable style transfer via test-time training of implicit neural representation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast generalized ramp loss support vector machine for
pattern classification. <em>PR</em>, <em>146</em>, 109987. (<a
href="https://doi.org/10.1016/j.patcog.2023.109987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is widely recognized as an effective classification tool and has demonstrated superior performance in diverse applications. However, for large-scale pattern classification problems, it may require much memory and incur prohibitive computational costs. Motivated by this, we propose a new SVM model with novel generalized ramp loss ( L R LR -SVM). The first-order optimality conditions for the non-convex and non-smooth L R LR -SVM are developed by the newly developed P-stationary point, based on which, the L R LR support vectors and working set of L R LR -SVM are defined, interestingly, which shows that all of the L R LR support vectors are on the two support hyperplanes under mild conditions . A fast proximal alternating direction method of multipliers with working set ( L R LR - ADMM ) is developed to handle L R LR -SVM and L R LR - ADMM has been demonstrated to achieve global convergence while maintaining a significantly low computational complexity . Numerical comparisons with nine leading solvers show that L R LR - ADMM demonstrates outstanding performance, particularly when applied to large-scale pattern classification problems with fewer support vectors, higher prediction accuracy and shorter computational time .},
  archive      = {J_PR},
  author       = {Huajun Wang and Yuanhai Shao},
  doi          = {10.1016/j.patcog.2023.109987},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109987},
  shortjournal = {Pattern Recognition},
  title        = {Fast generalized ramp loss support vector machine for pattern classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information geometry based extreme low-bit neural network
for point cloud. <em>PR</em>, <em>146</em>, 109986. (<a
href="https://doi.org/10.1016/j.patcog.2023.109986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly advanced three-dimensional computer vision applied to point clouds. Nevertheless, the substantial consumption of time, storage, and energy substantially limits its deployment on edge devices with constrained resources. Extremely low bit quantization has received wide attention due to its extremely high compression ratio, but the problem of a significant drop in accuracy cannot be ignored. To alleviate the obvious accuracy degradation for extreme low-bit quantization, this paper proposes a novel compression framework for binary and ternary neural networks applied to point clouds, which introduces information geometry to model quantization to compensate the severe feature manifold distortion. It applies differential geometry on manifolds to study the implicit information of the point cloud feature data. Based on the theoretical analysis from the novel perspective of information geometry, two optimization modules are proposed to alleviate severe geometry distortions on differential manifolds. The first module, scaling recovery, provides layer-wise scaling parameters to reduce geometric distortion caused by quantization. The second module, Pooling Recovery, is specially designed to alleviate more severe pooling geometry distortions in point clouds. These two modules benefit both binary and ternary neural networks with ignored overheads. For ternary quantization, optimizations on convolution weights and gradients are additionally introduced. The proposed self-adaptive gradient estimation provides a more accurate approximation to the non-differential ternary staircase function. Convolution weight optimization is implemented on an information-geometry optimized model to achieve even higher accuracy and less memory consumption. Experimental results validate that the proposed models significantly outperform state-of-the-art methods and demonstrate better scalability. Overall, this compression framework has the potential to facilitate the deployment of deep learning models on edge devices with limited resources, opening up new opportunities for applications of three-dimensional computer vision.},
  archive      = {J_PR},
  author       = {Zhi Zhao and Yanxin Ma and Ke Xu and Jianwei Wan},
  doi          = {10.1016/j.patcog.2023.109986},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109986},
  shortjournal = {Pattern Recognition},
  title        = {Information geometry based extreme low-bit neural network for point cloud},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Temporal segment dropout for human action video
recognition. <em>PR</em>, <em>146</em>, 109985. (<a
href="https://doi.org/10.1016/j.patcog.2023.109985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal information is important for human action video recognition . With the widely used spatio-temporal neural networks , researchers have found that the learned high-level features preserve overfitted spatial information and limited temporal information, leading to inferior performance. This is because existing networks lack efficient regularization for the temporal structure . To learn more robust temporal features, we propose a temporal regularization method named Temporal Segment Dropout (TSD). TSD drops the most salient spatial features in order to enhance the temporal features in a clip of temporal segments. Without learning from complex examples, TSD can be easily deployed in existing networks. In the experiment, TSD is extensively evaluated on benchmark action recognition datasets, which brings consistent improvements over the baselines, especially for the action-centric classes.},
  archive      = {J_PR},
  author       = {Yu Zhang and Zhengjie Chen and Tianyu Xu and Junjie Zhao and Siya Mi and Xin Geng and Min-Ling Zhang},
  doi          = {10.1016/j.patcog.2023.109985},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109985},
  shortjournal = {Pattern Recognition},
  title        = {Temporal segment dropout for human action video recognition},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple instance learning from similarity-confidence bags.
<em>PR</em>, <em>146</em>, 109984. (<a
href="https://doi.org/10.1016/j.patcog.2023.109984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple instance learning (MIL) is a classic weakly supervised learning approach , in which samples are grouped into bags that may contain varying numbers of instances. A bag is designated as positive if it contains at least one positive instance ; otherwise, it is considered negative. Previous studies have consistently assumed that the bag labels are completely known. In fact, labeling every bag can be extremely challenging or even unfeasible due to the exorbitant expenses in terms of time and labor. Fortunately, it is much easier to obtain the similarity confidence, which represents the probability of two bags sharing the same label. How to employ it in MIL is worthy of study. Inspired by the above study, we present the first attempt to investigate MIL from similarity-confidence bags. Therefore, this paper proposes a new framework for training bag-level classifiers that adheres to the principle of empirical risk minimization . Moreover, we theoretically derive a generalization error bound to guarantee model convergence. Finally, we implement risk correction to mitigate potential over-fitting problem and provide theoretical consistency. Numerical experiments on eight datasets further validate the effectiveness of the proposed bag-level classifier.},
  archive      = {J_PR},
  author       = {Xuan Zhang and Yitian Xu and Xuhua Liu},
  doi          = {10.1016/j.patcog.2023.109984},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109984},
  shortjournal = {Pattern Recognition},
  title        = {Multiple instance learning from similarity-confidence bags},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coherent chord computation and cross ratio for accurate
ellipse detection. <em>PR</em>, <em>146</em>, 109983. (<a
href="https://doi.org/10.1016/j.patcog.2023.109983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new method for detecting ellipses in images, which has many applications in pattern recognition and robotic tasks. Previous approaches typically use sophisticated arc grouping strategies or calculate differential such as tangents, and thereby they are less efficient or more sensitive to noise. In this work, we present a novel ellipse detector, based on the simple yet effective chord computation , and on the projective invariant cross ratio , which achieves promising performance in both accuracy and efficiency. First, elliptical arcs are extracted by fast vector computations along with the removal of straight segments to speed up detection. Then, arcs from the same ellipse are grouped together according to the relative location and the intersecting chord constraints , both are on coherent chord computation without differential. Additionally, an efficient additive principle is applied to further accelerate the grouping process. Finally, a novel and robust verification by area-deduced cross ratio is introduced to pick out salient ellipses. Compared with predecessor methods, cross ratio is not only simple for computation, but also has invariant properties (used to discriminate ellipses). Extensive experiments on seven public datasets (including synthetic and real-world images) are implemented. The results highlight the salient advantages of the proposed method compared to state-of-the-art detectors: Easier to implementation, more robust against occlusion and noise, as well as attaining higher F-measure.},
  archive      = {J_PR},
  author       = {Mingyang Zhao and Xiaohong Jia and Lei Ma and Li-Ming Hu and Dong-Ming Yan},
  doi          = {10.1016/j.patcog.2023.109983},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109983},
  shortjournal = {Pattern Recognition},
  title        = {Coherent chord computation and cross ratio for accurate ellipse detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaze estimation with semi-supervised eye landmark detection
as an auxiliary task. <em>PR</em>, <em>146</em>, 109980. (<a
href="https://doi.org/10.1016/j.patcog.2023.109980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The changes in gaze are often reflected in the movements of eye landmarks, highlighting the relevance of eye landmark learning for accurate gaze estimation. To leverage eye landmarks, we propose a gaze estimation framework that incorporates eye landmark detection as an auxiliary task. However, obtaining eye landmark annotations for real-world gaze datasets is challenging. To address this, we exploit synthetic data , which provides precise eye landmark labels, by jointly training an eye landmark detector using labeled synthetic data and unlabeled real-world data in a semi-supervised manner. To reduce the influence of discrepancy between synthetic and real-world data, we improve the generalization ability of the landmark detector by performing a self-supervised learning strategy on a large scale of unlabeled real-world images. The proposed method outperforms other state-of-the-art gaze estimation methods on three gaze datasets, indicating the effectiveness of leveraging eye landmark detection as an auxiliary task to enhance gaze estimation performance.},
  archive      = {J_PR},
  author       = {Yunjia Sun and Jiabei Zeng and Shiguang Shan},
  doi          = {10.1016/j.patcog.2023.109980},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109980},
  shortjournal = {Pattern Recognition},
  title        = {Gaze estimation with semi-supervised eye landmark detection as an auxiliary task},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TransBoNet: Learning camera localization with transformer
bottleneck and attention. <em>PR</em>, <em>146</em>, 109975. (<a
href="https://doi.org/10.1016/j.patcog.2023.109975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {6DoF camera localization is an important component of autonomous driving and navigation. Deep learning has achieved impressive results in localization, but its robustness in dynamic environments has not been adequately addressed. In this paper, we propose a framework based on hybrid attention mechanism which can be generally applied to existing CNN-based pose regressors to improve their robustness in dynamic environments. Specifically, we propose a novel Transformer Bottleneck (TBo) block including convolution, channel attention, and a position-aware self-attention mechanism, which extracts more geometrically robust features by capturing the corresponding long-term dependencies between pixels. Furthermore, we introduce shuffle attention (SA) before the pose regressor, which integrates feature information in both spatial and channel dimensions, forcing the network to learn geometrically robust features, reducing the effects of dynamic objects and illumination conditions to improve camera localization accuracy . We evaluate our method on commonly benchmarked indoor and outdoor datasets and the experimental results show that our proposed method can significantly improve localization performance compared compare favorably to contemporary pose regressors schemes. In addition, extensive ablation evaluations are conducted to prove the effectiveness of our proposed hybrid attention bottleneck block for pose regression networks.},
  archive      = {J_PR},
  author       = {Xiaogang Song and Hongjuan Li and Li Liang and Weiwei Shi and Guo Xie and Xiaofeng Lu and Xinhong Hei},
  doi          = {10.1016/j.patcog.2023.109975},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109975},
  shortjournal = {Pattern Recognition},
  title        = {TransBoNet: Learning camera localization with transformer bottleneck and attention},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shadow-aware dynamic convolution for shadow removal.
<em>PR</em>, <em>146</em>, 109969. (<a
href="https://doi.org/10.1016/j.patcog.2023.109969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a wide range of shadows in many collected images, shadow removal has aroused increasing attention since uncontaminated images are of vital importance for many computer vision tasks . Current methods consider the same convolution operations for both shadow and non-shadow regions while ignoring the large gap between the color mappings for the shadow region and the non-shadow region, leading to poor quality of reconstructed images and a heavy computation burden. To solve this problem, this paper introduces a novel plug-and-play Shadow-Aware Dynamic Convolution (SADC) module to decouple the interdependence between the shadow region and the non-shadow region. Inspired by the fact that the color mapping of the non-shadow region is easier to learn, our SADC processes the non-shadow region with a lightweight convolution module in a computationally cheap manner and recovers the shadow region with a more complicated convolution module to ensure the quality of image reconstruction. Given that the non-shadow region often contains more background color information, we further develop a novel intra-convolution distillation loss to strengthen the information flow from the non-shadow region to the shadow region. Extensive experiments on the ISTD and SRD datasets show our method achieves better performance in shadow removal over many state-of-the-art methods. Codes have been made available at https://github.com/xuyimin0926/SADC.},
  archive      = {J_PR},
  author       = {Yimin Xu and Mingbao Lin and Hong Yang and Fei Chao and Rongrong Ji},
  doi          = {10.1016/j.patcog.2023.109969},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109969},
  shortjournal = {Pattern Recognition},
  title        = {Shadow-aware dynamic convolution for shadow removal},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Updatable siamese tracker with two-stage one-shot learning.
<em>PR</em>, <em>146</em>, 109965. (<a
href="https://doi.org/10.1016/j.patcog.2023.109965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offline-trained Siamese networks have realized very promising tracking precision and efficiency. However, the performance is still limited by the drawbacks in online update. Traditional strategies cannot tackle the irregular variations of object and the sampling noise, so it is quite risky to adopt them to update Siamese trackers. In this paper, we present a two-stage one-shot learner by exploring the learning scheme of Siamese network, which reveals there are two key issues during online update, i.e., feature fusion and feature comparison. Based on this finding, we propose an updatable Siamese tracker by introducing two independent transformers (SiamTOL). Concretely, a Cross-aware transformer is designed to combine the features of the initial and the dynamic templates, while a Decoder-favored transformer is exploited to compare the fusing template and the search region. By combining these transformers, our tracker is able to adequately model the feature dependencies between multi-frame object samples. Extensive experimental results on several popular benchmarks well manifest that the proposed approach achieves the leading performance, and outperforms other state-of-the-art trackers.},
  archive      = {J_PR},
  author       = {Xinglong Sun and Haijiang Sun and Jianan Li},
  doi          = {10.1016/j.patcog.2023.109965},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109965},
  shortjournal = {Pattern Recognition},
  title        = {Updatable siamese tracker with two-stage one-shot learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Transformer-based visual object tracking via fine–coarse
concatenated attention and cross concatenated MLP. <em>PR</em>,
<em>146</em>, 109964. (<a
href="https://doi.org/10.1016/j.patcog.2023.109964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based trackers have demonstrated promising performance in visual object tracking tasks. Nevertheless, two drawbacks limited the potential performance improvement of transformer-based trackers. Firstly, the static receptive field of the tokens within one attention layer of the original self-attention learning neglects the multi-scale nature in the object tracking task. Secondly, the learning procedure of the multi-layer perception (MLP) in the feed forward network (FFN) is lack of local interaction information among samples. To address the above issues, a new self-attention learning method, fine–coarse concatenated attention (FCA), is proposed to learn self-attention with fine and coarse granularity information. Moreover, the cross-concatenation MLP (CC-MLP) is developed to capture local interaction information across samples. Based on the two proposed modules, a novel encoder and decoder are constructed, and augmented in an all-attention tracking algorithm, FCAT. Comprehensive experiments on popular tracking datasets, OTB2015, LaSOT, GOT-10K and TrackingNet, reveal the effectiveness of FCA and CC-MLP, and FCAT achieves the state-of-art on the datasets.},
  archive      = {J_PR},
  author       = {Long Gao and Langkun Chen and Pan Liu and Yan Jiang and Yunsong Li and Jifeng Ning},
  doi          = {10.1016/j.patcog.2023.109964},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109964},
  shortjournal = {Pattern Recognition},
  title        = {Transformer-based visual object tracking via fine–coarse concatenated attention and cross concatenated MLP},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reparameterizing and dynamically quantizing image features
for image generation. <em>PR</em>, <em>146</em>, 109962. (<a
href="https://doi.org/10.1016/j.patcog.2023.109962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For autoregressive image generation , vector-quantized VAEs (VQ-VAEs) quantize image features with discrete codebook entries and reconstruct images from quantized features. However, they treat each codebook entry separately, which causes losses of image details. In this paper, we propose to reparameterize image features with weight vectors to treat all codebook entries as an entity, and present a novel dynamically vector quantized VAE (DVQ-VAE) to quantize reparameterized image features. Specifically, each image feature corresponds to a weight vector and we sum weighted codebook entries to obtain values of image features. In this way, image features can incorporate information from different codebook entries. Additionally, a novel continuous weight regularization loss is proposed to improve the reconstruction of image details. Our method achieves competitive results with prior state-of-the-art works for image generation and extensive experiments are conducted to take a deep insight into our DVQ-VAE.},
  archive      = {J_PR},
  author       = {Mingzhen Sun and Weining Wang and Xinxin Zhu and Jing Liu},
  doi          = {10.1016/j.patcog.2023.109962},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109962},
  shortjournal = {Pattern Recognition},
  title        = {Reparameterizing and dynamically quantizing image features for image generation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing texture representation with deep tracing pattern
encoding. <em>PR</em>, <em>146</em>, 109959. (<a
href="https://doi.org/10.1016/j.patcog.2023.109959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texture representation is a challenging problem due to the complex underlying physics of texture as well as the variations caused by changes in viewpoint. Recent progress in texture analysis has been made by the power of convolutional neural networks (CNNs) in feature learning . However, most current methods aggregate the features from the last convolutional layer of the CNN to obtain a global feature vector, which fails to leverage shallow low-level visual cues and cross-layer feature patterns, limiting their performance. In this paper, we propose to trace the features generated along the convolutional layers via a histogram of local 3D invariant binary patterns, called deep tracing patterns. This leads to a highly discriminative yet robust global feature representation module. Building such a module into a CNN backbone, we develop an effective approach for texture recognition. Extensive experiments on six benchmark datasets show that the proposed approach provides a discriminative and robust texture descriptor , with state-of-the-art performance achieved.},
  archive      = {J_PR},
  author       = {Zhile Chen and Yuhui Quan and Ruotao Xu and Lianwen Jin and Yong Xu},
  doi          = {10.1016/j.patcog.2023.109959},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109959},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing texture representation with deep tracing pattern encoding},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-lingual summarization method based on cross-lingual
fact-relationship graph generation. <em>PR</em>, <em>146</em>, 109952.
(<a href="https://doi.org/10.1016/j.patcog.2023.109952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of cross-lingual summarization (CLS) is to condense the content of a document in one language into a summary in another language. In essence, a CLS model requires both translation and summarization capabilities, which presents a unique challenge, as the model must effectively tackle the difficulties associated with both tasks simultaneously (e.g., semantic alignment, information compression and factual inconsistency). Graph-based semantic representation can model important text information in a structured manner, which may alleviate these challenges. Therefore, in this paper, we propose a Cross-Lingual Summarization method based on cross-lingual Fact-relationship Graph Generation (FGGCLS). Specifically, we first construct fact-relationship graphs for source language documents and target language summaries. Then, we introduce a cross-lingual fact-relationship graph generation method, which converts the CLS problem into a cross-lingual fact-relationship graph generation problem. This approach simplifies semantic alignment and information compression through the generation of graphs and leads to improved fact consistency. Finally, the generated fact-relationship graph of the target language summary serves as a draft for generating the summary, which enhances the quality of the generated summary. We conduct systematic experiments on the Zh2EnSum and En2ZhSum datasets, and the results demonstrate that our method can effectively improve the performance of CLS and alleviate factual inconsistency.},
  archive      = {J_PR},
  author       = {Yongbing Zhang and Shengxiang Gao and Yuxin Huang and Kaiwen Tan and Zhengtao Yu},
  doi          = {10.1016/j.patcog.2023.109952},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109952},
  shortjournal = {Pattern Recognition},
  title        = {A cross-lingual summarization method based on cross-lingual fact-relationship graph generation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A network classification method based on density time
evolution patterns extracted from network automata. <em>PR</em>,
<em>146</em>, 109946. (<a
href="https://doi.org/10.1016/j.patcog.2023.109946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network modeling has proven to be an efficient tool for many interdisciplinary areas, including social, biological, transportation, and various other complex real-world systems. In addition, cellular automata (CA) are a formalism that has received significant attention in recent decades as a model for investigating patterns in the dynamic spatio-temporal behavior of these systems, based on local rules. Some studies investigate the use of cellular automata to analyze the dynamic behavior of networks and refer to them as network automata (NA). Recently, it has been demonstrated that NA is effective for network classification, as it employs a Time-Evolution Pattern (TEP) for feature extraction. However, the TEPs investigated in previous studies consist of binary values (states) that do not capture the intrinsic details of the analyzed network. Therefore, in this work, we propose alternative sources of information that can be used as descriptors for the classification task , which we refer as Density Time-Evolution Pattern (D-TEP) and State Density Time-Evolution Pattern (SD-TEP). We examine the density of alive neighbors of each node, which is a continuous value, and compute feature vectors based on histograms of TEPs. Our results demonstrate significant improvement over previous studies on five synthetic network datasets, as well as seven real datasets. Our proposed method is not only a promising approach for pattern recognition in networks, but also shows considerable potential for other types of data that can be transformed into network.},
  archive      = {J_PR},
  author       = {Kallil M.C. Zielinski and Lucas C. Ribas and Jeaneth Machicao and Odemir M. Bruno},
  doi          = {10.1016/j.patcog.2023.109946},
  journal      = {Pattern Recognition},
  month        = {2},
  pages        = {109946},
  shortjournal = {Pattern Recognition},
  title        = {A network classification method based on density time evolution patterns extracted from network automata},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A quantitative method for the assessment of facial
attractiveness based on transfer learning with fine-grained image
classification. <em>PR</em>, <em>145</em>, 109970. (<a
href="https://doi.org/10.1016/j.patcog.2023.109970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate a new approach based on a combination of three-dimensional (3D) facial images and deep transfer learning (TL) with fine-grained image classification (FGIC) for quantitative evaluation of facial attractiveness. The 3D facial surface images of patients with and without filtering and the publicly available SCUT-FBP5500 dataset was used for transfer training and model pre-training, respectively. Experimental results show that a bilinear CNN model with a Gaussian filter freezing 80 % of the weights exhibit the strongest performance and lowest average error as a deep learning prediction model; the model was subsequently adopted for automatic assessment of facial attractiveness in clinical application. This is the first TL model with FGIC using 3D facial images for automatic quantitative evaluation of facial attractiveness in patients undergoing Orthognathic surgery (OGS). The developed web browser–based user interface enables effective and rapid assessment, thus contributing to effective patient–clinician communication and decision-making.},
  archive      = {J_PR},
  author       = {Lun-Jou Lo and Chao-Tung Yang and Wen-Chung Chiang and Hsiu-Hsia Lin},
  doi          = {10.1016/j.patcog.2023.109970},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109970},
  shortjournal = {Pattern Recognition},
  title        = {A quantitative method for the assessment of facial attractiveness based on transfer learning with fine-grained image classification},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph fairing convolutional networks for anomaly detection.
<em>PR</em>, <em>145</em>, 109960. (<a
href="https://doi.org/10.1016/j.patcog.2023.109960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolution is a fundamental building block for many deep neural networks on graph-structured data. In this paper, we introduce a simple, yet very effective graph convolutional network with skip connections for semi-supervised anomaly detection . The proposed layerwise propagation rule of our model is theoretically motivated by the concept of implicit fairing in geometry processing, and comprises a graph convolution module for aggregating information from immediate node neighbors and a skip connection module for combining layer-wise neighborhood representations. This propagation rule is derived from the iterative solution of the implicit fairing equation via the Jacobi method . In addition to capturing information from distant graph nodes through skip connections between the network’s layers, our approach exploits both the graph structure and node features for learning discriminative node representations. These skip connections are integrated by design in our proposed network architecture . The effectiveness of our model is demonstrated through extensive experiments on five benchmark datasets, achieving better or comparable anomaly detection results against strong baseline methods . We also demonstrate through an ablation study that skip connection helps improve the model performance.},
  archive      = {J_PR},
  author       = {Mahsa Mesgaran and A. Ben Hamza},
  doi          = {10.1016/j.patcog.2023.109960},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109960},
  shortjournal = {Pattern Recognition},
  title        = {Graph fairing convolutional networks for anomaly detection},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequence-level affective level estimation based on pyramidal
facial expression features. <em>PR</em>, <em>145</em>, 109958. (<a
href="https://doi.org/10.1016/j.patcog.2023.109958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People tend to focus on changes in a certain complex human affect in the majority of practical applications of affective computing . Facial expression classification models are unable to represent all human affects through a limited number of expression categories. In this backdrop, this paper studies the Sequence-level affective level estimation (S-ALE), which is more relevant to real scenarios and can depict individual affective level in continuous manner. A spatio-temporal framework applied to S-ALE is proposed, which consists of a Facial Expression Features Pyramid Network (FEFPN) and a Temporal Transformer Encoder (TTE). FEFPN is capable of extracting pyramidal facial expression features, while TTE can effectively capture coarse-grained and fine-grained temporal variations of facial sequences. The proposed model is evaluated on six public datasets across three typical S-ALE tasks (engagement prediction, fatigue detection, and pain assessment), and experimental results show that our method is comparable to or outperforms the state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Jiacheng Liao and Yan Hao and Zhuoyi Zhou and Jiahui Pan and Yan Liang},
  doi          = {10.1016/j.patcog.2023.109958},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109958},
  shortjournal = {Pattern Recognition},
  title        = {Sequence-level affective level estimation based on pyramidal facial expression features},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive error-correcting output codes algorithm based on
gene expression programming and similarity measurement matrix.
<em>PR</em>, <em>145</em>, 109957. (<a
href="https://doi.org/10.1016/j.patcog.2023.109957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-class classification task is one of the most common tasks in machine learning. As a typical solution based on a partitioning strategy, Error-Correcting Output Codes (ECOC) can transform a multi-class classification problem into multiple binary classification problems. The key of ECOC is to construct an effective codematrix to represent a set of class decomposition schemes, which transforms a multiclass problem into a group of binary class problems. Consequently, the design of a fast and effective ECOC codematrix generation method is of great research significance and value for solving multi-class classification problems. In ECOC algorithms, the design of codematrix is treated as a combination problem between different code columns, in which the evolutionary algorithm shows a great advantage. Based on this consideration, the Gene Expression Programming (GEP) is applied to search for the codematrix with high performance because its expressive tree structure makes it well represent codematrcies for subsequent optimization operations. This paper proposes an adaptive ECOC algorithm based on Gene Expression Programming (GEP) and similarity measurement matrix, named GEP-ECOC. In our GEP, each individual represents a set of columns to form a random ECOC codematrix, which is optimized in the evolutionary process. Meanwhile, the crossover and mutation operations are modified to include a legality checking process to ensure that the generated codematrix satisfies the ECOC constraints. The GEP-based ECOC codematrix generation algorithm can quickly produce a codematrix with better performance, which ensures the efficiency of the algorithm to a certain extent. In addition, an adaptive algorithm based on a similarity measurement matrix is proposed to add new columns to the current codematrix, aiming to better handle hard classes. Our algorithm is compared with other algorithms on various data sets, and the experimental results confirm that our GEP-ECOC can balance the efficiency and performance of the algorithm and achieve higher performance.},
  archive      = {J_PR},
  author       = {Shutong Xie and Zongbao He and Lifang Pan and Kunhong Liu and Shubin Su},
  doi          = {10.1016/j.patcog.2023.109957},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109957},
  shortjournal = {Pattern Recognition},
  title        = {An adaptive error-correcting output codes algorithm based on gene expression programming and similarity measurement matrix},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training feedforward neural nets in hopfield-energy-based
configuration: A two-step approach. <em>PR</em>, <em>145</em>, 109954.
(<a href="https://doi.org/10.1016/j.patcog.2023.109954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce Hopfield-Energy-Based Learning, a general learning framework that is inspired by energy-based models, to train feedforward neural nets. Our approach includes two training phases applied iteratively: first, the minimization of the internal energy, which captures dependencies between input samples and network parameters, is carried out in an unsupervised manner ; second, the problem-dependent supervised external energy ( e.g., cross-entropy loss) combined with partially reversed internal energy gradients are back-propagated in a standard manner. The intuition is that the first stage helps parameters to settle into the state that simply partitions data into clusters; while in the second stage, the network is allowed to deviate from that clustering a bit (hence gradient reversal) in order to converge to parameters that ultimately perform well on the task at hand. Notably, the data used for the two steps might not be one and the same ( e.g., can come from different domains) and the approach naturally tailors itself to solve unsupervised domain adaptation problems without adopting any distribution alignment techniques. We also show that the proposed training strategy substantially improves the performance of several ConvNets on standard supervised classification tasks ; showing improvements of at least 1.2% (2.64% on CIFAR-10, 4.5% on CIFAR-100, and 1.35% on ImageNet). Our formulation is general, performs well in practice, and holds promise for scenarios where labeled data is limited.},
  archive      = {J_PR},
  author       = {Jing Wang and Jiahong Chen and Kuangen Zhang and Leonid Sigal},
  doi          = {10.1016/j.patcog.2023.109954},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109954},
  shortjournal = {Pattern Recognition},
  title        = {Training feedforward neural nets in hopfield-energy-based configuration: A two-step approach},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label borderline oversampling technique. <em>PR</em>,
<em>145</em>, 109953. (<a
href="https://doi.org/10.1016/j.patcog.2023.109953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance problem commonly exists in multi-label classification (MLC) tasks. It has non-negligible impacts on the classifier performance and has drawn extensive attention in recent years. Borderline oversampling has been widely used in single-label learning as a competitive technique in dealing with class imbalance . Nevertheless, the borderline samples in multi-label data sets (MLDs) have not been studied. Hence, this paper deeply discussed the borderline samples in MLDs and found they have different neighboring relationships with class borders, which makes their roles different in the classifier training. For that, they are divided into two types named the self-borderline samples and the cross-borderline samples. Further, a novel MLDs resampling approach called Multi-Label Borderline Oversampling Technique (MLBOTE) is proposed for multi-label imbalanced learning. MLBOTE identifies three types of seed samples, including interior, self-borderline, and cross-borderline samples, and different oversampling mechanisms are designed for them, respectively. Meanwhile, it regards not only the minority classes but also the classes suffering from one-vs-rest imbalance as those in need of oversampling. Experiments on eight data sets with nine MLC algorithms and three base classifiers are carried out to compare MLBOTE with some state-of-art MLDs resampling techniques. The results show MLBOTE outperforms other methods in various scenarios.},
  archive      = {J_PR},
  author       = {Zeyu Teng and Peng Cao and Min Huang and Zheming Gao and Xingwei Wang},
  doi          = {10.1016/j.patcog.2023.109953},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109953},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label borderline oversampling technique},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking deep models on salient object detection.
<em>PR</em>, <em>145</em>, 109951. (<a
href="https://doi.org/10.1016/j.patcog.2023.109951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance discrepancies caused by different implementation details may obscure the actual progress of the Salient Object Detection (SOD) task. In this paper, we construct a SALient Object Detection (SALOD) benchmark to ensure a fair and comprehensive evaluation by unifying the implementation details of SOD methods. By doing so, we can reveal the reasons behind recent progress by analyzing the impact of network structure and optimization strategy . Based on the experimental results, we first find that U-shaped networks, both older and more recent variants, achieve better performance than other structures. Second, optimization strategy , e.g. , training strategy and loss function, significantly impacts SOD accuracy. Finally, we provide a new perspective to validate the generalizability of SOD methods on objectness shifting. Code is available at https://github.com/moothes/SALOD .},
  archive      = {J_PR},
  author       = {Huajun Zhou and Yang Lin and Lingxiao Yang and Jianhuang Lai and Xiaohua Xie},
  doi          = {10.1016/j.patcog.2023.109951},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109951},
  shortjournal = {Pattern Recognition},
  title        = {Benchmarking deep models on salient object detection},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MCNet: Magnitude consistency network for domain adaptive
object detection under inclement environments. <em>PR</em>,
<em>145</em>, 109947. (<a
href="https://doi.org/10.1016/j.patcog.2023.109947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based object detection methods have achieved promising results in normal scenarios, while such methods often fail to locate objects from the disturbed images captured in inclement environments. Most existing methods utilized denoise modules to assist the detection network or exploit prior knowledge to reduce the environment interference remaining in the features, which ignores the essential role of the frequency element. From a novel perspective, we observe that the inclement environment alters the frequency content in the features, which in turn crushes the detection. To tickle this problem, we present the Magnitude Consistency Network (MCNet) to distill the irrelevant contents in the frequency domain. The MCNet is composed of the detection network and the magnitude corrector . The detection network is able to locate and classify objects. However, in inclement environments, the crucial information about the objects of interest is disrupted by the nuisance noise introduced from the inclement environments. The magnitude corrector can recover relevant information about the objects in the frequency domain by distilling the irrelevant factors and refining the affected features. Alternately optimizing the magnitude corrector and the detection network gradually makes the frequency content between the disturbed image and the clear image to be consistent. By distilling the irrelevant noise in the feature, the detection network can learn domain-invariant representations. Extensive experiments prove that the proposed method is effective and outperforms existing methods by a clear margin on four datasets.},
  archive      = {J_PR},
  author       = {Jian Pang and Weifeng Liu and Bingfeng Zhang and Xinghao Yang and Baodi Liu and Dapeng Tao},
  doi          = {10.1016/j.patcog.2023.109947},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109947},
  shortjournal = {Pattern Recognition},
  title        = {MCNet: Magnitude consistency network for domain adaptive object detection under inclement environments},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label feature selection by strongly relevant label
gain and label mutual aid. <em>PR</em>, <em>145</em>, 109945. (<a
href="https://doi.org/10.1016/j.patcog.2023.109945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection, which addresses the challenge of high dimensionality in multi-label learning, has wide applicability in pattern recognition, machine learning , and related domains. Most existing studies on multi-label feature selection assume that all labels have the same importance with respect to features, however, they overlook the differences between labels and candidate features relative to selected features and the internal influence of the label space. To address this issue, we propose a novel method for multi-label feature selection that accounts for both the strongly relevant label gain and the label mutual aid. Firstly, we advance two new potential relationships between labels and candidate features relative to selected features, and the label discriminant function is introduced. Secondly, the mutual aid information between labels is presented to describe the internal correlation of the label space. Thirdly, the concept of strongly relevant label gain is defined based on the label discriminant function, which allows better exploration of positive correlation between features. Finally, the experimental results on sixteen multi-label benchmark datasets indicate that the proposed method outperforms other compared representative multi-label feature selection methods.},
  archive      = {J_PR},
  author       = {Jianhua Dai and Weiyi Huang and Chucai Zhang and Jie Liu},
  doi          = {10.1016/j.patcog.2023.109945},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109945},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection by strongly relevant label gain and label mutual aid},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient disentangled representation learning for
multi-modal finger biometrics. <em>PR</em>, <em>145</em>, 109944. (<a
href="https://doi.org/10.1016/j.patcog.2023.109944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multi-modal biometric systems use multiple devices to capture different traits and directly fuse multi-modal data while ignoring correlation information between modalities. In this paper, finger skin and finger vein images are acquired from the same region of the finger and therefore have a higher correlation. To represent data efficiently, we propose a novel Finger Disentangled Representation Learning Framework (FDRL-Net) that is based on a factorization concept, which disentangles each modality into shared and private features, thereby improving complementarity for better fusion and extracting modality-invariant features for heterogeneous recognition. Besides, to capture as much finger texture as possible, we utilize three-view finger images to reconstruct full-view multi-spectral finger traits, which increases the identity information and the robustness to finger posture variation. Finally, a Boat-Trackers-based multi-task distillation method is proposed to migrate the feature representation ability to a lightweight multi-task network. Extensive experiments on six single-view multi-spectral finger datasets and two full-view multi-spectral finger datasets demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Weili Yang and Junduan Huang and Dacan Luo and Wenxiong Kang},
  doi          = {10.1016/j.patcog.2023.109944},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109944},
  shortjournal = {Pattern Recognition},
  title        = {Efficient disentangled representation learning for multi-modal finger biometrics},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale self-supervised representation learning with
temporal alignment for multi-rate time series modeling. <em>PR</em>,
<em>145</em>, 109943. (<a
href="https://doi.org/10.1016/j.patcog.2023.109943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep sequential networks have shown great power in time series regression and classification. So far, most approaches naturally assume that the time sequential data are uniformly sampled. In practice, however, different variables usually have different sampling rates , thereby forming multi-rate time series (MR-TS). Particularly, the target variable ( i.e. , label) to be predicted usually has a lower sampling frequency due to the difficulty of manual annotations. The multi-rate problem poses two challenges. One is the diverse dynamics at different sampling rates, which is defined as multi-scale dynamics. The other is label scarcity. To tackle the above obstacles, this paper developed a M ulti-scale S elf-supervised R epresentation L earning technique with a T emporal A lignment mechanism (MSRL-TA) as a coherent framework. Concretely, a probabilistic masked autoencoding approach is pertinently developed, in which segment-wise masking schemes and rate-aware positional encodings are devised to enable the characterization of multi-scale temporal dynamics. In the course of pre-training, the encoder networks are able to generate rich and holistic representations of multi-rate data, thereby alleviating the label scarcity issue for supervised fine-tuning. Furthermore, a Temporal Alignment mechanism is devised to refine synthesized features for dynamic predictive modeling through feature block division and block-wise convolution. With empirical evaluation through extensive experiments, our proposed MSRL-TA achieved consistent state-of-the-art in both multi-rate time series regression and classification tasks on five real-world datasets, including air quality prediction, industrial soft sensing, human activity recognition , and speech voice classification.},
  archive      = {J_PR},
  author       = {Jiawei Chen and Pengyu Song and Chunhui Zhao},
  doi          = {10.1016/j.patcog.2023.109943},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109943},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale self-supervised representation learning with temporal alignment for multi-rate time series modeling},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding open-set recognition by jacobian norm and
inter-class separation. <em>PR</em>, <em>145</em>, 109942. (<a
href="https://doi.org/10.1016/j.patcog.2023.109942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The findings on open-set recognition (OSR) show that models trained on classification datasets are capable of detecting unknown classes not encountered during the training process. Specifically, after trainig, the learned representations of known classes dissociate from the representations of the unknown class, facilitating OSR. In this paper, we investigate this emergent phenomenon by examining the relationship between the Jacobian norm of representations and the inter/intra-class learning dynamics. We provide a theoretical analysis, demonstrating that intra-class learning reduces the Jacobian norm for known class samples, while inter-class learning increases the Jacobian norm for unknown samples, even in the absence of direct exposure to any unknown sample. Overall, the discrepancy in the Jacobian norm between the known and unknown classes enables OSR. Based on this insight, which highlights the pivotal role of inter-class learning, we devise a marginal one-vs-rest (m-OvR) loss function that promotes strong inter-class separation. To further improve OSR performance, we integrate the m-OvR loss with additional strategies that maximize the Jacobian norm disparity. We present comprehensive experimental results that support our theoretical observations and demonstrate the efficacy of our proposed OSR approach.},
  archive      = {J_PR},
  author       = {Jaewoo Park and Hojin Park and Eunju Jeong and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.patcog.2023.109942},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109942},
  shortjournal = {Pattern Recognition},
  title        = {Understanding open-set recognition by jacobian norm and inter-class separation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A zero-shot learning boosting framework via
concept-constrained clustering. <em>PR</em>, <em>145</em>, 109937. (<a
href="https://doi.org/10.1016/j.patcog.2023.109937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize novel classes that have no labeled samples during the training phase, which leads to the domain shift problem. In reality, there exists a large number of compounded unlabeled samples . Therefore, it is crucial to accurately estimate the data distribution of these compounded unlabeled samples and improve the performance of ZSL. This paper proposes a zero-shot learning boosting framework. Specifically, ZSL is transformed into a co-training problem between the data distribution estimation of the unlabeled samples and ZSL. The data distribution estimation is modeled as concept-constrained clustering. Furthermore, we design an alternative optimization strategy to realize mutual guidance between the two processes. Finally, systematic experiments verify the effectiveness of the proposed concept-constrained clustering for alleviating the domain shift problem in ZSL and the universality of the proposed framework for boosting different base ZSL models.},
  archive      = {J_PR},
  author       = {Qin Yue and Junbiao Cui and Liang Bai and Jianqing Liang and Jiye Liang},
  doi          = {10.1016/j.patcog.2023.109937},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109937},
  shortjournal = {Pattern Recognition},
  title        = {A zero-shot learning boosting framework via concept-constrained clustering},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth map denoising network and lightweight fusion network
for enhanced 3D face recognition. <em>PR</em>, <em>145</em>, 109936. (<a
href="https://doi.org/10.1016/j.patcog.2023.109936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of consumer depth sensors, 3D face recognition (FR) has attracted more and more attention. However, the data acquired by these sensors are often coarse and noisy, making them impractical to use directly. In this paper, we introduce an innovative Depth map denoising network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to reduce noise and enhance the quality of facial depth images for low-quality 3D FR. After generating clean depth faces using DMDNet, we further design a powerful recognition network called Lightweight Depth and Normal Fusion network (LDNFNet) , which incorporates a multi-branch fusion block to learn unique and complementary features between different modalities such as depth and normal images. Comprehensive experiments conducted on four distinct low-quality databases demonstrate the effectiveness and robustness of our proposed methods. Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art results on the Lock3DFace database.},
  archive      = {J_PR},
  author       = {Ruizhuo Xu and Ke Wang and Chao Deng and Mei Wang and Xi Chen and Wenhui Huang and Junlan Feng and Weihong Deng},
  doi          = {10.1016/j.patcog.2023.109936},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109936},
  shortjournal = {Pattern Recognition},
  title        = {Depth map denoising network and lightweight fusion network for enhanced 3D face recognition},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel minutiae-oriented approach for partial
fingerprint-based MasterPrint mitigation. <em>PR</em>, <em>145</em>,
109935. (<a href="https://doi.org/10.1016/j.patcog.2023.109935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial fingerprint identification systems recognise an individual when the sensor size has a small form factor in accepting a full fingerprint. However, the distinctive features within a partial fingerprint are significantly less. Hence, the uniqueness of a partial fingerprint cannot be assured, leading to the possibility of identifying multiple users. A MasterPrint is a partial fingerprint identifying at least 4% distinct individuals in a partial fingerprint identification system. This work addresses the MasterPrint vulnerability by proposing a novel partial fingerprint identification scheme that extracts minutiae-oriented local features from binarized and thinned partial fingerprint images over eight axes emerging from a reference minutia . It also introduces a metric to compute the similarity score between two partial fingerprint templates. The results are compared with the baseline minutiae matching (BMM) method, a modified Speeded-Up Robust Features (SURF) based approach, VeriFinger 12.1 SDK, standard NIST NBIS , and Ridge Shape Feature (RSF) scheme. The experiments employing partial fingerprint datasets cropped from standard FVC2002 DB1_A, FVC2002 DB2_A, NIST Special Databases (sd302b and sd302d), and CrossMatch VeriFinger dataset have demonstrated that the proposed method generates the lowest MasterPrints with the highest identification accuracy.},
  archive      = {J_PR},
  author       = {Mahesh Joshi and Bodhisatwa Mazumdar and Somnath Dey},
  doi          = {10.1016/j.patcog.2023.109935},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109935},
  shortjournal = {Pattern Recognition},
  title        = {A novel minutiae-oriented approach for partial fingerprint-based MasterPrint mitigation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient supervised graph embedding hashing for large-scale
cross-media retrieval. <em>PR</em>, <em>145</em>, 109934. (<a
href="https://doi.org/10.1016/j.patcog.2023.109934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph based hashing has gained much attention due to its effectiveness in multi-media retrieval. Although several graph embedding based works have been designed and achieved promising performance, there are still some issues that need to be further studied, including, (1) one significant drawback of graph embedding is its expensive memory and computation cost caused by the graph Laplacian matrix ; (2) most pioneer works fail to fully explore the available class labels in training procedure, which generally makes them suffer from unsatisfactory retrieval performance . To overcome these drawbacks, we propose a simple yet effective supervised cross-media hashing scheme, termed Efficient Supervised Graph Embedding Hashing (ESGEH), which can simultaneously learn hash functions and discrete binary codes efficiently. Specifically, ESGEH leverages both class label based semantic embedding and graph embedding to generate a sharing semantic subspace, and class labels are also incorporated to minimize the quantization error for better approximating the generated binary codes. In order to reduce the computational sources, a well-designed intermediate terms decomposition is proposed to avoid explicitly computing the graph Laplacian matrix. Finally, an iterative discrete optimal algorithm is derived to solve above problem, and each subproblem can yield a closed-form solution. Extensive experimental results on four public datasets demonstrate the superiority of the proposed approach over several existing cross-media hashing methods in terms of both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Tao Yao and Ruxin Wang and Jintao Wang and Ying Li and Jun Yue and Lianshan Yan and Qi Tian},
  doi          = {10.1016/j.patcog.2023.109934},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109934},
  shortjournal = {Pattern Recognition},
  title        = {Efficient supervised graph embedding hashing for large-scale cross-media retrieval},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust implementation of foreground extraction and vessel
segmentation for x-ray coronary angiography image sequence. <em>PR</em>,
<em>145</em>, 109926. (<a
href="https://doi.org/10.1016/j.patcog.2023.109926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extraction of contrast-filled vessels from X-ray coronary angiography (XCA) image sequence has important clinical significance for intuitively diagnosis and therapy. In this study, the XCA image sequence is regarded as a 3D tensor input, the vessel layer is regarded as a sparse tensor, and the background layer is regarded as a low-rank tensor. Using tensor nuclear norm (TNN) minimization, a novel method for vessel layer extraction based on tensor robust principal component analysis (TRPCA) is proposed. Furthermore, considering the irregular movement of vessels and the low-frequency dynamic disturbance of surrounding irrelevant tissues, the total variation (TV) regularized spatial–temporal constraint is introduced to smooth the foreground layer. Subsequently, for vessel layer images with uneven contrast distribution, a two-stage region growing (TSRG) method is utilized for vessel enhancement and segmentation. A global threshold method is used as the preprocessing to obtain main branches, and the Radon-Like features (RLF) filter is used to enhance and connect broken minor segments. The final binary vessel mask is constructed by combining the two intermediate results. The visibility of TV-TRPCA algorithm for foreground extraction is evaluated on clinical XCA image sequences and third-party dataset, which can effectively improve the performance of commonly used vessel segmentation algorithms. Based on TV-TRPCA, the accuracy of TSRG algorithm for vessel segmentation is further evaluated. Both qualitative and quantitative results validate the superiority of the proposed method over existing state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Zeyu Fu and Zhuang Fu and Chenzhuo Lu and Jun Yan and Jian Fei and Hui Han},
  doi          = {10.1016/j.patcog.2023.109926},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109926},
  shortjournal = {Pattern Recognition},
  title        = {Robust implementation of foreground extraction and vessel segmentation for X-ray coronary angiography image sequence},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-center study of ultrasound images using a fully
automated segmentation architecture. <em>PR</em>, <em>145</em>, 109925.
(<a href="https://doi.org/10.1016/j.patcog.2023.109925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate organ segmentation in ultrasound (US) images remains challenging because such images have inhomogeneous intensity distributions in their regions of interest (ROIs) and speckle and imaging artifacts. We address this problem by developing a coarse-to-refinement architecture for the segmentation of multiple organs (i.e., the prostate and kidney) in US image datasets from multiple centers. Our proposed architecture has the following four advantages: (1) it inherits the ability of the deep learning models to locate an ROI automatically while also using a principal curve approach to automatically fit a dataset center; (2) it takes advantage of a principal curve-based enhanced polygon searching method, which inherits the principal curve&#39;s characteristic to automatically approach the center of the dataset; (3) it incorporates quantum characteristics into a storage-based evolution network together to improve the global search performance of our method, which includes several improvements, such as a new quantum mutation module, a cuckoo search method, and global optimum schemes; (4) it incorporates a suitable mathematical model to smooth the contour of ROIs, which is explained by the parameters of a neural network model. Application of our method to US image datasets of multiple organs and from multiple centers demonstrates that it achieves satisfactory segmentation performance.},
  archive      = {J_PR},
  author       = {Tao Peng and Caishan Wang and Caiyin Tang and Yidong Gu and Jing Zhao and Quan Li and Jing Cai},
  doi          = {10.1016/j.patcog.2023.109925},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109925},
  shortjournal = {Pattern Recognition},
  title        = {A multi-center study of ultrasound images using a fully automated segmentation architecture},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised penalty-based aggregation applied to
motor-imagery based brain-computer-interface. <em>PR</em>, <em>145</em>,
109924. (<a href="https://doi.org/10.1016/j.patcog.2023.109924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new version of penalty-based aggregation functions, the Multi Cost Aggregation choosing functions (MCAs), in which the function to minimize is constructed using a convex combination of two relaxed versions of restricted equivalence and dissimilarity functions instead of a penalty function. We additionally suggest two different alternatives to train a MCA in a supervised classification task in order to adapt the aggregation to each vector of inputs. We apply the proposed MCA in a Motor Imagery-based Brain–Computer Interface (MI-BCI) system to improve its decision making phase. We also evaluate the classical aggregation with our new aggregation procedure in two publicly available datasets. We obtain an accuracy of 82.31% for a left vs. right hand in the Clinical BCI challenge (CBCIC) dataset, and a performance of 62.43% for the four-class case in the BCI Competition IV 2a dataset compared to a 82.15% and 60.56% using the arithmetic mean. Finally, we have also tested the goodness of our proposal against other MI-BCI systems, obtaining better results than those using other decision making schemes and Deep Learning on the same datasets.},
  archive      = {J_PR},
  author       = {J. Fumanal-Idocin and C. Vidaurre and J. Fernandez and M. Gómez and J. Andreu-Perez and M. Prasad and H. Bustince},
  doi          = {10.1016/j.patcog.2023.109924},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109924},
  shortjournal = {Pattern Recognition},
  title        = {Supervised penalty-based aggregation applied to motor-imagery based brain-computer-interface},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel interval dual convolutional neural network method
for interval-valued stock price prediction. <em>PR</em>, <em>145</em>,
109920. (<a href="https://doi.org/10.1016/j.patcog.2023.109920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate interval-valued stock price prediction is challenging and of great interest to investors and for-profit organizations. In this study, by considering individual stock information and relevant stock information simultaneously, we propose a novel interval dual convolutional neural network (Dual-CNN I I ) model based method to predict interval-valued stock prices. First, the individual and relevant stock information are collected and transformed into images. Then, the Dual-CNN I I model is proposed to predict interval-valued stock prices. Specifically, two convolutional neural network (CNN) models with different structures are constructed to respectively extract individual stock features and relevant stock features, and then an interval multilayer perceptron (MLP I I ) model is used for final interval-valued stock price prediction. Finally, extensive experiments are conducted based on six randomly selected stocks, with comparison to several popular machine learning model based methods and interval-valued time series (ITS) prediction methods. The experimental results indicate that the proposed Dual-CNN I I based method has superior predictive ability .},
  archive      = {J_PR},
  author       = {Manrui Jiang and Wei Chen and Huilin Xu and Yanxin Liu},
  doi          = {10.1016/j.patcog.2023.109920},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109920},
  shortjournal = {Pattern Recognition},
  title        = {A novel interval dual convolutional neural network method for interval-valued stock price prediction},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mutual domain adaptation. <em>PR</em>, <em>145</em>, 109919.
(<a href="https://doi.org/10.1016/j.patcog.2023.109919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve the label sparsity problem, domain adaptation has been well-established, suggesting various methods such as finding a common feature space of different domains using projection matrices or neural networks. Despite recent advances, domain adaptation is still limited and is not yet practical. The most pronouncing problem is that the existing approaches assume source-target relationship between domains, which implies one domain supplies label information to another domain. However, the amount of label is only marginal in real-world domains, so it is unrealistic to find source domains having sufficient labels. Motivated by this, we propose a method that allows domains to mutually share label information. The proposed method finds a projection matrix that matches the respective distributions of different domains, preserves their respective geometries, and aligns their respective class boundaries. The experiments on benchmark datasets show that the proposed method outperforms relevant baselines. In particular, the results on varying proportions of labels present that the fewer labels the better improvement.},
  archive      = {J_PR},
  author       = {Sunghong Park and Myung Jun Kim and Kanghee Park and Hyunjung Shin},
  doi          = {10.1016/j.patcog.2023.109919},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109919},
  shortjournal = {Pattern Recognition},
  title        = {Mutual domain adaptation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InvFlow: Involution and multi-scale interaction for
unsupervised learning of optical flow. <em>PR</em>, <em>145</em>,
109918. (<a href="https://doi.org/10.1016/j.patcog.2023.109918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convolution neural network is still the main tool for extracting the image features and the motion features for most of the optical flow models. The convolution neural networks cannot model the long-range dependencies, and more details are lost in deeper layers. All the deficiencies in the extracted features affect the estimated flow. Therefore, in this work, we concentrated on optimizing the convolution neural network in both the encoder and decoder parts to improve the image and motion features. To enhance the image features, we utilize the involution to provide rich features and model the long-range dependencies. In addition, we propose a Multi-Scale-Interaction module which utilizes the self-attention to make an interaction between the feature scales to avoid detail loss. Additionally, we propose a Motion-Features-Optimization block that utilizes the deformable convolution to enhance the motion features. Our model achieves the state-of-the-art performance on Sintel and KITTI 2015 benchmarks.},
  archive      = {J_PR},
  author       = {Xuezhi Xiang and Rokia Abdein and Ning Lv and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.patcog.2023.109918},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109918},
  shortjournal = {Pattern Recognition},
  title        = {InvFlow: Involution and multi-scale interaction for unsupervised learning of optical flow},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multi-agent reinforcement learning via bayesian
distributional value estimation. <em>PR</em>, <em>145</em>, 109917. (<a
href="https://doi.org/10.1016/j.patcog.2023.109917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning in multi-agent scenarios is essential for real-world applications as it can vividly depict agents’ collaborative and competitive behaviors from a perspective closer to reality. However, most existing studies suffer from poor robustness, preventing multi-agent reinforcement learning from practical applications where robustness is the core indicator of system security and stability. In view of this, we propose a novel B ayesian M ulti- A gent R einforcement L earning method, named BMARL, which leverages the distributional value function calculated by Bayesian inference to improve the robustness of the model. Specifically, Bayesian linear regression is adopted to estimate a posterior distribution concerning value function parameters, rather than approximating an expectation value for Q-value by point estimation. In this way, the value function is more generalized than previously obtained by point estimation, which is beneficial to the robustness of our model. Meanwhile, we utilize the Gaussian prior knowledge to integrate more prior knowledge while estimating the value function, which improves learning efficiency. Extensive experimental results on three benchmark multi-agent environments comparing with seven state-of-the-art methods demonstrate the superiority of BMARL in terms of both robustness and efficiency.},
  archive      = {J_PR},
  author       = {Xinqi Du and Hechang Chen and Che Wang and Yongheng Xing and Jielong Yang and Philip S. Yu and Yi Chang and Lifang He},
  doi          = {10.1016/j.patcog.2023.109917},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109917},
  shortjournal = {Pattern Recognition},
  title        = {Robust multi-agent reinforcement learning via bayesian distributional value estimation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Compositional zero-shot learning using multi-branch graph
convolution and cross-layer knowledge sharing. <em>PR</em>,
<em>145</em>, 109916. (<a
href="https://doi.org/10.1016/j.patcog.2023.109916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of the Compositional Zero-Shot Learning (CZSL) is to recognize new state-object compositions of known objects and known states. For example, the CZSL model should recognize young cat when the model has seen images of a few state-object compositions like young tiger , old tiger and old cat . The visual features of a state may have significant variation across different compositions of the state with different objects. For example, in the compositions peeled apple and peeled orange , the state peeled has different visual features. This context dependency of state features is difficult to learn from the annotated images of different compositions. We propose a Graph Convolutional Network (GCN) with two distinct branches for object and state recognition. GCN utilizes its ability to aggregate features from the non-Euclidean neighbourhood. This aggregation ability of GCN can help our model to capture the intricate dependencies between visual features of state and object. We also propose a novel cross-layer knowledge sharing strategy for the purpose of reducing ambiguity in learning state features due to context dependency. The proposed cross-layer knowledge sharing helps in identifying a set of objects having feasible compositions with a particular state and thereby reducing the ambiguity in the state features. Finally, we propose a feasibility based penalization to better regularize the joint prediction from the two branches of the network. The proposed algorithm is evaluated on the challenging benchmarks and competitive results in comparison to state-of-the-art algorithms have been achieved.},
  archive      = {J_PR},
  author       = {Aditya Panda and Dipti Prasad Mukherjee},
  doi          = {10.1016/j.patcog.2023.109916},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109916},
  shortjournal = {Pattern Recognition},
  title        = {Compositional zero-shot learning using multi-branch graph convolution and cross-layer knowledge sharing},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised cross-modal visual retrieval from brain
activities. <em>PR</em>, <em>145</em>, 109915. (<a
href="https://doi.org/10.1016/j.patcog.2023.109915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of restoring visual stimuli from visually-evoked electroencephalography (EEG) signals. Using a supervised classification-then-generation framework, the reconstruction -based approaches learn the mapping between distributions of two modalities but fail to reproduce the exact visual stimulus. Instead, we propose a self-supervised cross-modal retrieval paradigm that seeks instance-level alignment by maximizing the mutual information between the EEG encoding and associated visual stimulus. We demonstrate the threefold advantages of the self-supervised retrieval over supervised reconstruction on the largest visual-evoked EEG dataset with two evaluation protocols. First, it restores the exact visual stimulus without accessing the image class information, which was not possible with previous approaches. Second, it produces more recognizable results than generated ones and bypasses the challenge of training an image generator. Finally, it illustrates the benefits of self-supervision over supervised models in handling open-set data.},
  archive      = {J_PR},
  author       = {Zesheng Ye and Lina Yao and Yu Zhang and Sylvia Gustin},
  doi          = {10.1016/j.patcog.2023.109915},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109915},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised cross-modal visual retrieval from brain activities},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual camera relocalization using both hand-crafted and
learned features. <em>PR</em>, <em>145</em>, 109914. (<a
href="https://doi.org/10.1016/j.patcog.2023.109914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The localization of the camera is essential in AR , MR, and robotics. Diverse pipelines employ a hand-crafted or learning based way to predict the camera pose as per the task. In the localization process, both weaknesses and strengths are maintained. However, few current frameworks consider these two features simultaneously. In this study, a novel relocalization pipeline for RGB or RGB-D input is proposed, including a coarse stage with learned features, further refinement with hand-crafted features, and a stable process to measure the confidence of both stages for improving localization robustness. Instead of directly regressing the camera pose, the coarse procedure uses registration to the known source and predicted weighted target point cloud to obtain the initial result. Therefore, we design a deep network called PGNet to construct the weighted target point cloud with the image and previous poses as inputs. Moreover, in consideration of dynamic surroundings, we add a segmentation branch distinguishing each point as either fixed or dynamic with the purpose of promoting dynamic perception. Correspondingly, the segmentation-extended Chamfer Distance is added to optimize PGNet. During the pose refinement, the feature space is established via hand-crafted feature extraction and matching on the training set. Based on the coarse pose, we obtain the accurate pose by applying Kabsch or Perspective-n-Point (PnP) algorithm to point-to-point correspondences built through searching the space and matching Oriented Fast and Rotated Brief (ORB) features. Furthermore, an additional process is presented by defining coarse and refinement metrics to gain a more stable performance. Finally, experiments on both static and dynamic scenes are conducted. On the one side, the results demonstrate the state-of-the-art performance over other existing methods on 7 Scenes, INDOOR-6, Cambridge Landmarks and TUM RGB-D. On the other side, the positive effects of the pose learning part, dynamic branch, confidence regression and hand-crafted feature based refinement are also provided.},
  archive      = {J_PR},
  author       = {Junyi Wang and Yue Qi},
  doi          = {10.1016/j.patcog.2023.109914},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109914},
  shortjournal = {Pattern Recognition},
  title        = {Visual camera relocalization using both hand-crafted and learned features},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ICAFusion: Iterative cross-attention guided feature fusion
for multispectral object detection. <em>PR</em>, <em>145</em>, 109913.
(<a href="https://doi.org/10.1016/j.patcog.2023.109913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective feature fusion of multispectral images plays a crucial role in multispectral object detection. Previous studies have demonstrated the effectiveness of feature fusion using convolutional neural networks , but these methods are sensitive to image misalignment due to the inherent deficiency in local-range feature interaction resulting in the performance degradation . To address this issue, a novel feature fusion framework of dual cross-attention transformers is proposed to model global feature interaction and capture complementary information across modalities simultaneously. This framework enhances the discriminability of object features through the query-guided cross-attention mechanism, leading to improved performance. However, stacking multiple transformer blocks for feature enhancement incurs a large number of parameters and high spatial complexity. To handle this, inspired by the human process of reviewing knowledge, an iterative interaction mechanism is proposed to share parameters among block-wise multimodal transformers, reducing model complexity and computation cost. The proposed method is general and effective to be integrated into different detection frameworks and used with different backbones. Experimental results on KAIST, FLIR, and VEDAI datasets show that the proposed method achieves superior performance and faster inference, making it suitable for various practical scenarios. Code will be available at https://github.com/chanchanchan97/ICAFusion .},
  archive      = {J_PR},
  author       = {Jifeng Shen and Yifei Chen and Yue Liu and Xin Zuo and Heng Fan and Wankou Yang},
  doi          = {10.1016/j.patcog.2023.109913},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109913},
  shortjournal = {Pattern Recognition},
  title        = {ICAFusion: Iterative cross-attention guided feature fusion for multispectral object detection},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PSCFormer: A lightweight hybrid network for gas
identification in electronic nose system. <em>PR</em>, <em>145</em>,
109912. (<a href="https://doi.org/10.1016/j.patcog.2023.109912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on their powerful feature extraction capability, a convolutional neural network (CNN) has been gradually applied to gas identification in the electronic nose (e-nose) system. The responses of different intensities in the e-nose system are significantly correlated, and CNN extracts the local gas features by convolution while ignoring their global correlation. Transformer combines different responses and obtains the correlation between global features by self-attention. This paper proposes a lightweight hybrid network called Peak Search-based Convolutional Transformers (PSCFormer). First, combining the data characteristics of gas information, the Local Peak Search and Feature Fusion (LPSF) module is proposed to focus on the key gas features. Second, Transformer Encoder (TE) is proposed to obtain the global correlation between global features, and the parallel Convolution Encoder (CE) is proposed to capture the local dependence. Finally, a reasonable feature complementation mechanism is presented, and the preference of TE is alleviated for the slow-down response while solving the receptive field limitation of CE. This paper has evaluated three different datasets to validate the effectiveness of PSCFormer, all of which show stable and excellent performance with a good tradeoff between efficiency and complexity. The results prove that PSCFormer is an efficient and lightweight gas identification network, which provides a method to promote the engineering application of the e-nose system.},
  archive      = {J_PR},
  author       = {Ziyang Li and Siyuan Kang and Ninghui Feng and Chongbo Yin and Yan Shi},
  doi          = {10.1016/j.patcog.2023.109912},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109912},
  shortjournal = {Pattern Recognition},
  title        = {PSCFormer: A lightweight hybrid network for gas identification in electronic nose system},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring global information for session-based
recommendation. <em>PR</em>, <em>145</em>, 109911. (<a
href="https://doi.org/10.1016/j.patcog.2023.109911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation (SBR) aims to recommend items based on anonymous behavior sequences. However, most existing SBR approaches focus solely on the current session while neglecting the item-transition information from other sessions, which suffer from the inability of modeling the complicated item-transition. To address the limitations, we introduce global item-transition information to augment the modeling of item-transitions. Specifically, we first propose a basic GNN-based framework (BGNN), which solely uses session-level item-transition information. Based on BGNN, we propose a novel approach, called S ession-based R ecommendation with G lobal I nformation ( SRGI ), which infers the user preferences via fully exploring item-transitions over all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM) , which recursively incorporates the neighbor embeddings of each node on global graph into the learning process of item representation; and (ii) Constrained-based Model (SRGI-CM) , which treats the global-level information as a constraint to ensure the learned item embeddings are consistent with the global item-transition. Extensive experiments conducted on three popular benchmark datasets demonstrate that both SRGI-FM and SRGI-CM outperform the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Ziyang Wang and Wei Wei and Ding Zou and Yifan Liu and Xiao-Li Li and Xian-Ling Mao and Minghui Qiu},
  doi          = {10.1016/j.patcog.2023.109911},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109911},
  shortjournal = {Pattern Recognition},
  title        = {Exploring global information for session-based recommendation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering performance analysis using a new
correlation-based cluster validity index. <em>PR</em>, <em>145</em>,
109910. (<a href="https://doi.org/10.1016/j.patcog.2023.109910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are various cluster validity indices used for evaluating clustering results . One of the main objectives of using these indices is to seek the optimal unknown number of clusters. Some indices work well for clusters with different densities, sizes, and shapes. Yet, one shared weakness of those validity indices is that they often provide only one optimal number of clusters. That number is unknown in real-world problems, and there might be more than one possible option. We develop a new cluster validity index based on a correlation between an actual distance between a pair of data points and a centroid distance of clusters that the two points occupy. Our proposed index constantly yields several local peaks and overcomes the previously stated weakness. Several experiments in different scenarios, including UCI real-world data sets, have been conducted to compare the proposed validity index with several well-known ones.},
  archive      = {J_PR},
  author       = {Nathakhun Wiroonsri},
  doi          = {10.1016/j.patcog.2023.109910},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109910},
  shortjournal = {Pattern Recognition},
  title        = {Clustering performance analysis using a new correlation-based cluster validity index},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MHCanonNet: Multi-hypothesis canonical lifting network for
self-supervised 3D human pose estimation in the wild video. <em>PR</em>,
<em>145</em>, 109908. (<a
href="https://doi.org/10.1016/j.patcog.2023.109908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 3D Human Pose Estimation using fully-supervised learning approach have shown impressive results; however, these methods heavily rely on large amounts of annotated 3D data, which are challenging to obtain outside controlled laboratory environments. Therefore, in this study, we propose a new self-supervised training method designed to train a 3D human pose estimation network using unlabeled multi-view images. The model trains relative depths between joints without any 3D annotation by satisfying multi-view consistency constraints from unlabeled multi-view videos without camera calibration , while simultaneously learning representations of multiple plausible pose hypotheses. For this reason, we call our proposed network a Multi-Hypothesis Canonical Lifting Network (MHCanonNet). By enriching the diversity of extracted features and keeping various possibilities open, our network accurately estimates the final 3D pose. The key idea lies in the design of a novel and unbiased reconstruction objective function that combines multiple hypotheses from different viewpoints. The proposed approach demonstrates state-of-the-art results not only on two popular benchmark datasets, Human3.6M and MPI-INF-3DHP but also on an in-the-wild dataset, Ski-Pose, surpassing existing self-supervised training methods.},
  archive      = {J_PR},
  author       = {Hyun-Woo Kim and Gun-Hee Lee and Woo-Jeoung Nam and Kyung-Min Jin and Tae-Kyung Kang and Geon-Jun Yang and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2023.109908},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109908},
  shortjournal = {Pattern Recognition},
  title        = {MHCanonNet: Multi-hypothesis canonical lifting network for self-supervised 3D human pose estimation in the wild video},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-scale contrastive triplet networks for graph
representation learning. <em>PR</em>, <em>145</em>, 109907. (<a
href="https://doi.org/10.1016/j.patcog.2023.109907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph representation learning aims to learn low-dimensional representation for the graph, which has played a vital role in real-world applications. Without requiring additional labeled data, contrastive learning based graph representation learning (or graph contrastive learning) has attracted considerable attention. Recently, one of the most exciting advancement in graph contrastive learning is Deep Graph Infomax (DGI), which maximizes the Mutual Information (MI) between the node and graph representations . However, DGI only considers the contextual node information, ignoring the intrinsic node information (i.e., the similarity between node representations in different views). In this paper, we propose a novel Cross-scale Contrastive Triplet Networks (CCTN) framework, which captures both contextual and intrinsic node information for graph representation learning. Specifically, to obtain the contextual node information, we utilize an infomax contrastive network to maximize the MI between node and graph representations. For acquiring the intrinsic node information, we present a Siamese contrastive network by maximizing the similarity between node representations in different augmented views. Two contrastive networks learn together through a shared graph convolution network to form our cross-scale contrastive triplet networks. Finally, we evaluate CCTN on six real-world datasets. Extensive experimental results demonstrate that CCTN achieves state-of-the-art performance on node classification and clustering tasks .},
  archive      = {J_PR},
  author       = {Yanbei Liu and Wanjin Shan and Xiao Wang and Zhitao Xiao and Lei Geng and Fang Zhang and Dongdong Du and Yanwei Pang},
  doi          = {10.1016/j.patcog.2023.109907},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109907},
  shortjournal = {Pattern Recognition},
  title        = {Cross-scale contrastive triplet networks for graph representation learning},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global semantic enhancement network for video captioning.
<em>PR</em>, <em>145</em>, 109906. (<a
href="https://doi.org/10.1016/j.patcog.2023.109906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning aims to briefly describe the content of a video in accurate and fluent natural language, which is a hot research topic in multimedia processing. As a bridge between video and natural language, video captioning is a challenging task that requires a deep understanding of video content and effective utilization of diverse video multimodal information. Existing video captioning methods usually ignore the relative importance between different frames when aggregating frame-level video features and neglect the global semantic correlations between videos and texts in learning visual representations, resulting in the learned representations less effective. To address these problems, we propose a novel framework, namely Global Semantic Enhancement Network (GSEN) to generate high-quality captions for videos. Specifically, a feature aggregation module based on a lightweight attention mechanism is designed to aggregate frame-level video features, which highlights features of informative frames in video representations. In addition, a global semantic enhancement module is proposed to enhance semantic correlations for video and language representations in order to generate semantically more accurate captions. Extensive qualitative and quantitative experiments on two public benchmark datasets MSVD and MSR-VTT demonstrate that the proposed GSEN can achieve superior performance than state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xuemei Luo and Xiaotong Luo and Di Wang and Jinhui Liu and Bo Wan and Lin Zhao},
  doi          = {10.1016/j.patcog.2023.109906},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109906},
  shortjournal = {Pattern Recognition},
  title        = {Global semantic enhancement network for video captioning},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relative-position embedding based spatially and temporally
decoupled transformer for action recognition. <em>PR</em>, <em>145</em>,
109905. (<a href="https://doi.org/10.1016/j.patcog.2023.109905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of human actions is to classify actions in a video. Recently, Vision Transformer (ViT) has been applied to action recognition. However, the Vision Transformer is unsuitable for high-resolution input videos due to the constraint of computing power since ViT splits frames into fixed-size patches embedded ( i.e ., tokens) with absolute-position information and adopts a pure Transformer encoder to model the relationships among these tokens. To address this issue, we propose a relative-position embedding based spatially and temporally decoupled Transformer (RPE-STDT) for action recognition, which can capture spatial–temporal information by stacked self-attention layers. The proposed RPE-STDT model consists of two separate series of Transformer encoders. The first series of encoders is the spatial Transformer encoders, which model interactions between tokens extracted from the same temporal index. The second series of encoders is the temporal Transformer encoders, which model interactions across time dimensions with a subsampling strategy. Furthermore, we replace the absolute-position embeddings in the Vision Transformer encoders with the proposed relative-position embeddings to capture the order of the embedded tokens to reduce computational costs. Finally, we conduct thorough ablation studies. Our RPE-STDT achieves state-of-the-art results on multiple action recognition datasets, exceeding prior convolution and Transformer-based networks.},
  archive      = {J_PR},
  author       = {Yujun Ma and Ruili Wang},
  doi          = {10.1016/j.patcog.2023.109905},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109905},
  shortjournal = {Pattern Recognition},
  title        = {Relative-position embedding based spatially and temporally decoupled transformer for action recognition},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot classification guided by generalization error
bound. <em>PR</em>, <em>145</em>, 109904. (<a
href="https://doi.org/10.1016/j.patcog.2023.109904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transfer learning has generated promising performance in few-shot classification by pre-training a backbone network on base classes and then applying it to novel classes. Nevertheless, there lacks a theoretical analysis on how to reduce the generalization error during the learning process. To fill this gap, we prove that the classification error bound on novel classes is mainly determined by the base-class generalization error, given the base-novel domain divergence and the novel-class generalization error produced by an incremental learner using novel samples. The novel-class generalization error is further decided by the base-class empirical error and the VC-dimension of the hypothesis space. Based on this theoretical analysis, we propose a Born-Again Networks under Self-supervised Label Augmentation (BANs-SLA) method to improve the generalization capability of classifiers . In this method, cross-entropy and supervised contrastive losses are simultaneously used to minimize the base-class empirical error in the expanded space with SLA. Afterward, BANs are adopted to transfer the knowledge sequentially across generations, which acts as an effective regularizer to trade-off the VC-dimension. Extensive experimental results have verified the effectiveness of our method, which establishes the new state-of-the-art performance on popular few-shot classification benchmark datasets.},
  archive      = {J_PR},
  author       = {Fan Liu and Sai Yang and Delong Chen and Huaxi Huang and Jun Zhou},
  doi          = {10.1016/j.patcog.2023.109904},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109904},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot classification guided by generalization error bound},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable frame resolution for efficient continuous sign
language recognition. <em>PR</em>, <em>145</em>, 109903. (<a
href="https://doi.org/10.1016/j.patcog.2023.109903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore the spatial redundancy in continuous sign language recognition (CSLR), aiming to improve its efficiency. Despite recent advances in accuracy in CSLR, state-of-the-art CSLR methods typically require large amounts of computations and memory occupation, which are not friendly towards fast inference under limited computation/memory budgets. Based on a simple observation that not all frames are equally important for CSLR, we propose AdaSize to handle this problem by modeling the frame resolution decision as an end-to-end learnable task to save unnecessary computations. Specifically, a lightweight 2D convolutional neural network (CNN) is first used to quickly browse input frames under a low resolution (e.g., 112 × 112). These extracted coarse and cheap features are sent into a recurrent policy network to dynamically determine the desired resolution for each frame. Once the optimal resolution for each frame is decided, frames with different resolutions are fed into the following backbones to extract representative features. Finally, these features pass through a sequence of temporal modules and a classifier to predict sentences. Extensive experiments on four large-scale datasets, including PHOENIX14, PHOENIX14-T, CSL-Daily and CSL, demonstrate the effectiveness of AdaSize. AdaSize could consistently achieve comparable accuracy with state-of-the-art CSLR methods, with only 0.38 × × computations, 0.41 × × memory usage and 1.25 × × throughput. Comparisons with commonly-used lightweight backbones and other efficient methods verify the superiority of AdaSize under similar computational/memory budgets. We finally plot the frame resolution decisions for AdaSize, hoping to provide insightful analysis of the inherent spatial redundancy in videos.},
  archive      = {J_PR},
  author       = {Lianyu Hu and Liqing Gao and Zekang Liu and Wei Feng},
  doi          = {10.1016/j.patcog.2023.109903},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109903},
  shortjournal = {Pattern Recognition},
  title        = {Scalable frame resolution for efficient continuous sign language recognition},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Jacobian norm with selective input gradient regularization
for interpretable adversarial defense. <em>PR</em>, <em>145</em>,
109902. (<a href="https://doi.org/10.1016/j.patcog.2023.109902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) can be easily deceived by imperceptible alterations known as adversarial examples . These examples can lead to misclassification , posing a significant threat to the reliability of deep learning systems in real-world applications. Adversarial training (AT) is a popular technique used to enhance robustness by training models on a combination of corrupted and clean data. However, existing AT-based methods often struggle to handle transferred adversarial examples that can fool multiple defense models, thereby falling short of meeting the generalization requirements for real-world scenarios. Furthermore, AT typically fails to provide interpretable predictions, which are crucial for domain experts seeking to understand the behavior of DNNs. To overcome these challenges, we present a novel approach called Jacobian norm and Selective Input Gradient Regularization (J-SIGR). Our method leverages Jacobian normalization to improve robustness and introduces regularization of perturbation-based saliency maps, enabling interpretable predictions. By adopting J-SIGR, we achieve enhanced defense capabilities and promote high interpretability of DNNs. We evaluate the effectiveness of J-SIGR across various architectures by subjecting it to powerful adversarial attacks. Our experimental evaluations provide compelling evidence of the efficacy of J-SIGR against transferred adversarial attacks, while preserving interpretability. The project code can be found at https://github.com/Lywu-github/jJ-SIGR.git .},
  archive      = {J_PR},
  author       = {Deyin Liu and Lin Yuanbo Wu and Bo Li and Farid Boussaid and Mohammed Bennamoun and Xianghua Xie and Chengwu Liang},
  doi          = {10.1016/j.patcog.2023.109902},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109902},
  shortjournal = {Pattern Recognition},
  title        = {Jacobian norm with selective input gradient regularization for interpretable adversarial defense},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mask-guided multiscale feature aggregation network for hand
gesture recognition. <em>PR</em>, <em>145</em>, 109901. (<a
href="https://doi.org/10.1016/j.patcog.2023.109901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition from images is a longstanding computer vision task that can be used to build a potential bridge for human-computer interaction and sign language translation. For number of methods proposed for hand gesture recognition (HGR); however, difficult scenarios such as different scales of hand gestures and complex backgrounds exist, making them less effective. In this paper, we propose an end-to-end multiscale feature learning network for HGR, which consists of a CNN-based backbone network, a feature aggregation pyramid network (FAPN) embedded with a two-stage expansion-squeeze-aggregation (ESA) module, and three task-specific prediction branches. First, the backbone network extracts multiscale features from the original hand gesture images. Furthermore, the FAPN embedded with two-stage ESA extensively exploits multiscale feature information and learns hand gesture-specific features at different scales. Then, the mask loss guides the network to locate hand-specific regions during the training stage, and finally, the classification and regression branches output the category and location of a hand gesture during the model training and prediction. The experimental results on two publicly available datasets show that the proposed method outperforms most state-of-the-art HGR methods.},
  archive      = {J_PR},
  author       = {Hao Liang and Lunke Fei and Shuping Zhao and Jie Wen and Shaohua Teng and Yong Xu},
  doi          = {10.1016/j.patcog.2023.109901},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109901},
  shortjournal = {Pattern Recognition},
  title        = {Mask-guided multiscale feature aggregation network for hand gesture recognition},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient-based multi-label feature selection considering
three-way variable interaction. <em>PR</em>, <em>145</em>, 109900. (<a
href="https://doi.org/10.1016/j.patcog.2023.109900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Multi-Label Feature Selection (MLFS) attracts more and more attention to tackle the high-dimensional problem in multi-label data. A key characteristic of existing gradient-based MLFS methods is that they typically consider two-way variable correlations between features and labels, including feature-feature and label-label correlations. However, two-way correlations are not sufficient to steer feature selection since such correlations vary given different additional variables in practical scenarios, which leads to the selected features with relatively-poor classification performance. Motivated by this, we capture three-way variable interactions including feature-feature-label and feature-label-label interactions to further characterize the fluctuating correlations in the context of another variable, and propose a new gradient-based MLFS approach incorporating the above three-way variable interactions into a global optimization objective. Specifically, based on information theory , we develop second-order regularization penalty terms to regard three-way interactions while jointly combining with the main loss term in regard to feature relevance. Then the objective function can be efficiently optimized via a block-coordinate gradient descent schema. Meanwhile, we provide a theoretical analysis demonstrating the effectiveness of the regularization terms in exploiting three-way interaction. In addition, experiments conducted on a series of benchmark data sets also verify the validity of the proposed method on multiple evaluation metrics .},
  archive      = {J_PR},
  author       = {Yizhang Zou and Xuegang Hu and Peipei Li},
  doi          = {10.1016/j.patcog.2023.109900},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109900},
  shortjournal = {Pattern Recognition},
  title        = {Gradient-based multi-label feature selection considering three-way variable interaction},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning correlation information for multi-label feature
selection. <em>PR</em>, <em>145</em>, 109899. (<a
href="https://doi.org/10.1016/j.patcog.2023.109899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world multi-label applications, the content of multi-label data is usually characterized by high dimensional features, which contains complex correlation information , i.e., label correlations and redundant features. To alleviate the problem, we present a novel scheme, called learning correlation information for multi-label feature selection (LCIFS) method, by jointly digging up label correlations and controlling feature redundancy. To be specific, the regression model via manifold framework is presented to fit the relationship between feature space and label distribution, during which adaptive spectral graph is leveraged to learn more precise structural correlations of labels simultaneously. Besides, we utilize the relevance of features to constrain the redundancy of the generated feature subset, and a general ℓ 2 , p ℓ2,p -norm regularized model is employed to fulfill more robust feature selection. The proposed method is transformed into an explicit optimization function, which is conquered by an efficient iterative optimization algorithm . Finally, we conduct comprehensive experiments on twelve realistic multi-label datasets, including text domain, image domain, and audio domain. The statistic results demonstrate the effectiveness and superiority of the proposed method among nine competition methods.},
  archive      = {J_PR},
  author       = {Yuling Fan and Jinghua Liu and Jianeng Tang and Peizhong Liu and Yaojin Lin and Yongzhao Du},
  doi          = {10.1016/j.patcog.2023.109899},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109899},
  shortjournal = {Pattern Recognition},
  title        = {Learning correlation information for multi-label feature selection},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PAMI: Partition input and aggregate outputs for model
interpretation. <em>PR</em>, <em>145</em>, 109898. (<a
href="https://doi.org/10.1016/j.patcog.2023.109898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing demand for interpretation of model predictions especially in high-risk applications. Various visualization approaches have been proposed to estimate the part of input which is relevant to a specific model prediction. However, most approaches require model structure and parameter details in order to obtain the visualization results, and in general much effort is required to adapt each approach to multiple types of tasks particularly when model backbone and input format change over tasks. In this study, a simple yet effective visualization framework called PAMI is proposed based on the observation that deep learning models often aggregate features from local regions for model predictions. The basic idea is to mask majority of the input and use the corresponding model output as the relative contribution of the preserved input part to the original model prediction. For each input, since only a set of model outputs are collected and aggregated, PAMI does not require any model detail and can be applied to various prediction tasks with different model backbones and input formats. Extensive experiments on multiple tasks confirm the proposed method performs better than existing visualization approaches in more precisely finding class-specific input regions, and when applied to different model backbones and input formats. The source code is available at https://github.com/fuermowei/PAMI .},
  archive      = {J_PR},
  author       = {Wei Shi and Wentao Zhang and Wei-shi Zheng and Ruixuan Wang},
  doi          = {10.1016/j.patcog.2023.109898},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109898},
  shortjournal = {Pattern Recognition},
  title        = {PAMI: Partition input and aggregate outputs for model interpretation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse self-attention transformer for image inpainting.
<em>PR</em>, <em>145</em>, 109897. (<a
href="https://doi.org/10.1016/j.patcog.2023.109897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based image inpainting methods have made remarkable progress in recent years. Nevertheless, these methods still suffer from issues such as blurring, artifacts, and inconsistent contents. The use of vanilla convolution kernels , which have limited perceptual fields and spatially invariant kernel coefficients, is one of the main causes for these problems. In contrast, the multi-headed attention in the transformer can effectively model non-local relations among input features by generating adaptive attention scores . Therefore, this paper explores the feasibility of employing the transformer model for the image inpainting task. However, the multi-headed attention transformer blocks pose a significant challenge due to their overwhelming computational cost. To address this issue, we propose a novel U-Net style transformer-based network for the inpainting task, called the sparse self-attention transformer (Spa-former). The Spa-former retains the long-range modeling capacity of transformer blocks while reducing the computational burden. It incorporates a new channel attention approximation algorithm that reduces attention calculation to linear complexity. Additionally, it replaces the canonical softmax function with the ReLU function to generate a sparse attention map that effectively excludes irrelevant features. As a result, the Spa-former achieves effective long-range feature modeling with fewer parameters and lower computational resources . Our empirical results on challenging benchmarks demonstrate the superior performance of our proposed Spa-former over state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Wenli Huang and Ye Deng and Siqi Hui and Yang Wu and Sanping Zhou and Jinjun Wang},
  doi          = {10.1016/j.patcog.2023.109897},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109897},
  shortjournal = {Pattern Recognition},
  title        = {Sparse self-attention transformer for image inpainting},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning geometric consistency and discrepancy for
category-level 6D object pose estimation from point clouds. <em>PR</em>,
<em>145</em>, 109896. (<a
href="https://doi.org/10.1016/j.patcog.2023.109896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Category-level 6D object pose estimation aims to predict the position and orientation of unseen object instances, which is a fundamental problem in robotic applications . Previous works mainly focused on exploiting visual cues from RGB images , while depth images received less attention. However, depth images contain rich geometric attributes about the object’s shape, which are crucial for inferring the object’s pose. This work achieves category-level 6D object pose estimation by performing sufficient geometric learning from depth images represented by point clouds. Specifically, we present a novel geometric consistency and geometric discrepancy learning framework called CD-Pose to resolve the intra-category variation, inter-category similarity, and objects with complex structures. Our network consists of a Pose-Consistent Module and a Pose-Discrepant Module. First, a simple MLP-based Pose-Consistent Module is utilized to extract geometrically consistent pose features of objects from the pre-computed object shape priors for each category. Then, the Pose-Discrepant Module, designed as a multi-scale region-guided transformer network, is dedicated to exploring each instance’s geometrically discrepant features. Next, the NOCS model of the object is reconstructed according to the integration of consistent and discrepant geometric representations . Finally, 6D object poses are obtained by solving the similarity transformation between the reconstruction and the observed point cloud. Experiments on the benchmark datasets show that our CD-Pose produces superior results to state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Lu Zou and Zhangjin Huang and Naijie Gu and Guoping Wang},
  doi          = {10.1016/j.patcog.2023.109896},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109896},
  shortjournal = {Pattern Recognition},
  title        = {Learning geometric consistency and discrepancy for category-level 6D object pose estimation from point clouds},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast multi-view subspace clustering with balance anchors
guidance. <em>PR</em>, <em>145</em>, 109895. (<a
href="https://doi.org/10.1016/j.patcog.2023.109895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering (MVSC) has acquired satisfactory clustering performance since it effectively integrates the information from multiple views. However, existing MVSC methods often suffer from high time costs and are difficult to be used in real-life large-scale data. Anchor-based MVSC methods have been presented to select crucial landmarks to reduce time-consuming effectively. In addition, the processes of anchor selection of existing methods are performed in the raw space, in which the high-dimensional data often involve lots of noise information and outliers that inevitably lead to the degradation of clustering performance. Moreover, these methods also ignore the balance structure of data, such that the selected anchors cannot fully characterize the intrinsic structure information of the original data. To tackle the aforementioned issues, we present a novel MVSC method named Fast Multi-view Subspace Clustering with Balance Anchors Guidance (FMVSC-BAG). Specifically, FMVSC-BAG integrates the learning processes of anchors, anchor graphs, and labels into a united framework in embedding space seamlessly. This way, they can reinforce each other to improve final clustering performance while eliminating noise and outliers hidden in the original data. Furthermore, FMVSC-BAG constrains the learned labels to preserve the balance structure by a novel balance strategy to promote further that the intrinsic balance structure information of original data can be reserved in the learned anchors and anchor graph. Finally, extensive experiments on eight real-life large-scale datasets prove its efficiency and superiority compared to some advanced clustering methods.},
  archive      = {J_PR},
  author       = {Yong Mi and Hongmei Chen and Zhong Yuan and Chuan Luo and Shi-Jinn Horng and Tianrui Li},
  doi          = {10.1016/j.patcog.2023.109895},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109895},
  shortjournal = {Pattern Recognition},
  title        = {Fast multi-view subspace clustering with balance anchors guidance},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust embedding regression for semi-supervised learning.
<em>PR</em>, <em>145</em>, 109894. (<a
href="https://doi.org/10.1016/j.patcog.2023.109894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To utilize both labeled data and unlabeled data in real-world applications, semi-supervised learning is widely used as an effective technique. However, most semi-supervised methods do not perform well when there are many noises and redundant information in the original data. To address these issues, in this paper, we proposed a novel approach called robust embedding regression (RER) for semi-supervised learning by inheriting the advantages of the existing semi-supervised learning, robust linear regression, and low-rank representation techniques. Specifically, RER constructs a more robust and accurate graph by adaptively arranging the weight coefficient for each data point. Furthermore, the low-rank representation is introduced to reduce the negative influence of the redundant features and noises residing in the original data while the graph construction . More importantly, the proper norms are imposed on both the reconstruction and regularization terms to further improve the robustness and earn feature/sample selection. We designed an effective iterative algorithm to optimize the problem of RER. Comprehensive experimental results conducted on both synthetic and real-world datasets indicate that RER is superior in classification and clustering performance and robust to different types of noise compared with the existing semi-supervised methods.},
  archive      = {J_PR},
  author       = {Jiaqi Bao and Mineichi Kudo and Keigo Kimura and Lu Sun},
  doi          = {10.1016/j.patcog.2023.109894},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109894},
  shortjournal = {Pattern Recognition},
  title        = {Robust embedding regression for semi-supervised learning},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Learning consensus-aware semantic knowledge for remote
sensing image captioning. <em>PR</em>, <em>145</em>, 109893. (<a
href="https://doi.org/10.1016/j.patcog.2023.109893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tremendous progresses have been made in remote sensing image captioning (RSIC) task in recent years, yet there still some unresolved problems: (1) facing the gap between the visual features and semantic concepts , (2) reasoning the higher-level relationships between semantic concepts. In this work, we focus on injecting high-level visual-semantic interaction into RSIC model. Firstly, the semantic concept extractor (SCE), end-to-end trainable, precisely captures the semantic concepts contained in the RSIs. In particular, the visual-semantic co-attention (VSCA) is designed to grain coarse concept-related regions and region-related concepts for multi-modal interaction. Furthermore, we incorporate the two types of attentive vectors with semantic-level relational features into a consensus exploitation (CE) block for learning cross-modal consensus-aware knowledge. The experiments on three benchmark data sets show the superiority of our approach compared with the reference methods.},
  archive      = {J_PR},
  author       = {Yunpeng Li and Xiangrong Zhang and Xina Cheng and Xu Tang and Licheng Jiao},
  doi          = {10.1016/j.patcog.2023.109893},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109893},
  shortjournal = {Pattern Recognition},
  title        = {Learning consensus-aware semantic knowledge for remote sensing image captioning},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimum description length clustering to measure meaningful
image complexity. <em>PR</em>, <em>145</em>, 109889. (<a
href="https://doi.org/10.1016/j.patcog.2023.109889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new image complexity metric. Existing complexity metrics cannot distinguish meaningful content from noise, and give a high score to white noise images, which contain no meaningful information. We use the minimum description length principle to determine the number of clusters and designate certain points as outliers and, hence, correctly assign white noise a low score. The presented method is a step towards humans’ ability to detect when data contain a meaningful pattern. It also has similarities to theoretical ideas for measuring meaningful complexity. We conduct experiments on seven different sets of images, which show that our method assigns the most accurate scores to all images considered. Additionally, comparing the different levels of the hierarchy of clusters can reveal how complexity manifests at different scales, from local detail to global structure. We then present ablation studies showing the contribution of the components of our method, and that it continues to assign reasonable scores when the inputs are modified in certain ways, including the addition of Gaussian noise and the lowering of the resolution. Code is available at https://github.com/Lou1sM/meaningful_image_complexity .},
  archive      = {J_PR},
  author       = {Louis Mahon and Thomas Lukasiewicz},
  doi          = {10.1016/j.patcog.2023.109889},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109889},
  shortjournal = {Pattern Recognition},
  title        = {Minimum description length clustering to measure meaningful image complexity},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ENInst: Enhancing weakly-supervised low-shot instance
segmentation. <em>PR</em>, <em>145</em>, 109888. (<a
href="https://doi.org/10.1016/j.patcog.2023.109888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address a weakly-supervised low-shot instance segmentation , an annotation-efficient training method to deal with novel classes effectively. Since it is an under-explored problem, we first investigate the difficulty of the problem and identify the performance bottleneck by conducting systematic analyses of model components and individual sub-tasks with a simple baseline model . Based on the analyses, we propose ENInst with sub-task enhancement methods: instance-wise mask refinement for enhancing pixel localization quality and novel classifier composition for improving classification accuracy . Our proposed method lifts the overall performance by enhancing the performance of each sub-task. We demonstrate that our ENInst is 7.5 times more efficient in achieving comparable performance to the existing fully-supervised few-shot models and even outperforms them at times.},
  archive      = {J_PR},
  author       = {Moon Ye-Bin and Dongmin Choi and Yongjin Kwon and Junsik Kim and Tae-Hyun Oh},
  doi          = {10.1016/j.patcog.2023.109888},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109888},
  shortjournal = {Pattern Recognition},
  title        = {ENInst: Enhancing weakly-supervised low-shot instance segmentation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network pruning via resource reallocation. <em>PR</em>,
<em>145</em>, 109886. (<a
href="https://doi.org/10.1016/j.patcog.2023.109886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is broadly recognized as an effective approach to obtain a small compact model through eliminating unimportant channels from a large cumbersome network. Contemporary methods typically perform iterative pruning procedure from the original over-parameterized model, which is both tedious and expensive especially when the pruning is aggressive. In this paper, we propose a simple yet effective channel pruning technique, termed network Pruning via rEsource rEalLocation (PEEL), to quickly produce a desired slim model with negligible cost. Specifically, PEEL first constructs a predefined backbone and then conducts resource reallocation on it to shift parameters from less informative layers to more important layers in one round, thus amplifying the positive effect of these informative layers. To demonstrate the effectiveness of PEEL , we perform extensive experiments on ImageNet with ResNet-18, ResNet-50, MobileNetV2 , MobileNetV3-small and EfficientNet-B0. Experimental results show that structures uncovered by PEEL exhibit competitive performance with state-of-the-art pruning algorithms under various pruning settings. Encouraging results are also observed when applying PEEL to compress the semantic segmentation model. Our code is available at https://github.com/cardwing/Codes-for-PEEL .},
  archive      = {J_PR},
  author       = {Yuenan Hou and Zheng Ma and Chunxiao Liu and Zhe Wang and Chen Change Loy},
  doi          = {10.1016/j.patcog.2023.109886},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109886},
  shortjournal = {Pattern Recognition},
  title        = {Network pruning via resource reallocation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Personalized recommendation via inductive spatiotemporal
graph neural network. <em>PR</em>, <em>145</em>, 109884. (<a
href="https://doi.org/10.1016/j.patcog.2023.109884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network-based collaborative filtering methods have achieved excellent performance in recommender systems . However, previous works have primarily focused on representation using the entire graph or a single sampled subgraph, which fails to address the issue of noise introduced by prolonged ineffective interactions. Moreover, these approaches lack the capacity to model temporal information in an inductive manner, thereby limiting their ability to fully capture the evolving dynamic interests of users over time in a real world scenario. To address the aforementioned challenges, we propose a novel personalized inductive spatiotemporal graph neural network-based framework PistGNN. PistGNN extracts multiple temporal-aware subgraphs from user–item pairs to avoid long-time gap meaningless interactions, which are then fed into a spatiotemporal graph neural network module. It is worth noting that PistGNN trains the model only by relying on subgraphs extracted from bipartite graphs , without depending on global information, and is therefore capable of predicting for new users or items. Finally, we propose an attention-based meta-learning method to personalize the aggregation of subgraphs’ embeddings. Extensive experiments conducted on four real-world datasets demonstrate the superiority of PistGNN over both inductive and transductive baseline methods .},
  archive      = {J_PR},
  author       = {Jibing Gong and Yi Zhao and Jinye Zhao and Jin Zhang and Guixiang Ma and Shaojie Zheng and Shuying Du and Jie Tang},
  doi          = {10.1016/j.patcog.2023.109884},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109884},
  shortjournal = {Pattern Recognition},
  title        = {Personalized recommendation via inductive spatiotemporal graph neural network},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). TransOSV: Offline signature verification with transformers.
<em>PR</em>, <em>145</em>, 109882. (<a
href="https://doi.org/10.1016/j.patcog.2023.109882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Signature verification is a frequently-used forensics technology in numerous safety-critical situations. Although convolutional neural networks (CNNs) have made significant advancements in the field of signature verification, their reliance on local neighborhood operations poses limitations in capturing the global contextual relationships among signature strokes. To overcome this weakness, in this paper, we propose a novel holistic-part unified model named TransOSV based on the vision transformer framework to solve offline signature verification problem. The signature images are first encoded into patch sequences by the proposed transformer-based holistic encoder to learn the global signature representation. Second, considering the subtle local difference between the genuine signature and forged signature, we design a contrast based part decoder along with a sparsity loss, which are utilized to learn the discriminative part features. With the learned holistic features and part features, the proposed model is optimized by the contrast loss function. To reduce the influence of sample imbalance, we also formulate a new focal contrast loss function. Furthermore, we conduct the proposed model to learn signature representations for writer-dependent signature verification task. The experimental results demonstrate the potential of the proposed TransOSV model for both writer-independent and writer-dependent signature verification tasks, achieving remarkable performance improvements and competitive results on four widely-used offline signature datasets.},
  archive      = {J_PR},
  author       = {Huan Li and Ping Wei and Zeyu Ma and Changkai Li and Nanning Zheng},
  doi          = {10.1016/j.patcog.2023.109882},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109882},
  shortjournal = {Pattern Recognition},
  title        = {TransOSV: Offline signature verification with transformers},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Dynamic contrastive learning guided by class confidence and
confusion degree for medical image segmentation. <em>PR</em>,
<em>145</em>, 109881. (<a
href="https://doi.org/10.1016/j.patcog.2023.109881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes an intra-Class-confidence and inter-Class-confusion guided Dynamic Contrastive (CCDC) learning framework for medical image segmentation . A core contribution is to dynamically select the most expressive pixels to build positive and negative pairs for contrastive learning at different training phases. For the positive pairs, dynamically adaptive sampling strategies are introduced for sampling different sets of pixels based on their hardness (namely the easiest, easy, and hard pixels). For the negative pairs, to efficiently learn from the classes with high confusion degree w.r.t a query class (i.e., a class containing the query pixels), a new hard class mining strategy is presented. Furthermore, pixel-level representations are extended to the neighbourhood region to leverage the spatial consistency of adjacent pixels . Extensive experiments on the three public datasets demonstrate that the proposed method significantly surpasses the state-of-the-art.},
  archive      = {J_PR},
  author       = {Jingkun Chen and Changrui Chen and Wenjian Huang and Jianguo Zhang and Kurt Debattista and Jungong Han},
  doi          = {10.1016/j.patcog.2023.109881},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109881},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic contrastive learning guided by class confidence and confusion degree for medical image segmentation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature disentanglement in one-stage object detection.
<em>PR</em>, <em>145</em>, 109878. (<a
href="https://doi.org/10.1016/j.patcog.2023.109878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an enhanced disentanglement module is proposed to address feature misalignment caused by inherently irreconcilable conflicts between classification and regression tasks in Convolutional Neural Network-based object detectors. The proposed method disentangles features in the feature pyramid network (FPN) at the neck of the architecture. In addition, a response alignment strategy is proposed to reduce inconsistent responses and suppress inferior predictions. Extensive experiments are performed on the MS COCO and PASCAL VOC datasets with different backbones, confirming that the proposed method improves performance significantly. The proposed method exhibits two main advantages over existing solutions—features are disentangled at the neck instead of at the head, enabling comprehensive resolution of feature misalignment, and independent outputs of the two tasks after feature disentanglement are avoided, thereby preventing response inconsistencies.},
  archive      = {J_PR},
  author       = {Wenjie Lin and Jun Chu and Lu Leng and Jun Miao and Lingfeng Wang},
  doi          = {10.1016/j.patcog.2023.109878},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109878},
  shortjournal = {Pattern Recognition},
  title        = {Feature disentanglement in one-stage object detection},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QBER: Quantum-based entropic representations for
un-attributed graphs. <em>PR</em>, <em>145</em>, 109877. (<a
href="https://doi.org/10.1016/j.patcog.2023.109877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework of computing the Quantum-based Entropic Representations (QBER) for un-attributed graphs, through the Continuous-time Quantum Walk (CTQW). To achieve this, we commence by transforming each original graph into a family of k k -level neighborhood graphs , where each k k -level neighborhood graph encapsulates the connected information between each vertex and its k k -hop neighbor vertices, providing a fine representation to reflect the multi-level topological information for the original global graph structure. To further capture the complicated structural characteristics of the original graph through its neighborhood graphs, we propose to characterize the structure of each neighborhood graph with the Average Mixing Matrix (AMM) of the CTQW, that encapsulates the time-averaged behavior of the CTQW evolved on the neighborhood graph. More specifically, we show how the AMM matrix allows us to compute a Quantum Shannon Entropy for each vertex, and thus compute an entropic signature for each neighborhood graph by measuring the averaged value or the Jensen–Shannon Divergence between the entropies of its vertices. For each original graph, the resulting QBER is defined by gauging how the entropic signat ures vary on its k k -level neighborhood graphs with increasing k k , reflecting the multi-dimensional entropy-based structure information of the original graph. Experiments on standard graph datasets demonstrate the effectiveness of the proposed QBER approach in terms of the classification accuracies . The proposed approach can significantly outperform state-of-the-art entropic complexity measuring methods, graph kernel methods , as well as graph deep learning methods.},
  archive      = {J_PR},
  author       = {Lixin Cui and Ming Li and Lu Bai and Yue Wang and Jing Li and Yanchao Wang and Zhao Li and Yunwen Chen and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2023.109877},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109877},
  shortjournal = {Pattern Recognition},
  title        = {QBER: Quantum-based entropic representations for un-attributed graphs},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical long-tailed classification based on
multi-granularity knowledge transfer driven by multi-scale feature
fusion. <em>PR</em>, <em>145</em>, 109842. (<a
href="https://doi.org/10.1016/j.patcog.2023.109842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed learning is attracting increasing attention due to the unbalanced distributions of real-world data. The aim is to train well-performing depth models. Traditional knowledge transfer methods for long-tailed learning are classified into feature-based horizontal knowledge transfer (HKT) and class-based vertical knowledge transfer (VKT). HKT transfers head-to-tail feature knowledge from different classes to improve classification performance when there are few tail classes. However, HKT easily leads to invalid transfer due to the deviation caused by the difference between the knowledge of head and tail classes. Fortunately, the class space has a multi-grained relationship and can form a multi-granularity knowledge graph (MGKG), which can be recast as coarse-grained and fine-grained losses to guide VKT. In this paper, we propose a hierarchical long-tailed classification method based on multi-granularity knowledge transfer (MGKT), which vertically transfers knowledge from coarse- to fine-grained classes. First, we exploit the semantic information of classes to construct an MGKG, which forms an affiliation of fine- and coarse-grained classes. Fine-grained knowledge can inherit coarse-grained knowledge to reduce transfer bias with the help of MGKG. We then propose a multi-scale feature fusion network, which aims to fully mine the rich information of the features to drive MGKT. Experiments show that the proposed model outperforms several state-of-the-art models in classifying long-tailed data. For example, our model performed 4.46% better than the next-best model on the SUN-LT dataset.},
  archive      = {J_PR},
  author       = {Wei Zhao and Hong Zhao},
  doi          = {10.1016/j.patcog.2023.109842},
  journal      = {Pattern Recognition},
  month        = {1},
  pages        = {109842},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical long-tailed classification based on multi-granularity knowledge transfer driven by multi-scale feature fusion},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
