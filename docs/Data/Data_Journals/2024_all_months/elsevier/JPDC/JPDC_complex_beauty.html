<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JPDC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jpdc---129">JPDC - 129</h2>
<ul>
<li><details>
<summary>
(2024). Clustering-based multi-objective optimization considering
fairness for multi-workflow scheduling on clouds. <em>JPDC</em>,
<em>194</em>, 104968. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed computing, such as cloud computing, provides promising platforms for orchestrating scientific workflows&#39; tasks based on their sequences and dependencies. Workflow scheduling plays an important role in optimizing concerned objectives for distributed computing, such as minimizing the makespan and cost. Many researchers have focused on optimizing a specific single workflow with multiple objectives. Currently, there are few studies on multi-workflow scheduling, with most research focusing on objectives such as cost and makespan. However, multi-workflow scheduling requires the design of specific objectives that reflect the unique characteristics of multiple workflows. On the other hand, clustering-based approaches have garnered significant attention in the field of workflow scheduling over distributed computing resources due to their advantage in reducing data communication among tasks. Despite this, the effectiveness of clustering-based algorithms has not been extensively studied and validated in the context of multi-objective multi-workflow scheduling models. Motivated by these factors, we propose an approach for multiple workflows&#39; multi-objective optimization (MOO), considering the new defined metric, fairness. We first mathematically formulate the fairness and define a fairness-involved MOO model. Then, we propose an advanced clustering-based resource optimization strategy in multiple workflow runs. Experimental results show that the proposed approach performs better than the compared algorithms without significant compromise of the overall makespan and cost as well as individual fairness, which can guide the simulation workflow scheduling on clouds.},
  archive      = {J_JPDC},
  author       = {Feng Li and Wen Jun Tan and Moon Gi Seok and Wentong Cai},
  doi          = {10.1016/j.jpdc.2024.104968},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {12},
  pages        = {104968},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Clustering-based multi-objective optimization considering fairness for multi-workflow scheduling on clouds},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StarPlat: A versatile DSL for graph analytics.
<em>JPDC</em>, <em>194</em>, 104967. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs model several real-world phenomena. With the growth of unstructured and semi-structured data, parallelization of graph algorithms is inevitable. Unfortunately, due to inherent irregularity of computation, memory access, and communication, graph algorithms are traditionally challenging to parallelize. To tame this challenge, several libraries, frameworks, and domain-specific languages (DSLs) have been proposed to reduce the parallel programming burden of the users, who are often domain experts. However, existing frameworks to model graph algorithms typically target a single architecture. In this paper, we present a graph DSL, named StarPlat, that allows programmers to specify graph algorithms in a high-level format, but generates code for three different backends from the same algorithmic specification. In particular, the DSL compiler generates OpenMP for multi-core systems, MPI for distributed systems, and CUDA for many-core GPUs. Since these three are completely different parallel programming paradigms, binding them together under the same language is challenging. We share our experience with the language design. Central to our compiler is an intermediate representation which allows a common representation of the high-level program, from which individual backend code generations begin. We demonstrate the expressiveness of StarPlat by specifying four graph algorithms: betweenness centrality computation, page rank computation, single-source shortest paths, and triangle counting. Using a suite of ten large graphs, we illustrate the effectiveness of our approach by comparing the performance of the generated codes with that obtained with hand-crafted library codes. We find that the generated code is competitive to library-based codes in many cases. More importantly, we show the feasibility to generate efficient codes for different target architectures from the same algorithmic specification of graph algorithms.},
  archive      = {J_JPDC},
  author       = {Nibedita Behera and Ashwina Kumar and Ebenezer Rajadurai T and Sai Nitish and Rajesh Pandian M and Rupesh Nasre},
  doi          = {10.1016/j.jpdc.2024.104967},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {12},
  pages        = {104967},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {StarPlat: A versatile DSL for graph analytics},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MapReduce algorithms for robust center-based clustering in
doubling metrics. <em>JPDC</em>, <em>194</em>, 104966. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a pivotal primitive for unsupervised learning and data analysis. A popular variant is the ( k , ℓ ) (k,ℓ) -clustering problem, where, given a pointset P from a metric space, one must determine a subset S of k centers minimizing the sum of the ℓ -th powers of the distances of points in P from their closest centers. This formulation covers the well-studied k -median ( ℓ = 1 ℓ=1 ) and k -means ( ℓ = 2 ℓ=2 ) clustering problems. A more general variant, introduced to deal with noisy pointsets, features a further parameter z and allows up to z points of P (outliers) to be disregarded when computing the sum. We present a distributed coreset-based 3-round approximation algorithm for the ( k , ℓ ) (k,ℓ) -clustering problem with z outliers, using MapReduce as a computational model. An important feature of our algorithm is that it obliviously adapts to the intrinsic complexity of the dataset, captured by its doubling dimension D . Remarkably, for D = O ( 1 ) D=O(1) , our algorithm requires sublinear local memory per reducer, and yields a solution whose approximation ratio is an additive term O ( γ ) O(γ) away from the one achievable by the best known sequential (possibly bicriteria) algorithm, where γ can be made arbitrarily small. To the best of our knowledge, no previous distributed approaches were able to attain similar quality-performance tradeoffs for metrics with constant doubling dimension.},
  archive      = {J_JPDC},
  author       = {Enrico Dandolo and Alessio Mazzetto and Andrea Pietracaprina and Geppino Pucci},
  doi          = {10.1016/j.jpdc.2024.104966},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {12},
  pages        = {104966},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MapReduce algorithms for robust center-based clustering in doubling metrics},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>193</em>, 104972.
(<a href="https://doi.org/10.1016/S0743-7315(24)00136-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(24)00136-9},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104972},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A trajectory privacy protection method using cached
candidate result sets. <em>JPDC</em>, <em>193</em>, 104965. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A trajectory privacy protection method using cached candidate result sets (TPP-CCRS) is proposed for the user trajectory privacy leakage problem. First, the user&#39;s area is divided into a grid to lock the user&#39;s trajectory range, and a cache area is set on the user&#39;s mobile side to cache the candidate result sets queried from the user&#39;s area. Second, a security center is deployed to register users securely and assign public and private keys for verifying location information. The same user&#39;s location information is randomly divided into M copies and sent to multi-anonymizers. Then, the random concurrent k -anonymization mechanism with multi-anonymizers is used to concurrently k -anonymize M copies of location information. Finally, the prefix tree is added on the location-based service (LBS) server side, and the location information is encrypted using the clustered data fusion privacy protection algorithm. The optimal binary tree algorithm queries user interest points. Security analysis and experimental verification show that the TPP-CCRS can effectively protect user trajectory privacy and improve location information query efficiency.},
  archive      = {J_JPDC},
  author       = {Zihao Shen and Yuyu Tang and Hui Wang and Peiqian Liu and Zhenqing Zheng},
  doi          = {10.1016/j.jpdc.2024.104965},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104965},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A trajectory privacy protection method using cached candidate result sets},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated bayesian optimization XGBoost model for
cyberattack detection in internet of medical things. <em>JPDC</em>,
<em>193</em>, 104964. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hospitals and medical facilities are increasingly concerned about network security and patient data privacy as the Internet of Medical Things (IoMT) infrastructures continue to develop. Researchers have studied customized network security frameworks and cyberattack detection tools driven by Artificial Intelligence (AI) to counter different types of attacks, such as spoofing, data alteration, and botnet attacks. However, carrying out routine IoMT services and tasks during an under-attack scenario is challenging. Machine Learning has been extensively suggested for detecting cyberattacks in IoMT and IoT infrastructures. However, the conventional centralized approach in ML cannot effectively detect newly emerging attacks without compromising patient data privacy and network flow data confidentiality. This study discusses a Federated Bayesian Optimization XGBoost framework that employs multimodal sensory signals from patient vital signs and network flow data to detect attack patterns and malicious network traffic in IoMT infrastructure while ensuring data privacy and detecting previously unknown attacks. The proposed model employs a Federated Bayesian Optimisation XGBoost approach, which allows us to search the parameter space quickly and find an optimal solution from each local server while aggregating the model parameters from each local server to the centralised server. The XGBoost algorithm generates a new tree by taking into account the previously estimated value for the tree&#39;s input data and then optimizing the prediction gain. This study used a dataset with 44 attributes and 16 318 instances. During the preprocessing phase, 10 features were dropped, and the remaining 34 features were used to evaluate the network flows and biometric data (patient vital signs). The performance evaluation reveals that the proposed model predicts data alteration, malware, and spoofing attacks in patients&#39; vital signs and network flow data with a prediction accuracy of 0.96. The results obtained from the experiment demonstrate that both the centralized and federated models are synchronized, with the latter occasionally being slightly reduced. The findings indicate that the suggested model can be incorporated into the IoMT domain to detect malicious patterns while maintaining data privacy and confidentiality efficiently.},
  archive      = {J_JPDC},
  author       = {Blessing Guembe and Sanjay Misra and Ambrose Azeta},
  doi          = {10.1016/j.jpdc.2024.104964},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104964},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Federated bayesian optimization XGBoost model for cyberattack detection in internet of medical things},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Verifiable and hybrid attribute-based proxy re-encryption
for flexible data sharing in cloud storage. <em>JPDC</em>, <em>193</em>,
104956. (<a href="https://doi.org/10.1016/j.jpdc.2024.104956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a promising service architecture that enables a data owner to share data in an economic and efficient manner. To ensure data privacy, a data owner will generate the ciphertext of the data before outsourcing. Attribute-based encryption (ABE) provides an elegant solution for a data owner to enforce fine-grained access control on the data to be outsourced. However, ABE cannot support ciphertext transformation when needing to share the underlying data with a public-key infrastructure (PKI) user further. In addition, an untrusted cloud server may return random ciphertexts to the PKI user to save expensive computational costs of ciphertext transformation. To address above issues, we introduce a novel cryptographic primitive namely verifiable and hybrid attribute-based proxy re-encryption (VHABPRE). VHABPRE provides a transformation mechanism that re-encrypts an ABE ciphertext to a PKI-based public key encryption (PKE) ciphertext such that the PKI user can access the underlying data, meanwhile this PKI user can ensure the validity of the transformed ciphertext. By leveraging a key blinding technique and computing the commitment of the data, we construct two VHABPRE schemes to achieve flexible data sharing. We give formal security proofs and comprehensive performance evaluation to show the security and efficiency of the VHABPRE schemes.},
  archive      = {J_JPDC},
  author       = {Lixue Sun and Chunxiang Xu and Fugeng Zeng},
  doi          = {10.1016/j.jpdc.2024.104956},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104956},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Verifiable and hybrid attribute-based proxy re-encryption for flexible data sharing in cloud storage},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating memory and i/o intensive HPC applications using
hardware compression. <em>JPDC</em>, <em>193</em>, 104955. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, accelerator-based compression/decompression was proposed to hide the storage latency of high-performance computing (HPC) applications that generate/ingest large data that cannot fit a node&#39;s memory. In this work, such a scheme has been implemented using a novel FPGA-based lossy compression/decompression scheme that has very low-latency. The proposed scheme completely overlaps the movement of the application&#39;s data with its compute kernels on the CPU with minimal impact on these kernels. Experiments showed that it can yield performance levels on-par with utilizing memory-only storage buffers, even though data is actually stored on disk. Experiments also showed that compared to CPU- and GPU-based compression frameworks, it achieves better performance levels at a fraction of the power consumption.},
  archive      = {J_JPDC},
  author       = {Saleh AlSaleh and Muhammad E.S. Elrabaa and Aiman El-Maleh and Khaled Daud and Ayman Hroub and Muhamed Mudawar and Thierry Tonellot},
  doi          = {10.1016/j.jpdc.2024.104955},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104955},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerating memory and I/O intensive HPC applications using hardware compression},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local certification of graph decompositions and applications
to minor-free classes. <em>JPDC</em>, <em>193</em>, 104954. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local certification consists in assigning labels to the vertices of a network to certify that some given property is satisfied, in such a way that the labels can be checked locally. In the last few years, certification of graph classes received considerable attention. The goal is to certify that a graph G belongs to a given graph class G G . Such certifications with labels of size O ( log ⁡ n ) O(log⁡n) (where n is the size of the network) exist for trees, planar graphs and graphs embedded on surfaces. Feuilloley et al. ask if this can be extended to any class of graphs defined by a finite set of forbidden minors. In this work, we develop new decomposition tools for graph certification, and apply them to show that for every small enough minor H , H -minor-free graphs can indeed be certified with labels of size O ( log ⁡ n ) O(log⁡n) . We also show matching lower bounds using a new proof technique.},
  archive      = {J_JPDC},
  author       = {Nicolas Bousquet and Laurent Feuilloley and Théo Pierron},
  doi          = {10.1016/j.jpdc.2024.104954},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104954},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Local certification of graph decompositions and applications to minor-free classes},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization-based disjoint and overlapping epsilon
decompositions of large-scale dynamical systems via graph theory.
<em>JPDC</em>, <em>193</em>, 104953. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the complexity challenge of a large-scale system, the decomposition into smaller subsystems is very crucial and demanding for distributed estimation and control purposes. This paper proposes novel optimization-based approaches to decompose a large-scale system into subsystems that are either weakly coupled or weakly coupled with overlapping components. To achieve this goal, first, the epsilon decomposition of large-scale systems is examined. Then, optimization frameworks are presented for disjoint and overlapping decompositions utilizing bipartite graphs. Next, the proposed decomposition algorithms are represented for particular cases of large-scale systems using directed graphs. In contrast to the existing user-based techniques, the proposed optimization-based methods can reach the solution rapidly and systematically. At last, the capability and efficiency of the proposed algorithms are investigated by conducting simulations on three case studies, which include a practical distillation column, a modified benchmark model, and the IEEE 118-bus power system.},
  archive      = {J_JPDC},
  author       = {Sahar Maleki and Hassan Zarabadipour and Mehdi Rahmani},
  doi          = {10.1016/j.jpdc.2024.104953},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104953},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimization-based disjoint and overlapping epsilon decompositions of large-scale dynamical systems via graph theory},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable communication in dynamic networks with locally
bounded byzantine faults. <em>JPDC</em>, <em>193</em>, 104952. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Byzantine tolerant reliable communication primitive is a fundamental building block in distributed systems that guarantees the authenticity, integrity, and delivery of information exchanged between processes. We study the implementability of such a primitive in a distributed system with a dynamic communication network (i.e., where the set of available communication channels changes over time). We assume the f -locally bounded Byzantine fault model and identify the conditions on the dynamic communication networks that allow reliable communication between all pairs of processes. In addition, we investigate its implementability on several classes of dynamic networks and provide insights into its use in asynchronous distributed systems.},
  archive      = {J_JPDC},
  author       = {Silvia Bonomi and Giovanni Farina and Sébastien Tixeuil},
  doi          = {10.1016/j.jpdc.2024.104952},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104952},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliable communication in dynamic networks with locally bounded byzantine faults},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection based on LSTM and autoencoders using
federated learning in smart electric grid. <em>JPDC</em>, <em>193</em>,
104951. (<a href="https://doi.org/10.1016/j.jpdc.2024.104951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In smart electric grid systems, various sensors and Internet of Things (IoT) devices are used to collect electrical data at substations. In a traditional system, a multitude of energy-related data from substations needs to be migrated to central storage, such as Cloud or edge devices, for knowledge extraction that might impose severe data misuse, data manipulation, or privacy leakage. This motivates to propose anomaly detection system to detect threats and Federated Learning to resolve the issues of data silos and privacy of data. In this article, we present a framework to identify anomalies in industrial data that are gathered from the remote terminal devices deployed at the substations in the smart electric grid system. The anomaly detection system is based on Long Short-Term Memory (LSTM) and autoencoders that employs Mean Standard Deviation (MSD) and Median Absolute Deviation (MAD) approaches for detecting anomalies. We deploy Federated Learning (FL) to preserve the privacy of the data generated by the substations. FL enables energy providers to train shared AI models cooperatively without disclosing the data to the server. In order to further enhance the security and privacy properties of the proposed framework, we implemented homomorphic encryption based on the Paillier algorithm for preserving data privacy. The proposed security model performs better with MSD approach using HE-128 bit key providing 97% F1-score and 98% accuracy for K=5 with low computation overhead as compared with HE-256 bit key.},
  archive      = {J_JPDC},
  author       = {Rakesh Shrestha and Mohammadreza Mohammadi and Sima Sinaei and Alberto Salcines and David Pampliega and Raul Clemente and Ana Lourdes Sanz and Ehsan Nowroozi and Anders Lindgren},
  doi          = {10.1016/j.jpdc.2024.104951},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104951},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Anomaly detection based on LSTM and autoencoders using federated learning in smart electric grid},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Staleness aware semi-asynchronous federated learning.
<em>JPDC</em>, <em>193</em>, 104950. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the attempts to distribute deep learning using personal data have increased, the importance of federated learning (FL) has also increased. Attempts have been made to overcome the core challenges of federated learning (i.e., statistical and system heterogeneity) using synchronous or asynchronous protocols. However, stragglers reduce training efficiency in terms of latency and accuracy in each protocol, respectively. To solve straggler issues, a semi-asynchronous protocol that combines the two protocols can be applied to FL; however, effectively handling the staleness of the local model is a difficult problem. We proposed SASAFL to solve the training inefficiency caused by staleness in semi-asynchronous FL. SASAFL enables stable training by considering the quality of the global model to synchronise the servers and clients. In addition, it achieves high accuracy and low latency by adjusting the number of participating clients in response to changes in global loss and immediately processing clients that did not to participate in the previous round. An evaluation was conducted under various conditions to verify the effectiveness of the SASAFL. SASAFL achieved 19.69%p higher accuracy than the baseline, 2.32 times higher round-to-accuracy and 2.24 times higher latency-to-accuracy. Additionally, SASAFL always achieved target accuracy that the baseline can&#39;t reach.},
  archive      = {J_JPDC},
  author       = {Miri Yu and Jiheon Choi and Jaehyun Lee and Sangyoon Oh},
  doi          = {10.1016/j.jpdc.2024.104950},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104950},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Staleness aware semi-asynchronous federated learning},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuron grouping and mapping methods for 2D-mesh NoC-based
DNN accelerators. <em>JPDC</em>, <em>193</em>, 104949. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have gained widespread adoption in various fields; however, their computational cost is often prohibitively high due to the large number of layers and neurons communicating with each other. Furthermore, DNNs can consume a significant amount of energy due to the large volume of data movement and computation they require. To address these challenges, there is a need for new architectures to accelerate DNNs. In this paper, we propose novel neuron grouping and mapping methods for 2D-mesh Network-on-Chip (NoC)-based DNN accelerators considering both fully connected and partially connected DNN models. We present Integer Linear Programming (ILP) and simulated annealing (SA)-based neuron grouping solutions with the objective of minimizing the total volume of data communication among the neuron groups. After determining a suitable graph representation of the DNN, we also apply ILP and SA methods to map the neurons onto a 2D-mesh NoC fabric with the objective of minimizing the total communication cost of the system. We conducted several experiments on various benchmarks and DNN models with different pruning ratios and achieved an average of 40-50% improvement in communication cost.},
  archive      = {J_JPDC},
  author       = {Furkan Nacar and Alperen Cakin and Selma Dilek and Suleyman Tosun and Krishnendu Chakrabarty},
  doi          = {10.1016/j.jpdc.2024.104949},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104949},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Neuron grouping and mapping methods for 2D-mesh NoC-based DNN accelerators},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning based controller placement and
optimal edge selection in SDN-based multi-access edge computing
environments. <em>JPDC</em>, <em>193</em>, 104948. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Access Edge Computing (MEC) can provide computility close to the clients to decrease response time and enhance Quality of Service (QoS). However, the complex wireless network consists of various network hardware facilities with different communication protocols and Application Programming Interface (API), which result in the MEC system&#39;s high running costs and low running efficiency. To this end, Software-defined networking (SDN) is applied to MEC, which can support access to massive network devices and provide flexible and efficient management. The reasonable SDN controller scheme is crucial to enhance the performance of SDN-assisted MEC. At First, we used the Convolutional Neural Networks (CNN)-Long Short-Term Memory (LSTM) model to predict the network traffic to calculate the load. Then, the optimization objective is formulated by ensuring the load balance and minimizing the system cost. Finally, the Deep Reinforcement Learning (DRL) algorithm is used to obtain the optimal value. Based on the controller placement algorithm ensuring the load balancing, the dynamical edge selection method based on the Channel State Information (CSI) is proposed to optimize the task offloading, and according to CSI, the strategy of task queue execution is designed. Then, the task offloading problem is modeled by using queuing theory. Finally, dynamical edge selection based on Lyapunov&#39;s optimization is introduced to get the model solution. In the experiment studies, the assessment method evaluated the performance of two sets of baseline algorithms, including SAPKM, the PSO, the K-means, the LADMA, the LATA, and the OAOP. Compared to the baseline algorithms, the proposed algorithms can effectively reduce the average communication delay and total system energy consumption and improve the utilization of the SDN controller.},
  archive      = {J_JPDC},
  author       = {Chunlin Li and Jun Liu and Ning Ma and Qingzhe Zhang and Zhengwei Zhong and Lincheng Jiang and Guolei Jia},
  doi          = {10.1016/j.jpdc.2024.104948},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104948},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Deep reinforcement learning based controller placement and optimal edge selection in SDN-based multi-access edge computing environments},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PiPar: Pipeline parallelism for collaborative machine
learning. <em>JPDC</em>, <em>193</em>, 104947. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative machine learning (CML) techniques, such as federated learning, have been proposed to train deep learning models across multiple mobile devices and a server. CML techniques are privacy-preserving as a local model that is trained on each device instead of the raw data from the device is shared with the server. However, CML training is inefficient due to low resource utilization. We identify idling resources on the server and devices due to sequential computation and communication as the principal cause of low resource utilization. A novel framework PiPar that leverages pipeline parallelism for CML techniques is developed to substantially improve resource utilization. A new training pipeline is designed to parallelize the computations on different hardware resources and communication on different bandwidth resources, thereby accelerating the training process in CML. A low overhead automated parameter selection method is proposed to optimize the pipeline, maximizing the utilization of available resources. The experimental results confirm the validity of the underlying approach of PiPar and highlight that when compared to federated learning: (i) the idle time of the server can be reduced by up to 64.1×, and (ii) the overall training time can be accelerated by up to 34.6× under varying network conditions for a collection of six small and large popular deep neural networks and four datasets without sacrificing accuracy. It is also experimentally demonstrated that PiPar achieves performance benefits when incorporating differential privacy methods and operating in environments with heterogeneous devices and changing bandwidths.},
  archive      = {J_JPDC},
  author       = {Zihan Zhang and Philip Rodgers and Peter Kilpatrick and Ivor Spence and Blesson Varghese},
  doi          = {10.1016/j.jpdc.2024.104947},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104947},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PiPar: Pipeline parallelism for collaborative machine learning},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMC-crypto: Low latency implementation of ECC point
multiplication for generic montgomery curves over GF(p). <em>JPDC</em>,
<em>193</em>, 104946. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptic Curve Cryptography (ECC) is the front-runner among available public key cryptography (PKC) schemes due to its potential to offer higher security per key bit. All ECC-based cryptosystems heavily rely on point multiplication operation where its efficient realization has attained notable focus in the research community. Low latency implementation of the point multiplication operation is frequently required in high-speed applications such as online authentication and web server certification. This paper presents a low latency ECC point multiplication architecture for Montgomery curves over generic prime filed G F ( p ) GF(p) . The proposed architecture is able to operate for a general prime modulus without any constraints on its structure. It is based on a new novel pipelined modular multiplier developed using the Montgomery multiplication and the Karatsuba-Offman technique with a four-part splitting methodology. The Montgomery ladder approach is adopted on a system level, where a high-speed scheduling strategy to efficiently execute G F ( p ) GF(p) operations is also presented. Due to these circuit and system-level optimizations, the proposed design delivers low-latency results without a significant increase in resource consumption. The proposed architecture is described in Verilog-HDL for 256-bit key lengths and implemented on Virtex-7 and Virtex-6 FPGA platforms using Xilinx ISE Design Suite. On the Virtex-7 FPGA platform, it performs a 256-bit point multiplication operation in just 110.9 u s with a throughput of almost 9017 operations per second. The implementation results demonstrate that despite its generic nature, it produces low latency as compared to the state-of-the-art. Therefore, it has prominent prospects to be used in high-speed authentication and certification applications.},
  archive      = {J_JPDC},
  author       = {Khalid Javeed and Yasir Ali Shah and David Gregg},
  doi          = {10.1016/j.jpdc.2024.104946},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104946},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GMC-crypto: Low latency implementation of ECC point multiplication for generic montgomery curves over GF(p)},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D DFT by block tensor-matrix multiplication via a modified
cannon’s algorithm: Implementation and scaling on distributed-memory
clusters with fat tree networks. <em>JPDC</em>, <em>193</em>, 104945.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A known scalability bottleneck of the parallel 3D FFT is its use of all-to-all communications. Here, we present S3DFT, a library that circumvents this by using point-to-point communication – albeit at a higher arithmetic complexity. This approach exploits three variants of Cannon&#39;s algorithm with adaptations for block tensor-matrix multiplications. We demonstrate S3DFT&#39;s efficient use of hardware resources, and its scaling using up to 16,464 cores of the JUWELS Cluster. However, in a comparison with well-established 3D FFT libraries, its parallel efficiency and performance were found to fall behind. A detailed analysis identifies the cause in two of its component algorithms, which scale poorly owing to how their communication patterns are mapped in subsets of the fat tree topology. This result exposes a potential drawback of running block-wise parallel algorithms on systems with fat tree networks caused by increased communication latencies along specific directions of the mesh of processing elements.},
  archive      = {J_JPDC},
  author       = {Nitin Malapally and Viacheslav Bolnykh and Estela Suarez and Paolo Carloni and Thomas Lippert and Davide Mandelli},
  doi          = {10.1016/j.jpdc.2024.104945},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104945},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {3D DFT by block tensor-matrix multiplication via a modified cannon&#39;s algorithm: Implementation and scaling on distributed-memory clusters with fat tree networks},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PPB-MCTS: A novel distributed-memory parallel
partial-backpropagation monte carlo tree search algorithm.
<em>JPDC</em>, <em>193</em>, 104944. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monte-Carlo Tree Search (MCTS) is an adaptive and heuristic tree-search algorithm designed to uncover sub-optimal actions at each decision-making point. This method progressively constructs a search tree by gathering samples throughout its execution. Predominantly applied within the realm of gaming, MCTS has exhibited exceptional achievements. Additionally, it has displayed promising outcomes when employed to solve NP-hard combinatorial optimization problems. MCTS has been adapted for distributed-memory parallel platforms. The primary challenges associated with distributed-memory parallel MCTS are the substantial communication overhead and the necessity to balance the computational load among various processes. In this work, we introduce a novel distributed-memory parallel MCTS algorithm with partial backpropagations, referred to as Parallel Partial-Backpropagation MCTS ( PPB-MCTS ). Our design approach aims to significantly reduce the communication overhead while maintaining, or even slightly improving, the performance in the context of combinatorial optimization problems. To address the communication overhead challenge, we propose a strategy involving transmitting an additional backpropagation message. This strategy avoids attaching an information table to the communication messages exchanged by the processes, thus reducing the communication overhead. Furthermore, this approach contributes to enhancing the decision-making accuracy during the selection phase. The load balancing issue is also effectively addressed by implementing a shared transposition table among the parallel processes. Furthermore, we introduce two primary methods for managing duplicate states within distributed-memory parallel MCTS, drawing upon techniques utilized in addressing duplicate states within sequential MCTS. Duplicate states can transform the conventional search tree into a Directed Acyclic Graph (DAG). To evaluate the performance of our proposed parallel algorithm, we conduct an extensive series of experiments on solving instances of the Job-Shop Scheduling Problem (JSSP) and the Weighted Set-Cover Problem (WSCP). These problems are recognized for their complexity and classified as NP-hard combinatorial optimization problems with considerable relevance within industrial applications. The experiments are performed on a cluster of computers with many cores. The empirical results highlight the enhanced scalability of our algorithm compared to that of the existing distributed-memory parallel MCTS algorithms. As the number of processes increases, our algorithm demonstrates increased rollout efficiency while maintaining an improved load balance across processes.},
  archive      = {J_JPDC},
  author       = {Yashar Naderzadeh and Daniel Grosu and Ratna Babu Chinnam},
  doi          = {10.1016/j.jpdc.2024.104944},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104944},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PPB-MCTS: A novel distributed-memory parallel partial-backpropagation monte carlo tree search algorithm},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-assisted full-session key agreement for secure
data sharing in cloud computing. <em>JPDC</em>, <em>193</em>, 104943.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing in cloud computing allows multiple data owners to freely share their data resources while security and privacy issues remain inevitable challenges. As a foundation of secure communication, authenticated key agreement (AKA) scheme has been recognized as a promising approach to solve such problems. However, most existing AKA schemes are based on the cloud-based architecture, privacy and security issues will inevitably occur once the centralized authority is attacked. Besides, most previous schemes require an online registration authority for authentication, which may consume significant resources. To address these drawbacks, for secure data sharing in cloud computing, a blockchain-assisted full-session key agreement scheme is proposed. After the registration phase, the registration authority does not engage in authentication and key agreement process. By utilizing blockchain technology, a common session key between the remote user and cloud server can be negotiated, and a shared group key among multiple remote users can be negotiated without private information leakage. Formal and informal security proof demonstrated the proposed scheme is able to meet the security and privacy requirements. The detail performance evaluation shows that the proposed scheme has lower computation costs and acceptable communication overheads while superior security is ensured.},
  archive      = {J_JPDC},
  author       = {Yangyang Long and Changgen Peng and Weijie Tan and Yuling Chen},
  doi          = {10.1016/j.jpdc.2024.104943},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104943},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-assisted full-session key agreement for secure data sharing in cloud computing},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experimental evaluation of a multi-installment scheduling
strategy based on divisible load paradigm for SAR image reconstruction
on a distributed computing infrastructure. <em>JPDC</em>, <em>193</em>,
104942. (<a href="https://doi.org/10.1016/j.jpdc.2024.104942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar loads, especially Synthetic Aperture Radar (SAR) image reconstruction loads use a large volume of data collected from satellites to create a high-resolution image of the earth. To design near-real-time applications that utilise SAR data, speeding up the image reconstruction algorithm is imperative. This can be achieved by deploying a set of distributed computing infrastructures connected through a network. Scheduling such complex and large divisible loads on a distributed platform can be designed using the Divisible Load Theory (DLT) framework. We performed distributed SAR image reconstruction experiments using the SLURM library on a cloud virtual machine network using two scheduling strategies, namely the Multi-Installment Scheduling with Result Retrieval (MIS-RR) strategy and the traditional EQual-partitioning Strategy (EQS). The DLT model proposed in the MIS-RR strategy is incorporated to make the load divisible. Based on the experimental results and performance analysis carried out using different pixel lengths, pulse set sizes, and the number of virtual machines, we observe that the time performance of MIS-RR is much superior to that of EQS. Hence the MIS-RR strategy is of practical significance in reducing the overall processing time, and cost, and in improving the utilisation of the compute infrastructure. Furthermore, we note that the DLT-based theoretical analysis of MIS-RR coincides well with the experimental data, demonstrating the relevance of DLT in the real world.},
  archive      = {J_JPDC},
  author       = {Gokul Madathupalyam Chinnappan and Bharadwaj Veeravalli and Koen Mouthaan and John Wen-Hao Lee},
  doi          = {10.1016/j.jpdc.2024.104942},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {11},
  pages        = {104942},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Experimental evaluation of a multi-installment scheduling strategy based on divisible load paradigm for SAR image reconstruction on a distributed computing infrastructure},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>192</em>, 104960.
(<a href="https://doi.org/10.1016/S0743-7315(24)00124-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(24)00124-2},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104960},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpChar: Characterizing the sparse puzzle via decision trees.
<em>JPDC</em>, <em>192</em>, 104941. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix computation is crucial in various modern applications, including large-scale graph analytics, deep learning, and recommender systems. The performance of sparse kernels varies greatly depending on the structure of the input matrix, making it difficult to gain a comprehensive understanding of sparse computation and its relationship to inputs, algorithms, and target machine architecture. Despite extensive research on certain sparse kernels, such as Sparse Matrix-Vector Multiplication (SpMV), the overall family of sparse algorithms has yet to be investigated as a whole. This paper introduces SpChar, a workload characterization methodology for general sparse computation. SpChar employs tree-based models to identify the most relevant hardware and input characteristics, starting from hardware and input-related metrics gathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis enables the creation of a characterization loop that facilitates the optimization of sparse computation by mapping the impact of architectural features to inputs and algorithmic choices. We apply SpChar to more than 600 matrices from the SuiteSparse Matrix collection and three state-of-the-art Arm Central Processing Units (CPUs) to determine the critical hardware and software characteristics that affect sparse computation. In our analysis, we determine that the biggest limiting factors for high-performance sparse computation are (1) the latency of the memory system, (2) the pipeline flush overhead resulting from branch misprediction, and (3) the poor reuse of cached elements. Additionally, we propose software and hardware optimizations that designers can implement to create a platform suitable for sparse computation. We then investigate these optimizations using the gem5 simulator to achieve a significant speedup of up to 2.63× compared to a CPU where the optimizations are not applied.},
  archive      = {J_JPDC},
  author       = {Francesco Sgherzi and Marco Siracusa and Ivan Fernandez and Adrià Armejach and Miquel Moretó},
  doi          = {10.1016/j.jpdc.2024.104941},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104941},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SpChar: Characterizing the sparse puzzle via decision trees},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PA-SPS: A predictive adaptive approach for an elastic stream
processing system. <em>JPDC</em>, <em>192</em>, 104940. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream Processing Systems (SPSs) dynamically process input events. Since the input is usually not a constant flow, presenting rate fluctuations, many works in the literature propose to dynamically replicate SPS operators, aiming at reducing the processing bottleneck induced by such fluctuations. However, these SPSs do not consider the problem of load balancing of the replicas or the cost involved in reconfiguring the system whenever the number of replicas changes. We present in this paper a predictive model which, based on input rate variation, execution time of operators, and queued events, dynamically defines the necessary current number of replicas of each operator. A predictor, composed of different models (i.e., mathematical and Machine Learning ones), predicts the input rate. We also propose a Storm-based SPS, named PA-SPS, which uses such a predictive model, not requiring reboot reconfiguration when the number of operators replica change. PA-SPS also implements a load balancer that distributes incoming events evenly among replicas of an operator. We have conducted experiments on Google Cloud Platform (GCP) for evaluation PA-SPS using real traffic traces of different applications and also compared it with Storm and other existing SPSs.},
  archive      = {J_JPDC},
  author       = {Daniel Wladdimiro and Luciana Arantes and Pierre Sens and Nicolás Hidalgo},
  doi          = {10.1016/j.jpdc.2024.104940},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104940},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PA-SPS: A predictive adaptive approach for an elastic stream processing system},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using hardware-transactional-memory support to implement
speculative task execution. <em>JPDC</em>, <em>192</em>, 104939. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loops take up most of the time of computer programs, so optimizing them so that they run in the shortest time possible is a continuous task. However, this task is not negligible; on the contrary, it is an open area of research since many irregular loops are hard to parallelize. Generally, these loops have loop-carried (DOACROSS) dependencies and the appearance of dependencies could depend on the context. Many techniques have been studied to be able to parallelize these loops efficiently; however, for example in the OpenMP standard there is no efficient way to parallelize them. This article presents Speculative Task Execution (STE), a technique that enables the execution of OpenMP tasks in a speculative way to accelerate certain hot-code regions (such as loops) marked by OpenMP directives. It also presents a detailed analysis of the application of Hardware Transactional Memory (HTM) support for executing tasks speculatively and describes a careful evaluation of the implementation of STE using HTM on modern machines. In particular, we consider the scenario in which speculative tasks are generated by the OpenMP taskloop construct ( Speculative Taskloop (STL) ). As a result, it provides evidence to support several important claims about the performance of STE over HTM in modern processor architectures. Experimental results reveal that: (a) by implementing STL on top of HTM for hot-code regions, speed-ups of up to 5.39× can be obtained in IBM POWER8 and of up to 2.41× in Intel processors using 4 cores; and (b) STL-ROT, a variant of STL using rollback-only transactions (ROTs), achieves speed-ups of up to 17.70× in IBM POWER9 processor using 20 cores.},
  archive      = {J_JPDC},
  author       = {Juan Salamanca and Alexandro Baldassin},
  doi          = {10.1016/j.jpdc.2024.104939},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104939},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Using hardware-transactional-memory support to implement speculative task execution},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-level parallel approach to increase the computation
efficiency of a global ocean temperature dataset reconstruction.
<em>JPDC</em>, <em>192</em>, 104938. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing need to provide real-time datasets for climate monitoring and applications. However, the current data products from all international groups have at least a month delay for data release. One reason for this delay is the long computing time of the global reconstruction algorithm (so-called mapping approach). To tackle this issue, this paper proposes a multi-level parallel computing model to improve the efficiency of data construction by parallelization of computation, reducing code branch prediction, optimizing data spatial locality, cache utilization, and other measures. This model has been applied to a mapping approach proposed by the Institute of Atmospheric Physics (IAP), one of the world&#39;s most widely used data products in the ocean and climate field. Compared with the traditional serial construction of MATLAB-based scheme on a single node, the speed of the construction after parallel optimizations is speeded up by ∼4.7 times. A large-scale parallel experiment of a long-term (∼1000 months) gridded dataset utilizing over 16,000 processor cores proves the model&#39;s scalability, improving ∼1200 times. In summary, this new model represents another example of the application of high-performance computing in oceanography and climatology.},
  archive      = {J_JPDC},
  author       = {Huifeng Yuan and Lijing Cheng and Yuying Pan and Zhetao Tan and Qian Liu and Zhong Jin},
  doi          = {10.1016/j.jpdc.2024.104938},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104938},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A multi-level parallel approach to increase the computation efficiency of a global ocean temperature dataset reconstruction},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRACO: Distributed resource-aware admission control for
large-scale, multi-tier systems. <em>JPDC</em>, <em>192</em>, 104935.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern distributed systems are designed to manage overload conditions, by throttling the traffic in excess that cannot be served through overload control techniques. However, the adoption of large-scale NoSQL datastores make systems vulnerable to unbalanced overloads , where specific datastore nodes are overloaded because of hot-spot resources and hogs. In this paper, we propose DRACO, a novel overload control solution that is aware of data dependencies between the application and the datastore tiers. DRACO performs selective admission control of application requests, by only dropping the ones that map to resources on overloaded datastore nodes, while achieving high resource utilization on non-overloaded datastore nodes. We evaluate DRACO on two case studies with high availability and performance requirements, a virtualized IP Multimedia Subsystem and a distributed fileserver. Results show that the solution can achieve high performance and resource utilization even under extreme overload conditions, up to 100x the engineered capacity.},
  archive      = {J_JPDC},
  author       = {Domenico Cotroneo and Roberto Natella and Stefano Rosiello},
  doi          = {10.1016/j.jpdc.2024.104935},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104935},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DRACO: Distributed resource-aware admission control for large-scale, multi-tier systems},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-fed IDS: Meta-learning and federated learning based
fog-cloud approach to detect known and zero-day cyber attacks in IoMT
networks. <em>JPDC</em>, <em>192</em>, 104934. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Medical Things (IoMT) is a transformative fusion of medical sensors, equipment, and the Internet of Things, positioned to transform healthcare. However, security and privacy concerns hinder widespread IoMT adoption, intensified by the scarcity of high-quality datasets for developing effective security solutions. Addressing these challenges, we propose a novel framework for cyberattack detection in dynamic IoMT networks. This framework integrates Federated Learning with Meta-learning, employing a multi-phase architecture for identifying known attacks, and incorporates advanced clustering and biased classifiers to address zero-day attacks. The framework&#39;s deployment is adaptable to dynamic and diverse environments, utilizing an Infrastructure-as-a-Service (IaaS) model on the cloud and a Software-as-a-Service (SaaS) model on the fog end. To reflect real-world scenarios, we introduce a specialized IoMT dataset. Our experimental results indicate high accuracy and low misclassification rates, demonstrating the framework&#39;s capability in detecting cyber threats in complex IoMT environments. This approach shows significant promise in bolstering cybersecurity in advanced healthcare technologies.},
  archive      = {J_JPDC},
  author       = {Umer Zukaib and Xiaohui Cui and Chengliang Zheng and Dong Liang and Salah Ud Din},
  doi          = {10.1016/j.jpdc.2024.104934},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104934},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Meta-fed IDS: Meta-learning and federated learning based fog-cloud approach to detect known and zero-day cyber attacks in IoMT networks},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing CNN inference speed over big social data through
efficient model parallelism for sustainable web of things.
<em>JPDC</em>, <em>192</em>, 104927. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of artificial intelligence and networking technologies has catalyzed the popularity of intelligent services based on deep learning in recent years, which in turn fosters the advancement of Web of Things (WoT). Big social data (BSD) plays an important role during the processing of intelligent services in WoT. However, intelligent BSD services are computationally intensive and require ultra-low latency. End or edge devices with limited computing power cannot realize the extremely low response latency of those services. Distributed inference of deep neural networks (DNNs) on various devices is considered a feasible solution by allocating the computing load of a DNN to several devices. In this work, an efficient model parallelism method that couples convolution layer (Conv) split with resource allocation is proposed. First, given a random computing resource allocation strategy, the Conv split decision is made through a mathematical analysis method to realize the parallel inference of convolutional neural networks (CNNs). Next, Deep Reinforcement Learning is used to get the optimal computing resource allocation strategy to maximize the resource utilization rate and minimize the CNN inference latency. Finally, simulation results show that our approach performs better than the baselines and is applicable for BSD services in WoT with a high workload.},
  archive      = {J_JPDC},
  author       = {Yuhao Hu and Xiaolong Xu and Muhammad Bilal and Weiyi Zhong and Yuwen Liu and Huaizhen Kou and Lingzhen Kong},
  doi          = {10.1016/j.jpdc.2024.104927},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104927},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimizing CNN inference speed over big social data through efficient model parallelism for sustainable web of things},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topo: Towards a fine-grained topological data processing
framework on tianhe-3 supercomputer. <em>JPDC</em>, <em>192</em>,
104926. (<a href="https://doi.org/10.1016/j.jpdc.2024.104926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data frameworks are widely deployed in supercomputers for analyzing large-scale datasets. Topological data processing is an emerging approach that focuses on analyzing the topological structures in high-dimensional scientific data. However, incorporating topological data processing into current big data frameworks presents three main challenges: (1) The frequent data exchange poses challenges to the traditional coarse-grained parallelism. (2) The spatial topology makes parallel programming harder using oversimplified MapReduce APIs. (3) The massive intermediate data and NUMA architecture hinder resource utilization and scalability on novel supercomputers and many-core processors. In this paper, we present Topo, a generic distributed framework that enhances topological data processing on many-core supercomputers. Topo relies on three concepts. (1) It employs fine-grained parallelism, with awareness of topological structures in datasets, to support interactions among collaborative workers before each shuffle phase. (2) It provides intuitive APIs for topological data operations. (3) It implements efficient collective I/O and NUMA-aware dynamic task scheduling to improve multi-threading and load balancing. We evaluate Topo&#39;s performance on the Tianhe-3 supercomputer, which utilizes state-of-the-art ARM many-core processors. Experimental results of execution time show that compared to popular frameworks, Topo achieves an average speedup of 5.3× and 6.3×, with a maximum speedup of 8.4× and 20×, on HPC workloads and big data benchmarks, respectively. Topo further reduces total execution time on processing skewed datasets by 41%.},
  archive      = {J_JPDC},
  author       = {Nan Hu and Yutong Lu and Zhuo Tang and Zhiyong Liu and Dan Huang and Zhiguang Chen},
  doi          = {10.1016/j.jpdc.2024.104926},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104926},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Topo: Towards a fine-grained topological data processing framework on tianhe-3 supercomputer},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast hardware-aware matrix-free algorithms for higher-order
finite-element discretized matrix multivector products on distributed
systems. <em>JPDC</em>, <em>192</em>, 104925. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent hardware-aware matrix-free algorithms for higher-order finite-element (FE) discretized matrix-vector multiplications reduce floating point operations and data access costs compared to traditional sparse matrix approaches. In this work, we address a critical gap in existing matrix-free implementations which are not well suited for the action of FE discretized matrices on very large number of vectors. In particular, we propose efficient matrix-free algorithms for evaluating FE discretized matrix-multivector products on both multi-node CPU and GPU architectures. To this end, we employ batched evaluation strategies, with the batchsize tailored to underlying hardware architectures, leading to better data locality and enabling further parallelization. On CPUs, we utilize even-odd decomposition, SIMD vectorization, and overlapping computation and communication strategies. On GPUs, we develop strategies to overlap compute with data movement for achieving efficient pipelining and reduced data accesses through the use of GPU-shared memory, constant memory and kernel fusion. Our implementation outperforms the baselines for Helmholtz operator action on 1024 vectors, achieving up to 1.4x improvement on one CPU node and up to 2.8x on one GPU node, while reaching up to 4.4x and 1.5x improvement on multiple nodes for CPUs (3072 cores) and GPUs (24 GPUs), respectively. We further benchmark the performance of the proposed implementation for solving a model eigenvalue problem for 1024 smallest eigenvalue-eigenvector pairs by employing the Chebyshev Filtered Subspace Iteration method, achieving up to 1.5x improvement on one CPU node and up to 2.2x on one GPU node while reaching up to 3.0x and 1.4x improvement on multi-node CPUs (3072 cores) and GPUs (24 GPUs), respectively.},
  archive      = {J_JPDC},
  author       = {Gourab Panigrahi and Nikhil Kodali and Debashis Panda and Phani Motamarri},
  doi          = {10.1016/j.jpdc.2024.104925},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104925},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast hardware-aware matrix-free algorithms for higher-order finite-element discretized matrix multivector products on distributed systems},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Routing and wavelength assignment for folded hypercube in
linear array WDM optical networks. <em>JPDC</em>, <em>192</em>, 104924.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The folded hypercube is one of the hypercube variants and is of great significance in the study of interconnection networks. In a folded hypercube, information can be broadcast using efficient distributed algorithms. In the context of parallel computing, folded hypercube has been studied as a possible network topology as an alternative to the hypercube. The routing and wavelength assignment (RWA) problem is significant, since it improves the performance of wavelength-routed all-optical networks constructed using wavelength division multiplexing approach. Given the physical network topology, the aim of the RWA problem is to establish routes for the connection requests and assign the fewest possible wavelengths in accordance with the wavelength continuity and distinct wavelength constraints. This paper discusses the RWA problem in a linear array for the folded hypercube communication pattern by using the congestion technique.},
  archive      = {J_JPDC},
  author       = {V. Vinitha Navis and A. Berin Greeni},
  doi          = {10.1016/j.jpdc.2024.104924},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104924},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Routing and wavelength assignment for folded hypercube in linear array WDM optical networks},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local outlier factor for anomaly detection in HPCC systems.
<em>JPDC</em>, <em>192</em>, 104923. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local Outlier Factor (LOF) is an unsupervised anomaly detection algorithm that finds anomalies by assessing the local density of a data point relative to its neighborhood. Anomaly detection is the process of finding anomalies in datasets. Anomalies in real-time datasets may indicate critical events like bank frauds, data compromise, network threats, etc. This paper deals with the implementation of the LOF algorithm in the HPCC Systems platform, which is an open-source distributed computing platform for big data analytics. Improved LOF is also proposed which efficiently detects anomalies in datasets rich in duplicates. The impact of varying hyperparameters on the performance of LOF is examined in HPCC Systems. This paper examines the performance of LOF with other algorithms like COF, LoOP, and kNN over several datasets in the HPCC Systems. Additionally, the efficacy of LOF is evaluated across big-data frameworks such as Spark, Hadoop, and HPCC Systems, by comparing their runtime performances.},
  archive      = {J_JPDC},
  author       = {Arya Adesh and Shobha G and Jyoti Shetty and Lili Xu},
  doi          = {10.1016/j.jpdc.2024.104923},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104923},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Local outlier factor for anomaly detection in HPCC systems},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PerfTop: Towards performance prediction of distributed
learning over general topology. <em>JPDC</em>, <em>192</em>, 104922. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed learning with multiple GPUs has been widely adopted to accelerate the training process of large-scale deep neural networks. However, misconfiguration of the GPU clusters with various communication primitives and topologies could potentially diminish the gains in parallel computation and lead to significant degradation in training efficiency. Predicting the performance of distributed learning enables service providers to identify potential bottlenecks beforehand. In this work, we propose a Perf ormance prediction framework over General Top ologies, called PerfTop, for accurate estimation of per-iteration execution time. The main strategy is to integrate computation time prediction with an analytical model to map the nonlinearity in communication and fine-grained computation-communication patterns. This enables accurate prediction of a variety of neural network models over general topologies, such as tree, hierarchical, and exponential. Our extensive experiments show that PerfTop outperforms existing methods in estimating both computation and communication time, particularly for communication, surpassing the existing methods by over 45%. Meanwhile, it achieves an accuracy of above 85% in predicting the execution time over general topologies compared to simple topologies such as star and ring from the previous works.},
  archive      = {J_JPDC},
  author       = {Changzhi Yan and Zehan Zhu and Youcheng Niu and Cong Wang and Cheng Zhuo and Jinming Xu},
  doi          = {10.1016/j.jpdc.2024.104922},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104922},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PerfTop: Towards performance prediction of distributed learning over general topology},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing privacy and performance in federated learning: A
systematic literature review on methods and metrics. <em>JPDC</em>,
<em>192</em>, 104918. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) as a novel paradigm in Artificial Intelligence (AI), ensures enhanced privacy by eliminating data centralization and brings learning directly to the edge of the user&#39;s device. Nevertheless, new privacy issues have been raised particularly during training and the exchange of parameters between servers and clients. While several privacy-preserving FL solutions have been developed to mitigate potential breaches in FL architectures, their integration poses its own set of challenges. Incorporating these privacy-preserving mechanisms into FL at the edge computing level can increase both communication and computational overheads, which may, in turn, compromise data utility and learning performance metrics. This paper provides a systematic literature review on essential methods and metrics to support the most appropriate trade-offs between FL privacy and other performance-related application requirements such as accuracy, loss, convergence time, utility, communication, and computation overhead. We aim to provide an extensive overview of recent privacy-preserving mechanisms in FL used across various applications, placing a particular focus on quantitative privacy assessment approaches in FL and the necessity of achieving a balance between privacy and the other requirements of real-world FL applications. This review collects, classifies, and discusses relevant papers in a structured manner, emphasizing challenges, open issues, and promising research directions.},
  archive      = {J_JPDC},
  author       = {Samaneh Mohammadi and Ali Balador and Sima Sinaei and Francesco Flammini},
  doi          = {10.1016/j.jpdc.2024.104918},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104918},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Balancing privacy and performance in federated learning: A systematic literature review on methods and metrics},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GraMeR: Graph meta reinforcement learning for
multi-objective influence maximization. <em>JPDC</em>, <em>192</em>,
104900. (<a href="https://doi.org/10.1016/j.jpdc.2024.104900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) is a combinatorial problem of identifying a subset of seed nodes in a network (graph), which when activated, provide a maximal spread of influence in the network for a given diffusion model and a budget for seed set size. IM has numerous applications such as viral marketing, epidemic control, sensor placement and other network-related tasks. However, its practical uses are limited due to the computational complexity of current algorithms. Recently, deep reinforcement learning has been leveraged to solve IM in order to ease the computational burden. However, there are serious limitations in current approaches, including narrow IM formulation that only consider influence via spread and ignore self activation, low scalability to large graphs, and lack of generalizability across graph families leading to a large running time for every test network. In this work, we address these limitations through a unique approach that involves: (1) Formulating a generic IM problem as a Markov decision process that handles both intrinsic and influence activations; (2) incorporating generalizability via meta-learning across graph families. There are previous works that combine deep reinforcement learning with graph neural network but this work solves a more realistic IM problem and incorporates generalizability across graphs via meta reinforcement learning. Extensive experiments are carried out in various standard networks to validate performance of the proposed Graph Meta Reinforcement learning (GraMeR) framework. The results indicate that GraMeR is multiple orders faster and generic than conventional approaches when applied on small to medium scale graphs.},
  archive      = {J_JPDC},
  author       = {Sai Munikoti and Balasubramaniam Natarajan and Mahantesh Halappanavar},
  doi          = {10.1016/j.jpdc.2024.104900},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {10},
  pages        = {104900},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GraMeR: Graph meta reinforcement learning for multi-objective influence maximization},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale and cooperative graybox parallel optimization on
the supercomputer fugaku. <em>JPDC</em>, <em>191</em>, 104921. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We design, develop and analyze parallel variants of a state-of-the-art graybox optimization algorithm, namely Drils (Deterministic Recombination and Iterated Local Search), for attacking large-scale pseudo-boolean optimization problems on top of the large-scale computing facilities offered by the supercomputer Fugaku. We first adopt a Master/Worker design coupled with a fully distributed Island-based model, ending up with a number of hybrid OpenMP/MPI implementations of high-level parallel Drils versions. We show that such a design, although effective, can be substantially improved by enabling a more focused iteration-level cooperation mechanism between the core graybox components of the original serial Drils algorithm. Extensive experiments are conducted in order to provide a systematic analysis of the impact of the designed parallel algorithms on search behavior, and their ability to compute high-quality solutions using increasing number of CPU-cores. Results using up to 1024×12-cores NUMA nodes, and NK-landscapes with up to 10 , 000 10,000 binary variables are reported, providing evidence on the relative strength of the designed hybrid cooperative graybox parallel search.},
  archive      = {J_JPDC},
  author       = {Lorenzo Canonne and Bilel Derbel and Miwako Tsuji and Mitsuhisa Sato},
  doi          = {10.1016/j.jpdc.2024.104921},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104921},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Large-scale and cooperative graybox parallel optimization on the supercomputer fugaku},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-aware quantum-inspired genetic algorithm for workflow
scheduling in hybrid clouds. <em>JPDC</em>, <em>191</em>, 104920. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing delivers a desirable environment for users to run their different kinds of applications in a cloud. Numerous of these applications (tasks), such as bioinformatics, astronomy, biodiversity, and image analysis, are deadline-sensitive. Such tasks must be properly allocated to virtual machines (VMs) to avoid deadline violations, and they should reduce their execution time and cost. Due to the contradictory environment, minimizing the application task&#39;s completion time and execution cost is extremely difficult. Thus, we propose a Cost-aware Quantum-inspired Genetic Algorithm (CQGA) to minimize the execution time and cost by meeting the deadline constraints. CQGA is motivated by quantum computing and genetic algorithm. It combines quantum operators (measure, interference, and rotation) with genetic operators (selection, crossover, and mutation). Quantum operators are used for better population diversity, quick convergence, time-saving, and robustness. Genetic operators help to produce new individuals, have good fitness values for individuals, and play a significant role in preserving the evolution quality of the population. In addition, CQGA used a quantum bit as a probabilistic representation because it has higher population diversity attributes than other representations. The simulation outcome exhibits that the proposed algorithm can obtain outstanding convergence performance and reduced maximum cost than benchmark algorithms.},
  archive      = {J_JPDC},
  author       = {Mehboob Hussain and Lian-Fu Wei and Amir Rehman and Muqadar Ali and Syed Muhammad Waqas and Fakhar Abbas},
  doi          = {10.1016/j.jpdc.2024.104920},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104920},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cost-aware quantum-inspired genetic algorithm for workflow scheduling in hybrid clouds},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HBPB, applying reuse distance to improve cache efficiency
proactively. <em>JPDC</em>, <em>191</em>, 104919. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cache memories play a significant role in the performance, area, and energy consumption of modern processors, and this impact is expected to grow as on-die memories become larger. While caches are highly effective for cache-friendly access patterns, they introduce unnecessary delays and energy wastage when they fail to serve the required data. Hence, cache bypassing techniques have been proposed to optimize the latency of cache-unfriendly memory accesses. In this scenario, we discuss HBPB , a history-based preemptive bypassing technique that accelerates cache-unfriendly access through the reduced latency of bypassing the caches. By extensively evaluating different real-world applications and hardware cache configurations, we show that HBPB yields energy reductions of up to 75% and performance improvements of up to 50% compared to a version that does not apply cache bypassing. More importantly, we demonstrate that HBPB does not affect the performance of applications with cache-friendly access patterns.},
  archive      = {J_JPDC},
  author       = {Arthur M. Krause and Paulo C. Santos and Arthur F. Lorenzon and Philippe O.A. Navaux},
  doi          = {10.1016/j.jpdc.2024.104919},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104919},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HBPB, applying reuse distance to improve cache efficiency proactively},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CCFTL: A novel continuity compressed page-level flash
address mapping method for SSDs. <em>JPDC</em>, <em>191</em>, 104917.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the distinctive characteristics of flash-based solid-state drives (SSDs), such as out-of-place update scheme, as compared to traditional block storage devices, a flash translation layer (FTL) has been introduced to hide these features. In the FTL, there is an address translation module that implements the conversion from logical addresses to physical addresses. However, existing address mapping algorithms currently fail to fully exploit the mapping information generated by large I/O requests. First, based on this observation, we propose a novel continuity compressed page-level flash address mapping method (CCFTL). This method effectively compresses the mapping relationship between consecutive logical addresses and physical addresses, enabling the storage of more mapping information within the same mapping cache size. Next, we introduce two-level LRU linked list to mitigate the issue of compressed mapping entry splitting that arises from handling write requests. Finally, our experiments show that CCFTL reduced average response times by 52.67%, 16.81%, and 12.71% compared to DFTL, TPFTL, and MFTL, respectively. As the mapping cache size decreases from 2 MB to 1 MB, then further decreases to 256 KB, 128 KB, and eventually down to 64 KB, CCFTL experiences an average decline ratio of less than 3% in average response time, while the other three algorithms show an average decline ratio of 9.51%.},
  archive      = {J_JPDC},
  author       = {Liangkuan Su and Mingwei Lin and Jianpeng Zhang and Yubiao Pan},
  doi          = {10.1016/j.jpdc.2024.104917},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104917},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CCFTL: A novel continuity compressed page-level flash address mapping method for SSDs},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated variational generative learning for heterogeneous
data in distributed environments. <em>JPDC</em>, <em>191</em>, 104916.
(<a href="https://doi.org/10.1016/j.jpdc.2024.104916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributedly training models across diverse clients with heterogeneous data samples can significantly impact the convergence of federated learning. Various novel federated learning methods address these challenges but often require significant communication resources and local computational capacity, leading to reduced global inference accuracy in scenarios with imbalanced label data distribution and quantity skew. To tackle these challenges, we propose FedVGL, a Federated Variational Generative Learning method that directly trains a local generative model to learn the distribution of local features and improve global target model inference accuracy during aggregation, particularly under conditions of severe data heterogeneity. FedVGL facilitates distributed learning by sharing generators and latent vectors with the global server, aiding in global target model training from mapping local data distribution to the variational latent space for feature reconstruction. Additionally, FedVGL implements anonymization and encryption techniques to bolster privacy during generative model transmission and aggregation. In comparison to vanilla federated learning, FedVGL minimizes communication overhead, demonstrating superior accuracy even with minimal communication rounds. It effectively mitigates model drift in scenarios with heterogeneous data, delivering improved target model training outcomes. Empirical results establish FedVGL&#39;s superiority over baseline federated learning methods under severe label imbalance and data skew condition. In a Label-based Dirichlet Distribution setting with α =0.01 and 10 clients using the MNIST dataset, FedVGL achieved an exceptional accuracy over 97% with the VGG-9 target model.},
  archive      = {J_JPDC},
  author       = {Wei Xie and Runqun Xiong and Jinghui Zhang and Jiahui Jin and Junzhou Luo},
  doi          = {10.1016/j.jpdc.2024.104916},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104916},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Federated variational generative learning for heterogeneous data in distributed environments},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient triple modular redundancy scheduling on
heterogeneous multi-core real-time systems. <em>JPDC</em>, <em>191</em>,
104915. (<a href="https://doi.org/10.1016/j.jpdc.2024.104915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Triple modular redundancy (TMR) fault tolerance mechanism can provide almost perfect fault-masking, which has the great potential to enhance the reliability of real-time systems. However, multiple copies of a task are executed concurrently, which will lead to a sharp increase in system energy consumption. In this work, the problem of parallel applications using TMR on heterogeneous multi-core platforms to minimize energy consumption is studied. First, the heterogeneous earliest finish time algorithm is improved, and then according to the given application&#39;s deadline constraints and reliability requirements, an algorithm to extend the execution time of the copies is designed. Secondly, based on the properties of TMR, an algorithm for minimizing the execution overhead of the third copy (MEOTC) is designed. Finally, considering the actual situation of task execution, an online energy management (OEM) method is proposed. The proposed algorithms were compared with the state-of-the-art AFTSA algorithm, and the results show significant differences in energy consumption. Specifically, for light fault detection, the energy consumption of the MEOTC and OEM algorithms was found to be 80% and 72% respectively, compared with AFTSA. For heavy fault detection, the energy consumption of MEOTC and OEM was measured at 61% and 55% respectively, compared with AFTSA.},
  archive      = {J_JPDC},
  author       = {Hongzhi Xu and Binlian Zhang and Chen Pan and Keqin Li},
  doi          = {10.1016/j.jpdc.2024.104915},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104915},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-efficient triple modular redundancy scheduling on heterogeneous multi-core real-time systems},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSI−FL: Self-sovereign identity based privacy-preserving
federated learning. <em>JPDC</em>, <em>191</em>, 104907. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional federated learning ( FL FL ) raises security and privacy concerns such as identity fraud, data poisoning attacks, membership inference attacks, and model inversion attacks. In the conventional FL FL , any entity can falsify its identity and initiate data poisoning attacks. Besides, adversaries ( AD AD ) holding the updated global model parameters can retrieve the plain text of the dataset by initiating membership inference attacks and model inversion attacks. To the best of our knowledge, this is the first work to propose a self-sovereign identity ( SSI SSI ) and differential privacy ( DP DP ) based FL FL namely SSI − FL SSI−FL for addressing all the above issues. The first step in the SSI − FL SSI−FL framework involves establishing a secure connection based on blockchain-based SSI SSI . This secure connection protects against unauthorized access attacks of any AD AD and ensures the transmitted data&#39;s authenticity, integrity, and availability. The second step applies DP DP to protect against model inversion attacks and membership inference attacks. The third step focuses on establishing FL FL with a novel hybrid deep learning to achieve better scores than conventional methods. The SSI − FL SSI−FL performance analysis is done based on security, formal, scalability, and score analysis. Moreover, the proposed method outperforms all the state-of-art techniques.},
  archive      = {J_JPDC},
  author       = {Rakib Ul Haque and A.S.M. Touhidul Hasan and Mohammed Ali Mohammed Al-Hababi and Yuqing Zhang and Dianxiang Xu},
  doi          = {10.1016/j.jpdc.2024.104907},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104907},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SSI−FL: Self-sovereign identity based privacy-preserving federated learning},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal dynamics analysis and parameter optimization
of a network epidemic-like propagation model based on neural network
method. <em>JPDC</em>, <em>191</em>, 104906. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a reaction-diffusion model is established to study the dynamic behavior of rumor propagation. Firstly, we consider the existence of the positive equilibrium points. Then, we perform a stability analysis to study the conditions for the occurrence of Turing instability. Secondly, we use multiscale analysis to derive the expression of the amplitude equation. In the process of numerical simulation, the reality is considered. It shows that controlling the spread rate of rumor and the number of new Internet users have a great effect on curbing the spread of online rumor. Furthermore, it is proved that the analysis of amplitude equation plays a decisive role in the formation of Turing patterns. We also discuss the phenomenon of Turing patterns when the network structure changes and verify the rationality of the model by Monte Carlo method. Finally, we consider two methods based on statistical principle and convolutional neural network severally to identify the parameters of the reaction-diffusion system with Turing instability by using stable patterns. The statistical principle-based method offers superior accuracy, whereas the convolutional neural network-based approach significantly reduces recognition time and cuts down time costs.},
  archive      = {J_JPDC},
  author       = {Shuling Shen and Xinlin Chen and Linhe Zhu},
  doi          = {10.1016/j.jpdc.2024.104906},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104906},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Spatiotemporal dynamics analysis and parameter optimization of a network epidemic-like propagation model based on neural network method},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The rabin numbers of enhanced hypercubes. <em>JPDC</em>,
<em>191</em>, 104905. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ω -Rabin number r ω ( G ) rω(G) and strong ω -Rabin number ⁎ r ω ⁎ ( G ) rω⁎(G) are two effective parameters to assess transmission latency and fault tolerance of an interconnection network G . As determining the Rabin number of a general graph is NP-complete, we consider the Rabin number of the enhanced hypercube Q n , k Qn,k which is a variant of the hypercube Q n Qn . For n ≥ k ≥ 5 n≥k≥5 , we prove that ⁎ r ω ( Q n , k ) = r ω ⁎ ( Q n , k ) = d ( Q n , k ) rω(Qn,k)=rω⁎(Qn,k)=d(Qn,k) for 1 ≤ ω &lt; n − ⌊ k 2 ⌋ 1≤ω&amp;lt;n−⌊k2⌋ ; ⁎ r ω ( Q n , k ) = r ω ⁎ ( Q n , k ) = d ( Q n , k ) + 1 rω(Qn,k)=rω⁎(Qn,k)=d(Qn,k)+1 for n − ⌊ k 2 ⌋ ≤ ω ≤ n + 1 n−⌊k2⌋≤ω≤n+1 , where d ( Q n , k ) d(Qn,k) is the diameter of Q n , k Qn,k . In addition, we present algorithms to construct internally disjoint paths of length at most ⁎ r ω ⁎ ( Q n , k ) rω⁎(Qn,k) from a source vertex to other ω ( 1 ≤ ω ≤ n + 1 1≤ω≤n+1 ) destination vertices (not necessarily distinct) in Q n , k Qn,k .},
  archive      = {J_JPDC},
  author       = {Chaoming Guo and Meijie Ma and Xiang-Jun Li and Guijuan Wang},
  doi          = {10.1016/j.jpdc.2024.104905},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104905},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {The rabin numbers of enhanced hypercubes},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DuMato: An efficient warp-centric subgraph enumeration
system for GPU. <em>JPDC</em>, <em>191</em>, 104903. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgraph enumeration is a heavy-computing procedure that lies at the core of Graph Pattern Mining (GPM) algorithms, whose goal is to extract subgraphs from larger graphs according to a given property. Scaling GPM algorithms for GPUs is challenging due to irregularity, high memory demand, and non-trivial choice of enumeration paradigms. In this work we propose a depth-first-search subgraph exploration strategy (DFS-wide) to improve the memory locality and access patterns across different enumeration paradigms. We design a warp-centric workflow to the problem that reduces divergences and ensures that accesses to graph data are coalesced. A weight-based dynamic workload redistribution is also proposed to mitigate load imbalance. We put together these strategies in a system called DuMato, allowing efficient implementations of several GPM algorithms via a common set of GPU primitives. Our experiments show that DuMato&#39;s optimizations are effective and that it enables exploring larger subgraphs when compared to state-of-the-art systems.},
  archive      = {J_JPDC},
  author       = {Samuel Ferraz and Vinicius Dias and Carlos H.C. Teixeira and Srinivasan Parthasarathy and George Teodoro and Wagner Meira Jr.},
  doi          = {10.1016/j.jpdc.2024.104903},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104903},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DuMato: An efficient warp-centric subgraph enumeration system for GPU},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ion-molecule collision cross-section calculations using
trajectory parallelization in distributed systems. <em>JPDC</em>,
<em>191</em>, 104902. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ion Mobility coupled with Mass Spectrometry (IM-MS) stands as a strong analytical method for structurally characterizing complex molecules. In IM-MS, the sample under investigation is ionized and propelled by an electric field into a drift tube, which collides against a buffer gas. The separation of the ion gas phase is then measured through the differences in their rotationally averaged Collision Cross-Section (CCS) values. The effectiveness of the measured Collision Cross-Section (CCS) for structural characterization critically depends on the validation against theoretical calculations. This validation process relies on intensive molecular mechanics simulations, which can be computationally demanding, especially for large systems such as molecular assemblies and viruses. Therefore, reliable and fast CCS calculations are needed to help interpret IM-MS experimental data. This work presents the MassCCS software, which considerably increases the CCS simulation performance by implementing a linked-cell-based algorithm, incorporating High-Performance Computing (HPC) techniques. We performed extensive tests regarding the system size, shape, and number of CPU cores. Experimental results reveal speedups up to 3 orders of magnitude faster than Collision Simulator for Ion Mobility Spectrometry (CoSIMS) and High-Performance Collision Cross Section (HPCCS), optimized solutions for CCS simulations, for a single node execution. In addition, we extended MassCCS at the inter-node level by employing OpenMP Cluster (OMPC). OMPC is an innovative programming model designed for the development of HPC applications. It streamlines the development process and simplifies software maintenance using only OpenMP directives. Notably, OMPC delivers a performance level comparable to a pure MPI implementation. This enhancement enabled expensive CCS calculations using nitrogen buffer gas for large systems such as human adenovirus with ∼11 million atoms in just ∼4 min, making MassCCS the most performant software nowadays, to the best of our knowledge. MassCCS is available as free software for Academic use at https://github.com/cces-cepid/massccs .},
  archive      = {J_JPDC},
  author       = {Samuel Cajahuaringa and Leandro N. Zanotto and Sandro Rigo and Hervé Yviquel and Munir S. Skaf and Guido Araujo},
  doi          = {10.1016/j.jpdc.2024.104902},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104902},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Ion-molecule collision cross-section calculations using trajectory parallelization in distributed systems},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel multi-cluster workflow system to support real-time
HPC-enabled epidemic science: Investigating the impact of vaccine
acceptance on COVID-19 spread. <em>JPDC</em>, <em>191</em>, 104899. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present MacKenzie, a HPC-driven multi-cluster workflow system that was used repeatedly to configure and execute fine-grained US national-scale epidemic simulation models during the COVID-19 pandemic. Mackenzie supported federal and Virginia policymakers, in real-time, for a large number of “what-if” scenarios during the COVID-19 pandemic, and continues to be used to answer related questions as COVID-19 transitions to the endemic stage of the disease. MacKenzie is a novel HPC meta-scheduler that can execute US-scale simulation models and associated workflows that typically present significant big data challenges. The meta-scheduler optimizes the total execution time of simulations in the workflow, and helps improve overall human productivity. As an exemplar of the kind of studies that can be conducted using Mackenzie, we present a modeling study to understand the impact of vaccine-acceptance in controlling the spread of COVID-19 in the US. We use a 288 million node synthetic social contact network (digital twin) spanning all 50 US states plus Washington DC, comprised of 3300 counties, with 12 billion daily interactions. The highly-resolved agent-based model used for the epidemic simulations uses realistic information about disease progression, vaccine uptake, production schedules, acceptance trends, prevalence, and social distancing guidelines. Computational experiments show that, for the simulation workload discussed above, MacKenzie is able to scale up well to 10 K CPU cores. Our modeling results show that, when compared to faster and accelerating vaccinations, slower vaccination rates due to vaccine hesitancy cause averted infections to drop from 6.7M to 4.5M, and averted total deaths to drop from 39.4 K to 28.2 K across the US. This occurs despite the fact that the final vaccine coverage is the same in both scenarios. We also find that if vaccine acceptance could be increased by 10% in all states, averted infections could be increased from 4.5M to 4.7M (a 4.4% improvement) and total averted deaths could be increased from 28.2 K to 29.9 K (a 6% improvement) nationwide.},
  archive      = {J_JPDC},
  author       = {Parantapa Bhattacharya and Dustin Machi and Jiangzhuo Chen and Stefan Hoops and Bryan Lewis and Henning Mortveit and Srinivasan Venkatramanan and Mandy L. Wilson and Achla Marathe and Przemyslaw Porebski and Brian Klahn and Joseph Outten and Anil Vullikanti and Dawen Xie and Abhijin Adiga and Shawn Brown and Christopher Barrett and Madhav Marathe},
  doi          = {10.1016/j.jpdc.2024.104899},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {9},
  pages        = {104899},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Novel multi-cluster workflow system to support real-time HPC-enabled epidemic science: Investigating the impact of vaccine acceptance on COVID-19 spread},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient topology reconfiguration for NoC-based
multiprocessors: A greedy-memetic algorithm. <em>JPDC</em>,
<em>190</em>, 104904. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-core processor systems, the Network-on-Chip (NoC) serves as a vital communication infrastructure. To ensure chip reliability during potential failures, this paper proposes a two-level topology reconfiguration algorithm with core-level redundancy technology. Initially, a heuristic topology reconfiguration method utilizing a greedy strategy is proposed to perform local replacement of faulty processing elements (PEs) and generate an initial logical topology with shorter interconnection paths between PEs. Then, an intelligent optimization method based on memetic algorithm is introduced to optimize the generated initial topology for better communication performance. The experimental results demonstrate that compared to the current state-of-the-art algorithm, the proposed algorithm achieves an average improvement of 13.92% and 30.83% on various size topologies in terms of distance factor (DF) and congestion factor (CF), which represent communication delay and traffic balance respectively. The proposed algorithm significantly enhances the communication performance of the target topology, mitigating communication latency and potential congestion problems.},
  archive      = {J_JPDC},
  author       = {Junyan Qian and Chuanfang Zhang and Zheng Wu and Hao Ding and Long Li},
  doi          = {10.1016/j.jpdc.2024.104904},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104904},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient topology reconfiguration for NoC-based multiprocessors: A greedy-memetic algorithm},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CUDA acceleration of MI-based feature selection methods.
<em>JPDC</em>, <em>190</em>, 104901. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection algorithms are necessary nowadays for machine learning as they are capable of removing irrelevant and redundant information to reduce the dimensionality of the data and improve the quality of subsequent analyses. The problem with current feature selection approaches is that they are computationally expensive when processing large datasets. This work presents parallel implementations for Nvidia GPUs of three highly-used feature selection methods based on the Mutual Information (MI) metric: mRMR, JMI and DISR. Publicly available code includes not only CUDA implementations of the general methods, but also an adaptation of them to work with low-precision fixed point in order to further increase their performance on GPUs. The experimental evaluation was carried out on two modern Nvidia GPUs (Turing T4 and Ampere A100) with highly satisfactory results, achieving speedups of up to 283x when compared to state-of-the-art C implementations.},
  archive      = {J_JPDC},
  author       = {Bieito Beceiro and Jorge González-Domínguez and Laura Morán-Fernández and Verónica Bolón-Canedo and Juan Touriño},
  doi          = {10.1016/j.jpdc.2024.104901},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104901},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CUDA acceleration of MI-based feature selection methods},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and lightweight in-memory computing architecture
for hardware security. <em>JPDC</em>, <em>190</em>, 104898. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative solution for improving the efficiency and speed of the Advanced Encryption Standard (AES) based cryptographic algorithm. The approach leverages in-memory computing (IMC) and is versatile for application across a broad spectrum of IoT applications, including robotic autonomous vehicles and various other scenarios. To achieve this goal, memristor (MR) designs are proposed to emulate the arithmetic operations required for different phases of the AES algorithm, enabling efficient in-memory processing. The key contributions of this work include; 1) The development of a 4 bit-MR state element for implementing different arithmetic operations in an AES hardware prototype; 2) The creation of a pipeline AES design for massive parallelism and MR integration compatibility; and 3) The hardware implementation of the AES-IMC based architecture using the MR emulator. The results show that AES-IMC performs better than existing architectures in terms of higher throughput and energy efficiency. Compared to conventional AES hardware, AES-IMC achieves a 30% power enhancement with comparable throughput. Additionally, when compared to state-of-the-art AES-based NVM engines, AES-IMC demonstrates comparable power dissipation and a 62% increase in throughput. The IMC architecture enables cost-effective real-time deployment of AES, leading to high-performance computing. By leveraging the power of in-memory computing, this system is able to provide improved computational efficiency and faster processing speeds, making it a promising solution for a wide range of applications in the field of autonomous driving and robotics. The potential benefits of this system include improved safety and security of unmanned devices, as well as enhanced performance and cost-effectiveness in a variety of computing environments.},
  archive      = {J_JPDC},
  author       = {Hala Ajmi and Fakhreddine Zayer and Amira Hadj Fredj and Hamdi Belgacem and Baker Mohammad and Naoufel Werghi and Jorge Dias},
  doi          = {10.1016/j.jpdc.2024.104898},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104898},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient and lightweight in-memory computing architecture for hardware security},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting inherent elasticity of serverless in algorithms
with unbalanced and irregular workloads. <em>JPDC</em>, <em>190</em>,
104891. (<a href="https://doi.org/10.1016/j.jpdc.2024.104891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service execution model in serverless computing has been successful in running large-scale computations like MapReduce, linear algebra, and machine learning. However, little attention has been given to executing highly-dynamic parallel applications with unbalanced and irregular workloads. These algorithms are difficult to execute with good parallel efficiency due to the challenge of provisioning the required computing resources in time, leading to resource over- and under-provisioning in clusters of static size. We propose that the elasticity and fine-grained “pay-as-you-go model” of the FaaS model can be a key enabler for effectively running these algorithms in the cloud. We use a simple serverless executor pool abstraction, and evaluate it using three algorithms with unbalanced and irregular workloads. Results show that their serverless implementation can outperform a static Spark cluster of large virtual machines by up to 55% with the same cost, and can even outperform a single large virtual machine running locally.},
  archive      = {J_JPDC},
  author       = {Gerard Finol and Gerard París and Pedro García-López and Marc Sánchez-Artigas},
  doi          = {10.1016/j.jpdc.2024.104891},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104891},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exploiting inherent elasticity of serverless in algorithms with unbalanced and irregular workloads},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing DNN training with pipeline model parallelism for
enhanced performance in embedded systems. <em>JPDC</em>, <em>190</em>,
104890. (<a href="https://doi.org/10.1016/j.jpdc.2024.104890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have gained widespread popularity in different domain applications due to their dominant performance. Despite the prevalence of massively parallel multi-core processor architectures, adopting large DNN models in embedded systems remains challenging, as most embedded applications are designed with single-core processors in mind. This limits DNN adoption in embedded systems due to inefficient leveraging of model parallelization and workload partitioning. Prior solutions attempt to address these challenges using data and model parallelism. However, they lack in finding optimal DNN model partitions and distributing them efficiently to achieve improved performance. This paper proposes a DNN model parallelism framework to accelerate model training by finding the optimal number of model partitions and resource provisions. The proposed framework combines data and model parallelism techniques to optimize the parallel processing of DNNs for embedded applications. In addition, it implements the pipeline execution of the partitioned models and integrates a task controller to manage the computing resources. The experimental results for image object detection demonstrate the applicability of our proposed framework in estimating the latest execution time and reducing overall model training time by almost 44.87% compared to the baseline AlexNet convolutional neural network (CNN) model.},
  archive      = {J_JPDC},
  author       = {Md Al Maruf and Akramul Azim and Nitin Auluck and Mansi Sahi},
  doi          = {10.1016/j.jpdc.2024.104890},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104890},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimizing DNN training with pipeline model parallelism for enhanced performance in embedded systems},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-dimensional time-aware cloud service recommendation
approach with enhanced similarity and trust. <em>JPDC</em>,
<em>190</em>, 104889. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Filtering (CF) is one of the most successful techniques for quality-of-service (QoS) prediction and cloud service recommendation. However, individual QoS are time-sensitive and fluctuating, resulting in the QoS predicted by CF to deviate from the actual values. In addition, existing CF approaches ignore inauthentic QoS values given by untrustworthy users. To address these problems, we develop a two-dimensional time-aware and trust-aware service recommendation approach (TaTruSR). First, considering both timeliness and fluctuation of service QoS, an integrative method incorporates time weight (time dimension) and temporal certainty (QoS dimension) are proposed to determine the contribution of co-invoked services. Time weight is computed by a personalized logistic decay function to measure QoS changes by weighting the length of the time interval, while temporal certainty is defined by entropy to acquire the degree of QoS fluctuation over a period of time. Second, a set of most similar and trusted neighbors can be identified from the view of the time-aware similarity model and trust model. In models, the direct similarity and local trust are calculated based on the QoS ratings and contribution of co-invoked services to improve the prediction accuracy and eliminate unreliable QoS. The indirect similarity and global trust are estimated based on user relationship networks to alleviate the data sparsity problem. Finally, missing QoS prediction and reliable service recommendation for the active user can be achieved based on enhanced similarity and trust. A case study and experimental evaluation on real-world datasets demonstrate the practicality and accuracy of the proposed approach.},
  archive      = {J_JPDC},
  author       = {Chunhua Tang and Shuangyao Zhao and Binbin Chen and Xiaonong Lu and Qiang Zhang},
  doi          = {10.1016/j.jpdc.2024.104889},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104889},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A two-dimensional time-aware cloud service recommendation approach with enhanced similarity and trust},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter identification method of a reaction-diffusion
network information propagation system based on optimization theory.
<em>JPDC</em>, <em>190</em>, 104888. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the times, rumors spread rapidly on the Internet. Firstly, this paper establishes a reaction-diffusion system with Allee effect to describe the rumor spreading process and derives the necessary conditions for the emergence of Turing bifurcation. Next, a parameter identification approach utilizing optimal control theory is shown. Ultimately, the impact of the magnitude of the certain parameters in the objective function on parameter identification is examined through numerous parameter identifications in continuous space and various complex networks. Additionally, the convergence rates and error magnitudes of different algorithms for parameter identification are studied across different spatial structures.},
  archive      = {J_JPDC},
  author       = {Yi Ding and Linhe Zhu},
  doi          = {10.1016/j.jpdc.2024.104888},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104888},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parameter identification method of a reaction-diffusion network information propagation system based on optimization theory},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Paired 2-disjoint path covers of k-ary n-cubes under the
partitioned edge fault model. <em>JPDC</em>, <em>190</em>, 104887. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k -ary n -cube Q n k Qnk serves as an indispensable interconnection network in the design of data center networks, network-on-chips, and parallel computing systems since it possesses numerous attractive properties. In these parallel architectures, the paired (or unpaired) many-to-many m -disjoint path cover ( m -DPC) plays a significant role in message transmission. Nevertheless, the construction of m -DPC is severely obstructed by large-scale edge faults due to the rapid growth of the system scale. In this paper, we investigate the existence of paired 2-DPC in Q n k Qnk under the partitioned edge fault (PEF) model, which is a novel fault model for enhancing the networks&#39; fault-tolerance related to path embedding problem. We exploit this model to evaluate the edge fault-tolerance of Q n k Qnk when a paired 2-DPC is embedded into Q n k Qnk . Compared to the other known works, our results can help Q n k Qnk to achieve large-scale edge fault-tolerance.},
  archive      = {J_JPDC},
  author       = {Hongbin Zhuang and Xiao-Yan Li and Jou-Ming Chang and Ximeng Liu},
  doi          = {10.1016/j.jpdc.2024.104887},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104887},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Paired 2-disjoint path covers of k-ary n-cubes under the partitioned edge fault model},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability assessment for k-ary n-cubes with faulty edges.
<em>JPDC</em>, <em>190</em>, 104886. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The g -restricted edge connectivity is an important measurement to assess the reliability of networks. The g -restricted edge connectivity of a connected graph G is the minimum size of a set of edges in G , if it exists, whose deletion separates G and leaves every vertex in the remaining components with at least g neighbors. The k -ary n -cube is an extension of the hypercube network and has many desirable properties. It has been used to build the architecture of the Supercomputer Fugaku. This paper establishes that for g ≤ n g≤n , the g -restricted edge connectivity of 3-ary n -cubes is 3 ⌊ g / 2 ⌋ ( 1 + ( g mod 2 ) ) ( 2 n − g ) 3⌊g/2⌋(1+(gmod2))(2n−g) , and the g -restricted edge connectivity of k -ary n -cubes with k ≥ 4 k≥4 is 2 g ( 2 n − g ) 2g(2n−g) . These results imply that in Q n 3 Qn3 with at most 3 ⌊ g / 2 ⌋ ( 1 + ( g mod 2 ) ) ( 2 n − g ) − 1 3⌊g/2⌋(1+(gmod2))(2n−g)−1 faulty edges, or Q n k ( k ≥ 4 ) Qnk(k≥4) with at most 2 g ( 2 n − g ) − 1 2g(2n−g)−1 faulty edges, if each vertex is incident with at least g fault-free edges, then the remaining network is connected.},
  archive      = {J_JPDC},
  author       = {Si-Yu Li and Xiang-Jun Li and Meijie Ma},
  doi          = {10.1016/j.jpdc.2024.104886},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104886},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliability assessment for k-ary n-cubes with faulty edges},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast knowledge graph completion using graphics processing
units. <em>JPDC</em>, <em>190</em>, 104885. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs can be used in many areas related to data semantics such as question-answering systems, knowledge based systems. However, the currently constructed knowledge graphs need to be complemented for better knowledge in terms of relations. It is called knowledge graph completion. To add new relations to the existing knowledge graph by using knowledge graph embedding models, we have to evaluate N × N × R N×N×R vector operations, where N is the number of entities and R is the number of relation types. It is very costly. In this paper, we provide an efficient knowledge graph completion framework on GPUs to get new relations using knowledge graph embedding vectors. In the proposed framework, we first define transformable to a metric space and then provide a method to transform the knowledge graph completion problem into the similarity join problem for a model which is transformable to a metric space . After that, to efficiently process the similarity join problem, we derive formulas using the properties of a metric space. Based on the formulas, we develop a fast knowledge graph completion algorithm. Finally, we experimentally show that our framework can efficiently process the knowledge graph completion problem.},
  archive      = {J_JPDC},
  author       = {Chun-Hee Lee and Dong-oh Kang and Hwa Jeon Song},
  doi          = {10.1016/j.jpdc.2024.104885},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104885},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast knowledge graph completion using graphics processing units},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel HPL-AI approach for FP16-only accelerator and its
instantiation on kunpeng+ascend AI-specific platform. <em>JPDC</em>,
<em>190</em>, 104884. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {HPL-AI, also known as HPL-MxP, is a new benchmark program used to evaluate the upper-bound performance of AI-related tasks on a specific computing cluster. It solves a large linear equation system in FP64, preconditioned by complete LU factorization in lower precision. In this paper, we propose a new HPL-AI approach that relies on the factorization of the coefficient matrix in mixed precision: FP32 diagonals and FP16 off-diagonals. Without compromising the quality of the resultant LU preconditioner, the proposed approach only utilizes the primitive of dense matrix multiplication in FP16 on the accelerator, maximizing the FP16 throughput. Numerical analysis and experiments validate our approach, ensuring avoidance of numerical underflow or overflow during factorization. We implement the proposed approach on Kunpeng+Ascend clusters, a novel AI-specific platform with exceedingly high FP16 peak performance. By applying various optimization techniques, including 2D lookahead, HCCL-based communication pipeline, and SYCL-based tasks overlapping, we achieve 975 TFlops on a single node and nearly 100 PFlops on a cluster of 128 nodes, with a weak scalability of 79.8%.},
  archive      = {J_JPDC},
  author       = {Zijian Cao and Qiao Sun and Wenhao Yang and Changcheng Song and Zhe Wang and Huiyuan Li},
  doi          = {10.1016/j.jpdc.2024.104884},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104884},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel HPL-AI approach for FP16-only accelerator and its instantiation on Kunpeng+Ascend AI-specific platform},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed learning based on robust diffusion SGD over
adaptive networks with noisy output data. <em>JPDC</em>, <em>190</em>,
104883. (<a href="https://doi.org/10.1016/j.jpdc.2024.104883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outliers and noises are unavoidable factors that cause performance of the distributed learning algorithms to be severely reduced. Developing a robust algorithm is vital in applications such as system identification and forecasting stock market, in which noise on the desired signals may intensely divert the solutions. In this paper, we propose a Robust Diffusion Stochastic Gradient Descent (RDSGD) algorithm based on the pseudo-Huber loss function which can significantly suppress the effect of Gaussian and non-Gaussian noises on estimation performances in the adaptive networks. Performance and convergence behavior of RDSGD are assessed in presence of the α -stable and Mixed-Gaussian noises in the stationary and non-stationary environments. Simulation results show that the proposed algorithm can achieve both higher convergence rate and lower steady-state misadjustment than the conventional diffusion algorithms and several robust algorithms.},
  archive      = {J_JPDC},
  author       = {Fatemeh Barani and Abdorreza Savadi and Hadi Sadoghi Yazdi},
  doi          = {10.1016/j.jpdc.2024.104883},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104883},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A distributed learning based on robust diffusion SGD over adaptive networks with noisy output data},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A characterization of soft-error sensitivity in
data-parallel and model-parallel distributed deep learning.
<em>JPDC</em>, <em>190</em>, 104879. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest advances in artificial intelligence deep learning models are unprecedented. A wide spectrum of application areas is now thriving thanks to available massive training datasets and gigantic complex neural network models. Those two characteristics demand outstanding computing power that only advanced computing platforms can provide. Therefore, distributed deep learning has become a necessity in capitalizing on the potential of cutting-edge artificial intelligence. Two basic schemes have emerged in distributed learning. First, the data-parallel approach, which aims at dividing the training dataset into multiple computing nodes. Second, the model-parallel approach, which splits layers of a model into several computing nodes. Each scheme has its upsides and downsides, particularly when running on large machines that are susceptible to soft errors. Those errors occur as a consequence of several factors involved in the manufacturing process of current electronic components of supercomputers. On many occasions, those errors are expressed as bit flips that do not cause the whole system to crash, but generate wrong numerical results in computations. To study the effect of soft error on different approaches for distributed learning, we leverage checkpoint alteration, a technique that injects bit flips on checkpoint files. It allows researchers to understand the effect of soft errors on applications that produce checkpoint files in HDF5 format. This paper uses the popular deep learning PyTorch tool on two distributed-learning platforms: one for data-parallel training and one for model-parallel training. We use well-known deep learning models with popular training datasets to provide a picture of how soft errors challenge the training phase of a deep learning model.},
  archive      = {J_JPDC},
  author       = {Elvis Rojas and Diego Pérez and Esteban Meneses},
  doi          = {10.1016/j.jpdc.2024.104879},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {8},
  pages        = {104879},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A characterization of soft-error sensitivity in data-parallel and model-parallel distributed deep learning},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cloud-edge-end workflow scheduling with multiple privacy
levels. <em>JPDC</em>, <em>189</em>, 104882. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cloud-edge-end architecture satisfies the execution requirements of various workflow applications. However, owing to the diversity of resources, the complex hierarchical structure, and different privacy requirements for users, determining how to lease suitable cloud-edge-end resources, schedule multi-privacy-level workflow tasks, and optimize leasing costs is currently one of the key challenges in cloud computing. In this paper, we address the scheduling optimization problem of workflow applications containing tasks with multiple privacy levels. To tackle this problem, we propose a heuristic privacy-preserving workflow scheduling algorithm (PWHSA) designed to minimize rental costs which includes time parameter estimation, task sub-deadline division, scheduling sequence generation, task scheduling, and task adjustment, with candidate strategies developed for each component. These candidate strategies in each step undergo statistical calibration across a comprehensive set of workflow instances. We compare the proposed algorithm with modified classical algorithms that target similar problems. The experimental results demonstrate that the PWHSA algorithm outperforms the comparison algorithms while maintaining acceptable execution times.},
  archive      = {J_JPDC},
  author       = {Shuang Wang and Zian Yuan and Xiaodong Zhang and Jiawen Wu and Yamin Wang},
  doi          = {10.1016/j.jpdc.2024.104882},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104882},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cloud-edge-end workflow scheduling with multiple privacy levels},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel framework for generic spark workload
characterization and similar pattern recognition using machine learning.
<em>JPDC</em>, <em>189</em>, 104881. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehensive workload characterization plays a pivotal role in comprehending Spark applications, as it enables the analysis of diverse aspects and behaviors. This understanding is indispensable for devising downstream tuning objectives, such as performance improvement. To address this pivotal issue, our work introduces a novel and scalable framework for generic Spark workload characterization, complemented by consistent geometric measurements. The presented approach aims to build robust workload descriptors by profiling only quantitative metrics at the application task-level, in a non-intrusive manner. We expand our framework for downstream workload pattern recognition by incorporating unsupervised machine learning techniques: clustering algorithms and feature selection. These techniques significantly improve the process of grouping similar workloads without relying on predefined labels. We effectively recognize 24 representative Spark workloads from diverse domains, including SQL, machine learning, web search, graph, and micro-benchmarks, available in HiBench. Our framework achieves a high accuracy F-Measure score of up to 90.9% and a Normalized Mutual Information of up to 94.5% in similar workload pattern recognition. These scores significantly outperform the results obtained in a comparative analysis with an established workload characterization approach in the literature.},
  archive      = {J_JPDC},
  author       = {Mariano Garralda-Barrio and Carlos Eiras-Franco and Verónica Bolón-Canedo},
  doi          = {10.1016/j.jpdc.2024.104881},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104881},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel framework for generic spark workload characterization and similar pattern recognition using machine learning},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-driven hybrid scaling for multi-type services in
cloud. <em>JPDC</em>, <em>189</em>, 104880. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to deal with the fast changing requirements of container based services in clouds, auto-scaling is used as an essential mechanism for adapting the number of provisioned resources with the variable service workloads. However, the latest auto-scaling approaches lack the comprehensive consideration of variable workloads and hybrid auto-scaling for multi-type services. Firstly, the historical data based proactive approaches are widely used to handle complex and variable workloads in advance. The decision-making accuracy of proactive approaches depends on the prediction algorithm, which is affected by the anomalies, missing values and errors in the historical workload data, and the unexpected workload cannot be handled. Secondly, the trigger based reactive approaches are seriously affected by workload fluctuation which causes the frequent invalid scaling of service resources. Besides, due to the existence of scaling time, there are different completion delays of different scaling actions. Thirdly, the latest approaches also ignore the different scaling time of hybrid scaling for multi-type services including stateful services and stateless services. Especially, when the stateful services are scaled horizontally, the neglected long scaling time causes the untimely supply and withdrawal of resources. Consequently, all three issues above can lead to the degradation of Quality of Services (QoS) and the inefficient utilization of resources. This paper proposes a new hybrid auto-scaling approach for multi-type services to resolve the impact of service scaling time on decision making. We combine the proactive scaling strategy with the reactive anomaly detection and correction mechanism. For making a proactive decision, the ensemble learning model with the structure improved deep network is designed to predict the future workload. On the basis of the predicted results and the scaling time of different types of services, the auto-scaling decisions are made by a Deep Reinforcement Learning (DRL) model with heterogeneous action space, which integrates horizontal and vertical scaling actions. Meanwhile, with the anomaly detection and correction mechanism, the workload fluctuation and unexpected workload can be detected and handled. We evaluate our approach against three different proactive and reactive auto-scaling approaches in the cloud environment, and the experimental results show the proposed approach can achieve the better scaling behavior compared to state-of-the-art approaches.},
  archive      = {J_JPDC},
  author       = {Haitao Zhang and Tongyu Guo and Wei Tian and Huadong Ma},
  doi          = {10.1016/j.jpdc.2024.104880},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104880},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Learning-driven hybrid scaling for multi-type services in cloud},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCIPIS: Scalable and concurrent persistent indexing and
search in high-end computing systems. <em>JPDC</em>, <em>189</em>,
104878. (<a href="https://doi.org/10.1016/j.jpdc.2024.104878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While it is now routine to search for data on a personal computer or discover data online, there is no such equivalent method for discovering data on large parallel and distributed file systems commonly deployed on HPC systems. In contrast to web search, which has to deal with a larger number of relatively small files, in HPC applications there is a need to also support efficient indexing of large files. We propose SCIPIS, an indexing and search framework, that can exploit the properties of modern high-end computing systems, with many-core architectures, multiple NUMA nodes and multiple NVMe storage devices. SCIPIS supports building and searching TFIDF persistent indexes, and can deliver orders of magnitude better performance than state-of-the-art approaches. We achieve scalability and performance of indexing by decomposing the indexing process into separate components that can be optimized independently, by building disk-friendly data structures in-memory that can be persisted in long sequential writes, and by avoiding communication between indexing threads that collaboratively build an index over a collection of large files. We evaluated SCIPIS with three types of datasets (logs, scientific data, and metadata), on systems with configurations up to 192-cores, 768 GiB of RAM, 8 NUMA nodes, and up to 16 NVMe drives, and achieved up to 29x better indexing while maintaining similar search latency when compared to Apache Lucene.},
  archive      = {J_JPDC},
  author       = {Alexandru Iulian Orhean and Anna Giannakou and Lavanya Ramakrishnan and Kyle Chard and Boris Glavic and Ioan Raicu},
  doi          = {10.1016/j.jpdc.2024.104878},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104878},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SCIPIS: Scalable and concurrent persistent indexing and search in high-end computing systems},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Corrigendum to “MLLess: Achieving cost efficiency in
serverless machine learning training” [journal of parallel and
distributed computing 183 (2024) 104764]. <em>JPDC</em>, <em>189</em>,
104871. (<a href="https://doi.org/10.1016/j.jpdc.2024.104871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Pablo Gimeno Sarroca and Marc Sánchez-Artigas},
  doi          = {10.1016/j.jpdc.2024.104871},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104871},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Corrigendum to “MLLess: Achieving cost efficiency in serverless machine learning training” [Journal of parallel and distributed computing 183 (2024) 104764]},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Public cloud object storage auditing: Design,
implementation, and analysis. <em>JPDC</em>, <em>189</em>, 104870. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage auditing is a technique that enables a user to remotely check the integrity of the outsourced data in the cloud storage. Although researchers have proposed various protocols for cloud storage auditing, the proposed schemes are theoretical in nature, which are not fit for existing mainstream cloud storage service practices. To bridge this gap, this paper proposes a cloud storage auditing system that works for current mainstream cloud object storage services. We design the proposed system over existing proof of data possession (PDP) schemes and make them practical as well as usable in the real world. Specifically, we propose an architecture that separates the compute and storage functionalities of a storage auditing scheme. Because cloud object storage only provides read and write interfaces, we leverage a cloud virtual machine to implement the user-defined computations that are needed in a PDP scheme. We store the authentication tags of the outsourced data as an independent object to allow existing popular cloud storage applications, e.g., file online previewing. We also present a cost model to analyze the economic cost of a cloud storage auditing scheme. The cost model allows a user to balance security, efficiency, and economic cost by tuning various system parameters. We implemented, open-sourced the proposed system over a mainstream cloud object storage service. Experimental analysis shows that the proposed system is pretty efficient and promising for a production environment usage. Specifically, for a 40 GB sized data, the proposed system only incurs 1.66% additional storage cost, 3796 bytes communication cost, 2.9 seconds maximum auditing time cost, and 0.9 CNY per auditing monetary cost.},
  archive      = {J_JPDC},
  author       = {Fei Chen and Fengming Meng and Zhipeng Li and Li Li and Tao Xiang},
  doi          = {10.1016/j.jpdc.2024.104870},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104870},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Public cloud object storage auditing: Design, implementation, and analysis},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dataflow optimization with layer-wise design variables
estimation method for enflame CNN accelerators. <em>JPDC</em>,
<em>189</em>, 104869. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As convolution layers have been proved to be the most time-consuming operation in convolutional neural network (CNN) algorithms, many efficient CNN accelerators have been designed to boost the performance of convolution operations. Previous works on CNN acceleration usually use fixed design variables for diverse convolutional layers, which would lead to inefficient data movements and low utilization of computing resource. We tackle this issue by proposing a flexible dataflow optimization method with design variables estimation for different layers. The optimization method first narrows the design space by the priori constraints, and then enumerates all legal solutions to select the optimal design variables. We demonstrate the effectiveness of the proposed optimization method by implementing representative CNN models (VGG-16, ResNet-18 and MobileNet V1) on Enflame Technology&#39;s programmable CNN accelerator, General Computing Unit (GCU). The results indicate that our optimization can significantly enhance the throughput of the convolution layers in ResNet, VGG and MobileNet on GCU, with improvement of up to 1.84×. Furthermore, it achieves up to 2.08× of GCU utilization specifically for the convolution layers of ResNet on GCU.},
  archive      = {J_JPDC},
  author       = {Tian Chen and Yu-an Tan and Zheng Zhang and Nan Luo and Bin Li and Yuanzhang Li},
  doi          = {10.1016/j.jpdc.2024.104869},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104869},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dataflow optimization with layer-wise design variables estimation method for enflame CNN accelerators},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive patch grid strategy for parallel protein folding
using atomic burials with NAMD. <em>JPDC</em>, <em>189</em>, 104868. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The definition of protein structures is an important research topic in molecular biology currently, since there is a direct relationship between the function of the protein in the organism and the 3D geometric configuration it adopts. The transformations that occur in the protein structure from the 1D configuration to the 3D form are called protein folding. Ab initio protein folding methods use physical forces to model the interactions among the atoms that compose the protein. In order to accelerate those methods, parallel tools such as NAMD were proposed. In this paper, we propose two contributions for parallel protein folding simulations: (a) adaptive patch grid (APG) and (b) the addition of atomic burials (AB) to the traditional forces used in the simulation. With APG, we are able to adapt the simulation box (patch grid) to the current shape of the protein during the folding process. AB forces relate the 3D protein structure to its geometric center and are adequate for modeling globular proteins. Thus, adding AB to the forces used in parallel protein folding potentially increases the quality of the result for this class of proteins. APG and AB were implemented in NAMD and tested in supercomputer environments. Our results show that, with APG, we are able to reduce the execution time of the folding simulation of protein 4LNZ (5,714 atoms, 15 million time steps) from 12 hours and 36 minutes to 11 hours and 8 minutes, using 16 nodes (256 CPU cores). We also show that our APG+AB strategy was successfully used in a realistic protein folding simulation (1.7 billion time steps).},
  archive      = {J_JPDC},
  author       = {Emerson A. Macedo and Alba C.M.A. Melo},
  doi          = {10.1016/j.jpdc.2024.104868},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {7},
  pages        = {104868},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive patch grid strategy for parallel protein folding using atomic burials with NAMD},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>188</em>, 104874.
(<a href="https://doi.org/10.1016/S0743-7315(24)00038-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(24)00038-8},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104874},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical sort-based parallel algorithm for dynamic
interest matching. <em>JPDC</em>, <em>188</em>, 104867. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publish–subscribe communication is a fundamental service used for message-passing between decoupled applications in distributed simulation. When abundant unnecessary data transfer is introduced, interest-matching services are needed to filter irrelevant message traffic. Frequent demands during simulation execution makes interest matching a bottleneck with increased simulation scale. Contemporary algorithms built for serial processing inadequately leverage multicore processor-based parallel resources. Parallel algorithmic improvements are insufficient for large-scale simulations. Therefore, we propose a hierarchical sort-based parallel algorithm for dynamic interest matching that embeds all update and subscription regions into two full binary trees, thereby transferring the region-matching task to one of node-matching. It utilizes the association between adjacent nodes and the hierarchical relation between parent‒child nodes to eliminate redundant operations, and achieves incremental parallel matching that only compares changed regions. We analyze the time and space complexity of this process. The new algorithm performs better and is more scalable than state-of-the-art algorithms.},
  archive      = {J_JPDC},
  author       = {Wenjie Tang and Yiping Yao and Lizhen Ou and Kai Chen},
  doi          = {10.1016/j.jpdc.2024.104867},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104867},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hierarchical sort-based parallel algorithm for dynamic interest matching},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HoneyTwin: Securing smart cities with machine
learning-enabled SDN edge and cloud-based honeypots. <em>JPDC</em>,
<em>188</em>, 104866. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the promise of higher throughput, and better response times, 6G networks provide a significant enabler for smart cities to evolve. The rapidly-growing reliance on connected devices within the smart city context encourages malicious actors to target these devices to achieve various malicious goals. In this paper, we present a novel defense technique that creates a cloud-based virtualized honeypot/twin that is designed to receive malicious traffic through edge-based machine learning-enabled detection system. The proposed system performs early identification of malicious traffic in a software defined network-enabled edge routing point to divert that traffic away from the 6G-enabled smart city endpoints. Testing of the proposed system showed an accuracy exceeding 99.8%, with an F 1 F1 score of 0.9984.},
  archive      = {J_JPDC},
  author       = {Mohammed M. Alani},
  doi          = {10.1016/j.jpdc.2024.104866},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104866},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HoneyTwin: Securing smart cities with machine learning-enabled SDN edge and cloud-based honeypots},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting i/o bandwidth-sharing strategies for HPC
applications. <em>JPDC</em>, <em>188</em>, 104863. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work revisits I/O bandwidth-sharing strategies for HPC applications. When several applications post concurrent I/O operations, well-known approaches include serializing these operations ( ) or fair-sharing the bandwidth across them ( FairShare ). Another recent approach, I/O-Sets, assigns priorities to the applications, which are classified into different sets based upon the average length of their iterations. We introduce several new bandwidth-sharing strategies, some of them simple greedy algorithms, and some of them more complicated to implement, and we compare them with existing ones. Our new strategies do not rely on any a-priori knowledge of the behavior of the applications, such as the length of work phases, the volume of I/O operations, or some expected periodicity. We introduce a rigorous framework, namely steady-state windows , which enables to derive bounds on the competitive ratio of all bandwidth-sharing strategies for three different objectives: minimum yield, platform utilization, and global efficiency. To the best of our knowledge, this work is the first to provide a quantitative assessment of the online competitiveness of any bandwidth-sharing strategy. This theory-oriented assessment is complemented by a comprehensive set of simulations, based upon both synthetic and realistic traces. The main conclusion is that two of our simple and low-complexity greedy strategies significantly outperform , FairShare and I/O-Sets, and we recommend that the I/O community would implement them for further assessment.},
  archive      = {J_JPDC},
  author       = {Anne Benoit and Thomas Herault and Lucas Perotin and Yves Robert and Frédéric Vivien},
  doi          = {10.1016/j.jpdc.2024.104863},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104863},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Revisiting I/O bandwidth-sharing strategies for HPC applications},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring multiprocessor approaches to time series analysis.
<em>JPDC</em>, <em>188</em>, 104855. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis is a key technique for extracting and predicting events in domains as diverse as epidemiology, genomics, neuroscience, environmental sciences, economics, etc. Matrix Profile , a state-of-the-art algorithm to perform time series analysis, finds out the most similar and dissimilar subsequences in a time series in deterministic time and it is exact. Matrix Profile has low arithmetic intensity and it operates on large amounts of time series data, which can be an issue in terms of memory requirements. On the other hand, Hardware Transactional Memory (HTM) is an alternative optimistic synchronization method that executes transactions speculatively in parallel while keeping track of memory accesses to detect and resolve conflicts. This work evaluates one of the best implementations of Matrix Profile exploring multiple multiprocessor variants and proposing new implementations that consider a variety of synchronization methods (HTM, locks, barriers), as well as algorithm organizations. We analyze these variants using real datasets, both short and large, in terms of speedup and memory requirements, the latter being a major issue when dealing with very large time series. The experimental evaluation shows that our proposals can achieve up to 100× speedup over the sequential algorithm for 128 threads, and up to 3× over the baseline, while keeping memory requirements low and even independent of the number of threads.},
  archive      = {J_JPDC},
  author       = {Ricardo Quislant and Eladio Gutierrez and Oscar Plata},
  doi          = {10.1016/j.jpdc.2024.104855},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104855},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exploring multiprocessor approaches to time series analysis},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast recovery for large disk enclosures based on RAID2.0:
Algorithms and evaluation. <em>JPDC</em>, <em>188</em>, 104854. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The RAID2.0 architecture, which uses dozens or even hundreds of disks, is widely adopted for large-capacity data storage. However, limited resources like memory and CPU cause RAID2.0 to execute batch recovery for disk failures. The traditional random data placement and recovery schemes result in highly skewed I/O access within a batch, which slows down the recovery speed. To address this issue, we propose DR-RAID, an efficient reconstruction scheme that balances local rebuilding workloads across all surviving disks within a batch. We dynamically select a batch of tasks with almost balanced read loads and make intra-batch adjustments for tasks with multiple solutions of reading source chunks. Furthermore, we use a bipartite graph model to achieve a uniform distribution of write loads. DR-RAID can be applied with homogeneous or heterogeneous disk rebuilding bandwidth. Experimental results demonstrate that in offline rebuilding, DR-RAID enhances the rebuilding throughput by up to 61.90% compared to the random data placement scheme. With varied rebuilding bandwidth, the improvement can reach up to 65.00%.},
  archive      = {J_JPDC},
  author       = {Qiliang Li and Min Lyu and Liangliang Xu and Yinlong Xu},
  doi          = {10.1016/j.jpdc.2024.104854},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104854},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fast recovery for large disk enclosures based on RAID2.0: Algorithms and evaluation},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the effectiveness of bat optimization in an
adaptive and energy-efficient network-on-chip routing framework.
<em>JPDC</em>, <em>188</em>, 104853. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive routing is effective in maintaining higher processor performance and avoids packets over minimal or non-minimal alternate routes without congestion for a multiprocessor system on chip . However, many systems cannot deal with the fact that sending packets over an alternative path rather than the shorter, fixed-priority route can result in packets arriving at the destination node out of order. This can occur if packets belonging to the same communication flow are adaptively routed through a different path. In real-world network systems , there are strategies and algorithms to efficiently handle out-of-order packets without requiring infinite memory. Techniques like buffering, sliding windows, and sequence number management are used to reorder packets while considering the practical constraints of available memory and processing power. The specific method used depends on the network protocol and the requirements of the application. In the proposed technique, a novel technique aimed at improving the performance of multiprocessor systems on chip by implementing adaptive routing based on the Bat algorithm . The framework employs 5 stage pipeline router, that completely gained and forward a packet at the perfect direction in an adaptive mode. Bat algorithm is used to enhance the performance, which can optimize route to transmit packets at the destination. A test was carried out on various NoC sizes (6 X 6 and 8 X 8) under multimedia benchmarks, compared with other related algorithms and implemented on Kintex-7 FPGA board. The outcomes of the simulation illustrate that the proposed algorithm reduces delay and improves the throughput over the other traditional adaptive algorithms.},
  archive      = {J_JPDC},
  author       = {B. Naresh Kumar Reddy and Aruru Sai Kumar},
  doi          = {10.1016/j.jpdc.2024.104853},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104853},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evaluating the effectiveness of bat optimization in an adaptive and energy-efficient network-on-chip routing framework},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative dispersion by silent robots. <em>JPDC</em>,
<em>188</em>, 104852. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the dispersion problem, a set of k co-located mobile robots must relocate themselves in distinct nodes of an unknown network. The network is modeled as an anonymous graph G = ( V , E ) G=(V,E) , where the graph&#39;s nodes are not labeled. The edges incident to a node v with degree d are labeled with port numbers in the range { 0 , 1 , … , d − 1 } {0,1,…,d−1} at v . The robots have unique IDs in the range [ 0 , L ] [0,L] , where L ≥ k L≥k , and are initially placed at a source node s . The task of the dispersion was traditionally achieved based on the assumption of two types of communication abilities: (a) when some robots are at the same node, they can communicate by exchanging messages between them, and (b) any two robots in the network can exchange messages between them. This paper investigates whether this communication ability among co-located robots is absolutely necessary to achieve dispersion. We establish that even in the absence of the ability of communication, the task of the dispersion by a set of mobile robots can be achieved in a much weaker model, where a robot at a node v has access to following very restricted information at the beginning of any round: (1) am I alone at v ? (2) did the number of robots at v increase or decrease compared to the previous round? We propose a deterministic distributed algorithm that achieves the dispersion on any given graph G = ( V , E ) G=(V,E) in time O ( k log ⁡ L + k 2 log ⁡ Δ ) O(klog⁡L+k2log⁡Δ) , where Δ is the maximum degree of a node in G . Further, each robot uses O ( log ⁡ L + log ⁡ Δ ) O(log⁡L+log⁡Δ) additional memory, i.e., memory other than the memory required to store its id. We also prove that the task of the dispersion cannot be achieved by a set of mobile robots with o ( log ⁡ L + log ⁡ Δ ) o(log⁡L+log⁡Δ) additional memory.},
  archive      = {J_JPDC},
  author       = {Barun Gorain and Partha Sarathi Mandal and Kaushik Mondal and Supantha Pandit},
  doi          = {10.1016/j.jpdc.2024.104852},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104852},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Collaborative dispersion by silent robots},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DQS: A QoS-driven routing optimization approach in SDN using
deep reinforcement learning. <em>JPDC</em>, <em>188</em>, 104851. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, the exponential growth of applications has intensified traffic demands, posing challenges in ensuring optimal user experiences within modern networks. Traditional congestion avoidance and control mechanisms embedded in conventional routing struggle to promptly adapt to new-generation networks. Current routing approaches risk-averse outcomes such as (1) scalability constraints, (2) high convergence times, and (3) congestion due to inadequate real-time traffic prioritization. To address these issues, this paper introduces a QoS-Driven Routing Optimization in Software-Defined Networking (SDN) using Deep Reinforcement Learning (DRL) to optimize routing and enhance QoS efficiency. Employing DRL, the proposed DQS optimizes routing decisions by intelligently distributing traffic, guided by a multi-objective function-driven DRL agent that considers both link and queue metrics. Despite the complexity of the network, DQS sustains scalability while significantly reducing convergence times. Through a Docker-based Openflow prototype, results highlight a substantial 20-30% reduction in end-to-end delay compared to baseline methods.},
  archive      = {J_JPDC},
  author       = {Lizeth Patricia Aguirre Sanchez and Yao Shen and Minyi Guo},
  doi          = {10.1016/j.jpdc.2024.104851},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104851},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DQS: A QoS-driven routing optimization approach in SDN using deep reinforcement learning},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An edge architecture for enabling autonomous aerial
navigation with embedded collision avoidance through remote nonlinear
model predictive control. <em>JPDC</em>, <em>188</em>, 104849. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present an edge-based architecture for enhancing the autonomous capabilities of resource-constrained aerial robots by enabling a remote nonlinear model predictive control scheme, which can be computationally heavy to run on the aerial robots&#39; onboard processors. The nonlinear model predictive control is used to control the trajectory of an unmanned aerial vehicle while detecting, and preventing potential collisions. The proposed edge architecture enables trajectory recalculation for resource-constrained unmanned aerial vehicles in relatively real-time, which will allow them to have fully autonomous behaviors . The architecture is implemented with a remote Kubernetes cluster on the edge side, and it is evaluated on an unmanned aerial vehicle as our controllable robot, while the robotic operating system is used for managing the source codes , and overall communication. With the utilization of edge computing and the architecture presented in this work, we can overcome computational limitations, that resource-constrained robots have, and provide or improve features that are essential for autonomous missions. At the same time, we can minimize the relative travel time delays for time-critical missions over the edge, in comparison to the cloud. We investigate the validity of this hypothesis by evaluating the system&#39;s behavior through a series of experiments by utilizing either the unmanned aerial vehicle or the edge resources for the collision avoidance mission.},
  archive      = {J_JPDC},
  author       = {Achilleas Santi Seisa and Björn Lindqvist and Sumeet Gajanan Satpute and George Nikolakopoulos},
  doi          = {10.1016/j.jpdc.2024.104849},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {6},
  pages        = {104849},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An edge architecture for enabling autonomous aerial navigation with embedded collision avoidance through remote nonlinear model predictive control},
  volume       = {188},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient offloading for DNN-based applications in
edge-cloud computing: A hybrid chaotic evolutionary approach.
<em>JPDC</em>, <em>187</em>, 104850. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Deep Neural Networks (DNNs) lays solid foundations for Internet of Things systems. However, mobile devices with limited processing capacity and short battery life confront the difficulties of executing complex DNNs. To satisfy different Quality of Service requirements, a feasible solution is offloading DNN layers to edge nodes and the cloud. The energy-efficient offloading problem for DNN-based applications with the deadline and budget constraints in the edge-cloud environment is still an open and challenging issue. To this end, this paper proposes a Hybrid Chaotic Evolutionary Algorithm (HCEA) incorporating diversification and intensification strategies and a DVFS-enabled version of it (HCEA-DVFS). The Archimedes Optimization Algorithm-based diversification strategy exploits global and local guiding information to improve population diversity during the updating process and employs Metropolis acceptance rule of Simulated Annealing to avoid premature convergence. The Genetic Algorithm-based chaotic intensification strategy is designed to enhance the local search capability of HCEA. Moreover, the Dynamic Voltage Frequency Scaling-enabled adjustment strategies can be embedded into HCEA to further reduce energy consumption by resetting frequency levels and reallocating DNN layers. Experimental results over four DNN-based applications demonstrate that HCEA-DVFS can reduce more energy consumption under different deadlines, budgets, and workloads on average by 7.93, 9.68, 11.02, 11.84, and 19.38 percent in comparison with HCEA, PSO-GA, MCEA, AOA, and Greedy, respectively.},
  archive      = {J_JPDC},
  author       = {Zengpeng Li and Huiqun Yu and Guisheng Fan and Jiayin Zhang and Jin Xu},
  doi          = {10.1016/j.jpdc.2024.104850},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104850},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Energy-efficient offloading for DNN-based applications in edge-cloud computing: A hybrid chaotic evolutionary approach},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An active queue management for wireless sensor networks with
priority scheduling strategy. <em>JPDC</em>, <em>187</em>, 104848. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Wireless Sensor Networks (WSNs), the packet congestion will lead to high delay and high packet loss rate, which severely affects the timely transmission of real-time packets. As a congestion control method, Random Early Detection (RED) is able to stabilize the queue length at a low level. However, it does not classify the data of WSNs to achieve a targeted queue management. Since real-time packets are more urgent and important than non-real-time packets, differential packets scheduling and queue management are necessary. To deal with these problems, we propose an Active Queue Management (AQM) method called Classified Enhanced Random Early Detection (CERED). In CERED, the preemption priority is conferred on real-time packets, and the queue management with enhanced initial drop probability is implemented for non-real-time packets. Next, we develop a preemptive priority M/M/1/ C vacation queueing model with queue management to evaluate the proposed method, and the finite-state matrix geometry method is used to solve the stationary distribution of the queueing model. Then we formulate a non-linear integer programming problem for the minimum delay of real-time packets, which subjects to constraints on the steady state and system cost. Finally, a numerical example is given to show the effectiveness of the proposed method.},
  archive      = {J_JPDC},
  author       = {Changzhen Zhang and Jun Yang and Ning Wang},
  doi          = {10.1016/j.jpdc.2024.104848},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104848},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An active queue management for wireless sensor networks with priority scheduling strategy},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective grey-wolf optimization based approach for
scheduling on cloud platforms. <em>JPDC</em>, <em>187</em>, 104847. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cloud computing environment processes user workloads or tasks by exploiting its high performance computational, storage, of reducing and network resources. The virtual machines in the cloud environment are allocated to tasks with the aim of reducing overall execution time . The use of high performance resources incurs monetary costs, as well as high power consumption . The heuristic based approaches implemented for scheduling tasks are unable to cope with the complexity of optimizing multiple parameters. In this paper, we propose a multi-objective grey-wolf optimization based algorithm for scheduling tasks on cloud platforms. The proposed algorithm targets to minimize schedule length (overall execution time), energy consumption, and monetary cost required for executing tasks. For optimization, the algorithm incorporates steps that are performed iteratively for mimicking the behavior of grey wolves attacking their prey. It uses discrete values for positioning wolves for encircling and attacking the prey. The assignment of tasks to virtual machines is performed using the solution found after multi-objective optimization that incorporates weighted sorting for arranging solutions. Our experimentation performed using the CloudSim framework shows that the proposed algorithm outperforms other algorithms with performance improvement ranging from 3.98% to 16.07%, while considering the schedule length, monetary cost, and energy consumption.},
  archive      = {J_JPDC},
  author       = {Minhaj Ahmad Khan and Raihan ur Rasool},
  doi          = {10.1016/j.jpdc.2024.104847},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104847},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A multi-objective grey-wolf optimization based approach for scheduling on cloud platforms},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable IoT analytics at scale. <em>JPDC</em>,
<em>187</em>, 104840. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Societies and legislations are moving towards automated decision-making based on measured data in safety-critical environments. Over the next years, density and frequency of measurements will increase to generate more insights and get a more solid basis for decisions, including through redundant low-cost sensor deployments . The resulting data characteristics lead to large-scale system design in which small input data errors may lead to severe cascading problems including ultimately wrong decisions. To ensure internal data consistency to mitigate this risk in such IoT environments, fast-paced data fusion and consensus among redundant measurements need to be achieved. In this context, we introduce history-aware sensor fusion powered by accurate voting with clustering as a promising approach to achieve fast and informed consensus, which can converge to the output up to 4X faster than the state of the art history-based voting. Leveraging three case studies , we investigate different voting schemes and show how this approach can improve data accuracy by up to 30% and performance by up to 12% compared to state-of-the-art sensor fusion approaches. We furthermore contribute a specification format for easily deploying our methods in practice and use it to develop a pilot implementation.},
  archive      = {J_JPDC},
  author       = {Panagiotis Gkikopoulos and Peter Kropf and Valerio Schiavoni and Josef Spillner},
  doi          = {10.1016/j.jpdc.2024.104840},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104840},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reliable IoT analytics at scale},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Antipaxos: Taking interactive consistency to the next level.
<em>JPDC</em>, <em>187</em>, 104839. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical Paxos-like consensus protocols limit system scalability due to a single leader and the inability to process conflicting proposals in parallel. We introduce a novel agreement protocol, called Antipaxos, that instead reaches agreement on a collection of proposals using an efficient leaderless fast path when the environment is synchronous and failure-free, and falls back on a more elaborate slow path to handle other cases. We first specify the main safety property of Antipaxos by formalizing a new agreement problem called k - Interactive Consistency ( k - IC ). Then, we present a solution to this problem in the Byzantine failure model. We prove safety and liveness, and also present an experimental performance evaluation in the Amazon cloud. Our experiments show that Antipaxos achieves several-fold higher failure-free peak throughput than Mir-BFT. The inherent efficiency of our approach stems from the low message complexity of the fast path: agreement on n batches of conflict-prone proposals is achieved using only Θ ( n 2 ) Θ(n2) messages in one consensus cycle, or Θ ( n ) Θ(n) amortized messages per batch.},
  archive      = {J_JPDC},
  author       = {Chunyu Mao and Wojciech Golab and Bernard Wong},
  doi          = {10.1016/j.jpdc.2024.104839},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104839},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Antipaxos: Taking interactive consistency to the next level},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HyLAC: Hybrid linear assignment solver in CUDA.
<em>JPDC</em>, <em>187</em>, 104838. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Linear Assignment Problem (LAP) is a fundamental combinatorial optimization problem with a wide range of applications. Over the years, significant progress has been made in developing efficient algorithms to solve the LAP, particularly in the realm of high-performance computing, leading to remarkable reductions in computation time . In recent years, hardware improvements in General Purpose Graphics Processing Units (GPGPUs) have shown promise in meeting the ever-increasing compute bandwidth requirements . This has attracted researchers to develop GPU-accelerated algorithms to solve the LAP. Recent work in the GPU domain has uncovered parallelism available in the problem structure to achieve significant performance improvements . However, each solution presented so far targets either sparse or dense instances of the problem and has some scope for improvement. The Hungarian algorithm is one of the most famous approaches to solving the LAP in polynomial time . Hungarian algorithm has classical O ( N 4 ) O(N4) ( Munkres&#39; ) and tree based O ( N 3 ) O(N3) ( Lawler&#39;s ) implementations. It is well established that the Munkres&#39; implementation is faster for sparse LAP instances while the Lawler&#39;s implementation is faster for dense instances. In this work, we blend the GPU implementations of Munkres&#39; and Lawler&#39;s to develop a Hybrid GPU-accelerated solver for LAP that switches between the two implementations based on available sparsity . Also, we improve the existing GPU implementations to reduce memory contention , minimize CPU-GPU synchronizations, and coalesced memory access. The resulting solver (HyLAC) works faster than existing CPU/GPU LAP solvers for sparse as well as dense problem instances. HyLAC achieves a speedup of up to 6.14× over existing state-of-the-art GPU implementation when run on the same hardware. We also develop an implementation to solve a list of small LAPs (tiled LAP), which is particularly useful in the optimization domain. This tiled LAP solver performs 22.59× faster than the existing implementation.},
  archive      = {J_JPDC},
  author       = {Samiran Kawtikwar and Rakesh Nagi},
  doi          = {10.1016/j.jpdc.2024.104838},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104838},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HyLAC: Hybrid linear assignment solver in CUDA},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proactive auto-scaling technique for web applications in
container-based edge computing using federated learning model.
<em>JPDC</em>, <em>187</em>, 104837. (<a
href="https://doi.org/10.1016/j.jpdc.2024.104837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has emerged as an attractive alternative to traditional cloud computing by utilizing processing, network, and storage resources close to end devices, such as Internet of Things (IoT) sensors. Edge computing is still in its infancy, and resource provisioning and service scheduling remain research concerns. Kubernetes is a container orchestration tool in distributed environments. Proactive auto-scaling techniques in Kubernetes improve utilization by allocating resources based on future workload prediction. However, prediction models run on central cloud nodes, necessitating data transfer between edge and cloud nodes, which increases latency and response time. We present FedAvg-BiGRU, a proactive auto-scaling method in edge computing based on FedAvg and multi-step prediction by a Bidirectional Gated Recurrent Unit (BiGRU). FedAvg is a technique for training machine learning models in a Federated Learning (FL) model. FL reduces network traffic by exchanging only model updates rather than raw data, relieving learning models of the need to store data on a centralized cloud server. In addition, a technique for determining the number of Kubernetes pods based on the Cool Down Time (CDT) concept has been developed, preventing contradictory scaling actions. To our knowledge, our work is the first to employ FL for proactive auto-scaling in cloud and edge computing. The results demonstrate that the FedAvg-BiGRU method has a slightly higher prediction error than the load centralized processing mode, although this difference is not statistically significant. At the same time, it reduces the amount of data transmission between the edge nodes and the cloud server.},
  archive      = {J_JPDC},
  author       = {Javad Dogani and Farshad Khunjush},
  doi          = {10.1016/j.jpdc.2024.104837},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104837},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Proactive auto-scaling technique for web applications in container-based edge computing using federated learning model},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient sequential consistency implementation with
dynamic race detection for GPUs. <em>JPDC</em>, <em>187</em>, 104836.
(<a href="https://doi.org/10.1016/j.jpdc.2023.104836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As GPUs are being used for general purpose computations, applications with different memory access requirements have emerged. In spite of the growing demand, only few GPU coherence protocols and memory models have been explored in research, and even fewer models have been implemented in products. However, in the CPU domain a diverse range of memory models for parallel programming have been proposed, which explore the interplay between performance and programmability . Sequential consistency (SC) is one of the strict memory models. It provides the most programmer intuitive execution of memory operation but it imposes strict ordering restrictions on memory operations that cause performance overhead. Hence, implementing and supporting SC is one of the most challenging tasks in any computing platform, and GPUs are no exception. As such in this paper, we propose a GPU architecture that implements SC memory model with minimal performance and power overhead. We achieve this goal by designing a mechanism to detect races between different streaming multiprocessors (SMs) dynamically at runtime. The race is detected using a signature-based mechanism to keep track of sets of unseen updates for each SM which significantly reduces the hardware implementation cost, with a small increase in invalidation traffic. Our experiments show that dynamic race detection can be used to implement sequential consistency with 5% performance overhead.},
  archive      = {J_JPDC},
  author       = {Abdulaziz Tabbakh and Murali Annavaram},
  doi          = {10.1016/j.jpdc.2023.104836},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104836},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient sequential consistency implementation with dynamic race detection for GPUs},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speedup and efficiency of computational parallelization: A
unifying approach and asymptotic analysis. <em>JPDC</em>, <em>187</em>,
104835. (<a href="https://doi.org/10.1016/j.jpdc.2023.104835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high performance computing environments, we observe an ongoing increase in the available number of cores. For example, the current TOP500 list reveals that nine clusters have more than 1 million cores. This development calls for re-emphasizing performance (scalability) analysis and speedup laws as suggested in the literature (e.g., Amdahl&#39;s law and Gustafson&#39;s law), with a focus on asymptotic performance. Understanding speedup and efficiency issues of algorithmic parallelism is useful for several purposes, including the optimization of system operations, temporal predictions on the execution of a program, the analysis of asymptotic properties, and the determination of speedup bounds. However, the literature is fragmented and shows a large diversity and heterogeneity of speedup models and laws. These phenomena make it challenging to obtain an overview of the models and their relationships, to identify the determinants of performance in a given algorithmic and computational context, and, finally, to determine the applicability of performance models and laws to a particular parallel computing setting. In this work, I provide a generic speedup (and thus also efficiency) model for homogeneous computing environments. My approach generalizes many prominent models suggested in the literature and allows showing that they can be considered special cases of a unifying approach. The genericity of the unifying speedup model is achieved through parameterization. Considering combinations of parameter ranges, I identify six different asymptotic speedup cases and eight different asymptotic efficiency cases. Jointly applying these speedup and efficiency cases, I derive eleven scalability cases, from which I build a scalability typology. Researchers can draw upon my suggested typology to classify their speedup model and to determine the asymptotic behavior when the number of parallel processing units increases. Also, the description of two computational experiments demonstrates the practical application of the model and the typology. In addition, my results may be used and extended in future research to address various extensions of my setting.},
  archive      = {J_JPDC},
  author       = {Guido Schryen},
  doi          = {10.1016/j.jpdc.2023.104835},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104835},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Speedup and efficiency of computational parallelization: A unifying approach and asymptotic analysis},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified smell detection algorithm for optimal paths
engineering in hybrid SDN. <em>JPDC</em>, <em>187</em>, 104834. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal path engineering in hybrid software defined networking environments involves leveraging traditional networking and software defined capabilities to determine the most efficient paths for network traffic. In hybrid software defined networks , some parts of the network may be controlled by controllers, while others rely on traditional routing protocols. Leveraging the OpenFlow protocol , widely used in such deployments, to program flow entries in software defined switches is one approach to achieving optimal path engineering in hybrid software defined networks . By specifying specific rules and policies in the switches, we can control traffic forwarding behaviour and steer traffic along desired paths based on application requirements. In this work, the smell detection algorithm , a new variant of bio-inspired method, is adopted to explore its capability in routing. The new paradigm of smell detection agent, its architecture, protocols used, and the concept of hybrid defined network is also explained. A comprehensive experimental examination of within the controller, an algorithm based on smell detecting agents computes the most efficient route from where it began to the intended destination. Simulation results readily demonstrate the smell detection agent based technique&#39;s effective functioning in getting the optimum direction. In this work, bio-inspired algorithms outperformed conventional algorithms in terms of performance.},
  archive      = {J_JPDC},
  author       = {S.S. V̄inod Chandra and S. Anand Hareendran},
  doi          = {10.1016/j.jpdc.2023.104834},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104834},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Modified smell detection algorithm for optimal paths engineering in hybrid SDN},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-centric workloads with MPI_sort. <em>JPDC</em>,
<em>187</em>, 104833. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sorting is a fundamental task in computing and plays a central role in information technology. The advent of rack-scale and warehouse-size data processing shaped the architecture of data analysis platforms towards supercomputing. In turn, established techniques on supercomputers have become relevant to a wider range of application domains. This work is concerned with multi-way mergesort with exact splitting on distributed memory architectures. At its core, our approach leverages a novel and parallel algorithm for multi-way selection problems. Remarkably concise, the algorithm relies on MPI_Allgather and MPI_ReduceScatter_block , two collective communication schemes that find hardware support in most high-end networks. A software implementation of our approach is used to process the Terabyte-size Data Challenge 2 signal, released by the SKA radio telescopes organization. On the supercomputer considered herein, our approach outperforms the state of the art by up to 2.6X using 9,216 cores. Our implementation is released as a compact open source library compliant to the MPI programming model. By supporting the most popular elementary key types, and arbitrary fixed-size value types, the library can be straightforwardly integrated into third-party MPI-based software.},
  archive      = {J_JPDC},
  author       = {P. Zulian and S. Ben Bader and G. Fourestey and R. Krause and D. Rossinelli},
  doi          = {10.1016/j.jpdc.2023.104833},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104833},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Data-centric workloads with MPI_Sort},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Starlight: A kernel optimizer for GPU processing.
<em>JPDC</em>, <em>187</em>, 104832. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, GPUs have found widespread adoption in many scientific domains, offering notable performance and energy efficiency advantages compared to CPUs. However, optimizing GPU high-performance kernels poses challenges given the complexities of GPU architectures and programming models. Moreover, current GPU development tools provide few high-level suggestions and overlook the underlying hardware. Here we present Starlight, an open-source, highly flexible tool for enhancing GPU kernel analysis and optimization. Starlight autonomously describes Roofline Models , examines performance metrics, and correlates these insights with GPU architectural bottlenecks. Additionally, Starlight predicts potential performance enhancements before altering the source code . We demonstrate its efficacy by applying it to literature genomics and physics applications, attaining speedups from 1.1× to 2.5× over state-of-the-art baselines. Furthermore, Starlight supports the development of new GPU kernels, which we exemplify through an image processing application, showing speedups of 12.7× and 140× when compared against state-of-the-art FPGA- and GPU-based solutions.},
  archive      = {J_JPDC},
  author       = {Alberto Zeni and Emanuele Del Sozzo and Eleonora D&#39;Arnese and Davide Conficconi and Marco D. Santambrogio},
  doi          = {10.1016/j.jpdc.2023.104832},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104832},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Starlight: A kernel optimizer for GPU processing},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating performance portability of five shared-memory
programming models using a high-order unstructured CFD solver.
<em>JPDC</em>, <em>187</em>, 104831. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents implementing and optimizing a high-order unstructured computational fluid dynamics (CFD) solver using five shared-memory programming models: CUDA, OpenACC , OpenMP, Kokkos, and OP2. The study aims to evaluate the performance of these models on different hardware architectures, including NVIDIA GPUs , x86-based Intel/AMD, and Arm-based systems. The goal is to determine whether these models can provide developers with performance-portable solvers running efficiently on various architectures. The paper forms a more holistic view of a high-order solver across multiple platforms by visualizing performance portability (PP) and measuring productivity. It gives guidelines for translating existing codebases and their data structures to these models.},
  archive      = {J_JPDC},
  author       = {Zhe Dai and Liang Deng and YongGang Che and Ming Li and Jian Zhang and Yueqing Wang},
  doi          = {10.1016/j.jpdc.2023.104831},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {5},
  pages        = {104831},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evaluating performance portability of five shared-memory programming models using a high-order unstructured CFD solver},
  volume       = {187},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>186</em>, 104843.
(<a href="https://doi.org/10.1016/S0743-7315(24)00007-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(24)00007-8},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104843},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient SSSP algorithm on time-evolving graphs with
prediction of computation results. <em>JPDC</em>, <em>186</em>, 104830.
(<a href="https://doi.org/10.1016/j.jpdc.2023.104830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications need to execute Single-Source Shortest Paths (SSSP) algorithm on each snapshot of a time-evolving graph, leading to long waiting times experienced by the users of such applications. However, these applications are often time-sensitive, the delayed computation results can lead to the loss of best decision-making opportunities. To address this problem, in this paper we propose an efficient SSSP algorithm for time-evolving graphs, called V-Grouper. The main idea of V-Grouper is to avoid the redundant computations of the same vertex in different snapshots. Our experimental results over real-world time-evolving graphs show that, due to the high similarity of consecutive snapshots, the computation results of one vertex in neighboring snapshots are equal with a high probability. At the beginning of computation, V-Grouper first divides all the versions of a given vertex in different snapshots into vertex groups, where the computation result of each version is predicted based on the aforementioned insight of neighboring snapshots having equal results. The versions of the vertex in each group have the same predicted computation result. During the computation process for each vertex group, only one version needs to participate in computation, avoiding a large number of redundant computations. Experimental results show that V-Grouper is up to 64.31× faster than the state-of-the-art SSSP algorithm.},
  archive      = {J_JPDC},
  author       = {Yongli Cheng and Chuanjie Huang and Hong Jiang and Xianghao Xu and Fang Wang},
  doi          = {10.1016/j.jpdc.2023.104830},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104830},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient SSSP algorithm on time-evolving graphs with prediction of computation results},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified hybrid memory system for scalable deep learning
and big data applications. <em>JPDC</em>, <em>186</em>, 104820. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging non-volatile memory (NVM) technologies are of dynamic random access memory (DRAM)-like, high capacity, and low cost, at the expense of slower bandwidth and higher read/write latency compared to DRAM. Typically, NVM finds its primary application in serving as an extension of conventional DRAM to create hybrid memory systems tailored to non-uniform memory access (NUMA) architectures. This strategic integration offers the potential for high performance, enhanced capacity efficiency, and a favorable balance of cost considerations. Traditional NUMA memory management policies distribute data uniformly across both DRAM and NVM, overlooking the inherent performance gap between these heterogeneous memory systems. This challenge becomes particularly pronounced when provisioning resources for deep learning and big data applications in hybrid memory systems. To tackle the performance issues in the hybrid memory systems, we propose and develop a unified memory system, UniRedl, which automatically optimizes data migration between DRAM and NVM based on data access patterns and computation graphs of applications. To improve application performance, we provide a new memory allocation strategy named HiLowAlloc . We further design two data migration strategies in UniRedl, Idle Migration and Dynamic Migration, for management of hybrid memory systems. Specifically, Idle Migration aims to manage data placed in DRAM, while Dynamic Migration manages data saved in NVM. The experimental results demonstrate that on average UniRedl improves application performance by 33.2%, 20.6%, 19.0%, and 17.5% compared to the traditional NUMA, NUMA with anb, BMPM, and OIM, respectively. It also achieves 52.0%, 34.3%, 30.6%, 22.1% on average improvement in data locality against the state-of-the-art solutions.},
  archive      = {J_JPDC},
  author       = {Wei Rang and Huanghuang Liang and Ye Wang and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1016/j.jpdc.2023.104820},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104820},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A unified hybrid memory system for scalable deep learning and big data applications},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU-optimized approaches to molecular docking-based virtual
screening in drug discovery: A comparative analysis. <em>JPDC</em>,
<em>186</em>, 104819. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding a novel drug is a very long and complex procedure. Using computer simulations , it is possible to accelerate the preliminary phases by performing a virtual screening that filters a large set of drug candidates to a manageable number. This paper presents the implementations and comparative analysis of two GPU-optimized implementations of a virtual screening algorithm targeting novel GPU architectures. This work focuses on the analysis of parallel computation patterns and their mapping onto the target architecture. The first method adopts a traditional approach that spreads the computation for a single molecule across the entire GPU. The second uses a novel batched approach that exploits the parallel architecture of the GPU to evaluate more molecules in parallel. Experimental results showed a different behavior depending on the size of the database to be screened, either reaching a performance plateau sooner or having a more extended initial transient period to achieve a higher throughput (up to 5x), which is more suitable for extreme-scale virtual screening campaigns.},
  archive      = {J_JPDC},
  author       = {Emanuele Vitali and Federico Ficarelli and Mauro Bisson and Davide Gadioli and Gianmarco Accordi and Massimiliano Fatica and Andrea R. Beccari and Gianluca Palermo},
  doi          = {10.1016/j.jpdc.2023.104819},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104819},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GPU-optimized approaches to molecular docking-based virtual screening in drug discovery: A comparative analysis},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterization of matroidal connectivity of regular
networks. <em>JPDC</em>, <em>186</em>, 104818. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High performance computing system, which takes an interconnection network as its infrastructure topology, has been utilized in scientific computing , big data analysis as well as artificial intelligence . With the rapid growth of infrastructure topology (interconnection network) in high performance computing system, the probability of network element malfunction increases dramatically. Generally, the reliability of interconnection network is measured by the traditional (edge) connectivity, which is limited by the minimum degree of the network. In order to overcome the defect, the matroidal connectivity has been newly proposed to extend the edge connectivity. For the regular network G G , let B k ( G ) = { B 1 , B 2 , … , B k } Bk(G)={B1,B2,…,Bk} be a fixed edge partition ordered sequence of G G and χ k = ( x 1 , x 2 , … , x k ) χk=(x1,x2,…,xk) be a sequence of k integers. For any D ⊆ E ( G ) D⊆E(G) , if | D ∩ B i | ≤ x i |D∩Bi|≤xi ( i ∈ [ k ] ) (i∈[k]) and G − D G−D is disconnected, then D is named matroidal cut set of G G . The matroidal connectivity of G G , denoted by λ ( G , B k ( G ) ) λ(G,Bk(G)) , is the minimum size of matroidal cut set of G G . In this work, we characterize the matroidal connectivity of a class of n -dimensional regular networks G n Gn . To be more specific, we show that λ ( G n , B n ( G n ) ) = ( k − l + 1 ) f ( l + j ) λ(Gn,Bn(Gn))=(k−l+1)f(l+j) , where f ( n ) f(n) is a function about n , and l , k , n l,k,n are positive integers ( l ≥ 1 l≥1 ), j = n − k j=n−k . As empirical analysis, we directly determine the matroidal connectivity of some well-known interconnection networks, such as hypercube Q n Qn , star graph S T n STn and alternating group graph A G n AGn .},
  archive      = {J_JPDC},
  author       = {Hong Zhang and Shuming Zhou},
  doi          = {10.1016/j.jpdc.2023.104818},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104818},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Characterization of matroidal connectivity of regular networks},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hotspot resolution in cloud computing: A γ-robust knapsack
approach for virtual machine migration. <em>JPDC</em>, <em>186</em>,
104817. (<a href="https://doi.org/10.1016/j.jpdc.2023.104817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud providers offer virtual machines (VMs) located on physical machines (PMs) to meet the increasing demand for computational services. When the instantaneous utilized capacities of VMs exceed a PM&#39;s threshold, hotspots form and degrade VM performance. To resolve hotspots, some VMs must be live migrated to other PMs, but selecting which VMs is challenging as their utilized capacities change continuously. We propose a Predicted Mixed Integer Linear Programming (MILP) Robust Solver (PMRS) that applies Γ-robustness theory to ensure PMs are hotspot-safe with a desired probability. PMRS uses a “prediction + optimization” framework that first predicts VMs&#39; future behaviors and then formulates the problem as a Γ-robust knapsack problem (Γ-RKP) solvable with a novel MILP model . Experiments with real-trace and synthetic data demonstrate PMRS&#39;s effectiveness. Moreover, we apply PMRS in a real production environment in Huawei Cloud, and observe significant benefits in resolving existing hotspots and 94%+ potential future hotspots with minimal migration cost.},
  archive      = {J_JPDC},
  author       = {Jiaxi Wu and Wenquan Yang and Xinming Han and Yunzhe Qiu and Andrei Gudkov and Jie Song},
  doi          = {10.1016/j.jpdc.2023.104817},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104817},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hotspot resolution in cloud computing: A Γ-robust knapsack approach for virtual machine migration},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Read/write fence-free work-stealing with multiplicity.
<em>JPDC</em>, <em>186</em>, 104816. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been shown that any nonblocking algorithm for work-stealing in the standard asynchronous shared memory model of computation must use expensive Read-After-Write synchronization patterns or atomic Read-Modify-Write instructions. Algorithms for relaxations of work-stealing have been proposed, which only partially avoid this impossibility result. In restricted models of computation, work-stealing algorithms have been designed that avoid Read-After-Write patterns and atomic Read-Modify-Write instructions. This paper considers work-stealing with multiplicity , a relaxation in which every task is taken by at least one process, and any process can take any task at most once . We study two variants of the relaxation, for each variant designing a Read / Write wait-free algorithm in the standard asynchronous shared memory model . The two algorithms are devoid of Read-After-Write synchronization patterns in all their operations, and additionally, the algorithm for the second variant has constant step complexity and is fence-free . Namely, no specific ordering among the algorithm&#39;s instructions is required beyond what is implied by data dependence . To our knowledge, these are the first algorithms for relaxations of work-stealing possessing all these properties. The algorithms are also solutions to relaxed versions of single-enqueuer multi-dequeuer queues. The theoretical results are complemented with an experimental evaluation showing that the benefits of relaxed work-stealing vary depending on the application, concretely on the complexity of the work associated with each task.},
  archive      = {J_JPDC},
  author       = {Armando Castañeda and Miguel Piña},
  doi          = {10.1016/j.jpdc.2023.104816},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104816},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Read/write fence-free work-stealing with multiplicity},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of federated deep learning on vehicle-based speed
control in mixed traffic flows. <em>JPDC</em>, <em>186</em>, 104812. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the application of Federated Learning (FL) in an environment of mixed traffic flow with Connected and Automated Vehicles (CAVs). The focus of this study is set on the CAV cruise control speed adjustment which is enforced by the Intelligent Speed Adaptation (ISA) system. It is modelled as an Actor-Critic-based learning architecture with the goal of computing the vehicle speed based on the data of the nearby vehicles. Some CAVs achieve poor travel time performance in comparison to others. Thus, the proposed FL framework selects periodically best-performing CAVs which are local clients in the context of FL. The updated global model is computed based on the weighted averaging of the best-performing models of clients. The use case environment is modelled as a simulation of a synthetic motorway with an induced capacity drop in the form of a work-zone under moderate and high traffic demand. It is shown that the FL framework can stabilise the individual client learning convergence and reduce their travel time compared to the local learning approach . The overall motorway throughput is also increased by the proposed FL framework. This indicates that the global model of FL can utilise the optimal ratio between generalisation and bias towards the most effective control policy of individual speed control. Results show that those effects are more prominent under moderate traffic demand.},
  archive      = {J_JPDC},
  author       = {Martin Gregurić and Filip Vrbanić and Edouard Ivanjko},
  doi          = {10.1016/j.jpdc.2023.104812},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104812},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Impact of federated deep learning on vehicle-based speed control in mixed traffic flows},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stab-FD: A cooperative and adaptive failure detector for
wide area networks. <em>JPDC</em>, <em>186</em>, 104803. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure detectors (FDs) are a fundamental abstraction that plays a central role in the design of distributed systems. FDs are distributed oracles that provide processes with unreliable information about process failures, often in the form of a list of trusted or suspected process identities. In this article, we propose a timer-based FD which assesses the quality of its input links, and exchanges its local estimations with other nodes. Nodes use this information to adjust their timers dynamically. Capturing the variations in the quality of each link reduces the number of false suspicions without degrading failure detection time. We present experiments on a dataset of real traces collected on PlanetLab, and compare our approach to well-known state-of-the-art algorithms. Our results show that our new algorithms yield a good trade-off in terms of failure detection speed and accuracy in real scenarios.},
  archive      = {J_JPDC},
  author       = {Pierre Sens and Luciana Arantes and Anubis Graciela De Moraes Rossetto and Olivier Marin},
  doi          = {10.1016/j.jpdc.2023.104803},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104803},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Stab-FD: A cooperative and adaptive failure detector for wide area networks},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of machine learning for network-on-chips.
<em>JPDC</em>, <em>186</em>, 104778. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of Machine Learning (ML) has extended to numerous disciplines, including the domain of Network-on-chips (NoCs), leading to a consequential impact. Recent works have explored ML models&#39; applicability for NoCs design, optimization, and performance evaluation. ML-based NoCs design has demonstrated superior performance to heuristic methods employed by human experts in NoCs design. This has facilitated a tight collaboration between ML and NoCs research, offering novel perspectives and optimization strategies to advance NoCs design. In this paper, we present a comprehensive survey into implementing ML techniques for NoCs. Initially, we provide an overview of ML-based research for NoCs in two aspects: (i) the adoption of ML for performance modeling and prediction and (ii) ML-based for NoCs design, including individual components (such as routing algorithm , arbitration, traffic control, etc.). Subsequently, we summarize the challenges and difficulties in designing NoCs for applying ML techniques and discuss the preliminary solutions to these issues. Finally, we prospect the perspective on future research directions for expanding the application of ML techniques to diverse scenarios of NoCs, exploring the adoption of ML techniques for NoCs design automation. We expect this paper can be helpful for design experts to optimize NoCs using ML techniques, leading to high-performance, energy-efficient, and easy-to-implement NoCs.},
  archive      = {J_JPDC},
  author       = {Xiaoyun Zhang and Dezun Dong and Cunlu Li and Shaocong Wang and Liquan Xiao},
  doi          = {10.1016/j.jpdc.2023.104778},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {4},
  pages        = {104778},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A survey of machine learning for network-on-chips},
  volume       = {186},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketch-fusion: A gradient compression method with
multi-layer fusion for communication-efficient distributed training.
<em>JPDC</em>, <em>185</em>, 104811. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gradient compression is an effective technique for improving the efficiency of distributed training. However, introducing gradient compression can reduce model accuracy and training efficiency. Furthermore, we also find that using a layer-wise gradient compression algorithm would lead to significant compression and communication overhead , which can negatively impact the scaling efficiency of the distributed training system. To address these issues, we propose a new method called S k e t c h − F u s i o n Sketch−Fusion SGD , which leverages the Count-Sketch data structure to enhance the scalability and training speed of distributed deep learning systems. Moreover, our method employs LayerFusion to optimize gradient compression algorithms&#39; scalability and convergence efficiency by formulating an optimal multi-layer fusion strategy without introducing extra hyperparameters. We evaluate our method on a cluster of 16 GPUs and demonstrate that it can improve training efficiency by up to 18.6% without compromising the model&#39;s accuracy. In addition, we find that applying our LayerFusion algorithm to other gradient compression methods improved their scalability by up to 2.87×.},
  archive      = {J_JPDC},
  author       = {Lingfei Dai and Luqi Gong and Zhulin An and Yongjun Xu and Boyu Diao},
  doi          = {10.1016/j.jpdc.2023.104811},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104811},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Sketch-fusion: A gradient compression method with multi-layer fusion for communication-efficient distributed training},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eventually lattice-linear algorithms. <em>JPDC</em>,
<em>185</em>, 104802. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-linear systems allow nodes to execute asynchronously. We introduce eventually lattice-linear algorithms, where lattices are induced only among the states in a subset of the state space. The algorithm guarantees that the system transitions to a state in one of the lattices. Then, the algorithm behaves lattice linearly while traversing to an optimal state through that lattice. We present a lattice-linear self-stabilizing algorithm for service demand based minimal dominating set (SDMDS) problem. Using this as an example, we elaborate the working of, and define, eventually lattice-linear algorithms. Then, we present eventually lattice-linear self-stabilizing algorithms for minimal vertex cover (MVC), maximal independent set (MIS), graph colouring (GC) and 2-dominating set problems (2DS). Algorithms for SDMDS, MVC and MIS converge in 1 round plus n moves (within 2 n moves), GC in n + 4 m n+4m moves, and 2DS in 1 round plus 2 n moves (within 3 n moves). These results are an improvement over the existing literature. We also present experimental results to show performance gain demonstrating the benefit of lattice-linearity.},
  archive      = {J_JPDC},
  author       = {Arya Tanmay Gupta and Sandeep S. Kulkarni},
  doi          = {10.1016/j.jpdc.2023.104802},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104802},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Eventually lattice-linear algorithms},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed runtime verification of metric temporal
properties. <em>JPDC</em>, <em>185</em>, 104801. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Systems are often composed of geographically separated components, where the clocks may not be perfectly synchronized. As such verifying the correctness of such system properties are a major challenge and are of utmost importance . In this paper, we describe a centralized runtime monitoring technique for distributed system. First, we propose a generalized runtime verification technique for verifying partially synchronous distributed computations for the metric temporal logic ( MTL ) by exploiting bounded-skew clock synchronization . Second, we introduce a progression-based formula rewriting scheme for monitoring MTL specifications which employs SMT solving techniques and report experimental results. Third, we also quantify each event according to the possible time of occurrence and calculate the probabilistic guarantee for generating the verification verdict. Lastly, we have implemented the entire procedure and report on extensive synthetic experimental results using UPPAAL, a set of cross-chain transactions implemented on Ethereum and an Industrial Control System of a water treatment plant .},
  archive      = {J_JPDC},
  author       = {Ritam Ganguly and Yingjie Xue and Aaron Jonckheere and Parker Ljung and Benjamin Schornstein and Borzoo Bonakdarpour and Maurice Herlihy},
  doi          = {10.1016/j.jpdc.2023.104801},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104801},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed runtime verification of metric temporal properties},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ML-driven risk estimation for memory failure in a data
center environment with convolutional neural networks, self-supervised
data labeling and distribution-based model drift determination.
<em>JPDC</em>, <em>185</em>, 104800. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the trend towards multi-socket server systems, the demand for random access memory (RAM) per server increased. The consequence are more DIMM sockets per server. Since every dual in-line memory module (DIMM), which comprises a series of dynamic random-access memory integrated circuits, has a probability of failure, RAM issues became a dominant failure pattern for servers. The concept introduced in this work contributes to improving the reliability of data centers by avoiding RAM failures and mitigating their impact. For this purpose, an ML-driven framework is provided to estimate the probability of memory failure for each RAM module. The ML framework is based on structural information between correctable (CE) and uncorrectable errors (UE). In a common memory scenario, a corrupted bit within a module can be restored by redundancy using an error correction code (ECC), resulting in a CE. However, if there is more than one corrupted bit within a group of bits covered by the ECC, the information cannot be restored, resulting in a UE. Consequently, the related task requesting the memory content, and the corresponding service may crash. There is evidence that UEs have a CE history and structural relation between the CEs. However, for the case of UEs without a CE history or of a false decision of the ML framework, we extend the total framework by engineering measures to mitigate the impact of a UE by avoiding kernel panic and using backups. The engineering measures use a mapping between physical and logical memory addresses.},
  archive      = {J_JPDC},
  author       = {Tim Breitenbach and Shrikanth Malavalli Divakar and Lauritz Rasbach and Patrick Jahnke},
  doi          = {10.1016/j.jpdc.2023.104800},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104800},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ML-driven risk estimation for memory failure in a data center environment with convolutional neural networks, self-supervised data labeling and distribution-based model drift determination},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting thread configuration of SpMV kernels on GPU: A
machine learning based approach. <em>JPDC</em>, <em>185</em>, 104799.
(<a href="https://doi.org/10.1016/j.jpdc.2023.104799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) optimization on GPUs has been challenging due to irregular memory accesses and unbalanced workloads. The majority of existing solutions assign a fixed number of threads to one or more rows of sparse matrices according to empirical formulas. However, this method does not give the optimal thread configuration and results in a significant performance loss . This paper proposes a new machine learning-based thread assignment strategy for SpMV on GPU, predicting the near-optimal thread configuration for matrices. Further, we partition irregular sparse matrices into blocks according to the distribution of non-zero elements and predict the optimal thread configuration for each block. A new SpMV kernel is designed to accelerate the execution of different blocks. Experimental results show that our machine learning-based approach can select the near-optimal thread configuration for most matrices. The efficiency of SpMV for irregular matrices is also improved by matrix partitioning and blockwise prediction. Finally, we dive into the trained model to find out the connection between the features of a sparse matrix and its optimal thread configuration.},
  archive      = {J_JPDC},
  author       = {Jianhua Gao and Weixing Ji and Jie Liu and Yizhuo Wang and Feng Shi},
  doi          = {10.1016/j.jpdc.2023.104799},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104799},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Revisiting thread configuration of SpMV kernels on GPU: A machine learning based approach},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel fractional explicit group modified AOR iterative
method for solving fractional poisson equation with multi-core
architecture. <em>JPDC</em>, <em>185</em>, 104798. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research studies the Multi-core Architecture for the Modified Accelerated Overrelaxation (MAOR) scheme for solving the fractional Poisson equation. The equation is discretized using the fractional explicit group (FEG) finite difference method . This research also presents the parallel implementation of the fractional order iterative methods with the chessboard (CB) ordering strategies. The experimental results of the test problems were compared with other well known relaxation schemes to test their feasibility on parallel environment. The parameters that is measured include number of iteration, error, execution time and computational cost for various number of processors.},
  archive      = {J_JPDC},
  author       = {Nik Amir Syafiq and Mohamed Othman and Norazak Senu and Fudziah Ismail and Nor Asilah Wati Abdul Hamid},
  doi          = {10.1016/j.jpdc.2023.104798},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104798},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel fractional explicit group modified AOR iterative method for solving fractional poisson equation with multi-core architecture},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring energy saving opportunities in fault tolerant HPC
systems. <em>JPDC</em>, <em>185</em>, 104797. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, improving the energy efficiency of high-performance computing (HPC) systems is one of the main drivers in scientific and technological research. As large-scale HPC systems require some fault-tolerant method, the opportunities to reduce energy consumption should be explored. In particular, rollback-recovery methods using uncoordinated checkpoints prevent all processes from re-executing when a failure occurs. In this context, it is possible to take actions to reduce the energy consumption of the nodes whose processes do not re-execute. This work is an extension of a previous one, in which we proposed a series of strategies to manage energy consumption at failure-time. In this work, we have enriched our simulator and the experimentation by including non-blocking communications (with and without system buffering) and a largest number of candidate processes to be analyzed. We have called the latter as cascade analysis , because it includes processes that gets blocked by communication indirectly with the failed process. The simulations show that the savings were negligible in the worst case, but in some scenarios, it was possible to achieve significant ones; the maximum saving achieved was 90% in a time interval of 16 minutes. As a result, we show the feasibility of improving energy efficiency in HPC systems in the presence of a failure.},
  archive      = {J_JPDC},
  author       = {Marina Morán and Javier Balladini and Dolores Rexachs and Enzo Rucci},
  doi          = {10.1016/j.jpdc.2023.104797},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104797},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Exploring energy saving opportunities in fault tolerant HPC systems},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSLShard: An efficient sharding-based trust management
framework for blockchain-empowered IoT access control. <em>JPDC</em>,
<em>185</em>, 104795. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is widely used in modern smart areas such as smart cities, but data security remains a significant challenge. Blockchain-based IoT access control addresses data security issues by preventing unauthorized devices from accessing limited IoT resources. However, most existing blockchain-based access control schemes still have scalability and security issues. A lightweight and secure framework, MSLShard, is proposed for the blockchain-based access control via sharding, which consists of two parts. First, an adaptive network sharding scheme based on network distance, node credibility, and access frequency is designed in the cloud-edge-end collaboration-based IoT architecture to reduce nodes&#39; storage pressure and improve the scheme&#39;s scalability. Second, the trust management based on multidimensional subjective logic explicitly portrays the heterogeneity among edge nodes to motivate cooperation and further improve the security of the blockchain. Experimental results show that MSLShard can achieve high scalability and security in large-scale IoT environments compared to existing methods.},
  archive      = {J_JPDC},
  author       = {Jin Tian and JunFeng Tian and RuiZhong Du},
  doi          = {10.1016/j.jpdc.2023.104795},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104795},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MSLShard: An efficient sharding-based trust management framework for blockchain-empowered IoT access control},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectively computing high strength mixed covering arrays
with constraints. <em>JPDC</em>, <em>185</em>, 104791. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covering arrays have become a key piece in Combinatorial Testing. In particular, we focus on the efficient construction of Covering Arrays with Constraints of high strength. SAT solving technology has been proven to be well suited when solving Covering Arrays with Constraints. However, the size of the SAT reformulations rapidly grows up with higher strengths. To this end, we present a new incomplete algorithm that mitigates substantially memory blow-ups and its parallel version that allows reducing run-time consumption. Thanks to these new developments we provide a tool for Combinatorial Testing in practical environments. The experimental results confirm the goodness of the approach, opening avenues for new practical applications.},
  archive      = {J_JPDC},
  author       = {Carlos Ansótegui and Eduard Torres},
  doi          = {10.1016/j.jpdc.2023.104791},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {3},
  pages        = {104791},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Effectively computing high strength mixed covering arrays with constraints},
  volume       = {185},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reviewer thank you. <em>JPDC</em>, <em>184</em>, 104808. (<a
href="https://doi.org/10.1016/S0743-7315(23)00178-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(23)00178-8},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104808},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reviewer thank you},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>184</em>, 104806.
(<a href="https://doi.org/10.1016/S0743-7315(23)00176-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(23)00176-4},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104806},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSHGN: Multi-scenario adaptive hierarchical spatial graph
convolution network for GPU utilization prediction in heterogeneous GPU
clusters. <em>JPDC</em>, <em>184</em>, 104796. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting GPU utilization is crucial for effectively managing heterogeneous GPU clusters, yet existing prediction methods are tailored to homogeneous clusters or ignore the unique characteristics of heterogeneous ones. To address this problem, we propose the Multi-Scenarios Adaptive Hierarchical Spatial Graph Convolution Network (MSHGN) model. This model leverages the hierarchical relationships among users, tasks, and machines to construct multiple scenarios&#39; undirected graphs and uses Graph Convolution Neural (GCN) to capture the spatial dependency relationships between the computational resources consumed during execution. The MSHGN model accurately predicts the future GPU utilization rates of each machine in a heterogeneous GPU cluster, achieving an RMSE of only 0.0101, an MAE of 0.0072, and an MAPE of 0.0641. We evaluate the MSHGN model on a real-world Alibaba dataset collected from a heterogeneous GPU cluster and find that it achieves superior accuracy and robustness in predicting resource utilization , outperforming other baseline models .},
  archive      = {J_JPDC},
  author       = {Sheng Wang and Shiping Chen and Fei Meng and Yumei Shi},
  doi          = {10.1016/j.jpdc.2023.104796},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104796},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MSHGN: Multi-scenario adaptive hierarchical spatial graph convolution network for GPU utilization prediction in heterogeneous GPU clusters},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vampire: A smart energy meter for synchronous monitoring in
a distributed computer system. <em>JPDC</em>, <em>184</em>, 104794. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design and implementation of a low-cost system oriented to the synchronised and real-time surveillance and monitoring of electrical parameters of different computer devices. To measure energy consumption in a computer system, it is proposed to use, instead of a general-purpose wattmeter, one designed ad-hoc and synchronously collects the energy consumption of its various nodes or devices. The implementation of the devised system is based on the confluence of several technologies or tools widely used in the Internet of Things. Thus, this article the intelligent objects are the power meters, whose connections are based on the low-cost ESP32 microcontroller. The message transmission between devices is carried out with the standard message queuing telemetry transport (MQTT) protocol, the measurements are grouped in a database on an InfluxDB server that store the sensor data as time series, and Grafana is used as a graphical user interface. The efficiency of the proposed energy monitoring system is demonstrated by the experimental results of a real application that successfully and synchronously records the voltage, current, active power and cumulative energy consumption of a distributed cluster that includes a total of 60 cores.},
  archive      = {J_JPDC},
  author       = {Antonio F. Díaz and Beatriz Prieto and Juan José Escobar and Thomas Lampert},
  doi          = {10.1016/j.jpdc.2023.104794},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104794},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Vampire: A smart energy meter for synchronous monitoring in a distributed computer system},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evaluation of GPU filters for accelerating the 2D convex
hull. <em>JPDC</em>, <em>184</em>, 104793. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Convex Hull is one of the most relevant structures in computational geometry , with many applications such as in computer graphics , robotics, and data mining. Despite the advances in the new algorithms in this area, it is often needed to improve the performance to solve more significant problems quickly or in real-time processing. This work presents an experimental evaluation of GPU filters to reduce the cost of computing the 2D convex hull. The techniques first perform a preprocessing of the input set, filtering all points within an eight-vertex polygon to obtain a reduced set of candidate points. We use parallel computation and the use of the Manhattan distance as a metric to find the vertices of the polygon and perform the point filtering. For the filtering stage we study different approaches; from custom CUDA kernels to libraries such as Thrust and Cub. Four types of point distributions are tested: a normal distribution (favorable case), uniform (favorable case), circumference (the worst case), and a case where points are shifted randomly from the circumference (intermediate case). The experimental evaluation shows that the GPU filtering algorithm can be up to 17.5× faster than a sequential CPU implementation, and the whole convex hull computation can be up to 160× faster than the fastest implementation provided by the CGAL library for a uniform distribution and 23× for a normal distribution.},
  archive      = {J_JPDC},
  author       = {Roberto Carrasco and Héctor Ferrada and Cristóbal A. Navarro and Nancy Hitschfeld},
  doi          = {10.1016/j.jpdc.2023.104793},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104793},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An evaluation of GPU filters for accelerating the 2D convex hull},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-resource scheduling of moldable workflows.
<em>JPDC</em>, <em>184</em>, 104792. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource scheduling plays a vital role in High-Performance Computing (HPC) systems. Most scheduling research in HPC has focused on only a single type of resource (e.g., computing cores or I/O resource). With the advancement in hardware architectures and the increase in data-intensive HPC applications , there is a need to simultaneously consider a diverse set of resources (e.g., computing cores, cache, memory, I/O, and network resources) in the design of runtime schedulers for improving the overall application performance. In this paper, we study multi-resource scheduling to minimize the makespan of computational workflows comprised of moldable parallel jobs. Moldable jobs allow the scheduler to flexibly select a variable set of resources before execution, thus can adapt to the available system resources (as compared to rigid jobs) while staying easy to design and implement (as compared to malleable jobs). We propose a Multi-Resource Scheduling Algorithm (MRSA), which combines a novel resource allocation strategy and an extended list scheduling scheme to schedule the jobs. We prove that, on a system with d types of schedulable resources, MRSA achieves an approximation ratio of 1.619 d + 2.545 d + 1 1.619d+2.545d+1 for any d ≥ 1 d≥1 , and a ratio of d + 3 d 2 3 + O ( d 3 ) d+3d23+O(d3) when d is large (i.e., d ≥ 22 d≥22 ). We also present improved approximation results for workflows comprised of jobs with special precedence constraints (e.g., series-parallel graphs, trees, and independent jobs). Further, we prove a lower bound of d on the approximation ratio of any list-based scheduling algorithm with local priority considerations. Finally, we conduct a comprehensive set of simulations to evaluate the performance of the algorithm using synthetic workflows of different structures and moldable jobs following different speedup models. The results show that MRSA fares better than the theoretical bound predicts, and that it consistently outperforms two baseline heuristics under a variety of parameter settings, illustrating its robust practical performance.},
  archive      = {J_JPDC},
  author       = {Lucas Perotin and Sandhya Kandaswamy and Hongyang Sun and Padma Raghavan},
  doi          = {10.1016/j.jpdc.2023.104792},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104792},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multi-resource scheduling of moldable workflows},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPU-accelerated transportation simplex algorithm.
<em>JPDC</em>, <em>184</em>, 104790. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transportation Problem (TP) is a popular linear program for optimally matching several supply centers to several demand centers at the smallest transportation cost. Recent disruptions in the physical supply chains and the growth of internet marketplaces such as ride-sharing, doorstep delivery, and expedited shipping have engendered a need for efficient algorithms to solve large-scale TPs in near real-time. The Transportation Simplex Algorithm (TSA) is a traditional method to solve TP to optimality . However, TSA is unsuitable for these new applications because of its long run time. The evolution of accelerated computing using Graphics Processing Units (GPUs) has recently attracted some interest in solving optimization problems . In this paper, we develop a GPU-accelerated TSA for large-dimensions. The underlying parallelism in the iterative steps of TSA has been uncovered and exploited. The results show that the accelerated algorithm performs up to 8 times faster on average compared to the known sequential algorithm and up to 4 times faster on average compared to the state-of-the-art commercial Linear Programming solver.},
  archive      = {J_JPDC},
  author       = {Mohit Mahajan and Rakesh Nagi},
  doi          = {10.1016/j.jpdc.2023.104790},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104790},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GPU-accelerated transportation simplex algorithm},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable atomic broadcast: A leaderless hierarchical
algorithm. <em>JPDC</em>, <em>184</em>, 104789. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atomic Broadcast is an essential broadcast primitive as it ensures the consistency of distributed replicas. However, it is notoriously non-scalable. In this work, we introduce the Leaderless Hierarchical Atomic Broadcast (LHABcast) algorithm, which has two properties to improve scalability. First, it is a fully decentralized algorithm that does not rely on a sequencer/leader, which is often a significant bottleneck. Processes running LHABcast send messages with local sequence numbers and order messages received from other processes using timestamps inspired on Lamport&#39;s logical clocks. A process that receives the required set of timestamps can make a decision about the overall sequence of message delivery. Second, the algorithm is hierarchical: processes are organized on a vCube logical overlay network , which has several logarithmic properties and allows the construction of autonomous spanning trees. vCube also works as a failure detector , assuming crash faults and an asynchronous system model. In this paper, LHABcast is described, specified, and proven to be correct. Both simulation and experimental results are presented. A comparison with an all-to-all strategy shows that the number of messages sent by LHABcast is significantly lower in both fault-free and faulty scenarios. An implementation of LHABcast in Akka.io achieved up to 3.9 times higher throughput in fault-free scenarios than an implementation of the Raft-based Apache Ratis.},
  archive      = {J_JPDC},
  author       = {Lucas V. Ruchel and Edson Tavares de Camargo and Luiz Antonio Rodrigues and Rogério C. Turchetti and Luciana Arantes and Elias Procópio Duarte Jr.},
  doi          = {10.1016/j.jpdc.2023.104789},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104789},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scalable atomic broadcast: A leaderless hierarchical algorithm},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). General-purpose data stream processing on heterogeneous
architectures with WindFlow. <em>JPDC</em>, <em>184</em>, 104782. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many emerging applications analyze data streams by running graphs of communicating tasks called operators . To develop and deploy such applications, Stream Processing Systems (SPSs) like Apache Storm and Flink have been made available to researchers and practitioners. They exhibit imperative or declarative programming interfaces to develop operators running arbitrary algorithms working on structured or unstructured data streams. In this context, the interest in leveraging hardware acceleration with GPUs has become more pronounced in high-throughput use cases. Unfortunately, GPU acceleration has been studied for relational operators working on structured streams only, while non-relational operators have often been overlooked. This paper presents WindFlow , a library supporting the seamless GPU offloading of general partitioned-stateful operators, extending the range of operators that benefit from hardware acceleration. Its design provides high throughput still exposing a high-level API to users compared with the raw utilization of GPUs in Apache Flink.},
  archive      = {J_JPDC},
  author       = {Gabriele Mencagli and Massimo Torquati and Dalvan Griebler and Alessandra Fais and Marco Danelutto},
  doi          = {10.1016/j.jpdc.2023.104782},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104782},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {General-purpose data stream processing on heterogeneous architectures with WindFlow},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scheduling independent tasks on multiple cloud-assisted edge
servers with energy constraint. <em>JPDC</em>, <em>184</em>, 104781. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study task scheduling with or without energy constraint in mobile edge computing with multiple cloud-assisted edge servers as combinatorial optimization problems within the framework of classical scheduling theory . The first problem is to schedule a list of independent tasks on a mobile device and several heterogeneous edge servers and cloud servers, such that the makespan is minimized. The second problem is to schedule a list of independent tasks and to determine the computation and communication speeds of a mobile device, such that the makespan is minimized and the energy consumption of the mobile device does not exceed certain energy budget. The paper makes the following tangible contributions. We design heuristic task scheduling algorithms for both problems by considering the heterogeneity of computation and communication speeds. We derive a lower bound for the optimal schedule and prove an asymptotic performance bound for our heuristic algorithms . We experimentally evaluate the performance of our heuristic algorithms and show that their performance is very close to that of an optimal algorithm. Our analysis employs three key techniques, namely, the method of communication unification (i.e., all tasks have the same communication to computation ratio), the concept of effective speed of an edge server or a cloud server (i.e., the perceived speed of a server by ignoring the details and differences of communication speed and computation speed, wireless communication time and wired communication time, a regular edge server and a cloud-assisted edge server, execution time and waiting time), and the construction of virtual tasks (i.e., imaginary tasks which do not exist). Such unique techniques make it possible to derive a lower bound for the optimal solution, to derive an upper bound for the heuristic solution , to prove an asymptotic performance bound, and to find the best edge server order. To the best of the author&#39;s knowledge, this is the first paper in the literature which optimizes the makespan of task scheduling with or without energy constraint in mobile edge computing with multiple cloud-assisted edge servers.},
  archive      = {J_JPDC},
  author       = {Keqin Li},
  doi          = {10.1016/j.jpdc.2023.104781},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104781},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scheduling independent tasks on multiple cloud-assisted edge servers with energy constraint},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tiny twins for detecting cyber-attacks at runtime using
concise rebeca time transition system. <em>JPDC</em>, <em>184</em>,
104780. (<a href="https://doi.org/10.1016/j.jpdc.2023.104780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a method for detecting cyber-attacks in cyber-physical systems using a monitor. The method employs an abstract model called Tiny Twin, which is built at design time and is used at runtime to detect inconsistencies. Tiny Twin is a state transition system that represents the observable behavior of the system from the monitor point of view. We model the behavior of the system in the Rebeca modeling language and use Afra model checker to generate the state space. The Tiny Twin is built automatically, by abstracting the state space while keeping the observable actions and preserving the trace equivalence. For doing that we had to solve the complexities in the state space introduced by time-shifts, nondeterministic assignments and abstraction of internal actions . We formally define the state space as Concise Rebeca Timed Transition System (CRTTS), and then map CRTTS to an LTS. The LTS is then fed to a tool to abstract away the non-observable actions.},
  archive      = {J_JPDC},
  author       = {Fereidoun Moradi and Bahman Pourvatan and Sara Abbaspour Asadollah and Marjan Sirjani},
  doi          = {10.1016/j.jpdc.2023.104780},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104780},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Tiny twins for detecting cyber-attacks at runtime using concise rebeca time transition system},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accelerating block lifecycle on blockchain via hardware
transactional memory. <em>JPDC</em>, <em>184</em>, 104779. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The processing of block lifecycles is essential to the efficiency of a blockchain , which consists of four steps: creation, execution, consensus, and validation. The permissionless blockchain systems typically had very limited transaction throughput because of the performance bottleneck of consensus protocols. With recent advances in consensus protocols, the execution and validation of transactions have become the new performance bottleneck. We propose a novel framework, called FastBlock , to speed up the execution and validation steps by introducing fine-grained concurrency. Our early design of FastBlock supported three key modules: (1) a symbolic execution-based analyzer that automatically identifies minimal atomic sections in each transaction; (2) a concurrent execution step that executes possibly conflicting transactions in parallel using hardware transactional memory; (3) a concurrent validation step that introduces a happen-before relation to deterministically re-execute transactions. The improved FastBlock presented in this article supports the nonce mechanism to schedule concurrent transactions from the same account. Moreover, we empirically study the impact of concurrency on Ethereum except for performance and shed light on potential optimizations of FastBlock . Finally, we implemented FastBlock and then evaluated the performance of FastBlock . Our result shows that the FastBlock outperforms state-of-art solutions significantly in performance: the execution step and validation step speed up to 3.0x and 2.3x on average over the original serial model, respectively, with eight concurrent threads. In addition, we evaluated the impact of the nonce mechanism, and the result shows that the performance loss caused by this mechanism is acceptable in practice.},
  archive      = {J_JPDC},
  author       = {Yue Li and Han Liu and Jianbo Gao and Jiashuo Zhang and Zhi Guan and Zhong Chen},
  doi          = {10.1016/j.jpdc.2023.104779},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {2},
  pages        = {104779},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerating block lifecycle on blockchain via hardware transactional memory},
  volume       = {184},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph based routing algorithm for torus topology and its
evaluation for the angara interconnect. <em>JPDC</em>, <em>183</em>,
104765. (<a href="https://doi.org/10.1016/j.jpdc.2023.104765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several approaches and techniques exist to resolve load balancing problem in general and torus topology networks . Graph methods are natural ways to perform balancing of routing paths . A routing balancing algorithm must operate within the constraints of the underlying network architecture that limits several parameters, such as the number of logical paths in the network. In this paper, we consider a torus topology that is one of the common topologies that are used in high performance computing systems. We introduce a routing graph that corresponds to a interconnect topology, abstracts interconnect routing rules, providing one-to-one correspondence of network routes and graph paths. We propose a deadlock-free routing algorithm based on a fast single-source shortest path algorithm in a routing graph for the deterministic routing of a torus topology interconnect with a single virtual channel. The proposed algorithm is a tradeoff between two existing routing algorithms, also the algorithm is optimized for multidimensional torus topology and can be applied to any torus topology HPC system. We present a complete description of a routing graph that abstracts the Angara interconnect with 4D torus topology. We evaluated the proposed routing algorithm and benchmarked the maximum performance improvement of 71% for the Alltoall pattern for torus topology systems up to 432 nodes on the Angara interconnect simulator. The performance improvement of more than 5% was obtained for the NPB FT and IS application kernels on a 32-node supercomputer .},
  archive      = {J_JPDC},
  author       = {Anatoly Mukosey and Alexander Semenov and Aleksandr Tretiakov},
  doi          = {10.1016/j.jpdc.2023.104765},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104765},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Graph based routing algorithm for torus topology and its evaluation for the angara interconnect},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). MLLess: Achieving cost efficiency in serverless machine
learning training. <em>JPDC</em>, <em>183</em>, 104764. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this endeavor, we propose MLLess , a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100 ms). Our results certify that MLLess can be 15X faster than serverful ML systems [27] at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization . Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers.},
  archive      = {J_JPDC},
  author       = {Pablo Gimeno Sarroca and Marc Sánchez-Artigas},
  doi          = {10.1016/j.jpdc.2023.104764},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104764},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MLLess: Achieving cost efficiency in serverless machine learning training},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A seer knows best: Auto-tuned object storage shuffling for
serverless analytics. <em>JPDC</em>, <em>183</em>, 104763. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless platforms offer high resource elasticity and pay-as-you-go billing, making them a compelling choice for data analytics . To craft a “pure” serverless solution, the common practice is to transfer intermediate data between serverless functions via serverless object storage (IBM COS; AWS S3). However, prior works have led to inconclusive results about the performance of object storage systems, since they have left large margin for optimization. To verify that object storage has been underrated, we devise a novel shuffle manager for serverless data analytics called Seer . Specifically, Seer dynamically chooses between two shuffle algorithms to maximize performance. The algorithm choice is made online based on some predictive models , and very importantly, without end users having to specify intermediate shuffle data sizes at the time of the job submission. We integrate Seer with PyWren-IBM [31] , a well-known serverless analytics framework, and evaluate it against both serverful (e.g., Spark) and serverless systems (e.g., Google BigQuery, Caerus [46] and SONIC [22] ). Our results certify that our new shuffle manager can deliver performance improvements over them.},
  archive      = {J_JPDC},
  author       = {Germán T. Eizaguirre and Marc Sánchez-Artigas},
  doi          = {10.1016/j.jpdc.2023.104763},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104763},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A seer knows best: Auto-tuned object storage shuffling for serverless analytics},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ESMA: Towards elevating system happiness in a decentralized
serverless edge computing framework. <em>JPDC</em>, <em>183</em>,
104762. (<a href="https://doi.org/10.1016/j.jpdc.2023.104762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid growth in the adoption of numerous technologies, such as smartphones and the Internet of Things (IoT), edge and serverless computing have started gaining momentum in today&#39;s computing infrastructure. It has led to the production of huge amounts of data and has also resulted in increased network traffic, which if not managed well can cause network congestion . To address this and maintain the quality of service (QoS), in this work, a novel dispatch (destination selection) algorithm called Egalitarian Stable Matching Algorithm (ESMA) for faster data processing has been developed while also considering the best use of server resources in a decentralized Serverless-Edge environment. This will allow us to effectively utilize the enormous volumes of data that are generated. The proposed algorithm has been able to achieve lower overall dissatisfaction scores for the entire system. Individually, the client&#39;s happiness as well as the server&#39;s happiness have improved over the baseline. Moreover, there has been a drop of 25.7% in the total execution time and the total network resources consumed are lower as compared to the baseline algorithm as well as random-allocation algorithm.},
  archive      = {J_JPDC},
  author       = {Somoshree Datta and Sourav Kanti Addya and Soumya K. Ghosh},
  doi          = {10.1016/j.jpdc.2023.104762},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104762},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ESMA: Towards elevating system happiness in a decentralized serverless edge computing framework},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Construction algorithms of fault-tolerant paths and disjoint
paths in k-ary n-cube networks. <em>JPDC</em>, <em>183</em>, 104761. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for information, the number of nodes in the network is increasing rapidly, followed by the problem of information loss and communication delay caused by insufficient communication. Therefore, how to effectively transmit data and minimize communication delay has been a key issue in networks. In this paper, we first propose a fault-tolerant path algorithm of the k -ary n -cube network Q n k Qnk to obtain a fault-free path between any two distinct fault-free nodes with a faulty node set F ⊂ V ( Q n k ) F⊂V(Qnk) and | F | ≤ 2 n − 1 |F|≤2n−1 . Then, we design two algorithms to construct disjoint paths between any two distinct nodes of Q n k Qnk without any faults according to whether the two nodes are adjacent or not. Extensive evaluations show that the proposed algorithms has great advantages in average running time.},
  archive      = {J_JPDC},
  author       = {Mengjie Lv and Jianxi Fan and Baolei Cheng and Jia Yu and Xiaojua Jia},
  doi          = {10.1016/j.jpdc.2023.104761},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104761},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Construction algorithms of fault-tolerant paths and disjoint paths in k-ary n-cube networks},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HeRAFC: Heuristic resource allocation and optimization in
MultiFog-cloud environment. <em>JPDC</em>, <em>183</em>, 104760. (<a
href="https://doi.org/10.1016/j.jpdc.2023.104760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By bringing computing capacity from a remote cloud environment closer to the user, fog computing is introduced. As a result, users can access the services from more nearby computing environments, resulting in better quality of service and lower latency on the network. From the service providers&#39; point of view, this addresses the network latency and congestion issues. This is achieved by deploying the services in cloud and fog computing environments. The responsibility of service providers is to manage the heterogeneous resources available in both computing environments. In recent years, resource management strategies have made it possible to efficiently allocate resources from nearby fog and clouds to users&#39; applications. Unfortunately, these existing resource management strategies fail to give the desired result when the service providers have the opportunity to allocate the resources to the users&#39; application from fog nodes that are at a multi-hop distance from the nearby fog node. The complexity of this resource management problem drastically increases in a MultiFog-Cloud environment. This problem motivates us to revisit and present a novel Heuristic Resource Allocation and Optimization algorithm in a MultiFog-Cloud (HeRAFC) environment. Taking users&#39; application priority, execution time , and communication latency into account, HeRAFC optimizes resource utilization and minimizes cloud load. The proposed algorithm is evaluated and compared with related algorithms. The simulation results show the efficiency of the proposed HeRAFC over other algorithms.},
  archive      = {J_JPDC},
  author       = {Chinmaya Kumar Dehury and Bharadwaj Veeravalli and Satish Narayana Srirama},
  doi          = {10.1016/j.jpdc.2023.104760},
  journal      = {Journal of Parallel and Distributed Computing},
  month        = {1},
  pages        = {104760},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HeRAFC: Heuristic resource allocation and optimization in MultiFog-cloud environment},
  volume       = {183},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
