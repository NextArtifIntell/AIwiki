<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---82">DKE - 82</h2>
<ul>
<li><details>
<summary>
(2024). VarClaMM: A reference meta-model to understand DNA variant
classification. <em>DKE</em>, <em>154</em>, 102370. (<a
href="https://doi.org/10.1016/j.datak.2024.102370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the significance of a DNA variant in patients’ health status – a complex process known as variant classification – is highly critical for precision medicine applications. However, there is still debate on how to combine and weigh diverse available evidence to achieve proper and consistent conclusions. Indeed, currently, there are more than 200 different variant classification guidelines available to the scientific community, aiming to establish a framework for standardizing the classification process. Yet, these guidelines are qualitative and vague by nature, hindering their practical application and potential automation. Consequently, more precise definitions are needed. In this work, we discuss our efforts to create VarClaMM, a UML meta-model that aims to provide a clear specification of the key concepts involved in variant classification, serving as a common framework for the process. Through this accurate characterization of the domain, we were able to find contradictions or inconsistencies that might have an effect on the classification results. VarClaMM’s conceptualization efforts will lay the ground for the operationalization of variant classification, enabling any potential automation to be based on precise definitions.},
  archive      = {J_DKE},
  author       = {Mireia Costa and Alberto García S. and Ana León and Anna Bernasconi and Oscar Pastor},
  doi          = {10.1016/j.datak.2024.102370},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102370},
  shortjournal = {Data Knowl. Eng.},
  title        = {VarClaMM: A reference meta-model to understand DNA variant classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NoSQL document data migration strategy in the context of
schema evolution. <em>DKE</em>, <em>154</em>, 102369. (<a
href="https://doi.org/10.1016/j.datak.2024.102369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Agile development, one approach cannot be chosen and used all the time. Constant updates and strategy changes are necessary. We want to show that combining several migration strategies is better than choosing only one. Also, we emphasize the need to consider the type of schema change. This paper introduces a novel approach designed to optimize the migration process for NoSQL databases. The approach represents a significant advancement in migration strategy planning, providing a quantitative framework to guide decision-making. By incorporating critical factors such as schema changes, database size, the necessity of data in search functionalities, and potential latency issues, the approach comprehensively evaluates the migration feasibility and identifies the optimal migration path. Unlike existing methodologies, this approach adapts to the dynamic nature of NoSQL databases, offering a scalable and flexible approach to migration planning.},
  archive      = {J_DKE},
  author       = {Solomiia Fedushko and Roman Malyi and Yuriy Syerov and Pavlo Serdyuk},
  doi          = {10.1016/j.datak.2024.102369},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102369},
  shortjournal = {Data Knowl. Eng.},
  title        = {NoSQL document data migration strategy in the context of schema evolution},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change pattern relationships in event logs. <em>DKE</em>,
<em>154</em>, 102368. (<a
href="https://doi.org/10.1016/j.datak.2024.102368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining utilises process execution data to discover and analyse business processes. Event logs represent process executions, providing information about the activities executed. In addition to generic event attributes like activity name and timestamp, events might contain domain-specific attributes, such as a blood sugar measurement in a healthcare environment. Many of these values change during a typical process quite frequently. We refer to those as dynamic event attributes. Change patterns can be derived from dynamic event attributes, describing if the attribute values change from one activity to another. So far, change patterns can only be identified in an isolated manner, neglecting the chance of finding co-occuring change patterns. This paper provides an approach to identifying relationships between change patterns by utilising correlation methods from statistics. We applied the proposed technique on two event logs derived from the MIMIC-IV real-world dataset on hospitalisations in the US and evaluated the results with a medical expert. It turns out that relationships between change patterns can be detected within the same directly or eventually follows relation and even beyond that. Further, we identify unexpected relationships that are occurring only at certain parts of the process. Thus, the process perspective reveals novel insights on how dynamic event attributes change together during process execution. The approach is implemented in Python using the PM4Py framework.},
  archive      = {J_DKE},
  author       = {Jonas Cremerius and Hendrik Patzlaff and Mathias Weske},
  doi          = {10.1016/j.datak.2024.102368},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102368},
  shortjournal = {Data Knowl. Eng.},
  title        = {Change pattern relationships in event logs},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic redesign of business processes in the digital age:
A framework. <em>DKE</em>, <em>154</em>, 102367. (<a
href="https://doi.org/10.1016/j.datak.2024.102367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Organizations constantly seek ways to improve their business processes by using digital technologies as enablers. However, simply substituting an existing technology with a new one has limited value compared to using the capabilities of digital technologies to redesign business processes. Therefore, process analysts try to understand how the capabilities of digital technologies can enable the redesign of business processes. In this paper, we conduct a systematic literature review and examine 40 case studies where digital technologies were used to redesign business processes. We identified that, within the context of business process improvement, capabilities of digitalization, communication, analytics, digital representation, and connectivity can enable business process redesign. Furthermore, we note that these capabilities enable applying nine redesign heuristics. Based on our review, we map how each capability can facilitate the implementation of specific redesign heuristics. Finally, we illustrate how such a capability-driven approach can be applied to Metaverse as an example of a digital technology. Our mapping and classification framework can aid analysts in identifying candidate redesigns that capitalize on the capabilities of digital technologies.},
  archive      = {J_DKE},
  author       = {Fredrik Milani and Kateryna Kubrak and Juuli Nava},
  doi          = {10.1016/j.datak.2024.102367},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102367},
  shortjournal = {Data Knowl. Eng.},
  title        = {Strategic redesign of business processes in the digital age: A framework},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Timed alignments with mixed moves. <em>DKE</em>,
<em>154</em>, 102366. (<a
href="https://doi.org/10.1016/j.datak.2024.102366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study conformance checking for timed models, that is, process models that consider both the sequence of events that occur, as well as the timestamps at which each event is recorded. Time-aware process mining is a growing subfield of research, and as tools that seek to discover timing-related properties in processes develop, so does the need for conformance-checking techniques that can tackle time constraints and provide insightful quality measures for time-aware process models. One of the most useful conformance artefacts is the alignment, that is, finding the minimal changes necessary to correct a new observation to conform to a process model. In this paper, we extend the notion of timed distance from a previous work where an edit on an event’s timestamp came in two types, depending on whether or not it would propagate to its successors. Here, these different types of edits have a weighted cost each, and the ratio of their costs is denoted by α α . We then solve the purely timed alignment problem in this setting for a large class of these weighted distances (corresponding to α ∈ { 1 } ∪ [ 2 , ∞ ) α∈{1}∪[2,∞) ). For these distances, we provide linear time algorithms for both distance computation and alignment on models with sequential causal processes.},
  archive      = {J_DKE},
  author       = {Neha Rino and Thomas Chatain},
  doi          = {10.1016/j.datak.2024.102366},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102366},
  shortjournal = {Data Knowl. Eng.},
  title        = {Timed alignments with mixed moves},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State-transition-aware anomaly detection under concept
drifts. <em>DKE</em>, <em>154</em>, 102365. (<a
href="https://doi.org/10.1016/j.datak.2024.102365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting temporal abnormal patterns over streaming data is challenging due to volatile data properties and the lack of real-time labels. The abnormal patterns are usually hidden in the temporal context, which cannot be detected by evaluating single points. Furthermore, the normal state evolves over time due to concept drifts. A single model does not fit all data over time. Autoencoders have recently been applied for unsupervised anomaly detection. However, they are trained on a single normal state and usually become invalid after distributional drifts in the data stream. This paper uses an Autoencoder-based approach STAD for anomaly detection under concept drifts. In particular, we propose a state-transition-aware model to map different data distributions in each period of the data stream into states, thereby addressing the model adaptation problem in an interpretable way. In addition, we analyzed statistical tests to detect the drift by examining the sensitivity and powers. Furthermore, we present considerable ways to estimate the probability density function for comparing the distributional similarity for state transitions. Our experiments evaluate the proposed method on synthetic and real-world datasets. While delivering comparable anomaly detection performance as the state-of-the-art approaches, STAD works more efficiently and provides extra interpretability. We also provide insightful analysis of optimal hyperparameters for efficient model training and adaptation.},
  archive      = {J_DKE},
  author       = {Bin Li and Shubham Gupta and Emmanuel Müller},
  doi          = {10.1016/j.datak.2024.102365},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102365},
  shortjournal = {Data Knowl. Eng.},
  title        = {State-transition-aware anomaly detection under concept drifts},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Big data classification using SpinalNet-fuzzy-ResNeXt based
on spark architecture with data mining approach. <em>DKE</em>,
<em>154</em>, 102364. (<a
href="https://doi.org/10.1016/j.datak.2024.102364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern networking topology, big data is highly essential for several domains like e-commerce, healthcare, and finance. Big data classification has offered effectual performance in several applications. Still, big data classification is highly difficult and the recognized classification approaches require a longer duration and numerous resources for executing the accessible data. For resolving such issues, the spark-based classification approach is required. In this work, the hybrid SpinalNet-Fuzzy-ResNeXt model called SFResNeXt is implemented to classify the big data. Here, the SpinalNet and ResNeXt are merged, where the layers are fused with the fuzzy concept. The initial process is the outlier detection. The Holoentrophy method is used to detect the outlier data, and it is removed. Moreover, duplicate detection is performed by fingerprinting approach to detect the repeated data. The, Association Rule Mining (ARM) method is employed for feature selection. The big data is classified by the SFResNeXt. Furthermore, the SFResNeXt-based big data classification offered the accuracy, sensitivity, and specificity of 0.905, 0.914, and 0.922 using the heart disease dataset.},
  archive      = {J_DKE},
  author       = {M. Robinson Joel and K. Rajakumari and S. Anu Priya and M. Navaneethakrishnan},
  doi          = {10.1016/j.datak.2024.102364},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102364},
  shortjournal = {Data Knowl. Eng.},
  title        = {Big data classification using SpinalNet-fuzzy-ResNeXt based on spark architecture with data mining approach},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relating behaviour of data-aware process models.
<em>DKE</em>, <em>154</em>, 102363. (<a
href="https://doi.org/10.1016/j.datak.2024.102363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Petri nets (DPNs) have gained traction as a model for data-aware processes, thanks to their ability to balance simplicity with expressiveness, and because they can be automatically discovered from event logs. While model checking techniques for DPNs have been studied, more complex analysis tasks that are highly relevant for BPM are beyond methods known in the literature. We focus here on equivalence and inclusion of process behaviour with respect to language and configuration spaces, optionally taking data into account. Such comparisons are important in the context of key process mining tasks, namely process repair and discovery, and related to conformance checking. To solve these tasks, we propose approaches for bounded DPNs based on constraint graphs , which are faithful abstractions of the reachable state space. Though the considered verification tasks are undecidable in general, we show that our method is a decision procedure DPNs that admit a finite history set . This property guarantees that constraint graphs are finite and computable, and was shown to hold for large classes of DPNs that are mined automatically, and DPNs presented in the literature. The new techniques are implemented in the tool ada , and an evaluation proving feasibility is provided.},
  archive      = {J_DKE},
  author       = {Marco Montali and Sarah Winkler},
  doi          = {10.1016/j.datak.2024.102363},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102363},
  shortjournal = {Data Knowl. Eng.},
  title        = {Relating behaviour of data-aware process models},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SRank: Guiding schema selection in NoSQL document stores.
<em>DKE</em>, <em>154</em>, 102360. (<a
href="https://doi.org/10.1016/j.datak.2024.102360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of big data has led to a greater need for applications to change their schema frequently. NoSQL databases provide flexibility in organizing data and offer multiple choices for structuring and storing similar information. While schema flexibility speeds up initial development, choosing schemas wisely is crucial, as they significantly impact performance, affecting data redundancy, navigation cost, data access cost, and maintainability. This paper emphasizes the importance of schema design in NoSQL document stores. It proposes a model to analyze and evaluate different schema alternatives and suggest the best schema out of various schema alternatives. The model is divided into four phases. The model inputs the Entity-Relationship (ER) model and workload queries. In the Transformation Phase, the schema alternatives are initially developed for each ER model, and subsequently, a schema graph is generated for each alternative. Concurrently, workload queries undergo conversion into query graphs. In the Schema Evaluation phase, the Schema Rank (SRank) is calculated for each schema alternative using query metrics derived from the query graphs and path coverage generated from the schema graphs. Finally, in the Output phase, the schema with the highest SRank is recommended as the most suitable choice for the application. The paper includes a case study of a Hotel Reservation System (HRS) to demonstrate the application of the proposed model. It comprehensively evaluates various schema alternatives based on query response time, storage efficiency, scalability, throughput, and latency. The paper validates the SRank computation for schema selection in NoSQL databases through an extensive experimental study. The alignment of SRank values with each schema&#39;s performance metrics underscores this ranking system&#39;s effectiveness. The SRank simplifies the schema selection process, assisting users in making informed decisions by reducing the time, cost, and effort of identifying the optimal schema for NoSQL document stores.},
  archive      = {J_DKE},
  author       = {Shelly Sachdeva and Neha Bansal and Hardik Bansal},
  doi          = {10.1016/j.datak.2024.102360},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102360},
  shortjournal = {Data Knowl. Eng.},
  title        = {SRank: Guiding schema selection in NoSQL document stores},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reasoning on responsibilities for optimal process alignment
computation. <em>DKE</em>, <em>154</em>, 102353. (<a
href="https://doi.org/10.1016/j.datak.2024.102353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process alignment aims at establishing a matching between a process model run and a log trace. To improve such a matching, process alignment techniques often exploit contextual conditions to enable computations that are more informed than the simple edit distance between model runs and log traces. The paper introduces a novel approach to process alignment which relies on contextual information expressed as responsibilities . The notion of responsibility is fundamental in business and organization models, but it is often overlooked. We show the computation of optimal alignments can take advantage of responsibilities. We leverage on them in two ways. First, responsibilities may sometimes justify deviations. In these cases, we consider them as correct behaviors rather than errors. Second, responsibilities can either be met or neglected in the execution of a trace. Thus, we prefer alignments where neglected responsibilities are minimized. The paper proposes a formal framework for responsibilities in a process model, including the definition of cost functions for computing optimal alignments. We also propose a branch-and-bound algorithm for optimal alignment computation and exemplify its usage by way of two event logs from real executions.},
  archive      = {J_DKE},
  author       = {Matteo Baldoni and Cristina Baroglio and Elisa Marengo and Roberto Micalizio},
  doi          = {10.1016/j.datak.2024.102353},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102353},
  shortjournal = {Data Knowl. Eng.},
  title        = {Reasoning on responsibilities for optimal process alignment computation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for understanding event abstraction problem
solving: Current states of event abstraction studies. <em>DKE</em>,
<em>154</em>, 102352. (<a
href="https://doi.org/10.1016/j.datak.2024.102352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event abstraction is a crucial step in applying process mining in real-world scenarios. However, practitioners often face challenges in selecting relevant research for their specific needs. To address this, we present a comprehensive framework for understanding event abstraction, comprising four key components: event abstraction sub-problems, consideration of process properties, data types for event abstraction, and various approaches to event abstraction. By systematically examining these components, practitioners can efficiently identify research that aligns with their requirements. Additionally, we analyze existing studies using this framework to provide practitioners with a clearer view of current research and suggest expanded applications of existing methods.},
  archive      = {J_DKE},
  author       = {Jungeun Lim and Minseok Song},
  doi          = {10.1016/j.datak.2024.102352},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102352},
  shortjournal = {Data Knowl. Eng.},
  title        = {A framework for understanding event abstraction problem solving: Current states of event abstraction studies},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unraveling the foundations and the evolution of conceptual
modeling—intellectual structure, current themes, and trajectories.
<em>DKE</em>, <em>154</em>, 102351. (<a
href="https://doi.org/10.1016/j.datak.2024.102351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of conceptual modeling has now been in existence for over five decades. To understand how this field has evolved and should continue to evolve, it is useful to examine the contributions made over time and the themes that have emerged. In this research, we apply bibliometric analysis to a corpus of over 4700 research papers spanning from 1976 to 2023. We successively apply co-citation, bibliographic coupling, and main path analysis. Co-citation and citation networks are produced that surface the intellectual structure of the field, the main themes, and the relationships among major and influential research papers over time. We identify four areas in the intellectual structure of the field: conceptual modeling and databases; grammars and guidelines for conceptual modeling; requirements engineering and information systems design methodologies; and ontology constructs for conceptual modeling. Between 2017 and 2023, we distinguish nine research themes, including domain-specific conceptual modeling and applications, ontologies and applications, genomics, and datastores and multi-model data. The main path analysis identifies several trajectories among the major and most influential papers. This leads to insights into the lineage of key, influential papers in conceptual modeling research. The primordial nature of the main paths identified encompasses two important aspects. The first revolves around refining and complementing the entity-relationship model. The second identifies the contribution of ontologies for conceptual modeling to make the models more robust. Based on the findings from this bibliometric analysis, we propose several directions for future conceptual modeling research.},
  archive      = {J_DKE},
  author       = {Jacky Akoka and Isabelle Comyn-Wattiau and Nicolas Prat and Veda C. Storey},
  doi          = {10.1016/j.datak.2024.102351},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102351},
  shortjournal = {Data Knowl. Eng.},
  title        = {Unraveling the foundations and the evolution of conceptual modeling—Intellectual structure, current themes, and trajectories},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Capturing and analysing employee behaviour: An honest day’s
work record. <em>DKE</em>, <em>154</em>, 102350. (<a
href="https://doi.org/10.1016/j.datak.2024.102350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a range of reasons, organisations collect data on the work behaviour of their employees. However, each data collection technique displays its own unique mix of intrusiveness, information richness, and risks. For the sake of understanding the differences between data collection techniques, we conducted a multiple-case study in a multinational professional services organisation, tracking six participants throughout a workday using non-participant observation, screen recording, and timesheet techniques. This led to 136 hours of data. Our findings show that relying on one data collection technique alone cannot provide a comprehensive and accurate account of activities that are screen-based, offline, or overtime. The collected data also provided an opportunity to investigate the use of process mining for analysing employee behaviour, specifically with respect to the completeness of the collected data. Our study underlines the importance of judiciously selecting data collection techniques, as well as using a sufficiently broad data set to generate reliable insights into employee behaviour.},
  archive      = {J_DKE},
  author       = {Iris Beerepoot and Tea Šinik and Hajo A. Reijers},
  doi          = {10.1016/j.datak.2024.102350},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102350},
  shortjournal = {Data Knowl. Eng.},
  title        = {Capturing and analysing employee behaviour: An honest day’s work record},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discovering outlying attributes of outliers in data streams.
<em>DKE</em>, <em>154</em>, 102349. (<a
href="https://doi.org/10.1016/j.datak.2024.102349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data streams, continuous sequences of timestamped data points, necessitate real-time monitoring due to their time-sensitive nature. In various data stream applications, such as network security and credit card transaction monitoring, real-time detection of outliers is crucial, as these outliers often signify potential threats. Equally important is the real-time explanation of outliers, enabling users to glean insights and thereby shorten their investigation time. The investigation time for outliers is closely tied to their number of attributes, making it essential to provide explanations that detail which attributes are responsible for the abnormality of a data point, referred to as outlying attributes. However, the unbounded volume of data and concept drift of data streams pose challenges for discovering the outlying attributes of outliers in real time. In response, in this paper we propose EXOS, an algorithm designed for discovering the outlying attributes of multi-dimensional outliers in data streams. EXOS leverages cross-correlations among data streams, accommodates varying data stream schemas and arrival rates, and effectively addresses challenges related to the unbounded volume of data and concept drift. The algorithm is model-agnostic for point outlier detection and provides real-time explanations based on the local context of the outlier, derived from time-based tumbling windows. The paper provides a complexity analysis of EXOS and an experimental analysis comparing EXOS with existing algorithms. The evaluation includes an assessment of performance on both real-world and synthetic datasets in terms of average precision, recall, F1-score, and explanation time. The evaluation results show that, on average, EXOS achieves a 45.6% better F1 Score and is 7.3 times lower in explanation time compared to existing outlying attribute algorithms.},
  archive      = {J_DKE},
  author       = {Egawati Panjei and Le Gruenwald},
  doi          = {10.1016/j.datak.2024.102349},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102349},
  shortjournal = {Data Knowl. Eng.},
  title        = {Discovering outlying attributes of outliers in data streams},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conceptual framework for the government big data ecosystem
(“datagov.eco”). <em>DKE</em>, <em>154</em>, 102348. (<a
href="https://doi.org/10.1016/j.datak.2024.102348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The public sector, private firms, and civil society constantly create data of high volume, velocity, and veracity from diverse sources. This kind of data is known as big data. As in other industries, public administrations consider big data as the “new oil&quot; and employ data-centric policies to transform data into knowledge, stimulate good governance, innovative digital services, transparency, and citizens&#39; engagement in public policy. More and more public organizations understand the value created by exploiting internal and external data sources, delivering new capabilities, and fostering collaboration inside and outside of public administrations. Despite the broad interest in this ecosystem, we still lack a detailed and systematic view of it. In this paper, we attempt to describe the emerging Government Big Data Ecosystem as a socio-technical network of people, organizations, processes, technology, infrastructure, standards &amp; policies, procedures, and resources. This ecosystem supports data functions such as data collection, integration, analysis, storage, sharing, use, protection, and archiving. Through these functions, value is created by promoting evidence-based policymaking, modern public services delivery, data-driven administration and open government, and boosting the data economy. Through a Design Science Research methodology, we propose a conceptual framework, which we call ‘datagov.eco’. We believe our ‘datagov.eco’ framework will provide insights and support to different stakeholders’ profiles, including administrators, consultants, data engineers, and data scientists.},
  archive      = {J_DKE},
  author       = {Syed Iftikhar Hussain Shah and Vassilios Peristeras and Ioannis Magnisalis},
  doi          = {10.1016/j.datak.2024.102348},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102348},
  shortjournal = {Data Knowl. Eng.},
  title        = {A conceptual framework for the government big data ecosystem (‘datagov.eco’)},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing a decision support system for healthcare
practices: A design science research approach. <em>DKE</em>,
<em>154</em>, 102344. (<a
href="https://doi.org/10.1016/j.datak.2024.102344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach for designing a decision support system (DSS) for the transformation of healthcare practices. Practice transformation helps practices transition from their current state to patient-centered medical home (PCMH) model of care. Our approach employs activity theory to derive the elements of practice transformation by designing and integrating two ontologies: a domain ontology and a task ontology. By incorporating both goal-oriented and task-oriented aspects of the practice transformation process and specifying how they interact, our integrated design model for the DSS provides prescriptive knowledge on assessing the current status of a practice with respect to PCMH recognition and navigating efficiently through a complex solution space. This knowledge, which is at a moderate level of abstraction and expressed in a language that practitioners understand, contributes to the literature by providing a formulation for a nascent design theory. We implement the integrated design model as a DSS prototype; results of validation tests conducted on the prototype indicate that it is superior to the existing PCMH readiness tracking tool with respect to effectiveness, usability, efficiency, and sustainability.},
  archive      = {J_DKE},
  author       = {Arun Sen and Atish P. Sinha and Cong Zhang},
  doi          = {10.1016/j.datak.2024.102344},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {11},
  pages        = {102344},
  shortjournal = {Data Knowl. Eng.},
  title        = {Developing a decision support system for healthcare practices: A design science research approach},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data engineering and modeling for artificial intelligence.
<em>DKE</em>, <em>153</em>, 102346. (<a
href="https://doi.org/10.1016/j.datak.2024.102346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Carlos Ordonez and Wojciech Macyna and Ladjel Bellatreche},
  doi          = {10.1016/j.datak.2024.102346},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102346},
  shortjournal = {Data Knowl. Eng.},
  title        = {Data engineering and modeling for artificial intelligence},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-adaptive density-based clustering algorithm for
varying densities datasets with strong disturbance factor. <em>DKE</em>,
<em>153</em>, 102345. (<a
href="https://doi.org/10.1016/j.datak.2024.102345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a fundamental task in data mining, aiming to group similar objects together based on their features or attributes. With the rapid increase in data analysis volume and the growing complexity of high-dimensional data distribution, clustering has become increasingly important in numerous applications, including image analysis, text mining, and anomaly detection. DBSCAN is a powerful tool for clustering analysis and is widely used in density-based clustering algorithms. However, DBSCAN and its variants encounter challenges when confronted with datasets exhibiting clusters of varying densities in intricate high-dimensional spaces affected by significant disturbance factors. A typical example is multi-density clustering connected by a few data points with strong internal correlations, a scenario commonly encountered in the analysis of crowd mobility. To address these challenges, we propose a Self-adaptive Density-Based Clustering Algorithm for Varying Densities Datasets with Strong Disturbance Factor (SADBSCAN). This algorithm comprises a data block splitter, a local clustering module, a global clustering module, and a data block merger to obtain adaptive clustering results. We conduct extensive experiments on both artificial and real-world datasets to evaluate the effectiveness of SADBSCAN. The experimental results indicate that SADBSCAN significantly outperforms several strong baselines across different metrics, demonstrating the high adaptability and scalability of our algorithm.},
  archive      = {J_DKE},
  author       = {Zihao Cai and Zhaodong Gu and Kejing He},
  doi          = {10.1016/j.datak.2024.102345},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102345},
  shortjournal = {Data Knowl. Eng.},
  title        = {A self-adaptive density-based clustering algorithm for varying densities datasets with strong disturbance factor},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Increasing the precision of public transit user activity
location detection from smart card data analysis via spatial–temporal
DBSCAN. <em>DKE</em>, <em>153</em>, 102343. (<a
href="https://doi.org/10.1016/j.datak.2024.102343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Card (SC) systems have been increasingly adopted by public transit (PT) agencies all over the world, which facilitates not only fare collection but also PT service analyses and evaluations. Spatial clustering is one of the most important methods to investigate this big data in terms of activity locations, travel patterns, user behaviours, etc. Besides spatio-temporal analysis of the clusters provide further precision for detection of PT traveller activity locations and durations. This study focuses on investigation and comparison of the effectiveness of two density-based clustering algorithms, DBSCAN, and ST-DBSCAN. The numeric results are obtained using SC data (public bus system) from the metropolitan city of Konya, Turkey, and clustering algorithms are applied to a sample of this smart card data, and activity clusters are detected for the users. The results of the study suggested that ST-DBSCAN constitutes more compact clusters in both time and space for transportation researchers who want to accurately detect passengers’ individual activity regions using SC data.},
  archive      = {J_DKE},
  author       = {Fehmi Can Ozer and Hediye Tuydes-Yaman and Gulcin Dalkic-Melek},
  doi          = {10.1016/j.datak.2024.102343},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102343},
  shortjournal = {Data Knowl. Eng.},
  title        = {Increasing the precision of public transit user activity location detection from smart card data analysis via spatial–temporal DBSCAN},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating quality of ontology-driven conceptual models
abstractions. <em>DKE</em>, <em>153</em>, 102342. (<a
href="https://doi.org/10.1016/j.datak.2024.102342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of an (ontology-driven) conceptual model highly correlates with the complexity of the domain and software for which it is designed. With that in mind, an algorithm for producing ontology-driven conceptual model abstractions was previously proposed. In this paper, we empirically evaluate the quality of the abstractions produced by it. First, we have implemented and tested the last version of the algorithm over a FAIR catalog of models represented in the ontology-driven conceptual modeling language OntoUML. Second, we performed three user studies to evaluate the usefulness of the resulting abstractions as perceived by modelers. This paper reports on the findings of these experiments and reflects on how they can be exploited to improve the existing algorithm.},
  archive      = {J_DKE},
  author       = {Elena Romanenko and Diego Calvanese and Giancarlo Guizzardi},
  doi          = {10.1016/j.datak.2024.102342},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102342},
  shortjournal = {Data Knowl. Eng.},
  title        = {Evaluating quality of ontology-driven conceptual models abstractions},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interactive approach to semantic enrichment with
geospatial data. <em>DKE</em>, <em>153</em>, 102341. (<a
href="https://doi.org/10.1016/j.datak.2024.102341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous availability of datasets has spurred the utilization of Artificial Intelligence methods and models to extract valuable insights, unearth hidden patterns, and predict future trends. However, the current process of data collection and linking heavily relies on expert knowledge and domain-specific understanding, which engenders substantial costs in terms of both time and financial resources. Therefore, streamlining the data acquisition, harmonization, and enrichment procedures to deliver high-fidelity datasets readily usable for analytics is paramount. This paper explores the capabilities of SemTUI , a comprehensive framework designed to support the enrichment of tabular data by leveraging semantics and user interaction. Utilizing SemTUI, an iterative and interactive approach is proposed to enhance the flexibility, usability and efficiency of geospatial data enrichment. The approach is evaluated through a pilot case study focused on urban planning, with a particular emphasis on geocoding. Using a real-world scenario involving the analysis of kindergarten accessibility within walking distance, the study demonstrates the proficiency of SemTUI in generating precise and semantically enriched location data. The incorporation of human feedback in the enrichment process successfully enhances the quality of the resulting dataset, highlighting SemTUI’s potential for broader applications in geospatial analysis and its usability for users with limited expertise in manipulating geospatial data.},
  archive      = {J_DKE},
  author       = {Flavio De Paoli and Michele Ciavotta and Roberto Avogadro and Emil Hristov and Milena Borukova and Dessislava Petrova-Antonova and Iva Krasteva},
  doi          = {10.1016/j.datak.2024.102341},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102341},
  shortjournal = {Data Knowl. Eng.},
  title        = {An interactive approach to semantic enrichment with geospatial data},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological querying of music scores. <em>DKE</em>,
<em>153</em>, 102340. (<a
href="https://doi.org/10.1016/j.datak.2024.102340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For centuries, sheet music scores have been the traditional way to preserve and disseminate Western music works. Nowadays, their content can be encoded in digital formats, making possible to store music score data in digital score libraries (DSL). To supply intelligent services (extracting and analysing relevant information from data), the new generation of DSL has to rely on digital representations of the score content as structured objects apt at being manipulated by high-level operators. In the present paper, we propose the Muster model, a graph-based data model for representing the music content of a digital score, and we discuss the querying of such data through graph pattern queries. We then present a proof-of-concept of this approach, which allows storing graph-based representations of music scores in the Neo4j database, and performing musical pattern searches through graph pattern queries with the Cypher query language. A benchmark study, using (real) datasets stemming from the Neuma Digital Score Library, complements this implementation.},
  archive      = {J_DKE},
  author       = {Philippe Rigaux and Virginie Thion},
  doi          = {10.1016/j.datak.2024.102340},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102340},
  shortjournal = {Data Knowl. Eng.},
  title        = {Topological querying of music scores},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable machine learning approach for automated
medical decision support of heart disease. <em>DKE</em>, <em>153</em>,
102339. (<a href="https://doi.org/10.1016/j.datak.2024.102339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary Heart Disease (CHD) is the dominant cause of mortality around the world. Every year, it causes about 3.9 million deaths in Europe and 1.8 million in the European Union (EU). It is responsible for 45 % and 37 % of all deaths in Europe and the European Union, respectively. Using machine learning (ML) to predict heart diseases is one of the most promising research topics, as it can improve healthcare and consequently increase the longevity of people&#39;s lives. However, although the ability to interpret the results of the predictive model is essential, most of the related studies do not propose explainable methods. To address this problem, this paper presents a classification method that not only exhibits reliable performance but is also interpretable, ensuring transparency in its decision-making process. SHapley Additive exPlanations, known as the SHAP method was chosen for model interpretability. This approach presents a comparison between different classifiers and parameter tuning techniques, providing all the details necessary to replicate the experiment and help future researchers working in the field. The proposed model achieves similar performance to those proposed in the literature, and its predictions are fully interpretable.},
  archive      = {J_DKE},
  author       = {Francisco Mesquita and Gonçalo Marques},
  doi          = {10.1016/j.datak.2024.102339},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102339},
  shortjournal = {Data Knowl. Eng.},
  title        = {An explainable machine learning approach for automated medical decision support of heart disease},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive methodology to construct standardised
datasets for science and technology parks. <em>DKE</em>, <em>153</em>,
102338. (<a href="https://doi.org/10.1016/j.datak.2024.102338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a standardised approach to create datasets for Science and Technology Parks (STPs), facilitating future analysis of STP characteristics, trends and performance. STPs are the most representative examples of innovation ecosystems. The ETL (extraction-transformation-load) structure was adapted to a global field study of STPs. A selection stage and quality check were incorporated, and the methodology was applied to Spanish STPs. This study applies diverse techniques such as expert labelling and information extraction which uses language technologies. A novel methodology for building quality and standardised STP datasets was designed and applied to a Spanish STP case study with 49 STPs. An updatable dataset and a list of the main features impacting STPs are presented. Twenty-one ( n = 21) core features were refined and selected, with fifteen of them (71.4 %) being robust enough for developing further quality analysis. The methodology presented integrates different sources with heterogeneous information that is often decentralised, disaggregated and in different formats: excel files, and unstructured information in HTML or PDF format. The existence of this updatable dataset and the defined methodology will enable powerful AI tools to be applied that focus on more sophisticated analysis, such as taxonomy, monitoring, and predictive and prescriptive analytics in the innovation ecosystems field.},
  archive      = {J_DKE},
  author       = {Olga Francés and Javi Fernández and José Abreu-Salas and Yoan Gutiérrez and Manuel Palomar},
  doi          = {10.1016/j.datak.2024.102338},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102338},
  shortjournal = {Data Knowl. Eng.},
  title        = {A comprehensive methodology to construct standardised datasets for science and technology parks},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Entity type inference based on path walking and inter-types
relationships. <em>DKE</em>, <em>153</em>, 102337. (<a
href="https://doi.org/10.1016/j.datak.2024.102337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a crucial task for knowledge graphs (KGs), knowledge graph entity type inference (KGET) has garnered increasing attention in recent years. However, recent methods overlook the long-distance information pertaining to entities and the inter-types relationships. The neglect of long-distance information results in the omission of crucial entity relationships and neighbors, consequently leading to the loss of path information associated with missing types. To address this, a path-walking strategy is utilized to identify two-hop triplet paths of the crucial entity for encoding long-distance entity information. Moreover, the absence of inter-types relationships can lead to the loss of the neighborhood information of types, such as co-occurrence information. To ensure a comprehensive understanding of inter-types relationships, we consider interactions not only with the types of single entity but also with different types of entities. Finally, in order to comprehensively represent entities for missing types, considering both the dimensions of path information and neighborhood information, we propose an entity type inference model based on path walking and inter-types relationships, denoted as “ET-PT”. This model effectively extracts comprehensive entity information, thereby obtaining the most complete semantic representation of entities. The experimental results on publicly available datasets demonstrate that the proposed method outperforms state-of-the-art approaches.},
  archive      = {J_DKE},
  author       = {Yi Gan and Zhihui Su and Gaoyong Lu and Pengju Zhang and Aixiang Cui and Jiawei Jiang and Duanbing Chen},
  doi          = {10.1016/j.datak.2024.102337},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102337},
  shortjournal = {Data Knowl. Eng.},
  title        = {Entity type inference based on path walking and inter-types relationships},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Providing healthcare shopping advice through knowledge-based
virtual agents. <em>DKE</em>, <em>153</em>, 102336. (<a
href="https://doi.org/10.1016/j.datak.2024.102336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based virtual shopping agents, that advise their users about which products to buy, are well used in technical markets such as healthcare e-commerce. To ensure the proper adoption of this technology, it is important to consider aspects of users’ psychology early in the software design process. When traditional adoption models such as UTAUT-2 work well for many technologies, they overlook important specificities of the healthcare e-commerce domain and of knowledge-based virtual agents technology. Drawing upon health information technology and virtual agent literature, we propose a complementary adoption model incorporating new predictors and moderators reflecting these domains’ specificities. The model is tested using 903 observations gathered through an online survey conducted in collaboration with a major actor in the herbal medicine market. Our model can serve as a basis for many phases of the knowledge-based agents software development. We propose actionable recommendations for practitioners and ideas for further research.},
  archive      = {J_DKE},
  author       = {Claire Deventer and Pietro Zidda},
  doi          = {10.1016/j.datak.2024.102336},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102336},
  shortjournal = {Data Knowl. Eng.},
  title        = {Providing healthcare shopping advice through knowledge-based virtual agents},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A design theory for data quality tools in data ecosystems:
Findings from three industry cases. <em>DKE</em>, <em>153</em>, 102333.
(<a href="https://doi.org/10.1016/j.datak.2024.102333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data ecosystems are a novel inter-organizational form of cooperation. They require at least one data provider and one or more data consumers. Existing research mainly addresses generativity mechanisms in this relationship, such as business models or role models for data ecosystems. However, an essential prerequisite for thriving data ecosystems is high data quality in the shared data. Without sufficient data quality, sharing data might lead to negative business consequences, given that the information drawn from them or services built on them might be incorrect or produce fraudulent results. We tackle this issue precisely since we report on a multi-case study deploying data quality tools in data ecosystem scenarios. From these cases, we derive generalized prescriptive design knowledge as a design theory to make the knowledge available for others designing data quality tools for data sharing. Subsequently, our study contributes to integrating the issue of data quality in data ecosystem research and provides practitioners with actionable guidelines inferred from three real-world cases.},
  archive      = {J_DKE},
  author       = {Marcel Altendeitering and Tobias Moritz Guggenberger and Frederik Möller},
  doi          = {10.1016/j.datak.2024.102333},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102333},
  shortjournal = {Data Knowl. Eng.},
  title        = {A design theory for data quality tools in data ecosystems: Findings from three industry cases},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CGT: A clause graph transformer structure for aspect-based
sentiment analysis. <em>DKE</em>, <em>153</em>, 102332. (<a
href="https://doi.org/10.1016/j.datak.2024.102332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of natural language processing (NLP), aspect-based sentiment analysis plays a pivotal role. Recently, there has been a growing emphasis on techniques leveraging Graph Convolutional Neural Network (GCN). However, there are several challenges associated with current approaches: (1) Due to the inherent transitivity of CGN, training inevitably entails the acquisition of irrelevant semantic information. (2) Existing methodologies heavily depend on the dependency tree, neglecting to consider the contextual structure of the sentence. (3) Another limitation of the majority of methods is their failure to account for the interactions occurring between different aspects. In this study, we propose a Clause Graph Transformer Structure (CGT) to alleviate these limitations. Specifically, CGT comprises three modules. The preprocessing module extracts aspect clauses from each sentence by bi-directionally traversing the constituent tree, reducing reliance on syntax trees and extracting semantic information from the perspective of clauses. Additionally, we assert that a word’s vector direction signifies its underlying attitude in the semantic space, a feature often overlooked in recent research. Without the necessity for additional parameters, we introduce the Clause Attention encoder (CA-encoder) to the clause module to effectively capture the directed cross-correlation coefficient between the clause and the target aspect. To enhance the representation of the target component, we propose capturing the connections between various aspects. In the inter-aspect module, we intricately design a Balanced Attention encoder (BA-encoder) that forms an aspect sequence by navigating the extracted phrase tree. To effectively capture the emotion of implicit components, we introduce a Top-K Attention Graph Convolutional Network (KA-GCN). Our proposed method has showcased state-of-the-art (SOTA) performance through experiments conducted on four widely used datasets. Furthermore, our model demonstrates a significant improvement in the robustness of datasets subjected to disturbances.},
  archive      = {J_DKE},
  author       = {Zelong Su and Bin Gao and Xiaoou Pan and Zhengjun Liu and Yu Ji and Shutian Liu},
  doi          = {10.1016/j.datak.2024.102332},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102332},
  shortjournal = {Data Knowl. Eng.},
  title        = {CGT: A clause graph transformer structure for aspect-based sentiment analysis},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explanation, semantics, and ontology. <em>DKE</em>,
<em>153</em>, 102325. (<a
href="https://doi.org/10.1016/j.datak.2024.102325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The terms ‘semantics’ and ‘ontology’ are increasingly appearing together with ‘explanation’, not only in the scientific literature, but also in everyday social interactions, in particular, within organizations. Ontologies have been shown to play a key role in supporting the semantic interoperability of data and knowledge representation structures used by information systems. With the proliferation of applications of Artificial Intelligence (AI) in different settings and the increasing need to guarantee their explainability (but also their interoperability) in critical contexts, the term ‘explanation’ has also become part of the scientific and technical jargon of modern information systems engineering. However, all of these terms are also significantly overloaded. In this paper, we address several interpretations of these notions, with an emphasis on their strong connection. Specifically, we discuss a notion of explanation termed ontological unpacking , which aims at explaining symbolic domain descriptions (e.g., conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their so-called truthmakers , i.e., the entities in one’s ontology that are responsible for the truth of a description. To illustrate this methodology, we employ an ontological theory of relations to explain a symbolic model encoded in the de facto standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of explanation processes) in supporting semantic interoperability tasks. Furthermore, we revisit a proposal for quality criteria for explanations from philosophy of science to assess our approach. Finally, we discuss the relation between ontological unpacking and other forms of explanation in philosophy and science, as well as in the subarea of Artificial Intelligence known as Explainable AI (XAI).},
  archive      = {J_DKE},
  author       = {Giancarlo Guizzardi and Nicola Guarino},
  doi          = {10.1016/j.datak.2024.102325},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102325},
  shortjournal = {Data Knowl. Eng.},
  title        = {Explanation, semantics, and ontology},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extended TF-IDF method for improving keyword extraction
in traditional corpus-based research: An example of a climate change
corpus. <em>DKE</em>, <em>153</em>, 102322. (<a
href="https://doi.org/10.1016/j.datak.2024.102322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keyword extraction involves the application of Natural Language Processing (NLP) algorithms or models developed in the realm of text mining. Keyword extraction is a common technique used to explore linguistic patterns in the corpus linguistic field, and Dunning’s Log-Likelihood Test (LLT) has long been integrated into corpus software as a statistic-based NLP model. While prior research has confirmed the widespread applicability of keyword extraction in corpus-based research, LLT has certain limitations that may impact the accuracy of keyword extraction in such research. This paper summarized the limitations of LLT, which include benchmark corpus interference, elimination of grammatical and generic words, consideration of sub-corpus relevance, flexibility in feature selection, and adaptability to different research goals. To address these limitations, this paper proposed an extended Term Frequency-Inverse Document Frequency (TF-IDF) method. To verify the applicability of the proposed method, 20 highly cited research articles on climate change from the Web of Science (WOS) database were used as the target corpus, and a comparison was conducted with the traditional method. The experimental results indicated that the proposed method could effectively overcome the limitations of the traditional method and demonstrated the feasibility and practicality of incorporating the TF-IDF algorithm into relevant corpus-based research.},
  archive      = {J_DKE},
  author       = {Liang-Ching Chen},
  doi          = {10.1016/j.datak.2024.102322},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102322},
  shortjournal = {Data Knowl. Eng.},
  title        = {An extended TF-IDF method for improving keyword extraction in traditional corpus-based research: An example of a climate change corpus},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hermes, a low-latency transactional storage for binary data
streams from remote devices. <em>DKE</em>, <em>153</em>, 102315. (<a
href="https://doi.org/10.1016/j.datak.2024.102315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many contexts where data is streamed on a large scale, such as video surveillance systems, there is a dual requirement: secure data storage and continuous access to audio and video content by third parties, such as human operators or specific business logic, even while the media files are still being collected. However, using transactions to ensure data persistence often limits system throughput and latency. This paper presents a solution that enables both high ingestion rates with transactional data persistence and near real-time, low-latency access to the stream during collection. This immediate access enables the prompt application of specialized data engineering algorithms during data acquisition. The proposed solution is particularly suitable for binary data sources such as audio and video recordings in surveillance systems, and it can be extended to various big data scenarios via well-defined general interfaces. The scalability of the approach is based on the microservice architecture. Preliminary results obtained with Apache Kafka and MongoDB replica sets show that the proposed solution provides up to 3 times higher throughput and 2.2 times lower latency compared to standard multi-document transactions.},
  archive      = {J_DKE},
  author       = {Gabriele Scaffidi Militone and Daniele Apiletti and Giovanni Malnati},
  doi          = {10.1016/j.datak.2024.102315},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {9},
  pages        = {102315},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hermes, a low-latency transactional storage for binary data streams from remote devices},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large language models: Expectations for semantics-driven
systems engineering. <em>DKE</em>, <em>152</em>, 102324. (<a
href="https://doi.org/10.1016/j.datak.2024.102324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hype of Large Language Models manifests in disruptions, expectations or concerns in scientific communities that have focused for a long time on design-oriented research. The current experiences with Large Language Models and associated products (e.g. ChatGPT) lead to diverse positions regarding the foreseeable evolution of such products from the point of view of scholars who have been working with designed abstractions for most of their careers - typically relying on deterministic design decisions to ensure systems and automation reliability. Such expectations are collected in this paper in relation to a flavor of systems engineering that relies on explicit knowledge structures, introduced here as “semantics-driven systems engineering”. The paper was motivated by the panel discussion that took place at CAiSE 2023 in Zaragoza, Spain, during the workshop on Knowledge Graphs for Semantics-driven Systems Engineering (KG4SDSE). The workshop brought together Conceptual Modeling researchers with an interest in specific applications of Knowledge Graphs and the semantic enrichment benefits they can bring to systems engineering. The panel context and consensus are summarized at the end of the paper, preceded by a proposed research agenda considering the expressed positions.},
  archive      = {J_DKE},
  author       = {Robert Buchmann and Johann Eder and Hans-Georg Fill and Ulrich Frank and Dimitris Karagiannis and Emanuele Laurenzi and John Mylopoulos and Dimitris Plexousakis and Maribel Yasmina Santos},
  doi          = {10.1016/j.datak.2024.102324},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102324},
  shortjournal = {Data Knowl. Eng.},
  title        = {Large language models: Expectations for semantics-driven systems engineering},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic requirements construction using ontologies and
boilerplates. <em>DKE</em>, <em>152</em>, 102323. (<a
href="https://doi.org/10.1016/j.datak.2024.102323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a combination of an ontology and boilerplates, which are requirements templates for the syntactic structure of individual requirements that try to alleviate the problem of ambiguity caused using natural language and make it easier for inexperienced engineers to create requirements. However, still the use of boilerplates restricts the use of natural language only syntactically and not semantically. Boilerplates consists of fixed and attributes elements. Using ontologies, restricts the vocabulary of the words used in the requirements boilerplates to entities, their properties and entity relationships that are semantically meaningful to the application domain, leading thus to fewer errors. In this work we combine the advantages of boilerplates and ontologies. Usually, the attributes of boilerplates are completed with the help of the ontology. The contribution of this paper is that the whole boilerplates are stored in the ontology, based on the fact that RDF triples have similar syntax to the boilerplate syntax, so that attributes and fixed elements are part of the ontology. This combination helps to construct semantically and syntactically correct requirements. The contribution and novelty of our method is that we exploit the natural language syntax of boilerplates mapping them to Resource Description Framework triples which have also a linguistic nature. In this paper we created and present the development of a domain-specific ontology as well as a minimal set of boilerplates for a specific application domain , namely that of engineering software for an ATM , while maintaining flexibility on the one hand and generality on the other.},
  archive      = {J_DKE},
  author       = {Christina Antoniou and Kalliopi Kravari and Nick Bassiliades},
  doi          = {10.1016/j.datak.2024.102323},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102323},
  shortjournal = {Data Knowl. Eng.},
  title        = {Semantic requirements construction using ontologies and boilerplates},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing fuzzy semantics of reviews for multi-criteria
recommendations. <em>DKE</em>, <em>152</em>, 102314. (<a
href="https://doi.org/10.1016/j.datak.2024.102314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hotel reviews play a vital role in tourism recommender system. They should be analyzed effectively to enhance the accuracy of recommendations which can be generated either from crisp ratings on a fixed scale or real sentiments of reviews. But crisp ratings cannot represent the actual feelings of reviewers. Existing tourism recommender systems mostly recommend hotels on the basis of vague and sparse ratings resulting in inaccurate recommendations or preferences for online users. This paper presents a semantic approach to analyze the online reviews being crawled from tripadvisor.in. It discovers the underlying fuzzy semantics of reviews with respect to the multiple criteria of hotels rather than using the crisp ratings. The crawled reviews are preprocessed via data cleaning such as stopword and punctuation removal, tokenization, lemmatization, pos tagging to understand the semantics efficiently. Nouns representing frequent features of hotels are extracted from pre-processed reviews which are further used to identify opinion phrases. Fuzzy weights are derived from normalized frequency of frequent nouns and combined with sentiment score of all the synonyms of adjectives in the identified opinion phrases. This results in fuzzy semantics which form an ideal representation of reviews for a multi-criteria tourism recommender system. The proposed work is implemented in python by crawling the recent reviews of Jaipur hotels from TripAdvisor and analyzing their semantics. The resultant fuzzy semantics form a manually tagged dataset of reviews tagged with sentiments of identified aspects, respectively. Experimental results show improved sentiment score while considering all the synonyms of adjectives. The results are further used to fine-tune BERT models to form encodings for a query-based recommender system. The proposed approach can help tourism and hospitality service providers to take advantage of such sentiment analysis to examine the negative comments or unpleasant experiences of tourists and making appropriate improvements. Moreover, it will help online users to get better recommendations while planning their trips.},
  archive      = {J_DKE},
  author       = {Navreen Kaur Boparai and Himanshu Aggarwal and Rinkle Rani},
  doi          = {10.1016/j.datak.2024.102314},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102314},
  shortjournal = {Data Knowl. Eng.},
  title        = {Analyzing fuzzy semantics of reviews for multi-criteria recommendations},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To prompt or not to prompt: Navigating the use of large
language models for integrating and modeling heterogeneous data.
<em>DKE</em>, <em>152</em>, 102313. (<a
href="https://doi.org/10.1016/j.datak.2024.102313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manually integrating data of diverse formats and languages is vital to many artificial intelligence applications. However, the task itself remains challenging and time-consuming. This paper highlights the potential of Large Language Models (LLMs) to streamline data extraction and resolution processes. Our approach aims to address the ongoing challenge of integrating heterogeneous data sources, encouraging advancements in the field of data engineering. Applied on the specific use case of learning disorders in higher education, our research demonstrates LLMs’ capability to effectively extract data from unstructured sources. It is then further highlighted that LLMs can enhance data integration by providing the ability to resolve entities originating from multiple data sources. Crucially, the paper underscores the necessity of preliminary data modeling decisions to ensure the success of such technological applications. By merging human expertise with LLM-driven automation, this study advocates for the further exploration of semi-autonomous data engineering pipelines.},
  archive      = {J_DKE},
  author       = {Adel Remadi and Karim El Hage and Yasmina Hobeika and Francesca Bugiotti},
  doi          = {10.1016/j.datak.2024.102313},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102313},
  shortjournal = {Data Knowl. Eng.},
  title        = {To prompt or not to prompt: Navigating the use of large language models for integrating and modeling heterogeneous data},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial. <em>DKE</em>, <em>152</em>, 102311. (<a
href="https://doi.org/10.1016/j.datak.2024.102311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Wookey Lee and Herwig Unger},
  doi          = {10.1016/j.datak.2024.102311},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102311},
  shortjournal = {Data Knowl. Eng.},
  title        = {Editorial},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Business intelligence and cognitive loads: Proposition of a
dashboard adoption model. <em>DKE</em>, <em>152</em>, 102310. (<a
href="https://doi.org/10.1016/j.datak.2024.102310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision makers in organizations strive to improve the quality of their decisions. One way to improve that process is to objectify the decisions with facts. Data-driven Decision Support Systems (data-driven DSS), and more specifically business intelligence (BI) intend to achieve this. Organizations invest massively in the development of BI data-driven DSS and expect them to be adopted and to effectively support decision makers. This raises many technical and methodological challenges, especially regarding the design of BI dashboards, which can be seen as the visible tip of the BI data-driven DSS iceberg and which play a major role in the adoption of the entire system. In this paper, the dashboard content is investigated as one possible root cause for BI data-driven DSS dashboard adoption or rejection through an early empirical research. More precisely, this work is composed of three parts. In the first part, the concept of cognitive loads is studied in the context of BI dashboards and the informational, the representational and the non-informational loads are introduced. In the second part, the effects of these loads on the adoption of BI dashboards are then studied through an experiment with 167 respondents and a Structural Equation Modeling (SEM) analysis. The result is a Dashboard Adoption Model, enriching the seminal Technology Acceptance Model with new content-oriented variables to support the design of more supportive BI data-driven DSS dashboards. Finally, in the third part, a set of indicators is proposed to help dashboards designers in the monitoring of the loads of their dashboards practically.},
  archive      = {J_DKE},
  author       = {Corentin Burnay and Mathieu Lega and Sarah Bouraga},
  doi          = {10.1016/j.datak.2024.102310},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102310},
  shortjournal = {Data Knowl. Eng.},
  title        = {Business intelligence and cognitive loads: Proposition of a dashboard adoption model},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using berlin SPARQL benchmark to evaluate virtual SPARQL
endpoints over relational databases. <em>DKE</em>, <em>152</em>, 102309.
(<a href="https://doi.org/10.1016/j.datak.2024.102309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The RDF is a popular and well-documented format for publishing structured data on the web. It enables data to be consumed without the knowledge of how the data is internally stored. There are already several native RDF storage solutions that provide a SPARQL endpoint. However, native RDF stores are not widely adopted. It is still more common to store data in a relational database. One of the useful features of native RDF storage solutions is providing a SPARQL endpoint, a web service to query RDF data with SPARQL. To provide this feature also on top of prevalent relational databases, solutions for virtual SPARQL endpoints on top of a relational database have appeared. To benchmark these solutions, a state-of-the-art tool, the Berlin SPARQL Benchmark (BSBM), is used. However, BSBM was designed primarily to benchmark native RDF stores. It can also be used to benchmark solutions for virtual SPARQL endpoints. However, since BSBM was not designed for virtual SPARQL endpoints, each implementation uses that tool differently for evaluation. As a result, the evaluation is not consistent and therefore hardly comparable. In this paper, we demonstrate how this well-defined benchmarking tool for SPARQL endpoints can be used to evaluate virtual endpoints over relational databases, perform the evaluation on the available implementations, and provide instructions on how to repeat the same evaluation in the future.},
  archive      = {J_DKE},
  author       = {Milos Chaloupka and Martin Necasky},
  doi          = {10.1016/j.datak.2024.102309},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102309},
  shortjournal = {Data Knowl. Eng.},
  title        = {Using berlin SPARQL benchmark to evaluate virtual SPARQL endpoints over relational databases},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A graph based named entity disambiguation using clique
partitioning and semantic relatedness. <em>DKE</em>, <em>152</em>,
102308. (<a href="https://doi.org/10.1016/j.datak.2024.102308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disambiguating name mentions in texts is a crucial task in Natural Language Processing, especially in entity linking. The credibility and efficiency of such systems depend largely on this task. For a given name entity mention in a text, there are many potential candidate entities that may refer to it in the knowledge base. Therefore, it is very difficult to assign the correct candidate from the whole set of candidate entities of this mention. To solve this problem, collective entity disambiguation is a prominent approach. In this paper, we present a novel algorithm called CPSR for collective entity disambiguation, which is based on a graph approach and semantic relatedness. A clique partitioning algorithm is used to find the best clique that contains a set of candidate entities. These candidate entities provide the answers to the corresponding mentions in the disambiguation process. To evaluate our algorithm, we carried out a series of experiments on seven well-known datasets, namely, AIDA/CoNLL2003-TestB, IITB, MSNBC, AQUAINT, ACE2004, Cweb, and Wiki. The Kensho Derived Wikimedia Dataset (KDWD) is used as the knowledge base for our system. From the experimental results, our CPSR algorithm outperforms both the baselines and other well-known state-of-the-art approaches.},
  archive      = {J_DKE},
  author       = {Ramla Belalta and Mouhoub Belazzoug and Farid Meziane},
  doi          = {10.1016/j.datak.2024.102308},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102308},
  shortjournal = {Data Knowl. Eng.},
  title        = {A graph based named entity disambiguation using clique partitioning and semantic relatedness},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for predicting off-block delays: A case
study at paris — charles de gaulle international airport. <em>DKE</em>,
<em>152</em>, 102303. (<a
href="https://doi.org/10.1016/j.datak.2024.102303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Punctuality is a sensitive issue in large airports and hubs for passenger experience and for controlling operational costs. This paper presents a real and challenging problem of predicting and explaining flight off-block delays. We study the case of the international airport Paris Charles de Gaulle (Paris-CDG) starting from the specificities of this problem at Paris-CDG until the proposal of modelings then solutions and the analysis of the results on real data covering an entire year of activity. The proof of concept provided in this paper allows us to believe that the proposed approach could help improve the management of delays and reduce the impact of the resulting consequences.},
  archive      = {J_DKE},
  author       = {Thibault Falque and Bertrand Mazure and Karim Tabia},
  doi          = {10.1016/j.datak.2024.102303},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {7},
  pages        = {102303},
  shortjournal = {Data Knowl. Eng.},
  title        = {Machine learning for predicting off-block delays: A case study at paris — charles de gaulle international airport},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Managerial risk data analytics applications using grey
influence analysis (GINA). <em>DKE</em>, <em>151</em>, 102312. (<a
href="https://doi.org/10.1016/j.datak.2024.102312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We observe and analyze the causal relations among risk factors in a system, considering the manufacturing supply chains. Seven major categories of risks were identified and scrutinized and the detailed analysis of causal relations using the grey influence analysis (GINA) methodology is outlined. With expert response based survey, we conduct an initial analysis of the risks using risk matrix analysis (RMA) and the risks under high priority are identified. Later, the GINA is implemented to understand the causal relations among various categories of risks, which is particularly useful in group decision-making environments. The results from RMA concludes that the capacity risks (CR) and delays (DL) are in the category of very high priority risks. GINA results also ratify the conclusions from RMA and observes that managers need to control and manage capacity risks (CR) and delays (DL) with high priorities. Additionally from the results of GINA, the causal factors disruptions (DS) and forecast risks (FR) appear to be primary importance and if unattended can lead to the initiation of several other risks in supply chains. Managers are recommended to identify disruptions at an early stage in supply chains and reduce the forecast errors to avoid bullwhips in supply chains.},
  archive      = {J_DKE},
  author       = {R. Rajesh},
  doi          = {10.1016/j.datak.2024.102312},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102312},
  shortjournal = {Data Knowl. Eng.},
  title        = {Managerial risk data analytics applications using grey influence analysis (GINA)},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating transformers and linguistic features integration
for author profiling tasks in spanish. <em>DKE</em>, <em>151</em>,
102307. (<a href="https://doi.org/10.1016/j.datak.2024.102307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Author profiling consists of extracting their demographic and psychographic information by examining their writings. This information can then be used to improve the reader experience and to detect bots or propagators of hoaxes and/or hate speech. Therefore, author profiling can be applied to build more robust and efficient Knowledge-Based Systems for tasks such as content moderation, user profiling, and information retrieval. Author profiling is typically performed automatically as a document classification task. Recently, language models based on transformers have also proven to be quite effective in this task. However, the size and heterogeneity of novel language models, makes it necessary to evaluate them in context. The contributions we make in this paper are four-fold: First, we evaluate which language models are best suited to perform author profiling in Spanish. These experiments include basic, distilled, and multilingual models. Second, we evaluate how feature integration can improve performance for this task. We evaluate two distinct strategies: knowledge integration and ensemble learning. Third, we evaluate the ability of linguistic features to improve the interpretability of the results. Fourth, we evaluate the performance of each language model in terms of memory, training, and inference times. Our results indicate that the use of lightweight models can indeed achieve similar performance to heavy models and that multilingual models are actually less effective than models trained with one language. Finally, we confirm that the best models and strategies for integrating features ultimately depend on the context of the task.},
  archive      = {J_DKE},
  author       = {José Antonio García-Díaz and Ghassan Beydoun and Rafel Valencia-García},
  doi          = {10.1016/j.datak.2024.102307},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102307},
  shortjournal = {Data Knowl. Eng.},
  title        = {Evaluating transformers and linguistic features integration for author profiling tasks in spanish},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective text classification using BERT, MTM LSTM, and DT.
<em>DKE</em>, <em>151</em>, 102306. (<a
href="https://doi.org/10.1016/j.datak.2024.102306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text classification plays a critical role in managing large volumes of electronically produced texts. As the number of such texts increases, manual analysis becomes impractical, necessitating an intelligent approach for processing information. Deep learning models have witnessed widespread application in text classification, including the use of recurrent neural networks like Many to One Long Short-Term Memory (MTO LSTM). Nonetheless, this model is limited by its reliance on only the last token for text labelling. To overcome this limitation, this study introduces a novel hybrid model that combines Bidirectional Encoder Representations from Transformers (BERT), Many To Many Long Short-Term Memory (MTM LSTM), and Decision Templates (DT) for text classification. In this new model, the text is first embedded using the BERT model and then trained using MTM LSTM to approximate the target at each token. Finally, the approximations are fused using DT. The proposed model is evaluated using the well-known IMDB movie review dataset for binary classification and Drug Review Dataset for multiclass classification. The results demonstrate superior performance in terms of accuracy, recall, precision, and F1 score compared to previous models. The hybrid model presented in this study holds significant potential for a wide range of text classification tasks and stands as a valuable contribution to the field.},
  archive      = {J_DKE},
  author       = {Saman Jamshidi and Mahin Mohammadi and Saeed Bagheri and Hamid Esmaeili Najafabadi and Alireza Rezvanian and Mehdi Gheisari and Mustafa Ghaderzadeh and Amir Shahab Shahabi and Zongda Wu},
  doi          = {10.1016/j.datak.2024.102306},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102306},
  shortjournal = {Data Knowl. Eng.},
  title        = {Effective text classification using BERT, MTM LSTM, and DT},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CQuAE: A new contextualized QUestion answering corpus on
education domain. <em>DKE</em>, <em>151</em>, 102305. (<a
href="https://doi.org/10.1016/j.datak.2024.102305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating education-related questions and answers remains an open issue while being useful for students, teachers, and teaching aids. Given textual course material, we are interested in generating non-factual questions that require an elaborate answer (relying on analysis or reasoning). Despite the availability of annotated corpora of questions and answers, the effort to develop a generator using deep learning faces two main challenges. Firstly, freely accessible and qualitative data are insufficient to train generative approaches. Secondly, for a stand-alone application, we do not have explicit support to guide the generation toward complex questions. To tackle the first issue, we propose a new corpus based on education documents. For the second point, we propose to study several retargetable language algorithms to produce answers by extracting text spans from contextual documents to help the generation of questions. We particularly study the contribution of deep neural syntactic parsing and transformer-based semantic representation, taking into account the question type (according to our specific question typology) and the contextual support text span. Additionally, recent advances in generation models have proven the efficiency of the instruction-based approach for natural language generation. Consequently, we propose a first investigation of very large language models to generate questions related to the education domain.},
  archive      = {J_DKE},
  author       = {Thomas Gerald and Louis Tamames and Sofiane Ettayeb and Ha-Quang Le and Patrick Paroubek and Anne Vilnat},
  doi          = {10.1016/j.datak.2024.102305},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102305},
  shortjournal = {Data Knowl. Eng.},
  title        = {CQuAE: A new contextualized QUestion answering corpus on education domain},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence in digital twins—a systematic
literature review. <em>DKE</em>, <em>151</em>, 102304. (<a
href="https://doi.org/10.1016/j.datak.2024.102304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence and digital twins have become more popular in recent years and have seen usage across different application domains for various scenarios. This study reviews the literature at the intersection of the two fields, where digital twins integrate an artificial intelligence component. We follow a systematic literature review approach, analyzing a total of 149 related studies. In the assessed literature, a variety of problems are approached with an artificial intelligence-integrated digital twin, demonstrating its applicability across different fields. Our findings indicate that there is a lack of in-depth modeling approaches regarding the digital twin, while many articles focus on the implementation and testing of the artificial intelligence component. The majority of publications do not demonstrate a virtual-to-physical connection between the digital twin and the real-world system. Further, only a small portion of studies base their digital twin on real-time data from a physical system, implementing a physical-to-virtual connection.},
  archive      = {J_DKE},
  author       = {Tim Kreuzer and Panagiotis Papapetrou and Jelena Zdravkovic},
  doi          = {10.1016/j.datak.2024.102304},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102304},
  shortjournal = {Data Knowl. Eng.},
  title        = {Artificial intelligence in digital twins—A systematic literature review},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging an isolation forest to anomaly detection and data
clustering. <em>DKE</em>, <em>151</em>, 102302. (<a
href="https://doi.org/10.1016/j.datak.2024.102302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding why some points in a data set are considered as anomalies cannot be done without taking into account the structure of the regular points. Whereas many machine learning methods are dedicated to the identification of anomalies on one side, or to the identification of the data inner-structure on the other side, a solution is introduced to answers these two tasks using a same data model, a variant of an isolation forest. The initial algorithm to construct an isolation forest is indeed revisited to preserve the data inner structure without affecting the efficiency of the outlier detection. Experiments conducted both on synthetic and real-world data sets show that, in addition to improving the detection of abnormal data points, the proposed variant of isolation forest allows for a reconstruction of the subspaces of high density. Therefore, the former can serve as a basis for a unified approach to detect global and local anomalies, which is a necessary condition to then provide users with informative descriptions of the data.},
  archive      = {J_DKE},
  author       = {Véronne Yepmo and Grégory Smits and Marie-Jeanne Lesot and Olivier Pivert},
  doi          = {10.1016/j.datak.2024.102302},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102302},
  shortjournal = {Data Knowl. Eng.},
  title        = {Leveraging an isolation forest to anomaly detection and data clustering},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The unresolved need for dependable guarantees on security,
sovereignty, and trust in data ecosystems. <em>DKE</em>, <em>151</em>,
102301. (<a href="https://doi.org/10.1016/j.datak.2024.102301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data ecosystems emerged as a new paradigm to facilitate the automated and massive exchange of data from heterogeneous information sources between different stakeholders. However, the corresponding benefits come with unforeseen risks as sensitive information is potentially exposed, questioning data ecosystem reliability. Consequently, data security is of utmost importance and, thus, a central requirement for successfully realizing data ecosystems. Academia has recognized this requirement, and current initiatives foster sovereign participation via a federated infrastructure where participants retain local control over what data they offer to whom. However, recent proposals place significant trust in remote infrastructure by implementing organizational security measures such as certification processes before the admission of a participant. At the same time, the data sensitivity incentivizes participants to bypass the organizational security measures to maximize their benefit. This issue significantly weakens security, sovereignty, and trust guarantees and highlights that organizational security measures are insufficient in this context. In this paper, we argue that data ecosystems must be extended with technical means to (re)establish dependable guarantees. We underpin this need with three representative use cases for data ecosystems, which cover personal, economic, and governmental data, and systematically map the lack of dependable guarantees in related work. To this end, we identify three enablers of dependable guarantees, namely trusted remote policy enforcement, verifiable data tracking, and integration of resource-constrained participants. These enablers are critical for securely implementing data ecosystems in data-sensitive contexts.},
  archive      = {J_DKE},
  author       = {Johannes Lohmöller and Jan Pennekamp and Roman Matzutt and Carolin Victoria Schneider and Eduard Vlad and Christian Trautwein and Klaus Wehrle},
  doi          = {10.1016/j.datak.2024.102301},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102301},
  shortjournal = {Data Knowl. Eng.},
  title        = {The unresolved need for dependable guarantees on security, sovereignty, and trust in data ecosystems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-aware structure matching for temporal knowledge graph
alignment. <em>DKE</em>, <em>151</em>, 102300. (<a
href="https://doi.org/10.1016/j.datak.2024.102300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entity alignment, aiming at identifying equivalent entity pairs across multiple knowledge graphs (KGs), serves as a vital step for knowledge fusion. As the majority of KGs undergo continuous evolution, existing solutions utilize graph neural networks (GNNs) to tackle entity alignment within temporal knowledge graphs (TKGs). However, this prevailing method often overlooks the consequential impact of relation embedding generation on entity embeddings through inherent structures. In this paper, we propose a novel model named Time-aware Structure Matching based on GNNs (TSM-GNN) that encompasses the learning of both topological and inherent structures. Our key innovation lies in a unique method for generating relation embeddings, which can enhance entity embeddings via inherent structure. Specifically, we utilize the translation property of knowledge graphs to obtain the entity embedding that is mapped into a time-aware vector space. Subsequently, we employ GNNs to learn global entity representation. To better capture the useful information from neighboring relations and entities, we introduce a time-aware attention mechanism that assigns different importance weights to different time-aware inherent structures. Experimental results on three real-world datasets demonstrate that TSM-GNN outperforms several state-of-the-art approaches for entity alignment between TKGs.},
  archive      = {J_DKE},
  author       = {Wei Jia and Ruizhe Ma and Li Yan and Weinan Niu and Zongmin Ma},
  doi          = {10.1016/j.datak.2024.102300},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102300},
  shortjournal = {Data Knowl. Eng.},
  title        = {Time-aware structure matching for temporal knowledge graph alignment},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Insights into commonalities of a sample: A visualization
framework to explore unusual subset-dataset relationships. <em>DKE</em>,
<em>151</em>, 102299. (<a
href="https://doi.org/10.1016/j.datak.2024.102299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain experts are driven by business needs, while data analysts develop and use various algorithms, methods, and tools, but often without domain knowledge. A major challenge for companies and organizations is to integrate data analytics in business processes and workflows. We deduce an interactive process and visualization framework to enable value creating collaboration in inter- and cross-disciplinary teams. Domain experts and data analysts are both empowered to analyze and discuss results and come to well-founded insights and implications. Inspired by a typical auditing problem, we develop and apply a visualization framework to single out unusual data in general subsets for potential further investigation. Our framework is applicable to both unusual data detected manually by domain experts or by algorithms applied by data analysts. Application examples show typical interaction, collaboration, visualization, and decision support.},
  archive      = {J_DKE},
  author       = {Nikolas Stege and Michael H. Breitner},
  doi          = {10.1016/j.datak.2024.102299},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102299},
  shortjournal = {Data Knowl. Eng.},
  title        = {Insights into commonalities of a sample: A visualization framework to explore unusual subset-dataset relationships},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A knowledge-sharing platform for space resources.
<em>DKE</em>, <em>151</em>, 102286. (<a
href="https://doi.org/10.1016/j.datak.2024.102286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing interest of academia, industry, and government institutions in space resource information highlights the difficulty of finding, accessing, integrating, and reusing this information. Although information is regularly published on the internet, it is disseminated on many different websites and in different formats, including scientific publications, patents, news, and reports. We are currently developing a knowledge management and sharing platform for space resources. This tool, which relies on the combined use of knowledge graphs and ontologies, formalises the domain knowledge contained in the above-mentioned documents and makes it more readily available to the community. In this article, we describe the concepts and techniques of knowledge extraction and management adopted during the design and implementation of the platform.},
  archive      = {J_DKE},
  author       = {Marcos Da Silveira and Louis Deladiennee and Emmanuel Scolan and Cedric Pruski},
  doi          = {10.1016/j.datak.2024.102286},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102286},
  shortjournal = {Data Knowl. Eng.},
  title        = {A knowledge-sharing platform for space resources},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph-based image classification. <em>DKE</em>,
<em>151</em>, 102285. (<a
href="https://doi.org/10.1016/j.datak.2024.102285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a deep learning method for image classification that leverages knowledge formalized as a graph created from information represented by pairs attribute/value. The proposed method investigates a loss function that adaptively combines the classical cross-entropy commonly used in deep learning with a novel penalty function. The novel loss function is derived from the representation of nodes after embedding the knowledge graph and incorporates the proximity between class and image nodes. Its formulation enables the model to focus on identifying the boundary between the most challenging classes to distinguish. Experimental results on several image databases demonstrate improved performance compared to state-of-the-art methods, including classical deep learning algorithms and recent algorithms that incorporate knowledge represented by a graph.},
  archive      = {J_DKE},
  author       = {Franck Anaël Mbiaya and Christel Vrain and Frédéric Ros and Thi-Bich-Hanh Dao and Yves Lucas},
  doi          = {10.1016/j.datak.2024.102285},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102285},
  shortjournal = {Data Knowl. Eng.},
  title        = {Knowledge graph-based image classification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the identification of relevant variants in genome
information systems: A methodological approach with a case study on
early onset alzheimer’s disease. <em>DKE</em>, <em>151</em>, 102284. (<a
href="https://doi.org/10.1016/j.datak.2024.102284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease is the most common type of dementia in the elderly. Nevertheless, there is an early onset form that is difficult to diagnose precisely. As the genetic component is the most critical factor in developing this disease, identifying relevant genetic variants is key to obtaining a more reliable and straightforward diagnosis. The information about these variants is stored in an extensive number of data sources, which must be carefully analyzed to select only the information with sufficient quality to be used in a clinical setting. This selection has become complex due to the increasing available genomic information. The SILE method was designed to systematize identifying relevant variants for a disease in this challenging context. However, several problems on how SILE identifies relevant variants were discovered when applying the method to the early onset form of Alzheimer&#39;s disease. More specifically, the method failed to address specific features of this disease such as its low incidence and familiar component. This paper proposes an improvement of the identification process defined by the SILE method to make it applicable to a further spectrum of diseases. Details of how the proposed solution has been applied are also reported. As a result of this improvement, a set of 29 variants has been identified (25 variants Accepted with a Limited Evidence and 4 Accepted with Moderate Evidence). This constitutes a valuable result that facilitates and reinforces the genetic diagnosis of the disease.},
  archive      = {J_DKE},
  author       = {Mireia Costa and Ana León and Óscar Pastor},
  doi          = {10.1016/j.datak.2024.102284},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102284},
  shortjournal = {Data Knowl. Eng.},
  title        = {Improving the identification of relevant variants in genome information systems: A methodological approach with a case study on early onset alzheimer&#39;s disease},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy-ontology based knowledge driven disease risk level
prediction with optimization assisted ensemble classifier. <em>DKE</em>,
<em>151</em>, 102278. (<a
href="https://doi.org/10.1016/j.datak.2024.102278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern medicinal analysis is a complex procedure, requiring precise patient data, scientific knowledge obtained over numerous years and a theoretical understanding of related medical literature. To improve the accuracy and to reduce the time for diagnosis, clinical decision support systems (DSS) were introduced, which incorporate data mining schemes for enhancing the disease diagnosing accuracy. This work proposes a new disease-predicting model that involves 3 stages. Initially, “improved stemming and tokenization” are carried out in the pre-processing stage. Then, the “Fuzzy ontology, improved mutual information (MI), and correlation features” are extracted. Then, prediction is carried out via ensemble classifiers that include “improved Fuzzy logic, Long Short Term Memory (LSTM), Deep Convolution Neural Network (DCNN), and Bidirectional Gated Recurrent Unit (Bi-GRU)”.The outcomes from improved fuzzy logic, LSTM, and DCNN are further classified via Bi-GRU which offers the results. Specifically, Bi-GRU weights are optimally tuned using Deer Hunting Update Explored Arithmetic Optimization (DHUEAO). Finally, the efficiency of the proposed work is determined concerning a variety of metrics.},
  archive      = {J_DKE},
  author       = {Huma Parveen and Syed Wajahat Abbas Rizvi and Raja Sarath Kumar Boddu},
  doi          = {10.1016/j.datak.2024.102278},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {5},
  pages        = {102278},
  shortjournal = {Data Knowl. Eng.},
  title        = {Fuzzy-ontology based knowledge driven disease risk level prediction with optimization assisted ensemble classifier},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion learning of preference and bias from ratings and
reviews for item recommendation. <em>DKE</em>, <em>150</em>, 102283. (<a
href="https://doi.org/10.1016/j.datak.2024.102283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation methods improve rating prediction performance by learning selection bias phenomenon-users tend to rate items they like. These methods model selection bias by calculating the propensities of ratings, but inaccurate propensity could introduce more noise, fail to model selection bias, and reduce prediction performance. We argue that learning interaction features can effectively model selection bias and improve model performance, as interaction features explain the reason of the trend. Reviews can be used to model interaction features because they have a strong intrinsic correlation with user interests and item interactions. In this study, we propose a preference- and bias-oriented fusion learning model (PBFL) that models the interaction features based on reviews and user preferences to make rating predictions. Our proposal both embeds traditional user preferences in reviews, interactions, and ratings and considers word distribution bias and review quoting to model interaction features. Six real-world datasets are used to demonstrate effectiveness and performance. PBFL achieves an average improvement of 4.46% in root-mean-square error (RMSE) and 3.86% in mean absolute error (MAE) over the best baseline.},
  archive      = {J_DKE},
  author       = {Junrui Liu and Tong Li and Zhen Yang and Di Wu and Huan Liu},
  doi          = {10.1016/j.datak.2024.102283},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102283},
  shortjournal = {Data Knowl. Eng.},
  title        = {Fusion learning of preference and bias from ratings and reviews for item recommendation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate hierarchical DBSCAN model for enhanced maritime
data analytics. <em>DKE</em>, <em>150</em>, 102282. (<a
href="https://doi.org/10.1016/j.datak.2024.102282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is an important data analytics technique and has numerous use cases. It leads to the determination of insights and knowledge which would not be readily discernible on routine examination of the data. Enhancement of clustering techniques is an active field of research, with various optimisation models being proposed. Such enhancements are also undertaken to address particular issues being faced in specific applications. This paper looks at a particular use case in the maritime domain and how an enhancement of Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering results in the apt use of data analytics to solve a real-life issue. Passage of vessels over water is one of the significant utilisations of maritime regions. Trajectory analysis of these vessels helps provide valuable information, thus, maritime movement data and the knowledge extracted from manipulation of this data play an essential role in various applications, viz., assessing traffic densities, identifying traffic routes, reducing collision risks, etc. Optimised trajectory information would help enable safe and energy-efficient green operations at sea and assist autonomous operations of maritime systems and vehicles. Many studies focus on determining trajectory densities but miss out on individual trajectory granularities. Determining trajectories by using unique identities of the vessels may also lead to errors. Using an unsupervised DBSCAN method of identifying trajectories could help overcome these limitations. Further, to enhance outcomes and insights, the inclusion of temporal information along with additional parameters of Automatic Identification System (AIS) data in DBSCAN is proposed. Towards this, a new design and implementation for data analytics called the Multivariate Hierarchical DBSCAN method for better clustering of Maritime movement data, such as AIS, has been developed, which helps determine granular information and individual trajectories in an unsupervised manner. It is seen from the evaluation metrics that the performance of this method is better than other data clustering techniques.},
  archive      = {J_DKE},
  author       = {Nitin Newaliya and Yudhvir Singh},
  doi          = {10.1016/j.datak.2024.102282},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102282},
  shortjournal = {Data Knowl. Eng.},
  title        = {Multivariate hierarchical DBSCAN model for enhanced maritime data analytics},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new sentence embedding framework for the education and
professional training domain with application to hierarchical
multi-label text classification. <em>DKE</em>, <em>150</em>, 102281. (<a
href="https://doi.org/10.1016/j.datak.2024.102281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Natural Language Processing (NLP) has made significant advances through advanced general language embeddings, allowing breakthroughs in NLP tasks such as semantic similarity and text classification . However, complexity increases with hierarchical multi-label classification (HMC), where a single entity can belong to several hierarchically organized classes. In such complex situations, applied on specific-domain texts, such as the Education and professional training domain, general language embedding models often inadequately represent the unique terminologies and contextual nuances of a specialized domain. To tackle this problem, we present HMCCCProbT , a novel hierarchical multi-label text classification approach . This innovative framework chains multiple classifiers , where each individual classifier is built using a novel sentence-embedding method BERTEPro based on existing Transformer models, whose pre-training has been extended on education and professional training texts, before being fine-tuned on several NLP tasks. Each individual classifier is responsible for the predictions of a given hierarchical level and propagates local probability predictions augmented with the input feature vectors to the classifier in charge of the subsequent level. HMCCCProbT tackles issues of model scalability and semantic interpretation, offering a powerful solution to the challenges of domain-specific hierarchical multi-label classification. Experiments over three domain-specific textual HMC datasets indicate the effectiveness of HMCCCProbT to compare favorably to state-of-the-art HMC algorithms in terms of classification accuracy and also the ability of BERTEPro to obtain better probability predictions, well suited to HMCCCProbT , than three other vector representation techniques.},
  archive      = {J_DKE},
  author       = {Guillaume Lefebvre and Haytham Elghazel and Theodore Guillet and Alexandre Aussem and Matthieu Sonnati},
  doi          = {10.1016/j.datak.2024.102281},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102281},
  shortjournal = {Data Knowl. Eng.},
  title        = {A new sentence embedding framework for the education and professional training domain with application to hierarchical multi-label text classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Issues in inter-organizational data sharing: Findings from
practice and research challenges. <em>DKE</em>, <em>150</em>, 102280.
(<a href="https://doi.org/10.1016/j.datak.2024.102280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharing data is highly potent in assisting companies in internal optimization and designing new products and services. While the benefits seem obvious, sharing data is accompanied by a spectrum of concerns ranging from fears of sharing something of value, unawareness of what will happen to the data, or simply a lack of understanding of the short- and mid-term benefits. The article analyzes data sharing in inter-organizational relationships by examining 13 cases in a qualitative interview study and through public data analysis. Given the importance of inter-organizational data sharing as indicated by large research initiatives such as Gaia-X and Catena-X, we explore issues arising in this process and formulate research challenges. We use the theoretical lens of Actor-Network Theory to analyze our data and entangle its constructs with concepts in data sharing.},
  archive      = {J_DKE},
  author       = {Ilka Jussen and Frederik Möller and Julia Schweihoff and Anna Gieß and Giulia Giussani and Boris Otto},
  doi          = {10.1016/j.datak.2024.102280},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102280},
  shortjournal = {Data Knowl. Eng.},
  title        = {Issues in inter-organizational data sharing: Findings from practice and research challenges},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data analytics and knowledge discovery on big data:
Algorithms, architectures, and applications. <em>DKE</em>, <em>150</em>,
102279. (<a href="https://doi.org/10.1016/j.datak.2024.102279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Robert Wrembel and Johann Gamper},
  doi          = {10.1016/j.datak.2024.102279},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102279},
  shortjournal = {Data Knowl. Eng.},
  title        = {Data analytics and knowledge discovery on big data: Algorithms, architectures, and applications},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning model for predicting the number of stores
and average sales in commercial district. <em>DKE</em>, <em>150</em>,
102277. (<a href="https://doi.org/10.1016/j.datak.2024.102277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a plan for preparing for changes in the business environment by analyzing and predicting business district data in Seoul. The COVID-19 pandemic and economic crisis caused by inflation have led to an increase in store closures and a decrease in sales, which has had a significant impact on commercial districts. The number of stores and sales are critical factors that directly affect the business environment and can help prepare for changes. This study conducted correlation analysis to extract factors related to the commercial district’s environment in Seoul and estimated the number of stores and sales based on these factors. Using the Kendaltau correlation coefficient, the study found that existing population and working population were the most influential factors. Linear regression, tensor decomposition, Factorization Machine, and deep neural network models were used to estimate the number of stores and sales, with the deep neural network model showing the best performance in RMSE and evaluation indicators. This study also predicted the number of stores and sales of the service industry in a specific area using the population prediction results of the neural prophet model. The study’s findings can help identify commercial district information and predict the number of stores and sales based on location, industry, and influencing factors, contributing to the revitalization of commercial districts.},
  archive      = {J_DKE},
  author       = {Suan Lee and Sangkeun Ko and Arousha Haghighian Roudsari and Wookey Lee},
  doi          = {10.1016/j.datak.2024.102277},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102277},
  shortjournal = {Data Knowl. Eng.},
  title        = {A deep learning model for predicting the number of stores and average sales in commercial district},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bitwise approach on influence overload problem.
<em>DKE</em>, <em>150</em>, 102276. (<a
href="https://doi.org/10.1016/j.datak.2023.102276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly developing online social networks has enabled users to send or receive information very fast. However, due to the availability of an excessive amount of data in today’s society, managing the information has become very cumbersome, which may lead to the problem of information overload. This highly eminent problem, where the existence of too much relevant information available becomes a hindrance rather than a help, may cause losses, delays, and hardships in making decisions. Thus, in this paper, by defining information overload from a different aspect, we aim to maximize the information propagation while minimizing the information overload (duplication). To do so, we theoretically present the lower and upper bounds for the information overload using a bitwise-based approach as the leverage to mitigate the computation complexities and obtain an approximation ratio of 1 − 1 e 1−1e . We propose two main algorithms, B-square and C-square, and compare them with the existing algorithms. Experiments on two types of datasets, synthetic and real-world networks, verify the effectiveness and efficiency of the proposed approach in addressing the problem.},
  archive      = {J_DKE},
  author       = {Charles Cheolgi Lee and Jafar Afshar and Arousha Haghighian Roudsari and Woong-Kee Loh and Wookey Lee},
  doi          = {10.1016/j.datak.2023.102276},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102276},
  shortjournal = {Data Knowl. Eng.},
  title        = {A bitwise approach on influence overload problem},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A transformer-based neural network framework for full names
prediction with abbreviations and contexts. <em>DKE</em>, <em>150</em>,
102275. (<a href="https://doi.org/10.1016/j.datak.2023.102275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid spread of information, abbreviations are used more and more common because they are convenient. However, the duplication of abbreviations can lead to confusion in many cases, such as information management and information retrieval. The resultant confusion annoys users. Thus, inferring a full name from an abbreviation has practical and significant advantages. The bulk of studies in the literature mainly inferred full names based on rule-based methods, statistical models, the similarity of representation, etc. However, these methods are unable to use various grained contexts properly. In this paper, we propose a flexible framework of Multi-attention mask Abbreviation Context and Full name language model , named MACF to address the problem. With the abbreviation and contexts as the inputs, the MACF can automatically predict a full name by generation, where the contexts can be variously grained. That is, different grained contexts ranging from coarse to fine can be selected to perform such complicated tasks in which contexts include paragraphs, several sentences, or even just a few keywords. A novel multi-attention mask mechanism is also proposed, which allows the model to learn the relationships among abbreviations, contexts, and full names, a process that makes the most of various grained contexts. The three corpora of different languages and fields were analyzed and measured with seven metrics in various aspects to evaluate the proposed framework. According to the experimental results, the MACF yielded more significant and consistent outputs than other baseline methods . Moreover, we discuss the significance and findings, and give the case studies to show the performance in real applications.},
  archive      = {J_DKE},
  author       = {Ziming Ye and Shuangyin Li},
  doi          = {10.1016/j.datak.2023.102275},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102275},
  shortjournal = {Data Knowl. Eng.},
  title        = {A transformer-based neural network framework for full names prediction with abbreviations and contexts},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mining keys for graphs. <em>DKE</em>, <em>150</em>, 102274.
(<a href="https://doi.org/10.1016/j.datak.2023.102274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keys for graphs are a class of data quality rules that use topological and value constraints to uniquely identify entities in a data graph. They have been studied to support object identification, knowledge fusion, data deduplication , and social network reconciliation. Manual specification and discovery of graph keys is tedious and infeasible over large-scale graphs. To make GKeys GKeys useful in practice, we study the GKey GKey discovery problem, and present GKMiner GKMiner , an algorithm that mines keys over graphs. Our algorithm discovers keys in a graph via frequent subgraph expansion, and notably, identifies recursive keys, i.e., where the unique identification of an entity type is dependent upon the identification of another entity type. We introduce the key properties, minimality and support , which effectively help to reduce the space of candidate keys. GKMiner GKMiner uses a set of auxillary structures to summarize an input graph, and to identify likely candidate keys for greater pruning efficiency and evaluation of the search space . Our evaluation shows that identifying and using recursive keys in entity linking , lead to improved accuracy, over keys found using existing graph key mining techniques.},
  archive      = {J_DKE},
  author       = {Morteza Alipourlangouri and Fei Chiang},
  doi          = {10.1016/j.datak.2023.102274},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102274},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mining keys for graphs},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach to on-demand extension of multidimensional cubes
in multi-model settings: Application to IoT-based agro-ecology.
<em>DKE</em>, <em>150</em>, 102267. (<a
href="https://doi.org/10.1016/j.datak.2023.102267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing unstructured and heterogeneous data , integrating them, and enabling their analysis are among the key challenges in data ecosystems, together with the need to accommodate a progressive growth in these systems by seamlessly supporting extensibility. This is particularly relevant for OLAP analyses on multidimensional cubes stored in data warehouses (DWs), which naturally span large portions of heterogeneous data, possibly relying on different data models (relational, document-based, graph-based). While the management of model heterogeneity in DWs, using for instance multi-model databases, has already been investigated, not much has been done to support extensibility. In a previous paper we have investigated a schema-on-read scenario aimed at granting the extensibility of multidimensional cubes by proposing an architecture to support it and discussing the main open issues associated. This paper takes a step further by presenting xCube , an approach to provide on-demand extensibility of multidimensional cubes in a supply-driven fashion. xCube lets users choose a multidimensional element to be extended, using additional data, possibly uploaded from a data lake. Then, the multidimensional schema is extended by considering the functional dependencies implied by these additional data, and the extended multidimensional schema is made available to users for OLAP analyses. After explaining our approach with reference to a motivating case study in agro-ecology, we propose a proof-of-concept implementation using AgensGraph and Mondrian.},
  archive      = {J_DKE},
  author       = {Sandro Bimonte and Fagnine Alassane Coulibaly and Stefano Rizzi},
  doi          = {10.1016/j.datak.2023.102267},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102267},
  shortjournal = {Data Knowl. Eng.},
  title        = {An approach to on-demand extension of multidimensional cubes in multi-model settings: Application to IoT-based agro-ecology},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI system architecture design methodology based on IMO
(input-AI model-output) structure for successful AI adoption in
organizations. <em>DKE</em>, <em>150</em>, 102264. (<a
href="https://doi.org/10.1016/j.datak.2023.102264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of AI technology, the successful AI adoption in organizations has become a top priority in modern society. However, many organizations still struggle to articulate the necessary AI, and AI experts have difficulties understanding the problems faced by these organizations. This knowledge gap makes it difficult for organizations to identify the technical requirements , such as necessary data and algorithms, for adopting AI. To overcome this problem, we propose a new AI system architecture design methodology based on the IMO (Input-AI Model-Output) structure. The IMO structure enables effective identification of the technical requirements necessary to develop real AI models. While previous research has identified the importance and challenges of technical requirements, such as data and AI algorithms, for AI adoption, there has been little research on methodology to concretize them. Our methodology is composed of three stages: problem definition, system AI solution, and AI technical solution to design the AI technology and requirements that organizations need at a system level. The effectiveness of our methodology is demonstrated through a case study , logical comparative analysis with other studies, and experts reviews, which demonstrate that our methodology can support successful AI adoption to organizations.},
  archive      = {J_DKE},
  author       = {Seungkyu Park and Joong yoon Lee and Jooyeoun Lee},
  doi          = {10.1016/j.datak.2023.102264},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102264},
  shortjournal = {Data Knowl. Eng.},
  title        = {AI system architecture design methodology based on IMO (Input-AI model-output) structure for successful AI adoption in organizations},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Increase development productivity by domain-specific
conceptual modeling. <em>DKE</em>, <em>150</em>, 102263. (<a
href="https://doi.org/10.1016/j.datak.2023.102263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the question of whether and how the development and use of a domain-specific modeling method (DSMM) can increase productivity in the development of technical systems in an industrial setting. This is because an essential prerequisite for DSMMs to become established in operational practice is that productivity increases can be achieved with them and qualitative benefits such as quality assurance, innovation potential, and the like can be exploited. After all, managers’ decisions are ultimately based on whether or not the use of a new method pays off. We illustrate our findings using the example of a DSMM development for the design and realization of electric vehicle testbeds , which we carried out as part of a cooperation project. This work sets the base for possible generalization into other automotive, mechatronic, and technical areas.},
  archive      = {J_DKE},
  author       = {Martin Paczona and Heinrich C. Mayr and Guenter Prochart},
  doi          = {10.1016/j.datak.2023.102263},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102263},
  shortjournal = {Data Knowl. Eng.},
  title        = {Increase development productivity by domain-specific conceptual modeling},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving speech emotion recognition by fusing
self-supervised learning and spectral features via mixture of experts.
<em>DKE</em>, <em>150</em>, 102262. (<a
href="https://doi.org/10.1016/j.datak.2023.102262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech Emotion Recognition (SER) is an important area of research in speech processing that aims to identify and classify emotional states conveyed through speech signals. Recent studies have shown considerable performance in SER by exploiting deep contextualized speech representations from self-supervised learning (SSL) models. However, SSL models pre-trained on clean speech data may not perform well on emotional speech data due to the domain shift problem. To address this problem, this paper proposes a novel approach that simultaneously exploits an SSL model and a domain-agnostic spectral feature (SF) through the Mixture of Experts (MoE) technique. The proposed approach achieves the state-of-the-art performance on weighted accuracy compared to other methods in the IEMOCAP dataset. Moreover, this paper demonstrates the existence of the domain shift problem of SSL models in the SER task.},
  archive      = {J_DKE},
  author       = {Jonghwan Hyeon and Yung-Hwan Oh and Young-Jun Lee and Ho-Jin Choi},
  doi          = {10.1016/j.datak.2023.102262},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102262},
  shortjournal = {Data Knowl. Eng.},
  title        = {Improving speech emotion recognition by fusing self-supervised learning and spectral features via mixture of experts},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recognition algorithm for cross-texting in text chat
conversations. <em>DKE</em>, <em>150</em>, 102261. (<a
href="https://doi.org/10.1016/j.datak.2023.102261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the development of the Internet and IT technology, short-text based communication is so popular compared with voice based one. Chat-based communication enables rapid, short and massive exchange of message with many people, creates new social problems. ‘Cross-texting’ is one of them. It refers to accidentally sending a text to an unintended person during the concurrent conversations with separated multiple people. Cross-texting would be a serious problem in languages where respectful expressions are required. As text-based communication is getting popular, it is a crucial work to prevent cross-texting by detecting it in advance in languages with honorifics expression such as Korean. In this paper, we proposed two methods detecting a cross-text using a deep learning model. The first model is the formal feature vector, which models dialog by explicitly defining the politeness and completeness features. The second one is the grpah2vec based ChatGram-net model, which models the dialog based on the syllable occurrence relationship. To evaluate the detection performance, we suggest a generating method for cross-text datasets from a actual messenger corpus. In experiment we show that both proposed models detected cross-text effectively, and exceeded the performance of the baseline models .},
  archive      = {J_DKE},
  author       = {Da-Young Lee and Hwan-Gue Cho},
  doi          = {10.1016/j.datak.2023.102261},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102261},
  shortjournal = {Data Knowl. Eng.},
  title        = {Recognition algorithm for cross-texting in text chat conversations},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A-MKMC: An effective adaptive-based multilevel k-means
clustering with optimal centroid selection using hybrid heuristic
approach for handling the incomplete data. <em>DKE</em>, <em>150</em>,
102243. (<a href="https://doi.org/10.1016/j.datak.2023.102243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, clustering is defined as partitioning similar and dissimilar objects into several groups. It has been widely used in applications like pattern recognition, image processing , and data analysis. When the dataset contains some missing data or value, it is termed incomplete data. In such implications, the incomplete dataset issue is untreatable while validating the data. Due to these flaws, the quality or standard level of the data gets an impact. Hence, the handling of missing values is done by influencing the clustering mechanisms for sorting out the missing data. Yet, the traditional clustering algorithms fail to combat the issues as it is not supposed to maintain large dimensional data . It is also caused by errors of human intervention or inaccurate outcomes. To alleviate the challenging issue of incomplete data, a novel clustering algorithm is proposed. Initially, incomplete or mixed data is garnered from the five different standard data sources . Once the data is to be collected, it is undergone the pre-processing phase, which is accomplished using data normalization . Subsequently, the final step is processed by the new clustering algorithm that is termed Adaptive centroid based Multilevel K-Means Clustering (A-MKMC), in which the cluster centroid is optimized by integrating the two conventional algorithms such as Border Collie Optimization (BCO) and Whale Optimization Algorithm (WOA) named as Hybrid Border Collie Whale Optimization (HBCWO). Therefore, the validation of the novel clustering model is estimated using various measures and compared against traditional mechanisms. From the overall result analysis, the accuracy and precision of the designed HBCWO-A-MKMC method attain 93 % and 95 %. Hence, the adaptive clustering process exploits the higher performance that aids in sorting out the missing data issuecompared to the other conventional methods.},
  archive      = {J_DKE},
  author       = {Hima Vijayan and Subramaniam M and Sathiyasekar K},
  doi          = {10.1016/j.datak.2023.102243},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {3},
  pages        = {102243},
  shortjournal = {Data Knowl. Eng.},
  title        = {A-MKMC: An effective adaptive-based multilevel K-means clustering with optimal centroid selection using hybrid heuristic approach for handling the incomplete data},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating psychological analysis tables for children’s
drawings using deep learning. <em>DKE</em>, <em>149</em>, 102266. (<a
href="https://doi.org/10.1016/j.datak.2023.102266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usefulness of drawing-based psychological testing has been demonstrated in a variety of studies. By using the familiar medium of drawing, drawing-based psychological testing can be applied to a wide range of age groups and is particularly effective with children who have difficulty expressing themselves verbally. Drawing tests are usually implemented face-to-face, requiring specialized counseling staff, and can be time-consuming and expensive to apply to large numbers of children. These problems seem to be solved by applying highly developed artificial intelligence techniques. If artificial intelligence (AI) can analyze children&#39;s drawings and perform psychological analysis, it will be possible to use it as a service and take tests online or through smartphones. There have been various attempts to automate the drawing of psychological tests by utilizing deep learning technology to process images. Previous studies using classification have been limited in their ability to extract structural information. In this paper, we analyze the House-Tree-Person Test (HTP), one of the drawing psychological tests widely used in clinical practice, by utilizing object detection technology that can extract more diverse information from images. In addition, we extend the existing research that has been limited to the extraction of relatively simple psychological features and generate a psychological analysis table based on the extracted features that can be used to assist experts in the process of psychological testing. Our research findings indicate that the object detection performance achieves a mean Average Precision (mAP) of approximately 92.6∼94.1 %, and the average accuracy of the psychological analysis table is 94.4 %.},
  archive      = {J_DKE},
  author       = {Moonyoung Lee and Youngho Kim and Young-Kuk Kim},
  doi          = {10.1016/j.datak.2023.102266},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102266},
  shortjournal = {Data Knowl. Eng.},
  title        = {Generating psychological analysis tables for children&#39;s drawings using deep learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards deep understanding of graph convolutional networks
for relation extraction. <em>DKE</em>, <em>149</em>, 102265. (<a
href="https://doi.org/10.1016/j.datak.2023.102265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relation extraction aims at identifying semantic relations between pairs of named entities from unstructured texts and is considered an essential prerequisite for many downstream tasks in natural language processing (NLP). Owing to the ability in expressing complex relationships and interdependency , graph neural networks (GNNs) have been gradually used to solve the relation extraction problem and have achieved state-of-the-art results. However, the designs of GNN-based relation extraction methods are mostly based on empirical intuition, heuristic, and experimental trial-and-error. A clear understanding of why and how GNNs perform well in relation extraction tasks is lacking. In this study, we investigate three well-known GNN-based relation extraction models, CGCN, AGGCN, and SGCN , and aim to understand the underlying mechanisms of the extractions. In particular, we provide a visual analytic to reveal the dynamics of the models and provide insight into the function of intermediate convolutional layers . We determine that entities, particularly subjects and objects in them, are more important features than other words for relation extraction tasks. With various masking strategies, the significance of entity type to relation extraction is recognized. Then, from the perspective of the model architecture, we find that graph structure modeling and aggregation mechanisms in GCN do not significantly affect the performance improvement of GCN-based relation extraction models. The above findings are of great significance in promoting the development of GNNs. Based on these findings, an engineering oriented MLP-based GNN relation extraction model is proposed to achieve a comparable performance and greater efficiency.},
  archive      = {J_DKE},
  author       = {Tao Wu and Xiaolin You and Xingping Xian and Xiao Pu and Shaojie Qiao and Chao Wang},
  doi          = {10.1016/j.datak.2023.102265},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102265},
  shortjournal = {Data Knowl. Eng.},
  title        = {Towards deep understanding of graph convolutional networks for relation extraction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepScraper: A complete and efficient tweet scraping method
using authenticated multiprocessing. <em>DKE</em>, <em>149</em>, 102260.
(<a href="https://doi.org/10.1016/j.datak.2023.102260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a scraping method for collecting tweets, which we call DeepScraper . DeepScraper provides the complete scraping for the entire tweets written by a certain group of users or them containing search keywords with a fast speed. To improve the crawling speed of DeepScraper, we devise a multiprocessing architecture while providing authentication to the multiple processes based on the simulation of the user access behavior to Twitter. This allows us to maximize the parallelism of crawling even in a single machine . Through extensive experiments, we show that DeepScraper can crawl the entire tweets of 99 users, which amounts to 5,798,052 tweets while Twitter standard API can crawl only 243,650 tweets of them due to the constraints of the number of tweets to scrape. In other words, DeepScraper could collect 23.7 times more tweets for the 99 users than the standard API. We also show the efficiency of DeepScraper. First, we show the effect of the authenticated multiprocessing by showing that it increases the crawling speed from 2.03 ∼ ∼ 10.57 times as the number of running processes increases from 2 to 32 compared to DeepScraper with a single process. Then, we compare the crawling speed of DeepScraper with the existing studies. The result shows that DeepScraper is compared to even Twitter standard APIs and Twitter4J while DeepScraper can scrape much more tweets than them. Furthermore, DeepScraper is much faster than Twitter Scrapy roughly 3.69 times while both can scrape the entire tweets for the target users or keywords.},
  archive      = {J_DKE},
  author       = {Jaebeom You and Kisung Lee and Hyuk-Yoon Kwon},
  doi          = {10.1016/j.datak.2023.102260},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102260},
  shortjournal = {Data Knowl. Eng.},
  title        = {DeepScraper: A complete and efficient tweet scraping method using authenticated multiprocessing},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial for VSI: NLDB-saarbruecken-2021. <em>DKE</em>,
<em>149</em>, 102259. (<a
href="https://doi.org/10.1016/j.datak.2023.102259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Helmut Horacek and Epaminondas Kapetanios and Elisabeth Metais and Farid Meziane},
  doi          = {10.1016/j.datak.2023.102259},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102259},
  shortjournal = {Data Knowl. Eng.},
  title        = {Editorial for VSI: NLDB-saarbruecken-2021},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S_IDS: An efficient skyline query algorithm over incomplete
data streams. <em>DKE</em>, <em>149</em>, 102258. (<a
href="https://doi.org/10.1016/j.datak.2023.102258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient processing of mass stream data has attracted wide attention in the database field. The skyline query on the sensor data stream can monitor multiple targets in real time, to avoid abnormal events such as fire and explosion, which is very useful in the practical application of sensor data monitoring . However, real-world stream data may often contain incomplete data attributes due to faulty sensing devices or imperfect data collection techniques. Skyline queries over incomplete data streams may lead to a lack of transitivity and loop domination issues. To solve the problem of the skyline query over incomplete data streams, firstly, this paper uses differential dependency rule (DD) to fill the missing attribute values of data in the incomplete data stream. Then, the hierarchical grid index (HGrid) is introduced into the field of skyline query to improve pruning efficiency. In the process of skyline calculation, this paper only keeps as few calculation results as possible for the data that may affect the result to avoid a large number of repeated calculations. Thus, S_IDS (Skyline query algorithm over Incomplete Data Stream) is proposed to query skyline results with high confidence from the incomplete data stream. Finally, by comparing with the most advanced skyline query algorithms over incomplete data streams, the correctness and efficiency of the proposed S_IDS algorithm are proved.},
  archive      = {J_DKE},
  author       = {Mei Bai and Yuxue Han and Peng Yin and Xite Wang and Guanyu Li and Bo Ning and Qian Ma},
  doi          = {10.1016/j.datak.2023.102258},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102258},
  shortjournal = {Data Knowl. Eng.},
  title        = {S_IDS: An efficient skyline query algorithm over incomplete data streams},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based ontology driven reference framework for
security risk management. <em>DKE</em>, <em>149</em>, 102257. (<a
href="https://doi.org/10.1016/j.datak.2023.102257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security risk management (SRM) is crucial for protecting valuable assets from malicious harm. While blockchain technology has been proposed to mitigate security threats in traditional applications, it is not a perfect solution, and its security threats must be managed. This paper addresses the research problem of having no unified and formal knowledge models to support the SRM of traditional applications using blockchain and the SRM of blockchain-based applications. In accordance with this, we present a blockchain-based reference model (BbRM) and an ontology driven reference framework (OntReF) for the SRM of traditional and blockchain-based applications. The BbRM consolidates security threats of traditional and blockchain-based applications, structured following the SRM domain model and offers guidance for creating the OntReF using the domain model. OntReF is grounded on unified foundational ontology (UFO) and provides semantic interoperability and supporting the dynamic knowledge representation and instantiation of information security knowledge for the SRM. Our evaluation approaches demonstrate that OntReF is practical to use.},
  archive      = {J_DKE},
  author       = {Mubashar Iqbal and Aleksandr Kormiltsyn and Vimal Dwivedi and Raimundas Matulevičius},
  doi          = {10.1016/j.datak.2023.102257},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102257},
  shortjournal = {Data Knowl. Eng.},
  title        = {Blockchain-based ontology driven reference framework for security risk management},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable influenza forecasting scheme using DCC-based
feature selection. <em>DKE</em>, <em>149</em>, 102256. (<a
href="https://doi.org/10.1016/j.datak.2023.102256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As influenza is easily converted to another type of virus and spreads very quickly from person to person, it is more likely to develop into a pandemic. Even though vaccines are the most effective way to prevent influenza, it takes a lot of time to produce them. Due to this, there has been an imbalance in the supply and demand of influenza vaccines every year. For a smooth vaccine supply, it is necessary to accurately forecast vaccine demand at least three to six months in advance. So far, many machine learning-based predictive models have shown excellent performance. However, their use was limited due to performance deterioration due to inappropriate training data and inability to explain the results. To solve these problems, in this paper, we propose an explainable influenza forecasting model. In particular, the model selects highly related data based on the distance correlation coefficient for effective training and explains the prediction results using shapley additive explanations. We evaluated its performance through extensive experiments. We report some of the results.},
  archive      = {J_DKE},
  author       = {Sungwoo Park and Jaeuk Moon and Seungwon Jung and Seungmin Rho and Eenjun Hwang},
  doi          = {10.1016/j.datak.2023.102256},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102256},
  shortjournal = {Data Knowl. Eng.},
  title        = {Explainable influenza forecasting scheme using DCC-based feature selection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated detection and localization of concept drifts in
process mining with batch and stream trace clustering support.
<em>DKE</em>, <em>149</em>, 102253. (<a
href="https://doi.org/10.1016/j.datak.2023.102253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining can help organizations by extracting knowledge from event logs. However, process mining techniques often assume business processes are stationary, while actual business processes are constantly subject to change because of the complexity of organizations and their external environment. Thus, addressing process changes over time – known as concept drifts – allows for a better understanding of process behavior and can provide a competitive edge for organizations, especially in an online data stream scenario. Current approaches to handling process concept drift focus primarily on detecting and locating concept drifts, often through an integrated, albeit offline, approach. However, part of these integrated approaches rely on complex data structures related to tree-based process models, usually discovered through algorithms whose results are influenced by specific heuristic rules. Moreover, most of the proposed approaches have not been tested on public true concept drift-labeled event logs commonly used as benchmark, making comparative analysis difficult. In this article, we propose an online approach to detect and localize concept drifts in an integrated way using batch and stream trace clustering support. In our approach, cluster models provide input information for both concept drift detection and localization methods . Each cluster abstracts a behavior profile underlying the process and reveals descriptive information about the discovered concept drifts. Experiments with benchmark synthetic event logs with different control-flow changes, as well as with real-world event logs, showed that our approach, when relying on the same clustering model, is competitive in relation to baselines concept drift detection method. In addition, the experiment showed our approach is able to correctly locate the concept drifts detected and allows the analysis of such concept drifts through different process behavior profiles.},
  archive      = {J_DKE},
  author       = {Rafael Gaspar de Sousa and Antonio Carlos Meira Neto and Marcelo Fantinato and Sarajane Marques Peres and Hajo Alexander Reijers},
  doi          = {10.1016/j.datak.2023.102253},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102253},
  shortjournal = {Data Knowl. Eng.},
  title        = {Integrated detection and localization of concept drifts in process mining with batch and stream trace clustering support},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The power and potentials of flexible query answering
systems: A critical and comprehensive analysis. <em>DKE</em>,
<em>149</em>, 102246. (<a
href="https://doi.org/10.1016/j.datak.2023.102246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of chatbots , such as ChatGPT , has brought research attention to question answering systems , capable to generate natural language answers to user’s natural language queries. However, also in other kinds of systems, flexibility of querying, including but also going beyond the use of natural language, is an important feature. With this consideration in mind the paper presents a critical and comprehensive analysis of recent developments, trends and challenges of Flexible Query Answering Systems (FQASs). Flexible query answering is a multidisciplinary research field that is not limited to question answering in natural language, but comprises other query forms and interaction modalities, which aim to provide powerful means and techniques for better reflecting human preferences and intentions to retrieve relevant information. It adopts methods at the crossroad of several disciplines among which Information Retrieval (IR), databases, knowledge based systems , knowledge and data engineering , Natural Language Processing (NLP) and the semantic web may be mentioned. The analysis principles are inspired by the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) framework, characterized by a top-down process, starting with relevant keywords for the topic of interest to retrieve relevant articles from meta-sources And complementing these articles with other relevant articles from seed sources Identified by a bottom-up process. to mine the retrieved publication data a network analysis is performed Which allows to present in a synthetic way intrinsic topics of the selected publications. issues dealt with are related to query answering methods Both model-based and data-driven (the latter based on either machine learning or deep learning) And to their needs for explainability and fairness to deal with big data Notably by taking into account data veracity. conclusions point out trends and challenges to help better shaping the future of the FQAS field .},
  archive      = {J_DKE},
  author       = {Troels Andreasen and Gloria Bordogna and Guy De Tré and Janusz Kacprzyk and Henrik Legind Larsen and Sławomir Zadrożny},
  doi          = {10.1016/j.datak.2023.102246},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102246},
  shortjournal = {Data Knowl. Eng.},
  title        = {The power and potentials of flexible query answering systems: A critical and comprehensive analysis},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CALEB: A conditional adversarial learning framework to
enhance bot detection. <em>DKE</em>, <em>149</em>, 102245. (<a
href="https://doi.org/10.1016/j.datak.2023.102245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high growth of Online Social Networks (OSNs) over the last few years has allowed automated accounts, known as social bots, to gain ground. As highlighted by other researchers, many of these bots have malicious purposes and tend to mimic human behavior, posing high-level security threats on OSN platforms. Moreover, recent studies have shown that social bots evolve over time by reforming and reinventing unforeseen and sophisticated characteristics, making them capable of evading the current machine learning state-of-the-art bot detection systems. This work is motivated by the critical need to establish adaptive bot detection methods in order to proactively capture unseen evolved bots towards healthier OSNs interactions. In contrast with most earlier supervised ML approaches which are limited by the inability to effectively detect new types of bots, this paper proposes CALEB, a robust end-to-end proactive framework based on the Conditional Generative Adversarial Network (CGAN) and its extension, Auxiliary Classifier GAN (AC-GAN), to simulate bot evolution by creating realistic synthetic instances of different bot types. These simulated evolved bots augment existing bot datasets and therefore enhance the detection of emerging generations of bots before they even appear. Furthermore, we show that our augmentation approach overpasses other earlier augmentation techniques which fail at simulating evolving bots. Extensive experimentation on well established public bot datasets, show that our approach offers a performance boost of up to 10% regarding the detection of new unseen bots. Finally, the use of the AC-GAN Discriminator as a bot detector, has outperformed former ML approaches, showcasing the efficiency of our end to end framework.},
  archive      = {J_DKE},
  author       = {Ilias Dimitriadis and George Dialektakis and Athena Vakali},
  doi          = {10.1016/j.datak.2023.102245},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102245},
  shortjournal = {Data Knowl. Eng.},
  title        = {CALEB: A conditional adversarial learning framework to enhance bot detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global and item-by-item reasoning fusion-based multi-hop
KGQA. <em>DKE</em>, <em>149</em>, 102244. (<a
href="https://doi.org/10.1016/j.datak.2023.102244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing embedded multi-hop Question Answering over Knowledge Graph (KGQA) methods attempted to handle Knowledge Graph (KG) sparsity using Knowledge Graph Embedding (KGE) to improve KGQA. However, they almost ignore the intermediate path reasoning process of answer prediction, do not consider the information interaction between the question and the KG, and rarely consider the problem that the triple scoring reasoning mechanism is inadequate in extracting deep features. To address the above issues, this paper proposes Global and Item-by-item Reasoning Fusion-based Multi-hop KGQA (GIRFM-KGQA). In global reasoning, a convolutional attention reasoning mechanism is proposed and fused with the triple scoring reasoning mechanism to jointly implement global reasoning, thus enhancing the long-chain reasoning ability of the global reasoning model. In item-by-item reasoning, the reasoning path is formed by serially predicting relations, and then the answer is predicted, which effectively solves the problem that the embedded multi-hop KGQA method lacks the intermediate path reasoning ability. In addition, we introduce an information interaction method between the question and the KG to improve the accuracy of the answer prediction. Finally, we merge the global reasoning score with the item-by-item reasoning score to jointly predict the answer. Our model, compared to the baseline model (EmbedKGQA), achieves an accuracy improvement of 0.5% and 2.7% on two-hop questions, and 6.2% and 4.6% on three-hop questions for the MetaQA_Full and MetaQA_Half datasets, and 1.7% on the WebQuestionSP dataset, respectively. The experimental results show that the proposed model can effectively improve the accuracy of the multi-hop KGQA model and enhance the interpretability of the model. We have made our model’s source code available at github: https://github.com/feixiongfeixiong/GIRFM .},
  archive      = {J_DKE},
  author       = {Tongzhao Xu and Turdi Tohti and Askar Hamdulla},
  doi          = {10.1016/j.datak.2023.102244},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102244},
  shortjournal = {Data Knowl. Eng.},
  title        = {Global and item-by-item reasoning fusion-based multi-hop KGQA},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What is the business value of your data? A multi-perspective
empirical study on monetary valuation factors and methods for data
governance. <em>DKE</em>, <em>149</em>, 102242. (<a
href="https://doi.org/10.1016/j.datak.2023.102242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digitalization has greatly increased the importance of data in recent years, making data an indispensable resource for value creation in our time. There is currently still a lack of theories as well as practicable methods and techniques for the monetary valuation of data, and data is therefore not yet sufficiently managed in terms of business management principles. In this context, this research is intended to design theory ingrained principles for a multidimensional conceptual approach to the monetary valuation of data as assets. We draw on the theory of dynamic capabilities as a further development of resource theory as well as value theory. To this end, the research conducts a qualitative field study followed by a quantitative survey study. Literature analysis is used to explain different dimensions in the qualitative field study. Structural equation modeling is used to analyze empirical data collected in the quantitative study. The results show that data value determination is a multidimensional and hierarchical construct consisting of three primary dimensions. These are the benefit-oriented, cost-oriented, and quality-oriented dimensions. The results also confirm that institutional pressures (coercive, normative, mimetic) that influence organizational behaviors lead to a greater intention for organizations to adapt a monetary data value determination.},
  archive      = {J_DKE},
  author       = {Frank Bodendorf and Jörg Franke},
  doi          = {10.1016/j.datak.2023.102242},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102242},
  shortjournal = {Data Knowl. Eng.},
  title        = {What is the business value of your data? a multi-perspective empirical study on monetary valuation factors and methods for data governance},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for approximate product search using faceted
navigation and user preference ranking. <em>DKE</em>, <em>149</em>,
102241. (<a href="https://doi.org/10.1016/j.datak.2023.102241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the problems that e-commerce users face is that the desired products are sometimes not available and Web shops fail to provide similar products due to their exclusive reliance on Boolean faceted search . User preferences are also often not taken into account. In order to address these problems, we present a novel framework specifically geared towards approximate faceted search within the product catalog of a Web shop. It is based on adaptations to the p-norm extended Boolean model, to account for the domain-specific characteristics of faceted search in an e-commerce environment. These e-commerce specific characteristics are, for example, the use of quantitative properties and the presence of user preferences. Our approach explores the concept of facet similarity functions in order to better match products to queries. In addition, the user preferences are used to assign importance weights to the query terms. Using a large-scale experimental setup based on real-world data, we conclude that the proposed algorithm outperforms the considered benchmark algorithms. Last, we have performed a user-based study in which we found that users who use our approach find more relevant products with less effort.},
  archive      = {J_DKE},
  author       = {Damir Vandic and Lennart J. Nederstigt and Flavius Frasincar and Uzay Kaymak and Enzo Ido},
  doi          = {10.1016/j.datak.2023.102241},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102241},
  shortjournal = {Data Knowl. Eng.},
  title        = {A framework for approximate product search using faceted navigation and user preference ranking},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical framework for interpretable and specialized
deep reinforcement learning-based predictive maintenance. <em>DKE</em>,
<em>149</em>, 102240. (<a
href="https://doi.org/10.1016/j.datak.2023.102240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning holds significant potential for application in industrial decision-making, offering a promising alternative to traditional physical models. However, its black-box learning approach presents challenges for real-world and safety-critical systems, as it lacks interpretability and explanations for the derived actions. Moreover, a key research question in deep reinforcement learning is how to focus policy learning on critical decisions within sparse domains. This paper introduces a novel approach that combines probabilistic modeling and reinforcement learning, providing interpretability and addressing these challenges in the context of safety-critical predictive maintenance . The methodology is activated in specific situations identified through the input–output hidden Markov model, such as critical conditions or near-failure scenarios. To mitigate the challenges associated with deep reinforcement learning in safety-critical predictive maintenance, the approach is initialized with a baseline policy using behavioral cloning, requiring minimal interactions with the environment. The effectiveness of this framework is demonstrated through a case study on predictive maintenance for turbofan engines , outperforming previous approaches and baselines, while also providing the added benefit of interpretability. Importantly, while the framework is applied to a specific use case, this paper aims to present a general methodology that can be applied to diverse predictive maintenance applications.},
  archive      = {J_DKE},
  author       = {Ammar N. Abbas and Georgios C. Chasparis and John D. Kelleher},
  doi          = {10.1016/j.datak.2023.102240},
  journal      = {Data &amp; Knowledge Engineering},
  month        = {1},
  pages        = {102240},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hierarchical framework for interpretable and specialized deep reinforcement learning-based predictive maintenance},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
