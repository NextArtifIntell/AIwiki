<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMCOM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comcom---326">COMCOM - 326</h2>
<ul>
<li><details>
<summary>
(2024). Editorial special issue: Extended papers from the 18th
wireless on-demand network systems and services “WONS 2023” conference.
<em>COMCOM</em>, <em>228</em>, 107988. (<a
href="https://doi.org/10.1016/j.comcom.2024.107988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Renato Lo Cigno and Stefano Basagni and Paolo Casari},
  doi          = {10.1016/j.comcom.2024.107988},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107988},
  shortjournal = {Comput. Commun.},
  title        = {Editorial special issue: Extended papers from the 18th wireless on-demand network systems and services “WONS 2023” conference},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue of IFIP networking 2023. <em>COMCOM</em>,
<em>228</em>, 107987. (<a
href="https://doi.org/10.1016/j.comcom.2024.107987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Mun Choon Chan and Xavier Gelabert and Violet R. Syrotiuk},
  doi          = {10.1016/j.comcom.2024.107987},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107987},
  shortjournal = {Comput. Commun.},
  title        = {Special issue of IFIP networking 2023},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HMLB: Holonic multi-agent approach for preventive
controllers load-balancing in SDN-enabled smart grid. <em>COMCOM</em>,
<em>228</em>, 107984. (<a
href="https://doi.org/10.1016/j.comcom.2024.107984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grid networks present advantages like improving reliability, security, scalability, etc. However, designing an efficient communication infrastructure for smart grid networks is a great challenge. This is because of its dependency on proprietary protocols and specific vendors. Software-defined-enabled smart grid (SDN-SG) tackles this problem by incorporating diverse protocols and standards including open source platforms. One of the most important questions in Software-defined Networking (SDN) is the controller placement problem being NP-Hard in nature. Therefore, the predominant goal of this paper is to diminish the time complexity by modeling the controller placement problem based on the holonic multi-agent system. The hierarchical structure of a holonic organization improves the computational complexity through the divide and conquer mechanism. Such an idea also decreases the distributed controllers&#39; synchronization overhead which is an issue in the realm of SDN. On the other hand, the proper functioning of the smart grid has a strict dependency on time-critical services. Accordingly, the controller placement is supposed to be a Quality of Service-aware (QoS-aware) one. Also, intermittent topology changes in the smart grid and the occasional joining and leaving of members result in an unsteady traffic pattern and dynamicity of controller load. This research is a pioneer in providing a QoS-aware and dynamic controller placement mechanism for SDN-SG. Experimental results certify the preponderance of the approach over similar ones concerning computational complexity, packet loss, controllers’ synchronization overhead, and also load-balancing overhead.},
  archive      = {J_COMCOM},
  author       = {Marjan Keramati and Sauleh Etemedi and Nasser Mozayani},
  doi          = {10.1016/j.comcom.2024.107984},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107984},
  shortjournal = {Comput. Commun.},
  title        = {HMLB: Holonic multi-agent approach for preventive controllers load-balancing in SDN-enabled smart grid},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DAR-DRL: A dynamic adaptive routing method based on deep
reinforcement learning. <em>COMCOM</em>, <em>228</em>, 107983. (<a
href="https://doi.org/10.1016/j.comcom.2024.107983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-centric wireless networks offer users a diverse range of services and experiences. However, existing intelligent routing methods often struggle to make suitable routing decisions during dynamic network changes, significantly limiting transmission performance. This paper proposes a dynamic adaptive routing method based on Deep Reinforcement Learning (DAR-DRL) to effectively address these challenges. First, to accurately model network state information in complex and dynamically changing routing tasks, we introduce a link-aware graph learning model (LA-GNN) that efficiently senses network information of varying structures through a hierarchical aggregated message-passing neural network. Second, to ensure routing reliability in dynamic environments, we design a hop-by-hop routing strategy featuring a large acceptance domain and a reliability guarantee reward function. This mechanism adaptively avoids routing holes and loops across various network scenarios while enhancing the robustness of routing under dynamic conditions. Experimental results demonstrate that the proposed DAR-DRL method achieves the network routing task with shorter end-to-end delays, lower packet loss rates, and higher throughput compared to existing mainstream methods across common dynamic network scenarios, including cases with dynamic traffic variations, random link failures (small topology changes), and significant topology alterations.},
  archive      = {J_COMCOM},
  author       = {Zheheng Rao and Yanyan Xu and Ye Yao and Weizhi Meng},
  doi          = {10.1016/j.comcom.2024.107983},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107983},
  shortjournal = {Comput. Commun.},
  title        = {DAR-DRL: A dynamic adaptive routing method based on deep reinforcement learning},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting critical downlink traffic in LoRaWAN.
<em>COMCOM</em>, <em>228</em>, 107981. (<a
href="https://doi.org/10.1016/j.comcom.2024.107981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN, a low-power wide-area network (LPWAN) technology, has been successfully used in the Internet of Things (IoT) industry over the last decade. It is an easy-to-use, long-distance communication protocol combined with minimal power consumption. Supporting critical downlink traffic in LoRaWAN networks is crucial for ensuring the reliable and efficient delivery of essential data in certain actuating applications. However, challenges arise when prioritizing critical downlink traffic, including commands, alerts, and emergency notifications that demand immediate attention from actuating devices. This paper explores strategies to improve downlink traffic delivery in LoRaWAN networks, focusing on enhancing reliability, fairness, and energy efficiency through prioritization techniques and network parameter configurations in the EU868 spectrum. Theoretical as well as simulation results provide insights into the effectiveness of the available solutions for supporting critical downlink traffic in LoRaWAN networks.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas and Aruzhan Sabyrbek},
  doi          = {10.1016/j.comcom.2024.107981},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107981},
  shortjournal = {Comput. Commun.},
  title        = {Supporting critical downlink traffic in LoRaWAN},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proactive scheduling for mmWave wireless LANs.
<em>COMCOM</em>, <em>228</em>, 107979. (<a
href="https://doi.org/10.1016/j.comcom.2024.107979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To cope with growing wireless bandwidth demand, millimeter wave (mmWave) communication has been identified as a promising technology to deliver Gbps throughput. However, due to the susceptibility of mmWave signals to blockage, applications can experience significant performance variability as users move around due to rapid and significant variation in channel conditions. In this context, proactive schedulers that make use of future data rate prediction have potential to bring a significant performance improvement as compared to traditional schedulers. In this work, we explore the possibility of proactive scheduling that uses mobility prediction and some knowledge of the environment to predict future channel conditions. We present both an optimal proactive scheduler, which is based on an integer linear programming formulation and provides an upper bound on proactive scheduling performance, and a greedy heuristic proactive scheduler that is suitable for practical implementation. Extensive simulation results show that proactive scheduling has the potential to increase average user data rate by up to 35% over the classic proportional fair scheduler without any loss of fairness and incurring only a small increase in jitter. The results also show that the efficient proactive heuristic scheduler achieves from 60% to 75% of the performance gains of the optimal proactive scheduler. Finally, the results show that proactive scheduling performance is sensitive to the quality of mobility prediction and, thus, use of state-of-the-art mobility prediction techniques will be necessary to realize its full potential.},
  archive      = {J_COMCOM},
  author       = {Ang Deng and Douglas M. Blough},
  doi          = {10.1016/j.comcom.2024.107979},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107979},
  shortjournal = {Comput. Commun.},
  title        = {Proactive scheduling for mmWave wireless LANs},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing key value indicators in intent-based networks
through digital twins aided service orchestration mechanisms.
<em>COMCOM</em>, <em>228</em>, 107977. (<a
href="https://doi.org/10.1016/j.comcom.2024.107977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many years, the orchestration of network resources and services has been addressed by considering homogeneous communication infrastructures and simple Service Level Agreements (SLAs), generally defined through a list of traditional Key Performance Indicators (KPIs). Unfortunately, state-of-the-art solutions risk being quite ineffective for future telecommunication systems. Beyond 5G networks, for instance, are emerging as complex and heterogeneous ecosystems where resources belonging to diverse network domains with evolving capabilities can be dynamically exposed to support much more complex and cross-domain services and applications. At the same time, SLAs will be defined by also considering novel performance demands, including security, economic, and environmental needs. Based on these premises, this work proposes a novel orchestration strategy designed to fulfill service requirements expressed through Key Value Indicators (KVIs), while combining the potentials of both Network Digital Twins and Intent-Based Networking. Leveraging insights from Network Digital Twins, multiple service orchestration options are explored to optimize resource utilization. Simultaneously, Intent-Based Networking is adopted to streamline network management via intents, specifying Beyond 5G requirements through KPIs and KVIs. An optimal orchestration scheme has been conceived through a multi-criteria decision-making algorithm and a many-to-many matching game between domains and service requests mapped into intents, aiming to minimize SLA violations over time. The performance of the conceived solution has been investigated through computer simulations in realistic scenarios. The obtained results clearly highlight its effectiveness and demonstrate that it is able to reduce SLA violations (related to latency, throughput, costs, and cyber risk requirements) by up to 22.44% compared to other baseline techniques.},
  archive      = {J_COMCOM},
  author       = {Federica de Trizio and Giancarlo Sciddurlo and Ilaria Cianci and Giuseppe Piro and Gennaro Boggia},
  doi          = {10.1016/j.comcom.2024.107977},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107977},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing key value indicators in intent-based networks through digital twins aided service orchestration mechanisms},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical and lightweight defense against website
fingerprinting. <em>COMCOM</em>, <em>228</em>, 107976. (<a
href="https://doi.org/10.1016/j.comcom.2024.107976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Website fingerprinting is a passive network traffic analysis technique that enables an adversary to identify the website visited by a user despite encryption and the use of privacy services such as Tor. Several website fingerprinting defenses built on top of Tor have been proposed to guarantee a user’s privacy by concealing trace features that are important to classification. However, some of the best defenses incur a high bandwidth and/or latency overhead. To combat this, new defenses have sought to be both lightweight — i.e., introduce a small amount of bandwidth overhead — and zero-delay to real network traffic. This work introduces a novel zero-delay and lightweight website fingerprinting defense, called BRO, which conceals the feature-rich beginning of a trace while still enabling the obfuscation of features deeper into the trace without spreading the padding budget thin. BRO schedules padding with a randomized beta distribution that can skew to both the extreme left and right, keeping the applied padding clustered to a finite portion of a trace. This work specifically targets deep learning attacks, which continue to be among the most accurate website fingerprinting attacks. Results show that BRO outperforms other well-known website fingerprinting defenses, such as FRONT, with similar bandwidth overhead.},
  archive      = {J_COMCOM},
  author       = {Colman McGuan and Chansu Yu and Kyoungwon Suh},
  doi          = {10.1016/j.comcom.2024.107976},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107976},
  shortjournal = {Comput. Commun.},
  title        = {Practical and lightweight defense against website fingerprinting},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient proof-of-authority consensus scheme against
cloning attacks. <em>COMCOM</em>, <em>228</em>, 107975. (<a
href="https://doi.org/10.1016/j.comcom.2024.107975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proof-of-Authorization (PoA) consensus algorithms are widely used in permissioned blockchain networks due to their high throughput, security, and efficiency. However, PoA is susceptible to cloning attacks, where attackers copy the authenticator identity and key, thereby compromising the consensus integrity. This study proposes a novel randomized authenticator within the PoA framework to mitigate cloning attacks and solve the leader selection bottleneck. The main contributions include 1) Introducing unpredictability in leader selection through Verifiable Random Functions (VRFs) to prevent identity duplication.2) Dynamic group management using a hierarchical decentralized architecture of distributed ledgers that balances security and performance.3) Using threshold signatures to avoid a single point of failure among validators.4) Comprehensively analyzing attacks, security, randomness, and availability.5) Evaluating the effectiveness of a randomized authenticator by means of OMNET++ simulations to assess efficiency. By integrating randomness into leader selection and robust consensus design, the approach enables reliable and secure dynamic group management in decentralized networks.},
  archive      = {J_COMCOM},
  author       = {Shu-Ping Lu and Chin-Laung Lei and Meng-Han Tsai},
  doi          = {10.1016/j.comcom.2024.107975},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107975},
  shortjournal = {Comput. Commun.},
  title        = {An efficient proof-of-authority consensus scheme against cloning attacks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning approach to forecast 5G metrics in a
commercial and operational 5G platform: 5G and mobility.
<em>COMCOM</em>, <em>228</em>, 107974. (<a
href="https://doi.org/10.1016/j.comcom.2024.107974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for more secure, available, reliable, and fast networks emerges in a more interconnected society. In this context, 5G networks aim to transform how we communicate and interact. However, studies using 5G data are sparse since there are only a few number of publicly available 5G datasets (especially about commercial 5G network metrics with real users). In this work, we analyze the data of a commercial 5G deployment with real users, and propose forecasting techniques to help understand the trends and to manage 5G networks. We propose the creation of a metric to measure the traffic load. We forecast the metric using several machine learning models, and we choose LightGBM as the best approach. We observe that this approach obtains results with a good accuracy, and better than other machine learning approaches, but its performance decreases if the patterns contain unexpected events. Taking advantage of the lower accuracy in the performance, this is used to detect changes in the patterns and manage the network in real-time, supporting network resource elasticity by generating alarms and automating the scaling during these unpredictable fluctuations. Moreover, we introduce mobility data and integrate it with the previously traffic load metric, understanding its correlation and the prediction of 5G metrics through the use of the mobility data. We show again that LightGBM is the best model in predicting both types of 5G handovers, intra- and inter-gNB handovers, using the mobility information through Radars in the several roads, and lanes, near the 5G cells.},
  archive      = {J_COMCOM},
  author       = {Ana Almeida and Pedro Rito and Susana Brás and Filipe Cabral Pinto and Susana Sargento},
  doi          = {10.1016/j.comcom.2024.107974},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107974},
  shortjournal = {Comput. Commun.},
  title        = {A machine learning approach to forecast 5G metrics in a commercial and operational 5G platform: 5G and mobility},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probing-aided spectrum sensing-based hybrid access strategy
for energy harvesting CRNs. <em>COMCOM</em>, <em>228</em>, 107973. (<a
href="https://doi.org/10.1016/j.comcom.2024.107973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of energy supply and spectrum scarcity in Internet of Things (IoT), energy harvesting (EH) and cognitive radio (CR) technologies have been proposed and widely applied. In EH-CR networks (EH-CRNs), miss detection causes significant energy and time wastage of IoT devices, especially secondary users (SUs), and causes serious interference to primary users (PUs). To alleviate this concern, we propose a probing-aided spectrum sensing (PaSS) model for EH-CRNs, where M pairs of PUs and one pair of SUs coexist. The secondary transmitter (ST) harvests energy from the radio frequency (RF) signals of PUs for opportunistic spectrum access. In the PaSS model, probing operation is employed to further confirm the real state of the spectrum that has been sensed as free in order to avoid the waste of time and energy resulting from miss detection. Based on the PaSS model, we propose a novel hybrid access strategy, where the ST’s actions (i.e., sensing, probing, EH, underlay/overlay transmission mode) depend on the belief vector of M channels, energy state and data buffer state of the ST. By developing an adjusted double deep Q-network (ADDQN) reinforcement learning algorithm, we aim to find the optimal strategy that minimizes the long-term average number of packet losses (ANPL) and the ANPL minimization problem is an integer programming problem. Simulation results validate the ANPL performance of the ST in the ADDQN-PaSS model, and reveal impacts of network parameters on the performance of the ST, and find that at least 7.9% reduction of ANPL is achieved by using the ADDQN-PaSS model.},
  archive      = {J_COMCOM},
  author       = {Xiaoying Liu and Xinyu Kuang and Yuxin Chen and Kechen Zheng and Jia Liu},
  doi          = {10.1016/j.comcom.2024.107973},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107973},
  shortjournal = {Comput. Commun.},
  title        = {Probing-aided spectrum sensing-based hybrid access strategy for energy harvesting CRNs},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smart blockchain networks: Revolutionizing donation tracking
in the web 3.0. <em>COMCOM</em>, <em>228</em>, 107972. (<a
href="https://doi.org/10.1016/j.comcom.2024.107972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A donation-tracking system leveraging smart contracts and blockchain technology holds transformative potential for reshaping the landscape of charitable giving, especially within the context of Web 3.0. This paper explores how smart contracts and blockchain can be used to create a transparent and secure ledger for tracking charitable donations. We highlight the limitations of traditional donation systems and how a blockchain-based system can help overcome these challenges. The functionality of smart contracts in donation tracking, offering advantages such as automation, reduced transaction fees, and enhanced accountability, is elucidated. The decentralized and tamper-proof nature of blockchain technology is emphasized for increased transparency and fraud prevention. While elucidating the benefits, we also address challenges in implementing such a system, including the need for technical expertise and security considerations. By fostering trust and accountability, a donation-tracking system in Web 3.0, empowered by smart blockchain networks, aims to catalyze a profound positive impact in the realm of philanthropy.},
  archive      = {J_COMCOM},
  author       = {Chaimaa Nairi and Murtaza Cicioğlu and Ali Çalhan},
  doi          = {10.1016/j.comcom.2024.107972},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107972},
  shortjournal = {Comput. Commun.},
  title        = {Smart blockchain networks: Revolutionizing donation tracking in the web 3.0},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of provably secure and lightweight authentication
protocol for unmanned aerial vehicle systems. <em>COMCOM</em>,
<em>228</em>, 107971. (<a
href="https://doi.org/10.1016/j.comcom.2024.107971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones also called Unmanned Aerial Vehicles (UAVs) have become more prominent in several applications such as package delivery, real-time object detection, tracking, traffic monitoring, security surveillance systems, and many others. As a key member of IoT, the group of Radio Frequency IDentification (RFID) technologies is referred to as Automatic Identification and Data Capturing (AIDC). In particular, RFID technology is becoming a contactless and wireless technique used to automatically identify and track the tagged objects via radio frequency signals. It also has drawn a lot of attention among researchers, scientists, industries, and practitioners due to its broad range of real-world applications in various fields. However, RFID systems face two key concerns related to security and privacy, where an adversary performs eavesdropping, tampering, modification, and even interception of the secret information of the RFID tags, which may cause forgery and privacy problems. In contrast to security and privacy, RFID tags have very limited computational power capability. To deal with these issues, this paper puts forward an RFID-based Lightweight and Provably Secure Authentication Protocol (LPSAP) for Unmanned Aerial Vehicle Systems. The proposed protocol uses secure Physically Unclonable Functions (PUFs), Elliptic-Curve Cryptography (ECC), secure one-way hash, bitwise XOR, and concatenation operations. We use Ouafi and Phan’s formal security model for analyzing security and privacy features such as traceability and mutual authentication. The rigorous informal analysis is carried out which ensures that our proposed protocol achieves various security and privacy features as well as resists various known security attacks. The performance analysis demonstrates that our proposed protocol outperforms other existing protocols. In addition, Scyther and Automated Validation of Internet Security Protocols and Applications (AVISPA) tool simulation results demonstrates that there is no security attack possible within bounds. Therefore, our proposed LPSAP protocol achieves an acceptable high level of security with the least computational, communication, and storage costs on passive RFID tags.},
  archive      = {J_COMCOM},
  author       = {Mohd Shariq and Mauro Conti and Karan Singh and Sanjeev Kumar Dwivedi and Mohammad Abdussami and Ruhul Amin and Mehedi Masud},
  doi          = {10.1016/j.comcom.2024.107971},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107971},
  shortjournal = {Comput. Commun.},
  title        = {Design of provably secure and lightweight authentication protocol for unmanned aerial vehicle systems},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of the age of correlated information in V2X
networks with edge computing. <em>COMCOM</em>, <em>228</em>, 107970. (<a
href="https://doi.org/10.1016/j.comcom.2024.107970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-Everything (V2X) communication has the potential to revolutionize the travel experience in intelligent transportation, which has received the great attention recently. However, ensuring the freshness of information from multiple sources is critical for the real-time and reliable communication in vehicular networks, especially for timely updates of service centers. To address this issue, we use a promising metric called Age of Correlated Information (AoCI), which can characterize the freshness of multi-source information. Therefore, we propose a novel model that can dynamically regulate the channel activation matching and edge computing collaboration strategy to minimize AoCI in V2X vehicular networks. Firstly, we describe the system model of a V2X network with edge computing, including definitions and assumptions for freshness of information, edge co-computing, etc. Secondly, we formulate the joint optimization problem as a source-related age minimization (SRAM) problem, which is NP-complete. A heuristic algorithm is proposed to solve it under fast-fading channel. Finally, since traditional graph models cannot capture the changing correlation between nodes in dynamic networks, we use graph convolutional networks(GCN) to extract the features of multi-source correlation. The features extracted by GCN include relevant attributes of the sources and its communication links. The features are provided as input to a double deep Q network (DDQN) for training the model that can adapt to a dynamic network environment. Extensive simulation experiments in different network scenarios validate that our proposed method can effectively and efficiently reduce the average AoCI and the computational resources.},
  archive      = {J_COMCOM},
  author       = {Jian Wang and Tengfei Cao and Xingyan Chen and Xiaoying Wang},
  doi          = {10.1016/j.comcom.2024.107970},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107970},
  shortjournal = {Comput. Commun.},
  title        = {Optimization of the age of correlated information in V2X networks with edge computing},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive framework for 5G indoor localization.
<em>COMCOM</em>, <em>228</em>, 107968. (<a
href="https://doi.org/10.1016/j.comcom.2024.107968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Localization inside legacy private 5G networks is a daunting task that involves solving the problem of indoor localization using commercial off-the-shelf proprietary hardware. While some previous work has focused on experimental analysis, none has undertaken to develop a realistic solution based on commercial equipment. In this study, we present the first comprehensive and concrete 5G framework that combines fingerprinting with the 3GPP Enhanced Cell ID (E-CID) approach. Our methodology consists of a machine-learning model to deduce the user’s position by comparing the signal strength received from the User Equipment (UE) with a reference radio power map. To achieve this, the 3GPP protocols and functions are improved to provide open, centralized, and universal localization functions. A new reference map paradigm named Optical Radio Power Estimation using Light Analysis (ORPELA) is introduced. Real-world experiments prove that it is reproducible and more accurate than state-of-the-art radio-planning software. Machine-learning models are then designed, trained, and optimized for an ultra-challenging radio context. Finally, a large-scale experimental campaign encompassing a wide range of cases, including line-of-sight or mobility, is being conducted to demonstrate expected location performance within realistic 5G private networks.},
  archive      = {J_COMCOM},
  author       = {Antonin Le Floch and Rahim Kacimi and Pierre Druart and Yoann Lefebvre and André-Luc Beylot},
  doi          = {10.1016/j.comcom.2024.107968},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107968},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive framework for 5G indoor localization},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An on-demand collaborative edge caching strategy for
edge–fog–cloud environment. <em>COMCOM</em>, <em>228</em>, 107967. (<a
href="https://doi.org/10.1016/j.comcom.2024.107967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the critical challenges of content edge caching, such as limited storage capacity, content popularity prediction, dynamic user demand, and user privacy, issues that most existing studies only address partially. We present an innovative Genetic Algorithm-based On-demand Collaborative Edge Caching mechanism (GAOCEC), which introduces a multi-tiered caching architecture integrating cloud, fog, and edge computing. To enhance caching efficiency and minimize system cost, a novel on-demand caching quota mechanism is proposed that dynamically allocates cache resources to edge servers. To strengthen user privacy protection during content popularity prediction, a CNN-BiLSTM-based Federated Learning algorithm (CBFL) is presented that ensures high prediction accuracy without the need to upload local data to the cloud. We also refine the genetic algorithm for content placement by fine-tuning various parameter sets to identify the optimal balance between latency reduction and caching cost. Our experimental results validate the effectiveness of our approach, demonstrating increased cache hit rates, decreased content response times, and an overall improvement in system efficiency. This work provides a comprehensive, adaptive, and privacy-preserving solution for the edge–fog–cloud environment.},
  archive      = {J_COMCOM},
  author       = {Shimin Sun and Jinqi Dong and Ze Wang and Xiangyun Liu and Li Han},
  doi          = {10.1016/j.comcom.2024.107967},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107967},
  shortjournal = {Comput. Commun.},
  title        = {An on-demand collaborative edge caching strategy for edge–fog–cloud environment},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIPPLE-WiN: An efficient protocol for loop-free multipath
routing in wireless networks. <em>COMCOM</em>, <em>228</em>, 107966. (<a
href="https://doi.org/10.1016/j.comcom.2024.107966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new routing protocol for wireless networks called RIPPLE-WiN (Routing Information Protocol with Probing for Looplessness and Efficiency in Wireless Networks). RIPPLE-WiN is designed to provide loop-free multipath routing in wireless networks. RIPPLE-WiN accomplishes loop-free routing by replacing destination sequence numbers used in such routing protocols as AODV and DSDV with hop references. Simulation experiments based on the ns3 simulator are used to illustrate that RIPPLE-WiN is far more efficient that OLSR, DSDV and AODV. The simulation results show that RIPPLE-WiN attains better packet delivery rates and delays than the other routing protocols in the presence of failures and mobility while incurring less signaling overhead.},
  archive      = {J_COMCOM},
  author       = {J.J. Garcia-Luna-Aceves and Dylan Cirimelli-Low},
  doi          = {10.1016/j.comcom.2024.107966},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107966},
  shortjournal = {Comput. Commun.},
  title        = {RIPPLE-WiN: An efficient protocol for loop-free multipath routing in wireless networks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NFTs for accessing, monetizing, and teleporting digital
twins and digital artifacts in the metaverse. <em>COMCOM</em>,
<em>228</em>, 107965. (<a
href="https://doi.org/10.1016/j.comcom.2024.107965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twins and digital artifacts have become integral components of metaverse platforms, providing users with a rich, immersive, and interactive digital experience through the deployment of diverse digital twins and digital artifacts such as 3D avatars, images, and objects. To date, a significant challenge persists in the lack of practical mechanisms to enable seamless teleportation and cross-metaverse interoperability for these digital twins and digital artifacts. There is also a lack of trusted monetization methods that facilitate trading and leasing of digital twins and digital artifacts. To address these important challenges, this paper proposes a blockchain and Non-Fungible Token (NFT)-based solution that facilitates the integration and teleportation of these digital twins and digital artifacts by providing trusted metadata, verifying ownership, and ensuring the authenticity of digital creations in the virtual world. Key to our solution is the introduction of a bridging mechanism that enables cross-metaverse interoperability, allowing for the portable transfer of NFTs across decentralized metaverse platforms. In addition, our solution focuses on empowering original digital creators by enabling the monetization of their creations through the ownership management capabilities offered by NFTs. To reliably and securely store the metadata and content of tokenized digital twins and digital artifacts, we integrate into our solution the Interplanetary File System (IPFS), a decentralized storage system. To demonstrate the feasibility of our solution, we have developed and deployed all necessary smart contracts that govern the main functionalities and interactions of the proposed system on the Ethereum Goerli Testnet. We present our proposed system architecture, accompanied by informative sequence diagrams, algorithms, and testing details. We discuss how our proposed solution attains the main objectives outlined in the paper. We evaluate our proposed solution in terms of cost and security. We have made the complete source code of our smart contracts publicly available on GitHub.},
  archive      = {J_COMCOM},
  author       = {Senay A. Gebreab and Ahmad Musamih and Haya R. Hasan and Khaled Salah and Raja Jayaraman and Yousof Al Hammadi and Mohammed Omar},
  doi          = {10.1016/j.comcom.2024.107965},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107965},
  shortjournal = {Comput. Commun.},
  title        = {NFTs for accessing, monetizing, and teleporting digital twins and digital artifacts in the metaverse},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning: A cutting-edge survey of the latest
advancements and applications. <em>COMCOM</em>, <em>228</em>, 107964.
(<a href="https://doi.org/10.1016/j.comcom.2024.107964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust machine learning (ML) models can be developed by leveraging large volumes of data and distributing the computational tasks across numerous devices or servers. Federated learning (FL) is a technique in the realm of ML that facilitates this goal by utilizing cloud infrastructure to enable collaborative model training among a network of decentralized devices. Beyond distributing the computational load, FL targets the resolution of privacy issues and the reduction of communication costs simultaneously. To protect user privacy, FL requires users to send model updates rather than transmitting large quantities of raw and potentially confidential data. Specifically, individuals train ML models locally using their own data and then upload the results in the form of weights and gradients to the cloud for aggregation into the global model. This strategy is also advantageous in environments with limited bandwidth or high communication costs, as it prevents the transmission of large data volumes. With the increasing volume of data and rising privacy concerns, alongside the emergence of large-scale ML models like Large Language Models (LLMs), FL presents itself as a timely and relevant solution. It is therefore essential to review current FL algorithms to guide future research that meets the rapidly evolving ML demands. This survey provides a comprehensive analysis and comparison of the most recent FL algorithms, evaluating them on various fronts including mathematical frameworks, privacy protection, resource allocation, and applications. Beyond summarizing existing FL methods, this survey identifies potential gaps, open areas, and future challenges based on the performance reports and algorithms used in recent studies. This survey enables researchers to readily identify existing limitations in the FL field for further exploration.},
  archive      = {J_COMCOM},
  author       = {Azim Akhtarshenas and Mohammad Ali Vahedifar and Navid Ayoobi and Behrouz Maham and Tohid Alizadeh and Sina Ebrahimi and David López-Pérez},
  doi          = {10.1016/j.comcom.2024.107964},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107964},
  shortjournal = {Comput. Commun.},
  title        = {Federated learning: A cutting-edge survey of the latest advancements and applications},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtualized network functions resource allocation in network
functions virtualization using mathematical programming.
<em>COMCOM</em>, <em>228</em>, 107963. (<a
href="https://doi.org/10.1016/j.comcom.2024.107963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Functions Virtualization (NFV) revolutionizes network services by eliminating the need for dedicated hardware. This virtualization enables flexible and efficient deployment of various network functions like proxies, firewalls, and load balancers. Providing the service requested by the user in the network is done by a sequence of virtual network functions, which are known as service functions chain. One of the main challenges in the development of network functions virtualization architecture is the allocation of resources to the requested network services in network infrastructures, this challenge is called network function virtualization resource allocation problem. Therefore, this paper addresses the resource allocation problem in Network Functions Virtualization (NFV) architectures using mathematical programming techniques. A multi-objective mixed-integer linear programming (MILP) model is proposed to optimize resource allocation for virtual network functions (VNFs). The model incorporates constraints related to node and link resource capacities, as well as delay requirements. The objective functions focus on maximizing network throughput, minimizing node resource costs (CPU cores and memory), reducing capital and operational expenses, and ensuring efficient execution time. These constraints and objective functions are formally defined by mathematical functions. The proposed mathematical model is implemented and solved using the Cplex solver. To evaluate the effectiveness of the proposed mathematical model, various network topologies were evaluated under different parameters. These parameters included the length of Service Function Chains (SFCs), the number and length of flows, node resource capacities, the number of nodes and VNFs. The experimental results demonstrated the model’s ability to efficiently allocate resources to VNFs across these different scenarios.},
  archive      = {J_COMCOM},
  author       = {Mahsa Moradi and Mahmood Ahmadi and Latif PourKarimi},
  doi          = {10.1016/j.comcom.2024.107963},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107963},
  shortjournal = {Comput. Commun.},
  title        = {Virtualized network functions resource allocation in network functions virtualization using mathematical programming},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing point-of-sale services in MEC enabled near field
wireless communications using multi-agent reinforcement learning.
<em>COMCOM</em>, <em>228</em>, 107962. (<a
href="https://doi.org/10.1016/j.comcom.2024.107962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the next-generation communication system, near-field communication (NFC) is a key enabler of contactless transactions, including mobile payments, ticketing, and access control. With the growing demand for contactless solutions, NFC technology will play a pivotal role in enabling secure and convenient payment experiences across various sectors. In contrast, Internet of Things (IoT) devices such as phones’ Point of Sale (PoS) constitute limited battery life and finite computational resources that act as a bottleneck to doing the authentication in a minimal amount of time. Because of this, it garnered considerable attention in both academic and industrial realms. To overcome this, in this work we consider the Multiple Mobile Edge Computing (MEC) as an effective solution that provides extensive computation to PoS connected to it. To address the above, this work considers the PoS-enabled multi-MEC network to guarantee NFC communication reliably and effectively. For this, we formulate the joint optimization problem to maximize the probability of successful authentication while minimizing the queueing delay by jointly optimizing the computation and communication resources by utilizing a multi-agent reinforcement learning optimization approach. Through extensive simulations based on real-world scenarios, the effectiveness of the proposed approach was demonstrated. The results demonstrate that adjusting the complexity and learning rates of the model, coupled with strategic allocation of edge resources, significantly increased authentication success rates. Furthermore, the optimal allocation strategy was found to be crucial in reducing latency and improving authentication success by approximately 9.75%, surpassing other approaches. This study highlights the importance of resource management in optimizing MEC systems, paving the way for advancements in establishing secure, efficient, and dependable systems within the Internet of Things framework.},
  archive      = {J_COMCOM},
  author       = {Ateeq Ur Rehman and Mashael Maashi and Jamal Alsamri and Hany Mahgoub and Randa Allafi and Ashit Kumar Dutta and Wali Ullah Khan and Ali Nauman},
  doi          = {10.1016/j.comcom.2024.107962},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107962},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing point-of-sale services in MEC enabled near field wireless communications using multi-agent reinforcement learning},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of SR-IOV in docker containers using RTT
measurements. <em>COMCOM</em>, <em>228</em>, 107961. (<a
href="https://doi.org/10.1016/j.comcom.2024.107961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing, a central pillar of modern IT infrastructure, faces constant challenges in provisioning and optimizing network performance, specifically regarding low-latency communication. This study investigates the impact of Single Root I/O Virtualization (SR-IOV) as a critical Quality of Service (QoS) enabler in virtualized environments. Data plane innovative technologies for virtual servers, especially SR-IOV technology, emerged as a promising solution adopted in data centers. When combined with Peripheral Component Interconnect (PCI) Passthrough in Docker environments, SR-IOV promises significant network performance gains. Our rigorous experimental methodology demonstrates that integrating SR-IOV reduces Round-Trip Time (RTT) latency by up to 15 times compared to the traditional Linux based Bridge configuration used in Docker, without significant additional costs. This research is particularly relevant for system administrators, data center professionals, and network traffic engineers, providing them valuable information into optimizing communication in cloud computing environments. By addressing this critical gap in knowledge, our study serves as a practical guide for the effective implementation these emerging technologies for network virtualization. In terms of practical applicability, the results raise valuable insights into the performance and implications of implementing SR-IOV and PCI Passthrough in a Docker environment. As a result, more informed decisions are tailored to the specific requirements of different usage scenarios.},
  archive      = {J_COMCOM},
  author       = {Assis T. de Oliveira Filho and Eduardo Freitas and Pedro R.X. do Carmo and Eduardo Souto and Judith Kelner and Djamel F.H. Sadok},
  doi          = {10.1016/j.comcom.2024.107961},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107961},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of SR-IOV in docker containers using RTT measurements},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-empowered receiver design of OFDM communication
systems. <em>COMCOM</em>, <em>228</em>, 107960. (<a
href="https://doi.org/10.1016/j.comcom.2024.107960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With deep learning, we perform channel estimation and signal detection in massive Multiple Input Multiple Output (MIMO)-Orthogonal Frequency Division Multiplexing (OFDM) systems in this paper. Specifically, we design and extend the basic framework of receivers for MIMO-OFDM systems in an end-to-end approach. A Transformer-based MIMO-OFDM receiver called TCD-Receiver is proposed, which introduces a multi-attention mechanism to learn the channel characteristics by introducing a generic and flexible Transformer network structure. The network parameters are updated based on the relationship between the received signal and the original signal, where the final signal information is obtained without explicit channel estimation and the predicted transmit bits are directly output. The experimental results show that the TCD-Receiver proposed can effectively solve the channel distortion and detect the transmitted signals compared with the traditional communication receivers, and its performance can be comparable to that of the traditional OFDM receivers, and it also has obvious advantages in combating the complex and difficult-to-model channel environment as well as the nonlinear interference factors.},
  archive      = {J_COMCOM},
  author       = {Binglei Yue and Siyi Qiu and Chun Yang and Limei Peng and Yin Zhang},
  doi          = {10.1016/j.comcom.2024.107960},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107960},
  shortjournal = {Comput. Commun.},
  title        = {Transformer-empowered receiver design of OFDM communication systems},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A heterogeneous ring signcryption scheme with privacy
protection and conditional tracing for smart grid. <em>COMCOM</em>,
<em>228</em>, 107959. (<a
href="https://doi.org/10.1016/j.comcom.2024.107959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart grid develops rapidly, but there are still security risks such as user privacy leakage, power data tampering and audit data inconsistency. The existing schemes to ensure data security mainly use traceable ring signcryption, which is applied in distributed application scenarios such as smart grid. Traceable ring signcryption can ensure the anonymity, integrity, unforgeability and confidentiality of data, and can trace the real identity of anonymous users. However, the traceability of these schemes is arbitrary, any actor can trace the identity of anonymous users, and they do not resolve disputes caused by tampered or inconsistent data. To remedy these deficiencies, we combine ring signcryption with consortium blockchain technology for the first time to achieve privacy protection and conditional tracing, which can effectively avoid anonymous user identity being revealed at will. Consortium blockchain is a semi-distributed P2P network that can solve data disputes and is suitable for organizations that require certain access control mechanisms such as smart grid. In this paper, we propose a heterogeneous ring signcryption scheme with privacy protection and conditional tracing (CTHRSC) which between certificateless cryptographic system (CLC) and public key infrastructure (PKI). Besides, we prove that our scheme is secure under the discrete logarithm problem (DLP) and decisional Diffie–Hellman problem (DDHP) in random oracle model (ROM). Compared with other signature or signcryption schemes, our advantages are satisfying conditional tracing and known temporary session key security (KTSKS), requiring less computation cost and communication overhead.},
  archive      = {J_COMCOM},
  author       = {Xinhuang Zhou and Ming Luo and Minrong Qiu},
  doi          = {10.1016/j.comcom.2024.107959},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107959},
  shortjournal = {Comput. Commun.},
  title        = {A heterogeneous ring signcryption scheme with privacy protection and conditional tracing for smart grid},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of network topology changes on information source
localization. <em>COMCOM</em>, <em>228</em>, 107958. (<a
href="https://doi.org/10.1016/j.comcom.2024.107958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Well-established methods of locating the source of information in a complex network are usually derived with the assumption of complete and exact knowledge of network topology. We study the performance of three such algorithms (Limited Pinto–Thiran–Vetterli Algorithm — LPTVA, Gradient Maximum Likelihood Algorithm — GMLA and Pearson Correlation Algorithm — PCA) in scenarios that do not fulfill this assumption by modifying the network before localization. This is done by adding superfluous new links, hiding existing ones, or reattaching links following the network’s structural Hamiltonian. Our results show that GMLA is highly resilient to adding superfluous edges, as its precision falls by more than statistical uncertainty only when the number of links is approximately doubled. On the other hand, if the edge set is underestimated or reattachment has taken place, the performance of GMLA drops significantly. In such a scenario, PCA is preferable, retaining most of its performance when other simulation parameters favor successful localization (high density of observers, highly deterministic propagation). It is also generally more accurate than LPTVA and orders of magnitude faster. The differences between localization algorithms can be intuitively explained, although further theoretical research is needed.},
  archive      = {J_COMCOM},
  author       = {Piotr Machura and Robert Paluch},
  doi          = {10.1016/j.comcom.2024.107958},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107958},
  shortjournal = {Comput. Commun.},
  title        = {Impact of network topology changes on information source localization},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based reinforcement learning approach for federated
learning resource allocation and parameter optimization.
<em>COMCOM</em>, <em>228</em>, 107957. (<a
href="https://doi.org/10.1016/j.comcom.2024.107957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the performance of a model-based approach for solving resource allocation and parameter adjustment problems in federated learning (FL) within a wireless network. Given the existence of models for energy, communication channels, and accuracy, such models can be leveraged to achieve improved performance. Additionally, machine learning techniques can be employed to identify known parts of the model and also exploit training data for unknown parts of the model, enabling the creation of complex policies. Model-based reinforcement learning (RL) methods have the potential to offer such solutions, particularly in resource allocation and parameter optimization settings where the model can be partially derived mathematically. Our results demonstrate that the use of such a method in FL scenarios leads to improvements in both performance and the number of iterations required to identify the desired policy. Our simulations demonstrate the significance of allocating appropriate resources for FL applications through proper consideration of inherent tradeoffs, as performance will not improve beyond a certain saturation point. Additionally, our proposed FL model takes intelligently into account the presence of slow users to propose efficient policies for users that may have access to more abundant resources.},
  archive      = {J_COMCOM},
  author       = {Farzan Karami and Babak Hossein Khalaj},
  doi          = {10.1016/j.comcom.2024.107957},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107957},
  shortjournal = {Comput. Commun.},
  title        = {Model-based reinforcement learning approach for federated learning resource allocation and parameter optimization},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-based visibility prediction for terahertz
communications in 6G networks. <em>COMCOM</em>, <em>228</em>, 107956.
(<a href="https://doi.org/10.1016/j.comcom.2024.107956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terahertz communications are envisioned as a key enabler for 6G networks. The abundant spectrum available in such ultra high frequencies has the potential to increase network capacity to huge data rates. However, they are extremely affected by blockages, to the point of disrupting ongoing communications. In this paper, we elaborate on the relevance of predicting visibility between users and access points (APs) to improve the performance of THz-based networks by minimizing blockages, that is, maximizing network availability, while at the same time keeping a low reconfiguration overhead. We propose a novel approach to address this problem, by combining a neural network (NN) for predicting future user–AP visibility probability, with a probability threshold for AP reselection to avoid unnecessary reconfigurations. Our experimental results demonstrate that current state-of-the-art handover mechanisms based on received signal strength are not adequate for THz communications, since they are ill-suited to handle hard blockages. Our proposed NN-based solution significantly outperforms them, demonstrating the interest of our strategy as a research line.},
  archive      = {J_COMCOM},
  author       = {Pablo Fondo-Ferreiro and Cristina López-Bravo and Francisco Javier González-Castaño and Felipe Gil-Castiñeira and David Candal-Ventureira},
  doi          = {10.1016/j.comcom.2024.107956},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107956},
  shortjournal = {Comput. Commun.},
  title        = {Learning-based visibility prediction for terahertz communications in 6G networks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open RAN testbeds with controlled air mobility.
<em>COMCOM</em>, <em>228</em>, 107955. (<a
href="https://doi.org/10.1016/j.comcom.2024.107955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With its promise of increasing softwarization, improving disaggregability, and creating an open-source based ecosystem in the area of Radio Access Networks, the idea of Open RAN has generated rising interest in the community. Even as the community races to provide and verify complete Open RAN systems, the importance of verification of systems based on Open RAN under real-world conditions has become clear, and testbed facilities for general use have been envisioned, in addition to private testing facilities. Aerial robots, including autonomous ones, are among the increasingly important and interesting clients of RAN systems, but also present a challenge for testbeds. Based on our experience in architecting and operating an advanced wireless testbed with aerial robots as a primary citizen, we present considerations relevant to the design of Open RAN testbeds, with particular attention to making such a testbed capable of controlled experimentation with aerial clients. We also present representative results from the NSF AERPAW testbed on Open RAN slicing, programmable vehicles, and programmable radios.},
  archive      = {J_COMCOM},
  author       = {Magreth Mushi and Yuchen Liu and Shreyas Sreenivasa and Ozgur Ozdemir and Ismail Guvenc and Mihail Sichitiu and Rudra Dutta and Russ Gyurek},
  doi          = {10.1016/j.comcom.2024.107955},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107955},
  shortjournal = {Comput. Commun.},
  title        = {Open RAN testbeds with controlled air mobility},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRL-assisted task offloading in enhanced time-expanded graph
(eTEG)-modeled aerial computing. <em>COMCOM</em>, <em>228</em>, 107954.
(<a href="https://doi.org/10.1016/j.comcom.2024.107954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space–air–ground integrated networks (SAGINs), categorized under aerial computing (AC), are emerging as a promising hierarchical platform designed to meet the seamless connectivity demands of the forthcoming 6G era. However, efficiently offloading ground tasks to space entities via SAGINs presents unprecedented challenges, primarily due to the mobility of these networks. In response, an enhanced time-expanded graph (eTEG) is proposed to model the dynamic distribution of heterogeneous SAGIN resources, including transmission bandwidth, computation, and storage, thereby optimizing task offloading and resource allocation by employing eTEG. Specifically, this optimization challenge is addressed using a deep reinforcement learning (DRL) approach, aimed at streamlining decision-making for task offloading and resource management to significantly reduce end-to-end delay and enhance network performance. Simulation experiments conducted to evaluate the proposed DRL-based method demonstrate its effectiveness in reducing energy consumption and improving stability, thereby outperforming other methods by achieving reduced delays and satisfying user requirements.},
  archive      = {J_COMCOM},
  author       = {Jiang Mo and Ke Zhao and Limei Peng and Jiyeon Lee and Li Ma and Lixin Pu and Jipeng Fan},
  doi          = {10.1016/j.comcom.2024.107954},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107954},
  shortjournal = {Comput. Commun.},
  title        = {DRL-assisted task offloading in enhanced time-expanded graph (eTEG)-modeled aerial computing},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource allocation in RISs-assisted UAV-enabled MEC network
with computation capacity improvement. <em>COMCOM</em>, <em>228</em>,
107953. (<a href="https://doi.org/10.1016/j.comcom.2024.107953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-enabled mobile edge computing (MEC) networks have recently been considered to be a support for ground MEC networks to enhance their computation capability. However, the line-of-sight (LOS) channels between the UAV and Internet of Things (IoT) devices can be interfered by various obstacles, such as trees and buildings, resulting in a considerable reduction in the capacity of MEC networks. To solve this issue, a system that combines multiple reconfigurable intelligence surfaces (RISs) with a UAV-enabled MEC network is proposed in this study. A UAV equipped with edge servers is treated as an aerial computing platform for IoT devices, and multi-RISs are utilized to simultaneously improve the communication quality between enhanced UAV and IoT devices. To maximize the sum computation bits of the system, a problem that jointly optimizes the time slot partition, local computation frequency, transmit power of the devices, UAV receive beamforming vectors, phase shifts of the RISs, and the trajectory of the UAV is formulated. The problem is a typical nonconvex optimization problem; therefore, we propose a recursive algorithm based on the successive convex approximation (SCA) and block coordinate descent (BCD) technology to find an approximate optimal solution. Simulation results demonstrate the effectiveness of the proposed algorithm compared with various benchmark schemes.},
  archive      = {J_COMCOM},
  author       = {Long Jiao and Ling Gao and Jie Zheng and Peiqing Yang and Wei Xue},
  doi          = {10.1016/j.comcom.2024.107953},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107953},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation in RISs-assisted UAV-enabled MEC network with computation capacity improvement},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On construction of quality virtual backbone in wireless
networks using cooperative communication. <em>COMCOM</em>, <em>228</em>,
107952. (<a href="https://doi.org/10.1016/j.comcom.2024.107952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An extended connected dominating set (ECDS) in a wireless network with cooperative communication (CC) is a subset of nodes such that its induced subgraph is connected and each node outside the ECDS is covered by either one neighbor or several quasineighbors in the ECDS. Traditionality, the size of virtual backbone (VB) is the only factor considered in the problem of CDS construction. However, diameter is also an important factor to evaluate VB. In this paper we consider the problem of constructing quality ECDSs in unit disk graphs under CC with both of these two factors. We propose a two-phase centralized algorithm BD-ECDS to construct an ECDS for a given UDG with CC, which has a constant performance ratio (PR) and diameter. To obtain the PR of this two-phase centralized algorithm, we first give an upper bound of the EDS and use this upper bound to prove that the size of the ECDS under CC generated by the centralized algorithm is no greater than 120 | E C D S o p t | − 2 120|ECDSopt|−2 , where E C D S o p t ECDSopt is the size of the minimum ECDS. Furthermore, our theoretical analysis and simulation results show that our algorithm BD-ECDS is superior to previous approaches.},
  archive      = {J_COMCOM},
  author       = {Junhao Wang and Jiarong Liang and Qingnian Li},
  doi          = {10.1016/j.comcom.2024.107952},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107952},
  shortjournal = {Comput. Commun.},
  title        = {On construction of quality virtual backbone in wireless networks using cooperative communication},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Malware containment with immediate response in IoT networks:
An optimal control approach. <em>COMCOM</em>, <em>228</em>, 107951. (<a
href="https://doi.org/10.1016/j.comcom.2024.107951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of Internet of Things (IoT) devices has triggered a substantial increase in cyber-attacks targeting these systems. Recent statistics show a surge of over 100 percent in such attacks, underscoring the urgent need for robust cybersecurity measures. When a cyber-attack breaches an IoT network, it initiates the dissemination of malware across the network. However, to counteract this threat, an immediate installation of a new patch becomes imperative. The time frame for developing and deploying the patch can vary significantly, contingent upon the specifics of the cyber-attack. This paper aims to address the challenge of pre-emptively mitigating cyber-attacks prior to the installation of a new patch. The main novelties of our work include: (1) A well-designed node-level model known as Susceptible, Infected High, Infected Low, Recover First, and Recover Complete ( SI H I L R F R C ) (SIHILRFRC) . It categorizes the infected node states into infected high and infected low, according to the categorization of infection states for IoT devices, to accelerate containment strategies for malware propagation and improve mitigation of cyber-attacks targeting IoT networks by incorporating immediate response within a restricted environment. (2) Development of an optimal immediate response strategy (IRS) by modeling and analyzing the associated optimal control problem. This model aims to enhance the containment of malware propagation across IoT networks by swiftly responding to cyber threats. Finally, several numerical analyses were performed to fully illustrate the main findings. In addition, a dataset has been constructed for experimental purposes to simulate real-world scenarios within IoT networks, particularly in smart home environments.},
  archive      = {J_COMCOM},
  author       = {Mousa Tayseer Jafar and Lu-Xing Yang and Gang Li and Qingyi Zhu and Chenquan Gan and Xiaofan Yang},
  doi          = {10.1016/j.comcom.2024.107951},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107951},
  shortjournal = {Comput. Commun.},
  title        = {Malware containment with immediate response in IoT networks: An optimal control approach},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting and mitigating cyber threats through data mining
and machine learning. <em>COMCOM</em>, <em>228</em>, 107949. (<a
href="https://doi.org/10.1016/j.comcom.2024.107949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With cyber threats evolving alongside technological progress, strengthening network resilience to combat security vulnerabilities is crucial. This research extends cyber-crime analysis with an innovative approach, utilizing data mining and machine learning to not only predict cyber incidents but also reinforce network robustness. We introduce a real-time data collection framework to provide up-to-date cyberattack data, addressing current research limitations. By analyzing collected attack data, we identified temporal correlations in attack volumes across consecutive time frames. Our predictive model, developed using advanced machine learning and deep learning techniques, forecasts the frequency of cyber-attacks within specific time windows, demonstrating over a 15% improvement in accuracy compared to conventional baseline models. The methodologies employed include the use of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) for capturing complex patterns in time series data, and the integration of a sliding window technique to transform raw data into a format suitable for supervised learning. Our experiments evaluated the performance of various models, including ARIMA, Random Forest, Support Vector Regression, and K-Nearest Neighbors Regression, across multiple scenarios. Furthermore, we developed a Power BI platform for visualizing global cyber-attack trends, providing valuable insights for enhancing cybersecurity defences. Our research demonstrates that cyber incidents are not entirely random, and advanced AI tools can significantly enhance cybersecurity defences by analyzing patterns and trends from previous instances. This comprehensive approach not only improves prediction accuracy but also offers a robust framework for reducing the risk and impact of future cyber-crimes through enhanced detection and prediction capabilities.},
  archive      = {J_COMCOM},
  author       = {Nusrat Samia and Sajal Saha and Anwar Haque},
  doi          = {10.1016/j.comcom.2024.107949},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107949},
  shortjournal = {Comput. Commun.},
  title        = {Predicting and mitigating cyber threats through data mining and machine learning},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative IoT learning with secure peer-to-peer
federated approach. <em>COMCOM</em>, <em>228</em>, 107948. (<a
href="https://doi.org/10.1016/j.comcom.2024.107948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a powerful model for training collaborative machine learning (ML) models while maintaining the privacy of participants’ data. However, traditional FL methods can exhibit limitations such as increased communication overhead, vulnerability to poisoning attacks, and reliance on a central server, which can impede their practicality in certain IoT applications. In such cases, the necessity of a central server to oversee the learning process may be infeasible, particularly in situations with limited connectivity and energy management. To address these challenges, peer-to-peer FL (P2PFL) offers an alternative approach, providing greater adaptability by enabling participants to collaboratively train their own models alongside their peers. This paper introduces an original framework that combines P2PFL and Homomorphic Encryption (HE), enabling secure computations on encrypted data. Furthermore, a defense approach against poisoning attacks based on cosine similarity is introduced These techniques enable users to collectively learn while preserving data privacy and accounting for ideal energy optimization. The proposed approach has demonstrated superior performance metrics in terms of accuracy, F-scores, and loss when compared to other similar approaches. Furthermore, it offers robust privacy and security measures, leading to an enhanced security level, with improvements ranging from 5.5% to 14.6%. Moreover, we demonstrate that the proposed approach effectively reduces the communication overhead. The proposed approach resulted in impressive reductions in communication overhead ranging from 63.8% to 79.6%. The implementation of these security models was cumbersome, but we have made the code publicly available for your reference 1 .},
  archive      = {J_COMCOM},
  author       = {Neveen Mohammad Hijazi and Moayad Aloqaily and Mohsen Guizani},
  doi          = {10.1016/j.comcom.2024.107948},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107948},
  shortjournal = {Comput. Commun.},
  title        = {Collaborative IoT learning with secure peer-to-peer federated approach},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel OTFS-based directional transmission scheme for
airborne networks with ISAC technology. <em>COMCOM</em>, <em>228</em>,
107941. (<a href="https://doi.org/10.1016/j.comcom.2024.107941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the directional transmission scheme for airborne networks (ANs) with orthogonal time frequency space (OTFS) modulation, to cope with the degradations of communication performance due to the aircraft’s uncertain and high-speed mobilities. Besides showing better communication performance in high mobility scenarios such as AN, OTFS has also been verified for realizing the integrated sensing and communication (ISAC) systems. In this case, this paper proposes a sensing-assisted beam prediction method by exploiting echoes to predict the next locations of moving aircraft, solving the beam rendezvous problem at the transmitter. Besides, for the data detection problem at the receiver, this paper proposes a novel pilot placement scheme relying on the predicted delays and Dopplers, realizing accurate channel estimation with lower overheads. Simulation results show that the proposed OTFS-based directional transmission scheme can achieve reliable communication performance with a low bit error rate.},
  archive      = {J_COMCOM},
  author       = {Xinyu Hong and Na Lv and Xiang Wang and Zhiyuan You},
  doi          = {10.1016/j.comcom.2024.107941},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107941},
  shortjournal = {Comput. Commun.},
  title        = {A novel OTFS-based directional transmission scheme for airborne networks with ISAC technology},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing MRAI on large scale BGP networks: An
emulation-based approach. <em>COMCOM</em>, <em>228</em>, 107940. (<a
href="https://doi.org/10.1016/j.comcom.2024.107940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modifying protocols that pertain to global Internet control is extremely challenging, because experimentation is almost impossible and both analytic and simulation models are not detailed and accurate enough to guarantee that changes will not affect negatively the Internet. Federated testbeds like the ones offered by the Fed4FIRE+ project offer a different solution: off-line Internet-scale experiments with thousands of Autonomous Systems (ASs). This work exploits Fed4FIRE+ for a large-scale experimental analysis of Border Gateway Protocol (BGP) convergence time under different hypotheses of Minimum Route Advertisement Interval (MRAI) setting, including an original proposal to improve its management by dynamically setting MRAI based on the topological position of the ASs in relation to the specific route being advertised with the UPDATE messages. MRAI is a timer that regulates the frequency of successive UPDATE messages sent by a BGP router to a specific peer for a given destination. Its large default value significantly slows down convergence after path changes, but its uncoordinated reduction can trigger storms of UPDATE messages, and set off unstable behaviors known as route flapping. The work is based on standard-compliant modifications of the BIRD BGP daemon and shows the tradeoffs between convergence time and signaling overhead with different management techniques.},
  archive      = {J_COMCOM},
  author       = {Mattia Milani and Michele Segata and Luca Baldesi and Marco Nesler and Renato Lo Cigno and Leonardo Maccari},
  doi          = {10.1016/j.comcom.2024.107940},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107940},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing MRAI on large scale BGP networks: An emulation-based approach},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a novel service broker policy for choosing the
appropriate data center in cloud environments. <em>COMCOM</em>,
<em>228</em>, 107939. (<a
href="https://doi.org/10.1016/j.comcom.2024.107939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing cloud computing services leads to quick access of users to dynamic and distributed resources. Increasing demand has created challenges such as resource availability, privacy, and security to provide efficient services in cloud computing. Cloud environments contain various computing resources, and allocating a suitable node to process a request can improve the quality of service on a large scale. Load balancing is one of the strategies to improve service quality and resource utilization, which refers to the distribution of load among different nodes in a distributed system. The cloud application service broker is responsible for load balancing by choosing the appropriate geo-distributed datacenter to process the requests of each end user. Parameters such as transmission delay, network delay, processing time, number of servers, workload, and service cost can be considered to select a suitable datacenter in close proximity. To reduce the adverse effects of choosing a datacenter by a service broker, this paper presents Rank-based Load Balancing in Geo-Distributed datacenters (RLBGD) as an effective service broker strategy in cloud environments. RLBGD uses a weighted combination of several criteria such as processing time, number of servers, workload, processing speed, service cost, and response time for dynamic ranking and determining the appropriate datacenter. CloudAnalyst tool is used to simulate and analyze the performance of the proposed method. The results of experiments show the effectiveness of RLBGD in terms of metrics such as service cost and processing time in different scenarios.},
  archive      = {J_COMCOM},
  author       = {Lin Shan and Li Sun and Amin Rezaeipanah},
  doi          = {10.1016/j.comcom.2024.107939},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107939},
  shortjournal = {Comput. Commun.},
  title        = {Towards a novel service broker policy for choosing the appropriate data center in cloud environments},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust speech command recognition in challenging industrial
environments. <em>COMCOM</em>, <em>228</em>, 107938. (<a
href="https://doi.org/10.1016/j.comcom.2024.107938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech is among the main forms of communication between humans and robots in industrial settings, being the most natural way for a human worker to issue commands. However, the presence of pervasive and loud environmental noise poses significant challenges to the adoption of Speech-Command Recognition systems onboard manufacturing robots; indeed, they are expected to perform in real time on hardware with limited computational capabilities and also to be robust and accurate in such complex environments. In this paper, we propose an innovative system based on an End-to-End architecture with a Conformer backbone. Our system is specifically designed to achieve high accuracy in noisy industrial environments and to guarantee a minimal computational burden to meet stringent real-time requirements while running on computing devices that are embedded in robots. In order to increase the generalization capability of the system, the training procedure is driven by a Curriculum Learning strategy combined with dynamic data augmentation techniques, that progressively increase the complexity of input samples by increasing the noise during the training phase. We have conducted extensive experimentation to assess the effectiveness of our system, using a dataset composed of more than 50,000 samples, of which about 2,000 have been acquired during the daily operations of a Stellantis Italian factory. The results confirm the suitability of the proposed approach to be adopted in a real industrial environment; indeed, it is able to achieve, on both English and Italian commands, an accuracy higher than 90%, maintaining a compact model size (the network is 1.81 M B MB ) and running in real-time on an industrial embedded device (namely 41 ms 41ms over an NVIDIA Xavier NX).},
  archive      = {J_COMCOM},
  author       = {Stefano Bini and Vincenzo Carletti and Alessia Saggese and Mario Vento},
  doi          = {10.1016/j.comcom.2024.107938},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107938},
  shortjournal = {Comput. Commun.},
  title        = {Robust speech command recognition in challenging industrial environments},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning-accelerated network slice management for
next generation services. <em>COMCOM</em>, <em>228</em>, 107937. (<a
href="https://doi.org/10.1016/j.comcom.2024.107937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current trend in user services places an ever-growing demand for higher data rates, near-real-time latencies, and near-perfect quality of service. To meet such demands, fundamental changes were made to the traditional radio access network (RAN), introducing Open RAN (O-RAN). This new paradigm is based on a virtualized and intelligent RAN architecture. However, with the increased complexity of 5G applications, traditional application-specific placement techniques have reached a bottleneck. Our paper presents a Transfer Learning (TL) augmented Reinforcement Learning (RL) based networking slicing (NS) solution targeting more effective placement and improving downtime for prolonged slice deployments. To achieve this, we propose an approach based on creating a robust and dynamic repository of specialized RL agents and network slices geared towards popular user service types such as eMBB, URLLC, and mMTC. The proposed solution consists of a heuristic-controlled two-module-based ML Engine and a repository. The objective function is formulated to minimize the downtime incurred by the VNFs hosted on the commercial-off-the-shelf (COTS) servers. The performance of the proposed system is evaluated compared to traditional approaches using industry-standard 5G traffic datasets. The evaluation results show that the proposed solution consistently achieves lower downtime than the traditional algorithms.},
  archive      = {J_COMCOM},
  author       = {Sam Aleyadeh and Ibrahim Tamim and Abdallah Shami},
  doi          = {10.1016/j.comcom.2024.107937},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107937},
  shortjournal = {Comput. Commun.},
  title        = {Transfer learning-accelerated network slice management for next generation services},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial imitation learning assisted virtual
network embedding algorithm for space-air-ground integrated network.
<em>COMCOM</em>, <em>228</em>, 107936. (<a
href="https://doi.org/10.1016/j.comcom.2024.107936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The space-air-ground integrated network (SAGIN) comprises a multitude of interconnected and integrated heterogeneous networks. Its network is large in scale, complex in structure, and highly dynamic. Virtual network embedding (VNE) is designed to efficiently allocate resources within the physical host to diverse virtual network requests (VNRs) with different constraints while improving the acceptance ratio of VNRs. However, in a heterogeneous SAGIN environment, improving the utilization of network resources while ensuring the performance of the VNE algorithm is a very challenging topic. To address the aforementioned issues, we first introduce a services diversion strategy (SDS) to select embedded nodes based on different service types and network state, thereby alleviating the uneven use of resources in different network domains. Subsequently, we propose a VNE algorithm (GAIL-VNE) based on generative adversarial imitation learning (GAIL). We construct a generator network based on the actor-critic architecture, which can generate the probability of physical nodes being embedded based on the observed network state. Secondly, we construct a discriminator network to distinguish between generator samples and expert samples, which aids in updating the generator network. After offline training, the generator and discriminator reach a Nash equilibrium through game confrontation. During the embedding process of VNRs, the output of the generator provides an effective basis for generating VNE solutions. Finally, we verify the effectiveness of this method through experiments involving offline training and online embedding.},
  archive      = {J_COMCOM},
  author       = {Peiying Zhang and Ziyu Xu and Neeraj Kumar and Jian Wang and Lizhuang Tan and Ahmad Almogren},
  doi          = {10.1016/j.comcom.2024.107936},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107936},
  shortjournal = {Comput. Commun.},
  title        = {Generative adversarial imitation learning assisted virtual network embedding algorithm for space-air-ground integrated network},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A certificateless designated verifier sanitizable signature
in e-health intelligent mobile communication system. <em>COMCOM</em>,
<em>228</em>, 107935. (<a
href="https://doi.org/10.1016/j.comcom.2024.107935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread use of mobile communication and smart devices in the medical field, mobile healthcare has gained significant attention due to its ability to overcome geographical limitations and provide more efficient and high-quality medical services. In mobile healthcare, various instruments and wearable device data are collected, encrypted, and uploaded to the cloud, accessible to medical professionals, researchers, and insurance companies, among others. However, ensuring the security and privacy of healthcare data in the context of mobile networks has been a highly challenging issue. Certificateless signature schemes allow patients to conceal their respective privacy information for different sharing needs. Nevertheless, existing mobile healthcare data protection solutions suffer from costly certificate management and the inability to restrict signature verifiers. This paper proposes a certificateless designated verifier sanitizable signature for mobile healthcare scenarios, aiming to enhance the security and privacy of mobile healthcare data. This scheme enables the sanitization of sensitive data without the need for certificate management and allows for the specification of signature verifiers. This ensures the confidentiality of medical data, protects patient privacy, and prevents unauthorized access to healthcare data. Through security analysis and experimental comparisons, it is demonstrated that the proposed scheme is both efficient and effectively ensures data security and user privacy. Therefore, it is well-suited for privacy protection in mobile healthcare data.},
  archive      = {J_COMCOM},
  author       = {Yonghua Zhan and Yang Yang and Bixia Yi and Renjie He and Rui Shi and Xianghan Zheng},
  doi          = {10.1016/j.comcom.2024.107935},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107935},
  shortjournal = {Comput. Commun.},
  title        = {A certificateless designated verifier sanitizable signature in e-health intelligent mobile communication system},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locally verifiable approximate multi-member quantum
threshold aggregation digital signature scheme. <em>COMCOM</em>,
<em>228</em>, 107934. (<a
href="https://doi.org/10.1016/j.comcom.2024.107934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locally verifiable aggregate signature primitives can reduce the complexity of aggregate signature verification by computing locally open algorithms to generate auxiliary parameters. However, the breakthrough results of quantum computers at this stage indicate that it will be possible for quantum computers to break through the security of traditional hardness-based aggregated signature schemes. In order to solve the above problems, this paper proposes for the first time a new locally verifiable class of multi-member quantum threshold aggregated digital signature scheme based on the property that the verification of quantum coset states is a projection on the trans-subspace. Combined with the idea of auxiliary parameter generation in traditional locally verifiable aggregated signatures, it makes the current stage of threshold quantum digital signatures realize the aggregated features, and reduces the complexity of the verification of aggregated signatures while realizing post-quantum security. In addition, the verification of the signature key (quantum state) of the signature members does not require measurement operations, and the generated signatures are classical, so the communication between the trusted third center (TC), the set of signature members, the classical digital signature verifier (CV), and the third-party trusted aggregation generator (TA) are all classical, simplifying the communication model. In the performance analysis we make this quantum aggregation signature scheme more flexible as well as less quantum state preparation compared to other schemes.},
  archive      = {J_COMCOM},
  author       = {Zixuan Lu and Qingshui Xue and Tianhao Zhang and Jiewei Cai and Jing Han and Yixun He and Yinhang Li},
  doi          = {10.1016/j.comcom.2024.107934},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107934},
  shortjournal = {Comput. Commun.},
  title        = {Locally verifiable approximate multi-member quantum threshold aggregation digital signature scheme},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on maximizing real demand response based on link
addition in social networks. <em>COMCOM</em>, <em>228</em>, 107933. (<a
href="https://doi.org/10.1016/j.comcom.2024.107933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of social networks on real-life scenarios is intensifying with the diversification of the information they disseminate. Consequently, the interconnection between social networks and tangible networks is strengthening. Notably, we have observed that messages disseminated on social networks, particularly those soliciting aid, exert a significant influence on the underlying network structure. This study aims to investigate the role and importance of social networks in the information dissemination process, as well as to construct a linear threshold model tailored for the dissemination of emergency information across both real and social networks, leveraging conventional models of information spread. We have developed a model to increase the number of connection edges in social networks in order to enhance their worth. Additionally, we discovered that the objective function possesses submodular features and thus the created problem is NP-hard. As a result, we can use algorithms with approximative assurances of 1 − e − 1 − θ ′ 1−e−1−θ′ to solve our problem and ensures the accuracy of the solution. We also analyze the complexity of the algorithm in solving this problem. Finally we validated our conclusions with three publicly available datasets and one real data set to analysis the results of the solution.},
  archive      = {J_COMCOM},
  author       = {Yuxin Gao and Jianming Zhu and Peikun Ni},
  doi          = {10.1016/j.comcom.2024.107933},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107933},
  shortjournal = {Comput. Commun.},
  title        = {Research on maximizing real demand response based on link addition in social networks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noncoherent multiuser massive SIMO with mixed differential
and index modulation. <em>COMCOM</em>, <em>228</em>, 107931. (<a
href="https://doi.org/10.1016/j.comcom.2024.107931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new mixed differential and index modulation framework for noncoherent multiuser massive single-input multiple-output (SIMO) systems. While differential modulation and detection is a popular noncoherent scheme, its constellation collisions limit the resulting error performance. To address this issue, we introduce a user with binary index modulation (IM) among the differential users, achieving much reduced collisions. We then analyze a three-user SIMO system with binary modulations, attained a closed-form bit error rate (BER) expression with a fast noncoherent maximum-likelihood (ML) detection algorithm for each user. Furthermore, a closed-form optimal power loading vector is derived by minimizing the worst-case BER under individual power constraints. Finally, an efficient one-dimensional bisection search algorithm is employed to optimize constellations for arbitrary numbers of differential users and constellation sizes by minimizing the system BER. Simulation results validate the theoretical analysis and demonstrate the superiority of the proposed scheme compared to existing differential schemes.},
  archive      = {J_COMCOM},
  author       = {Xiangchuan Gao and Yancong Li and Zheng Dong and Xingwang Li},
  doi          = {10.1016/j.comcom.2024.107931},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107931},
  shortjournal = {Comput. Commun.},
  title        = {Noncoherent multiuser massive SIMO with mixed differential and index modulation},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative edge-caching based transmission with minimum
effective delay in heterogeneous cellular networks. <em>COMCOM</em>,
<em>228</em>, 107928. (<a
href="https://doi.org/10.1016/j.comcom.2024.107928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous cellular networks (HCNs), neighboring users often request similar contents asynchronously. Based on the content popularity, base stations (BSs) can download and cache contents when the network is idle, and transmit them locally when the network is busy, which can effectively reduce the backhaul burden and the transmission delay. We consider a two-tier HCN, where macro base stations (MBSs) and small base stations (SBSs) can cooperatively and probabilistically cache contents. Each user is associated to the BS with the maximum average received signal power in any tier. With the cooperative content transfer between MBS tier and SBS tier, users can adaptively obtain contents from BSs or remote content servers. We properly model both wired and wireless delays when a user requests an arbitrary content, and propose the concept of effective delay. Content caching probabilities are optimized using the Marine Predators Algorithm via minimizing the average effective delay. Numerical results show that our proposed cooperative caching scheme achieves much shorter delays than the benchmark caching schemes.},
  archive      = {J_COMCOM},
  author       = {Jiachao Yu and Chao Zhai and Hao Dai and Lina Zheng and Yujun Li},
  doi          = {10.1016/j.comcom.2024.107928},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107928},
  shortjournal = {Comput. Commun.},
  title        = {Cooperative edge-caching based transmission with minimum effective delay in heterogeneous cellular networks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DTL-5G: Deep transfer learning-based DDoS attack detection
in 5G and beyond networks. <em>COMCOM</em>, <em>228</em>, 107927. (<a
href="https://doi.org/10.1016/j.comcom.2024.107927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is considered as a key enabler for 5G and beyond mobile networks for supporting a variety of new services, including enhanced mobile broadband, ultra-reliable and low-latency communication, and massive connectivity, on the same physical infrastructure. However, this technology increases the susceptibility of networks to cyber threats, particularly Distributed Denial-of-Service (DDoS) attacks. These attacks have the potential to cause service quality degradation by overloading network function(s) that are central to network slices to operate seamlessly. This calls for an Intrusion Detection System (IDS) as a shield against a wide array of DDoS attacks. In this regard, one promising solution would be the use of Deep Learning (DL) models for detecting possible DDoS attacks, an approach that has already made its way into the field given its manifest effectiveness. However, one particular challenge with DL models is that they require large volumes of labeled data for efficient training, which are not readily available in operational networks. A possible workaround is to resort to Transfer Learning (TL) approaches that can utilize the knowledge learned from prior training to a target domain with limited labeled data. This paper investigates how Deep Transfer Learning (DTL) based approaches can improve the detection of DDoS attacks in 5G networks by leveraging DL models, such as Bidirectional Long Short-Term Memory (BiLSTM), Convolutional Neural Network (CNN), Residual Network (ResNet), and Inception as base models. A comprehensive dataset generated in our 5G network slicing testbed serves as the source dataset for DTL, which includes both benign and different types of DDoS attack traffic. After learning features, patterns, and representations from the source dataset using initial training, we fine-tune base models using a variety of TL processes on a target DDoS attack dataset. The 5G-NIDD dataset, which has a sparse amount of annotated traffic pertaining to several DDoS attack generated in a real 5G network, is chosen as the target dataset. The results show that the proposed DTL models have performance improvements in detecting different types of DDoS attacks in 5G-NIDD dataset compared to the case when no TL is applied. According to the results, the BiLSTM and Inception models being identified as the top-performing models. BiLSTM indicates an improvement of 13.90%, 21.48%, and 12.22% in terms of accuracy, recall, and F1-score, respectively, whereas, Inception demonstrates an enhancement of 10.09% in terms of precision, compared to the models that do not adopt TL.},
  archive      = {J_COMCOM},
  author       = {Behnam Farzaneh and Nashid Shahriar and Abu Hena Al Muktadir and Md. Shamim Towhid and Mohammad Sadegh Khosravani},
  doi          = {10.1016/j.comcom.2024.107927},
  journal      = {Computer Communications},
  month        = {12},
  pages        = {107927},
  shortjournal = {Comput. Commun.},
  title        = {DTL-5G: Deep transfer learning-based DDoS attack detection in 5G and beyond networks},
  volume       = {228},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ABBR: An augmented BBR for collaborative intelligent
transmission over heterogeneous networks in IIoT. <em>COMCOM</em>,
<em>226-227</em>, 107932. (<a
href="https://doi.org/10.1016/j.comcom.2024.107932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Industry 5.0, with the deep convergence of Industrial Internet of Things (IIoT) and 5G technology, stable transmission of massive data in heterogeneous networks becomes crucial. This is not only the key to improving the efficiency of human–machine collaboration, but also the basis for ensuring system continuity and reliability. The arrival of 5G has brought new challenges to the communication of IIoT in heterogeneous environments. Due to the inherent characteristics of wireless networks, such as random packet loss and network jitter, traditional transmission control schemes often fail to achieve optimal performance. In this paper we propose a novel transmission control algorithm, aBBR. It is an augmented algorithm based on BBRv3. aBBR dynamically adjusts the sending window size through real-time analysis to enhance the transmission performance in heterogeneous networks. Simulation results show that, compared to traditional algorithms, aBBR demonstrates the best comprehensive performance in terms of throughput, latency, and retransmission. When random packet loss exists in the link, aBBR improves the throughput by an average of 29.3% and decreases the retransmission rate by 18.5% while keeping the transmission delay at the same level as BBRv3.},
  archive      = {J_COMCOM},
  author       = {Shujie Yang and Kefei Song and Zhenhui Yuan and Lujie Zhong and Mu Wang and Xiang Ji and Changqiao Xu},
  doi          = {10.1016/j.comcom.2024.107932},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107932},
  shortjournal = {Comput. Commun.},
  title        = {ABBR: An augmented BBR for collaborative intelligent transmission over heterogeneous networks in IIoT},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 5G configured grant scheduling for seamless integration with
TSN industrial networks. <em>COMCOM</em>, <em>226-227</em>, 107930. (<a
href="https://doi.org/10.1016/j.comcom.2024.107930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of 5G (5th Generation) and TSN (Time Sensitive Networking) networks is key for the support of emerging Industry 4.0 applications, where the flexibility and adaptability of 5G will be combined with the deterministic communications features provided by TSN. For an effective and efficient 5G-TSN integration both networks need to be coordinated. However, 5G has not been designed to provide deterministic communications. In this context, this paper proposes a 5G configured grant scheduling scheme that coordinates its decision with the TSN scheduling to satisfy the deterministic and end-to-end latency requirements of industrial applications. The proposed scheme avoids scheduling conflicts that can happen when packets of different TSN flows are generated with different periodicities. The proposed scheme efficiently coordinates the access to the radio resources of different TSN flows and complies with the 3GPP (Third Generation Partnership Project) standard requirements.},
  archive      = {J_COMCOM},
  author       = {Ana Larrañaga-Zumeta and M. Carmen Lucas-Estañ and Javier Gozálvez and Aitor Arriola},
  doi          = {10.1016/j.comcom.2024.107930},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107930},
  shortjournal = {Comput. Commun.},
  title        = {5G configured grant scheduling for seamless integration with TSN industrial networks},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure workflow scheduling algorithm utilizing hybrid
optimization in mobile edge computing environments. <em>COMCOM</em>,
<em>226-227</em>, 107929. (<a
href="https://doi.org/10.1016/j.comcom.2024.107929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of mobile communication technology and devices has greatly improved our way of life. It also presents a new possibility that data sources can be used to accomplish computing tasks at nearby locations. Mobile Edge Computing (MEC) is a computing model that provides computer resources specifically designed to handle mobile tasks. Nevertheless, there are certain obstacles that must be carefully tackled, specifically regarding the security and quality of services in the workflow scheduling over MEC. This research proposes a new method called Feedback Artificial Remora Optimization (FARO)-based workflow scheduling method to address the issues of scheduling processes with improved security in MEC. In this context, the fitness functions that are taken into account include multi-objective, such as CPU utilization, memory utilization, encryption cost, and execution time. These functions are used to enhance the scheduling of workflow tasks based on security considerations. The FARO algorithm is a combination of the Feedback Artificial Tree (FAT) and the Remora Optimization Algorithm (ROA). The experimental findings have demonstrated that the developed approach surpassed current methods by a large margin in terms of CPU use, memory consumption, encryption cost, and execution time, with values of 0.012, 0.010, 0.017, and 0.036, respectively.},
  archive      = {J_COMCOM},
  author       = {Dileep Kumar Sajnani and Xiaoping Li and Abdul Rasheed Mahesar},
  doi          = {10.1016/j.comcom.2024.107929},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107929},
  shortjournal = {Comput. Commun.},
  title        = {Secure workflow scheduling algorithm utilizing hybrid optimization in mobile edge computing environments},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RIS-NOMA communications over nakagami-m fading with
imperfect successive interference cancellation. <em>COMCOM</em>,
<em>226-227</em>, 107926. (<a
href="https://doi.org/10.1016/j.comcom.2024.107926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering imperfect successive interference cancellation (SIC) for non-orthogonal multiple access (NOMA) communications, this work studies the cooperative reconfigurable intelligent surface (RIS)- and relay-assisted system under Nakagami- m fading. We focus on the performance comparison for such cooperative schemes, under different channel conditions and system parameters. In addition, we analyze the minimum required RIS elements number of the RIS-assisted scheme to achieve the same performance of the relay-assisted scheme with given signal-to-noise ratio (SNR). The cases of the optimal continuous phase shift and discrete phase shift designs of RIS are also discussed. We make a comparison between them, aiming to study the impact of the residual phase errors on performance. Specially, we compare the active RIS and passive RIS with the same system power constraint. Simulation results demonstrating the reliability of the analysis, validate that the relay-assisted scheme is superior to that of RIS-assisted one when the RIS elements number is small and transmitted power is lower. The results also confirm that the deployment of RIS should consider the actual situation of the application scenario.},
  archive      = {J_COMCOM},
  author       = {Jinyuan Gu and Mingxing Wang and Wei Duan and Lei Zhang and Huaiping Zhang},
  doi          = {10.1016/j.comcom.2024.107926},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107926},
  shortjournal = {Comput. Commun.},
  title        = {RIS-NOMA communications over nakagami-m fading with imperfect successive interference cancellation},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning-based resource scheduling for
energy optimization and load balancing in SDN-driven edge computing.
<em>COMCOM</em>, <em>226-227</em>, 107925. (<a
href="https://doi.org/10.1016/j.comcom.2024.107925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional techniques for edge computing resource scheduling may result in large amounts of wasted server resources and energy consumption; thus, exploring new approaches to achieve higher resource and energy efficiency is a new challenge. Deep reinforcement learning (DRL) offers a promising solution by balancing resource utilization, latency, and energy optimization. However, current methods often focus solely on energy optimization for offloading and computing tasks, neglecting the impact of server numbers and resource operation status on energy efficiency and load balancing. On the other hand, prioritizing latency optimization may result in resource imbalance and increased energy waste. To address these challenges, we propose a novel energy optimization method coupled with a load balancing strategy. Our approach aims to minimize overall energy consumption and achieve server load balancing under latency constraints. This is achieved by controlling the number of active servers and individual server load states through a two stage DRL-based energy and resource optimization algorithm. Experimental results demonstrate that our scheme can save an average of 19.84% energy compared to mainstream reinforcement learning methods and 49.60% and 45.33% compared to Round Robin (RR) and random scheduling, respectively. Additionally, our method is optimized for reward value, load balancing, runtime, and anti-interference capability.},
  archive      = {J_COMCOM},
  author       = {Xu Zhou and Jing Yang and Yijun Li and Shaobo Li and Zhidong Su},
  doi          = {10.1016/j.comcom.2024.107925},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107925},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning-based resource scheduling for energy optimization and load balancing in SDN-driven edge computing},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FQ-SAT: A fuzzy q-learning-based MPQUIC scheduler for data
transmission optimization. <em>COMCOM</em>, <em>226-227</em>, 107924.
(<a href="https://doi.org/10.1016/j.comcom.2024.107924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 5G and beyond era, multipath transport protocols, including MPQUIC, are necessary in various use cases. In MPQUIC, one of the most critical issues is efficiently scheduling the upcoming transmission packets on several paths considering path dynamicity. To this end, this paper introduces FQ-SAT - a novel Fuzzy Q-learning-based MPQUIC scheduler for data transmission optimization, including download time, in heterogeneous wireless networks. Different from previous works, FQ-SAT combines Q-learning and Fuzzy logic in an MPQUIC scheduler to determine optimal transmission on heterogeneous paths. FQ-SAT leverages the self-learning ability of reinforcement learning (i.e., in a Q-learning model) to deal with heterogeneity. Moreover, FQ-SAT facilitates Fuzzy logic to dynamically adjust the proposed Q-learning model’s hyper-parameters along with the networks’ rapid changes. We evaluate FQ-SAT extensively in various scenarios in both simulated and actual networks. The results show that FQ-SAT reduces the single-file download time by 3.2%–13.5% in simulation and by 4.1%–13.8% in actual network, reduces the download time of all resources up to 20.4% in web browsing evaluation, and reaches percentage of on-time segments up to 97.5% in video streaming, compared to state-of-the-art MPQUIC schedulers.},
  archive      = {J_COMCOM},
  author       = {Thanh Trung Nguyen and Minh Hai Vu and Thi Ha Ly Dinh and Thanh Hung Nguyen and Phi Le Nguyen and Kien Nguyen},
  doi          = {10.1016/j.comcom.2024.107924},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107924},
  shortjournal = {Comput. Commun.},
  title        = {FQ-SAT: A fuzzy Q-learning-based MPQUIC scheduler for data transmission optimization},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAV-AGV cooperated remote toxic gas sensing and automated
alarming scheme in smart factory. <em>COMCOM</em>, <em>226-227</em>,
107923. (<a href="https://doi.org/10.1016/j.comcom.2024.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces the innovative concepts of the cooperation between unmanned aerial vehicles (UAVs) and automated guided vehicles (AGVs) in remote toxic gas sensing and alarming schemes in a smart factory. Initially, the UAV is dispatched in different directions to detect toxic gas leakage on the fly in the smart factory premises. However, due to the UAVs’ concern about smokeless and high-density gas detection capabilities, AGVs are proposed to cooperate with UAVs in the smart factory, especially in the basement areas. Because of their limited computational power, UAVs and AGVs securely transfer sensor data to a nearby multi-access edge computing (MEC) server for processing. A hybrid cryptographic technique and unique data authentication mechanisms are exploited to ensure security while transmitting the data in this proposed scheme. Subsequently, the MEC server automatically triggers an emergency alarm during toxic gas leakage to alert all the employees inside the boundaries of the smart factory. The implementation results exhibit that the proposed scheme can successfully sense toxic gas leakage using UAVs and AGVs, securely transfer the data to the MEC server to process, and enhance the overall quality of service compared with the other existing literature. Finally, the outcome analysis demonstrates that the proposed scheme is more worthwhile and has distinctive features than other literary works.},
  archive      = {J_COMCOM},
  author       = {Md Masuduzzaman and Ramdhan Nugraha and Soo Young Shin},
  doi          = {10.1016/j.comcom.2024.08.005},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107923},
  shortjournal = {Comput. Commun.},
  title        = {UAV-AGV cooperated remote toxic gas sensing and automated alarming scheme in smart factory},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved dynamic byzantine fault tolerant consensus
mechanism. <em>COMCOM</em>, <em>226-227</em>, 107922. (<a
href="https://doi.org/10.1016/j.comcom.2024.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Byzantine Fault Tolerance (BFT) consensus protocols are widely used in consortium blockchain to ensure data consistency. However, BFT protocols are generally static which means that the dynamic joining and exiting of nodes will lead to the reconfiguration of the consortium blockchain system. Moreover, most BFT protocols cannot support the clearing operation of slow, crashed, or faulty nodes, which limits the application of consortium blockchain. In order to solve these problems, this paper proposes a new Dynamic Scalable BFT (D-SBFT) protocol. D-SBFT optimizes SBFT by using Distributed Key Generation (DKG) technology and BLS aggregate signature scheme. On the basis of SBFT, we add Join , Exit , and Clear algorithms. Among them, Join and Exit algorithms enable nodes to actively join and exit the consortium blockchain more flexibly. Clear can remove slow, crashed or faulty nodes from the consortium blockchain. Experimental results show that our D-SBFT protocol can efficiently implement node dynamic change while exhibiting good performance in consensus process.},
  archive      = {J_COMCOM},
  author       = {Fei Tang and Jinlan Peng and Ping Wang and Huihui Zhu and Tingxian Xu},
  doi          = {10.1016/j.comcom.2024.08.004},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107922},
  shortjournal = {Comput. Commun.},
  title        = {Improved dynamic byzantine fault tolerant consensus mechanism},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRTP: A generic differentiated reliable transport protocol.
<em>COMCOM</em>, <em>226-227</em>, 107921. (<a
href="https://doi.org/10.1016/j.comcom.2024.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With rapid development of network communications, the performance requirements of applications are getting more differentiated. Many applications like high-definition video transfer require high throughput but do tolerate occasional packet losses. Traditional generic transport protocols however, only provide inflexible data transfer guarantees (TCP (Transmission Control Protocol) offers full reliability guarantees and UDP (User Datagram Protocol) offers no guarantees). Moreover, TCP pays a significant price to ensure a full reliability guarantee over lossy wireless communications environment like 5G millimeter wave (mmWave) communications. While existing, “partially” reliable transport protocols are either specifically designed for certain applications or need router’s support. In this paper, we design a new generic Differentiated Reliable Transport Protocol (DRTP), aiming to provide a differentiated and deterministic reliability guarantee for upper layer applications while maximizing the throughput under the constraint of guaranteeing a required reliability of data transfer. DRTP is a generic and pure end-to-end partially reliable transport protocol, and as such is easy to deploy regardless of the application in use and with no need for router’s support. The performance of DRTP is evaluated under various network conditions using extensive NS-3 (Network Simulator) simulations and practical experiments over the mmWave communications environment. The results show much higher throughput compared to typical transport protocols while guaranteeing the required transfer reliability.},
  archive      = {J_COMCOM},
  author       = {Yongmao Ren and Yundong Zhang and Ming Yin and Anmin Xu and Xu Zhou and Cong Li and Yifang Qin and Qinghua Wu and Mohamed Ali Kaafar and Gaogang Xie},
  doi          = {10.1016/j.comcom.2024.08.003},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107921},
  shortjournal = {Comput. Commun.},
  title        = {DRTP: A generic differentiated reliable transport protocol},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the optimal design of fully identifiable next-generation
in-vehicle networks. <em>COMCOM</em>, <em>226-227</em>, 107920. (<a
href="https://doi.org/10.1016/j.comcom.2024.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the emerging advances in connected and autonomous vehicles, today’s in-vehicle networks, unlike traditional networks, are not only internally connected but externally as well, exposing the vehicle to the outside world and making it more vulnerable to cyber-security threats. Monitoring the in-vehicle network, thus, becomes one of the essential and crucial tasks to be implemented in vehicles. However, the closed-in nature of the vehicle’s components hinders the global monitoring of the in-vehicle network, leading to incomplete measurements, which may result in undetected failures. One solution to this is to use network tomography. Nevertheless, applying network tomography in in-vehicle networks is not a trivial task. Mainly because it requires that the in-vehicle network topology should be identifiable . To this end, we propose in this work an identifiable in-vehicle network topology that enables overall monitoring of the network using network tomography. The new topology is proposed based on extensive analysis to ensure full identifiability under the constraint that only edge nodes can monitor the network, which is the case for in-vehicle networks where internal nodes are not directly accessible. We propose two main algorithms to transform existing in-vehicle network topologies. The first algorithm applies to an existing topology which can be transformed into full identifiability by adding extra nodes/links. Evaluation results show the effectiveness of the proposed transformation algorithms with a maximum added weight of only 3% of the original weight. Furthermore, a new optimisation algorithm is also proposed to minimise the topology weight whilst maintaining the full identifiability by redesigning a new topology. With this algorithm, the results show that the total weight can be reduced by 6%. In addition, compared with the existing approaches, monitoring the in-vehicle networks with the proposed approach can achieve better monitoring overhead and a 100% identifiability ratio.},
  archive      = {J_COMCOM},
  author       = {Amani Ibraheem and Zhengguo Sheng and George Parisis},
  doi          = {10.1016/j.comcom.2024.08.002},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107920},
  shortjournal = {Comput. Commun.},
  title        = {On the optimal design of fully identifiable next-generation in-vehicle networks},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Experimental evaluation of interactive edge/cloud virtual
reality gaming over wi-fi using unity render streaming. <em>COMCOM</em>,
<em>226-227</em>, 107919. (<a
href="https://doi.org/10.1016/j.comcom.2024.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices. However, the quality of the VR experience heavily relies on Wireless Fidelity (Wi-Fi) performance, since it serves as the last hop in the network chain. Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a Wi-Fi simulator. In this work, we further evaluate Wi-Fi’s suitability for VR streaming in terms of the Quality of Service (QoS) it provides. In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using Web Real-Time Communication (WebRTC), considering a server physically located at the network’s edge, near the end user. Our findings demonstrate the system’s sustained network performance, showcasing minimal round-trip time (RTT) and jitter at 60 and 90 frames per second (fps). In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a distinctive video transmission approach inherent to WebRTC-based services: the systematic packetization of video frames (VFs) and their transmission in discrete batches at regular intervals, regardless of the targeted frame rate. This interval-based transmission strategy maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption. Our results demonstrate that shortening the interval between batches is advantageous, as it enhances Wi-Fi efficiency and reduces delays in delivering complete frames.},
  archive      = {J_COMCOM},
  author       = {Miguel Casasnovas and Costas Michaelides and Marc Carrascosa-Zamacois and Boris Bellalta},
  doi          = {10.1016/j.comcom.2024.08.001},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107919},
  shortjournal = {Comput. Commun.},
  title        = {Experimental evaluation of interactive Edge/Cloud virtual reality gaming over wi-fi using unity render streaming},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-powered migration of social digital twins at the
network edge. <em>COMCOM</em>, <em>226-227</em>, 107918. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Twins (DTs), which are paired to Internet of Things (IoT) devices to represent them and augment their capabilities, are gaining ground as a promising technology to enable a wide variety of applications in the sixth-generation (6G) ecosystem, ranging from autonomous driving to extended reality and metaverse. In particular, “social” IoT (SIoT) devices, which are devices capable to establish social relationships with other devices, can be coupled with their virtual counterparts, i.e., social DTS (SDTs), to improve service discovery enabled by browsing the social network of friend devices. However, the mobility of SIoT devices (e.g., smartphones, wearables, vehicular on board units, etc.) may require frequent changes in the corresponding SDT placement in the edge domain to maintain a low latency between the physical device and its digital replica. Triggering SDT relocation at the right time is a critical task, because an incorrect choice could lead to either increased delays or a waste of network resources. This work proposes a learning-powered social-aware orchestration that predicts the mobility of SIoT devices to make more judicious migration decisions and efficiently move the paired SDTs accordingly, while ensuring the minimization of both intra-twin and inter-twin communication latencies. Different machine learning (ML) and deep learning (DL) algorithms are used for SIoT device mobility prediction and compared in terms of a wide set of meaningful metrics in order to identify the model that achieves the best trade-off between prediction accuracy and inference times under different scenarios. Simulation results showcase the improvements of the proposal in terms of reduced network overhead (by up to a factor of 3) and intra-twin and inter-twin communication latency (by up to 10%) compared to a more traditional solution, which activates the relocation of the DTs at fixed time intervals following periodic optimizations.},
  archive      = {J_COMCOM},
  author       = {Olga Chukhno and Nadezhda Chukhno and Giuseppe Araniti and Claudia Campolo and Antonio Iera and Antonella Molinaro},
  doi          = {10.1016/j.comcom.2024.07.019},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107918},
  shortjournal = {Comput. Commun.},
  title        = {Learning-powered migration of social digital twins at the network edge},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-transfer-based path loss prediction in asymmetric
massive MIMO IoT systems. <em>COMCOM</em>, <em>226-227</em>, 107905. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymmetric massive multiple-input–multiple-output (MIMO) array improves system capacity and provides wide-area coverage for the Internet of Things (IoT). In this paper, we propose a novel attention-based model for path loss (PL) prediction in asymmetric massive MIMO IoT systems. To represent the propagation characteristics, the propagation image that considers the detailed environment, beamwidth pattern, and propagation-statistics feature is designed. Benefiting from the shuffle attention computation, the proposed model, termed a shuffle-attention-based convolutional neural network (SAN), can effectively extract the detailed features of the propagation scenario from the image. Besides, we design the beamwidth-scenario transfer learning (BWSTL) algorithm to assist the SAN model in predicting PL in the new asymmetric massive MIMO IoT systems, where the beamwidth configuration and propagation scenario are different. It is shown that the proposed model outperforms the empirical model and other state-of-the-art artificial intelligence-based models. Aided by the BWSTL algorithm, the SAN model can be transferred to new propagation conditions with limited samples, which is beneficial to the fast deployment in the new asymmetric massive MIMO IoT systems.},
  archive      = {J_COMCOM},
  author       = {Yan Zhang and Mingyu Chen and Meng Yuan and Wancheng Zhang and Luis A. Lago},
  doi          = {10.1016/j.comcom.2024.07.006},
  journal      = {Computer Communications},
  month        = {10},
  pages        = {107905},
  shortjournal = {Comput. Commun.},
  title        = {Attention-transfer-based path loss prediction in asymmetric massive MIMO IoT systems},
  volume       = {226-227},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy efficiency optimization for 6G multi-IRS multi-cell
NOMA vehicle-to-infrastructure communication networks. <em>COMCOM</em>,
<em>225</em>, 350–360. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent Reflecting Surfaces (IRS), software-controlled metasurfaces, have emerged as an upcoming sixth-generation (6G) wireless communication technology. IRS intelligently manipulates and optimizes signal propagation using a large-scale array of intelligent elements, enhancing signal coverage, increasing capacity, mitigating path loss, and combating multipath fading This work provides a new energy-efficiency model for multi-IRS-assisted multi-cell non-orthogonal multiple access (NOMA) vehicular to infrastructure communication networks. The objective is the joint optimization of the total power budget at the roadside unit (RSU), NOMA power allocation for the user equipment, and designing phase shifts for IRS in each cell to maximize the achievable energy efficiency of the system. Due to non-convexity, the original non-convex problem is first decoupled and transformed using block coordinate descent and successive convex approximation methods. Then, an efficient solution is achieved using Gradient-based and interior-point methods. We also consider two benchmark schemes: (1) NOMA power optimization at RSU with random phase shift design at IRS and (2) orthogonal multiple access power allocation with optimal phase shift design at IRS. Numerical results show the superiority of the proposed solution compared to the benchmark schemes. The proposed solution outperforms the benchmarks, demonstrating a 59.57% and 151.21% improvement over the NOMA and orthogonal schemes, respectively, at p c t = 2 pct=2 dBm. Additionally, it shows up to a 10.43% better performance than OMA at 10 IRS elements.},
  archive      = {J_COMCOM},
  author       = {Mashael Maashi and Eatedal Alabdulkreem and Noha Negm and Abdulbasit A. Darem and Mesfer Al Duhayyim and Ashit Kumar Dutta and Wali Ullah Khan and Ali Nauman},
  doi          = {10.1016/j.comcom.2024.07.018},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {350-360},
  shortjournal = {Comput. Commun.},
  title        = {Energy efficiency optimization for 6G multi-IRS multi-cell NOMA vehicle-to-infrastructure communication networks},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving WiFi fingerprint-based people counting
for crowd management. <em>COMCOM</em>, <em>225</em>, 339–349. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practice of people counting serves as an indispensable tool for meticulously monitoring crowd dynamics, enabling informed decision-making in critical situations, and optimizing the management of urban spaces, facilities, and services. Beyond its fundamental role in safety and security, tracking people’s flows has evolved into a necessity for diverse business applications and the effective administration of both outdoor and indoor urban environments. In the ongoing exploration of the study, emphasis is placed on employing a passive counting technique. This method leverages WiFi probe request messages emitted by smart devices to assess the number of devices, providing a reliable estimate of the number of people in a specific area. However, it is crucial to acknowledge the dynamic landscape of privacy regulations and the concerted efforts by leading smart-device manufacturers to fortify user privacy, as evidenced by the adoption of MAC address randomization. In response to these considerations, an enhanced iteration of the WiFi traffic generator has been introduced. This upgraded version is designed to generate realistic datasets with ground truth, aligning with the evolving privacy landscape. Additionally, leveraging a profound understanding of probe requests and the capabilities of the designed generator, a novel crowd monitoring solution that incorporates machine learning techniques, named ARGO , has been developed. This innovative approach effectively addresses challenges posed by randomized MAC addresses, incorporating Bloom filters to ensure a formal “deniability” that complies with stringent regulations, including the European GDPR (European Parliament, Council of the European Union, Regulation (EU), 2016). The proposed solution adeptly addresses the pivotal task of people counting by harnessing WiFi probe request messages. Significantly, it prioritizes users’ privacy, aligning with the foundational principles outlined in regulations such as the European GDPR.},
  archive      = {J_COMCOM},
  author       = {Riccardo Rusca and Diego Gasco and Claudio Casetti and Paolo Giaccone},
  doi          = {10.1016/j.comcom.2024.07.010},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {339-349},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving WiFi fingerprint-based people counting for crowd management},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning optimal edge processing with offloading and energy
harvesting. <em>COMCOM</em>, <em>225</em>, 324–338. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern portable devices can execute increasingly sophisticated AI models on sensed data. The complexity of such processing tasks is data-dependent and has relevant energy cost. This work develops an Age of Information Markovian model for a system where multiple battery-operated devices perform data processing and energy harvesting in parallel. Part of their computational burden is offloaded to an edge server which polls devices at given rate. The structural properties of an optimal policy for a single device-server system are derived. They permit to define a new model-free reinforcement learning method specialized for monotone policies, namely Ordered Q-Learning, providing a fast procedure to learn the optimal policy. The method is oblivious to the devices’ battery capacities, the cost and the value of data batch processing and to the dynamics of the energy harvesting process. Finally, the polling strategy of the server is optimized by combining this policy improvement technique with stochastic approximation methods. Extensive numerical results provide insight into the system properties and demonstrate that the proposed learning algorithms outperform existing baselines.},
  archive      = {J_COMCOM},
  author       = {Andrea Fox and Francesco De Pellegrini and Eitan Altman},
  doi          = {10.1016/j.comcom.2024.07.009},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {324-338},
  shortjournal = {Comput. Commun.},
  title        = {Learning optimal edge processing with offloading and energy harvesting},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid aggregation for federated learning under blockchain
framework. <em>COMCOM</em>, <em>225</em>, 311–323. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning based on local differential privacy and blockchain can effectively mitigate the privacy issues of server and provide strong privacy against multiple kinds of attack. However, the actual privacy of users gradually decreases with the frequency of user updates, and noises from perturbation cause contradictions between privacy and utility. To enhance user privacy while ensuring data utility, we propose a Hybrid Aggregation mechanism based on Shuffling, Subsampling and Shapley value (HASSS) for federated learning under blockchain framework. HASSS includes two procedures, private intra-local domain aggregation and efficient inter-local domain evaluation. During the private aggregation, the local updates of users are selected and randomized to achieve gradient index privacy and gradient privacy, and then are shuffled and subsampled by shufflers to achieve identity privacy and privacy amplification. During the efficient evaluation, local servers that aggregated updates within domains broadcast and receive updates from other local servers, based on which the contribution of each local server is calculated to select nodes for global update. Two comprehensive sets are applied to evaluate the performance of HASSS. Simulations show that our scheme can enhance user privacy while ensuring data utility.},
  archive      = {J_COMCOM},
  author       = {Xinjiao Li and Guowei Wu and Lin Yao and Shisong Geng},
  doi          = {10.1016/j.comcom.2024.06.009},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {311-323},
  shortjournal = {Comput. Commun.},
  title        = {Hybrid aggregation for federated learning under blockchain framework},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tethering layer 2 solutions to the blockchain: A survey on
proving schemes. <em>COMCOM</em>, <em>225</em>, 289–310. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A blockchain is a data structure consisting of a list of blocks containing transactions and maintained by a network of nodes in a decentralized manner. In permissionless blockchains, anyone can contribute to the decentralization and security of the transactions. With the advent of smart contracts, programs whose execution is replicated by all the nodes of the network, the blockchain can be deemed not only a reliable and auditable data repository, but also a secure and verifiable computational infrastructure. However, due to the aforementioned features, the throughput of most permissionless blockchains is low, and executing a smart contract can be expensive, depending on its computational complexity. To mitigate these issues, a popular research line studies the implementation of Layer 2 solutions, which consists of nodes that operate off-chain yet remaining tethered to the blockchain. Our literature analysis revealed that a majority of the research articles surveying Layer 2 technologies and solutions typically classify them on the basis of the Layer 2 operations they perform, as well as their ability to improve the processing capacity of the blockchain. In this paper, instead, we survey the methodologies that provide a secure binding between Layer 2 and the blockchain. We refer to these binding techniques as “proving schemes” which we classify as: data integrity proofs, validity proofs, and fraud proofs. For each proving scheme, we describe its intended purpose, the advantages it offers, the methodologies commonly used to connect the operations performed at Layer 2 with the blockchain, and the applications that benefit from such scheme. Finally, we discuss and compare them to give a general comprehension about how schemes can satisfy general requirements common to most Decentralized Applications.},
  archive      = {J_COMCOM},
  author       = {Domenico Tortola and Andrea Lisi and Paolo Mori and Laura Ricci},
  doi          = {10.1016/j.comcom.2024.07.017},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {289-310},
  shortjournal = {Comput. Commun.},
  title        = {Tethering layer 2 solutions to the blockchain: A survey on proving schemes},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BTMDS: Blockchain trusted medical data sharing scheme with
privacy protection and access control. <em>COMCOM</em>, <em>225</em>,
279–288. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of smart healthcare, eliminating information silos through trusted data sharing has become a social consensus, but there are still many problems that need to be solved. The multi-party sharing process of medical data usually occurs in an untrusted network environment, and the separation of data ownership and usage rights can result in the leakage of patients’ private information. Meanwhile, the communication and computation overheads of existing medical data sharing schemes are too large, resulting in inefficient data sharing. To address the above problems, we propose a blockchain-based trusted medical data sharing scheme (BTMDS) with privacy protection and access control. In it, we subdivided patient privacy into identity and data privacy, and designed a privacy protection mechanism for blockchain medical data sharing using local differential privacy technology and searchable encryption technology. The cloud server acts as a proxy server, and the on-chain-off-chain storage structure of the blockchain and the cloud server implements fine-grained access control to prevent conspiracy attacks. Security analysis proves the security of BTMDS and prioritizes it over other schemes. In terms of performance, BTMDS saves 30% and 48% in the decryption phase compared to Feng and Chen schemes, which is more suitable for digital healthcare data sharing services.},
  archive      = {J_COMCOM},
  author       = {Liqiu Chen and Tao Feng and Rong Ma and Jianming Shi},
  doi          = {10.1016/j.comcom.2024.07.007},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {279-288},
  shortjournal = {Comput. Commun.},
  title        = {BTMDS: Blockchain trusted medical data sharing scheme with privacy protection and access control},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDNRoute: Proactive routing optimization in software defined
networks. <em>COMCOM</em>, <em>225</em>, 250–278. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite opening attractive perspectives, the concept of Software Defined Networking raises doubts about the performance and practical feasibility. To contradict these concerns, we propose a deployment-ready system aimed at proactive and periodic optimization of flow paths. The modular system consists of modules responsible for traffic prediction, static optimization, measurements, flow management, and validation of optimization results. To make the system efficient, we resolved several scientific issues and proposed novel and valuable solutions, for example methods for efficient proactive flow management and periodic re-optimization of routing policies. Simultaneously, to make the system production-ready and create a reliable research environment, we provide solutions to several technical obstacles. We validate the proposed system with three network topologies, each with three load levels, following real-life traffic models. We consider Equal-Cost Multi-Path Routing as a baseline. The results indicate that the system allows network operators to handle more traffic (packet loss reduced by up to 30%), improve quality of service (less congested links resulted in even 2.5 times lower latency), and reduce operational expenses (energy consumption lowered by up to 10%).},
  archive      = {J_COMCOM},
  author       = {Piotr Boryło and Piotr Chołda and Jerzy Domżał and Piotr Jaglarz and Piotr Jurkiewicz and Michał Rzepka and Grzegorz Rzym and Robert Wójcik},
  doi          = {10.1016/j.comcom.2024.07.015},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {250-278},
  shortjournal = {Comput. Commun.},
  title        = {SDNRoute: Proactive routing optimization in software defined networks},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QUIC congestion control algorithm characteristics in mixed
satellite–terrestrial emergency communication scenarios.
<em>COMCOM</em>, <em>225</em>, 239–249. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable communications play a pivotal role in ensuring an efficient response and the coordination of recovery and rescue efforts. However, conventional communication methods may not always be accessible or dependable in such situations. In such circumstances, constellations of Low Earth Orbit (LEO) satellites can provide high bandwidth capabilities with relatively low latency, making them well-suited for supporting on-the-ground disaster management teams. Satellites can either complement or replace terrestrial telecommunication infrastructures. In this context, reliance on the recently defined QUIC protocol allows for a seamless transition from terrestrial to satellite communication as needed. Therefore, we investigate the possible use of a dual-stack node architecture along with the employment of the QUIC transport protocol for emergency communications, assuming that the backhaul link used to transfer users’ applications data may need to be changed (seamlessly). We conduct an extensive emulation study, evaluating the performance of QUIC under varying queuing policies and Congestion Control Algorithm (CCA) behaviour, providing practical insights and recommendations to enhance the protocol’s efficiency and robustness. The key aspects and configurations of QUIC protocol stack are identified, presenting optimal communication configurations leveraging CoDel and BBR CCA.},
  archive      = {J_COMCOM},
  author       = {Armir Bujari and Mirko Franco and Claudio E. Palazzi and Mattia Quadrini and Cesare Roseti and Francesco Zampognaro},
  doi          = {10.1016/j.comcom.2024.07.013},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {239-249},
  shortjournal = {Comput. Commun.},
  title        = {QUIC congestion control algorithm characteristics in mixed satellite–terrestrial emergency communication scenarios},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure communication of intelligent reflecting surface-aided
NOMA in massive MIMO networks. <em>COMCOM</em>, <em>225</em>, 229–238.
(<a href="https://doi.org/10.1016/j.comcom.2024.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Non-Orthogonal Multiple Access (NOMA) network, in which the base station (BS) incorporates massive Multiple-Input Multiple-Output (MIMO) technology, is considered in this paper. This research study focuses on investigating physical layer security in this network when a jammer is present, leveraging intelligent Reflecting Surface (IRS) technology. The IRS is an innovative approach strategically implemented to enhance communication quality by assisting distant users in establishing a reliable connection with the BS. Two key metrics in physical layer security are evaluated: the secrecy rate (SR) for pairs of NOMA users and the secrecy outage probability (SOP). Additionally, the impact of using a jammer is assessed by comparing the network’s performance with and without a jammer. The results indicate that by increasing in the antenna numbers, the rate of secrecy is improved, and the SOP is decreased. Moreover, as the transmit signal-to-noise ratio (SNR) increases, the SR is enhanced, but the SOP is degraded. However, the increase in the IRS element numbers results in a tendency for the SOP to rise. Furthermore, it is evident that incorporating a jammer improves the network’s performance.},
  archive      = {J_COMCOM},
  author       = {Bahar Hazrati},
  doi          = {10.1016/j.comcom.2024.07.003},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {229-238},
  shortjournal = {Comput. Commun.},
  title        = {Secure communication of intelligent reflecting surface-aided NOMA in massive MIMO networks},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel machine learning-based approach for tsunami waves
forecasting using regression trees. <em>COMCOM</em>, <em>225</em>,
217–228. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following a seismic event, tsunami early warning systems (TEWSs) try to provide precise forecasts of the maximum height of incoming waves at designated target points along the coast. This information is crucial to trigger early warnings in areas where the impact of tsunami waves is predicted to be dangerous (or potentially cause destruction), to help the management of the potential impact of a tsunami as well as reduce environmental destruction and losses of human lives. For such a reason, it is crucial that TEWSs produce predictions with short computation time while maintaining a high prediction accuracy. This paper presents a parallel machine learning approach, based on regression trees, to discover tsunami predictive models from simulation data. In order to achieve the results in a short time, the proposed approach relies on the parallelization of the most time consuming tasks and on incremental learning executions, in order to achieve higher performances in terms of execution time, efficiency and scalability. The experimental evaluation, performed on two real tsunami cases occurred in the Western and Eastern Mediterranean basin in 2003 and 2017, shows reasonable advantages in terms of scalability and execution time, which is an important benefit in a urgent-computing scenarios.},
  archive      = {J_COMCOM},
  author       = {Eugenio Cesario and Salvatore Giampá and Enrico Baglione and Louise Cordrie and Jacopo Selva and Domenico Talia},
  doi          = {10.1016/j.comcom.2024.07.016},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {217-228},
  shortjournal = {Comput. Commun.},
  title        = {A parallel machine learning-based approach for tsunami waves forecasting using regression trees},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mission-critical UAV swarm coordination and cooperative
positioning using an integrated ROS-LoRa-based communications
architecture. <em>COMCOM</em>, <em>225</em>, 205–216. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the recent years unmanned aerial vehicles (UAVs) have been utilized extensively in mission-critical operations, especially as it relates to disaster management scenarios. Clearly, during these missions, the UAVs should be able to communicate effectively (amongst themselves and with the ground control station (GCS)) in order to transmit and receive commands and other related information. Moreover, accurate positioning is paramount during this type of operations. This has motivated the exploration of a number of alternative navigation methods to address robustness issues that arise when the global positioning system (GPS) becomes unavailable, due to GNSS disruption or sensor malfunction. This work addresses these issues by initially developing and implementing an integrated LoRa-ROS-based (long range communication - robot operating system) system for UAV-to-X communications. Subsequently, it presents a novel cooperative positioning approach, where a group of autonomous UAVs employ various algorithms (detection, tracking, communication, and localization) for cooperative positioning in order to counter any GPS/sensor malfunction. For evaluation purposes, a prototype multi-agent system is designed and implemented, utilizing the proposed integrated ROS-LoRa-based communication architecture, as well as sensor (inertial measurement unit - IMU) fusion. Specifically, LoRA mesh networking (using a custom printed circuit board - BALORA), is incorporated to maintain communication and distribute the sensor information between the UAVs. The prototype of the proposed communications architecture and cooperative relative positioning system (CRPS) is subsequently tested in a real-world environment, demonstrating the feasibility and effectiveness of the proposed communications solution, as well as the robust and accurate localization that is analogous to the ground truth (GPS+IMU).},
  archive      = {J_COMCOM},
  author       = {Nicolas Souli and Maria Karatzia and Christos Georgiades and Panayiotis Kolios and Georgios Ellinas},
  doi          = {10.1016/j.comcom.2024.07.011},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {205-216},
  shortjournal = {Comput. Commun.},
  title        = {Mission-critical UAV swarm coordination and cooperative positioning using an integrated ROS-LoRa-based communications architecture},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust multi-key authority system for privacy-preserving
distribution and access control of healthcare data. <em>COMCOM</em>,
<em>225</em>, 195–204. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovation in medical technology and communication has rapidly empowered the development of smart healthcare devices. This has led to privacy breaches, threats and vulnerabilities to sensitive patient data that result in unwanted or targeted advertising. Previous research has focused on protecting access to sensitive patient data from unauthorized entities, especially by defining roles of healthcare entities in the overall system with their access privileges. However, such efforts need to be further robust due to the involvement of a single key authority that may lead to a critical point of failure. In this paper, this vulnerability has been addressed by developing a novel approach to crucially increase the number of key authorities using homomorphic encryption. The proposed approach ensures genuine access to the verified entity by forming a subsystem of t key authorities from a total of n authorities ( t &lt; n ) (t&amp;lt;n) . This creates rigorous challenge to a malicious attacker, obfuscating the selection and functioning of key access packets in a multi-key authority setup. The results of the proposed approach achieve medical data confidentiality, entity authentication, and strategic data sharing. The security of the proposed approach is assessed for different vulnerabilities of the overall system using a challenge–response game model. Moreover, the proposed approach is found to be better and secure as compared to existing schemes.},
  archive      = {J_COMCOM},
  author       = {Amitesh Singh Rajput and Arnav Agarwal and Kiran B. Raja},
  doi          = {10.1016/j.comcom.2024.07.005},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {195-204},
  shortjournal = {Comput. Commun.},
  title        = {A robust multi-key authority system for privacy-preserving distribution and access control of healthcare data},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive updates of convolutional neural networks for
enhanced reliability in small satellite applications. <em>COMCOM</em>,
<em>225</em>, 185–194. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small satellites enable many important applications for both economic and scientific purposes. Many of these applications are inherently data-centric and rely on large amounts of high-resolution satellite imagery to be delivered in a timely manner. However, communicating this data to Earth is challenging due to intermittent connectivity, high packet losses, low data rates, and similar issues. Therefore, efficient onboard prioritization and data processing are essential for future satellite missions. Machine learning methods, such as deep neural networks, are very suitable for such prioritization, as they are already used extensively for satellite imagery processing and they can be deployed onboard of satellites. However, updating them to support new classification requirements when the satellite is already in orbit is difficult, as often multiple passes are required to complete model transmission due to the communication challenges. To cope with this issue, we propose a progressive transmission mechanism for model updates, which leverages vector quantization and arithmetic coding. Our mechanism allows to achieve high accuracies even with partially updated models. Evaluation results show that our mechanism significantly outperforms other less optimized transmission schemes.},
  archive      = {J_COMCOM},
  author       = {Olga Kondrateva and Stefan Dietzel and Maximilian Schambach and Johannes Otterbach and Björn Scheuermann},
  doi          = {10.1016/j.comcom.2024.07.012},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {185-194},
  shortjournal = {Comput. Commun.},
  title        = {Progressive updates of convolutional neural networks for enhanced reliability in small satellite applications},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Training a graph neural network to solve URLLC and eMBB
coexisting in 5G networks. <em>COMCOM</em>, <em>225</em>, 171–184. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coexistence of enhanced mobile broadband and ultra-reliable low latency communication in 5G networks is a challenging problem due to the conflicting requirements. In this paper, we decompose the problem into eMBB and URLLC resource allocation phases. For the first phase we propose a heuristic algorithm with O ( n ) O(n) runtime and prove its efficiency and optimality under min–max fairness paradigm. For the URLLC resource allocation, the puncturing framework is adopted and a novel approach using the Graph Neural Networks is proposed to maximize eMBB data rates and fairness while minimizing URLLC outage probability. We show that the runtime of this GNN-based algorithm is also O ( n ) O(n) . To train the GNN, an application-specific loss function is designed and empirically shown to be convergent. Our simulation results show that our proposed approach performs very well in terms of eMBB data rates, fairness, and URLLC outage probability in comparison to a number of thoughtfully chosen baselines. We also demonstrate that the proposed GNN is robust to changes in network topology and traffic volume. As we show our algorithm has O ( n ) O(n) runtime, it is fully practical for solving the resource allocation problem in the very short time spans that are required by 5G and future generation networks.},
  archive      = {J_COMCOM},
  author       = {Seyyed Mohammad Mahdi Hosseini Daneshvar and Sayyed Majid Mazinani},
  doi          = {10.1016/j.comcom.2024.07.008},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {171-184},
  shortjournal = {Comput. Commun.},
  title        = {Training a graph neural network to solve URLLC and eMBB coexisting in 5G networks},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eco-FL: Enhancing federated learning sustainability in edge
computing through energy-efficient client selection. <em>COMCOM</em>,
<em>225</em>, 156–170. (<a
href="https://doi.org/10.1016/j.comcom.2024.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of edge cloud computing (ECC), Federated Learning (FL) revolutionizes the decentralization of machine learning (ML) models by enabling their training across multiple devices. In this way, FL preserves privacy and minimizes the need for centralized data by processing data near the source. From a communication standpoint, only the model weights are exchanged between devices. By avoiding the need to send data to a centralized location for processing, FL reduces the energy required for data transfer and supports more efficient use of computing resources at the edge. FL is particularly advantageous for resource-constrained devices, such as smartphones and IoT devices. However, this limited computational power and battery capacity and the challenge of energy consumption are critical aspects of FL systems. This paper introduces Eco-FL, an innovative methodology designed to optimize energy consumption in FL systems, in the field of Green Edge Cloud Computing (GECC). Our approach employs a device selection process that considers the entropy of the data held by the devices and their available energy reserves. This ensures that devices with lower energy availability are less likely to participate in the training rounds, prioritizing those with higher energy capacities. To evaluate the efficacy of our methodology, we utilize FedEntropy, an entropy-based aggregation method, alongside established aggregation methods such as FedAvg and FedProx for performance comparison. The effectiveness of Eco-FL in reducing energy consumption without compromising the accuracy of the FL process is demonstrated through analyses conducted on three distinct datasets. These analyses vary the β β parameter of the Dirichlet distribution and account for scenarios with both homogeneous and heterogeneous initial device charges. Our findings validate Eco-FL’s potential to enhance the sustainability of FL systems by judiciously managing client participation based on energy criteria, presenting a significant step forward in the development of energy-efficient FL.},
  archive      = {J_COMCOM},
  author       = {Martina Savoia and Edoardo Prezioso and Valeria Mele and Francesco Piccialli},
  doi          = {10.1016/j.comcom.2024.07.014},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {156-170},
  shortjournal = {Comput. Commun.},
  title        = {Eco-FL: Enhancing federated learning sustainability in edge computing through energy-efficient client selection},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Making stateless and stateful network performance
measurements unbiased. <em>COMCOM</em>, <em>225</em>, 141–155. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Benchmarking Working Group (BMWG) of the Internet Engineering Task Force (IETF) has defined a series of Requests for Comments (RFC) to standardize the benchmarking of network interconnect devices (e.g., bridges, routers, different IPv6 transition solutions). The paper points out that there are cases where the performance results are significantly different when a single IP address pair or multiple IP addresses are used. The cause of this phenomenon is rooted in the recent hardware and software advancements: Receive Side Scaling (RSS) makes it possible to distribute packet processing workload over multiple CPU cores. However, this may be implemented in two ways: the first way only includes the IP addresses into the hash function used to distribute the workload among the CPU cores, whereas the second one also includes the port numbers. RFC 4814 proposed an excellent solution for the second case by recommending the usage of pseudorandom port numbers during benchmarking; however, the first case was not handled properly, because no explicit recommendation was given regarding the usage of multiple IP addresses. This paper attempts to bridge this methodological gap; a practical solution is proposed for using pseudorandom IP addresses in various scenarios including the benchmarking of IPv4 and IPv6 routers and Network Address Translation from IPv6 Clients to IPv4 Servers (stateful NAT64) gateways. Its feasibility is shown by disclosing the details of its implementation in siitperf. Then the proposed solution is validated by both stateless and stateful tests. It is shown that the measurement results of the tests following the proposed solution can better characterize the true performance of the network interconnect devices that follow the first type of RSS implementation than the results of the tests using a single IP address pair.},
  archive      = {J_COMCOM},
  author       = {G. Lencse},
  doi          = {10.1016/j.comcom.2024.05.018},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {141-155},
  shortjournal = {Comput. Commun.},
  title        = {Making stateless and stateful network performance measurements unbiased},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locating multiple rumor sources in social networks using
partial information of monitors. <em>COMCOM</em>, <em>225</em>, 126–140.
(<a href="https://doi.org/10.1016/j.comcom.2024.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rumors in social media platforms and the identification of their sources is a challenging issue in modern-day computer communication. Existing approaches mostly fail to localize the source node accurately due to the lack of complete network information or timestamps . Besides, most of the techniques focused on single-source identification only, while sometimes multiple sources exist in the network. In this paper, we designed a new algorithm called Multi Snowballing with Partial Timestamps (MSPT) to find multiple sources utilizing partial timestamps available to monitors. We have explored the snowballing technique to determine the vulnerable radius that may contain the rumor source based on the partial timestamps of a few nodes. The overall complexity of the algorithm is O ( N S ∗ ( N S + E S ) ) O(NS∗(NS+ES)) , where N S NS is the set of snowball nodes and E S ES represents edges in between snowball nodes. Extensive empirical analysis is performed on a variety of networks, which include small-scale, large-scale, and artificial networks. Empirical outcomes demonstrate that the presented algorithm is efficient in terms of error distance and execution time compared to baseline algorithms.},
  archive      = {J_COMCOM},
  author       = {Ravi Kishore Devarapalli and Soumita Das and Anupam Biswas},
  doi          = {10.1016/j.comcom.2024.07.004},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {126-140},
  shortjournal = {Comput. Commun.},
  title        = {Locating multiple rumor sources in social networks using partial information of monitors},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource allocation-aware efficient interference management
technique for ultra-dense femto environment. <em>COMCOM</em>,
<em>225</em>, 120–125. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the challenge of severe inter-layer and intra-layer interference in densely deployed Femtocells , we introduce an efficient resource allocation strategy centered on clustering. Initially, we employ the Fractional Frequency Reuse (FFR) technique to mitigate interference by segmenting cells into distinct zones. Subsequently, leveraging graph theory and convex optimization, we cluster the Femto Base Stations (FBS) to enhance efficiency. An algorithm prioritizing user fairness governs sub-channel allocation among FBSs. Additionally, a distributed power control algorithm dynamically adjusts FBS power levels, further enhancing system throughput. Comparative analysis reveals that our approach surpasses traditional non-clustered methods and three other comparison methods, exhibiting superior system throughput and Signal-to-Interference Ratio (SINR). Notably, our method significantly enhances user fairness, promoting greater satisfaction among Femto User Equipment (FUE) users.},
  archive      = {J_COMCOM},
  author       = {Wanying Guo and Bojun Wang and Lian Zhao and Isma Farah Siddiqui},
  doi          = {10.1016/j.comcom.2024.05.013},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {120-125},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation-aware efficient interference management technique for ultra-dense femto environment},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The evolution of detection systems and their application for
intelligent transportation systems: From solo to symphony.
<em>COMCOM</em>, <em>225</em>, 96–119. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of autonomous driving technologies has been significantly influenced by advancements in perception systems. Traditional single-agent detection models, while effective in certain scenarios, exhibit limitations in complex environments, necessitating the shift towards collaborative detection models. While numerous studies have investigated the fundamental architecture and primary elements within this domain, comprehensive analyses focusing on the evolution from single-agent-based detection systems to collaborative detection systems are notably absent. This paper provides a comprehensive examination of this transition, delineating the development from single agent to collaborative perception models in autonomous driving. Initially, this paper delves into single-agent detection models, discussing their capabilities, limitations, and application scenarios. Subsequently, the focus shifts to collaborative detection models, which leverage Vehicle-to-Everything (V2X) communication to enhance perception and decision-making in complex environments. Fundamental concepts about mainstream collaborative approaches and mechanisms are reviewed to present the general organization of collaborative detection models. Furthermore, we critically evaluates various collaborative models, comparing their performance, data fusion strategies, and adaptability in dynamic settings. The integration of V2X-enabled Internet-of-Vehicles (IoV) introduces a pivotal evolution in the transition from single-agent-based detection to multi-agent collaborative sensing. This advancement allows for real-time interaction of sensory information between vehicles, augmenting the development of collaborative sensing. However, the interaction of sensory information also increases the load on the network, highlighting the need for strategies that achieve a balance between communication overhead and the improvement in perception capabilities. We concludes with future perspectives, emphasizing the potential issues the development of collaborative detection models will meet and the promising directions for future research.},
  archive      = {J_COMCOM},
  author       = {Zedian Shao and Kun Yang and Peng Sun and Yulin Hu and Azzedine Boukerche},
  doi          = {10.1016/j.comcom.2024.06.015},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {96-119},
  shortjournal = {Comput. Commun.},
  title        = {The evolution of detection systems and their application for intelligent transportation systems: From solo to symphony},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of 5G base station coverage based on
self-adaptive mutation genetic algorithm. <em>COMCOM</em>, <em>225</em>,
83–95. (<a href="https://doi.org/10.1016/j.comcom.2024.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication network planning, a rational base station layout plays a crucial role in improving communication speed, ensuring service quality, and reducing investment costs. To address this, the article calibrated the urban microcell (UMa) signal propagation model using the least squares method, based on road test data collected from three distinct environments: dense urban areas, general urban areas, and suburbs. With the calibrated model, a detailed link budget analysis was performed on the planning area, calculating the maximum coverage radius required for a single base station to meet communication demands, and accordingly determining the number of base stations needed. Subsequently, this article proposed the Adaptive Mutation Genetic Algorithm (AMGA) and formulated a mathematical model for optimizing 5G base station coverage to improve the base station layout. Simulation experiments were conducted in three different scenarios, and the results indicate that the proposed AMGA algorithm effectively enhances base station coverage while reducing construction costs, thoroughly demonstrating the value of base station layout optimization in practical applications.},
  archive      = {J_COMCOM},
  author       = {Jianpo Li and Jinjian Pang and Xiaojuan Fan},
  doi          = {10.1016/j.comcom.2024.07.002},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {83-95},
  shortjournal = {Comput. Commun.},
  title        = {Optimization of 5G base station coverage based on self-adaptive mutation genetic algorithm},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time prevention of trust-related attacks in social IoT
using blockchain and apache spark. <em>COMCOM</em>, <em>225</em>, 65–82.
(<a href="https://doi.org/10.1016/j.comcom.2024.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Social Internet of Things (Social IoT) introduces a fresh approach to promote the usability of IoT networks and enhance service discovery by incorporating social contexts. However, this approach encounters various challenges that impact its performance and reliability. One of the most prominent challenges is trust, specifically trust-related attacks, where certain users engage in malicious behaviors and launch attacks to spread harmful services. To ensure a trustworthy experience for end-users and prevent such attacks in real-time, it is highly significant to incorporate a trust management mechanism within the Social IoT network. To address this challenge, we propose a novel trust management mechanism that leverages blockchain technology. By integrating this technology, we aim to prevent trust-related attacks and create a secure environment. Additionally, we introduce a new consensus protocol for the blockchain called Spark-based Proof of Trust-related Attacks (SPoTA). This protocol is designed to process stream transactions in real-time using Apache Spark, a distributed stream processing engine. To implement SPoTA, we have developed a new classifier utilizing Spark Libraries. This classifier is capable of accurately categorizing transactions as either malicious or secure. As new transaction streams are read, the classifier is employed to classify and assign a label to each stream. This label assists the SPoTA protocol in making informed decisions regarding the validation or rejection of transactions. Our research findings demonstrate the effectiveness of our classifier in predicting malicious transactions, outstripping our previous works and other approaches reported in the literature. Additionally, our new protocol exhibits improved transaction processing times.},
  archive      = {J_COMCOM},
  author       = {Mariam Masmoudi and Ikram Amous and Corinne Amel Zayani and Florence Sèdes},
  doi          = {10.1016/j.comcom.2024.06.019},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {65-82},
  shortjournal = {Comput. Commun.},
  title        = {Real-time prevention of trust-related attacks in social IoT using blockchain and apache spark},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid CSMA/CA and HCCA uplink medium access control
protocol for VLC based heterogeneous users. <em>COMCOM</em>,
<em>225</em>, 54–64. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light fidelity (LiFi) is an emerging wireless networking technology of visible light communication (VLC) paradigm for multiuser communication. This technology enables high data rates due to the availability of large visible light spectrum. While current studies have shown the potential for LiFi technology, they borrow the MAC-layer protocols from traditional WiFi. However, a number of prior studies have shown the challenges faced by the MAC-layer of WiFi in the presence of large number and types of devices. In this work, we show that the hybrid-coordination-function-controlled-access (HCCA) MAC protocol in LiFi provides higher throughput than the traditional CSMA/CA mechanism to user devices. We also show that HCCA has the limitation of higher message overhead in the presence of a large number of devices. We also evaluate the collision probability, busy channel probability, and delay for HCCA and CSMA/CA MAC protocol. We utilize both theoretical analysis and extensive simulations to study these performance tradeoffs and identify a threshold when a LiFi access point should switch to HCCA from CSMA/CA and vice-versa. Finally, based on our findings, we design a hybrid-MAC mechanism that switches between HCCA and CSMA/CA based on the number and type of devices present. Our evaluation shows that this hybrid mechanism can outperform both HCCA and CSMA/CA individually in the presence of different number of devices.},
  archive      = {J_COMCOM},
  author       = {Saswati Paramita and Arani Bhattacharya and Anand Srivastava and Vivek Ashok Bohara},
  doi          = {10.1016/j.comcom.2024.06.017},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {54-64},
  shortjournal = {Comput. Commun.},
  title        = {Hybrid CSMA/CA and HCCA uplink medium access control protocol for VLC based heterogeneous users},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring data plane updates on p4 switches with P4Runtime.
<em>COMCOM</em>, <em>225</em>, 44–53. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development and roll-out of new Ethernet standards increase the available bandwidths in computer networks. This growth presents significant advantages, enabling novel applications. At the same time, the increase introduces new challenges; higher data rates reduce the available time budget to process each packet. This development also impacts software-defined networks. Their data planes need to keep up with the increased traffic rates. Nevertheless, the control plane must not be ignored; fast reaction times are necessary to handle the increased rates handled by data planes efficiently. In our work, we analyze the interaction of a high-performance data plane and different implementations for the control plane. We selected a P4 switching ASIC as our data plane. For the control plane, we investigate vendor-specific implementations and a standardized implementation called P4Runtime. To determine the performance of the control plane, we introduce a novel measurement methodology. This methodology allows measuring the delay between the initiation of rule updates on the control plane and their application on the data plane. We investigate the behavior of the data plane, its performance and non-atomicity of updates. Based on our findings, we apply different optimization strategies to improve control plane performance. Our measurements show that neglecting the control plane performance may impact network behavior due to delayed updates, but we also show how to minimize this delay and, thereby, its impact. We have released the experiment artifacts of our study including experiment scripts and measurement data.},
  archive      = {J_COMCOM},
  author       = {Henning Stubbe and Sebastian Gallenmüller and Manuel Simon and Eric Hauser and Dominik Scholz and Georg Carle},
  doi          = {10.1016/j.comcom.2024.06.020},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {44-53},
  shortjournal = {Comput. Commun.},
  title        = {Exploring data plane updates on p4 switches with P4Runtime},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective task offloading for highly dynamic
heterogeneous vehicular edge computing: An efficient reinforcement
learning approach. <em>COMCOM</em>, <em>225</em>, 27–43. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Edge Computing (VEC) provides a flexible distributed computing paradigm for offloading computations to the vehicular network, which can effectively solve the problem of limited vehicle computing resources and meet the on-vehicle computing requests of users. However, the conflict of interest between vehicle users and service providers leads to the need to consider a variety of conflict optimization goals for computing offloading, and the dynamic nature of vehicle networks, such as vehicle mobility and time-varying network conditions, make the offloading effectiveness of vehicle computing requests and the adaptability to complex VEC scenarios challenging. To address these challenges, this paper proposes a multi-objective optimization model suitable for computational offloading of dynamic heterogeneous VEC networks. By formulating the dynamic multi-objective computational offloading problem as a multi-objective Markov Decision Process (MOMDP), this paper designs a novel multi-objective reinforcement learning algorithm EMOTO, which aims to minimize the average task execution delay and average vehicle energy consumption, and maximize the revenue of service providers. In this paper, a preference priority sampling module is proposed, and a model-augmented environment estimator is introduced to learn the environmental model for multi-objective optimization, so as to solve the problem that the agent is difficult to learn steadily caused by the highly dynamic change of VEC environment, thus to effectively realize the joint optimization of multiple objectives and improve the decision-making accuracy and efficiency of the algorithm. Experiments show that EMOTO has superior performance on multiple optimization objectives compared with advanced multi-objective reinforcement learning algorithms. In addition, the algorithm shows robustness when applied to different environmental settings and better adapting to highly dynamic environments, and balancing the conflict of interest between vehicle users and service providers.},
  archive      = {J_COMCOM},
  author       = {ZhiDong Huang and XiaoFei Wu and ShouBin Dong},
  doi          = {10.1016/j.comcom.2024.06.018},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {27-43},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective task offloading for highly dynamic heterogeneous vehicular edge computing: An efficient reinforcement learning approach},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using ranging for collision-immune IEEE 802.11 rate
selection with statistical learning. <em>COMCOM</em>, <em>225</em>,
10–26. (<a href="https://doi.org/10.1016/j.comcom.2024.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate data rate selection at the physical layer is crucial for Wi-Fi network performance: too high rates lead to loss of data frames, while too low rates cause increased latency and inefficient channel use. Most existing methods adopt a probing approach and empirically assess the transmission success probability for each available rate. However, a transmission failure can also be caused by frame collisions. Thus, each collision leads to an unnecessary decrease in the data rate. We avoid this issue by resorting to the fine timing measurement (FTM) procedure, part of IEEE 802.11, which allows stations to perform ranging, i.e., measure their spatial distance to the AP. Since distance is not affected by sporadic distortions such as internal and external channel interference, we use this knowledge for data rate selection. Specifically, we propose FTMRate, which applies statistical learning (a form of machine learning) to estimate the distance based on measurements, predicts channel quality from the distance, and selects data rates based on channel quality. We define three distinct estimation approaches: exponential smoothing, Kalman filter, and particle filter. Then, with a thorough performance evaluation using simulations and an experimental validation with real-world devices, we show that our approach has several positive features: it is resilient to collisions, provides near-instantaneous convergence, is compatible with commercial-off-the-shelf devices, and supports pedestrian mobility. Thanks to these features, FTMRate outperforms existing solutions in a variety of line-of-sight scenarios, providing close to optimal results. Additionally, we introduce Hybrid FTMRate, which can intelligently fall back to a probing-based approach to cover non-line-of-sight cases. Finally, we discuss the applicability of the method and its usefulness in various scenarios.},
  archive      = {J_COMCOM},
  author       = {Wojciech Ciezobka and Maksymilian Wojnar and Krzysztof Rusek and Katarzyna Kosek-Szott and Szymon Szott and Anatolij Zubow and Falko Dressler},
  doi          = {10.1016/j.comcom.2024.07.001},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {10-26},
  shortjournal = {Comput. Commun.},
  title        = {Using ranging for collision-immune IEEE 802.11 rate selection with statistical learning},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interference aware path planning for mobile robots under
joint communication and sensing in mmWave networks. <em>COMCOM</em>,
<em>225</em>, 1–9. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The beyond fifth-generation (B5G) and emerging sixth-generation (6G) wireless networks are considered as key enablers in supporting a diversified set of applications for industrial mobile robots (MRs). In this paper, we consider mobile robots that autonomously roam on an industrial floor and perform a variety of tasks at different locations, whilst utilizing high directivity beamformers in millimeter wave (mmWave) small cells for joint communication and sensing. In such scenarios, the potential close proximity of MRs connected to different base stations, may cause excessive levels of interference having as a net result a decrease in the overall achievable data rate and network service degradation. To mitigate this effect an interference aware path planning algorithm is proposed by explicitly taking into account both achievable communication and sensing performance. More specifically, the proposed heuristic scheme aims to find paths with minimal interfering time whilst improving the overall joint communication and sensing quality of service. A wide set of numerical investigations reveal that the proposed heuristic path planning scheme for the mmWave networked mobile robots can improve the overall achievable communication throughput and the sensing mutual information by up to 35.6% and 23.6% respectively compared with an interference oblivious scheme. Most importantly, those gains are attained without penalizing noticeably the total travel time of the MRs.},
  archive      = {J_COMCOM},
  author       = {Yijing Ren and Vasilis Friderikos},
  doi          = {10.1016/j.comcom.2024.06.012},
  journal      = {Computer Communications},
  month        = {9},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {Interference aware path planning for mobile robots under joint communication and sensing in mmWave networks},
  volume       = {225},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGCM: A multi-stage attack correlation and scenario
reconstruction method based on graph aggregation. <em>COMCOM</em>,
<em>224</em>, 302–313. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an increase in the complexity and scale of networks, cybersecurity faces increasingly severe challenges. For instance, an attacker can combine individual attacks into complex multi-stage attacks to infiltrate targets. Traditional intrusion detection systems (IDS) generate large number of alerts during an attack, including attack clues along with many false positives. Furthermore, due to the complexity and changefulness of attacks, security analysts spend considerable time and effort on discovering attack paths. Existing methods rely on attack knowledgebases or predefined correlation rules but can only identify known attacks. To address these limitations, this paper presents an attack correlation and scenario reconstruction method. We transform the abnormal flows corresponding to the alerts into abnormal states relationship graph (ASR-graph) and automatically correlate attacks through graph aggregation and clustering. We also implemented an attack path search algorithm to mine attack paths and trace the attack process. This method does not rely on prior knowledge; thus, it can well adapt to the changed attack plan, making it effective in correlating unknown attacks and identifying attack paths. Evaluation results show that the proposed method has higher accuracy and effectiveness than existing methods.},
  archive      = {J_COMCOM},
  author       = {Hongshuo Lyu and Jing Liu and Yingxu Lai and Beifeng Mao and Xianting Huang},
  doi          = {10.1016/j.comcom.2024.06.016},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {302-313},
  shortjournal = {Comput. Commun.},
  title        = {AGCM: A multi-stage attack correlation and scenario reconstruction method based on graph aggregation},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure data sharing scheme with privacy-preserving and
certificateless integrity auditing in cloud storage. <em>COMCOM</em>,
<em>224</em>, 285–301. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With cloud storage services becoming wildspread and low-cost, individuals and organizations select outsourcing large amounts of document to cloud. Cloud storage applications provide data sharing services with varying functionality. In some cloud storage systems that provide data sharing, such as e-health systems, hiding sensitive information fields is a necessity. Meanwhile, in sharing process, unauthorized entities may have access to these privacy fields. The resource-constrained computation capability for data owner (DO) is universal. To tackle above problems, we propose a sanitizer-based secure data sharing scheme where sanitizer performs most computations of encryption and signature after receiving the partially blinded data from DO. It also checks the legitimacy of visitors before CSP returns stored data file. Besides, adopting an attribute-based access policy guarantees further privacy and fine-grained access authorization. After verification, cloud will return the required data to the legitimate visitors. Moreover, integrity auditing to shared data file is based on certificateless authentication technique, which removes certificates issue of traditional cryptographic technique, avoids key-escrow attack risk caused by identity-based cryptography. Finally, the performance analysis and experimental results show that our proposed scheme is competitive in practical applications since it reduces the authenticator generation overhead by up to 21.6% and proof generation overhead by up to 12% compared with related schemes.},
  archive      = {J_COMCOM},
  author       = {Xuening Guan and Jinyong Chang and Wei Zhang},
  doi          = {10.1016/j.comcom.2024.06.013},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {285-301},
  shortjournal = {Comput. Commun.},
  title        = {Secure data sharing scheme with privacy-preserving and certificateless integrity auditing in cloud storage},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CATFSID: A few-shot human identification system based on
cross-domain adversarial training. <em>COMCOM</em>, <em>224</em>,
275–284. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of wireless sensing technology, human identification based on WiFi sensing has garnered significant attention in the fields of human–computer interaction and home security. Despite the initial success of WiFi sensing based human identification when the environment is fixed, the performance of the trained identity sensing model will be severely degraded when applied to unfamiliar environments. In this paper, a cross-domain human identification system (CATFSID) is proposed, which is able to achieve environment migration of trained model using up to 3-shot. CATFSID utilizes a dual adversarial training network, including cross-adversarial training between source and source domain classifiers, and adversarial training between source and target domain discriminators to extract environment-independent identity features. Introducing a method based on pseudo-label prediction, which assigns labels to target domain samples similar to the source domain samples, reduces the distribution bias of identity features between the source and target domains. The experimental results show accuracy of 90.1% and F1- Score of 89.33% when using 3 samples per user in the new environment.},
  archive      = {J_COMCOM},
  author       = {Zhongcheng Wei and Wei Chen and Weitao Tao and Shuli Ning and Bin Lian and Xiang Sun and Jijun Zhao},
  doi          = {10.1016/j.comcom.2024.06.014},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {275-284},
  shortjournal = {Comput. Commun.},
  title        = {CATFSID: A few-shot human identification system based on cross-domain adversarial training},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wireless sensing applications with wi-fi channel state
information, preprocessing techniques, and detection algorithms: A
survey. <em>COMCOM</em>, <em>224</em>, 254–274. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, using Wi-Fi devices as sensing technology has garnered significant attention and found numerous applications. This trend has given birth to what is known as Wi-Fi sensing. This innovative approach has been made possible by analyzing the Channel State Information (CSI) of the Wi-Fi communication channel, which can be calculated from the frame preamble. CSI captures multipath effects, attenuations, and phase shifts in signal propagation, among other fading effects caused, for example, by reflection with objects or a person. Extensive research in this field has revealed that these effects can be utilized for sensing purposes, supposing that the cause of the effects is a person, leading to the development of Wi-Fi sensing applications such as Activity Recognition, Gesture Recognition, and Vital Signs monitoring. This Survey highlights recent works and advances in Wi-Fi sensing applications, identifying and exposing significant applications. For each paper selected, the following sections have been emphasized: Data Collection, Data Preprocessing, Detection Algorithm, Performance, and Application, as these are part of the methodology followed for the functioning of Wi-Fi sensing systems. Furthermore, the techniques and tools commonly used in Wi-Fi sensing are described. Finally, the current challenges associated with Wi-Fi as a wireless sensing technology and possible ways to address these challenges are discussed.},
  archive      = {J_COMCOM},
  author       = {Jesus A. Armenta-Garcia and Felix F. Gonzalez-Navarro and Jesus Caro-Gutierrez},
  doi          = {10.1016/j.comcom.2024.06.011},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {254-274},
  shortjournal = {Comput. Commun.},
  title        = {Wireless sensing applications with wi-fi channel state information, preprocessing techniques, and detection algorithms: A survey},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UMPL- VINS: Generalized SLAM for multi-scene metaverse
applications. <em>COMCOM</em>, <em>224</em>, 242–253. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of line features to improve the localization accuracy of point-based visual inertial SLAM (VINS) is of increasing interest because of the additional constraints they can provide on the scene structure. It is found that although the introduction of line features improves the accuracy of relative position estimation in some scenes due to the additional constraints, the two constraints only achieve an equalization for the estimated relative position. Therefore, in some environments two constraints can reduce the accuracy of the single-feature constraint algorithm and also make the real-time performance of the system more challenging. To address such issues, in this paper, we design a generalized SLAM system with point-line features that can be applied to multiple metaverse scenarios. We first enhance the image frames used for feature extraction by eliminating motion blur frames through the fuzzy metric method and mutation modeling of velocity and rotation. Then we improve the traditional line detection model by short line fusion, uniform distribution of line features, and refinement of edge features to obtain high-quality line features for building SLAM. Finally, based on the application of point features and line features for different scenes, a point and line feature separation-union model is proposed. In addition, we design a transformation model for line-point features to enhance the processing of point-line features. Experimental results on EuRoc, TUM_VI, KITTI, PennCOSYVIO and our own recorded dataset prove that the method proposed in this paper realizes a generalized SLAM with point and line features applied to multi-scenes with good advantages.},
  archive      = {J_COMCOM},
  author       = {Hao Jiang and Yilin Shang and Shan Xue and Dongsheng Guo and Weidong Zhang},
  doi          = {10.1016/j.comcom.2024.05.016},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {242-253},
  shortjournal = {Comput. Commun.},
  title        = {UMPL- VINS: Generalized SLAM for multi-scene metaverse applications},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supporting VoIP communication in IEEE 802.11ax networks: A
new admission control and scheduling resource allocation scheme.
<em>COMCOM</em>, <em>224</em>, 225–241. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current IEEE 802.11ax standard enhances Wi-Fi networks with a series of new features, such as multi-user (MU) transmission, an Orthogonal Frequency Division Multiple Access (OFDMA) scheme and the ability to multiplex traffic from different access categories (ACs). These features can be utilized to enhance the QoS support for VoIP traffic and optimize the usage of IEEE 802.11ax network resources. This work proposes a new scheme to multiplex VoIP calls in IEEE 802.11ax MU frames and a new scheduler and resource allocation algorithm specifically designed for VoIP data traffic. Our scheduler allocates VoIP packets requiring longer transmission times into the same frame(s), minimizing the channel air-time assigned to VoIP transmission. In addition, unused radio resources in the MU frame are leveraged to transmit best-effort packets along with VoIP packets. For completeness, we also define a call admission control (CAC) algorithm that anticipates channel saturation conditions to ensure VoIP users can maintain a guaranteed level of QoS. Based on simulation results, our scheme is more efficient in reducing channel utilization than other schedulers such as multi-user round-robin (RR) (implemented by ns-3) or single-user FIFO. For example, for 30 VoIP stations using the G.711 codec under mixed channel conditions, our scheme reduces by 30% the air-time required to transmit VoIP packets. When coupled with the ability to also send best-effort packets along with VoIP packets, this translates into a higher throughput (i.e. 10 Mbit/s vs 4 Mbit/s) and more simultaneous VoIP users with guaranteed QoS (up to 46 VoIP users vs 26 and 28 users for the multi-user RR and single-user FIFO scheduling algorithms, respectively).},
  archive      = {J_COMCOM},
  author       = {Rafael Estepa and Mark Davis and Vicente Mayor and Antonio Estepa},
  doi          = {10.1016/j.comcom.2024.06.010},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {225-241},
  shortjournal = {Comput. Commun.},
  title        = {Supporting VoIP communication in IEEE 802.11ax networks: A new admission control and scheduling resource allocation scheme},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BBR-based and fairness-guaranteed congestion control and
packet scheduling for MPQUIC over heterogeneous networks.
<em>COMCOM</em>, <em>224</em>, 213–224. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipath QUIC (MPQUIC) enables concurrent transmission over multiple paths, making it attractive to enhance the delivery performance of smartphones with multiple interfaces. Nevertheless, several issues in the current MPQUIC specification remain inadequately addressed. Congestion control in MPQUIC primarily relies on packet loss and favors paths with lower drop rates, which results in significant throughput degradation, especially in heterogeneous wireless networks with frequent random packet loss and varying path characteristics. Additionally, network fairness principles limits its performance, even in scenarios with independent and non-shared bottleneck paths. A final issue is that packet scheduling in dynamic and asymmetric paths poses a significant challenge, potentially causing out-of-order (OFO) arrivals and Head-of-Line (HoL) blocking if not appropriately managed. Inspired by Google’s BBR algorithm, this paper introduces a novel scheme called Shared Bottleneck and BBR-based Congestion Control (S2B2C) to address the above issues. It identifies subsets of subflows that share common bottlenecks within the MPQUIC connection based on the BBR’s internal mechanism. The scheme dynamically adjusts the pacing rate of these subflows to enhance goodput while preserving fairness with legacy single-path flows. Then, to mitigate inter-stream and intra-stream HoL blocking caused by OFO arrivals over asymmetric paths, we present a fine-grained Stream-path-bounded Priority-based Packet Scheduler (S2PS), which includes a stream dependency tree, a priority-based queuing, and a packet scheduling algorithm that uses a mixed-integer program formulation. Experimental results show that S2B2C achieves higher throughput while maintaining bottleneck fairness, and S2PS effectively mitigates HoL blocking with fewer OFO arrivals and shorter flow completion time compared to existing MPQUIC schemes in heterogeneous networks.},
  archive      = {J_COMCOM},
  author       = {Zhenjie Deng and Yanwei Liu and Jinxia Liu and Antonios Argyriou and Dacai Liu},
  doi          = {10.1016/j.comcom.2024.06.006},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {213-224},
  shortjournal = {Comput. Commun.},
  title        = {BBR-based and fairness-guaranteed congestion control and packet scheduling for MPQUIC over heterogeneous networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal smart contracts for controlling the environment in
electric vehicles based on an internet of things network.
<em>COMCOM</em>, <em>224</em>, 192–212. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific community has recently focused on intelligent models for predicting and optimizing EV energy management. Despite numerous studies in energy management optimization, there is a critical need to address the trade-off between energy consumption and occupant comfort. Existing IoT systems face challenges in data analytics security and authenticity, highlighting the need for contemporary models to overcome data privacy and cost-related issues. This study introduces a smart contract model based on optimization and control modules, aiming to manage energy consumption while satisfying user comfort requirements intelligently. Introducing a smart contract model with hierarchical layers—prediction, optimization, control, and Blockchain—the proposed approach intelligently manages energy consumption while meeting user comfort requirements. Utilizing a Kalman filter for prediction and the BAT algorithm for optimization, the model integrates modules to tailor user preferences and enhance comfort. The synergy between the optimization module and a convolutional FLC enhances system performance, ensuring minimized energy usage and elevated user comfort levels. The study also evaluates the model’s implementation of the Hyperledger Fabric network, assessing outcomes regarding caliper, latency, throughput, and resource utilization.},
  archive      = {J_COMCOM},
  author       = {Mohammad Hijjawi and Faisal Jamil and Harun Jamil and Tariq Alsboui and Richard Hill and Ibrahim A. Hameed},
  doi          = {10.1016/j.comcom.2024.06.004},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {192-212},
  shortjournal = {Comput. Commun.},
  title        = {Optimal smart contracts for controlling the environment in electric vehicles based on an internet of things network},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interference-conscious reduced routing overhead protocol
for device-to-device (D2D) networks. <em>COMCOM</em>, <em>224</em>,
169–191. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Device-to-Device (D2D) communication has attracted much popularity in the recent years. D2D communication requires a multi-hop route to establish the communication between D2D users which are not in the communication range of each other. D2D users communicate at lower power to keep the interference to a minimum level which necessitates the need of an interference aware routing scheme. The existing interference-aware routing schemes for D2D communication either consider observed average interference or Signal-to-Interference-and-Noise-Ratio (SINR). However, to the best of our knowledge, there is no scheme in the literature that satisfies the network requirements of having better SINR and minimum interference simultaneously. In this paper, we propose a novel routing metric and novel route discovery mechanisms for D2D communication. The novel routing metric, MIIS (Metric for Interference Impact and SINR) selects routes with higher SINR and lower interference. The novel route discovery mechanism, reactive centralized routing, takes advantage of the presence of BS to establish D2D routes which reduces routing overhead as compared to distributed routing scheme. We also extend the reactive centralized routing to proactive centralized routing. The performance evaluation in OMNeT++ network simulator verifies that our proposed schemes outperform other schemes in terms of average hop count, routing overhead, packet loss ratio and end-to-end delay.},
  archive      = {J_COMCOM},
  author       = {Farrukh Salim Shaikh and Yasir Saleem and Roland Wismüller},
  doi          = {10.1016/j.comcom.2024.06.001},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {169-191},
  shortjournal = {Comput. Commun.},
  title        = {An interference-conscious reduced routing overhead protocol for device-to-device (D2D) networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure multi-party computation with secret sharing for
real-time data aggregation in IIoT. <em>COMCOM</em>, <em>224</em>,
159–168. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time analytics in Industrial Internet-of-Things (IIoT) has received remarkable attention recently due to its capacity to prevent downtime and manage risks. However, the sensed data in IIoT is considered private. Thus, the sensed data of IIoT nodes cannot be transmitted and utilized directly in the cloud server due to the risk of privacy leakage. Data aggregation can effectively balance the availability of data with privacy concerns, making it particularly well-suited for IIoT systems. Although several privacy-preserving aggregation schemes in IIoT have been proposed, the majority of them can only support a single type of aggregation that limits the application scenarios of data aggregation. To address the problems mentioned above, a real-time aggregation analysis scheme for IIoT is proposed, which is constructed based on secure multi-party computation with secret sharing. Specifically, the multi-party computation with secret sharing is utilized to implement data aggregation process for IIoT that achieves multiple types of data aggregation. In addition, the secret sharing is utilized in the proposed scheme that can significantly improve the efficiency of the proposed scheme compared with similar schemes. Moreover, the proposed scheme does not require the involvement of a trusted authority in the data aggregation. Security and performance analyses show that the proposed scheme can enhance the security of the sensed data while effectively aggregating data for real-time analytics in IIoT.},
  archive      = {J_COMCOM},
  author       = {Dengzhi Liu and Geng Yu and Zhaoman Zhong and Yuanzhao Song},
  doi          = {10.1016/j.comcom.2024.06.002},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {159-168},
  shortjournal = {Comput. Commun.},
  title        = {Secure multi-party computation with secret sharing for real-time data aggregation in IIoT},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a quantum-safe 5G: Quantum key distribution in core
networks. <em>COMCOM</em>, <em>224</em>, 145–158. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Service-Based Architecture (SBA) of the fifth generation (5G) of cellular networks introduced great advancements in flexibility, efficiency, and performance of modern networks, thanks to the distribution and softwarization of core network entities called Network Functions (NFs). Due to the distributed nature of 5G systems, these NFs are usually located in different sites to provide better Quality of Service (QoS) or specific services, which poses a great challenge regarding security. However, even though the 3GPP standard identifies security requirements by leveraging Transport Layer Security (TLS) authentication and OAuth2.0 authorization, current experimental and commercial deployments often disregard these aspects for the sake of simplicity and performance. In fact, even though data and communication protection is crucial, the implementation of security protocols and mechanisms complicates the deployments and may reduce the efficiency of 5G systems. This paper presents a novel solution that implements TLS for authentication and encryption leveraging entanglement-based Quantum Key Distribution (QKD), ensuring that all the communications between NFs in the 5G core network are secured by QKD keys, and are therefore quantum-safe. Additionally, this paper presents a QKD key repository using the Network Repository Function (NRF), which stores and relays TLS session keys to authenticated consumer NFs to access producer NFs in a fast but quantum-safe process. The solution was validated in a real hardware environment, and obtained results demonstrate that the proposed solution enhances the security of the SBA communications in an efficient way, reducing an 85% the service request time compared to the traditional TLS-based procedure, as well as 29.96% fewer bytes transmitted throughout the process.},
  archive      = {J_COMCOM},
  author       = {Asier Atutxa and Ane Sanz and Jorge Sasiain and Jasone Astorga and Eduardo Jacob},
  doi          = {10.1016/j.comcom.2024.06.005},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {145-158},
  shortjournal = {Comput. Commun.},
  title        = {Towards a quantum-safe 5G: Quantum key distribution in core networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COSIER: A comprehensive lightweight blockchain system for
IoT networks. <em>COMCOM</em>, <em>224</em>, 125–144. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) networks have spread to many important fields and applications. The security of these networks remains a big challenge due to their unique characteristics and delicate operations. The blockchain has recently been proposed as a system to secure IoT data and communications. However, the blockchain demands a heavy load that cannot be handled by light IoT devices. For this reason, a large number of “lightweight blockchain” solutions were proposed that aim to modify the blockchain structure and characteristics to make it suitable for IoT networks. In this paper, we propose a novel and unique blockchain system for IoT networks that is based on four lightweight features: “lightweight architecture”, “lightweight authentication”, “lightweight consensus”, and “lightweight storage”. While previous research works focused on one or two of these features, we propose a system that combines the four features into a comprehensive lightweight blockchain system that utilizes a “lightweight cryptography” mechanism to achieve better performance from several perspectives, such as transaction latency, block generation delay, network overhead, and resilience to attacks.},
  archive      = {J_COMCOM},
  author       = {Khaleel Mershad},
  doi          = {10.1016/j.comcom.2024.06.007},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {125-144},
  shortjournal = {Comput. Commun.},
  title        = {COSIER: A comprehensive lightweight blockchain system for IoT networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive simulated annealing-based computational
offloading scheme in UAV-assisted MEC networks. <em>COMCOM</em>,
<em>224</em>, 118–124. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) is an advanced technology that enables fifth-generation (5G) applications. It uses a mechanism to offload computation from mobile devices (MDs) to edge servers at the access point (AP) to improve the quality of computation. By leveraging the strengths of unmanned aerial vehicles (UAVs), we can install edge servers on UAVs to support task offloading. However, this is constrained by the limited computational resources of UAVs and high energy consumption. Due to the complexity of such architecture, the joint optimization of energy consumption and delay by applying classical optimization algorithms cannot work well either. So far, there is no research work that addresses the problem of computation offloading in UAV-based MEC networks considering the UAV-AP-MD architecture that is well applicable in 5G scenarios; this motivates us to propose this work. We first formulate this problem as an optimization problem and then develop an adaptive simulated annealing-based method for joint optimization of energy consumption and delay. We design a mechanism to dynamically increase the cooling rate as a function of decreasing temperature to achieve better convergence, and implement a task queue for MDs and servers. Finally, we perform numerical simulations and find that the proposed method reduces the energy consumption (17%) and delay (50%) of MDs and UAV while it has the best task dropped rate (50%) and fairness (4.2%) compared to other known methods.},
  archive      = {J_COMCOM},
  author       = {Ching-Kuo Hsu},
  doi          = {10.1016/j.comcom.2024.06.008},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {118-124},
  shortjournal = {Comput. Commun.},
  title        = {An adaptive simulated annealing-based computational offloading scheme in UAV-assisted MEC networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VSFL: Trajectory prediction framework based on
validity-aware semi-asynchronous federated learning in internet of
vehicles. <em>COMCOM</em>, <em>224</em>, 106–117. (<a
href="https://doi.org/10.1016/j.comcom.2024.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of vehicle trajectories on the Internet of Vehicles is crucial for improving traffic safety. Federated learning has been proposed to address the issue of islands in shared data. Traditional synchronous federated learning requires waiting for each node to complete training before model aggregation can occur, and the aggregation algorithm of the server cannot adjust the aggregation weight in real-time, which leads to increased communication overhead, time cost, and decreased accuracy. To balance these issues, this paper proposes a Validity-aware Semi-asynchronous Federated Learning-based framework for predicting vehicle trajectories. The framework makes the following improvements - firstly, based on the asynchronous cooperation algorithm of generalization research, the models of vehicle nodes within the same RoadSide Unit (RSU) range are aggregated into this RSU to reduce communication overhead and time costs. To further ensure the privacy and security of all participants, we added differential privacy algorithms during the model training process. Secondly, the server uses a weighted aggregation algorithm based on effective perception to aggregate all RSU models into a global model to address potential accuracy issues. This article uses a Social-LSTM like model to validate the effectiveness of this framework in the Next Generation Simulation (NGSIM) Vehicle Trajectories and Support Data dataset. Compared with traditional federated averaging algorithms and trust aware federated learning algorithms, this framework reduces distance errors by 27.80 % and 19.23 %, respectively, and improves accuracy by 22.56 % and 13.59 %.},
  archive      = {J_COMCOM},
  author       = {Yang Li and Xiaolong Xu and Gengjun Huang and Meiqi Yao and Lijuan Sun and Jian Xu},
  doi          = {10.1016/j.comcom.2024.06.003},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {106-117},
  shortjournal = {Comput. Commun.},
  title        = {VSFL: Trajectory prediction framework based on validity-aware semi-asynchronous federated learning in internet of vehicles},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IoT video analytics for surveillance-based systems in smart
cities. <em>COMCOM</em>, <em>224</em>, 95–105. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart city applications are revolutionizing the way people interact with diverse systems in city-wide applications. Internet of Things (IoT) and machine learning are two enabling technologies for smart cities. IoT sensors are deployed for data collection from city environments and machine learning models are used for data processing, inference, and predictions aimed at controlling IoT actuators and providing information for users. Surveillance applications in smart cities often use cloud and edge servers for the data analytics of video streamed by IoT cameras. This approach might increase the system’s latency due to the communication delay and networking congestion in the links connecting IoT cameras to distant cloud servers. A promising approach to mitigate the latency in video analytics applications is to perform the video analytics locally at the IoT devices and IoT gateways by using lightweight convolutional neural network (CNN) models. In this paper, we propose an architecture for implementing at IoT devices the data pipeline required for local video analytics in smart cities. Moreover, we instantiate the proposed framework to conduct an extensive performance evaluation of five CNN models on IoT devices, in a traffic surveillance smart city application. Obtained results show the feasibility of studied CNN models, in terms of frame processing rate and energy efficiency in the considered IoT devices.},
  archive      = {J_COMCOM},
  author       = {Kasra Aminiyeganeh and Rodolfo W.L. Coutinho and Azzedine Boukerche},
  doi          = {10.1016/j.comcom.2024.05.021},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {95-105},
  shortjournal = {Comput. Commun.},
  title        = {IoT video analytics for surveillance-based systems in smart cities},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identity management for internet of things: Concepts,
challenges and opportunities. <em>COMCOM</em>, <em>224</em>, 72–94. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of connected devices is growing exponentially, fueling the rise of diverse new Internet of Things (IoT) applications. While the benefits of IoT for society are undeniable, the lack of a proper Identity Management (IdM) system for IoT raises significant concerns about privacy and security. Traditionally, IdM systems have focused on managing people’s digital identities and regulating their access to web services. However, in the IoT context, applying traditional IdMs proves inadequate due to their incapacity to handle the sheer number of devices and the diversity of device types in IoT. Additionally, these systems often fail to address IoT environments’ unique security and privacy challenges, lacking scalability, availability, and robustness. In this survey, we provide a comprehensive state-of-the-art review of IdMs, explicitly presenting the fundamental concepts and challenges when developing an IdM for IoT applications. We offer an overview of the IdM concept, covering its objectives, components, and models, and then establish connections between these concepts and IoT characteristics. Subsequently, we delve into the primary challenges of adapting IdMs for IoT. We explore existing works that propose solutions to address at least one component of IdMs in response to the current challenges. Finally, the survey highlights current unresolved issues in this context, including the scalability of identity provisioning, authentication for IoT devices, and concerns regarding the performance and management of the authorization processes.},
  archive      = {J_COMCOM},
  author       = {Bruno Cremonezi and Alex B. Vieira and José Nacif and Edelberto Franco Silva and Michele Nogueira},
  doi          = {10.1016/j.comcom.2024.05.014},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {72-94},
  shortjournal = {Comput. Commun.},
  title        = {Identity management for internet of things: Concepts, challenges and opportunities},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective optimization of SFC deployment using service
aggregation and computing offload. <em>COMCOM</em>, <em>224</em>, 60–71.
(<a href="https://doi.org/10.1016/j.comcom.2024.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New technologies such as virtualization and Software Defined Networking (SDN) have given traditional networks unprecedented flexibility. A Service Function Chain (SFC) formed by concatenating multiple Virtual Network Functions (VNFs) expands network functions to meet the growing personalized network requirements of applications. However, due to the decentralization of VNF instances and the tight cloud load, how to ensure Quality of Service (Qos) while deploying the SFC brings new challenges. Hence, a SFC deployment strategy based on multi-objective optimization, named MO-SACO is proposed, which regards delay optimization, reliability assurance and cost reduction as the main objectives. Specifically, we first design a VNF aggregation rule to handle raw SFC requests, which effectively reduces the SFC path latency and improves reliability. Then, VNF placement and traffic routing is modeled as a computation offloading decision based on cloud-fog-edge collaboration with the goal of target optimization according to functional, location, and resource constraints. Finally, we also take into account the performance changes of deployed SFCs and handle non-compliant requests in a timely manner. We conduct extensive simulations in networks of varying sizes and the results demonstrate that the proposed MO-SACO strategy not only achieves lower latency and cost, higher reliability compared with the state-of-the-art methods, but also has unexpected performance in terms of the deployment success rate and node load.},
  archive      = {J_COMCOM},
  author       = {Junbi Xiao and Jiaqi Zheng and Wu Wen and Mohsen Guizani and Peiying Zhang and Lizhuang Tan},
  doi          = {10.1016/j.comcom.2024.05.017},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {60-71},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective optimization of SFC deployment using service aggregation and computing offload},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized provisioning technique of future services with
different QoS requirements in multi-access edge computing.
<em>COMCOM</em>, <em>224</em>, 42–59. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the present trend towards the use of microservices instead of monolithic software, future applications with diverse computational and delay-sensitive requirements, such as virtual reality (VR) or holographic communications (HC), will be redesigned based on loosely coupled microservices due to their more efficient deployment according to available resources. However, it remains a big challenge to provide such services under stringent quality of service (QoS) requirements at minimum time and cost and with an efficient utilization of the network. Presently, multi-access edge computing (MEC) has been proposed as a promising alternative to deploy future microservice-based applications to achieve faster response times while reducing the overall network load. However, network conditions, edge nodes state and service requests are dynamic, increasing the difficulty for the provisioning of services by MECs. In this context, we propose an optimization deployment heuristic (ODA) to obtain sub-optimal deployments in reasonable time, meeting the requirements of some representative future applications under study. In particular, according to latency requirements, there will be delay-tolerant and delay-sensitive applications, playing a major role regarding the alternatives for the deployment onto single or multiple MEC systems. The evaluation analysis shows that our proposal performs significantly better than other benchmark solutions in most of the metrics used for the experiments, especially in minimizing deployment costs in minimal time.},
  archive      = {J_COMCOM},
  author       = {John Bosco Ssemakula and Juan-Luis Gorricho and Godfrey Kibalya and Joan Serrat-Fernandez},
  doi          = {10.1016/j.comcom.2024.05.023},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {42-59},
  shortjournal = {Comput. Commun.},
  title        = {Optimized provisioning technique of future services with different QoS requirements in multi-access edge computing},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic task offloading and service caching based on game
theory in vehicular edge computing networks. <em>COMCOM</em>,
<em>224</em>, 29–41. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular edge computing (VEC) enables task offloading from vehicle to edge servers deployed on Road Side Units (RSUs) and to vehicle with idle resources, which is considered as a promising vehicle network architecture. However, efficient task offloading and task resource cache management in the VEC network is challenging. In this work, we formulate a joint task offloading and service caching problem (JTOSCP) for VEC with edge-vehicle cooperation, aiming to minimize the total task delay-energy value and cost value of all vehicles while guaranteeing task processing delay tolerance. The formulated JTOSCP is proven as NP-hard problem. Due to the complexity of JTOSCP, we propose a cooperative task service caching and task offloading algorithm named CO-MATCH. In this algorithm, we design the dynamic programming-based service caching (DPSC) Algorithm to incentivize edge services and volunteer vehicles to cache, which takes into account both service caching preferences and social similarity. The Many-to-One Matching Game (MOMG) Algorithm is proposed to stimulate edge device games and achieve the optimal task offloading. Then, the CO-MATCH method is proven to be stable. Simulation results demonstrate that our proposed approach can significantly improve the quality of service of user vehicles compared to the other methods.},
  archive      = {J_COMCOM},
  author       = {Chen Cheng and Linbo Zhai and Xiumin Zhu and Yujuan Jia and Yumei Li},
  doi          = {10.1016/j.comcom.2024.05.020},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {29-41},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic task offloading and service caching based on game theory in vehicular edge computing networks},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive supervised pedestrian detection algorithm for
green edge–cloud computing. <em>COMCOM</em>, <em>224</em>, 16–28. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel supervised pedestrian detection algorithm tailored for Green Edge–Cloud Computing (GECC), aimed at addressing the challenges associated with pre-trained models in real-world settings. These models often exhibit performance degradation due to the divergence between training datasets and complex real-world scenarios. To bridge this gap, our proposed algorithm employs a progressive occlusion-aware iterative training strategy that significantly enhances the representation of occluded pedestrians, a prevailing issue in dense urban environments. Additionally, we introduce a K-stratified hard sampling strategy, which accelerates the training process and substantially lowers the energy consumption required for model retraining. We also explore a refined non-maximal value suppression algorithm that employs neighboring centroids separation, improving the precision of pedestrian detection within crowded scenes. Through rigorous experiments on established pedestrian detection datasets, we demonstrate that our algorithm not only elevates pedestrian detection accuracy in high-density situations but also markedly reduces the model training duration.},
  archive      = {J_COMCOM},
  author       = {Liang She and Wei Wang and Jianyuan Wang and Zhili Lin and Yangyan Zeng},
  doi          = {10.1016/j.comcom.2024.05.022},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {16-28},
  shortjournal = {Comput. Commun.},
  title        = {Progressive supervised pedestrian detection algorithm for green edge–cloud computing},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DA2Wa: A secure pairing protocol between a DApp and a wallet
for the blockchain scenario. <em>COMCOM</em>, <em>224</em>, 1–15. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wallet applications play a crucial role in securely storing users’ private keys needed to interact with the blockchain, while decentralized applications (DApps) take profit of blockchain technologies to create transparent, tamper-proof environments without the need for trust relationships. As DApps need private keys to interact with the blockchain, the secure interconnection of these applications is vital yet still challenging. This article introduces DA2Wa, a protocol designed to establish a secure pairing between a cryptocurrency wallet and a DApp, both of which run as isolated applications on the same machine. The protocol utilizes a six-character PIN exchange mechanism, delivering a security level equivalent to that of Bluetooth. To demonstrate the security of DA2Wa, we employ Tamarin Prover, a tool for symbolically modelling and analysing security protocols.},
  archive      = {J_COMCOM},
  author       = {Fernando Román-García and Juan Hernández-Serrano and Oscar Esparza},
  doi          = {10.1016/j.comcom.2024.05.019},
  journal      = {Computer Communications},
  month        = {8},
  pages        = {1-15},
  shortjournal = {Comput. Commun.},
  title        = {DA2Wa: A secure pairing protocol between a DApp and a wallet for the blockchain scenario},
  volume       = {224},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LEO satellites selection-based computation offloading
algorithm in aircraft–satellite multi-access edge computing networks.
<em>COMCOM</em>, <em>223</em>, 115–127. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of sixth-generation (6G) mobile networks and non-terrestrial networks (NTNs) has led to increased interest in low Earth orbit (LEO) satellite-based communication networks for their potential to provide global coverage and ubiquitous connectivity. In this paper, we investigate the LEO satellites selection-based computation offloading problem in the aircraft–satellite multi-access edge computing (ASMEC) network, where LEO satellites and edge computing processors are integrated to provide ubiquitous and low-latency communication and computation services for aircraft during flights. In contrast to most existing works, which directly assume a fixed number of satellites or orbits in satellite-based MEC networks, we investigate the problem of how many satellites and which satellites to select in the ASMEC network. Our objective is to minimize the average total time delay of tasks during aircraft–satellite computation offloading. To achieve this, we formulate a nonlinear integer programming (NLIP) problem and propose the LEO satellites selection-based computation offloading (LSSBCO) algorithm to solve it, which includes the shortest aircraft–satellite distance based access satellite selection (ASS-SD) algorithm and the nearest k ( t ) k(t) neighboring satellites selection (NSS- k ( t ) k(t) ) algorithm. We evaluate the performance of the LSSBCO algorithm in terms of the average total time delay, the maximum throughput, the average aircraft–satellite distance, the average connection duration, and the number of satellite handovers. Numerical results show that the proposed algorithm outperforms the benchmark algorithms with a lower average total time delay.},
  archive      = {J_COMCOM},
  author       = {Jiadong Zhang and Ruidong Zhang and Wenxiao Shi},
  doi          = {10.1016/j.comcom.2024.05.011},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {115-127},
  shortjournal = {Comput. Commun.},
  title        = {LEO satellites selection-based computation offloading algorithm in aircraft–satellite multi-access edge computing networks},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SURA-LB: Software-defined IDS with UAV resource aware
load-balancing in FANET disaster scenarios. <em>COMCOM</em>,
<em>223</em>, 101–114. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In critical scenarios like natural disasters, the physical infrastructure of the network may be heavily damaged or completely torn down making difficult communications and rescue activities. In such cases, due to their nature, Unmanned Aerial Vehicles (UAVs) are usually employed to provide a temporary support network by forming a Flying Ad-Hoc Network (FANET). However, this network should be carefully monitored and safeguarded to guarantee its stability by deploying network security appliances, like Intrusion Detection Systems (IDSs). In the wake of these considerations, this paper delves into the utilization of IDS functions deployed on UAVs, named IDS-enabled UAVs, in emergencies leveraging the modern Software-Defined Network (SDN) paradigm. Specifically, the study proposes a dynamic approach to promptly activate IDS-enabled UAVs, in an SDN-FANET, responding to network traffic increasing and/or malicious activities taking place within the network. In order to achieve this objective, a Software-defined IDS with UAV Resource Aware Load Balancing strategy – S URA SURA -LB – is proposed and implemented within the SDN controller, leveraging its programmable nature and its holistic view of the network. The S URA SURA -LB strategy equally distributes the load among the set of IDS-enabled UAVs by considering their resources and energy utilization along with the experienced network traffic conditions. Finally, an extensive experimental campaign shows the benefits of the proposed load-balancing strategy in reducing and fairly distributing the workload among the IDS-enabled UAVs outperforming naive and baseline load-balancing strategies like Random and Round-Robin and, therefore, bolstering the resilience and the reliability of the set of IDS-enabled UAVs while the traffic to be monitored and analyzed increases.},
  archive      = {J_COMCOM},
  author       = {Mattia Giovanni Spina and Mauro Tropea and Floriano De Rango},
  doi          = {10.1016/j.comcom.2024.05.015},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {101-114},
  shortjournal = {Comput. Commun.},
  title        = {SURA-LB: Software-defined IDS with UAV resource aware load-balancing in FANET disaster scenarios},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QUIC on the fast lane: Extending performance evaluations on
high-rate links. <em>COMCOM</em>, <em>223</em>, 90–100. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QUIC is a new protocol standardized in 2021 designed to improve on the widely used TCP / TLS stack. The main goal is to speed up web traffic via HTTP, but it is also used in other areas like tunneling. Based on UDP, it offers features like reliable in-order delivery, flow and congestion control, stream-based multiplexing, and always-on encryption using TLS 1.3. Unlike TCP, QUIC integrates these capabilities in user space, relying on kernel interaction solely for UDP. Operating in user space allows more flexibility but sacrifices some kernel-level efficiency and optimization that TCP benefits from. Various QUIC implementations exist, each distinct in programming language, architecture, and design. QUIC is already widely deployed on the Internet and has been evaluated, focussing on low latency, interoperability, and standard compliance. However, benchmarks on high-speed network links are still scarce. This paper presents an extension to the QUIC Interop Runner, a framework for testing the interoperability of QUIC implementations. Our contribution enables reproducible QUIC benchmarks on dedicated hardware and high-speed links. We provide results on 10G links, including multiple implementations, evaluate how OS features like buffer sizes and NIC offloading impact QUIC performance, and show which data rates can be achieved with QUIC compared to TCP. Moreover, we analyze different CPUs and CPU architectures influence reproducible and comparable performance measurements. Furthermore, our framework can be applied to evaluate the effects of future improvements to the protocol or the OS. Our results show that QUIC performance varies widely between client and server implementations from around 50 Mbit/s to over 6000 Mbit/s. We show that the OS generally sets the default buffer size too small. Based on our findings, the buffer size should be increased by at least an order of magnitude. Our profiling analysis identifies Packet I/O as the most expensive task for QUIC implementations. Furthermore, QUIC benefits less from AES NI hardware acceleration while both features improve the goodput of TCP to around 8000 Mbit/s. The lack of support for NIC offloading from QUIC implementations results in missed opportunities for performance improvement. The assessment of CPUs from different vendors and generations revealed significant performance variations. We employed core pinning to examine if the performance of QUIC implementations is affected by the allocation to specific CPU cores. The results indicated an increased goodput of up to 20% when running on a specifically chosen core compared to a randomly assigned core. This outcome highlights the impact of CPU core selection on the performance of QUIC implementations but also for reproducible measurements.},
  archive      = {J_COMCOM},
  author       = {Marcel Kempf and Benedikt Jaeger and Johannes Zirngibl and Kevin Ploch and Georg Carle},
  doi          = {10.1016/j.comcom.2024.04.038},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {90-100},
  shortjournal = {Comput. Commun.},
  title        = {QUIC on the fast lane: Extending performance evaluations on high-rate links},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating coverage and capacity of high frequency mobile
networks in ultradense urban areas. <em>COMCOM</em>, <em>223</em>,
81–89. (<a href="https://doi.org/10.1016/j.comcom.2024.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High frequency communications (mmWave and TeraHz) in urban areas require a higher density of base stations compared to pre-5G mobile networks, but open the way to a quantum leap in increased throughput and reduced latency. However, we currently have no indication of how much we need to densify the deployment, and on the trade-off between the density of base stations and the performance improvement. This paper studies the problem of base stations placement to guarantee coverage to vehicles and pedestrians in urban areas when using high frequency communications. Our novel methodology takes advantage of vehicular traffic simulations and precise urban maps to generate a realistic demand model for vehicles and pedestrians in urban areas. We use a bounded error heuristic to find the maximal coverage that can be achieved with a given density of base stations, primarily using line-of-sight communications. We implemented the heuristic using CUDA libraries on Nvidia GPUs and evaluated the coverage in an urban area in the city of Luxembourg, for which vehicular traffic patterns are available. We focus on coverage and capacity analysis for the mmWave frequency, but the results are easily extended to TeraHz communications. Our results are the first to show that a reasonably low density (15 base stations per km 2 2 ) is sufficient to provide coverage for vehicles in urban environments. However, optimizing on vehicles or on pedestrians are competing objectives: the operator needs to choose which one to target based on its business model when designing the network infrastructure. Our algorithms, code and open data can be used to perform this task and reproduce our results in different settings.},
  archive      = {J_COMCOM},
  author       = {Gabriele Gemmi and Michele Segata and Leonardo Maccari},
  doi          = {10.1016/j.comcom.2024.04.030},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {81-89},
  shortjournal = {Comput. Commun.},
  title        = {Estimating coverage and capacity of high frequency mobile networks in ultradense urban areas},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cross-chain access control mechanism based on blockchain
and the threshold paillier cryptosystem. <em>COMCOM</em>, <em>223</em>,
68–80. (<a href="https://doi.org/10.1016/j.comcom.2024.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous maturation of blockchain technology and the increasing demands for various industry applications, data sharing and interoperability among different blockchain networks face significant challenges. Research on cross-chain interoperability mechanisms has facilitated data collaboration across organizations and industries, enhancing the value and utility of data. When engaging in cross-chain data interactions, access control ensures the security and privacy of data while promoting collaboration and information exchange among multiple chains. Attribute-based access control can provide fine-grained authorization support, matching complex business scenarios. However, publicly disclosed policies and attributes in a transparent blockchain network may pose privacy and security issues. To address these issues, this paper proposes a cross-heterogeneous multichain data access control scheme based on attributes and threshold homomorphic encryption, achieving fine-grained and secure cross-domain access control in cross-chain networks. This scheme uses the threshold Paillier cryptosystem to encrypt and conceal user attributes and access policies. Through smart contracts, homomorphic differential computation is performed on the ciphertext of policies and attributes, to protect data privacy. This solution leverages private key decryption shares from multiple relay nodes in the cross-chain network to jointly decrypt the computation results, providing secure access control in complex cross-chain scenarios. Security analysis and experimental results demonstrate that the proposed scheme ensures security with reasonable computational overhead, to meet the access control requirements in cross-chain networks.},
  archive      = {J_COMCOM},
  author       = {Haiping Si and Weixia Li and Nan Su and Tingting Li and Yanling Li and Chuanhu Zhang and Bacao Fernando and Changxia Sun},
  doi          = {10.1016/j.comcom.2024.05.012},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {68-80},
  shortjournal = {Comput. Commun.},
  title        = {A cross-chain access control mechanism based on blockchain and the threshold paillier cryptosystem},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAVs-assisted QoS guarantee scheme of IoT applications for
reliable mobile edge computing. <em>COMCOM</em>, <em>223</em>, 55–67.
(<a href="https://doi.org/10.1016/j.comcom.2024.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) assisted mobile edge computing (MEC) is an emerging network architecture that has been considered as a promising transformative service paradigm. It enhances the coverage of intelligent mobile communication network and promotes the rapid development of a wide range of internet of things (IoT) applications. However, the ever-increasing data transmission demands given rise by these IoT applications based on MEC have posed significant challenges to the service of quality (QoS) guarantee of UAVs-assisted MEC (UAVs-MEC) network. To address the issue, we propose a novel UAVs-MEC QoS guarantee scheme that can enhances the reliability of IoT applications in UAVs-MEC network. Specifically, a dynamic QoS-aware based service level agreement (SLA) compliance verification model for MEC service is proposed in the scheme. It can monitor and detect the consistency of dynamic QoS of IoT applications through UAVs to ensure the continuous SLA compliance of service provided by MEC. Additionally, an objective weight assignment method and an adaptive update method are presented to improve the accuracy and scalability of the scheme. Finally, the simulation experiments conducted using a real-world dataset show that the proposed scheme can efficiently and effectively verify the SLA compliance of MEC service for guaranteeing QoS of IoT applications in UAVs-MEC context while outperforms other existing SLA violation detection methods.},
  archive      = {J_COMCOM},
  author       = {Xiang Li and Peng Xiao and Di Tang and Xingguo Li and Qixu Wang and Dajiang Chen},
  doi          = {10.1016/j.comcom.2024.05.010},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {55-67},
  shortjournal = {Comput. Commun.},
  title        = {UAVs-assisted QoS guarantee scheme of IoT applications for reliable mobile edge computing},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the precision of time synchronization protocols in
ultra-wideband networks estimating the time of flight of the radio
signal. <em>COMCOM</em>, <em>223</em>, 44–54. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern applications of wireless sensor networks such as the conduction of collaborative tasks, the distributed detection of events or the alignment of data samples gathered by several nodes, require a precise time coordination of the nodes composing the network. Ultra Wide Band (UWB) technology has enabled the development of precise time synchronization protocols. The intrinsic properties of this technology allow an accurate estimation of the time of flight (ToF) of the radio signal in scenarios covering short distances. The integration of UWB transceivers in Wireless Sensor Networks (WSN) nodes is nowadays possible due to the availability of commercial chips implementing this technology. This combination facilitates the development of accurate time synchronization protocols that establish a global common clock throughout the network. In this context, the main contribution of this work is a protocol that allows the synchronization of the local clocks of these nodes with a common global reference kept in the master node. The master periodically broadcasts synchronization packets to keep the rest of the nodes synchronized. However, since the precision achieved in the time synchronization of the local clocks with UWB technology is comparable to the ToF of the RF signal for distances in the order of several meters, some considerations to estimate this delay are required. In this paper, a new algorithm is proposed that performs the estimation of the ToF of the UWB signal as well as the implementation of linear models to synchronize the local clocks of the network node, eliminating the dependence on the ToF. The experiments conducted with real devices reveal that an accuracy of tens of nanoseconds can be achieved regardless of the distance separating the network nodes.},
  archive      = {J_COMCOM},
  author       = {Juan J. Pérez-Solano and Antonio Soriano-Asensi and Santiago Felici-Castell and Jaume Segura-Garcia},
  doi          = {10.1016/j.comcom.2024.05.006},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {44-54},
  shortjournal = {Comput. Commun.},
  title        = {Improving the precision of time synchronization protocols in ultra-wideband networks estimating the time of flight of the radio signal},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HeavySeparation: A generic framework for stream processing
faster and more accurate. <em>COMCOM</em>, <em>223</em>, 36–43. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch as a probability data structure has been widely used in high volume, fast data streams. At the cost of a tiny accuracy in frequency estimation, it achieves a high speed with small memory usage. However, skewed data streams pose a significant challenge for existing sketches in terms of accuracy and speed using limited memory. To address this issue, we proposed a framework, called HeavySeparation, to enhance existing sketches by filtering elephant flow efficiently and accurately. We adopt a power-weakening increment strategy to allow sufficient competition in the early stages of identifying elephant flows and amplifying relative advantage when the frequency of candidate flow is large. To verify the effectiveness and efficiency of our framework, we apply the framework to two typical sketches and two common stream processing tasks. Results show that HeavySeparation framework reduces the error by around 1–2 orders of magnitude on average compared to the state-of-the-art in frequency estimation.},
  archive      = {J_COMCOM},
  author       = {Jie Lu and Hongchang Chen and Zhen Zhang},
  doi          = {10.1016/j.comcom.2024.04.036},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {36-43},
  shortjournal = {Comput. Commun.},
  title        = {HeavySeparation: A generic framework for stream processing faster and more accurate},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning dynamics of multi-level spatiotemporal graph data
for traffic flow prediction. <em>COMCOM</em>, <em>223</em>, 26–35. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {—Accurate and timely traffic flow prediction can effectively improve road network efficiency and alleviate a series of negative impacts caused by traffic congestion. Recent research has focused on exploring the spatiotemporal correlations of traffic data, and achieved some progress. However, due to the highly nonlinear and stochastic nature of traffic data, the most prediction methods can only learn the fixed physically connected relationships by predefined empirical matrices, but cannot fully capture the spatial correlation, especially the dynamic characteristics. In this paper, we propose a Multiscale Gated Spatiotemporal Graph Convolution Network (MS-GSTGCN). A self-learning adjacency matrix is constructed by the graph fusion module aggregating the information of the adaptive adjacency matrix and the innovative similarity adjacency matrix, which helps to capture the hidden features of the nodes and the dynamic spatial correlation. Meanwhile, a multilevel graph neural structure is designed to extract data features utilizing hierarchical framework, and a gating mechanism is introduced to minimize the accumulation of errors when the information propagates layer by layer, so as to enhance the model&#39;s ability to capture complex spatiotemporal correlations in long-term predictions. The experiments on two real world datasets show that it outperforms state-of-the-art related methods.},
  archive      = {J_COMCOM},
  author       = {Zhongbing Li and Yuli Wei and Guihui Chen and Kai Lu and Xinyu Zheng},
  doi          = {10.1016/j.comcom.2024.05.007},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {26-35},
  shortjournal = {Comput. Commun.},
  title        = {Learning dynamics of multi-level spatiotemporal graph data for traffic flow prediction},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy enhanced data aggregation based on federated
learning in internet of vehicles (IoV). <em>COMCOM</em>, <em>223</em>,
15–25. (<a href="https://doi.org/10.1016/j.comcom.2024.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Internet of Vehicles (IoV) environment continues to evolve, artificial intelligence (AI) technologies have been utilized to provide several applications such as traffic flow prediction and vehicular object detection. Cyberattacks are on the rise since aggregated datasets from multiple vehicles may contain a significant amount of information related to privacy. Federated learning (FL) has gained more attention as it enables data training on local devices without sharing the actual datasets. However, there are still challenges to security, privacy, and heavy communication overhead when applying FL to the IoV. To cope with these issues, we propose a new privacy-enhanced data aggregation scheme based on FL. We apply not only additive secret sharing (ASS) with homomorphic encryption (HE) but also asymmetric encryption such as RSA to guarantee the privacy and security of aggregation results. In our proposed scheme, a roadside unit (RSU), which serves as a group leader, groups vehicles and then chooses qualifying vehicles to not only ensure efficiency with high accuracy but also to reduce communication overhead and improve the scalability of a large number of vehicles. After the initial training that involves all RSUs and selected vehicles, only a randomly selected group is able to update the global model parameter by reflecting on training their local model. It is also helpful not only to reduce communication overhead while keeping acceptable prediction errors but also to prevent continuous receiving from malicious vehicles. The results of security and performance evaluations demonstrate that our proposed scheme is effective in enhancing privacy and efficient in operation.},
  archive      = {J_COMCOM},
  author       = {Hyeran Mun and Kyusuk Han and Ernesto Damiani and Tae-Yeon Kim and Hyun Ku Yeun and Deepak Puthal and Chan Yeob Yeun},
  doi          = {10.1016/j.comcom.2024.05.009},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {15-25},
  shortjournal = {Comput. Commun.},
  title        = {Privacy enhanced data aggregation based on federated learning in internet of vehicles (IoV)},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A privacy-preserving federated learning protocol with a
secure data aggregation for the internet of everything. <em>COMCOM</em>,
<em>223</em>, 1–14. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although there are significant advantages to the popular use of connected devices on the Internet of Everything, there remain distinct concerns surrounding privacy. Federated learning has been one of the solutions suggested to remediate such issues and looks promising given the collaborative model it provides, which is shared across local gradients G r d Grd s without the exposure of raw data. However, the shared G r d Grd s threaten the local data with a privacy risk. The central server could produce summarized results. Besides, in federated learning devices with resource constraints frequently dropout. There are only two possible solutions now: efficiency or privacy preservation. Designing a verifiable secure aggregation on the scale of federated learning is still a difficult test. Here, an efficient privacy-preserving federated learning protocol is proposed, with a secure data aggregation for the Internet of Everything. Aggregator encryption effectively masks the local G r d Grd s of clients. The central server aggregates the obscured G r d Grd s without exposing the local data. At the same time, clients can effectively verify whether the aggregated result is correct. Secondly, the suggested protocol employs a group management mechanism to tolerate the dropout of clients without negatively affecting their further participation in subsequent learning activities. Security analysis shows that the protocol guarantees privacy-preserving federated learning’s security demand. The results of experiments carried out on datasets reveal the highly efficient practical performance of the proposed protocol.},
  archive      = {J_COMCOM},
  author       = {Sultan Basudan},
  doi          = {10.1016/j.comcom.2024.05.005},
  journal      = {Computer Communications},
  month        = {7},
  pages        = {1-14},
  shortjournal = {Comput. Commun.},
  title        = {A privacy-preserving federated learning protocol with a secure data aggregation for the internet of everything},
  volume       = {223},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-agent reinforcement learning based computation
offloading and resource allocation for LEO satellite edge computing
networks. <em>COMCOM</em>, <em>222</em>, 268–276. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations caused by geographical conditions and economic requirements, it is difficult to provide computing services by terrestrial networks for mobile terminals in remote areas. To address this issue, mobile edge computing (MEC) servers can be deployed in the low earth orbit (LEO) satellites to act as a complement and accommodate the unserved terminals. However, offloading computing tasks to servers in satellites may increase the energy consumption of ground terminals. Considering the limited battery capacity of ground terminals, how to perform the computation offloading and resource allocation are key challenges in the LEO satellite edge computing networks. Therefore, in this paper, we investigate the energy minimization problem for LEO satellite edge computing networks, where a multi-agent deep reinforcement learning algorithm with global rewards is proposed to optimize the transmit power, CPU frequency, bit allocation, offloading decision and bandwidth allocation via a decentralized method. Simulation results show that our proposed algorithm can converge faster. Most importantly, compared with the random algorithm, the proximal policy optimization (PPO) algorithm, and the deep deterministic policy gradient (DDPG) algorithm, the ground terminals’ energy consumption can be effectively reduced by our proposed algorithm.},
  archive      = {J_COMCOM},
  author       = {Hai Li and Jinyang Yu and Lili Cao and Qin Zhang and Zhengyu Song and Shujuan Hou},
  doi          = {10.1016/j.comcom.2024.05.008},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {268-276},
  shortjournal = {Comput. Commun.},
  title        = {Multi-agent reinforcement learning based computation offloading and resource allocation for LEO satellite edge computing networks},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic channel aggregation strategy with a maximum
aggregation number in cognitive radio networks based on channel
classification. <em>COMCOM</em>, <em>222</em>, 256–267. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional cognitive radio networks (CRNs), users are generally divided into two categories: primary users (PUs) and secondary users (SUs). The transmission rates of different channels may vary due to the complexities of network transmissions. Therefore, in this paper, a channel classification mechanism that divides the channels in the system into high-speed channels and low-speed channels according to their transmission capacity is proposed. In addition, to improve the performance of users, a dynamic channel aggregation strategy for CRNs with two types of channels is proposed. In this strategy, packets first consider accessing high-speed channels. We set a fixed channel aggregation strategy for PU packets and a dynamic channel aggregation strategy for SU packets. The number of channel aggregation of SU packets is restricted by the maximum aggregation number. Based on the proposed strategy, a multi-service discrete-time queueing model is established and analyzed, and the performance index expressions of PU packets and SU packets are derived. Moreover, we conduct numerical experiments, obtain the system performance figures and analyze the system performance. Finally, the performance of the proposed channel aggregation strategy is compared with that of the non-aggregation strategy, verifying the effectiveness of the proposed channel aggregation strategy.},
  archive      = {J_COMCOM},
  author       = {Yuan Zhao and Shuangshuang Yuan and Yan Li and Hongmin Gao},
  doi          = {10.1016/j.comcom.2024.05.001},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {256-267},
  shortjournal = {Comput. Commun.},
  title        = {A dynamic channel aggregation strategy with a maximum aggregation number in cognitive radio networks based on channel classification},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of mobility robustness optimization in ultra-dense
heterogeneous networks. <em>COMCOM</em>, <em>222</em>, 241–255. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stable and reliable wireless connections during the user equipment&#39;s mobility are the critical issue in future mobile communication networks especially during UE&#39;s high-speed scenarios over dense heterogeneous networks (HetNets). Thus, setting an accurate value for the handover control parameters (HCPs) (i.e., time-to-trigger (TTT) and handover margin (HOM)) at different speed scenarios is required for system performance. This study proposes six different systems and each one employs different HCP settings using various mobile speed scenarios over a HetNet. Several performance metrics (i.e., received signal reference power (RSRP), handover ping-pong (HOPP), radio link failure (RLF), handover probability (HOP), handover interruption time (HOIT), and handover failure (HOF)) were used as key performance indicators (KPIs) for systems evaluations. Furthermore, 20 users were evaluated in this study using a 40 ms measurement interval. The results show a different impacts of each system on the performance of the deployed HetNet. However, System 1 (lowest TTT &amp; moderate HOM) was the best system in term of RSRP and RLF but on the expense of HOPP. Furthermore, assigning high level of the TTT and HOM (i.e., TTT &gt;1000 ms and HOM &gt;8 dB) leads to 0 HOPP but high RLFs. Moreover, the best system performance in term of HOP, HOIT, and HOF were System 6 (highest TTT &amp; high HOM) with 0.06 %, 3 %, and 0.004 %, respectively. System 4 represents the best trading off system between HOPP and RLF due to the proper configuration of the HCPs. However, the aims of evaluating several systems based on fixed HCPs over several mobile speed scenarios were to see the behaviors of the systems performance when different setting values are applied. Furthermore, this study proposes a HO self-optimization algorithm for auto-tuning the HCPs. Weighted function algorithm along with a trigger timer is used for reducing unnecessary HOs while RLF minimized by setting a trigger timer under a certain conditions when the RSRP of the serving base station (BS) goes below the received signal strength indicators (RSSI). The proposed algorithm shows a significant improvement in term of HOPP, RLF, HOP, and HOF compared to System 4 and FLC algorithm. The RSRP of the proposed algorithm is kept within acceptable range. Thus, accurate setting values for the HCPs are significant to keep the tradeoff between RLF and HOPP at the minimum level.},
  archive      = {J_COMCOM},
  author       = {Waheeb Tashan and Ibraheem Shayea and Sultan Aldirmaz-Colak},
  doi          = {10.1016/j.comcom.2024.04.033},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {241-255},
  shortjournal = {Comput. Commun.},
  title        = {Analysis of mobility robustness optimization in ultra-dense heterogeneous networks},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource management for computational offload in MEC
networks with energy harvesting and relay assistance. <em>COMCOM</em>,
<em>222</em>, 230–240. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC) networks, the integrated application of energy harvesting (EH) and relay cooperation is a promising technology, which ensures the real-time performance of the system and computing power supply. Relay and EH cooperation also expand communication range and extend the battery life of energy-constrained devices. However, in wireless powered and relay-assisted MEC networks, it is challenging to jointly dispatch energy, and computing resources to coordinate heterogeneous performance requirements. To fill this gap, this paper examines the fundamental compromise between utility energy efficiency (UEE) and stable queue length in wireless powered and relay-assisted MEC network systems, and the amount of data transferred for each unit of energy is called UEE. The data queue and energy queue models are introduced and the constraints of stability of two queues in long term are included in the formulated problem. To tackle the fractional and nonconvex optimization problems, the Dinkelsbach method is utilized. Since optimization problems change from time to time and the long-term averaging queue stability is considered, the Lyapunov optimization theory is introduced to convert the non-convex problems into the solvable one. An online resource offloading allocation algorithm is proposed to determine the solution. In addition, the algorithm implements the control parameter V to achieve the compromise between UEE and stable queue length, while the impact of various parameters are also revealed on system performance.},
  archive      = {J_COMCOM},
  author       = {Zhixin Liu and Yuanzi Wu and Jiawei Su and Zhaobin Wu and Kit Yan Chan},
  doi          = {10.1016/j.comcom.2024.05.004},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {230-240},
  shortjournal = {Comput. Commun.},
  title        = {Resource management for computational offload in MEC networks with energy harvesting and relay assistance},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3GPP edge–fog federation: Transparent 3rd-party
authentication and application mobility. <em>COMCOM</em>, <em>222</em>,
220–229. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3GPP edge and fog computing paradigms provide computational services to users at low latency. These paradigms alone are not enough to fulfill the users’ requirements completely. Therefore, a federation among these computing paradigms is necessary. To realize such federation, there is a need of an authentication mechanism where subscribers of a fog, can access 3GPP edge’s services, or vice versa, without buying new subscription, and an application mobility mechanism for continuous service during handover from 3GPP edge to fog, or vice versa, without re-authentication. In this work, 3 we propose: (1) a proxy-based state transfer and third-party authentication (PS3A), that uses a transparent proxy to transfer the authentication and application state information between 3GPP edge and fog, and (2) a token-based state transfer and proxy-based third-party authentication (TSP3A), that uses the proxy to transfer the authentication information and tokens to transfer application state information between 3GPP edge and fog. The proxy plays different roles, via virtual counterparts of entities involved in these protocols, to provide transparency. When the 3GPP edge, using EPS-AKA, receives an authentication request, the proxy relays and behaves as a virtual Home Subscriber Server (vHSS) for the 3GPP edge, and behaves as a virtual user for the fog, which is using OIDC. We applied PS3A and TSP3A to three federation scenarios among 3GPP edge and fog. Experimental results show that PS3A and TSP3A provide authentication within 0.345–2.858 s for a 0–100 Mbps proxy load. The results further show that TSP3A provides application mobility, while taking 40%–52% less time than PS3A, using state tokens. TSP3A and PS3A also reduce the service interruption latency by 82.4% and 84.6%, compared to the cloud-based service, via tokens and prefetching.},
  archive      = {J_COMCOM},
  author       = {Asad Ali and Minhajul Islam and Tushin Mallick and Mohammad Sakibul Islam and Sadman Sakib and Md. Shohrab Hossain and Ying-Dar Lin},
  doi          = {10.1016/j.comcom.2024.05.002},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {220-229},
  shortjournal = {Comput. Commun.},
  title        = {3GPP Edge–Fog federation: Transparent 3rd-party authentication and application mobility},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A secure and lightweight cloud data deduplication scheme
with efficient access control and key management. <em>COMCOM</em>,
<em>222</em>, 209–219. (<a
href="https://doi.org/10.1016/j.comcom.2024.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication technology is extensively employed to enhance the storage efficiency of cloud servers by eliminating redundant files. Cloud users commonly encrypt their data prior to uploading it to the server. Conventional encryption algorithms, however, lead to the encryption of duplicated data from different users into distinct ciphertexts. Consequently, these ciphertexts must be stored in the cloud since the cloud server cannot identify such duplicated data. In this paper, we introduce a hybrid cloud-based secure deduplication scheme tailored for implementation on large-scale data systems. Specifically, our approach leverages ciphertext-policy attribute-based encryption (CP-ABE), which enables us to establish access control and key management via a private cloud server. Simultaneously, we leverage a public cloud server to cater to enterprises and groups seeking secure data storage. Notably, our approach ensures mutual zero-interaction verification between both public and private cloud servers through ElGamal encryption, thereby guaranteeing data unforgeability. The security assessment illustrates that our proposed approach ensures both data privacy and integrity. We also show that the approach resists brute-force attacks on the dictionary, prevents malicious users from deceiving cloud servers to return incorrect ciphertext, and achieves secure and efficient access control and key management. Furthermore, functional and performance evaluation underscores the superiority of our method over five other classical data deduplication schemes. Under the premise of having more comprehensive security settings, the performance of the scheme still maintains a good level at every stage.},
  archive      = {J_COMCOM},
  author       = {Xinyu Tang and Cheng Guo and Kim-Kwang Raymond Choo and Xueru Jiang and Yining Liu},
  doi          = {10.1016/j.comcom.2024.05.003},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {209-219},
  shortjournal = {Comput. Commun.},
  title        = {A secure and lightweight cloud data deduplication scheme with efficient access control and key management},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating TCP BBRv3 performance in wired broadband
networks. <em>COMCOM</em>, <em>222</em>, 198–208. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Introduced by Google in 2016, the first version of the Bottleneck Bandwidth and Round-Trip Time (BBRv1) congestion control algorithm (CCA) marked a significant advancement in network communication. Unlike traditional loss-based CCAs such as CUBIC and Reno, BBRv1 focused on balancing throughput and delay without relying on packet losses as a congestion signal. However, BBRv1 faced fairness issues due to its aggressiveness when interacting with loss-based CCAs. To address this issue, BBRv2 was developed, incorporating multiple metrics to improve fairness with loss-based CCAs. Despite improvements, BBRv2 encountered bugs and performance limitations, prompting the release of BBRv3 in 2023. BBRv3 addressed bugs found in BBRv2 and fine-tuned parameters to enhance flow coexistence. This paper evaluates the performance of BBRv3 across diverse network scenarios by comparing it with CUBIC and further contrasts this comparison with those involving CUBIC against BBRv2 and BBRv1. The evaluation explores BBRv3’s behavior under different conditions, considering variations in the number of flows, propagation delays, loss rates, and buffer sizes. Additionally, this paper explores the influence of Active Queue Management (AQM) algorithms in addressing the RTT unfairness issue. The results indicate that BBRv3’s coexistence with CUBIC is comparable to that observed in BBRv2, and it depends on factors such as buffer size and the number of flows. BBRv3 maintains high throughput and lower retransmissions at 1% loss rates compared to its predecessors. Moreover, BBRv3 consistently keeps low queue occupancy and achieves low Flow Completion Times (FCTs) in scenarios with short and long flows, even with large buffer sizes.},
  archive      = {J_COMCOM},
  author       = {Jose Gomez and Elie F. Kfoury and Jorge Crichigno and Gautam Srivastava},
  doi          = {10.1016/j.comcom.2024.04.037},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {198-208},
  shortjournal = {Comput. Commun.},
  title        = {Evaluating TCP BBRv3 performance in wired broadband networks},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning-based dynamic load balancing in edge
computing networks. <em>COMCOM</em>, <em>222</em>, 188–197. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing (EC) has emerged as a paradigm aimed at reducing data transmission latency by bringing computing resources closer to users. However, the limited scale and constrained processing power of EC pose challenges in matching the resource availability of larger cloud networks. Load balancing (LB) algorithms play a crucial role in distributing workload among edge servers and minimizing user latency. This paper presents a novel set of distributed LB algorithms that leverage machine learning techniques to overcome the three limitations of our previous LB algorithm, EVBLB: (i) its reliance on static time intervals for execution, (ii) the need for comprehensive information about all server resources and queued requests for neighbor selection, and (iii) the use of a central coordinator to dispatch incoming user requests over edge servers. To offer increased control, custom configuration, and scalability for LB on edge servers, we propose three efficient algorithms: Q-learning (QL), multi-armed bandit (MAB), and gradient bandit (GB) algorithms. The QL algorithm predicts the subsequent execution time of the EVBLB algorithm by incorporating rewards obtained from previous executions, thereby improving performance across various metrics. The MAB and GB algorithms prioritize near-optimal neighbor node servers while considering dynamic changes in request rate, request size, and edge server resources. Through simulations, we evaluate and compare the algorithms in terms of network throughput, average user response time, and a novel LB metric for workload distribution across edge servers.},
  archive      = {J_COMCOM},
  author       = {Mohammad Esmaeil Esmaeili and Ahmad Khonsari and Vahid Sohrabi and Aresh Dadlani},
  doi          = {10.1016/j.comcom.2024.04.009},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {188-197},
  shortjournal = {Comput. Commun.},
  title        = {Reinforcement learning-based dynamic load balancing in edge computing networks},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing human–robot communication with a comprehensive
language-conditioned imitation policy for embodied robots in smart
cities. <em>COMCOM</em>, <em>222</em>, 177–187. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Embodied Robots into a smart city’s networked system can significantly enhance the city’s operational efficiency. These robots can be connected to the city’s network, receiving and transmitting data in real time for enhancing human–robot communications. Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. However, most language-conditioned policy-related research is limited to specific datasets and cannot generalize across different environments. In this study, we propose a novel imitation learning framework tailored for language-conditioned robotic tasks. Our framework includes specialized encoders designed for various benchmarks and utilizes two distinct models: the Transformer and Diffusion models. We rigorously evaluate this framework in three different robotic environments. Our findings indicate that the framework consistently delivers superior performance across multiple domains. Notably, we observe that the Transformer model is particularly effective in managing tasks with long trajectories, whereas the Diffusion model demonstrates enhanced proficiency in generating trajectories from limited training datasets. Our approach showcases remarkable generalization capabilities across a range of tasks and achieves significantly higher success rates in task completion.},
  archive      = {J_COMCOM},
  author       = {Zhaoxun Ju and Hongbo Wang and Jingjing Luo and Fuchun Sun},
  doi          = {10.1016/j.comcom.2024.04.029},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {177-187},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing human–robot communication with a comprehensive language-conditioned imitation policy for embodied robots in smart cities},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain and differential privacy-based data processing
system for data security and privacy in urban computing.
<em>COMCOM</em>, <em>222</em>, 161–176. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, big data related to human movement, air quality, and meteorology have been generated in urban computing through sensing technology and the computing infrastructure. However, security problems arise as data utilization increases. If the sensing data from internet of things devices are constantly exposed, the users’ private information can be determined, a critical security risk that could result in privacy breaches. This paper proposes a secure data processing system using the blockchain and differential privacy for data security and privacy protection in urban computing. When a service provider requests information, the system generates it from urban computing data using machine learning. We apply differential privacy to these data to protect privacy. However, if a query repeats, differential privacy may provide insufficient privacy protection. Therefore, we reduce the total privacy cost by reusing noise for the same data and privacy parameters using the blockchain. Machine learning accuracy may decrease when noisy data are used for training. Thus, we increase accuracy by storing and appropriately using the model parameters generated by the same data in the blockchain. We design, simulate, and analyze the results of an experimental environment for reusing noise for differential privacy and parameter utilization of machine learning using the blockchain. The proposed approach reduces privacy costs compared to the existing mechanism while protecting data privacy. We demonstrate that, through parameter utilization, the accuracy improves compared to conventional mechanisms.},
  archive      = {J_COMCOM},
  author       = {Gabin Heo and Inshil Doh},
  doi          = {10.1016/j.comcom.2024.04.027},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {161-176},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain and differential privacy-based data processing system for data security and privacy in urban computing},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Security analysis and prediction of multi-relay networks
over fisher–snedecor f fading channels. <em>COMCOM</em>, <em>222</em>,
150–160. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the physical layer security (PLS) of multi-relay networks over Fisher–Snedecor F F fading channels. Precise expressions for outage probability (OP), intercept probability (IP), secure outage probability (SOP), and probability of strictly positive secrecy capacity (SPSC) are derived based on this considered model, which the SOP and SPSC are provided in a unified manner using Meijer’s G-function. Moreover, simulation consequences of Monte Carlo experiments validate derived mathematical expressions. Through theoretical analysis and simulation results, the interesting results are that larger m s R D msRD , smaller m R D mRD , μ R E μRE and m s R E msRE the better secrecy performance of this system. Because deep learning has characteristics of low latency, strong learning ability, and adaptability, in order to pursue lower latency and universal applicability, we combine deep learning and wireless communication. On the basis of constructing a data set with exact closed-form expressions, the secrecy performance is predicted by utilizing a multi-layer perceptron (MLP) deep learning model. Moreover, by comparing the three algorithms of convolutional neural network (CNN), deep neural network (DNN), and two-layer long short-term memory network (LSTM), the MLP has the advantages of higher performance and lower complexity. Experimentations exhibit that the presented algorithm enhances the precision by 75.69% with LSTM for comparison and reduces the time complexity by 80.05% compared with DNN.},
  archive      = {J_COMCOM},
  author       = {Yanyang Zeng and Dawei Zhang and Kai Tang and Jiangfeng Sun},
  doi          = {10.1016/j.comcom.2024.04.032},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {150-160},
  shortjournal = {Comput. Commun.},
  title        = {Security analysis and prediction of multi-relay networks over Fisher–Snedecor f fading channels},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-efficient RAN slicing for service provisioning in
5G/B5G. <em>COMCOM</em>, <em>222</em>, 141–149. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing represents a substantial technological advance in 5G mobile network, greatly expanding the variety and manifoldness of network services to be supported. Additionally, 3GPP 5G New Radio (NR) has introduced novel features such as mixed numerology and mini-slots, which can be harnessed by network slicing to cater to the diverse requirements of 5G services. While however the co-existence of multiple network slices leads to a challenging resource allocation problem, these new features also severely complicate the management of radio resources. As a further point of attention, the virtualization of radio functions may exact a significant toll from the, already limited, computing resources at the network edge. It follows that a cost-efficient resource allocation across all the slices becomes crucial. In this paper, we address the above-mentioned issues by modeling a cost-efficient radio resource management in 5G NR featuring network slicing, named CERS, through a Mixed Integer Quadratically constrained Program (MIQCP). We maximize the profit of all slices simultaneously guaranteeing the target data rate and delay specified in the service level agreements (SLAs) fo the different traffic flows. To reduce the complexity of the MIQCP problem, we decompose it into two sub-problems, namely, the scheduling problem of enhanced Mobile Broadband (eMBB) user equipments (UEs) on a time-slot basis and of Ultra-Reliable Low Latency Communications (uRLLC) UEs on a mini-slot basis, while keeping the objective unchanged. To address the scheduling issue of eMBB UEs, we employ a heuristic technique, and, by leveraging the outcome of this heuristic, we derive an optimal solution for the problem of uRLLC UEs. The significance of the proposed approach over a baseline approach is evaluated through extensive numerical simulations in terms of the number of allocated uRLLC resource blocks (RBs) per mini-slot. We also assess our approach by measuring the impact of the uRLLC slice changes on the eMBB slice, and vice versa, including delay for uRLLC users and data rates for eMBB users.},
  archive      = {J_COMCOM},
  author       = {Somreeta Pramanik and Adlen Ksentini and Carla Fabiana Chiasserini},
  doi          = {10.1016/j.comcom.2024.04.026},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {141-149},
  shortjournal = {Comput. Commun.},
  title        = {Cost-efficient RAN slicing for service provisioning in 5G/B5G},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of BLE-based audio broadcasting under
probabilistic interference. <em>COMCOM</em>, <em>222</em>, 130–140. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Bluetooth audio communication has become an inherent part of everyday life, from listening to a podcast on our headphones to streaming music on multiple speakers. Up until now, broadcasting audio to multiple receivers has always required a proprietary implementation. Therefore, recent advances in the specification allow Broadcast Audio streams to be set up on top of Bluetooth Low Energy (BLE). To cope with the unpredictability of wireless media, audio frame retransmission opportunities are provided. However, determining the applicable number of retransmissions for a broadcast stream that is exposed to volatile environmental conditions, is a complex research challenge. This paper presents a model that is capable of simulating a BLE broadcast stream schedule, exposed to various environmental conditions while using a variable number of audio frame retransmissions. The evaluation employs several existing Packet Loss Concealment (PLC) techniques to cope with audio frame losses. The results provide insights into the impact of various frame loss patterns on the audio quality and intelligibility of broadcasted speech. The more advanced PLC techniques can handle a higher frame loss rate threshold. The analysis also shows that large audio frames requiring fragmentation exhibit a higher amount of frame loss for the same BLE packet loss rate and that overlap with in-use 802.11 channels can lead to a large variability in frame loss behavior. The model provides a baseline for the next research challenge related to continuous management of Broadcast Audio streams operating under volatile environmental conditions.},
  archive      = {J_COMCOM},
  author       = {Mathias Baert and Bart Moons and Jowan Pittevils and Yanjue Song and Nilesh Madhu and Jeroen Hoebeke},
  doi          = {10.1016/j.comcom.2024.04.034},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {130-140},
  shortjournal = {Comput. Commun.},
  title        = {Evaluation of BLE-based audio broadcasting under probabilistic interference},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Autonomous integration of TSN-unaware applications with QoS
requirements in TSN networks. <em>COMCOM</em>, <em>222</em>, 118–129.
(<a href="https://doi.org/10.1016/j.comcom.2024.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern industrial networks transport both best-effort and real-time traffic. Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic. In a TSN network, applications signal their QoS requirements to the network before transmitting data. The network then allocates resources to meet these requirements. However, TSN-unaware applications can neither perform this registration process nor profit from TSN’s QoS benefits. The contributions of this paper are twofold. First, we introduce a novel network architecture in which an additional device acts as a central user configuration (CUC) for TSN-unaware applications and autonomously signals their QoS requirements to the network. Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling. It leverages a Deep Recurrent Neural Network (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application. As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees. Our evaluations underline the effectiveness of the proposed architecture and processing method.},
  archive      = {J_COMCOM},
  author       = {Moritz Flüchter and Steffen Lindner and Lukas Osswald and Jérôme Arnaud and Michael Menth},
  doi          = {10.1016/j.comcom.2024.04.021},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {118-129},
  shortjournal = {Comput. Commun.},
  title        = {Autonomous integration of TSN-unaware applications with QoS requirements in TSN networks},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-power timely random access: Packet-based or
connection-based? <em>COMCOM</em>, <em>222</em>, 108–117. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies low-power random access protocols for timely status update systems with information freshness requirements, measured by age of information (AoI). A fundamental challenge for such networks is to schedule a large number of transmitters to access the wireless channel in a way that achieves a low network-wide AoI at low power consumption. Conventional packet-based random access protocols involve transmitters contending for the channel by sending their entire data packets. When packets are of long duration, the time and energy wasted due to packet collisions is considerable. In contrast, connection-based random access protocols establish connections with the receiver before transmitting data packets. From an information freshness perspective, there should be conditions that favor one approach over the other. Therefore, we conduct a comparative study of the average AoI of packet-based and connection-based random access protocols. Specifically, we consider frame slotted Aloha (FSA) as a representative of packet-based random access and design a request-then-access (RTA) protocol for connection-based random access. Our analyses indicate that the choice between packet-based and connection-based protocols depends mainly on the payload size of update packets and the transmit power budget. In particular, RTA significantly reduces AoI and saves power, especially when the payload size is large. Overall, our investigation offers insights into the practical design of random access protocols for low-power timely status update systems.},
  archive      = {J_COMCOM},
  author       = {Tse-Tin Chan and Jian Feng and Haoyuan Pan},
  doi          = {10.1016/j.comcom.2024.04.017},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {108-117},
  shortjournal = {Comput. Commun.},
  title        = {Low-power timely random access: Packet-based or connection-based?},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ER-OCN: Toward efficient network routing in ocean city based
on deep reinforcement learning. <em>COMCOM</em>, <em>222</em>, 97–107.
(<a href="https://doi.org/10.1016/j.comcom.2024.04.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart ocean cities are crucial to future city development, and offshore networks are an essential part of smart ocean cities. Nowadays, communication in offshore networks is becoming increasingly important. However, the offshore networks are highly heterogeneous, with low reliability, narrow bandwidth, and dramatic dynamics. Offshore networks are more challenging to manage than traditional terrestrial networks, and the standard quality of service routing faces significant challenges. Currently, artificial intelligence provides a way to meet the requirements. This paper proposes ER-OCN ( E fficient R outing in O cean C ity N etworks), a two-stage routing optimization scheme driven by artificial intelligence to cope with the complex offshore networks environment. The first stage will calculate the flow paths based on Lagrangian relaxation. Besides, the ocean environment is under dramatic dynamics, leading to unpredictable network topology or link bandwidth changes. Thus, the second stage deployed a deep reinforcement learning algorithm to monitor the environment in real-time and update the routing strategy when needed. We conducted experiments based on actual network topology and traffic data sets of offshore platforms’ network systems. The results prove that our scheme improves communication cost performance by 36.61% compared to traditional methods.},
  archive      = {J_COMCOM},
  author       = {Shu Yang and Yaofeng Liu and Laizhong Cui and Yidong Peng and Zhiqiang Li and Victor C.M. Leung},
  doi          = {10.1016/j.comcom.2024.04.031},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {97-107},
  shortjournal = {Comput. Commun.},
  title        = {ER-OCN: Toward efficient network routing in ocean city based on deep reinforcement learning},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DoS/DDoS attacks in software defined networks: Current
situation, challenges and future directions. <em>COMCOM</em>,
<em>222</em>, 77–96. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Network (SDN) is a perception of networking that separates the network into different layers responsible for applications, control, and data, thus providing a flexible and programmable architecture and making the network more reliable. However, from this centralization of network management, one of the main threats to SDNs remains Denial of Service and Distributed Denial of Service (DoS/DDoS) attacks, which are ever more developed and sophisticated nowadays. Despite the several works found in this field, notably reviews and surveys, and given the significant recourse for using SDNs, at the same time, the expansion of DoS/DDoS attacks, which are increasingly imposing and destructive. We believe that a good understanding of DoS/DDoS attacks will help us confront these cyber threats to better counter them according to the SDN context. This paper aims to analyze the DoS/DDoS attacks targeting the different layers and bounds of the SDN; it also attempts to provide an overview and summary of the current state-of-the-art in detecting and mitigating DoS/DDoS attacks over SDN environments and propose a classification of these solutions. Additionally, this work discusses the challenges and possible future research directions of DoS/DDoS attacks in SDN infrastructures and shares our vision issues related to this area.},
  archive      = {J_COMCOM},
  author       = {Mohamed Ali Setitra and Mingyu Fan and Ilyas Benkhaddra and Zine El Abidine Bensalem},
  doi          = {10.1016/j.comcom.2024.04.035},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {77-96},
  shortjournal = {Comput. Commun.},
  title        = {DoS/DDoS attacks in software defined networks: Current situation, challenges and future directions},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling lightweight immersive user interaction in smart
buildings through learning-based mobile panorama streaming.
<em>COMCOM</em>, <em>222</em>, 68–76. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart buildings integrate users’ digitized wearables with their physical surroundings, creating a seamless and interactive user experience. This is achieved through the utilization of multiple sensors, video streaming, artificial intelligence, and edge computing. These technologies gather extensive data and provide users with a wide range of applications, such as 3D audio/video in AR/VR, localization, virtual tours, and vigilant monitoring. Nevertheless, the current AR/VR devices face limitations due to the bulkiness and discomfort of the hardware used for on-body sensing, such as headsets and specialized glasses. These components often become uncomfortable during prolonged usage, posing a challenge for creating an immersive system that combines lightweight interaction with high-quality presentation. This paper presents a comprehensive system designed to enable immersive interaction in smart buildings with a focus on lightweight solutions. The system consists of the following components: (1). A lightweight panoramic imaging framework to address the challenges related to hardware size and functionality. (2). A learning-based video transcoding cost prediction framework for efficient load balancing. (3). A layered networking architecture designed to facilitate high-quality mobile panorama live streaming. Collectively, these components offer lightweight interaction paired with enhanced presentation quality. Our experimental results demonstrate the effectiveness of the system design, showcasing its seamless operation across different times, geographical locations, and heterogeneous wireless networks.},
  archive      = {J_COMCOM},
  author       = {Chi Xu and Zhengzhe Li and Guo Qing Huai and Jia Zhao and Yifei Zhu and Xiaoqiang Ma and Haiyang Wang},
  doi          = {10.1016/j.comcom.2024.04.002},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {68-76},
  shortjournal = {Comput. Commun.},
  title        = {Enabling lightweight immersive user interaction in smart buildings through learning-based mobile panorama streaming},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving in blockchain-based federated learning
systems. <em>COMCOM</em>, <em>222</em>, 38–67. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has recently arisen as a revolutionary approach to collaborative training Machine Learning models. According to this novel framework, multiple participants train a global model collaboratively, coordinating with a central aggregator without sharing their local data. As FL gains popularity in diverse domains, security, and privacy concerns arise due to the distributed nature of this solution. Therefore, integrating this strategy with Blockchain technology has been consolidated as a preferred choice to ensure the privacy and security of participants. This paper explores the research efforts carried out by the scientific community to define privacy solutions in scenarios adopting Blockchain-Enabled FL. It comprehensively summarizes the background related to FL and Blockchain, evaluates existing architectures for their integration, and the primary attacks and possible countermeasures to guarantee privacy in this setting. Finally, it reviews the main application scenarios where Blockchain-Enabled FL approaches have been proficiently applied. This survey can help academia and industry practitioners understand which theories and techniques exist to improve the performance of FL through Blockchain to preserve privacy and which are the main challenges and future directions in this novel and still under-explored context. We believe this work provides a novel contribution concerning the previous surveys and is a valuable tool to explore the current landscape, understand perspectives, and pave the way for advancements or improvements in this amalgamation of Blockchain and Federated Learning.},
  archive      = {J_COMCOM},
  author       = {Sameera K.M. and Serena Nicolazzo and Marco Arazzi and Antonino Nocera and Rafidha Rehiman K.A. and Vinod P. and Mauro Conti},
  doi          = {10.1016/j.comcom.2024.04.024},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {38-67},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving in blockchain-based federated learning systems},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient railway kilometer marker recognition via
spatio-temporal slimming and multi-view fusion. <em>COMCOM</em>,
<em>222</em>, 26–37. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently recognizing kilometer markers in railway systems is crucial for ensuring the safety and reliability of train operations, particularly within the Industrial Internet of Things (IIoT) framework where edge devices are often resource-constrained. This paper highlights the significance of a real-time AI-driven approach to railway kilometer marker recognition. We introduce an innovative method that employs spatio-temporal slimming and multi-view fusion techniques, elevating both precision and computational efficiency for real-time analytics in IIoT. Our approach begins with the implementation of an Adaptive Region of Interest (AROI) and a spatio-temporal calibration mechanism for effective marker detection in real-time. Furthermore, a multi-view fusion method addresses challenges such as occlusion, blurriness, and low-light conditions, common in real-world industrial environments, which includes a variation-aware memory bank for constructing informative views and a fusion network. Experimental results demonstrate the effectiveness of our method in a real-world rail transportation setting, significantly enhancing the accuracy and efficiency of kilometer marker recognition, thereby contributing to the safety and operational efficiency of rail systems.},
  archive      = {J_COMCOM},
  author       = {Xiaoyu Xian and Xiaoyu Guo and Yin Tian and Xiang Wei and Daxin Tian},
  doi          = {10.1016/j.comcom.2024.04.028},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {26-37},
  shortjournal = {Comput. Commun.},
  title        = {Efficient railway kilometer marker recognition via spatio-temporal slimming and multi-view fusion},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SMITS: Social and mobility aware intelligent task scheduling
in vehicular fog computing — a federated DRL approach. <em>COMCOM</em>,
<em>222</em>, 13–25. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular fog computing (VFC) has become an enticing research hot spot to provide resources for the Internet of Vehicles (IoV) application requests. Moreover, with the massive expansion of services in vehicular fog networks such as augmented reality, 3-D gaming, and autonomous driving, the allocation of fog computing resources to IoV applications constraining the deadline and energy consumption of the vehicles is becoming challenging. In this paper, we investigate a task scheduling problem in vehicular fog networks to jointly optimize the success rate of vehicular tasks and energy consumption of vehicles by considering the moving paths of the vehicles and the social relationships among the vehicles. In this problem, we model the social relationship among the vehicles based on their communication patterns and social characteristics to improve the success rate of the tasks. We also incorporate the mobility of vehicles using the Markov renewal process (MRP) technique. Initially, we formulate a mixed integer nonlinear programming (MINLP) problem for the proposed problem and further prove it to be an NP-hard problem. To solve the proposed problem, we design a federated deep reinforcement learning (FDRL) mechanism by converting the problem into a Markov decision process (MDP) problem. We compare our proposed algorithm with existing scheduling approaches, and simulation results show that the proposed algorithm performs efficiently for task scheduling problems. The proposed algorithm achieved a substantial improvement of 12% in the maximization of the success rate and 36% in the minimization of the energy consumption compared to the existing algorithms.},
  archive      = {J_COMCOM},
  author       = {Mekala Ratna Raju and Sai Krishna Mothku and Manoj Kumar Somesula},
  doi          = {10.1016/j.comcom.2024.04.023},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {13-25},
  shortjournal = {Comput. Commun.},
  title        = {SMITS: Social and mobility aware intelligent task scheduling in vehicular fog computing — a federated DRL approach},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the composition ratios of network services
carried in mixed traffic. <em>COMCOM</em>, <em>222</em>, 1–12. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network performance management (NPM) and quality of service (QoS) guarantee are the key concerns of network operators and service providers/consumers, respectively. The basis for traffic scheduling is one of the vital elements guiding the implementation of both NPM and QoS assurance. Although traffic identification technologies can theoretically be applied to providing basis for traffic scheduling, the computational complexity caused by the flow-by-flow processing makes them unsuitable for actual large-scale high-throughput network scenarios. In this work, we introduce a new scheme to provide a basis for service-oriented NPM and QoS guarantee. The proposed scheme treats the mixed traffic as a whole and estimates the composition ratios of network services carried in mixed traffic, which makes traffic scheduling for NPM and QoS more reasonable and flexible. To estimate the composition ratios, the observed mixed traffic is expressed as a bipartite graph according to the communication relationship. Then, the bipartite graph is further projected to a multi-dimensional hyper-image based on node coding, random walking and embedding mapping. By this way, we convert the service composition ratio estimation into a problem similar to image classification and enable it to be solved by general machine learning methods like convolutional neural networks. The proposed scheme facilitates batch traffic scheduling and can reorganize the distribution of compositions of network services for each link based on given management strategies. Its computational complexity is significantly lower than that of flow-by-flow processing methods. We validate the proposed solution via real network data and evaluate its performance with some benchmark methods based on traffic identification. The experimental results show that the proposed solution can accurately infer the service composition ratios of mixed traffic with low computational complexity.},
  archive      = {J_COMCOM},
  author       = {Zihui Wu and Yi Xie and Shensheng Tang and Xingcheng Liu},
  doi          = {10.1016/j.comcom.2024.04.022},
  journal      = {Computer Communications},
  month        = {6},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {Estimating the composition ratios of network services carried in mixed traffic},
  volume       = {222},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PETRAK: A solution against DDoS attacks in vehicular
networks. <em>COMCOM</em>, <em>221</em>, 142–154. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the frequently reported incidents of Distributed Denial of Service assaults on vehicular networks in various countries have made researchers find new protective solutions. DDoS attacks can propagate through the charging points for electric vehicles in a charging station and affect the production of critical infrastructures such as electric grids. Existing solutions are efficient in attack detection; however, current systems do not offer multi-level protection, and zero-day vulnerabilities are prone to escape from the detection systems. In this paper, we address the problems mentioned above and introduce the first Machine Learning (ML)–based DDoS protective solution to combine prevention and detection mechanisms in vehicular networks. To be more specific, our proposed model is the first to consider the adaptive traffic threshold to generate the alarm for a suspicious amount of traffic flow in an Intrusion Detection Prevention System (IDPS). We call our proposed approach Protecting vEhicular neTworks against distRibuted deniAl of service attacKs (PETRAK) . PETRAK uses four functions: prevention, alarm, training, and detection. The alarming system uses the flow parameters and activates the detection module to detect malicious packets. The prevention system works in two modes: immediate and future. PETRAK uses logistic regression to identify incoming packets and signatures of malicious packets to prevent future attacks. We also show that our proposed model is implacable towards advanced post-quantum cryptography-based traffic and also able to analyse side-channel attacks. We run a comprehensive set of experiments to test PETRAK on our synthesized dataset, KDDCUP’99 dataset, CIC-MalMem-2022, and the ToN-IoT dataset. We observe that PETRAK shows an accuracy of 99%. The results claim the efficiency of PETRAK in detecting and preventing DDoS attacks in vehicular networks.},
  archive      = {J_COMCOM},
  author       = {Amandeep Verma and Rahul Saha and Gulshan Kumar and Mauro Conti},
  doi          = {10.1016/j.comcom.2024.04.025},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {142-154},
  shortjournal = {Comput. Commun.},
  title        = {PETRAK: A solution against DDoS attacks in vehicular networks},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards energy-aware federated learning via collaborative
computing approach. <em>COMCOM</em>, <em>221</em>, 131–141. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research delves into the consequences of the high complexity of on-device operations executed during the federated learning process. We investigate how the varying computational capabilities and battery levels among mobile devices can introduce performance disparities and influence training quality. Hence, in order to deal with these challenges, we propose EAFL+ , a novel energy optimization technique, that focuses on managing power consumption in devices with limited battery capacity. EAFL+ is a cloud–edge–terminal collaborative approach that provides a new architectural design for achieving power-aware FL training by leveraging resource diversity and computation offloading. The innovative scheme enables the efficient selection of an approximately-optimal offloading target, from a set of Cloud-tier, Edge-tier, and Terminal-tier resources and achieves the best cost-quality tradeoff for the devices taking part in the FL system. Our evaluation shows EAFL+ can help conserve the devices’ energy participating in training, which improves the participation rates and increases the clients’ contributions, hence achieving higher accuracy and faster convergence. Through experiments on real datasets and traces in an emulated FL environment, EAFL+ reduces the drop-out of clients to zero and enhances accuracy by up to 24% and 9% compared to EAFL and Oort, respectively.},
  archive      = {J_COMCOM},
  author       = {Amna Arouj and Ahmed M. Abdelmoniem},
  doi          = {10.1016/j.comcom.2024.04.012},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {131-141},
  shortjournal = {Comput. Commun.},
  title        = {Towards energy-aware federated learning via collaborative computing approach},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGC sketch: An effective and accurate per-flow measurement
to adapt flow size distribution. <em>COMCOM</em>, <em>221</em>, 120–130.
(<a href="https://doi.org/10.1016/j.comcom.2024.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network measurements are indispensable for efficient network management. As a probabilistic data structure, sketch is widely used in network measurements. To adapt to the skewed and dynamic distribution of network traffic, most of the existing sketch-based schemes separate large and small flows either by asymmetric structures or multilayer structures. However, the former approach requires further accuracy of measurement due to increased errors of small flows. And the latter approach limits the throughput due to the increased hash calculations for large flows. To this end, a novel sketch, called AGC Sketch, is proposed in this paper. By storing the large flows and small flows separately while maintaining the small flows, it can not only adapt to the skewed network traffic to achieve efficient memory utilization, but also accurately estimate the size of flows with high throughput. Moreover, AGC Sketch is implemented on CPU and OVS platform. The experimental results show that AGC Sketch greatly reduces the error by 90% for flow size estimation compared with the state-of-the-art schemes.},
  archive      = {J_COMCOM},
  author       = {Zhuo Li and Ziyu Liu and Jindian Liu and Yu Zhang and Teng Liang and Kaihua Liu},
  doi          = {10.1016/j.comcom.2024.04.020},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {120-130},
  shortjournal = {Comput. Commun.},
  title        = {AGC sketch: An effective and accurate per-flow measurement to adapt flow size distribution},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing physical layer security with reconfigurable
intelligent surfaces and friendly jamming: A secrecy analysis.
<em>COMCOM</em>, <em>221</em>, 106–119. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its numerous advantages in enhancing wireless systems’ reliability and secrecy performance, the implementation of reconfigurable intelligent surfaces (RIS) is faced with several challenges, such as optimizing RIS phase-shift configuration to maximize the secrecy performance, particularly in multi-antenna and jamming-aided communication systems, and with the presence of phase quantization errors (PQEs) impairment due to the finite precision. Furthermore, it is crucial to provide an analytical evaluation for RIS-and-jamming-aided multi-antenna schemes. In this paper, the secrecy of a dual-hop wireless communication system, assisted by an RIS in each hop, is quantified. In particular, a multi-antenna relay assists the source node’s communication with a multi-antenna destination. Under the presence of PQEs and several eavesdroppers in the second hop, an RIS splitting technique into equal-size areas, along with friendly jamming, is used to maximize the received legitimate signal power at the relay and destination nodes and disrupt the eavesdroppers. The scheme’s secrecy level is evaluated by deriving novel approximate and asymptotic expressions of the secrecy outage probability (SOP) metric in terms of the main network parameters. Results show that the secrecy is significantly enhanced by increasing the jamming power, the number of reflective elements (REs), or the number of antennas at the relay. In particular, an SOP of 1 0 − 6 10−6 is reached with 2 receive antennas at the relay and destination, 25 REs per each RIS zone, and − 30 −30 dB of normalized jamming power-to-noise ratio even when the legitimate links’ average normalized signal-to-noise ratios are less than the eavesdropper’s one (i.e., strong eavesdropping). It is also shown that the number of quantization bits does not influence the secrecy when exceeding 3 bits. Lastly, the increase in the number of eavesdroppers does not yield a significant secrecy loss.},
  archive      = {J_COMCOM},
  author       = {Elmehdi Illi and Marwa Qaraqe and Faissal El Bouanani and Saif Al-Kuwari},
  doi          = {10.1016/j.comcom.2024.04.010},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {106-119},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing physical layer security with reconfigurable intelligent surfaces and friendly jamming: A secrecy analysis},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight blockchain-based remote user authentication for
fog-enabled IoT deployment. <em>COMCOM</em>, <em>221</em>, 90–105. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) seeks to enhance human life by embedding everyday objects with intelligence. As deployed in diverse environments, IoT devices exchange substantial data to offer autonomously smart and innovative services. Cloud computing serves as an effective solution for processing and storing data from IoT devices, enabling remote access for end-users. However, authenticating remote users poses a critical challenge. Although several existing schemes tackle this challenge, they have notable security and performance weaknesses. This paper introduces a lightweight, distributed multi-factor authentication scheme for remote users in IoT. The proposed approach, named Lightchain, integrates blockchain technology and fog computing while incorporating a lightweight cryptographic hash function. The security of Lightchain is rigorously confirmed through formal verification using the well-recognized Automated Validation of Internet Security Protocols and Applications (AVISPA) tool. Additionally, we develop and assess Lightchain’s functionality using the solidity language across diverse use cases. A comparative analysis is presented to evaluate performance costs and security requirements against recent methods. The proposed scheme demonstrates efficiency and robustness, making it well-suited for a range of IoT applications.},
  archive      = {J_COMCOM},
  author       = {Yasmine Harbi and Zibouda Aliouat and Saad Harous and Abdelhak Mourad Gueroui},
  doi          = {10.1016/j.comcom.2024.04.019},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {90-105},
  shortjournal = {Comput. Commun.},
  title        = {Lightweight blockchain-based remote user authentication for fog-enabled IoT deployment},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power allocation scheme for sum rate and fairness trade-off
in downlink NOMA networks. <em>COMCOM</em>, <em>221</em>, 78–89. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) is an essential enabler technology that is expected to help satisfy the key requirements of increased system throughput in future wireless networks. However, another equally important aspect that should go hand-in-hand with system throughput is user fairness for any network. But, to the best of our knowledge, even though there have been works that look at system throughput or user fairness maximization for NOMA-based networks, they looked at these as a single objective optimization problem, where one is the objective and the other is one of the constraints. However, quite often, joint optimization of both system throughput and user fairness is required to make optimized decisions in the face of trade-offs between these two equally important but conflicting objectives. In this regard, this paper formulates a multi-objective optimization problem to jointly maximize the sum rate and user fairness in a downlink transmission NOMA system, through optimize power allocation (PA), under system-imposed constraints. A weighted sum approach is used to turn the multi-objective optimization problem into a single-objective optimization problem to make it analytically tractable. The optimized PA is then obtained using Lagrange dual decomposition method and Karush–Kuhn–Tucker (KKT) conditions. Using our derived expressions, we propose an iterative PA algorithm that converges fast enough to be employed in practical NOMA networks. We also present simulation results to highlight the effectiveness of the proposed solution. Further, the performance of the proposed method is compared with that of the benchmark methods.},
  archive      = {J_COMCOM},
  author       = {Sachin Trankatwar and Prashant Wali},
  doi          = {10.1016/j.comcom.2024.04.018},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {78-89},
  shortjournal = {Comput. Commun.},
  title        = {Power allocation scheme for sum rate and fairness trade-off in downlink NOMA networks},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full/half-duplex unmanned aerial vehicles assisted wireless
systems: Performance analysis and optimization. <em>COMCOM</em>,
<em>221</em>, 66–77. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore the utilization of both full-duplex (FD) and half-duplex (HD) transmission modes on unmanned aerial vehicles (UAVs) to enhance wireless system performance. We consider two practical scenarios: one without a direct transmitter-user link and the other with such a link. For both cases, we derive mathematical expressions of outage probabilities (OPs) and throughputs of FD-UAV and HD-UAV systems, employing a realistic channel model aligned with the fifth and beyond generations (5G-B5G) standards. In cases where imperfect self-interference cancellation (SIC) occurs in FD-UAV systems, we introduce an optimal power allocation approach to enhance system performance. Numerical results underscore the benefits of cooperative communications, particularly when combining the direct link with the UAV link at the user, leading to an overall enhancement in system performance. Furthermore, we conduct a comprehensive analysis of various system parameters, including predefined rates, residual self-interference (RSI) levels, high carrier frequencies of Wi-Fi networks, and high altitudes of UAVs. The impact of RSI is particularly notable, and the proposed optimal power allocation approach significantly improves system performance in such cases. Specifically, the introduced scheme helps avoid error floors in regions of high transmit power, enabling throughputs to reach desired targets in FD-UAV systems. Crucially, the optimal power value is considerably lower than the traditional value often used without optimal power allocation, extending the operational duration of FD-UAV. Finally, to validate the derived mathematical expressions and affirm the effectiveness of the proposed approaches, we conduct Monte-Carlo simulations.},
  archive      = {J_COMCOM},
  author       = {Duc Thinh Vu and Ba Cao Nguyen and Nguyen Van Vinh and Taejoon Kim and Bao The Phung},
  doi          = {10.1016/j.comcom.2024.04.016},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {66-77},
  shortjournal = {Comput. Commun.},
  title        = {Full/half-duplex unmanned aerial vehicles assisted wireless systems: Performance analysis and optimization},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint mode selection and resource allocation for cellular
V2X communication using distributed deep reinforcement learning under 5G
and beyond networks. <em>COMCOM</em>, <em>221</em>, 54–65. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-everything (V2X) communication via cellular networks is a promising technique for 5G and beyond networks. The cars interact directly with one another, as well as with the infrastructure and various vehicles on the road, in this mode. It enables the interchange of time-sensitive and safety-critical data. Despite these benefits, unstable vehicle-to-vehicle (V2V) communications, insufficient channel status information, high transmission overhead, and the considerable communication cost of centralized resource allocation systems all pose challenges for defense applications. To address these difficulties, this study proposes a combined mode selection and resource allocation system based on distributed deep reinforcement learning (DRL) to optimize the overall network sum rate while maintaining the reliability and latency requirements of V2V pairs and the data rate of V2R connections. Because the optimization issue is non-convex and NP-hard, it cannot be solved directly. To tackle this problem, the defined problem is first translated into machine learning form using the Markov decision process (MDP) to construct the reward function and decide whether agent would conduct the action. Following that, the distributed coordinated duelling deep Q-network (DDQN) method based on prioritized sampling is employed to improve mode selection and resource allocation. This approach learns the action-value distribution by estimating both the state-value and action advantage functions using duelling deep networks. The results of the simulation show that the suggested scheme outperforms state-of-the-art decentralized systems in terms of sum rate and QoS satisfaction probability.},
  archive      = {J_COMCOM},
  author       = {Shalini Yadav and Rahul Rishi},
  doi          = {10.1016/j.comcom.2024.04.015},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {54-65},
  shortjournal = {Comput. Commun.},
  title        = {Joint mode selection and resource allocation for cellular V2X communication using distributed deep reinforcement learning under 5G and beyond networks},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-UAVs path planning for data harvesting in adversarial
scenarios. <em>COMCOM</em>, <em>221</em>, 42–53. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have been widely utilized for data harvesting in scenarios where reliable communication infrastructure is absent. A data requestor may first deploy a set of IoT devices into the monitoring regions like the disaster area or the battlefield, and deliver a group of UAVs to collaboratively collect data from these IoT devices. Constrained by the battery power and limited capabilities of UAVs, the data requestor must design flying routes for grouped UAVs such that the collected data volumes, the return and the flight safety of UAVs are properly balanced. In recent years, deep Q-network (DQN) models have been extensively applied for multi-UAV path planning owing to its superiority in representing the surrounding environment and action space for UAVs. However, previous solutions majorly focus on applying advanced DQNs for UAV path planning in different system settings, while the specific designs for non-cooperative and adversarial environment is seldom mentioned. This fact has reduced the performance for multi-UAV data collection in adversarial environment, like the rescue in disaster area and the investigation in conflicting regions. Therefore, this paper introduces a novel DQN framework addressing the critical issues in adversarial environments. First, we assume and formulate the existence of trapping and eavesdropping attacks in monitoring regions, which are conducted by rivals to directly hijack the UAVs or overhear the transmitted information among IoT devices and UAVs. Second, a novel Deep Q-learning model is designed which encodes the surrounding threats into the state space and Q-networks, by combining them with the battery capacity, the safe flying distance among UAVs and the volume of collected data. Besides, a novel reward function is also designed to further refine the action series of UAVs for better data coverage among devices and safety landing. Third, we design a Shamir threshold method based mechanism for secure information sharing between IoT devices and UAVs, such that the original data can be properly concealed under eavesdropping attacks. Finally, extensive simulation results have demonstrated the advanced performance for multi-UAV data collection in adversarial environment.},
  archive      = {J_COMCOM},
  author       = {Chengkai Zhou and Kadhim Mustafa Raad Kadhim and Xu Zheng},
  doi          = {10.1016/j.comcom.2024.04.004},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {42-53},
  shortjournal = {Comput. Commun.},
  title        = {Multi-UAVs path planning for data harvesting in adversarial scenarios},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDN-based detection and mitigation of DDoS attacks on smart
homes. <em>COMCOM</em>, <em>221</em>, 29–41. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of the Internet of Things (IoT) has proliferated across various domains, where everyday objects like refrigerators and washing machines are now equipped with sensors and connected to the internet. Undeniably, the security of such devices, which were not primarily designed for internet connectivity, is of utmost importance but has been largely neglected. In this paper, we propose a framework for the real-time DDoS attack detection and mitigation in SDN-enabled smart home networks. We capture network traffic during regular operations and during DDoS attacks. This captured traffic is used to train several machine learning (ML) models, including Support Vector Machine (SVM), Logistic Regression, Decision Trees, and K-Nearest Neighbors (KNN) algorithms. These trained models are executed as SDN controller applications and subsequently employed for real-time attack detection. While we utilize ML techniques to protect IoT devices, we propose the use of SNORT, a signature-based detection technique, to secure the SDN controller itself. Real-world experiments demonstrate that without SNORT, the SDN controller goes offline shortly after an attack, resulting in a 100% packet loss. Furthermore, we show that ML algorithms can efficiently classify traffic into benign and attack traffic, with the Decision Tree algorithm outperforming others with an accuracy of 99%.},
  archive      = {J_COMCOM},
  author       = {Usman Haruna Garba and Adel N. Toosi and Muhammad Fermi Pasha and Suleman Khan},
  doi          = {10.1016/j.comcom.2024.04.001},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {29-41},
  shortjournal = {Comput. Commun.},
  title        = {SDN-based detection and mitigation of DDoS attacks on smart homes},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trajectory privacy protection method with smart
contract-based query exchange in the social internet of vehicles.
<em>COMCOM</em>, <em>221</em>, 19–28. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Query exchange in the Social Internet of Vehicles (SIoV) can protect users’ trajectory information. However, this method lacks an appropriate incentive mechanism, which leads to cooperative users refusing to participate in query exchange. In order to provide cooperative users with incentives to participate in query exchange, this paper proposes a smart contract-based query exchange (SC-QE) trajectory privacy protection method. By creating a many-to-many smart contract, the method encourages the cooperative users to bid to the requesting users. Subsequently, in order to select a Best Similarity Deviation User (BSDU) for the requesting user to perform query exchange, the users in the smart contract are modeled as a weighted bipartite graph, and the matching between the requesting users and BSDUs is realized by means of a weighted bipartite graph best matching algorithm. Following successful verification of the query exchange transaction in the smart contract, the base station distributes rewards to the BSDU and uploads the query exchange transaction to the consortium blockchain. Experimental results show that compared with the deviation-based query exchange (DQE) method, the proposed method reduces the user processing time by 12% while increasing the continuous anonymous success rate by 29%. Therefore, the proposed method can reduce the service query time and improve the level of trajectory privacy protection.},
  archive      = {J_COMCOM},
  author       = {Lulu Liu and Ling Xing and Jianping Gao and Honghai Wu and Huahong Ma},
  doi          = {10.1016/j.comcom.2024.04.014},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {19-28},
  shortjournal = {Comput. Commun.},
  title        = {Trajectory privacy protection method with smart contract-based query exchange in the social internet of vehicles},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Game theory-based switch migration strategy for satellite
networks. <em>COMCOM</em>, <em>221</em>, 10–18. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) separates the control plane and data plane of the network, abstracting the underlying infrastructure of applications and network services and handing it over to the SDN controller for unified management. The introduction of SDN technology into satellite networks effectively addresses control challenges posed by dynamic topologies and intermittent inter-satellite links. Due to factors such as geography and usage patterns, the distribution of remote sensing data traffic is uneven in both space and time. Some remote sensing satellites may acquire large volumes of data within a short period and transmit them back to the ground through intermediate nodes, leading to a sudden surge in load on certain controllers. This situation adversely impacts the performance of remote sensing data transmission. To mitigate this, switch migration operations are required to alleviate the load on overloaded controllers and to achieve the load balance among all controllers. In this paper, a switch migration scheme based on game theory is proposed under a three-layer satellite network architecture, which is designed based on the management costs and ensures timely triggering of the migration operations. Moreover, a game theory-based matching optimization model is formulated in the scheme to address the matching problem between target controllers and migration candidate switches. We further propose a distributed algorithm to seek the Nash equilibrium within the game. Experimental results demonstrate that, compared to other relevant algorithms, this approach can find the near-optimal matching strategy for each target controller and reduce the average latency of these controllers.},
  archive      = {J_COMCOM},
  author       = {Xu Yan and Jinyao Liu and Ligang Cong and Xiaoqiang Di and Nannan Xie and Ziyang Xing and Hui Qi},
  doi          = {10.1016/j.comcom.2024.03.022},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {10-18},
  shortjournal = {Comput. Commun.},
  title        = {Game theory-based switch migration strategy for satellite networks},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A security-enhanced scheme for MQTT protocol based on
domestic cryptographic algorithm. <em>COMCOM</em>, <em>221</em>, 1–9.
(<a href="https://doi.org/10.1016/j.comcom.2024.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the rapid advancement of Industrial Internet of Things (IIoT) technology has led to the utilization of various communication protocols. Among these, the Message Queue Telemetry Transport (MQTT) protocol has become prevalent. Operating on a publish/subscribe model, MQTT’s popularity in IIoT because of simplicity, lightweight nature, and ease of implementation. However, traditional MQTT protocol standards have neglected critical security concerns during the communication process, such as ineffective identity authentication, weak data confidentiality, and a lack of access control functions. To address these challenges, this paper introduces a security-enhanced scheme for the MQTT protocol based on domestic cryptographic algorithms. The proposed method enforces mutual identity authentication between the MQTT Client and the MQTT Broker using digital certificates. It also implements access control through an Access Control List (ACL) and ensures end-to-end data security via the symmetric encryption algorithm SM4. Through comprehensive security analysis and experimental validation, this paper demonstrates that the enhanced scheme effectively rectifies the MQTT protocol’s security deficiencies, achieving these improvements with small overhead.},
  archive      = {J_COMCOM},
  author       = {Zechao Liu and Tao Liang and Jiazhuo Lyu and Dapeng Lang},
  doi          = {10.1016/j.comcom.2024.04.013},
  journal      = {Computer Communications},
  month        = {5},
  pages        = {1-9},
  shortjournal = {Comput. Commun.},
  title        = {A security-enhanced scheme for MQTT protocol based on domestic cryptographic algorithm},
  volume       = {221},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous signcryption scheme with equality test from
CLC to PKI for IoV. <em>COMCOM</em>, <em>220</em>, 149–159. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data confidentiality and availability in an open network environment are essential for Internet of Vehicles (IoV). Encryption technology can only ensure user privacy, but impedes the optimal use of vehicular data. As a solution, public key encryption with equality test (PKEET) is proposed. However, most existing PKEET schemes rely on the homogeneity of communication parties and reliability of algorithms. The former requires both communication parties to be under the same cryptographic system, while the latter requires cloud server (CS) is a semi-honest entity. To address the challenge, we propose a heterogeneous signcryption scheme with equality test, named CP-HSCET, which allows CS to verify whether the plaintexts corresponding to ciphertexts are equal without unsigncrypting, thus enhancing the availability. It also enables an IoV device in certificateless (CLC) cryptosystem to signcrypt messages and send them to another device in public key infrastructure (PKI)-based cryptosystem, thus achieving higher availability. To overcome the reliability problem, we incorporate blockchain technology into our algorithm, thus eliminating the dependence on an honest (or semi-honest) CS. We also prove its security notions and design it to satisfy confidentiality, integrity, non-repudiation, and authentication. Finally, we implement our scheme with Hyperledge Fabric and compare the proposed scheme with other four comparison schemes. The result shows that it reduces by 35.49%, 53.79%, 80.15% and 30.37% of other four schemes in terms of computation time at 500 ciphertexts. Therefore, our solution is the most suitable for IoV environments, where most of the devices have lower computational power.},
  archive      = {J_COMCOM},
  author       = {Chunhua Jin and Wenyu Qin and Zhiwei Chen and Kaijun Sun and Guanhua Chen and Jinsong Shan and Liqing Chen},
  doi          = {10.1016/j.comcom.2024.04.008},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {149-159},
  shortjournal = {Comput. Commun.},
  title        = {Heterogeneous signcryption scheme with equality test from CLC to PKI for IoV},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-cost fuzzing drone control system for configuration
errors threatening flight safety in edge terminals. <em>COMCOM</em>,
<em>220</em>, 138–148. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the wide deployment of edge-cloud computing, drones have drawn much attention to serving as edge nodes responsible for data collection. Numerous configuration parameters play a crucial role in regulating the attitude and altitude of the drone. If these parameters are misconfigured, drones will fall into abnormal flight states, such as trajectory deviation, and even crash to the ground. Previous research primarily addresses system memory errors which result in obvious system failures, but does not effectively detect the anomalies in drone flight states. Moreover, a large amount of computational cost on the detection will significantly increase the power consumption of drones, which violates the principle of green computing. This paper focuses on abnormal drone flight states caused by configuration parameter errors. We propose a novel state-guided fuzzing system called LDFuzzer, which searches for incorrect configuration parameter values that would trigger abnormal flight states. To enhance the capability of searching for multiple optimal solutions, we design a Quality-Diversity-Enhanced Genetic Algorithm (QDGA) to mutate configuration values. Moreover, we also propose a set of new test oracles to detect abnormal flight states of drones in real time with restricted computational resources. We evaluated LDFuzzer on the drone control system ArduPilot and successfully discovered 3399 incorrect configuration parameter values. Additionally, the results from our experiments show that LDFuzzer can automatically analyze fuzzing outcomes and has identified 8 software bugs linked to configuration parameters.},
  archive      = {J_COMCOM},
  author       = {Zhiwei Chang and Hanfeng Zhang and Yan Jia and Sihan Xu and Tong Li and Zheli Liu},
  doi          = {10.1016/j.comcom.2024.04.005},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {138-148},
  shortjournal = {Comput. Commun.},
  title        = {Low-cost fuzzing drone control system for configuration errors threatening flight safety in edge terminals},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic data access control scheme for hierarchical
structures in big data. <em>COMCOM</em>, <em>220</em>, 128–137. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is a promising solution to address the issues of data leakage and sharing in big data. When multiple files are shared, they often have hierarchical access relationships. However, existing hierarchical data CP-ABE schemes can only support simple hierarchical access structures. The limitation arises, necessitating the generation of multiple access structures to meet hierarchical access requirements, resulting in the wastage of computational and storage resources. Therefore, in this paper, we carefully design a dynamic hierarchy access control that allows its level nodes to flexibly participate in the decryption of other nodes, enabling the realization of complex hierarchical data relationships. Building upon this, we construct the Attribute-based Dynamic Hierarchical data Access Control (ADHAC) scheme, achieving efficient and dynamic access control for multiple datasets. Subsequently, we apply our scheme in distributed computing, which enables efficient access control for individuals and institutions over multiple data sets and fine-grained computation. Security analysis indicates that our scheme can resist chosen plaintext attack (CPA). Both theoretical and experimental analyses demonstrate that our scheme has impressive advantages in terms of computational efficiency and storage costs compared to existing schemes.},
  archive      = {J_COMCOM},
  author       = {Xinxin Deng and Changgen Peng and Haoxuan Yang and Zongfeng Peng and Chongyi Zhong},
  doi          = {10.1016/j.comcom.2024.04.006},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {128-137},
  shortjournal = {Comput. Commun.},
  title        = {A dynamic data access control scheme for hierarchical structures in big data},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning to optimize power allocation in cell-free massive
MIMO networks with hybrid green energy. <em>COMCOM</em>, <em>220</em>,
108–127. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cell-free (CF) massive multiple-input multiple-output (MIMO) networks, many access points (APs) simultaneously serve user equipment (UEs) using the same time and frequency resources. The resource allocation algorithm in such networks needs to handle many calculations within a limited time frame to determine the power allocation from each AP to each UE. Further, the dispersion and large number of APs in CF massive MIMO networks pose challenges in terms of energy supply. As access to the power grid may be limited, renewable energies are often required as a supplement. However, due to the non-deterministic nature of the charging process, the resource allocation algorithm needs to consider the dynamic and uncertain amount of energy available at the APs. In this article, we propose a scalable machine learning algorithmbased on the emerging Learning to Optimize (L2O) paradigm that utilizes neural networks (NNs) for resource allocation in CF massive MIMO networks powered by electrical and renewable energies. Our objective is to minimize energy consumption from the power grid while satisfying the spectral efficiency (SE) constraints of the UEs. Additionally, our algorithm adopts a forward-looking approach and employs a green energy budgeting mechanism to prevent service interruption at green APs during energy shortages. Our algorithm design approach involves formulating the offline version of the problem as a second-order cone program (SOCP). Subsequently, we train an NN using the solutions obtained from solving the online variant of the SOCP. Our results indicated that our L2O-based algorithm can accurately mimic the solutions of the optimization problem with significantly reduced computational complexity.},
  archive      = {J_COMCOM},
  author       = {Farzad Moradi and Vesal Hakami and Seyed Vahid Azhari},
  doi          = {10.1016/j.comcom.2024.03.009},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {108-127},
  shortjournal = {Comput. Commun.},
  title        = {Learning to optimize power allocation in cell-free massive MIMO networks with hybrid green energy},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning-driven continuous diagnostics and mitigation
program for secure edge management through zero-trust architecture.
<em>COMCOM</em>, <em>220</em>, 94–107. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-touch architecture (ZTA) is one of the emerging cybersecurity components used to secure organizational resource. The primary pillars of ZTA is users, devices, networks, applications, and analytics. As part of this architecture, several components are used to secure resources, including the policy engine, the administrator, and the enforcement points on control and data planes. In addition, it contains Continuous Diagnostics and Mitigation (CDM), active logs, database for user and compliances. Among them, CDM is one of the major components which enhance the system and network security. However, traditional architecture is static and does not learn from experiences, so it is not adaptive and autonomous. To improve the CDM program, we use learning models to identify and respond intelligently while securing networks and systems. In this context, we use a recurrent neural network (RNN) to diagnosis the problems based on the condition and mitigate them through efficient decision making within the systems autonomously. In our experiments, we found that the proposed RNN methods were able to achieve 96% accuracy while diagnosing and mitigating the problems with our learned CDM program. These results are superior to those obtained from the traditional CDM program used in ZTA.},
  archive      = {J_COMCOM},
  author       = {P. SumanPrakash and K. Seshadri Ramana and Renzon Daniel CosmePecho and M. Janardhan and Meryelem Tania Churampi Arellano and J. Mahalakshmi and M. Bhavsingh and K. Samunnisa},
  doi          = {10.1016/j.comcom.2024.04.007},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {94-107},
  shortjournal = {Comput. Commun.},
  title        = {Learning-driven continuous diagnostics and mitigation program for secure edge management through zero-trust architecture},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust and effective 3-factor authentication protocol for
smart factory in IIoT. <em>COMCOM</em>, <em>220</em>, 81–93. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart factory, as an intelligent application of industrial internet of things (IIoT), significantly enhances the efficiency of industrial processes while also reducing resource waste. In a system where security is crucial, the user authentication mechanism is essentially designed to prevent a range of security issues, including production failures caused by illegal intrusions from hackers. To verify the user’s identity and ensure data be accessed with a secure session key, a large number of authentication protocols have been provided for IIoT. However, existing newly alternatives show deficiency either in terms of superior performance or robust security. To get a better balance of security and efficiency, this paper designs a robust and effective 3-factor user authentication protocol. Then this paper shows the detailed security analyses and automated verification by the Proverif tool, where the ProVerif tool used in this paper can detect whether the eCK adversary with stronger attack ability can break the protocol, while the general ProVerif can only detect threat of the Dolev–Yao adversary with weaker attack ability to the protocol security. Subsequently, this paper presents performance comparison which indicates that the proposed protocol can be superior to those newly alternatives. Especially, the comparison results show that the computation cost consumed in the proposed protocol can be reduced by 62.2% than the average cost of all six compared alternatives. Lastly, the evaluation results on the energy consumption and the network delay indicate the practicability of proposed protocol for smart factory.},
  archive      = {J_COMCOM},
  author       = {Shihong Zou and Qiang Cao and Ruichao Lu and Chenyu Wang and Guoai Xu and Huanhuan Ma and Yingyi Cheng and Jinwen Xi},
  doi          = {10.1016/j.comcom.2024.04.011},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {81-93},
  shortjournal = {Comput. Commun.},
  title        = {A robust and effective 3-factor authentication protocol for smart factory in IIoT},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving and verifiable classifier training in
edge-assisted mobile communication systems. <em>COMCOM</em>,
<em>220</em>, 65–80. (<a
href="https://doi.org/10.1016/j.comcom.2024.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing classifier training services in mobile communication systems not only brings benefits, yet raises privacy issues for sensitive data and classifiers. To protect the privacy of data and classifiers, a common idea is to train classifiers over encrypted data among devices and the cloud. However, the existing solutions adopted to implement such an idea not only fall short in excessive bandwidth resource consumption and network latency , but also fail to provide data confidentiality and integrity verification simultaneously. In this paper, to bridge this gap, we design, implement, and evaluate Privacy-Preserving and Verifiable Classifier Training (PPVCT) services in edge-assisted mobile communication systems . Firstly, we propose a new privacy-preserving cloud–edge collaborative model that significantly shifts the processing of the cloud to the edge and securely trains the model among multiple parties. Secondly, we construct a new library of building blocks , which can be used to construct most classifiers as well, such as kNN, SVM , etc. Finally, we further devise a scheme to realize the integrity verification of the aggregated data , while most outsourcing time-consumed operations from the cloud to the edge server. Extensive property and performance analysis indicate that PPVCT can efficiently provide classifier training services while maintaining the confidentiality and integrity of sensitive data.},
  archive      = {J_COMCOM},
  author       = {Chen Wang and Jian Xu and Haoran Li and Fucai Zhou and Qiang Wang},
  doi          = {10.1016/j.comcom.2024.04.003},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {65-80},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving and verifiable classifier training in edge-assisted mobile communication systems},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Corrigendum to “a novel hardware efficient design for IEEE
802.11ax compliant OFDMA transceiver” [comput. Commun. 219 (2024)
173–181]. <em>COMCOM</em>, <em>220</em>, 64. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Muhammad Aslam and Xianjun Jiao and Wei Liu and Michael Mehari and Thijs Havinga and Ingrid Moerman},
  doi          = {10.1016/j.comcom.2024.03.021},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {64},
  shortjournal = {Comput. Commun.},
  title        = {Corrigendum to ‘A novel hardware efficient design for IEEE 802.11ax compliant OFDMA transceiver’ [Comput. commun. 219 (2024) 173–181]},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DRAMS: Double-RIS assisted multihop routing scheme for
device-to-device communication. <em>COMCOM</em>, <em>220</em>, 52–63.
(<a href="https://doi.org/10.1016/j.comcom.2024.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surfaces (RISs) is a promising solution for enhancing the performance of multihop wireless communication networks. In this paper, we propose a double-RIS assisted multihop routing scheme for a device-to-device (D2D) communication network. Specifically, the scheme is dependent on the already deployed RISs and users in the surroundings. Besides the RISs, the emphasis of this work is to make more use of the existing intermediate users (IUs), which can act as relays. Hence, the density of RIS deployment in the surroundings can be reduced, which leads to the avoidance of resource wastage. However, we cannot solely depend on the IUs because this implies complete dependence on their availability for relaying and as a result, the aspect of reliability in terms of delay-constrained information transfer cannot be guaranteed. Moreover, the IUs are considered capable of energy harvesting and as a result, they do not waste their own energy in the process of volunteering to act as a relay for other users. Numerical results demonstrate the advantage of the proposed scheme over some existing approaches and lastly, useful insights related to the scheme design are also drawn, where we characterize the maximum acceptable delay at each hop under different set-ups.},
  archive      = {J_COMCOM},
  author       = {Lakshmikanta Sau and Priyadarshi Mukherjee and Sasthi C. Ghosh},
  doi          = {10.1016/j.comcom.2024.03.020},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {52-63},
  shortjournal = {Comput. Commun.},
  title        = {DRAMS: Double-RIS assisted multihop routing scheme for device-to-device communication},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PrioMQTT: A prioritized version of the MQTT protocol.
<em>COMCOM</em>, <em>220</em>, 43–51. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {MQTT is an application layer protocol that, thanks to its simplicity and low overhead, is widely used in the Internet of Things (IoT) devices typically found in home automation and consumer applications. The MQTT properties make it an interesting option also for Industrial IoT (IIoT) applications. However, MQTT is not specifically devised for IIoT applications requiring low-latency and the support for time-constrained transmissions. For this reason, this paper proposes an IIoT-enabled version of MQTT called a Prioritized MQTT (PrioMQTT) that is able to provide low latencies to time-critical messages. Unlike the standard MQTT, PrioMQTT adopts the UDP/IP stack, which is more suitable than TCP/IP for low-latency communications. Moreover, PrioMQTT introduces a mechanism to prioritize the time-critical messages over the non-time-critical ones. The combination of the UDP/IP stack and priority support in the PrioMQTT protocol is achieved while maintaining the compliance with the MQTT standard message format. As a result, PrioMQTT can be implemented on commercial-off-the-shelves (COTS) devices without hardware modifications. The paper describes the PrioMQTT protocol and investigates its performance through an assessment in a realistic industrial scenario and in comparison with the standard MQTT protocol.},
  archive      = {J_COMCOM},
  author       = {Gaetano Patti and Luca Leonardi and Giuseppe Testa and Lucia Lo Bello},
  doi          = {10.1016/j.comcom.2024.03.018},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {43-51},
  shortjournal = {Comput. Commun.},
  title        = {PrioMQTT: A prioritized version of the MQTT protocol},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LFTDA: A lightweight and fault-tolerant data aggregation
scheme with privacy-enhanced property in fog-assisted smart grid.
<em>COMCOM</em>, <em>220</em>, 35–42. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the data aggregation scheme was widely adopted in smart grids (SG) to protect data privacy while ensuring data availability. Meanwhile, asymmetric homomorphic encryption is a popular technique that guarantees data aggregation and accurate computing results. However, it often brings heavy computation overheads and costs, which increases burdens to SG. In order to lighten SG’s burden, in this paper, a lightweight and fault-tolerant data aggregation scheme (LFTDA) with privacy-enhanced property is proposed. Unlike existing schemes, our proposed scheme uses a lightweight symmetric encryption algorithm and a skillful mathematical structure to achieve data aggregation. In addition, in the proposed scheme, smart meters are allowed to dynamically join and exit from the group while guaranteeing the correctness, security, and efficiency of aggregation. More importantly, our LFTDA is fault-tolerant, which means that the data collection from other devices will not be affected even if some of the smart meters or fog nodes do not work. From the view of security, LFTDA can defend against eavesdropping attack and collusion attack. Finally, we evaluate its performance along with other related schemes in terms of aggregation, decryption, and communication costs, which shows that LFTDA is very competitive and hence is suitable for many practical SG applications.},
  archive      = {J_COMCOM},
  author       = {Zhong Wang and Funing Zhang and Anling Zhang and Jinyong Chang},
  doi          = {10.1016/j.comcom.2024.03.019},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {35-42},
  shortjournal = {Comput. Commun.},
  title        = {LFTDA: A lightweight and fault-tolerant data aggregation scheme with privacy-enhanced property in fog-assisted smart grid},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secrecy enhancement of relay cooperative NOMA network based
on user behavior. <em>COMCOM</em>, <em>220</em>, 23–34. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the perspective of information security, relays exhibit varying levels of trustworthiness when forwarding information. For screening and eliminating the malicious users of cooperative NOMA relay networks, a reputation-based algorithm is proposed, in which the user relays (URs) are categorized into three types based on their behaviors—malicious users, speculative users, and honest users. Meanwhile, in order to minimize information leakage at the relay, cooperative interference signals are transmitted by far user (FU) and near user (NU) via the uplink while the base station (BS) is also transmitting signals to the relay. This approach increases inter-user interference (IUI) to confuse the eavesdropping capability of the relay. To verify the superiority of the proposed scheme, an asymptotic lower bound expression for the ergodic secrecy rate (ESR) of the downlink is derived to characterize the secrecy performance of the proposed scheme. Furthermore, in the case of limited system power, a joint power allocation algorithm based on the golden section method is also proposed to further enhance the security performances of the system. The simulation results validate the accuracy of the derived analysis results and demonstrate that the proposed scheme has a significant improvement in the ESR compared with the traditional scheme.},
  archive      = {J_COMCOM},
  author       = {Xin Song and Runfeng Zhang and Siyang Xu and Haiqi Hao and Jingyi Ma},
  doi          = {10.1016/j.comcom.2024.03.013},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {23-34},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy enhancement of relay cooperative NOMA network based on user behavior},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting abnormal behaviors in smart contracts using opcode
sequences. <em>COMCOM</em>, <em>220</em>, 12–22. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the fast growth of blockchain technology, blockchain as a decentralized distributed ledger technology has become more widely used and is gradually changing our way of life. But it also raises more and more security issues. As there are more and more smart contracts on the blockchain, and smart contracts cannot be changed once they are added to the blockchain, there is an opportunity for hackers to attack smart contracts. If not handled properly, it will cause serious economic losses to users. In this paper, we introduce a unique method for identifying abnormal behaviors of smart contract vulnerabilities using opcode sequences. We aim to identify the control flow paths triggered by transactions to capture the abnormal behaviors of smart contracts. The control flow paths are the traces on which the transaction is executed. Using Geth instrumentation, we collect the opcode sequences executed on the traces to represent the control flow paths. It should be noted that the process of detecting abnormal behaviors introduces some additional time overhead. However, our experimental results show that this method achieves high abnormal detection accuracy with minimal overhead. This suggests that our proposed method is effective in identifying potential security issues in smart contracts without significantly impacting the overall execution time.},
  archive      = {J_COMCOM},
  author       = {Peiqiang Li and Guojun Wang and Xiaofei Xing and Jinyao Zhu and Wanyi Gu and Guangxin Zhai},
  doi          = {10.1016/j.comcom.2024.03.016},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {12-22},
  shortjournal = {Comput. Commun.},
  title        = {Detecting abnormal behaviors in smart contracts using opcode sequences},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Impact of cybersecurity awareness on mobile malware
propagation: A dynamical model. <em>COMCOM</em>, <em>220</em>, 1–11. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent increase in mass media attention to mobile malware, it is still one of the major mobile security threats due to the lack of cybersecurity awareness. The compartment mode can divide the individuals into different states whose name is often a combination of the state names. To explore the impact of the security awareness level of users on susceptible and infected mobile devices, we propose a novel Unaware-Susceptible Aware-Susceptible Latent Breakout Quarantine Recovery ( S u S a L B Q R SuSaLBQR ) compartment model for mobile malware propagation. In this model, the susceptible mobile devices are divided into two sub-classes: the S u Su compartment and the S a Sa compartment depending on the security awareness level of their corresponding users. Based on some reasonable assumptions, a dynamical system is proposed to describe mobile malware propagation behaviors. To explore how malware ultimately spreads, two distinct equilibria ( i.e. , the malware-spreading and malware-free equilibria) are obtained. Furthermore, the global asymptotic stabilities of those two equilibria are proved by constructing the Lyapunov functions, respectively. The above analysis can provide a theoretical basis for controlling mobile malware propagation. Finally, based on the theoretical and numerical results, a discussion is provided. We find that the above two factors can significantly affect the presence and prevalence of malware in mobile networks.},
  archive      = {J_COMCOM},
  author       = {Qingyi Zhu and Xuhang Luo and Yuhang Liu and Chenquan Gan and Yu Wu and Lu-Xing Yang},
  doi          = {10.1016/j.comcom.2024.03.017},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {1-11},
  shortjournal = {Comput. Commun.},
  title        = {Impact of cybersecurity awareness on mobile malware propagation: A dynamical model},
  volume       = {220},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). KDSMALL: A lightweight small object detection algorithm
based on knowledge distillation. <em>COMCOM</em>, <em>219</em>, 271–281.
(<a href="https://doi.org/10.1016/j.comcom.2023.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of computer vision, small object detection has become a research pain point and difficulty in computer vision. Feature acquisition and accurate localization of small objects are two serious challenges that exist for small objects at present. In this paper, a generalized small object detection algorithm is formed based on a multi-scale feature extractor, a feature search network with hybrid attention mechanism, and knowledge distillation. The algorithm firstly performs feature extraction of small objects based on multi-scale feature extractor, secondly uses CBAM attention mechanism and Efficient network to perform feature search on features obtained from the feature map to help obtain more features of the small object, and finally performs knowledge distillation on the baseline model based on the idea of teacher–student knowledge distillation to help the baseline model locate the detected object. In this paper, YOLOv5s is selected as the benchmark experiment, and the designed algorithm is fused to YOLOv5s, compared with the baseline model, the fused model’s experimental metrics mAP on the VOC mixed dataset is improved by 14.45% on average. The experimental results show that the designed algorithm can effectively improve the detection performance of the object detection model for small objects.},
  archive      = {J_COMCOM},
  author       = {Wen Zhou and Xiaodon Wang and Yusheng Fan and Yishuai Yang and Yihan Wen and Yixuan Li and Yicheng Xu and Zhengyuan Lin and Langlang Chen and Shizhou Yao and Liu Zequn and Jianqing Wang},
  doi          = {10.1016/j.comcom.2023.12.018},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {271-281},
  shortjournal = {Comput. Commun.},
  title        = {KDSMALL: A lightweight small object detection algorithm based on knowledge distillation},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-round transmission and contention protocol in WLANs
with multi-packet reception. <em>COMCOM</em>, <em>219</em>, 259–270. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, there are many scenarios in Wi-Fi networks where many devices compete for network access via an Access Point to transmit their information. Thus, the ability to separate multiple packets sent simultaneously using advanced digital processing techniques in the physical layer, known as Multi-Packet Reception (MPR), has become crucial. As this feature may increase network throughput, medium access control (MAC) protocols that efficiently exploit the MPR capability have been investigated. This paper proposes a new MAC protocol that exploits the MPR capability in 802.11 RTS/CTS access mechanism networks. It is based on the use of multiple contention and transmission rounds. In this sense, the MPR MAC protocol schedules several transmission periods once the contention phase finishes. Multiple transmission rounds reduce the overhead and substantially increase throughput. The analytical expressions that support the proposed approach are derived and verified by simulation. The results show that the proposed MAC protocol can yield close to 94% of the ideal performance using only four transmission rounds, compared with 80% of the previous approaches in the literature.},
  archive      = {J_COMCOM},
  author       = {Victor Sandoval-Curmina and Aldo G. Orozco-Lugo and Ramon Parra-Michel and Mauricio Lara},
  doi          = {10.1016/j.comcom.2024.03.008},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {259-270},
  shortjournal = {Comput. Commun.},
  title        = {Multi-round transmission and contention protocol in WLANs with multi-packet reception},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking methodology for IPv4aaS technologies:
Comparison of the scalability of the jool implementation of 464XLAT and
MAP-t. <em>COMCOM</em>, <em>219</em>, 243–258. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method is proposed for the performance and scalability measurements of the IPv4-as-a-Service (IPv4aaS) technologies. It works according to the dual Device Under Test (DUT) setup of RFC 8219 and is suitable for benchmarking any of the five IPv4aaS technologies: Combination of Stateful and Stateless Translation (464XLAT), Dual-Stack Lite (DS-Lite), Lightweight 4over6 (Lw4o6), Mapping of Address and Port with Encapsulation (MAP-E), and Mapping of Address and Port using Translation (MAP-T). The method is based on the reduction of the aggregate of Customer Edge (CE) and Provider Edge (PE) devices to a stateful network address translation from IPv4 to IPv4 (stateful NAT44) gateway. The most important advantage of the novel method is that a stateful NAT44 tester can be used instead of a technology-specific tester, which usually does not exist. The proposed method is validated by the examination of the performance and scalability of the Jool implementation of 464XLAT and MAP-T. Scalability is defined by both (1) how performance increases with the number of active Central Processing Unit (CPU) cores; and (2) how performance decreases with the increasing number of concurrent sessions. Maximum connection establishment rate and throughput are used as performance metrics. The scalability of 464XLAT and MAP-T is measured from 1 to 16 CPU cores and from 1 million to 256 million connections. The measurement details and results are fully disclosed and discussed.},
  archive      = {J_COMCOM},
  author       = {Gábor Lencse and Ádám Bazsó},
  doi          = {10.1016/j.comcom.2024.03.007},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {243-258},
  shortjournal = {Comput. Commun.},
  title        = {Benchmarking methodology for IPv4aaS technologies: Comparison of the scalability of the jool implementation of 464XLAT and MAP-T},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness-oriented multicast routing for distributed
interactive applications. <em>COMCOM</em>, <em>219</em>, 229–242. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facing the challenges of network services on the edge, such as routing considering Quality of Service, is a crucial issue for current networking research efforts. Multicast routing is an essential technique for delivering routing services at a high level of optimization from the perspective of operators and application providers when there are user groups. Furthermore, routing that considers latency constraints has obstacles when it comes to merging different conditions in the solution, particularly when there is a need for fairness in the users’ communication. This work addresses this fairness requirement in multicasting, showing an efficient solution to increase the balance in routing by choosing better options for fair group interaction.},
  archive      = {J_COMCOM},
  author       = {Ibirisol Fontes Ferreira and Maycon Leone Maciel Peixoto and Gustavo Bittencourt Figueiredo},
  doi          = {10.1016/j.comcom.2024.03.015},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {229-242},
  shortjournal = {Comput. Commun.},
  title        = {Fairness-oriented multicast routing for distributed interactive applications},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covert communication in hybrid microwave/mmWave UAV-enabled
systems with transmission mode selection. <em>COMCOM</em>, <em>219</em>,
216–228. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the covert communication in an air-to-ground (A2G) system, where a UAV (Alice) can adopt the omnidirectional microwave (OM) or directional mmWave (DM) transmission mode to transmit covert data to a ground user (Bob) while suffering from the detection of an adversary (Willie). For both the OM and DM modes, we first conduct theoretical analysis to reveal the inherent relationship between the transmit rate/transmit power and basic covert performance metrics in terms of detection error probability (DEP), outage probability and effective covert rate (ECR). To facilitate the transmission mode selection at Alice, we then explore the optimization of transmit rate and transmit power for ECR maximization under the OM and DM modes, and further propose a hybrid OM/DM transmission mode which allows the UAV to adaptively select between the OM and DM modes to achieve the maximum ECR at a given location of UAV. Finally, extensive numerical results are provided to illustrate the covert performances of the concerned A2G system under different transmission modes, and demonstrate that the hybrid OM/DM transmission mode outperforms the pure OM or DM mode in terms of covert performance.},
  archive      = {J_COMCOM},
  author       = {Wenhao Zhang and Ji He and Yulong Shen and Xiaohong Jiang},
  doi          = {10.1016/j.comcom.2024.03.014},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {216-228},
  shortjournal = {Comput. Commun.},
  title        = {Covert communication in hybrid microwave/mmWave UAV-enabled systems with transmission mode selection},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COCKTAIL: Video streaming QoE optimization with chunk
replacement and guided learning. <em>COMCOM</em>, <em>219</em>, 204–215.
(<a href="https://doi.org/10.1016/j.comcom.2024.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive bitrate (ABR) algorithms aim to provide the best user quality of experience (QoE) under fluctuating operating environments. Prior ABR protocols address the QoE maximization problem with a plethora of approximate optimization techniques including model predictive control (MPC), Lyapunov optimization, and deep reinforcement learning (DRL). Even though these algorithms provide adequate performances, they focus only on bitrate selections, precluding the chunk replacement option. We proclaim that chunk replacement can enhance the QoE if duplicated downloading is carefully administered. Moreover, we point out that bitrate selection and chunk replacement should be closely coupled into the optimization problem to fully realize the potential of chunk replacement. We first formulate a novel optimization problem with an expanded decision space that encompasses chunk replacement as well as bitrate selection. We then propose COCKTAIL , a DRL-based ABR algorithm that discovers efficient solutions to the new optimization problem by using several learning techniques. Experiments on real-world network traces show that COCKTAIL outperforms state-of-the-art baselines with improvements up to 16.1% in average QoE.},
  archive      = {J_COMCOM},
  author       = {A-Hyun Lee and Hyeongho Bae and Chong-Kwon Kim},
  doi          = {10.1016/j.comcom.2024.01.014},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {204-215},
  shortjournal = {Comput. Commun.},
  title        = {COCKTAIL: Video streaming QoE optimization with chunk replacement and guided learning},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physical layer security analysis for RIS-aided NOMA systems
with non-colluding eavesdroppers. <em>COMCOM</em>, <em>219</em>,
194–203. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the realm of sixth-generation (6G) wireless systems, there exist two primary imperatives: establishing massive connections and ensuring robust data transmission security. Therefore, this paper delves into the realm of physical layer security (PLS) within the context of a reconfigurable intelligent surface (RIS)-assisted Non-Orthogonal Multiple Access (NOMA) network coupled with the Internet of Things (IoTs), while addressing the challenge of non-concluding eavesdroppers. Specifically, the utilization of NOMA technology is anticipated to yield a substantial enhancement in spectrum efficiency for 6G and forthcoming wireless networks. Furthermore, this study investigates the security aspects through metrics such as the secrecy outage probability (SOP) and the average secrecy capacity (ASC), with the derivation of closed-form approximations for these metrics. Based on these mathematical expressions, we unveil the asymptotic Secrecy Outage Probability (SOP) to extract comprehensive insights into the RIS-assisted NOMA system’s behavior. Furthermore, we employ an algorithm based on the Golden Section to showcase the optimal SOP for a more in-depth analysis. Our findings highlight that the number of RIS metasurface components and the average signal-to-noise ratio at the access point are the primary factors driving improvements in system performance. Finally, we confirmed the correctness of our derived expressions by conducting a comparative analysis between Monte-Carlo simulations and analytical results.},
  archive      = {J_COMCOM},
  author       = {Anh-Tu Le and Tran Dinh Hieu and Tan N. Nguyen and Thanh-Lanh Le and Sang Quang Nguyen and Miroslav Voznak},
  doi          = {10.1016/j.comcom.2024.03.011},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {194-203},
  shortjournal = {Comput. Commun.},
  title        = {Physical layer security analysis for RIS-aided NOMA systems with non-colluding eavesdroppers},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LoRa localisation using single mobile gateway.
<em>COMCOM</em>, <em>219</em>, 182–193. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective use of GPS and mobile networks for localisation in rangeland areas is constrained by their high power consumption and high deployment costs. Long-range (LoRa), a low-power wide area network (LPWAN) technology, can be employed to mitigate these challenges. In contrast to prior research where the prevalent approaches entail multiple gateways. This work proposes a valuable methodology focused on a single mobile LoRa gateway for localisation. A particle filtering and machine learning-based pipeline is employed to map the distance between a target node and the gateway from the received signal strength indicator (RSSI). Particle filtering is used to reduce the impact of noise on the RSSI values. Then, several machine learning techniques, such as support vector machines, random forest, and k-nearest neighbour, are used on the RSSI values to estimate the distance. The estimated distance is then used for tracking using a centroid pseudo-trilateration method. The proposed method was tested in a real-world semi-line-of-sight setting, using three datasets generated by LoRaWAN-specified hardware components and a server. Two forms of experiments were performed: active searching and passive monitoring. We propose an iterative estimation process to address the dilution of precision caused by the initial positions of the gateway required for active searching applications. The results show that active searching typically requires 2 to 3 hops to reach a target node. The accuracy of passive monitoring depends on the proximity of the gateway, which varies from 20 m to 170 m. This proposed approach has the potential to open the way for localising, tracking, or monitoring target objects within sparsely populated rangeland areas, even when resources are severely constrained.},
  archive      = {J_COMCOM},
  author       = {Khondoker Ziaul Islam and David Murray and Dean Diepeveen and Michael G.K. Jones and Ferdous Sohel},
  doi          = {10.1016/j.comcom.2024.03.012},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {182-193},
  shortjournal = {Comput. Commun.},
  title        = {LoRa localisation using single mobile gateway},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A novel hardware efficient design for IEEE 802.11ax
compliant OFDMA transceiver. <em>COMCOM</em>, <em>219</em>, 173–181. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of Multi User (MU) communication in IEEE 802.11ax in the frequency domain via MU Orthogonal Frequency Division Multiple Access (MU-OFDMA) and in the spatial domain via MU Multiple Input Multiple Output (MU-MIMO) enables the Access Point (AP) to serve up to 128 Stations (STAs) in a schedule. However, the MU functionality poses new challenges in the chip design. Existing MU Transceivers (TRXs) rely on the duplication approach wherein a dedicated hardware is utilized per user. The hardware footprint of such an approach increases proportionally with the number of users simultaneously served in the MU TRX. This paper introduces a novel hardware efficient design for an MU TRX. Unlike the duplication approach, the proposed design for the MU TRX has comparable hardware footprint of a Single User (SU) TRX regardless of the number of users being served, thanks to the hardware virtualization technique. The applicability of the design is initially validated for IEEE 802.11ax compliant MU-OFDMA transmitter on an FPGA of a modern SDR. The performance and hardware consumption is compared against the conventional duplication approach. The proof-of-concept implementation focuses on 20MHz, where maximum 9 STAs can be involved. Though we believe the design is extendable to support the maximum number of STAs in MU-OFDMA for IEEE 802.11ax standard. The experimental results show that the hardware virtualization based MU-OFDMA transmitter provides the same performance and consumes less than 13% hardware resources in comparison with the conventional duplication approach.},
  archive      = {J_COMCOM},
  author       = {Muhammad Aslam and Xianjun Jiao and Wei Liu and Michael Mehari and Thijs Havinga and Ingrid Moerman},
  doi          = {10.1016/j.comcom.2024.03.006},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {173-181},
  shortjournal = {Comput. Commun.},
  title        = {A novel hardware efficient design for IEEE 802.11ax compliant OFDMA transceiver},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated aerial assessment for seamless adaptive adhoc
restoration in partially collapsed network. <em>COMCOM</em>,
<em>219</em>, 153–172. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disasters often result in widespread infrastructure failures, leading to network blackouts that impede critical information dissemination and hamper search-and-rescue operations. This paper presents a novel methodology for expedited assessment and restoration of network connectivity in disaster-affected regions. While existing approaches have made significant contributions, they still face either one or a combination of challenges such as time-consuming procedures, limited coverage, integration complexities, and inaccessibility in certain areas. To address these challenges, we propose two innovative contributions: (1) a novel automated aerial assessment technique for remote identification of network blackouts, offering enhanced accessibility, reduced response times, and heightened reliability; (2) a pioneering automated approach utilizing personal mobile devices (PMDs) to establish alternate multi-hop ad hoc routes from affected regions, enabling auxiliary network formation and seamless integration with surviving networks. The proposed route setup, activated upon detection of network blackouts, facilitates self-managed network maintenance, which adapts to gradual restoration processes, thereby optimizing efficiency, connectivity span, stability, and energy conservation. Both methodologies are rigorously evaluated through simulations and test-bed experiments, demonstrating substantial advancements over existing methodologies. This research contributes to the field by significantly enhancing disaster response capabilities through improved network resilience and rapid connectivity restoration strategies.},
  archive      = {J_COMCOM},
  author       = {Vipin Kumar Pandey and Suddhasil De and Sukumar Nandi},
  doi          = {10.1016/j.comcom.2024.03.010},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {153-172},
  shortjournal = {Comput. Commun.},
  title        = {Automated aerial assessment for seamless adaptive adhoc restoration in partially collapsed network},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on integrated computing, caching, and communication
in the cloud-to-edge continuum. <em>COMCOM</em>, <em>219</em>, 128–152.
(<a href="https://doi.org/10.1016/j.comcom.2024.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud and edge computing have proposed different functionalities to enable multiple applications requiring different communication, computing, and caching (3C) resources. The upcoming futuristic applications (e.g., metaverse, holographic, and haptic communication) impose further stringent requirements (e.g., ultra-low latency, ultra-high reliability) on the infrastructure. These requirements call for a paradigm shift in the infrastructure architecture where all resource components and owners collaborate from the cloud up to the edge, creating a cloud-to-edge continuum of integrated resources. Furthermore, we argue that artificial intelligence (AI) and collaborative-based decisions are promising techniques to efficiently manage the highly complex architecture that jointly leverages 3C in the continuum. This article presents a comprehensive survey of existing research, including AI and collaborative-based studies, targeting the effective and seamless provision of 3C resources and services in the cloud-to-edge continuum. Through an extensive analysis of driving use cases, the synergy between these three main services is scrutinized to highlight its crucial role in the next-generation network infrastructures (NGNI). Finally, a discussion on the opportunities and challenges brought by integrating 3C in NGNI from different perspectives, including architectural design as well as the regulatory and business aspects, are presented.},
  archive      = {J_COMCOM},
  author       = {Adyson Maia and Akram Boutouchent and Youcef Kardjadja and Manel Gherari and Ece Gelal Soyak and Muhammad Saqib and Kacem Boussekar and Idil Cilbir and Sama Habibi and Soukaina Ouledsidi Ali and Wessam Ajib and Halima Elbiaze and Ozgur Erçetin and Yacine Ghamri-Doudane and Roch Glitho},
  doi          = {10.1016/j.comcom.2024.03.005},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {128-152},
  shortjournal = {Comput. Commun.},
  title        = {A survey on integrated computing, caching, and communication in the cloud-to-edge continuum},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secrecy outage performance of MIMO-NOMA relay systems with
MRT/MRC schemes. <em>COMCOM</em>, <em>219</em>, 116–127. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) systems have indicated that they can provide higher spectral efficiency and secrecy capability compared with orthogonal multiple access (OMA) conventional systems. Therefore, the NOMA technique is expected to replace the OMA systems. Moreover, multiple-input multiple-output (MIMO) communications can achieve a high-level security thank to high channel gain between users. However, the presence of eavesdroppers is a problem for future wireless systems. Consequently, we investigate the secrecy outage probability (SOP) of the MIMO - NOMA relay system under the presence of eavesdroppers with multiple antennas over Rayleigh fading channels. In order to improve legitimate link’s capacity, the maximum ratio transmission (MRT) and maximum ratio combination (MRC) schemes are applied at the transmitter and receivers, respectively. To evaluate the secrecy performance, the closed-form expressions of the SOP of each user, the overall SOP of the considered MIMO-NOMA relay system are derived, and then an impact of channel estimation error and location of eavesdroppers on the SOP of the system is explored. Moreover, Monte-Carlo simulations are utilized to verify the correctness of all derived mathematical expressions. Numerical results show that owning to utilizing multiple antennas, the considered MIMO-NOMA relay system in MRT and MRC schemes has superior secrecy performance than transmit antenna selection and selection combining schemes. Furthermore, the location of relay and power allocation coefficient of destination users are optimized in the sense of minimum SOP.},
  archive      = {J_COMCOM},
  author       = {Nguyen Le Cuong and Tran Manh Hoang and Pham Thanh Hiep},
  doi          = {10.1016/j.comcom.2024.03.004},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {116-127},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy outage performance of MIMO-NOMA relay systems with MRT/MRC schemes},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relay selection in underwater acoustic sensor networks for
QoS-based cooperative communication using game theory. <em>COMCOM</em>,
<em>219</em>, 104–115. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Acoustic Sensor Networks (UASN) plays a crucial role in monitoring and transmitting environmental information for marine resource exploration. However, underwater communication faces significant challenges such as signal attenuation, noise interference, and unstable underwater environments. Moreover, with the deployment of a large number of nodes, inefficient resource allocation strategies in cooperative communication lead to strained communication resources as well as increased energy consumption. This paper introduces a novel relay selection approach based on game theory that takes into account both competition and cooperation among nodes. By building a potential game model and a utility function, we derive the optimal relay selection strategy that can enhance system Quality of Service (QoS). We also define conditions for the feasibility of pure strategy Nash Equilibria (NE) and develop a low-complexity QoS-based Relay Selection Iterative Algorithm (QoS-RSIA) to procure feasible pure strategy NE for the designed game. Through comparative analysis with other relay selection algorithms, simulation results show that the proposed method not only enhances system throughput and average spectral efficiency by at least 8.3%, but also significantly reduces time complexity compared to exhaustive search algorithms.},
  archive      = {J_COMCOM},
  author       = {Fang Ye and Hengyu Xu and Jingpeng Gao},
  doi          = {10.1016/j.comcom.2024.03.003},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {104-115},
  shortjournal = {Comput. Commun.},
  title        = {Relay selection in underwater acoustic sensor networks for QoS-based cooperative communication using game theory},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interaction behavior enhanced community detection in online
social networks. <em>COMCOM</em>, <em>219</em>, 92–103. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection in online social networks (OSNs) has attracted a great deal of attention in recent years due to its potential prospects in applications such as recommendation systems, spam detection, targeted advertising, etc. However, community detection methods that have been proposed so far rely mostly on static information, such as the topology of the network, the attributes of the users, etc. In this paper, we propose a framework that enhances community detection with user interaction behaviors in OSNs since such behaviors should be important factors in characterizing the communities. In the framework, the various features of user interaction behaviors are expressed in the form of interaction indices through utilizing the cumulative distribution function and interaction divergence. The indices are then transformed into edge weights through a logistic regression model and combined with the static information to construct a weighted graph for the OSN. Finally, community detection is realized by applying a modularity maximization algorithm. A comprehensive evaluation using diverse real-world datasets shows that our proposed framework has superior performance over comparable ones in deriving high-quality communities. Critical metrics, such as modularity, average clustering coefficient, and average conductance, are evaluated to demonstrate the effectiveness of our approach. Furthermore, the synergy between network topology and user interaction behavior in community detection is also investigated.},
  archive      = {J_COMCOM},
  author       = {Xiangjun Ma and Jingsha He and Tiejun Wu and Nafei Zhu and Yakang Hua},
  doi          = {10.1016/j.comcom.2023.11.029},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {92-103},
  shortjournal = {Comput. Commun.},
  title        = {Interaction behavior enhanced community detection in online social networks},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive fault diagnosis in UAV-assisted sensor
networks: A three-phase automated approach. <em>COMCOM</em>,
<em>219</em>, 76–91. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring operational safety in an automated working environment is a crucial requirement. In this article, we propose a technique for automated fault diagnosis in unmanned aerial vehicle-based sensor networks (UAV-sensor networks) aimed explicitly at the composite nature of different faults. Our technique consists of 3-stages: (1) UAVs collecting data from sensors and running a modified Z-score statistics to diagnose faulty readings; (2) the UAV cluster head running a multivariate analysis of variance (MANOVA) to identify data faults in cluster members; and (3) utilizing a probabilistic neural network with correlation centroids at the ground control station for further diagnosis of collected UAV data. This approach is intended for progressive improvement in fault diagnosis at different stages of data collection in a UAV-sensor network for critical applications. We conduct an extensive evaluation of our 3-stage approach on the data collected in real-time from the sensors in a physically created UAV-sensor network. The performance of the proposed modified Z-score test improves fault detection accuracy of ∼ 20.8%, false alarm rate of ∼ 54.3%, and false positive rate of ∼ 67.2% with the traditional and existing sensor fault detection approach. The performance of MANOVA to identify the permanent UAV data fault shows fault detection accuracy of ∼ 10.9%, false alarm rate of ∼ 44.3%, and false positive rate of ∼ 48.7% superior to the intermittent and transient faults. Our preliminary results suggest that the proposed approach can handle multiple composite faults and outperforms existing research results.},
  archive      = {J_COMCOM},
  author       = {Sipra Swain and Pabitra Mohan Khilar and Biswa Ranjan Senapati and Rakesh Ranjan Swain},
  doi          = {10.1016/j.comcom.2024.03.002},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {76-91},
  shortjournal = {Comput. Commun.},
  title        = {Comprehensive fault diagnosis in UAV-assisted sensor networks: A three-phase automated approach},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STI: A self-evolutive traffic identification system for
unknown applications based on improved random forest. <em>COMCOM</em>,
<em>219</em>, 64–75. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence technology has proven potential and effective in traffic identification for network management and security. However, the accuracy of its identification is easily influenced by the massive unknown traffic. Given the fact of numerous unknown applications in real networks and even more as time goes by, a promising traffic identification system should have the ability to discover unknown applications and recognize their traffic. In this paper, an innovative and comprehensive traffic identification system, called STI, is proposed, which can achieve fulfilling high precision both on known and unknown traffic identification. More importantly, STI can self-evolve to identify incoming unknown applications and corresponding training samples based on a novel clustering process with minimal manual involvement. In addition, an improved random forest and a novel similarity calculation method are proposed and applied to STI to enhance the classification performance. Experiments on real network traffic demonstrate the core advantages of the proposed system.},
  archive      = {J_COMCOM},
  author       = {Yulong Liang and Fei Wang and Shuhui Chen and Beier Chen and Yunjiao Bo},
  doi          = {10.1016/j.comcom.2024.02.010},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {64-75},
  shortjournal = {Comput. Commun.},
  title        = {STI: A self-evolutive traffic identification system for unknown applications based on improved random forest},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AP-assisted adaptive video streaming in wireless networks
with high-density clients. <em>COMCOM</em>, <em>219</em>, 53–63. (<a
href="https://doi.org/10.1016/j.comcom.2024.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive video streaming over wireless networks has experienced tremendous growth in past few years. In order to guarantee users’ quality of experience (QoE), adaptive bitrate (ABR) algorithms have been extensively studied. With the recent emergency of high-density Wi-Fi networks, these solutions no longer perform well. On the one hand, bitrate decisions respond slowly to high network fluctuations, and on the other hand, wireless channel resources are insufficient especially under the cases that multiple clients compete for the limited bandwidth. Hence, users’ personalized QoE metrics need to be taken into account to cope with the QoE degradation. To this end, we design an access point (AP) assisted wireless dynamic adaptive video streaming over Hypertext Transfer Protocol (HTTP) solution called Wi-DASH, which aims to improve users’ QoE while considering channel utilization. In Wi-DASH, the video server aggregates real-time network status with the assistance of AP, and estimates clients’ player status through statistical analysis of chunk request logs. On this basis, a deep reinforcement learning (DRL) based ABR algorithm is designed, where the DRL model can deal with the complicated global status information including network status, player status, and QoE preferences. Finally, we implement Wi-DASH system, and conduct experiments with 4K resolution videos. Experimental results reveal that the Wi-DASH can more fully utilize wireless channel resources, and significantly improve users’ QoE.},
  archive      = {J_COMCOM},
  author       = {Wenjia Wu and Jiale Yuan and Sheng Ma and Ming Yang},
  doi          = {10.1016/j.comcom.2024.03.001},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {53-63},
  shortjournal = {Comput. Commun.},
  title        = {AP-assisted adaptive video streaming in wireless networks with high-density clients},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization and analysis of air-to-ground wireless link
parameters for UAV mounted adaptable radar antenna array.
<em>COMCOM</em>, <em>219</em>, 42–52. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) are rapidly used in a variety of applications such as surveillance, search and rescue operation, delivery, etc. These operations strongly depend on the performance of the air-to-ground (A2G) communication link. This link communicates vital information between the UAV and the ground station (GS). However, due to the UAV’s unpredictable mobility, the A2G link becomes unreliable, which limits the quality of data transfer. An Adaptable Radar Antenna Array (ARAA) can be used to solve this problem by effectively handling transmitted data through altering the antenna pattern using the beamforming process. This work proposes a new algorithm for selecting antenna patterns with proper beamforming by considering the effect of wobbling and mutual coupling (MC). In addition, the proposed algorithm is used to reduce the maximum sidelobe level (SLL) and optimize the distance between the interelement of ARAA, which directly controls the MC effect in a time-varying channel. Also, an analysis is done for various antenna configurations, such as Uniform Linear Array (ULA), Uniform Rectangular Array (URA), and Uniform Circular Array (UCA), while varying different UAV parameters like altitude, speed, location error, and beamforming capacity.},
  archive      = {J_COMCOM},
  author       = {Priti Mandal and Lakshi Prosad Roy and Santos Kumar Das},
  doi          = {10.1016/j.comcom.2024.02.021},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {42-52},
  shortjournal = {Comput. Commun.},
  title        = {Optimization and analysis of air-to-ground wireless link parameters for UAV mounted adaptable radar antenna array},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient and certificateless conditional
privacy-preserving authentication and key agreement scheme for smart
healthcare. <em>COMCOM</em>, <em>219</em>, 29–41. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart healthcare overcomes the limitations of time and space, allowing users to access medical and health services anywhere and anytime, thus improving the quality and efficiency of these services. However, due to its excessive reliance on wireless public networks, smart healthcare faces challenges and issues related to communication security. As a result, authentication and key agreement are critical as the first line of defense for smart healthcare communications. Over the past few years, numerous authentication and key agreement schemes have been proposed by experts and scholars, but most of these schemes are not applicable to practical scenarios due to their lack of security and efficiency. To address these issues, we propose an elliptic curve-based certificateless conditional privacy-preserving authentication and key agreement scheme. This scheme does not require endorsement from a Certificate Authority (CA) and features lightweight and secure properties, making it suitable for resource-limited smart healthcare communication. In addition, the security of the proposed scheme is proven under the random oracle model, and its safety is supplemented using BAN logic. Finally, performance analysis reveals that the proposed scheme not only satisfies security property and can withstand general attacks but also offers certain advantages in communication efficiency.},
  archive      = {J_COMCOM},
  author       = {Yihao Hu and Chunguang Huang and Hai Cheng},
  doi          = {10.1016/j.comcom.2024.02.020},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {29-41},
  shortjournal = {Comput. Commun.},
  title        = {An efficient and certificateless conditional privacy-preserving authentication and key agreement scheme for smart healthcare},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancement of edge-based surveillance videos based on
bilateral filtering. <em>COMCOM</em>, <em>219</em>, 19–28. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital pictures are modified during the process of image enhancement to provide outcomes that are more suited for display or additional image analysis. Up to now, many achievements have been made in the field of image enhancement processing. Image enhancement algorithms is consisted of deblurring, filtering, and contrast methods. And Gaussian filtering is the most widely used image enhancement algorithm among all. It is a linear filtering methods which serves to image smoothing and noise-removing But at the same time, it may lose details or produce halos when decomposing images, and it will not work well when processing images affected by strong light or insufficient illumination. Nowadays, deep learning algorithms are applied in the image enhancement too. Neural networks like AlexNet and many other convolutional neural networks. The application of the CNN helps to solve the problem of limited accuracy but at the same time it cost more time for calculation. Also, the gray value of the pixel is not considered. To address the issue of image enhancement under complex lighting environments, we utilize an image enhancement approach based on bilateral filtering to achieve effective image enhancement in too-dark or too-bright lighting and also remove the problem of blurring edge caused by Gaussian filtering. Bilateral filtering could remove the noise and remain the information of the edges at the same time. Due to its own mathematical properties, it has the advantage to process the color image. Also, bilateral filtering could be beneficial to the process of edge-based surveillance videos. The efficiency of the surveillance videos has always been an issue. At the most of the time, the video could be vague and the problem of over exposure is more frequent than the tranquil image. For the edge-based surveillance video, the most important thing is speed and the clarity. And bilateral filtering offers one of the solution to the enhancement of the video and make it more clear.},
  archive      = {J_COMCOM},
  author       = {Simai Chen and Jia Xu and Qiyou Wu},
  doi          = {10.1016/j.comcom.2024.01.031},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {19-28},
  shortjournal = {Comput. Commun.},
  title        = {Enhancement of edge-based surveillance videos based on bilateral filtering},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on privacy-preserving authentication protocols for
secure vehicular communication. <em>COMCOM</em>, <em>219</em>, 1–18. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure vehicular communication is one of the challenges of vehicular technology. The main goal of vehicular ad hoc networks is to prevent road accidents and collisions by transferring traffic signals and essential information to vehicles securely. Secure vehicular communication relies significantly on authentication techniques. There are numerous privacy-preserving authentication techniques for secure vehicular communication. Most of the protocols cannot provide security and privacy with less communication and computational costs. This survey focuses on the classification of various cryptographic authentication techniques that can provide secure vehicular communication. This study discusses the properties, advantages, and limitations of various cryptographic authentication techniques.},
  archive      = {J_COMCOM},
  author       = {Kartick Sutradhar and Beena G. Pillai and Ruhul Amin and Dayanand Lal Narayan},
  doi          = {10.1016/j.comcom.2024.02.024},
  journal      = {Computer Communications},
  month        = {4},
  pages        = {1-18},
  shortjournal = {Comput. Commun.},
  title        = {A survey on privacy-preserving authentication protocols for secure vehicular communication},
  volume       = {219},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimization NPUSCH uplink scheduling approach for NB-IoT
application via the feasible combinations of link adaptation, resource
assignment and energy efficiency. <em>COMCOM</em>, <em>218</em>,
276–293. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-power wide area (LPWA) technology, specifically Narrowband Internet of Things (NB-IoT), has emerged as a pivotal sixth-generation (6G) network technology, catering to the demands of massive-type communications ( m -MTC), extended battery life, wide range coverage, and high reliability. Despite its advancements, existing studies often fall short in simultaneously addressing energy efficiency, latency, link adaptation, and resource allocation in the bandwidth-constrained environments typical of NB-IoT. The complexity of finding a comprehensive optimal solution for resource allocation is compounded by its non-convex and NP-hard combinatorial nature. This research addresses these challenges by introducing a Hybrid-Optimizer approach, which integrates link adaptation, resource allocation, and scheduling algorithms. This approach aims to identify the optimal feasible combination for each device (UE), focusing on energy conservation and improved signal reception. Applied to the Narrow Physical Uplink Shared Channel (NPUSCH) uplink scheduling in NB-IoT, this method is benchmarked against the conventional Round-Robin scheduling algorithm. Simulation results demonstrate that the Hybrid-Optimizer approach significantly outperforms the Round-Robin algorithm in terms of scheduling processing time and energy efficiency. Furthermore, the proposed algorithm is capable of generating candidate combinations that enhance signal reception in UEs and meet the required subframe criteria, adhering to system modeling assumptions for scheduling optimization.},
  archive      = {J_COMCOM},
  author       = {Agung Mulyo Widodo and Hsing-Chung Chen},
  doi          = {10.1016/j.comcom.2024.02.016},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {276-293},
  shortjournal = {Comput. Commun.},
  title        = {An optimization NPUSCH uplink scheduling approach for NB-IoT application via the feasible combinations of link adaptation, resource assignment and energy efficiency},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Provably secured and lightweight authenticated encryption
protocol in machine-to-machine communication in industry 4.0.
<em>COMCOM</em>, <em>218</em>, 263–275. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 and the industrial Internet of Things (IIoT) aim to create a platform for data-driven decision-making through machine-to-machine (M2M) communication, often facilitated by the 6LoWPAN standard. However, as a resource-constrained device, 6LoWPAN raises security and privacy concerns for M2M communications, necessitating efficient and lightweight authentication and key establishment (AKE) protocols. Existing AKE protocols relying on asymmetric and symmetric cryptographic keys are susceptible to attacks and entail significant storage, communication, and computation overheads. This study examines a scheme called SLAP to uncover vulnerabilities and challenges in AKE-based M2M deployments in IIoT, including traceability, denial of service (DoS), perfect forward secrecy (PFS), and ephemeral secret leakage (ESL) attacks. Therefore, a privacy-preserving, secure, and lightweight authenticated encryption protocol called provably secure, lightweight, authenticated encryption (PSLAE) is proposed to address these issues. This approach includes hash operations, XOR operations, and authenticated encryption primitives for lightweight and secure mechanisms. It uses a one-time alias identity and fresh parameters to ensure privacy and protection against traceability and DoS, PFS, and ESL attacks. PSLAE undergoes rigorous informal and formal verification through SVO logic and Scyther, demonstrating resilience against the extended Canetti–Krawczyk and Dolev–Yao threat models. Moreover, it provides a lightweight, secure, efficient, and reduced storage, communication, and computation overhead compared with related works for AKE-based M2M in IIoT.},
  archive      = {J_COMCOM},
  author       = {Fatma Foad Ashrif and Elankovan A. Sundararajan and Mohammad Kamrul Hasan and Rami Ahmad and Aisha-Hassan Abdalla Hashim and Azhar Abu Talib},
  doi          = {10.1016/j.comcom.2024.02.008},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {263-275},
  shortjournal = {Comput. Commun.},
  title        = {Provably secured and lightweight authenticated encryption protocol in machine-to-machine communication in industry 4.0},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning to parallel offloading in heterogeneous
wireless networks. <em>COMCOM</em>, <em>218</em>, 253–262. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the forthcoming era of 6G, the deployment of dense and diverse wireless networks equipped with edge servers makes parallel offloading a crucial technology for multiple access edge computing. However, the effectiveness of parallel offloading is constrained by heterogeneous edge servers (HES) and heterogeneous wireless networks (HWN). Existing solutions often assume that the link state of the wireless network and the computing resources of the edge server are either known or can be estimated as prior knowledge, which is insufficient, especially for HWN. To address this challenge, we propose a novel online learning parallel offloading (OLPO) framework aimed at jointly selecting multiple radio networks and multiple edge servers (MRMS) by learning the unknown and stochastic metrics of wireless links and edge servers. Specifically, we design and compare three OLPO algorithms using multi-armed bandits (MAB) and construct a comparison matrix with the analytic hierarchy process (AHP). Our experimental results demonstrate the effectiveness of the proposed OLPO algorithms and establish their applicability for various traffic types, considering metrics such as delay, delay jitter, packet loss, rate, and energy delay product.},
  archive      = {J_COMCOM},
  author       = {Yulin Qin and Jie Zheng and Hai Wang and Ling Gao and Yuhui Ma and Shuo Ji and Yi Liu and Jie Ren and Rui Cao and Yongxing Zheng},
  doi          = {10.1016/j.comcom.2024.02.017},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {253-262},
  shortjournal = {Comput. Commun.},
  title        = {Online learning to parallel offloading in heterogeneous wireless networks},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TDS-NA: Blockchain-based trusted data sharing scheme with
PKI authentication. <em>COMCOM</em>, <em>218</em>, 240–252. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing has received much attention and research as an excellent way to unlock the value of data. Trusted data storage servers will participate in the data-sharing system to provide users with convenient data access and storage services. Currently, the vast majority of researchers design data-sharing systems based on centralized trusted authorities and key management centers, but they tend to ignore the problems of trust dependency and data leakage that exist in centralized trust and lead to the overall untrustworthiness of the system. To solve the above problems, this paper considers the use of public key infrastructure (PKI) to provide trusted authentication for data-sharing entities, but the traditional PKI has CA root trust and scenario adaptation problems, so we optimize the traditional PKI model for data-sharing scenarios and call it an improved PKI. Combining the decentralized trust property of blockchain, this paper proposes a TDS-NA scheme based on blockchain and improved PKI to build a distributed trusted, and secure data-sharing system in a semi-trusted network environment. TDS-NA can secure shared data in data sharing, and provide digital certificates that support entity-trusted authentication and reliable access control while designing digest blocks for efficient data auditing. In this paper, we demonstrate that the TDS-NA scheme is able to resist man-in-the-middle attacks and certificate forgery attacks through formal security analysis while satisfying the necessary security properties of data-sharing systems. We implement a prototype of the TDS-NA scheme in ethereum smart contracts and finally verify the security and feasibility of TDS-NA through experimental comparison and analysis.},
  archive      = {J_COMCOM},
  author       = {Zhenshen Ou and Xiaofei Xing and Siqi He and Guojun Wang},
  doi          = {10.1016/j.comcom.2024.02.018},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {240-252},
  shortjournal = {Comput. Commun.},
  title        = {TDS-NA: Blockchain-based trusted data sharing scheme with PKI authentication},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GEMLIDS-MIOT: A green effective machine learning intrusion
detection system based on federated learning for medical IoT network
security hardening. <em>COMCOM</em>, <em>218</em>, 209–239. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing use of Internet of Things (IoT) gadgets in a daily rate has heightened security apprehension, particularly within the healthcare sector. In order to prevent the unauthorized disclosure of sensitive data, it is imperative for Internet of Things (IoT) systems to promptly and effectively respond to harmful activities. Nevertheless, the act of transferring data to distant cloud servers for the purpose of analysis gives rise to both temporal delays and apprehensions regarding privacy. To ensure the security of medical Internet of Things (MIoT) networks, a power-efficient Intrusion Detection System (IDS) is employed for three primary objectives that it will result in three stages of execution: (i) The objective is to categorize different types of attacks, such as Man-in-the-Middle (MitM) and Distributed Denial of Service (DDoS), by utilizing well-established machine learning (ML) techniques. This classification stage will serve to enhance the Intrusion Detection System (IDS) and the reporting system. (ii) Anomaly detection (unknown attack identification), or detection of unknown attacks, will be employed to identify previously unknown attacks. This identification stage involves retraining the ML model to enable future recognition and classification of these unknown attacks when the anomaly attack detector identifies that an unknown attack is recognized. Then, a retraining of the first stage classification model is executed due to the anomaly detection. (iii) To ensure that a remote cloud server remains current with the latest classification model changes, Federated Learning (FL) will be utilized. FL allows for collaborative model training while preserving data privacy and security. The experimental findings indicate that the Enhanced Random Forest (also called ensemble random forest) algorithm achieves a remarkable accuracy rate of 99.98% in classifying attacks. Thus, it will be our first stage classifier. Continuing, the One-Class Support Vector Machine (SVM) algorithm demonstrates a high level of accuracy, reaching 99.7% in detecting anomalies so that it will be our second stage identifier. Finally, the third-stage approach, which has as a target the overall system model updater, will be our introduced Federated Learning approach that works with the Enhanced Random Forests and identifies the ERF differences from the old model in an optimal way. The efficacy of our technique is confirmed through the implementation of experiments involving an Internet of Things (IoT) system and a Raspberry Pi MIoT gateway and with simulations that simulate the FL updating process. These experiments successfully identify known and unknown attacks with a high reliability level while limiting resource utilization and energy consumption. Future studies of this work will focus on enhancing the scalability and efficiency of our Intrusion Detection System in MIoT networks.},
  archive      = {J_COMCOM},
  author       = {Iacovos Ioannou and Prabagarane Nagaradjane and Pelin Angin and Palaniappan Balasubramanian and Karthick Jeyagopal Kavitha and Palani Murugan and Vasos Vassiliou},
  doi          = {10.1016/j.comcom.2024.02.023},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {209-239},
  shortjournal = {Comput. Commun.},
  title        = {GEMLIDS-MIOT: A green effective machine learning intrusion detection system based on federated learning for medical IoT network security hardening},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving and verifiable data aggregation for
internet of vehicles. <em>COMCOM</em>, <em>218</em>, 198–208. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Internet of Vehicles (IoV), organizations or institutions are increasingly interested in aggregating vehicle perception data to perform data analysis. Nevertheless, the existing solutions are difficult to meet the comprehensive requirements of IoV scenario, such as low latency, privacy preservation and rich functions. Additionally, it is more challenging that the aggregation node may return tampered or forged aggregation results for vulnerabilities or ulterior motives. To this end, we propose a Privacy-Preserving and Verifiable Data Aggregation Scheme for IoV (PPVDA), which allows the efficient aggregation for inner product over multi-dimensional data, and the data requester can verify the correctness and integrity of the aggregation results. By exploiting homomorphic MAC and secret sharing techniques, we construct a lightweight and verifiable mechanism for results as well as data privacy protection. We also conduct a comprehensive security analysis of PPVDA under the malicious security model. Moreover, extensive theoretical analyses and experimental evaluations demonstrate that PPVDA performs efficiently while retaining more desired properties. When dealing with 4 × 1 0 4 4×104 VNs, 3000 RSUs and 500-dimensional vectors, each EN side only takes about 74.5 ms and the DR side only takes about 3.8 ms, is therefore suitable for practical IoV scenarios.},
  archive      = {J_COMCOM},
  author       = {Fucai Zhou and Qiyu Wu and Pengfei Wu and Jian Xu and Da Feng},
  doi          = {10.1016/j.comcom.2024.02.022},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {198-208},
  shortjournal = {Comput. Commun.},
  title        = {Privacy-preserving and verifiable data aggregation for internet of vehicles},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A secure certificateless ring signcryption scheme based on
SM2 algorithm in smart grid. <em>COMCOM</em>, <em>218</em>, 188–197. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of smart grids, bidirectional transmission of electricity information enables real-time electricity generation tailored to consumer needs. However, ensuring user privacy during data collection has emerged as a significant concern with the proliferation of data collection and transmission capabilities. Existing solutions such as group signature and pseudonym systems have limitations, such as a lack of trustworthiness in group signature administrators and increased system costs associated with pseudonym storage. To address these drawbacks, this paper proposes a certificateless ring signcryption scheme with conditional privacy protection based on the SM2 algorithm. The scheme efficiently enables users to ring signcryption-transmitted messages, thereby concealing the sender’s identity from the message receiver. This approach resolves the privacy concerns mentioned earlier. In addition, tracking algorithm and batch verification algorithm have been designed to improve computational efficiency while also providing the ability for trusted parties to track malicious users. This scheme achieves conditional privacy preservation while avoiding substantial storage costs for power resources. Compared to the latest available programs, our proposed scheme offers enhanced efficiency and lower communication costs.},
  archive      = {J_COMCOM},
  author       = {Shuanggen Liu and Zhentao Liu and Jueqin Liang and Wanju Zhang and Zirong Heng},
  doi          = {10.1016/j.comcom.2024.02.015},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {188-197},
  shortjournal = {Comput. Commun.},
  title        = {A secure certificateless ring signcryption scheme based on SM2 algorithm in smart grid},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A physical layer security scheme for 6G wireless networks
using post-quantum cryptography. <em>COMCOM</em>, <em>218</em>, 176–187.
(<a href="https://doi.org/10.1016/j.comcom.2024.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sixth generation (6G) of mobile networks is poised to revolutionize communication capabilities with its infinite-like reach. These networks will feature an ultra-dense topology, accommodating a wide range of devices, from macro devices like satellites to nano-devices integrated within the human body. However, the extensive data traffic handled by 6G networks, a significant portion of which is sensitive in nature, presents a security challenge. This paper presents the design and implementation of an encryption scheme that ensures the confidentiality of data transmission in the physical layer of 6G networks. The proposed physical layer security architecture relies on lattice cryptography, wherein each user is associated with a pair of bases (a public basis and a private basis) and a set of orthogonal sub-carriers. The encryption process involves projecting the vector of the transmitted data’s quadrature amplitude modulation (QAM) symbols onto the user’s public basis. A random error vector is then added before applying the inverse Fourier transform of the orthogonal frequency division multiplexing (OFDM) technique. The security of this encryption scheme hinges on the complexity of the closest vector problem in an integer lattice. To evaluate its efficacy, we analyze the security of our lattice-based physical layer encryption concerning the properties of the base pairs and error vectors. Our findings indicate that the proposed scheme offers satisfactory security protection against eavesdropping attacks by significantly enhancing the confidentiality of the transmitted signal. Furthermore, we assess the performance of our design through numerical experiments, demonstrating its resilience against various security attacks.},
  archive      = {J_COMCOM},
  author       = {Walid Abdallah},
  doi          = {10.1016/j.comcom.2024.02.019},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {176-187},
  shortjournal = {Comput. Commun.},
  title        = {A physical layer security scheme for 6G wireless networks using post-quantum cryptography},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distinguished-bit tracking knowledge-based query tree for
RFID tag identification. <em>COMCOM</em>, <em>218</em>, 166–175. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {—Many RFID systems own the information of RFID tags in the database, so the reader knows how many tags will be identified and which tags’ IDs might appear. How the RFID anti-collision protocols take advantage of this knowledge is a challenge. Some previous anti-collision protocols, including Query Tree with Shortcutting and Couple Resolution (QTSC) and Bit-tracking Knowledge-based Query Tree (BKQT), solved this task with the knowledge. We propose one novel protocol, Distinguished-bit Tracking Knowledge-based Query Tree (DKQT), which integrates the knowledge with the bit-tracking technique. Utilizing the knowledge and knowing the collided-bit locations, DKQT adopts a distinguishable technique, which can identify the tags that have a distinguished-bit representing that only one tag has a unique “0” or “1” in a bit location of the tag ID, to speed up the tag identification. Innovatively, adopting a distinguishable technique, DKQT forms an n -ary tree where a node has some leaves that are the tags having distinguished-bits. Thus, DKQT using the distinguishable technique can recognize multiple appearing tags in a collision by traversing this n -ary tree. The simulation results show that DKQT can reduce the number of slots by 36.94 % and 15.58 %, and the identification time by 37.47 % and 17.86 %, compared to the previous knowledge-based protocols, QTSC and BKQT, respectively.},
  archive      = {J_COMCOM},
  author       = {Chih-Chung Lin and Yuan-Cheng Lai and Zelalem Legese Hailemariam},
  doi          = {10.1016/j.comcom.2024.01.017},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {166-175},
  shortjournal = {Comput. Commun.},
  title        = {A distinguished-bit tracking knowledge-based query tree for RFID tag identification},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radio-in-the-loop simulation and emulation modeling for
energy-efficient and cognitive internet of things in smart cities: A
cross-layer optimization case study. <em>COMCOM</em>, <em>218</em>,
157–165. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless communication technologies and Internet of Things (IoT) applications are the main drivers of upcoming sustainable smart networks which require an effective resource management. The reduction of the transmission energy consumption and the efficient utilization of the available spectrum for wireless communication, for instance, have to be enabled by energy-efficient and cognitive IoT networks. These are implemented through optimized communication protocol stacks and algorithms that rely on actual physical layer and channel state information. The modeling and the prototype evaluation of protocol optimization approaches are mainly driven by pure simulation studies with abstracted physical layer and channel models. With the Radio-in-the-Loop (RIL) simulation (S. Böhm and H. König, 2023) and modeling (S. Böhm and H. König, 2021), we have created an evaluation approach that integrates real wireless hardware and radio environments into the simulation of protocol sequences and algorithms. In this paper, we summarize the fundamentals of our basic methodology and demonstrate a cross-layer optimization case study for energy efficient modeling using software-defined radios. We exemplary show the scenario of a receiver-sensitivity control to increase the energy efficiency of receiver-dominated IoT nodes in Smart City networks.},
  archive      = {J_COMCOM},
  author       = {Sebastian Böhm and Hartmut König},
  doi          = {10.1016/j.comcom.2024.02.006},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {157-165},
  shortjournal = {Comput. Commun.},
  title        = {Radio-in-the-loop simulation and emulation modeling for energy-efficient and cognitive internet of things in smart cities: A cross-layer optimization case study},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EAPRAD: A MAC protocol for enhancing access probability and
reducing access delay in VANETs. <em>COMCOM</em>, <em>218</em>, 148–156.
(<a href="https://doi.org/10.1016/j.comcom.2024.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Ad Hoc Networks (VANETs) present challenges in terms of access collision and merge collision, leading to increased access delay in end-to-end vehicle communication. How to address these issues remains an unsolved problem. In this paper, we propose EAPRAD (Enhancing Access Probability and Reducing Access Delay), a novel MAC (Media Access Control) protocol based on a Markov model. Our protocol defines the number of vehicle accessing time slot failures, with different failure states forming a partial Markov chain. The highest failure status vehicle is prioritized to access the time slot, effectively resolving the problem of indefinite time slot reservations during the back-off phase. Furthermore, after acquiring the time slot, vehicles may still experience package collision, particularly when vehicles traveling in opposite directions share the same frequency band. To address this, our protocol introduces a new MAC frame structure that partitions available frequency band resources based on distinct driving directions, eliminating merge collision in opposite directions. For vehicles traveling in the same direction, intermediate nodes are utilized to proactively anticipate and prevent package collision. To model the state transition of access collision probability and merge, we develop a Continuous Time Markov Chain (CTMC) analytical model. Simulation results demonstrate that EAPRAD reduces end-to-end delay and beacon lost rate (BLR), thereby improving overall network performance in VANETs.},
  archive      = {J_COMCOM},
  author       = {Xiaolin Wu and Demin Li and Peng Wang and Qinghua Tang and Xuemin Chen},
  doi          = {10.1016/j.comcom.2024.02.012},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {148-156},
  shortjournal = {Comput. Commun.},
  title        = {EAPRAD: A MAC protocol for enhancing access probability and reducing access delay in VANETs},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attack-resistant target localization in underwater based
on consensus fusion. <em>COMCOM</em>, <em>218</em>, 131–147. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target localization has been regarded as one of the most important techniques of underwater sensor networks (USNs). However, the challenges posed by weak communication, inhomogeneous medium, and openness of underwater environment make localization more difficult. With consideration of the asynchronous clock, isogradient sound speed and forging attack, this paper presents a consensus fusion-based target localization solution with USNs. The localization procedure is mainly divided into two phases, i.e., internal position estimation and external information fusion. In the first phase, the received signal strength (RSS) and the time of flight (TOF) measurements are jointly employed to develop an asynchronous localization protocol. Particularly, a ray compensation strategy is incorporated to remove the localization bias from assuming the straight-line transmission. Based on this, a consensus fusion based localization estimator is designed in the second phase to mitigate data falsification attacks introduced by malicious sensor nodes. It is worth mentioning that, the proposed localization solution in this paper can fuse the RSS and TOF measurements by the consensus interaction procedure. Furthermore, it is resistant to data falsification attacks in inhomogeneous water medium. Finally, simulation and experimental results reveal that our solution can reduce the influences of data forging attacks and improve the localization accuracy as compared with the other works.},
  archive      = {J_COMCOM},
  author       = {Chenlu Gao and Jing Yan and Xian Yang and Xiaoyuan Luo and Xinping Guan},
  doi          = {10.1016/j.comcom.2024.02.011},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {131-147},
  shortjournal = {Comput. Commun.},
  title        = {An attack-resistant target localization in underwater based on consensus fusion},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of greedy and CBF for ETSI non-area
GeoNetworking: The impact of DCC. <em>COMCOM</em>, <em>218</em>,
114–130. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper evaluates the performance of the two ETSI non-area forwarding algorithms in the GeoNetworking specification: Greedy Forwarding and Non-Area Contention-Based Forwarding (CBF). Non-area forwarding occurs when a packet is sent to a geographical Destination Area from a node located outside of this area, e.g., when a vehicle wants to alert of hazardous events to other vehicles located in a distant geographical area. The evaluation has been carried out both in urban and highway scenarios and takes into account the complete ETSI Architecture, including the interaction with the Decentralized Congestion Control (DCC) mechanism. We have also compared ETSI-defined mechanisms with optimizations found in the literature. Our main findings are that Greedy Forwarding, when combined with DCC, is extremely ineffective even with optimizations, and Non-Area CBFs (both ETSI CBF and an optimized version called S-FoT+) outperform Greedy Forwarding both in highway and urban scenarios.},
  archive      = {J_COMCOM},
  author       = {Oscar Amador and Maria Calderon and Manuel Urueña and Ignacio Soto},
  doi          = {10.1016/j.comcom.2024.02.009},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {114-130},
  shortjournal = {Comput. Commun.},
  title        = {Evaluation of greedy and CBF for ETSI non-area GeoNetworking: The impact of DCC},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Untargeted white-box adversarial attack with heuristic
defence methods in real-time deep learning based network intrusion
detection system. <em>COMCOM</em>, <em>218</em>, 97–113. (<a
href="https://doi.org/10.1016/j.comcom.2023.09.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Intrusion Detection System (NIDS) is a key component in securing the computer network from various cyber security threats and network attacks. However, consider an unfortunate situation where the NIDS is itself attacked and vulnerable; more specifically, we can ask, “How to defend the defender?“. In Adversarial Machine Learning (AML), the malicious actors aim to fool the Machine Learning (ML) and Deep Learning (DL) models to produce incorrect predictions with intentionally crafted adversarial examples. These adversarial perturbed examples have become the biggest vulnerability of ML and DL based systems and are major obstacles to their adoption in real-time and mission-critical applications such as NIDS. AML is an emerging research domain, and it has become a necessity for the in-depth study of adversarial attacks and their defence strategies to safeguard the computer network from various cyber security threads. In this research work, we aim to cover important aspects related to NIDS, adversarial attacks and its defence mechanism to increase the robustness of the ML and DL based NIDS. We implemented four powerful adversarial attack techniques, namely, Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini &amp; Wagner (C&amp;W) in NIDS. We analyzed its performance in terms of various performance metrics in detail. Furthermore, the three heuristics defence strategies, i.e., Adversarial Training (AT), Gaussian Data Augmentation (GDA) and High Confidence (HC), are implemented to improve the NIDS robustness under adversarial attack situations. The complete workflow is demonstrated in real-time network with data packet flow. This research work provides the overall background for the researchers interested in AML and its implementation from a computer network security point of view.},
  archive      = {J_COMCOM},
  author       = {Khushnaseeb Roshan and Aasim Zafar and Shiekh Burhan Ul Haque},
  doi          = {10.1016/j.comcom.2023.09.030},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {97-113},
  shortjournal = {Comput. Commun.},
  title        = {Untargeted white-box adversarial attack with heuristic defence methods in real-time deep learning based network intrusion detection system},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coverage performance of cooperative NOMA MmWave networks
with wireless power transfer. <em>COMCOM</em>, <em>218</em>, 85–96. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The millimeter-wave (mmWave) plays a vital role in the fifth generation (5G) and beyond-5G (B5G) cellular networks, which is also attractive for wireless power transfer (WPT) since the mmWave base stations (BSs) are usually equipped with large antenna arrays. This paper studies a wireless-powered mmWave communication system with non-orthogonal multiple access (NOMA) cooperative transmission. In this system, the BS intends to transmit signals to a near-wireless device (WD) and a far-WD simultaneously, where the location of near-WD is fixed that can be treated as a relay for cooperatively transferring the signal to a far-WD, which is far away from the BS. Since there are multiple far-WDs in the region, two pairing schemes based on the distance between the near and far WDs are investigated, i.e., 1) near-WD pairs with the nearest far-WD (NNF); 2) near-WD pairs with a randomly selected far-WD (NRF). We evaluate the downlink transmission performance by assuming that the near-WD is energy-constrained and performs cooperative transmission after scavenging sufficient radio frequency (RF) energy while successfully decoding both the signals for near-WD and far-WD. Since the near-WD may accumulate sufficient energy in several transmission blocks, the processes of charging and discharging are analyzed by a finite-state Markov chain. On this basis, the performance of two WD pairing schemes is analyzed by deriving the exact expressions of coverage probabilities for both the near-WD and far-WD. The simulation results show that the considered mmWave cooperative communication system with NOMA schemes can achieve a better coverage performance than the compared OMA and other schemes, while the coverage probability of far-WD with the NNF pairing scheme is better than that with the NRF pairing scheme.},
  archive      = {J_COMCOM},
  author       = {Kun Tang and Guitao Xu and Pengwei Yan and Haoshen Zhu and Wenjie Feng},
  doi          = {10.1016/j.comcom.2024.01.008},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {85-96},
  shortjournal = {Comput. Commun.},
  title        = {Coverage performance of cooperative NOMA MmWave networks with wireless power transfer},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyber–physical system architecture of autonomous robot
ecosystem for industrial asset monitoring. <em>COMCOM</em>,
<em>218</em>, 72–84. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by advancements in Industry 4.0, the Internet of Things (IoT), digital twins (DT), and cyber–physical systems (CPS), there is a growing interest in the digitalizing of asset integrity management. CPS, in particular, is a pivotal technology for the development of intelligent and interconnected systems. The design of a scalable, low-latency communication network with efficient data management is crucial for connecting physical and digital twins in heterogeneous robot fleets. This paper introduces a generalized cyber–physical architecture aimed at governing an autonomous multi-robot ecosystem via a scalable communication network. The objective is to ensure accurate and near-real-time perception of the remote environment by digital twins during robot missions. Our approach integrates techniques such as downsampling, compression, and dynamic bandwidth management to facilitate effective communication and cooperative inspection missions. This allow for efficient bi-directional data exchange between digital and physical twins, thereby enhancing the overall performance of the system. This study contributes to the ongoing research on the deployment of cyber–physical systems for heterogeneous multi-robot fleets in remote inspection missions. The feasibility of the approach has been demonstrated through simulations in a representative environment. In these experiments, a fleet of robots is used to map an unknown building and generate a common 3D probabilistic voxel-grid map, while evaluating and managing bandwidth requirements. This study represents a step forward towards the practical implementation of continuous remote inspection with multi-robot systems through cyber–physical infrastructure. It offers potential improvements in scalability, interoperability, and performance for industrial asset monitoring.},
  archive      = {J_COMCOM},
  author       = {Hasan Kivrak and Muhammed Zahid Karakusak and Simon Watson and Barry Lennox},
  doi          = {10.1016/j.comcom.2024.02.013},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {72-84},
  shortjournal = {Comput. Commun.},
  title        = {Cyber–physical system architecture of autonomous robot ecosystem for industrial asset monitoring},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based remote operation and maintenance network
slicing resource transaction method for intelligent manufacturing
products. <em>COMCOM</em>, <em>218</em>, 59–71. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the Industrial Internet of Things (IIoT), there will be more and more intelligent manufacturing products requiring remote monitoring, maintenance, management, and optimization. IIoT and intelligent manufacturing require precise control of the remote maintenance process of products. The traditional remote operation and maintenance network based on ordinary Internet technologies can no longer meet this requirement. It is necessary to use 5G network slicing technology and multi-operator collaboration to provide network resources to meet the differentiated network service requirements of intelligent manufacturing product operation and maintenance networks. However, the provision of network-slicing resources for the operation and maintenance of intelligent manufacturing products with the participation of multiple operators is often accompanied by problems such as distrust and deceptive attacks. As a new type of technology, blockchain has the characteristics of decentralization, immutability, traceability, security, etc., which can be well-suited for this scenario. In response to the above problems, this paper proposes a blockchain-based remote operation and maintenance network slicing resource transaction method for intelligent manufacturing products to build a safe and reliable resource transaction environment for intelligent manufacturing product operation and maintenance network. First, we designed a resource provider selection algorithm based on credit value and then introduced a PBFT consensus algorithm based on credit mechanism and promotion/demotion mechanism. Finally, the simulation results show that the proposed method can realize secure and trustworthy network-slicing resource transactions, and effectively reduce the communication traffic between consensus nodes.},
  archive      = {J_COMCOM},
  author       = {Jin Chen and Ziyang Guo and Liang Tan},
  doi          = {10.1016/j.comcom.2024.01.034},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {59-71},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-based remote operation and maintenance network slicing resource transaction method for intelligent manufacturing products},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AlLoRa: Empowering environmental intelligence through an
advanced LoRa-based IoT solution. <em>COMCOM</em>, <em>218</em>, 44–58.
(<a href="https://doi.org/10.1016/j.comcom.2024.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental intelligence aims to improve the decision-making process for high social and environmental value ecosystems. To this end, data are collected using different sensors to allow monitoring of different variables of interest. Typically, these ecosystems cover a large geographical area, with spots of low or no connectivity, preventing their monitoring in real time. In this work, we propose AlLoRa (Advanced Layer LoRa), a modular, low-power, long-range communication protocol based on LoRa, that allows monitoring of remote natural areas. AlLoRa has been evaluated and tested in an operational oceanographic buoy that has been deployed to address the specific environmental crisis of the Mar Menor lagoon in southeastern Spain - a region spanning 135 Km 2 2 currently undergoing severe eutrophication process. Our results reveal that AlLoRa offers good performance regarding transfer time, power consumption, and range. The throughput ranged from around 2 kbps with SF7 to approximately 300 bps with SF11; the power consumption per kilobyte transmitted varied from 395 μ Wh 395μWh to 428 μ Wh 428μWh depending on the specific device used. The Mesh mode test successfully maintained communication between nodes over 20.33 km. Further tests in various configurations under challenging conditions validated the mesh forwarding approach. Despite tripling the distance, the system maintained reliable data transfer, improving speeds from the original point-to-point setup.},
  archive      = {J_COMCOM},
  author       = {Benjamín Arratia and Erika Rosas and Carlos T. Calafate and Juan-Carlos Cano and José M. Cecilia and Pietro Manzoni},
  doi          = {10.1016/j.comcom.2024.02.014},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {44-58},
  shortjournal = {Comput. Commun.},
  title        = {AlLoRa: Empowering environmental intelligence through an advanced LoRa-based IoT solution},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PUF-based dynamic secret-key strategy with hierarchical
blockchain for UAV swarm authentication. <em>COMCOM</em>, <em>218</em>,
31–43. (<a href="https://doi.org/10.1016/j.comcom.2024.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolving UAV swarm flight environment exacerbates the difficulty of secure message delivery within the swarm. Physical Unclonable Functions (PUFs) have the potential to provide lightweight physical identities to UAV nodes. Nevertheless, the explicit storage of a private key within each UAV device invariably results in privacy leakage arising from physical hijacking. Therefore, to accomplish unified management and decentralized storage of identity informations for UAVs, as well as to simultaneously realize the dynamic updating of private key, we propose a P UF-based D ynamic S ecret-key strategy with H ierarchical B lockchain ( PDSHB ) for UAV swarm authentication. Specifically, the master blockchain ensures that all UAVs’ registration information is tamper-proof, while the dynamic sub-blockchain synchronizes the UAV information within the swarm. Meanwhile, we have developed a more reliable and efficient PUF-based dynamic secret-key strategy that eliminates the need to store any secret information within UAVs, effectively mitigating the risk of key leakage resulting from physical attacks. Protocol security is demonstrated through formal security proofs, informal security analysis, and AVISPA security checking. Performance experiments based on Raspberry Pi 4B and FPGA-based F450 UAVs also show that the proposed protocol can deal with various security threats of dynamic swarms of UAVs and reduce computational and communication overheads during authentication.},
  archive      = {J_COMCOM},
  author       = {Liquan Chen and Yaqing Zhu and Suhui Liu and Hongtao Yu and Bing Zhang},
  doi          = {10.1016/j.comcom.2024.02.001},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {31-43},
  shortjournal = {Comput. Commun.},
  title        = {PUF-based dynamic secret-key strategy with hierarchical blockchain for UAV swarm authentication},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive latency profiling study of the tofino p4
programmable ASIC-based hardware. <em>COMCOM</em>, <em>218</em>, 14–30.
(<a href="https://doi.org/10.1016/j.comcom.2024.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network softwarization has significantly evolved since programmable data planes became topical in academia and industry. Programming Protocol-Independent Packet Processors (P4) is a language to define packet forwarding behavior. Forwarding devices that are programmed with the P4 language support a flexible way to define headers, parse graphs, and data plane logic. However, extending the data plane with additional functionalities has an impact on packet data plane latency. For this reason, this paper analyzes the key factors that affect data pane latency to packets processed by the Tofino-based target (Tofino Native Architecture (TNA)), which can be considered the de facto production-ready and P4-programmable Application-Specific Integrated Circuit (ASIC). Our work first provides an extensive set of latency measurements and, afterwards, it includes a set of data plane latency predictions using the model derived from the latency results and machine learning (ML) algorithms. We demonstrate that the PCA-lasso polynomial (PLP) obtains the best results among the algorithms tested. The best-case results show that PLP obtained an accuracy of 98.22% prediction accuracy when considering the parser, deparser, and the control block for traffic running at 10 G/s (SFP+) and 100 G/s (QSFP28). To the best of our knowledge, this is the first work that provides such a comprehensive profiling, including a method to predict data plane latency in production-grade Tofino ASIC-based switching hardware, which could be leveraged to yield accurate latency values prior to investment and deployment.},
  archive      = {J_COMCOM},
  author       = {David Franco and Eder Ollora Zaballa and Mingyuan Zang and Asier Atutxa and Jorge Sasiain and Aleksander Pruski and Elisa Rojas and Marivi Higuero and Eduardo Jacob},
  doi          = {10.1016/j.comcom.2024.01.010},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {14-30},
  shortjournal = {Comput. Commun.},
  title        = {A comprehensive latency profiling study of the tofino p4 programmable ASIC-based hardware},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient failure-resilient mutual exclusion algorithm
for distributed systems leveraging a novel zero-message overlay
structure. <em>COMCOM</em>, <em>218</em>, 1–13. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide a novel failure-resilient token-based mutual exclusion (ME) algorithm for distributed systems. Like a few other solutions in the literature, the proposed solution leverages a logical tree as its underlying topology. However, unlike any other solution, the tree is built using a probabilistic, fully distributed approach, without exchanging messages. The current tree-based ME algorithms often overlook considerations for node/link failures or offer costly methods for failure recovery. The proposed algorithm overcomes these limitations by providing an effective solution to maintain a logarithmic cost in case of node failures. The overlay structure – a unique logical tree – used to control access to the critical section maintains its consistency even when nodes fail. An extensive simulation study demonstrates the viability and efficiency of the proposed algorithm under various node failure models, and relevant metrics (e.g., node queue dimension, number of exchanged messages, and number of disconnected nodes) indicate a graceful degradation in performance with decreasing number of functioning nodes. For instance, for 4,096 nodes organized in a logical tree of arity 4 and with 4 physical nodes for logical node, a negligible number of nodes is disconnected from the tree after 250 epochs when the per-node per-epoch failure probability is p f ≤ 0 . 0008 pf≤0.0008 . With a p f = 0 . 0016 pf=0.0016 , less than 10% of the nodes are disconnected. The proposed algorithm avoids node disconnection while minimally impacting the load of the logical tree nodes. In addition, for the same architecture, when both tree arity and cardinality are 4, after 250 epochs, the node load has demonstrated minimal variation and remains in close proximity to the original load. Moreover, experimental results also reveal a graceful degradation of algorithm performance. The fully distributed solution, rich parametrization for different trade-offs, and viability and resilience of the proposed algorithm also pave the way for future research.},
  archive      = {J_COMCOM},
  author       = {Mouna Rabhi and Roberto Di Pietro},
  doi          = {10.1016/j.comcom.2024.02.007},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {1-13},
  shortjournal = {Comput. Commun.},
  title        = {An efficient failure-resilient mutual exclusion algorithm for distributed systems leveraging a novel zero-message overlay structure},
  volume       = {218},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VeriBypasser: An automatic image verification code
recognition system based on CNN. <em>COMCOM</em>, <em>217</em>, 246–258.
(<a href="https://doi.org/10.1016/j.comcom.2023.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of network technology has brought great convenience to people’s daily life in recent years. The application of network technology in finance, economy, and education is flourishing, dramatically improving people’s quality of life and promoting society’s development. However, the development of network technology brought some new technical problems. Authenticating and controlling access to shared resources has become a significant problem, which limits the development of smart cities. Nowadays, most websites use accounts and passwords for authentication. In order to prevent violent scanning, image-based human–machine inspection solutions are widely used on major platforms. Nonetheless, some human–computer verification schemes can be bypassed or attacked by artificial intelligence. This paper proposes an improved generation method of image verification code and an automatic recognition system based on Cellular Neural Network (CNN), which can achieve an accuracy of 85.20% for a mix of the visual reasoning Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA) containing 4-bit case-insensitive letters and digits, the slider CAPTCHA containing one slide, and inference puzzle CAPTCHA containing a 4 × 2 grid. A brief evaluation of system threat assignment with such verification approaches based on four machine learning methods is also proposed.},
  archive      = {J_COMCOM},
  author       = {Weihang Ding and Yuxin Luo and Yifeng Lin and Yuer Yang and Siwei Lian},
  doi          = {10.1016/j.comcom.2023.12.022},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {246-258},
  shortjournal = {Comput. Commun.},
  title        = {VeriBypasser: An automatic image verification code recognition system based on CNN},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection applied to QoS/QoE modeling on video and
web-based mobile data services: An ordinal approach. <em>COMCOM</em>,
<em>217</em>, 230–245. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, mobile service providers perceive the user experience as a reliable indicator of the quality associated to a service. Given a set of Quality of Service (QoS) factors, the aim is to predict the Quality of Experience (QoE), measured in terms of the Mean Opinion Score (MOS). Although this problem is receiving much attention, there are still some challenges that require more research in order to find effective solutions for meeting user’s expectation in terms of service quality. A core challenge in this topic refers to the analysis of the contribution of each factor to the QoS/QoE Model. In this work, we study the mapping between QoS and QoE on video and web-based services using a machine learning approach. For such purpose, we design a lab-testing methodology to emulate different cellular transmission network scenarios. Then, we address the problem of inducing a predictive model and identifying relevant QoS factors. Results suggest that bandwidth is a key factor when analyzing user’s perception of service quality.},
  archive      = {J_COMCOM},
  author       = {Miguel García-Torres and Diego P. Pinto-Roa and Carlos Núñez-Castillo and Brenda Quiñonez and Gabriela Vázquez and Mauricio Allegretti and María E. García-Diaz},
  doi          = {10.1016/j.comcom.2024.02.004},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {230-245},
  shortjournal = {Comput. Commun.},
  title        = {Feature selection applied to QoS/QoE modeling on video and web-based mobile data services: An ordinal approach},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). When game theory meets satellite communication networks: A
survey. <em>COMCOM</em>, <em>217</em>, 208–229. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite communication networks have been considered an integral part of B5G and 6G networks to achieve global coverage and enhanced Internet services. However, the integration of satellite and terrestrial networks also brings many challenges, including the explosion of management complexity, the limited resource in satellite nodes, and the strategic behavior among network participants. To solve these challenges, game theory has emerged as a potential solution for rapidly evolving satellite communication networks. While there are some surveys discussing game theory in various networking scenarios, there is a lack of surveys targeting game theory-based solutions in satellite communication networks. To fill in this research gap, the objective and research motivation of this study are to summarize and present a comprehensive and up-to-date literature review of recent studies applying game theory to various applications in satellite networks. Both cooperative and non-cooperative games are covered, with a total number of fourteen different game models. Based on the review of existing studies, research challenges and opportunities are further proposed to inspire future research directions. To the best of our knowledge, this paper is the first comprehensive survey focusing on the application of game theory to satellite communication networks.},
  archive      = {J_COMCOM},
  author       = {Weiwei Jiang and Haoyu Han and Miao He and Weixi Gu},
  doi          = {10.1016/j.comcom.2024.02.005},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {208-229},
  shortjournal = {Comput. Commun.},
  title        = {When game theory meets satellite communication networks: A survey},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traffic matrix estimation using matrix-CUR decomposition.
<em>COMCOM</em>, <em>217</em>, 200–207. (<a
href="https://doi.org/10.1016/j.comcom.2024.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic Matrices (TMs) are the primary input for a multitude of network operations and management activities within a backbone network. TMs can be collected directly using monitoring tools but with an additional communication and processing overhead. Thus, TM estimation techniques gained prominence, where the objective is to obtain an estimate of the TM using easily available information without any additional overhead. Principal Component Analysis (PCA) and its variants have been extensively employed for TM estimation. This paper demonstrates the sensitivity of PCA towards the rank parameter for TM estimation. To overcome this limitation, this paper proposes a CUR decomposition-based technique for traffic matrix estimation. Experimental results on real world traffic matrices collected from Abilene backbone network show that the CUR-based estimation technique exhibits negligible sensitivity to the rank parameter for estimating traffic matrices, and also, attains low estimation error in comparison to the existing schemes.},
  archive      = {J_COMCOM},
  author       = {Awnish Kumar and Ngangbam Herojit Singh and Suyel Namasudra and Ruben Gonzalez Crespo and Nageswara Rao Moparthi},
  doi          = {10.1016/j.comcom.2024.02.002},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {200-207},
  shortjournal = {Comput. Commun.},
  title        = {Traffic matrix estimation using matrix-CUR decomposition},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A truthful double auction framework for security-driven and
deadline-aware task offloading in fog-cloud environment.
<em>COMCOM</em>, <em>217</em>, 183–199. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging as an alternative to cloud computing , fog computing is expected to fulfill the resource demand of mobile users and provide low-latency and high-bandwidth services for ever-increasing Internet of Things (IoT) applications. Offloading time-critical and computation-intensive tasks to the fog can significantly reduce response time . Due to dynamicity, heterogeneity, and limited resources, fog may fail to provide services with guaranteed performance. This paper addresses the problem of task offloading and resource allocation in a fog-cloud environment for IoT applications. The proposed Truthful Double Auction Task Offloading (TDATO) framework provides incentives for fog providers to maximize their revenues, execute as many as possible requests with guaranteed performance, and maintain a high reputation. Depending on the user requirements in terms of security level and maximum latency tolerance for task execution, we propose two deadline- and security-related algorithms for winners&#39; determination in the auction: Deadline-Minimum Security (DMS) and Deadline-Preferred Security (DPS). Moreover, two assignment algorithms are analyzed, Reputation-Based (RB) and Random Assignment (RA). The proposed double auction framework assures that all winning users&#39; requirements in terms of deadlines and security level are satisfied while winning providers’ resources are allocated to the users that value them the most. Furthermore, the assignment mechanism aims to maximize resource utilization for fog providers with the highest reputation. We conducted a theoretical analysis to demonstrate that TDATO satisfies truthfulness, individual rationality, and budget balance and has polynomial-time computation complexity. Extensive simulation experiments are performed to evaluate performance.},
  archive      = {J_COMCOM},
  author       = {Branka Mikavica and Aleksandra Kostic-Ljubisavljevic},
  doi          = {10.1016/j.comcom.2024.01.033},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {183-199},
  shortjournal = {Comput. Commun.},
  title        = {A truthful double auction framework for security-driven and deadline-aware task offloading in fog-cloud environment},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility aware and energy-efficient federated deep
reinforcement learning assisted resource allocation for 5G-RAN slicing.
<em>COMCOM</em>, <em>217</em>, 166–182. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is one of the foundations for the realization of 5G and beyond. However, due to the mobility of the users and the network dynamics, flexible and efficient radio access network (RAN) resource slicing is still a challenge. In this paper, we allocate the power and radio block (RB) resources in a multi-RAN scenario to the users of both rate-based and resource-based slices. We propose a mobility-aware and energy-efficient federated deep reinforcement learning-assisted resource allocation (ME-FDRL-RA) method for RAN slicing in a large multiple RAN environment. ME-FDRL-RA includes both federated deep reinforcement learning (FDRL) and deep learning (DL) models as follows: Stacked and bidirectional long-short-term-memory (SBiLSTM), allocate resources to slices on a large time-scale. Additionally, federated advantage actor-critic (F-A2C) allocates resources on a small time-scale and speeds up convergence. Moreover, to solve the optimization problem of determining the required resources for the users of slices, we propose an efficient iterative algorithm called the interference-aware and energy-efficient power allocation (IA-EPA) method. According to simulation results, ME-FDRL-RA outperforms competitive methods in terms of convergence speed , computational complexity , energy efficiency, and the number of accepted users while addressing the challenges of user mobility and maintaining a desirable degree of inter-slice isolation.},
  archive      = {J_COMCOM},
  author       = {Yaser Azimi and Saleh Yousefi and Hashem Kalbkhani and Thomas Kunz},
  doi          = {10.1016/j.comcom.2024.01.028},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {166-182},
  shortjournal = {Comput. Commun.},
  title        = {Mobility aware and energy-efficient federated deep reinforcement learning assisted resource allocation for 5G-RAN slicing},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Activate or sleep? An optimal two-threshold policy for
transmission service. <em>COMCOM</em>, <em>217</em>, 152–165. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In wireless communication networks (WCNs), activating the removable base station (BS) is an effective way to reduce the bursty congestion, while it will inevitably incur some extra costs for occupying more resources. In order to achieve a tradeoff between the costs and quality of service (QoS), the manager need to decide when to activate and when to sleep the removable BS. In this paper, a system with a fixed BS and a removable BS is considered. Benefitting from the explicit expression of stationary distribution, a set of optimal thresholds of activating and sleeping the removable BS is obtained. Interestingly, for any combination of the energy consumption cost, the activation cost and the storage cost , the optimal sleep control is that the removable BS, once activated, keeps working until there is no data packets. This interesting phenomenon is justified by numerical experiments, and the minimum total costs under optimal thresholds is discussed. Meanwhile, comparing with the system investing in two fixed BSs, simulation results show that it always has advantage to activate the removable BS.},
  archive      = {J_COMCOM},
  author       = {Jiaqi Fan and Jiankui Yang and Dacheng Yao},
  doi          = {10.1016/j.comcom.2024.01.015},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {152-165},
  shortjournal = {Comput. Commun.},
  title        = {Activate or sleep? an optimal two-threshold policy for transmission service},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Throughput maximization in multi-slice cooperative
NOMA-based system with underlay D2D communications. <em>COMCOM</em>,
<em>217</em>, 134–151. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifth generation (5G) and beyond-5G networks aim to meet the rapidly growing traffic demands while considering the scarcity of radio resources and the heterogeneity of services and technical requirements . Non-Orthogonal Multiple Access (NOMA) has been considered a key technology to address resource scarcity by enabling more users to share the resources. Furthermore, network slicing tackles the requirements’ heterogeneity by partitioning the physical network into multiple logical slices. In this study, the aforementioned key technologies are adopted to maximize the overall throughput and satisfy the technical requirements of a multi-slice system. We formulate an optimization problem in a multi-slice cooperative NOMA-based system with underlay D2D communications that jointly addresses user grouping, radio resource blocks allocation, and D2D admission. Its objective is to maximize the overall system throughput while considering each slice’s constraints. Given the complexity of the optimization problem, we propose a three-step, low-complexity solution: a matching theory-based approach for the user grouping and the D2D admission sub-problems, and a heuristic approach for the resource blocks allocation sub-problem. Numerical results demonstrate the: (1) low complexity of the proposed solution; (2) high impact of interference cancellation imperfection; (3) superior performance of the proposed solution compared to literature baselines. Specifically, under dense network, our solution achieves up to 34% enhancement in overall system throughput, up to 35% improvement in D2D admission, and up to 125% and 32%, respectively, in cellular users and D2D pairs satisfaction. It also outperforms under strict eMBB and URLLC requirements. Also, results show significant overestimation in Shannon’s evaluation of URLLC throughputs considered in some papers from the literature compared to the finite block length evaluation considered in our work, particularly at higher URLLC reliability requirements.},
  archive      = {J_COMCOM},
  author       = {Asmaa Amer and Sahar Hoteit and Jalel Ben Othman},
  doi          = {10.1016/j.comcom.2024.01.030},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {134-151},
  shortjournal = {Comput. Commun.},
  title        = {Throughput maximization in multi-slice cooperative NOMA-based system with underlay D2D communications},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing secrecy energy efficiency in RIS-assisted MISO
systems using deep reinforcement learning. <em>COMCOM</em>,
<em>217</em>, 126–133. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the maximization of secrecy energy efficiency (SEE) in B5G mobile systems where a suite of reconfigurable intelligent surface (RIS) modules is incorporated. Taking into account the location information of legitimate users and eavesdroppers, we formulate the problem as a joint optimization of the phase shifts, physical orientations, and locations of the RIS modules, as well as resource allocation at the base station (BS). The problem is then solved by leveraging a deep reinforcement learning (DRL) approach proposed in this paper. The case study results demonstrate the effectiveness of the proposed scheme in improving the secrecy energy efficiency of communication systems using RIS.},
  archive      = {J_COMCOM},
  author       = {Mian Muaz Razaq and Huanhuan Song and Limei Peng and Pin-Han Ho},
  doi          = {10.1016/j.comcom.2024.01.020},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {126-133},
  shortjournal = {Comput. Commun.},
  title        = {Optimizing secrecy energy efficiency in RIS-assisted MISO systems using deep reinforcement learning},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASSOCIATE: A simulator for assessing soft security in the
cognitive internet of things. <em>COMCOM</em>, <em>217</em>, 107–125.
(<a href="https://doi.org/10.1016/j.comcom.2024.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By augmenting the Internet of Things (IoT) with Artificial Intelligence (AI), the Cognitive Internet of Things (CIoT) has emerged, where intelligent objects autonomously sense, perceive, make decisions, and collaborate with minimal initial knowledge. Ensuring trust and security in such a cognitive society poses significant challenges. Soft security approaches, which leverage behavioral norms, have arisen as a promising solution to address these challenges. In this paper, we introduce ASSOCIATE , a modular and multi-layered tool designed for simulating and evaluating soft security models in the CIoT domain. ASSOCIATE applies a trust model to CIoT simulations and assesses the society’s soft security. The evaluation process encompasses society preparation, generation, trust model preparation, execution, and report generation. Input configuration files in the JSON format define simulation parameters, allowing the simulator to generate the requested society and conduct simulations. Evaluation results are presented through charts and graphs and saved in image and CSV formats. We demonstrate the tool’s functionality by enhancing the Santander dataset using the society generation module and analyzing various trust profiles. The proposed tool provides a valuable means of assessing the efficacy of soft security models in CIoT simulations, facilitating a comprehensive analysis of trust and security in CIoT systems.},
  archive      = {J_COMCOM},
  author       = {Masoud Narimani Zaman Abadi and Amir Jalaly Bidgoly and Yaghoub Farjami},
  doi          = {10.1016/j.comcom.2024.01.023},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {107-125},
  shortjournal = {Comput. Commun.},
  title        = {ASSOCIATE: A simulator for assessing soft security in the cognitive internet of things},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation of deep learning remote sensing images
based on band combination principle: Application in urban planning and
land use. <em>COMCOM</em>, <em>217</em>, 97–106. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the relevance of semantic segmentation of remote sensing images in urban planning and land use. We introduce a novel deep learning model that leverages the principle of band combination in remote sensing images to enhance the efficiency and accuracy of semantic segmentation. Our research focuses not only on advancing the segmentation capabilities of remote sensing images but also on applying this technology in urban planning and land use to foster sustainable development in smart cities. By integrating the band combination principle into the convolution operation , our approach improves feature extraction, thereby enhancing the quality of semantic segmentation in remote sensing images. This method outperforms traditional remote sensing image analysis techniques by combining automatic feature learning and the generalization capabilities of deep learning , thereby improving the segmentation model’s performance. A unique aspect of this study is the direct application of remote sensing image segmentation in urban planning and land use. Our model accurately identifies various land uses such as residential, commercial, and industrial areas, and tracks land-use change trends, aiding urban planners in future development planning. Compared to conventional methods, our model significantly reduces training time and increases computational efficiency under identical training conditions. Experimental comparisons and analyses reveal that, within the same training duration, our model’s accuracy surpasses that of similar models by 10%–15%. On the ISPRS dataset, our model achieved a segmentation accuracy of 82.43% for building surfaces, and 76.54% for trees. In scenarios with relatively uniform reflective surfaces, our model outperforms similar models by approximately 10%.},
  archive      = {J_COMCOM},
  author       = {Peiyan Jia and Chen Chen and Delong Zhang and Yulong Sang and Lei Zhang},
  doi          = {10.1016/j.comcom.2024.01.032},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {97-106},
  shortjournal = {Comput. Commun.},
  title        = {Semantic segmentation of deep learning remote sensing images based on band combination principle: Application in urban planning and land use},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Available energy routing algorithm considering QoS
requirements for LEO satellite network. <em>COMCOM</em>, <em>217</em>,
87–96. (<a href="https://doi.org/10.1016/j.comcom.2024.01.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite networks are becoming an integral part of future network infrastructure. However, premature depletion of energy in some satellites caused by the limited capacity of batteries and the uneven distribution of network traffic poses a significant challenge. Therefore, we have a keen interest in designing an available energy routing algorithm for LEO satellite networks to balance energy usage. Firstly, a satellite available energy model has been established, which considers the output power of satellite solar panels and the energy consumption of satellite under the influence of Earth’s obstruction. Then, our algorithm considered the different routing transmission requirements of network services and appropriately selected different decision factors to generate paths, which can reduce network routing energy consumption while meeting the delay requirements of services. Numerical result demonstrate that our algorithm can reduce energy consumption by about 12.53% and reduce packet loss rate by 23.72% compared with QoS-SR, which effectively improving the end-to-end delay of services and extending the life of satellite networks.},
  archive      = {J_COMCOM},
  author       = {Li Yang and Huitao Zhang and Yaowen Qi and Qilong Huang},
  doi          = {10.1016/j.comcom.2024.01.025},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {87-96},
  shortjournal = {Comput. Commun.},
  title        = {Available energy routing algorithm considering QoS requirements for LEO satellite network},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ms-van3t: An integrated multi-stack framework for virtual
validation of V2X communication and services. <em>COMCOM</em>,
<em>217</em>, 70–86. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automotive field is evolving towards high levels of automation, requiring seamless data exchange between vehicles through Vehicle-to-Everything (V2X) communications. Direct V2X technology is already being deployed on commercial vehicles, and it has the potential to deliver a range of safety and efficiency benefits on the road. However, the deployment of V2X-based applications is a complex process that demands extensive testing before these systems can be widely used by the public; indeed, high costs and safety concerns are among the main hurdles to overcome before applications leveraging V2X communication can become a reality. It is thus critical to reliably validate through simulation and emulation both the V2X technologies and the applications in realistic scenarios, before performing large-scale road tests. To address this pressing need, we present an open source framework for the virtual validation of V2X-based applications, amenable to the development and testing not only of different access technologies within the same environment (IEEE 802.11p, LTE-V2X, 5G NR-V2X, and LTE), but also of any kind of V2X-based application using ETSI-compliant messages. Our framework, called ms-van3t, is based on the ns-3 and SUMO (Simulation of Urban MObility) simulators, it implements a full ETSI C-ITS stack for CAM, DENM and IVIM messages, and it provides several novel features not found elsewhere. Further, ms-van3t enables the testing of V2X-based applications in HIL (Hardware-In-the-Loop) scenarios, thanks to a dedicated emulation mode , and it allows users to easily select different physical and MAC layer models, seamlessly collecting performance statistics. To showcase the capabilities of the framework, we present three sample applications as well as the performance results we obtained in terms of both application-related and network-related key performance indicators .},
  archive      = {J_COMCOM},
  author       = {F. Raviglione and C.M. Risma Carletti and M. Malinverno and C. Casetti and C.F. Chiasserini},
  doi          = {10.1016/j.comcom.2024.01.022},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {70-86},
  shortjournal = {Comput. Commun.},
  title        = {Ms-van3t: An integrated multi-stack framework for virtual validation of V2X communication and services},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secure opportunistic routing in 2-hop IEEE 802.15.4 networks
with SMOR. <em>COMCOM</em>, <em>217</em>, 57–69. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IEEE 802.15.4 radio standard features the possibility for IEEE 802.15.4 nodes to run on batteries for several years. This is made possible by duty-cycling medium access control (MAC) protocols, which allow IEEE 802.15.4 nodes to leave their radios in energy-saving sleep modes most of the time. Yet, duty-cycling MAC protocols usually incur long routing delays since it may take a while until a particular forwarder becomes available for forwarding a packet. Opportunistic routing alleviates this problem by opportunistically using a currently available forwarder, rather than waiting for a particular forwarder. Among all opportunistic routing schemes , so-called dynamic switch-based forwarding (DSF) schemes are most promising from a security and practical perspective, but some security and reliability issues with them persist. In this paper, we propose secure multipath opportunistic routing (SMOR), a DSF scheme that improves on current DSF schemes in three regards. First, SMOR builds on a denial-of-sleep-resilient MAC layer. Current DSF schemes, by comparison, rest on MAC protocols that put the limited energy reserves of battery-powered IEEE 802.15.4 nodes at risk. Second, SMOR operates in a distributed fashion and efficiently supports point-to-point traffic. All current DSF schemes, by contrast, suffer from a single point of failure and focus on convergecast traffic. Third, SMOR duplicates packets on purpose and routes them along disjoint paths. This makes SMOR tolerant of compromises of single IEEE 802.15.4 nodes, whereas current DSF schemes lack intrusion tolerance. We integrated SMOR into the network stack of the Contiki-NG operating system and benchmarked SMOR against the Routing Protocol for Low-Power and Lossy Networks (RPL) with the Cooja network simulator . Indeed, SMOR turns out to improve on RPL’s delays by between 33.51% and 39.84%, depending on the exact configurations and network dynamics. Furthermore, SMOR achieves between 0.16% and 2.03% higher mean packet delivery ratios (PDRs), thereby attaining mean PDRs of 99.999% in all simulated scenarios. Beyond that, SMOR has only a fraction of RPL’s memory requirements. SMOR’s intrusion tolerance, on the other hand, increases the mean energy consumption per IEEE 802.15.4 node by between 1.55% and 2.74% compared to RPL in our simulations. SMOR specifically targets IEEE 802.15.4 networks with a network diameter of 2, such as body area networks .},
  archive      = {J_COMCOM},
  author       = {Konrad-Felix Krentz and Thiemo Voigt},
  doi          = {10.1016/j.comcom.2024.01.024},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {57-69},
  shortjournal = {Comput. Commun.},
  title        = {Secure opportunistic routing in 2-hop IEEE 802.15.4 networks with SMOR},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LAMP: A latency-aware MAC protocol for joint scheduling of
CAM and DENM traffic over 5G-NR sidelink. <em>COMCOM</em>, <em>217</em>,
41–56. (<a href="https://doi.org/10.1016/j.comcom.2024.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative Awareness Messages (CAMs) and Decentralized Environmental Notification Messages (DENMs) are two types of messages used in Intelligent Transport Systems (ITS) for Vehicle-to-Everything (V2X) communication. CAMs are used to periodically exchange the current state of a vehicle with its nearby vehicles and the DENMs are used to aperiodically provide critical and time-sensitive information about current environmental conditions to other vehicles and infrastructure entities. While considerable research has been conducted on the efficient scheduling of CAM and DENM separately or at the network level coexistence, no work has been done to integrate the scheduling of both message types at the granularity of the per-vehicle level. The scheduling of CAM is commonly done through Semi-Persistent Scheduling (SPS). However, no equivalent scheduling algorithm has been developed for the aperiodic and variable-sized DENMs. As a result, there is a need for novel mechanisms that can efficiently prioritize and schedule mixed traffic of CAM and DENM messages. This paper introduces a Quality of Service (QoS) scheduler called LAMP for segregating traffic in the Radio Link Control (RLC) layer for joint sidelink scheduling of CAM and DENM at the vehicular level. The LAMP scheduler is aided with a special resource selection and reservation scheme for mixed traffic scenarios. The groundwork involves an experimental analysis in Network Simulator-3 (NS-3) that uses the New Radio (NR) Vehicle-to-Everything (V2X) module in Mode-2. The simulation findings demonstrate that LAMP could significantly reduce the end-to-end latency of DENM by 89.36% and CAM by 40.2% while also increasing the packet reception rate by 12.1% and 9.74% for CAM and DENM with repetitions, respectively.},
  archive      = {J_COMCOM},
  author       = {Suranjan Daw and Anwesha Kar and Venkatarami Reddy Chintapalli and Bheemarjuna Reddy Tamma and Siva Ram Murthy C.},
  doi          = {10.1016/j.comcom.2024.01.011},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {41-56},
  shortjournal = {Comput. Commun.},
  title        = {LAMP: A latency-aware MAC protocol for joint scheduling of CAM and DENM traffic over 5G-NR sidelink},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The anatomy of conspiracy theorists: Unveiling traits using
a comprehensive twitter dataset. <em>COMCOM</em>, <em>217</em>, 25–40.
(<a href="https://doi.org/10.1016/j.comcom.2024.01.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discourse around conspiracy theories is currently thriving amidst the rampant misinformation in online environments. Research in this field has been focused on detecting conspiracy theories on social media, often relying on limited datasets. In this study, we present a novel methodology for constructing a Twitter dataset that encompasses accounts engaged in conspiracy-related activities throughout the year 2022. Our approach centers on data collection that is independent of specific conspiracy theories and information operations. Additionally, our dataset includes a control group comprising randomly selected users who can be fairly compared to the individuals involved in conspiracy activities. This comprehensive collection effort yielded a total of 15K accounts and 37M tweets extracted from their timelines. We conduct a comparative analysis of the two groups across three dimensions: topics, profiles, and behavioral characteristics. The results indicate that conspiracy and control users exhibit similarity in terms of their profile metadata characteristics. However, they diverge significantly in terms of behavior and activity, particularly regarding the discussed topics, the terminology used, and their stance on trending subjects. In addition, we find no significant disparity in the presence of bot users between the two groups. Finally, we develop a classifier to identify conspiracy users using features borrowed from bot, troll and linguistic literature. The results demonstrate a high accuracy level (with an F1 score of 0.94), enabling us to uncover the most discriminating features associated with conspiracy-related accounts.},
  archive      = {J_COMCOM},
  author       = {Margherita Gambini and Serena Tardelli and Maurizio Tesconi},
  doi          = {10.1016/j.comcom.2024.01.027},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {25-40},
  shortjournal = {Comput. Commun.},
  title        = {The anatomy of conspiracy theorists: Unveiling traits using a comprehensive twitter dataset},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the design space of privacy-enhanced content
discovery for bitswap. <em>COMCOM</em>, <em>217</em>, 12–24. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IPFS is a content-addressed peer-to-peer data network, which follows the paradigm of information centric networking. In IPFS, data is exchanged with the Bitswap protocol. For content discovery, Bitswap queries all neighbors for the content, leaking the interest to all neighbors. In our paper, we develop three privacy-enhanced protocols for content discovery, which reduce the interest leak from all neighbors to ideally one content provider. Our protocols use probabilistic data structures like Bloom filter and cryptographic approaches like Private Set Intersection. We implement our protocols as proof of concept and show how they can be integrated into the go implementation of Bitswap. Furthermore, we provide a measurement supported performance, load, and privacy evaluation of the three protocols, showing their feasibility trade-offs.},
  archive      = {J_COMCOM},
  author       = {Erik Daniel and Florian Tschorsch},
  doi          = {10.1016/j.comcom.2024.01.029},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {12-24},
  shortjournal = {Comput. Commun.},
  title        = {Exploring the design space of privacy-enhanced content discovery for bitswap},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust and machine learning-driven identification scheme
for malicious nodes in UASNs. <em>COMCOM</em>, <em>217</em>, 1–11. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Acoustic Sensor Networks (UASNs) have garnered widespread adoption across maritime domains and national security sectors . Their deployment, often in remote and challenging conditions , necessitates robust trust models to assess node integrity and to pinpoint compromised units, particularly under adversarial assault. Prevailing trust models, however, predominantly focus on nodes’ communicative actions and energy consumption, neglecting the influence of ambient aquatic noise on trust assessments. Moreover, these models typically falter amidst the UASN-specific impediments of high packet collision rates and elevated error probabilities inherent to underwater signal transmission, leading to consequential inaccuracies in node evaluation. To address these limitations, this paper proposes a robust and machine learning-driven malicious node identification scheme (RMIS) for UASNs. The proposed scheme first models and quantifies the impact of packet collisions and the underwater environment on node communication, utilizing existing environment models and MAC protocols. Second, communication traffic is exploited as effective and reliable evidence of trust. The traffic evidence can reflect multiple types of attacks by analyzing changes in traffic in a UASN. Third, the evaluation models are trained using support vector machine and K-means++ algorithms. Finally, two types of trust update mechanisms are proposed to handle the dynamic underwater environment and on–off attack. The simulation results indicate that compared to the other three identification schemes, RMIS is more effective in identifying malicious nodes. The effectiveness of the RMIS becomes particularly noticeable as the ratio of malicious nodes increases. Furthermore, the trust update mechanisms effectively limit the impact of the on–off attack and improve the robustness of the RMIS.},
  archive      = {J_COMCOM},
  author       = {Xiangdang Huang and Chao Chen and Nuo Chen and Pengcheng Li and Rongxin Zhu and Qiuling Yang},
  doi          = {10.1016/j.comcom.2024.01.009},
  journal      = {Computer Communications},
  month        = {3},
  pages        = {1-11},
  shortjournal = {Comput. Commun.},
  title        = {A robust and machine learning-driven identification scheme for malicious nodes in UASNs},
  volume       = {217},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient resource allocation and user association in
NOMA-enabled vehicular-aided HetNets with high altitude platforms.
<em>COMCOM</em>, <em>216</em>, 374–386. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for massive connectivity and high data rates has made the efficient use of existing spectrum resources an increasingly challenging problem. Non-orthogonal multiple access (NOMA) is a potential solution for future heterogeneous networks (HetNets) due to its high capacity and spectrum efficiency. In this study, we analyze an uplink NOMA-enabled vehicular-aided HetNet , where multiple vehicular user equipment (VUEs) share the access link spectrum, and a high-altitude platform (HAP) communicates with roadside units (RSUs) through a backhaul communication link. We propose an improved algorithm for user association that selects VUEs for HAPs based on channel coefficient ratios and terrestrial VUEs based on a caching-state backhaul communication link. The joint optimization problems aim to maximize a utility function that considers VUE transmission rates and cross-tier interference while meeting the constraints of backhaul transmission rates and QoS requirements of each VUE. The joint resource allocation optimization problem consists of three sub-problems: bandwidth allocation , user association, and transmission power allocation . We derive a closed-form solution for bandwidth allocation and solve the transmission power allocation sub-problem iteratively using Taylor expansion to transform a non-convex term into a convex one. Our proposed three-stage iterative algorithm for resource allocation integrates all three sub-problems and is shown to be effective through simulation results. Specifically, the results demonstrate that our solution achieves performance improvements over existing approaches.},
  archive      = {J_COMCOM},
  author       = {Ali Nauman and Mashael Maashi and Hend K. Alkahtani and Fahd N. Al-Wesabi and Nojood O. Aljehane and Mohammed Assiri and Sara Saadeldeen Ibrahim and Wali Ullah Khan},
  doi          = {10.1016/j.comcom.2024.01.021},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {374-386},
  shortjournal = {Comput. Commun.},
  title        = {Efficient resource allocation and user association in NOMA-enabled vehicular-aided HetNets with high altitude platforms},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient neuro-fuzzy-based multi-node charging model
for WRSNs using multiple mobile charging vehicles. <em>COMCOM</em>,
<em>216</em>, 356–373. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile charging vehicles ( M C V s MCVs ) have played a significant role in advancing Wireless Rechargeable Sensor Networks (WRSNs). Recent research has primarily focused on on-demand charging in WRSNs using M C V s MCVs . However, minimal attention has been given to the simultaneous analysis of multiple M C V s MCVs with multi-node energy transfer capabilities. Additionally, when making scheduling decisions, most existing methods still need to consider various network parameters, while a few have overlooked the challenge of sensor nodes’ ( S N s SNs ) poor charging responses to non-uniform energy expenditure rates. This article addresses the issues above and proposes an innovative charging-scheduling Algorithm based on Neuro-fuzzy with PSO model (ANFIS-PSO) to resolve these challenges. To ensure uniform workload allocation among M C V s MCVs and prevent interference during the charging process, we deploy M C V s MCVs strategically using the Optimal Fuzzy C-Means clustering method . We then optimize M C V MCV visiting points to recharge energy-critical S N s SNs , implying reduced energy consumption and charging delay. We next adopt the ANFIS-PSO model, which blends various network parameters for creating the charging schedules of the M C V s MCVs . We devise an expression to find the S N s SNs ’ adaptive charging threshold values based on their remaining lifetime. We also formulate another expression called the average charging weight function to assist the ANFIS-PSO model in selecting the next promising S N SN for recharging. Experimental simulations validates that our method is effective and outperforms baseline methods , with a substantial 11% reduction in charging delay, a 6% improvement in the life-survival ratio, and a 7% increase in energy utilization efficiency across all performance metrics.},
  archive      = {J_COMCOM},
  author       = {Hyder Ali Hingoliwala and Naween Kumar and Anand Nayyar and Gandharba Swain},
  doi          = {10.1016/j.comcom.2023.12.028},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {356-373},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient neuro-fuzzy-based multi-node charging model for WRSNs using multiple mobile charging vehicles},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Node clustering scheme supporting mobility in NOMA-enabled
backscatter communication networks. <em>COMCOM</em>, <em>216</em>,
346–355. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the problem of mobile node clustering in non-orthogonal multiple access (NOMA) backscatter communication networks. The nodes are clustered based on the differences between their channel gains to accomplish NOMA transmission within the clusters. However, as the channel gains change constantly with node mobility, the nodes need to be re-clustered, which leads to a significant computational overhead. We propose a node clustering scheme (IGAC). This scheme formulates the node clustering problem as a combinatorial optimization problem to find the optimal node clustering that maximizes system throughput. To handle the large search space of multi-node clustering, we employ a genetic algorithm to solve this optimization problem effectively. Moreover, to overcome the NOMA principle violation problem(NPVP) caused by mobile nodes, we propose a node clustering adjustment scheme (SMSO) based on stable matching ideas and swapping operations. According to the theory of stable matching, optimizing the long-term throughput of the system can be achieved by swapping nodes between different clusters to eliminate blocking pairs in clustering. The simulation results show that our proposed node clustering scheme outperforms traditional schemes in terms of system throughput.},
  archive      = {J_COMCOM},
  author       = {Tingpei Huang and Shiyu Guo and Tiantian Zhang and Bairen Zhang and Jianhang Liu},
  doi          = {10.1016/j.comcom.2024.01.003},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {346-355},
  shortjournal = {Comput. Commun.},
  title        = {Node clustering scheme supporting mobility in NOMA-enabled backscatter communication networks},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ESIM and blockchain integrated secure zero-touch
provisioning for autonomous cellular-IoTs in 5G networks.
<em>COMCOM</em>, <em>216</em>, 324–345. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of the Internet of Things (IoT) paradigm has resulted in a proliferation of connected devices and their applications. Autonomous IoT (AIoT) refers to a network of interconnected devices that operate without human intervention, making decisions and performing tasks autonomously. Traditional methods of provisioning IoT devices , such as manual configuration and over-the-air updates, are error-prone and insecure. The emergence of eSIMs (embedded SIMs) provides a viable solution for secure and flexible identity management in IoT devices. This work implements a low-cost, zero-touch remote provisioning system using GSMA standard Over-The-Air (OTA) IoT-SAFE protocol. This research predicts that future IoT devices will be eSIM-enabled, which are simple to configure, provision, validate profiles, and check security policies remotely. IoT onboarding processes are designed where blockchains are used to verify immutable repositories to store this network manifests, verifiable by Ethereum smart contracts . The integrated framework combines blockchain contracts, eSIM-based remote SIM provisioning through IoT-SAFE protocol, and SDN to manage IoT ecosystems&#39; security. The proposed solution is evaluated using simulations and security analysis, and it demonstrates its feasibility at scale and resilience to attacks even under insecure environments. When compared with the baseline IEEE 802.15.4 protocol, our SDN-based Remote-SIM provisioning system (SIeSIM) reduces overhead to about 240 ms Time-To-Provision (TTP), outperforming manual provisioning by nearly 320 % and 210 % compared to expert provisioning in terms of TTP performances, respectively.},
  archive      = {J_COMCOM},
  author       = {Prabhakar Krishnan and Kurunandan Jain and Shivananda R. Poojara and Satish Narayana Srirama and Tulika Pandey and Rajkumar Buyya},
  doi          = {10.1016/j.comcom.2023.12.023},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {324-345},
  shortjournal = {Comput. Commun.},
  title        = {ESIM and blockchain integrated secure zero-touch provisioning for autonomous cellular-IoTs in 5G networks},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal ECDSA: A timestamp and signature mask enabled ECDSA
algorithm for IoT client node authentication. <em>COMCOM</em>,
<em>216</em>, 307–323. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) networks are vulnerable to various security threats, especially in client authentication . The current authentication methods require significant resources and are vulnerable to specific attacks. The Non-Interactive Zero Knowledge Proof (NIZKP) concept suits IoT device authentication. Elliptic Curve Digital Signature (ECDSA) is a popular algorithm for IoT device authentication based on elliptic curve cryptography (ECC) and NIZKP. However, an intelligent attacker who captures the ECDSA signatures generated by the same random number can recover the private key. This paper proposes a timestamp based approach to prevent signature reuse and fake signature generation in the ECDSA algorithm . The signature proof is generated differently at each occurrence to prevent the generation of valid signatures from leaked private keys. The proposed approach eliminates the expensive modular inversion operations in signature generation and verification phases , enhancing execution efficiency. The analysis results indicate that the proposed Temporal ECDSA algorithm effectively prevents most attacks on client authentication in IoT networks. Various metrics, including computational complexity and security measures, were used to evaluate the effectiveness of the proposed approach, demonstrating that it outperforms most ECDSA variants.},
  archive      = {J_COMCOM},
  author       = {Jiby J. Puthiyidam and Shelbi Joseph and Bharat Bhushan},
  doi          = {10.1016/j.comcom.2024.01.016},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {307-323},
  shortjournal = {Comput. Commun.},
  title        = {Temporal ECDSA: A timestamp and signature mask enabled ECDSA algorithm for IoT client node authentication},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transient analysis of energy-saving strategy for cognitive
radio networks using g-queue with heterogeneity. <em>COMCOM</em>,
<em>216</em>, 295–306. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy efficiency, high data speeds, and effective spectrum utilization stand as the primary factors governing 5G and beyond networks. A base station (BS) in a cognitive radio network (CRN) that conserves energy during sleep periods is a promising contender for achieving more effective spectrum allocation. An efficient energy-saving strategy is offered in this study to achieve greener communication in wireless cellular networks . This study aims to propose and analyze an energy-saving scheme for the BS, taking into account the heterogeneity and reliability of networks. The entire system is modeled as a three-dimensional Markov chain by establishing a discrete-time preemptive priority queueing model with single vacation, heterogeneous packets, and negative packets such as viruses. Then, the transient analysis of the Markov chain is performed using the recursive method. Utilizing the transient probability distribution, we present numerical results derived from reliability analysis and queueing analysis to substantiate the validity of the proposed model. Furthermore, results based on the energy-saving degree are showcased to affirm the effectiveness of the proposed scheme. Finally, a reward cost function is obtained depending on the successful transmission of secondary packets, and the Quasi-Newton method is used to determine the optimal reward cost.},
  archive      = {J_COMCOM},
  author       = {Rakhee Kulshrestha and Ajay Singh},
  doi          = {10.1016/j.comcom.2024.01.007},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {295-306},
  shortjournal = {Comput. Commun.},
  title        = {Transient analysis of energy-saving strategy for cognitive radio networks using G-queue with heterogeneity},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPARQ: SYN protection using acyclic redundancy check and
quartile range on p4 switches. <em>COMCOM</em>, <em>216</em>, 283–294.
(<a href="https://doi.org/10.1016/j.comcom.2023.12.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN), enabled by high-performance programmable switches, offers a new avenue to counter cyber attacks . Programmable switches offer the ability to customize and conduct in-depth packet analysis , thus providing efficient and timely responses to DDoS attacks . However, implementing sophisticated DDoS detection may be a challenge in programmable switches because the p4 language does not support floating-point arithmetic, logarithmic functions , or loops. Furthermore, the limited SRAM and TCAM memory on programmable switches makes storing the network connection state difficult. Hence, effective deployment of DDoS detection techniques remains challenging due to these limitations and the rising complexity of the attacks. Many researchers proposed the DDoS detection solution directly on a programmable switch, ignoring the pressing need for a distributed solution. Therefore, this paper presents an innovative, decentralized traffic analysis framework called SPARQ that optimally utilizes the data and control planes. SPARQ is based on Rényi entropy that filters TCP SYN DDoS attacks . It leverages the programming ability of data planes for traffic classification and utilizes the control plane to calculate the metrics and acyclic redundancy checks within the traffic. Moreover, SPARQ uses quartile ranges to track packet inter-arrival time so that abnormal traffic patterns can be identified. We implement SPARQ in a B M v 2 BMv2 switch using the p 4 r u n t i m e p4runtime controller, enabling seamless integration with SDN systems. We compare the performance of SPARQ with state-of-the-art solutions using the CAIDA dataset. The comparative analysis demonstrates that SPARQ provides a 20.59% reduction in CPU load, an average detection time shorter than 88%, and a 17.8% improvement in true positive rate (TPR).},
  archive      = {J_COMCOM},
  author       = {Vaishali A. Shirsath and Madhav M. Chandane and Chhagan Lal and Mauro Conti},
  doi          = {10.1016/j.comcom.2023.12.027},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {283-294},
  shortjournal = {Comput. Commun.},
  title        = {SPARQ: SYN protection using acyclic redundancy check and quartile range on p4 switches},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated sensing and communication-assisted beam
rendezvous in airborne networks. <em>COMCOM</em>, <em>216</em>, 274–282.
(<a href="https://doi.org/10.1016/j.comcom.2024.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the feasibility of the integrated sensing and communication (ISAC) technology in the future airborne network (AN), a large heterogeneous network of various aircraft to realize situation awareness and sharing. In order to achieve better transmissions with few overheads, an effective beam rendezvous scheme is fundamental. However, due to the nodes’ oblivious movements in AN, the conventional beam training methods may lead to significant overheads and latencies. Motivated by this, we develop a novel fast beam prediction scheme for communication in AN with sensing assistance to improve the sum-rates while satisfying the sensing requirements simultaneously. This paper introduces our vision of the ISAC-enabled AN and provides the signal models in the considered AN based on ISAC technology. Then, we propose a 3D beam prediction scheme that helps both sides to formulate the aligned beams utilizing the angular prediction results, without the heavy beam training processes to reduce the overheads. Numerical results demonstrate that the proposed scheme can achieve high-data-rate transmission while guaranteeing the sensing performance with lower complexity and link overheads.},
  archive      = {J_COMCOM},
  author       = {Xinyu Hong and Na Lv and Ying Pan},
  doi          = {10.1016/j.comcom.2024.01.019},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {274-282},
  shortjournal = {Comput. Commun.},
  title        = {Integrated sensing and communication-assisted beam rendezvous in airborne networks},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Securing IIoT applications in 6G and beyond using adaptive
ensemble learning and zero-touch multi-resource provisioning.
<em>COMCOM</em>, <em>216</em>, 260–273. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Industry 5.0 , driven by cutting-edge 6G technologies such as immersive cloud eXtended Reality (XR), Autonomous vehicles, holographic communication, and Digital Twin (DT), is set to trigger a substantial upswing in the deployment of the Industrial Internet of Things (IIoT) devices interconnected within networks. This expansion of IIoT devices will create a wider attack surface and increase the risk of data breaches, privacy violations, and system disruptions. Therefore, it is essential to design innovative mechanisms to ensure the reliability and security of these advanced 6G applications, as well as the IIoT devices that support them. In this context, we propose a novel Software-defined Networking (SDN)-based Ensemble Learning (EL) Framework for Secure IIoT applications in 6G and beyond, called AdaptSDN. The proposed framework leverages SDN technology to dynamically allocate network resources and deploy security measures on demand. It also leverages EL techniques to improve the intrusion detection system’s accuracy. By isolating IIoT devices into network slices, the framework limits the impact of attacks and reduces the potential for cascading failures . Digital twins are used for creating a virtual replica of the IIoT network, allowing for a real-time security threat detection. In particular, AdaptSDN includes three main modules: (1) A novel digital Twin (DT)-enabled data gathering and selection of informative features module to achieve two main objectives: reducing computational complexity and improving detection performance; (2) An SDN-based lightweight adaptive boosting module that uses an advanced boosting EL techniques to dynamically adjust weights and to effectively identify and respond to IIoT attacks in real-time; and (3) A zero-touch resources provisioning module that employs a non-cooperative game theory approach. This approach allows for automatically provisioning various resources in a network to efficiently mitigate network attacks; it enables SDN nodes to obtain the required virtual resources, i . e . , i.e., storage, computing, and bandwidth, from the main orchestrator with respect to IIoT attack type. We carried out comprehensive experiments to assess the effectiveness of our proposed framework in detecting real-world IIoT attacks. The numerical results confirm that AdaptSDN has the potential to enable secure and reliable IIoT applications in 6G and beyond, meeting the stringent service requirements of the new emerging applications.},
  archive      = {J_COMCOM},
  author       = {Zakaria Abou El Houda and Bouziane Brik and Adlen Ksentini},
  doi          = {10.1016/j.comcom.2024.01.018},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {260-273},
  shortjournal = {Comput. Commun.},
  title        = {Securing IIoT applications in 6G and beyond using adaptive ensemble learning and zero-touch multi-resource provisioning},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modulating LiFi for dual operation in the visible and
infrared spectra. <em>COMCOM</em>, <em>216</em>, 251–259. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light-Fidelity (LiFi) has emerged in the last few years as a promising technology for alleviating the stringent demand for wireless data services . Prior works have considered LiFi operating either in the visible light or infrared spectrum. Each spectrum band has its own advantages: visible light allows leveraging existing infrastructure for communication, while infrared is not affected by light dimming. In this work, we propose a modulation scheme that retains the benefits of both bands, introducing a simple, low-cost, yet efficient dimming solution for LiFi networks. We compare the performance of the proposed dimming scheme with both the digital and analog dimming techniques traditionally used in LiFi systems. Simulation results show that our dimming solution offers better communication and illumination performance than previous proposals, providing larger signal-to-noise ratio, spectral efficiency , and a full and fine-grained dimming range. Finally, we prototype our solution by designing an extended version of the OpenVLC 1.3 platform, and we experimentally show its robust communication performance under different dimming conditions. We make the implemented system publicly available to the research community.},
  archive      = {J_COMCOM},
  author       = {Dayrene Frometa Fonseca and Muhammad Sarmad Mir and Sergio Iglesias de Frutos and Borja Genoves Guzman and Domenico Giustiniano},
  doi          = {10.1016/j.comcom.2024.01.005},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {251-259},
  shortjournal = {Comput. Commun.},
  title        = {Modulating LiFi for dual operation in the visible and infrared spectra},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Secrecy outage performance of NOMA relay networks using
partial relay selection in the presence of multiple colluding
eavesdroppers. <em>COMCOM</em>, <em>216</em>, 238–250. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, security becomes one of the utmost important issues in designing mobile communication networks, especially for non-orthogonal multiple access (NOMA), in which the users share the same bandwidth and time resource. Here, we rigorously derive the closed-form formulas of secrecy outage probability (SOP) for each user as well as the overall SOP of a NOMA relay networks consisting of two users, against the wiretap from multiple colluding eavesdroppers. All channels are assumed to be Rayleigh fading . To enhance the reliability of the legitimate communication, we exploit a partial relay selection (PRS) algorithm to select the best relay for forwarding instead of using the traditional multiple-relay (MR) forwarding scheme. For practical consideration, the successive interference cancellation (SIC) at the NOMA users is assumed to be imperfect, which is different from most of the previous works. The SOP obtained in this work is also benchmarked with the one that is obtained by using MR instead of PRS and OMA instead of NOMA, respectively. The analytical results are verified by Monte Carlo simulations , which also confirm that the PRS method outperforms the conventional MR approach.},
  archive      = {J_COMCOM},
  author       = {Tran Manh Hoang and Ba Cao Nguyen and Huu Minh Nguyen and Phuong T. Tran},
  doi          = {10.1016/j.comcom.2024.01.012},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {238-250},
  shortjournal = {Comput. Commun.},
  title        = {Secrecy outage performance of NOMA relay networks using partial relay selection in the presence of multiple colluding eavesdroppers},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervision and early warning of abnormal data in internet
of things based on unsupervised attention learning. <em>COMCOM</em>,
<em>216</em>, 229–237. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of artificial intelligence and big data , more and more communication protocols have appeared in the network connection. Because the network protocol content specification should not need to obtain the content of the common protocol by any means, it faces different attacks, which increases the complexity of network security detection to an unlimited extent. The absolute security analysis of the content of the relevant data protocol is not too limited. Based on the content of the unknown binary number protocol in the Internet of Things , this paper does the reverse research. To address the inefficiency, learning is focused on the inverse aspect of the content processing logic of the protocol for unsupervised execution attention. This paper proposes a method based on the name of the Dirichlet process mixture model normal field and the unknown protocol of the Internet of Things . It also establishes the behavioral criteria that process the logical inference least squares method to infer the content of the target protocol. Based on the original external characteristics of abnormal data in the Internet of Things, the neural network model of attention mechanism and the model checking of variational self-encoder are chosen. An innovative multi-dimensional spatial neural network model with a centralized control mechanism is taken to extract the spatial relevant information from the original external features, which are regarded as the intermediate external features. The intermediate extrinsic features are input to a variational self-encoder for computation and reconstruction. The experimental results verify that the time required by the proposed model is 45% less than that of the traditional structure. The flow processing improves by 36.7% compared with the optimization of the parameter control function . The specific method only relies on the original external characteristics of the Internet of Things traffic and can achieve better data anomaly detection and classification under the condition of not significantly improving the detection time. The abnormal data detection scheme of the Internet of Things designed in this paper not only fills the detection gap of the current mainstream Internet of Things system which lacks an autonomous supervision mechanism, but also effectively blocks external network intrusion and further improves the management authority verification system . It plays an indispensable role in the long-term stable operation of the Internet of Things platform.},
  archive      = {J_COMCOM},
  author       = {Lili Wu and Majid Khan Majahar Ali and Ying Tian},
  doi          = {10.1016/j.comcom.2023.12.043},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {229-237},
  shortjournal = {Comput. Commun.},
  title        = {Supervision and early warning of abnormal data in internet of things based on unsupervised attention learning},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustered jamming in u-V2X communications with 3D antenna
beam-width fluctuations. <em>COMCOM</em>, <em>216</em>, 209–228. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Jamming is the disruption of wireless communication signals, rendering them unreadable or degraded, typically caused by jamming devices. Clustered jamming in vehicular communications is a highly sophisticated attack technique characterized by the coordinated deployment of multiple jammers with the strategic intent to deliberately disrupt and incapacitate communication links, thereby presenting a formidable challenge to the reliability and security of vehicular networks . It is critical to address the intricate challenges imposed by clustered jamming in the context of vehicle-to-everything (V2X) communications augmented by unmanned aerial vehicles (UAVs), given their critical role in facilitating essential vehicle-to-vehicle and vehicle-to-UAV communication links. In this paper, we examine UAV-V2X (U-V2X) communications within the context of clustered jamming and the variations in the 3D beam-width of antennas. Also, mitigation techniques for clustered jamming in V2X communications are addressed. We consider the distribution of UAVs is governed by the Poisson point process , the distribution of vehicular-nodes (VNs) is governed by the Poisson line process , and the distribution of jammers is governed by the Matern Cluster Process. The analytical equations for the association probability and the success probability (SP) of a typical VN with the UAV and near-by VN, along with the presence of jammers and the variations of antenna beam-width are provided. Furthermore, we investigate the SP and spectral efficiency performance of the U-V2X network under different network configurations of UAVs, VNs, roads, jamming clusters, and transmit power of jamming devices. Our findings reveal that clustered jammers influence U-V2X network’s performance, further compounded by UAV beam-width fluctuations. Moreover, in low-density areas, UAVs demonstrate superior reliability for connectivity compared to VNs, despite clustered jamming. Also, lower UAV antenna fluctuations enhance UAVs’ dependability in U-V2X networks amidst clustered jamming. Specifically, for a given setup with τ = τ= -5 dB, U-V2X link’s SP with jamming drops to one-quarter due to 2-degree beam-width fluctuations. Consequently, it is imperative to design and implement anti-jamming strategies, particularly as the number of jamming clusters and variations of antenna beam-width increase.},
  archive      = {J_COMCOM},
  author       = {Mohammad Arif and Wooseong Kim},
  doi          = {10.1016/j.comcom.2024.01.013},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {209-228},
  shortjournal = {Comput. Commun.},
  title        = {Clustered jamming in U-V2X communications with 3D antenna beam-width fluctuations},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive intelligent routing algorithm based on deep
reinforcement learning. <em>COMCOM</em>, <em>216</em>, 195–208. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mobile ad hoc network (MANET) is a self-organizing network composed of multiple mobile nodes. These nodes achieve distributed communication and wireless resource sharing through autonomous collaboration. However, typical routing protocols for MANETs suffer from uneven network load, low network throughput, poor anti-destruction performance, and low transmission reliability. In order to meet the routing protocol characteristics of distributed adaptability, reliability and anti-destruction, balance of load and energy consumption and guarantee of high QoS in MANETs, this paper proposes an adaptive intelligent routing algorithm based on the PER-D3QN model of deep reinforcement learning . To achieve better routing strategy selection, the routing problem is first modeled, a distributed multi-agent reinforcement learning routing framework is designed, and a deep reinforcement learning reward function considering multiple factors is used. Next, the implementation process of the adaptive intelligent routing algorithm using the PER-D3QN model is described in detail. Finally, a mobile self-organizing network environment was constructed based on Networkx, and corresponding data was collected to simulate the algorithm. The simulation results show that the adaptive intelligent routing algorithm proposed in this paper can balance the energy consumption of the network and the load of nodes, its average remaining energy has increased by more than 10% and PL is reduced by at least 10% compared to other algorithms, while guaranteeing high QoS performance of routing, that is, reducing the average end-to-end delay of data packet transmission by over 22% compared to other algorithms and maintaining a high packet delivery ratio .},
  archive      = {J_COMCOM},
  author       = {Jie Bai and Jingchuan Sun and Zhigang Wang and Xunwei Zhao and Aijun Wen and Chunling Zhang and Jianguo Zhang},
  doi          = {10.1016/j.comcom.2023.12.039},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {195-208},
  shortjournal = {Comput. Commun.},
  title        = {An adaptive intelligent routing algorithm based on deep reinforcement learning},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abnormal traffic detection system in SDN based on deep
learning hybrid models. <em>COMCOM</em>, <em>216</em>, 183–194. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined network (SDN) provides technical support for network construction of smart cities. However, the openness of SDN is also prone to more network attacks. Traditional abnormal traffic detection algorithms are complex and time-consuming, so it is difficult to find abnormalities in the network in time and unable to satisfy the requirements of abnormal traffic detection in the SDN environment. Therefore, we propose an abnormal traffic detection system based on deep learning hybrid model. The system adopts a hierarchical detection method. Firstly, it completes the rough detection of abnormal traffic in the network according to the statistical information of switch ports and then uses wavelet transform and deep learning technology to extract multi-dimensional features of all traffic data flowing through suspicious switches, so as to realize the fine detection of abnormal traffic from the surface. The experimental results show that the proposed detection method based on port information can quickly locate the source of abnormal traffic. Compared with the traditional abnormal traffic detection method in SDN, the fine detection method based on multi-dimensional features improves the accuracy by 1.7 %, the recall rate by 1.6 %, and the false positive rate by 91.3 %.},
  archive      = {J_COMCOM},
  author       = {Kun Wang and Yu Fu and Xueyuan Duan and Taotao Liu and Jianqiao Xu},
  doi          = {10.1016/j.comcom.2023.12.041},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {183-194},
  shortjournal = {Comput. Commun.},
  title        = {Abnormal traffic detection system in SDN based on deep learning hybrid models},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A federated learning system with data fusion for healthcare
using multi-party computation and additive secret sharing.
<em>COMCOM</em>, <em>216</em>, 168–182. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of medical things , data from a single source can be easily analyzed. Besides, it is paramount to collect data from multiple sources to provide consistent, accurate, and vital information as compared to that collected from a single source. Data fusion enables the combination of data from multiple sensors. However, privacy concerns need to be addressed during data aggregation. Furthermore, since medical data is becoming increasingly sophisticated via multimodal means, there is a high likelihood that a complex relationship among biological processes exists. This challenge motivates the study. This study is proposing a secure federated learning system based on data fusion using multi-party computation and additive secret-sharing method. The gradient parameters of the federated training model are protected rather than the actual data. The purpose of protecting the gradient parameters is to address the tradeoff between prediction accuracy and data privacy. In the proposed model, a convolutional neural network , which has an efficient weight-sharing approach, is employed for the prediction. The weight of the model is encrypted using a multi-computation and additive secret sharing. Extensive simulations are carried out to ascertain the efficiency of the proposed model. Besides, the proposed system model is compared with existing works in the literature in terms of accuracy, precision, recall and f1-score. Simulation results show that the proposed model has 97% accuracy as compared to 93% for random forest , 91% for Naive Bayes, 92% for logistic regression and 95% for decision tree . Furthermore, F1-score of the proposed model is 89% as compared to 85% for random forest, 80% for Naive Bayes, 82% for logistic regression and 79% for decision tree. Security analysis of the proposed system model is performed, which shows that the system is robust against weighted majority vote attack, honest-but-curious party and other security related attacks.},
  archive      = {J_COMCOM},
  author       = {Tasiu Muazu and Yingchi Mao and Abdullahi Uwaisu Muhammad and Muhammad Ibrahim and Umar Muhammad Mustapha Kumshe and Omaji Samuel},
  doi          = {10.1016/j.comcom.2024.01.006},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {168-182},
  shortjournal = {Comput. Commun.},
  title        = {A federated learning system with data fusion for healthcare using multi-party computation and additive secret sharing},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory-enhanced appearance-motion consistency framework for
video anomaly detection. <em>COMCOM</em>, <em>216</em>, 159–167. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern network communication systems extensively utilize video data for various applications, creating a pressing need for efficient Video Anomaly Detection (VAD) mechanisms. The essence of VAD lies in the detection of discrepancies in the appearance-motion dynamics of normal and abnormal events. However, existing methods primarily focus on the isolated analysis of appearance or motion anomalies, thereby overlooking the crucial aspect of appearance-motion consistency semantics . To this end, we introduce a Memory-enhanced Appearance-Motion Consistency (MAMC) framework, which is focused on understanding complex appearance-motion consistency patterns in video data for the discernment of anomalies. The first stage of our model involves the design of an Appearance-Motion Fusion (AMF) module, engineered to generate a robust representation of scene dynamics that captures appearance-motion consistency. This consistency data is then processed through the memory module, augmenting the distinction between normal and anomalous events. Experimental results on three benchmark datasets validate the effectiveness of our approach, which achieves AUCs of 96.7%, 87.6%, and 71.5% on three benchmark datasets (UCSD Ped2, CUHK Avenue, and ShanghaiTech datasets). Additional experiments and visualization analysis confirm the effectiveness of the proposed MAMC framework in anomaly detection scenarios.},
  archive      = {J_COMCOM},
  author       = {Zhiyuan Ning and Zile Wang and Yang Liu and Jing Liu and Liang Song},
  doi          = {10.1016/j.comcom.2024.01.004},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {159-167},
  shortjournal = {Comput. Commun.},
  title        = {Memory-enhanced appearance-motion consistency framework for video anomaly detection},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate data mapping in refresh-free DRAM for
energy-efficient computing in modern mobile systems. <em>COMCOM</em>,
<em>216</em>, 151–158. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of modern mobile and embedded communication systems , the increasing storage density of dynamic random access memory (DRAM) poses significant challenges in energy management , with DRAM refresh emerging as a major factor in energy consumption. Meanwhile, approximate computing is becoming an emerging energy-efficient paradigm for mobile and embedded systems involving the error-resilient applications of media processing, recognition, and data mining. This paper proposes to leverage approximate computing to eliminate all the unnecessary refreshes in DRAM and makes it as energy-efficient as “non-volatile” devices. The devised solution is based on the observation that mapping the virtual page of certain reusing distances into the memory rows of the particular retention time intervals can greatly decrease or even completely eliminate refresh operations for approximate computing workloads. In the proposed Non-Volatile DRAM (NV-DRAM), special page allocation and data mapping strategies have been developed for different types of systems to control the Quality of Result (QoR) of the system and best exploit the allowable quality margin (precision loss) for refresh removing. A static data mapping policy and a control-theory inspired PID controller are proposed to dynamically map approximate work-set to NV-DRAM rows without infringing the QoR constraint. The results show that the error rate of our predictive strategies can be controlled under 1%, and our proposal saves more than 99.9% of refresh energy consumption.},
  archive      = {J_COMCOM},
  author       = {Sen Li and Hui Jin and Yingke Gao and Ying Wang and Shuhong Dai and Yongjun Xu and Long Cheng},
  doi          = {10.1016/j.comcom.2023.12.037},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {151-158},
  shortjournal = {Comput. Commun.},
  title        = {Approximate data mapping in refresh-free DRAM for energy-efficient computing in modern mobile systems},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized federated learning based on blockchain:
Concepts, framework, and challenges. <em>COMCOM</em>, <em>216</em>,
140–150. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized federated learning integrates advanced technologies, including distributed computing and secure encryption methodologies, to facilitate a robust and efficient mechanism for safeguarding data privacy and security during collaborative model training endeavors. The incorporation of blockchain technology into Federated Learning provides a transformative framework characterized by its inherent decentralization and data immutability, making it a focal point of contemporary research inquiry. The literature on the integration of blockchain technology with federated learning frameworks is presently deficient in comprehensive summary works. Such summaries are essential for advancing understanding of the implementation challenges and for guiding future research efforts in this domain. Therefore, in this work, we first summarize a typical decentralized federated learning framework based on blockchain and describe its operational workflow and its applications in the fields of the Internet of Things , the Internet of Vehicles , etc. A systematic summary of the challenges confronting this framework and an analysis of the solutions proposed to address these challenges are provided. Finally, this work provides insight into the possible future research directions.},
  archive      = {J_COMCOM},
  author       = {Haoran Zhang and Shan Jiang and Shichang Xuan},
  doi          = {10.1016/j.comcom.2023.12.042},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {140-150},
  shortjournal = {Comput. Commun.},
  title        = {Decentralized federated learning based on blockchain: Concepts, framework, and challenges},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empower smart cities with sampling-wise dynamic facial
expression recognition via frame-sequence contrastive learning.
<em>COMCOM</em>, <em>216</em>, 130–139. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the construction of smart cities, facial expression analysis plays a crucial role. It can be used in traffic monitoring systems to alleviate traffic pressure by analyzing the emotional states of drivers and passengers. In the field of smart healthcare , it can provide more precise treatment and services to patients. In the realm of social entertainment, it can offer more intelligent and personalized interactions. In summary, the application of emotion computing technology will play an increasingly significant role in the development of smart cities in the future. In the task of dynamic facial expression recognition (DFER), analyzing the spatial–temporal features of video sequences has become a common research approach. However, facial expression sequences often contain a significant number of neutral frames and noisy frames, potentially increasing computational costs and reducing performance. Effectively extracting key frames for spatial–temporal feature analysis is a critical aspect of dynamic facial expression recognition. To address this issue, we proposed a sampling-wise dynamic facial expression recognition via frame-Sequence contrastive learning method, called SW-FSCL. The SW-FSCL method aims to improve the performance of DFER by using intelligent dual-stream sampling strategies and frame-sequence contrastive learning, extract key frame and reduce the impact of neutral frames and noisy frames. We proposed a key frame proposal (KFP) block to analyze the spatial–temporal features of sequences, calculating weight ratios for key frame extraction. Due to potential information loss in long sequences, we introduce a temporal aggregation (TA) block to prevent data loss and ensure the integrity of temporal information. The experimental results provide compelling evidence that the proposed approach not only outperforms all current state-of-the-art algorithms on two widely-used benchmark datasets (DFEW, FERV39k), but also visualization results produces insights into the interpretability of the SW-FSCL method.},
  archive      = {J_COMCOM},
  author       = {Shaoqi Yan and Yan Wang and Xinji Mai and Qing Zhao and Wei Song and Jun Huang and Zeng Tao and Haoran Wang and Shuyong Gao and Wenqiang Zhang},
  doi          = {10.1016/j.comcom.2023.12.032},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {130-139},
  shortjournal = {Comput. Commun.},
  title        = {Empower smart cities with sampling-wise dynamic facial expression recognition via frame-sequence contrastive learning},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Path planning of unmanned vehicles based on adaptive
particle swarm optimization algorithm. <em>COMCOM</em>, <em>216</em>,
112–129. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning technology is the basis of autonomous driving of unmanned vehicles. However, there are some problems in the traditional path planning technology. For example, high-quality global paths can&#39;t be generated quickly; Lacking of security verification ability; The performance of dynamic obstacle avoidance is poor. Therefore, this paper proposes a path planning method of unmanned vehicles based on adaptive particle swarm optimization algorithm (APSO). Firstly, a map simplification strategy (MSS) is proposed. The grid map is preprocessed by map simplification strategy to reduce the search space and time of path planning algorithm ; Secondly, an APSO algorithm is proposed. The algorithm coordinates the search of particles through three adaptive factors and Levy flight strategy. Then, a security checking strategy is proposed. Security checking strategy can be used to verify the safety of global path; Finally, a dynamic obstacle avoidance strategy based on behavior is proposed. Vehicles can independently analyze the types of collision and adopt corresponding obstacle avoidance strategies. The simulation results show that MSS-APSO algorithm and APSO algorithm surpass original algorithms and comparison algorithms; MSS-APSO algorithm has strong applicability in real map environment; The obstacle avoidance strategy has great obstacle avoidance ability and real-time performance; The map simplification strategy can improve iterations of the algorithm and quality of the global path.},
  archive      = {J_COMCOM},
  author       = {Jiale Zhao and Chaoshuo Deng and Huanhuan Yu and Hansheng Fei and Deshun Li},
  doi          = {10.1016/j.comcom.2023.12.040},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {112-129},
  shortjournal = {Comput. Commun.},
  title        = {Path planning of unmanned vehicles based on adaptive particle swarm optimization algorithm},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing modular application placement in a hierarchical
fog computing: A latency and communication cost-sensitive approach.
<em>COMCOM</em>, <em>216</em>, 95–111. (<a
href="https://doi.org/10.1016/j.comcom.2024.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, cloud computing has been a key enabler for handling complex applications and services for Internet of Things (IoT) devices situated at the edge of the network. Services and applications that are driven by the IoT environment commonly have stringent latency-sensitive requirements and may experience long network delays due to the long physical distance of cloud-based computational resources from IoT devices. Fog computing gained adoption as a solution in this case because it shortens this distance by spreading the computing power around the edge of the network in tiers. This contributes to network latency reduction and response times improvements of applications that have sensitive temporal requirements, besides improving the overall data traffic management in the network. Nevertheless, when certain requirements of an application are prioritized over others during the resource allocation process, a fog tier closer to IoT devices may experience resource depletion , forcing other latency-sensitive applications to use resources from a distant fog level and causing them to become non-responsive. To address this issue, this work proposes an approach for allocating modular applications in a hierarchical tier-based fog computing architecture . The proposed approach, named Least Impact - X (LI-X) , aims to minimize the response time of latency-sensitive applications and reduce data traffic on the network by mitigating the idle time of resources at the lower levels of the hierarchical fog. This is achieved by distributing the application modules among the fog tiers in order to minimize the response time of delay-sensitive applications, while also reducing the overall network traffic. The performance of LI-X was compared to previous studies in a simulated iFogSim environment. Results have demonstrated that LI-X outperforms these studies in most of the proposed scenarios, effectively reducing response time and minimizing communication data costs on the network.},
  archive      = {J_COMCOM},
  author       = {Leonan T. Oliveira and Luiz F. Bittencourt and Thiago A.L. Genez and Eyal de Lara and Maycon L.M. Peixoto},
  doi          = {10.1016/j.comcom.2024.01.002},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {95-111},
  shortjournal = {Comput. Commun.},
  title        = {Enhancing modular application placement in a hierarchical fog computing: A latency and communication cost-sensitive approach},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint optimization of UPF placement and traffic routing for
5G core network user plane. <em>COMCOM</em>, <em>216</em>, 86–94. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the dynamic deployment of the 5G core network user plane functions (UPFs) in the edge network under time-varying traffic conditions. The objective is to minimize the total cost, including energy consumption, user plane latency , and UPF deployment costs by jointly optimizing UPF placement and traffic routing over the entire planning horizon. We formulate the optimization problem as a mixed-integer linear programming (MILP) problem, which is proven to be NP-hard. To obtain the optimal solution for the problem, we propose a Benders decomposition-based algorithm. However, the high computational complexity of this algorithm limits its practical applicability for large-scale networks. Therefore, we propose a low-complexity algorithm based on nested Benders decomposition that can solve the problem suboptimally. The simulation results demonstrate the convergence and computational efficiency of the proposed algorithm and show that it outperforms two existing algorithms. Moreover, we demonstrate that cost-saving can be achieved by using more power-efficient servers or deploying more servers. Additionally, we show that adjusting the cost parameters can achieve a good tradeoff between energy consumption and user plane latency.},
  archive      = {J_COMCOM},
  author       = {Songyan Chen and Junjie Chen and Hongjun Li},
  doi          = {10.1016/j.comcom.2023.12.029},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {86-94},
  shortjournal = {Comput. Commun.},
  title        = {Joint optimization of UPF placement and traffic routing for 5G core network user plane},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent wireless resource management in industrial
camera systems: Reinforcement learning-based AI-extension for efficient
network utilization. <em>COMCOM</em>, <em>216</em>, 68–85. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an innovative method for replacing wired communication within an operational agile production cell with wireless communication , using a minimal number of access points . Our approach utilizes Reinforcement Learning (RL) to optimize the layout and minimize the required number of access points. We introduce an Artificial Intelligence (AI) agent that facilitates cooperative interaction between cameras and access points, optimizing camera stream utilization and radio coverage. This solution benefits from the recent standardization of The fifth generation of mobile communications (5G) Network Resource Management (NRM) features of the User Equipment (UE) attached to the cameras. To accommodate the dynamic nature of the production cell, our optimization algorithms run during cell operation, necessitating extended training times. Our contributions extend beyond the state-of-the-art by offering a Digital Twin (DT) environment equipped with physics and radio propagation capabilities, fast raytracing for radio propagation modeling, curriculum-learning-based reinforcement learning, and a Physical Twin (PT) of the radio-connected camera with self-adjusting scene setup capabilities. This comprehensive proposal combines the efficiency of 5G radio utilization with precise modeling and high-resolution simulations, while also significantly improving training speed. In the Agile Robotics for Industrial Automation Competition (ARIAC) 2020 environment, our methods showed compelling results, with curriculum learning achieving a remarkable 3x speed-up compared to non-curriculum approaches. We also demonstrated that high Quality of Service (QoS) demands can be reduced to just 2% of total time while maintaining consistent productivity Key Performance Indicators (KPIs). Furthermore, we explored the integration of these methods into Internet of Things (IoT) devices with limited performance capabilities. In conclusion, our approach offers significant benefits and savings for IoT devices, networks, edge clouds, and industry players.},
  archive      = {J_COMCOM},
  author       = {Géza Szabó and József Pető},
  doi          = {10.1016/j.comcom.2023.12.026},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {68-85},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent wireless resource management in industrial camera systems: Reinforcement learning-based AI-extension for efficient network utilization},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flow-based service time optimization in software-defined
networks using deep reinforcement learning. <em>COMCOM</em>,
<em>216</em>, 54–67. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Ternary Content-Addressable Memory (TCAM) constitutes a memory variant employed within Software Defined Networking (SDN) node flow tables. These TCAMs deliver swift processing, enabling rapid parallel lookups. Nonetheless, due to their high energy consumption and cost, TCAMs have limited dimensions. This size constraint influences rule capacity, and suboptimal rule management can degrade network quality of service . Although different techniques for flow table management have been proposed during recent years, such as eviction, idle and hard timeout mechanisms, this paper proposes a Deep-Reinforcement Learning (DRL) solution, namely DRL-Idle, that is able to maximize the service time of the flows in an SDN network without considering any assumption about their status throughout time. By means of a continuous learning, DRL-Idle is able to also minimize the number of rule installations that are required to serve the target flows. Based on the idea of dynamically modifying the idle timeout value of the flows according to their needs, DRL-Idle outperforms existing approaches aiming at solving the Rule Placement Problem, averaging a 30% increase in performance.},
  archive      = {J_COMCOM},
  author       = {Manuel Jiménez-Lázaro and Javier Berrocal and Jaime Galán-Jiménez},
  doi          = {10.1016/j.comcom.2023.12.038},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {54-67},
  shortjournal = {Comput. Commun.},
  title        = {Flow-based service time optimization in software-defined networks using deep reinforcement learning},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed noise-guided mutual constraint framework for
unsupervised anomaly detection in smart industries. <em>COMCOM</em>,
<em>216</em>, 45–53. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sensor and data acquisition systems, integrated with deep learning methodologies, play a pivotal role in enhancing the sustainability and security of smart city environments, exemplifying the critical significance of anomaly detection techniques. Anomaly detection in complex industrial scenarios presents various challenges, such as intricate working environments, limited anomaly samples, and lack of a priori information . Unsupervised anomaly detection based on knowledge distillation enables anomaly detection using only normal samples. However, the similarity in structure between teacher and student models, along with identical input data flow, hampers accurate anomaly detection and localization. To address these issues, we propose MNMC, an unsupervised anomaly detection model consisting of a mixed noise generation module emulating real defects, a mutual constraint module, and an anomaly segmentation module. Firstly, to enhance the student network’s ability to learn robust features, we construct a hybrid noise model comprising dead-leaves noise and perlin noise. This generates features with structural texture and distributional characteristics closer to real anomalies. Secondly, we design a mutual constraint framework to further improve the learning ability of the student network for normal features by constraining representations containing only a single noise. Lastly, for the detection of anomalies at different scales, we propose a new evaluation metric based on equal importance of normal and anomalous regions. Through ablation experiments, we demonstrate the effectiveness of the simulated real defect generation module and the mutual constraints module. Performance experiments on the MVTec dataset show that our method achieves competitive results compared to the current state-of-the-art anomaly detection methods.},
  archive      = {J_COMCOM},
  author       = {Qing Zhao and Yan Wang and Yuxuan Lin and Shaoqi Yan and Wei Song and Boyang Wang and Jun Huang and Yang Chang and Lizhe Qi and Wenqiang Zhang},
  doi          = {10.1016/j.comcom.2023.12.031},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {45-53},
  shortjournal = {Comput. Commun.},
  title        = {Mixed noise-guided mutual constraint framework for unsupervised anomaly detection in smart industries},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Direction-agnostic gesture recognition system using
commercial WiFi devices. <em>COMCOM</em>, <em>216</em>, 34–44. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, channel state information (CSI) has been used to recognize hand gestures for contactless human–computer interaction. However, most existing solutions require precision hardware or prior learning at the same angle both during training and for inference/training in order to achieve high recognition accuracy . This requirement is unrealistic for practical instrumentation, where the orientation of a subject relative to the RF receiver may be arbitrary. We present direction-agnostic hand gesture recognition utilizing commercial WiFi devices to overcome low accuracy in non-trained observation angles. To achieve equal conditions in all recognition angles, first of all, through the circular antenna arrangement to mitigate the impact of user direction changes. Then, the orientation of users is estimated by the Fresnel zone model. Finally, the feature mapping model of users in different orientations is established, and the gesture features in the estimated direction are mapped to the benchmark direction to eliminate the influence caused by the change of user orientation. Experimental results in a typical indoor environment show that WiNDR has superior performance, and the average recognition accuracy of five common gestures is 92.38%.},
  archive      = {J_COMCOM},
  author       = {Yuxi Qin and Stephan Sigg and Su Pan and Zibo Li},
  doi          = {10.1016/j.comcom.2023.12.033},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {34-44},
  shortjournal = {Comput. Commun.},
  title        = {Direction-agnostic gesture recognition system using commercial WiFi devices},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IMCNN: Intelligent malware classification using deep
convolution neural networks as transfer learning and ensemble learning
in honeypot enabled organizational network. <em>COMCOM</em>,
<em>216</em>, 16–33. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional malware detection systems based on signature-based detection methods cannot detect new and unseen malware . Moreover, conventional machine learning methods for malware detection have utilized features extracted through static program analysis or dynamic analysis, which requires code debugging and execution primarily through offline processing; hence not a scalable approach. This paper proposes a novel intelligent malware classification using a deep convolution neural network (IMCNN) in organizational networks enabled with Honeypots . Systematic customization of pre-trained convolutional neural networks(CNN) as a transfer learning and ensemble learning as a classification is presented to detect intelligent modern-day malware. Real-world malware samples are systematically labeled and visualized into grayscale images . Four cutting-edge deep CNN models - VGG16, VGG19 , InceptionV3 , and ResNet50 , are trained on the ImageNet database ( ≥ 1 ≥1 million) and fine-tuned as feature extractors along with a basic CNN model. Three strategies are designed for feature extraction and selection: Rectified linear unit (ReLU) fully connected layer embedded in a deep CNN model, principal component analysis (PCA), and singular value decomposition(SVD). Reduced sets of features are stacked and used to train k-nearest neighbor (k-NN), support vector machine (SVM), and random forest (RF) classifiers for predictions. Subsequently, the predictive probabilities of different machine-learned models are ensembled using a soft voting method for final classification. The proposed method is evaluated on MalImg datasets (9339 malware samples of 25 families) and real-world modern malware datasets (690 malware of 22 families). The experimental results reveal that despite using a reduced feature set, the IMCNN effectively detects malware with 99.36% test accuracy on unseen data for MalImg datasets and 92.11% for real-world malware. In addition, the proposed method is compared with several existing state-of-art malware detection models in terms of performance accuracy and found performing as the best. Experiments demonstrated that the proposed method is resilient to polymorphic code obfuscation used by the malware authors .},
  archive      = {J_COMCOM},
  author       = {Sanjeev Kumar and B. Janet and Subramanian Neelakantan},
  doi          = {10.1016/j.comcom.2023.12.036},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {16-33},
  shortjournal = {Comput. Commun.},
  title        = {IMCNN: Intelligent malware classification using deep convolution neural networks as transfer learning and ensemble learning in honeypot enabled organizational network},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable and resilient 360-degree-video adaptive streaming
over HTTP/2 against sudden network drops. <em>COMCOM</em>, <em>216</em>,
1–15. (<a href="https://doi.org/10.1016/j.comcom.2024.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic Virtual Reality is supported by 360° video, which provides viewers with an immersive watching experience. However, 360°video is bulky in size, while the transmission system has limited ability to provide bandwidth. As such, intelligent adaptive delivery solutions play a crucial role in enabling users to stream high-quality 360°video. In this research, we propose a novel approach for 360-degree video streaming over HTTP/2 that can provide consumers with a good watching experience (QoE) even in varying network circumstances and head-eye movements over time. The proposed method deploys the so-called BBAG algorithm (Buffer and Bandwidth Allocation Algorithm) using Scalable Video Coding to choose appropriate tile layers to resolve the trade-off between network and user adaptivity. With the support of HTTP/2’s stream termination capability, the delivery of late tile layers is terminated to handle abrupt interruptions. By employing multiple buffer thresholds, BBAG is able to adapt bitrates to changes in users’ perspective while watching a 360-degree video. BBAG is proven to improve QoE up to about 90% by maintaining high and stable buffer level, while enhancing average viewport bitrates by about 80% compared to state-of-the-art methods in different scenarios of network bandwidths .},
  archive      = {J_COMCOM},
  author       = {Viet Hung Nguyen and Duy Tien Bui and Thanh Lam Tran and Cong Thang Truong and Thu Huong Truong},
  doi          = {10.1016/j.comcom.2024.01.001},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {1-15},
  shortjournal = {Comput. Commun.},
  title        = {Scalable and resilient 360-degree-video adaptive streaming over HTTP/2 against sudden network drops},
  volume       = {216},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive partitioning and efficient scheduling for
distributed DNN training in heterogeneous IoT environment.
<em>COMCOM</em>, <em>215</em>, 169–179. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing proliferation of Internet-of-Things (IoT) devices, it is a growing trend toward training a deep neural network (DNN) model in pipeline parallelism across resource-constraint IoT devices. To ensure the model convergence and accuracy, synchronous pipeline parallelism is usually adopted. However, the synchronous pipeline can incur a long waiting time due to its gradient aggregation of all microbatches. It is urgent for a DNN model to design an adaptive partitioning and efficient scheduling scheme in heterogeneous IoT environment. To address this problem, we propose a policy gradient based model partitioning and scheduling scheme (PG-MPSS) to minimize per-iteration training time. More specifically, we first design a double-network framework to divide and schedule a DNN model. Then, we adopt a policy gradient algorithm to update the double-network parameters, aiming at learning an optimal double-network model. We conduct extensive experiments to compare the DNN training time of the PG-MPSS scheme with that of Dynamic Programming (DP), Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Average&amp;Greedy (AG) and Proximal Policy Optimization (PPO) five baseline algorithms under different experimental settings. The related experimental results demonstrate that the PG-MPSS scheme can greatly expedite synchronous pipeline training of a DNN model.},
  archive      = {J_COMCOM},
  author       = {Binbin Huang and Xunqing Huang and Xiao Liu and Chuntao Ding and Yuyu Yin and Shuiguang Deng},
  doi          = {10.1016/j.comcom.2023.12.034},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {169-179},
  shortjournal = {Comput. Commun.},
  title        = {Adaptive partitioning and efficient scheduling for distributed DNN training in heterogeneous IoT environment},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLSFF: Global–local specific feature fusion for
cross-modality pedestrian re-identification. <em>COMCOM</em>,
<em>215</em>, 157–168. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modality Pedestrian Re-identification (Pedestrian ReID) is an image retrieval technique used to match targets of interest in a library. The main challenge of Pedestrian cm-ReID is that the modality gap between visible and infrared images reduces the recognition effect. To reduce the gap, we propose a novel Pedestrian cm-ReID model called Global–Local Specific Feature Fusion (GLSFF) to integrate the person features extracted by backbone network. It contains the Global Feature Fusion Module (GFFM) and the Local Feature Fusion Module (LFFM). LFFM and GFFM fuse the local specific and global shared features of pedestrians respectively. Fusion features mitigate the gap modality in images and retain discriminative information, such as gait and silhouette. In addition, we propose a joint training method, which combines Heterogeneous Center Loss (HC Loss), Triplet Loss and Cross-Entropy Loss (CE Loss) to accelerate the convergence rate of the model. Extensive experiments were conducted on two popular datasets, SYSU-MM01 and RegDB, and the mAP of GLSFF reached 64.47% and 81.37%, respectively.},
  archive      = {J_COMCOM},
  author       = {Chen Xue and Zhongliang Deng and Shuo Wang and Enwen Hu and Yao Zhang and Wangwang Yang and Yiming Wang},
  doi          = {10.1016/j.comcom.2023.12.035},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {157-168},
  shortjournal = {Comput. Commun.},
  title        = {GLSFF: Global–local specific feature fusion for cross-modality pedestrian re-identification},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Malicious traffic detection for cloud-edge-end networks: A
deep learning approach. <em>COMCOM</em>, <em>215</em>, 150–156. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious traffic has a great impact on network security . This paper studies a malicious traffic detection method based on deep learning . Aiming at the small sample data problem of malicious traffic, it is proposed to enrich the diversity of data by generating an adversarial network so that it can realize the strengthening and expansion of data characteristics and improve the accuracy of data analysis results. The FlowGAN model is used to detect malicious network data and multiple convolutional encoders . Deconvolutional encoders are used to identify malicious traffic. Through the identification and model comparison of different data sets, the AUC values of the model for the three data sets reached 0.9814, 0.7541 and 0.8556, respectively. The model is applied to online system detection. The experimental results show that the average detection accuracy of the method proposed in this paper for malicious traffic reaches 93.89%, which has a good application value.},
  archive      = {J_COMCOM},
  author       = {Hanbing Liu and Fang Han and Yajuan Zhang},
  doi          = {10.1016/j.comcom.2023.12.024},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {150-156},
  shortjournal = {Comput. Commun.},
  title        = {Malicious traffic detection for cloud-edge-end networks: A deep learning approach},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key security measurement method of authentication based on
mobile edge computing in urban rail transit communication network.
<em>COMCOM</em>, <em>215</em>, 140–149. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rail transit industry, the communication network is also widely used in various professions. Rail transit stations have the characteristics of narrow space, high degree of airtightness , limited sight lines, and uneven distribution of personnel in various periods. Although the security degree of rail transit entrances and exits is relatively reliable compared with that of stations, the anti-risk ability is fragile, and the consequences are often disastrous. To solve the problems of low-security access rate, long response time and ineffective identification of attack components of urban rail transit communication platform users, a security measurement method based on mobile edge computing (MEC) for the authentication key of urban rail transit communication network is proposed. Firstly, the wireless communication service of the urban rail transit platform is analyzed, and the data key protocol is constructed to obtain the hyper-chaos key pair of wireless communication information. Then, the double chaotic mapping model is applied to determine the key form and input parameters, and the bit key stream is obtained, which is subjected to exclusive operation with the plaintext for key decryption. Next, the block matching structure is used to analyze the associated eigenvalues that affect the security risk of urban rail transit communication networks. Meanwhile, the MEC server is used to accelerate the extraction of its feature values. Finally, in the environment of MEC, the paper constructs a risk identification model, evaluates the security strength of the authentication key, controls the threshold value, analyzes the impact factors and weights of communication network security , and completes the traceability measurement of communication network security risk. Experimental results show that the security access times of the MEC platform are senior, the maximum attack component obtained is 23500, which is consistent with the actual value, and the global maximum response time is 0.30 ms.},
  archive      = {J_COMCOM},
  author       = {Yubian Wang and Zhongsheng Wang and Xiang Liu},
  doi          = {10.1016/j.comcom.2023.12.020},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {140-149},
  shortjournal = {Comput. Commun.},
  title        = {Key security measurement method of authentication based on mobile edge computing in urban rail transit communication network},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class incremental learning via dynamic regeneration with
task-adaptive distillation. <em>COMCOM</em>, <em>215</em>, 130–139. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Incremental Learning (CIL) is a paradigm that excels in efficiently training models on an expanding set of classes, while preserving performance in classes learned earlier. In the context of smart city development, CIL plays a crucial role in addressing the need for Networking Systems of Artificial Intelligence (NSAI) to continuously adapt to ever-evolving data. In recent years, the incremental network structure widely adopted in CIL suffers from poor knowledge distillation and rapid model parameter growth . In this paper, we introduce a novel strategy named dynamic regeneration with task-adaptive distillation (DRTAD), which dynamically adapts to new tasks, and sustains good performance even as the class set continues to grow. DRTAD adopts a two-stage training strategy: dynamic regeneration and dynamic retention. During dynamic regeneration, DRTAD enhances feature representation by creating a new feature extraction module that extracts features from new classes, while also utilizing features from previously learned classes. Additionally, DRTAD introduces task-adaptive distillation to improve the poor knowledge distillation, further mitigating catastrophic forgetting. During the dynamic retention phase, DRTAD achieves a higher pruning rate through RM operation. Comprehensive experiments on CIFAR-100 and ImageNet-100 datasets demonstrate DRTAD’s superior performance compared to existing CIL methods. Notably, in the CIFAR100-B50 5 steps incremental setting, DRTAD increases the last-phase accuracy from 65.51% to 70.54% (＋5.03%), while maintaining fewer parameters (−11.1%). Similarly, in the ImageNet100-B50 10 steps setting, the last-phase accuracy rises from 70.04% to 72.50% (＋2.46%). These results indicate DRTAD’s efficacy in mitigating catastrophic forgetting in incremental structure.},
  archive      = {J_COMCOM},
  author       = {Hao Yang and Wei He and Zhenyu Shan and Xiaoxin Fang and Xiong Chen},
  doi          = {10.1016/j.comcom.2023.12.030},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {130-139},
  shortjournal = {Comput. Commun.},
  title        = {Class incremental learning via dynamic regeneration with task-adaptive distillation},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bandwidth abstraction and service rate instantiation for
latency-bounded reliability provisioning in 5th generation wireless
networks. <em>COMCOM</em>, <em>215</em>, 120–129. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The provisioning of the statistical reliability regard to latency is crux for the bursty traffic in the 5th generation (5G) wireless network slicing. Based on martingale theory, a novel analysis framework of the latency-bounded reliability is proposed. For the queuing system with bursty traffic, a Queuing Length Martingale is constructed to reveal the impact of the entanglement between the arrival process and the service process on the latency performance, where these two stochastic processes are sharply heterogeneous. The complex martingale parameters are determined. Relying on the time shift feature of the Queuing Length Martingale, a Latency Martingale is defined, which enables to analyze the latency-bounded unreliability in the martingale domain. Leveraging the stopping time theory, a tight upper bound of the unreliability regard to latency is obtained. A bandwidth abstraction and service rate instantiation scheme is designed for wireless network slicing. Bandwidth abstraction facilitates to decouple the statistical reliability requirements as the desired bandwidth of the wireless node . In the 5G access network, a transmission power allocation problem is formulated with the constraint of abstracted bandwidth. According to the access states and channel characteristics, the demanded transmission power is captured dynamically to instantiate the service rates. Simulation results highlight the effectiveness of the bandwidth abstraction and service rate instantiation scheme.},
  archive      = {J_COMCOM},
  author       = {Wei Liu and Baozhu Yu and Xiangyu Liu and Xuefen Chi and Jinyi Zhang},
  doi          = {10.1016/j.comcom.2023.12.017},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {120-129},
  shortjournal = {Comput. Commun.},
  title        = {Bandwidth abstraction and service rate instantiation for latency-bounded reliability provisioning in 5th generation wireless networks},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Downlink spreading factor selection in LoRaWAN.
<em>COMCOM</em>, <em>215</em>, 112–119. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN is one of the most commonly used Internet of Things protocols for applications that require low cost, low power, and long range communications. It has been proved that – mainly due to regional radio duty cycle restrictions – the protocol does not scale well in presence of confirmed (downlink) traffic. To support downlink traffic , LoRaWAN employs two receive windows, RX1 and RX2, whereas a number of channels are assigned to each of those windows. The protocol uses a fixed Spreading Factor (SF) – a LoRa PHY modulation parameter – in RX2, while the SF of the uplink is employed in RX1. Since the SF of RX1 cannot be changed, selecting a low or a high value of SF in RX2 is of critical importance for the duty cycle resources of the gateways. On one hand, selecting high SF values, the time resources of the gateways may get depleted fast leading to low capacity because the transmission time increases with higher SF values. On the other hand, lower SF values reduce reachability due to the worse sensitivity which causes retransmissions , and thus, lower capacity. In this paper, a study of the total theoretical downlink capacity is provided giving useful insights of the protocol behavior as the number of uplinks increases, especially for congested network scenarios. An exhaustive SF selection solution is also presented to compute the maximum downlink capacity. It is shown that the RX2 SF which provides the best capacity may imply fairness compromises for part of the devices. Extensive simulations are employed to confirm the theoretical findings in scenarios with a single and multiple gateways. Experiments conducted on a test-bed for selected scenarios also confirm the theoretical findings.},
  archive      = {J_COMCOM},
  author       = {Dimitrios Zorbas},
  doi          = {10.1016/j.comcom.2023.12.013},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {112-119},
  shortjournal = {Comput. Commun.},
  title        = {Downlink spreading factor selection in LoRaWAN},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network security situation assessment and prediction method
based on multimodal transformation in edge computing. <em>COMCOM</em>,
<em>215</em>, 103–111. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network security play a critical role in ensuring the robustness and integrity of network systems . We propose a novel method based on multimodal transformation in edge computing for accurate and reliable data communication. The method leverages Graph Convolutional Networks (GCNs) to capture and analyze the complex relationships and dependencies among network entities, enabling enhanced prediction capabilities. By integrating multimodal transformation, diverse data sources are fused in the model. The assessments and comprehensive comparisons with established algorithms unequivocally establish the supremacy of our proposed approach, particularly with respect to metrics such as accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUC-ROC). Our method exhibits promising performance, enabling proactive threat detection and mitigation in network security by more then 90% accuracy. In the future work, we have included the different multimodal transformation techniques, addressing interoperability challenges, scaling the method for large-scale networks, and adapting the model to dynamic network environments.},
  archive      = {J_COMCOM},
  author       = {Meng Xu and Shenghan Liu and Xuewu Li},
  doi          = {10.1016/j.comcom.2023.12.014},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {103-111},
  shortjournal = {Comput. Commun.},
  title        = {Network security situation assessment and prediction method based on multimodal transformation in edge computing},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed cyber-physical intrusion detection using
stacking learning for wide-area protection system. <em>COMCOM</em>,
<em>215</em>, 91–102. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wide-area protection systems (WAPSs) heavy depends on communication technologies to operate, which leaves space for cyberattacks. A well-designed stealthy and coordinated cyberattacks can disrupt the seamless operation of WAPS by compromising measurement signals, control signals, or both. In this paper, we present a distributed cyber-physical intrusion detection system (DCPIDS) that utilizes the bilateral data from both the cyber side and the physical side to accurately detect cyberattacks on both measurement and control signals in WAPSs. DCPIDS consists of multiple slave agents (SAs) scattered in every area of the power system for regional-area intrusion detection and a master agent (MA) embedded in the system protection center for system status awareness. For the SAs, a hybrid-based intrusion detection method is utilized to conduct regional-area intrusion detection. The proposed method receives the bilateral data to simultaneously detect data integrity attacks on measurement and control signals using three classification models and performs the identification of single and coordinated attacks using a rule-based approach. Further, to train the classification models in the proposed method, a NewStacking-based model training algorithm is adopted. The proposed algorithm combines different selected classifiers that operate on two different feature subsets, which improves the detection accuracy of the models and extends the generalization ability with better robustness. Experimental results reveal that the proposed algorithm has better performance than existing machine learning algorithms and state-of-art works, the proposed method can identify single and coordinated attacks with high accuracy, and our DCPIDS satisfies the real-time requirements for practical online application.},
  archive      = {J_COMCOM},
  author       = {Qiuyu Lu and Qize Gao and June Li and XuanXuan Xie and Wenrui Guo and Jin Wang},
  doi          = {10.1016/j.comcom.2023.12.008},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {91-102},
  shortjournal = {Comput. Commun.},
  title        = {Distributed cyber-physical intrusion detection using stacking learning for wide-area protection system},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-assisted secure data transmission techniques for
next-generation HetNets: A review. <em>COMCOM</em>, <em>215</em>, 74–90.
(<a href="https://doi.org/10.1016/j.comcom.2023.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous Networks (HetNets) play an imperative role in enhancing the quality-of-service (QoS) for end-users by increasing the spectral efficiency (SE) of the network and reducing the power consumption of user equipment (UE). However, the dynamic and distributed nature of HetNets makes them susceptible to several types of attacks (e.g., jamming, eavesdropping). Also, new technologies of 5G like massive MIMO , mmWave, and NOMA bring unique security concerns to 5G HetNets, which were not present in pre-5G HetNets. Implementing traditional security techniques such as access control, encryption, and network security seems to be insufficient for 5G HetNets and their inherent vulnerabilities. Physical layer security (PLS) has evolved as a novel approach and robust substitute to cryptography methods for ensuring secure transmission of confidential data. Also, AI methods are critical in transforming the HetNets’ security from just allowing safe communications between UEs to security-enabled intelligence systems. Motivated by the benefits of AI and PLS in improving wireless security , this paper seeks to provide a comprehensive survey of artificial intelligence (AI) enabled PLS techniques for secure data transmission in 5G HetNets. We have introduced physical layer threats and significant security issues in 5G HetNets. Then, we provide an outline of the conventional PLS techniques and challenges associated with them in the design of secure transmission techniques for 5G HetNets, such as security-oriented beamforming , cooperative jamming, etc. Then we provide an in-depth analysis of the role of AI in optimizing and designing the intelligent PLS techniques, which can be used to enhance the security of data transmission in 5G HetNets. We have also performed a strengths, weaknesses, opportunities, and threats (SWOT) analysis of the existing PLS techniques to further assist the readers in designing secure transmission techniques. Finally, we discuss various issues and future research directions associated with designing AI-enabled secure data transmission techniques.},
  archive      = {J_COMCOM},
  author       = {Himanshu Sharma and Gitika Sharma and Neeraj Kumar},
  doi          = {10.1016/j.comcom.2023.12.015},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {74-90},
  shortjournal = {Comput. Commun.},
  title        = {AI-assisted secure data transmission techniques for next-generation HetNets: A review},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy optimization for full-duplex wireless-powered IoT
networks using rotary-wing UAV with multiple antennas. <em>COMCOM</em>,
<em>215</em>, 62–73. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel design for the rotary-wing unmanned aerial vehicle (UAV)-enabled full-duplex (FD) wireless-powered Internet of Things (IoT) networks. In this network, the UAV is equipped with an antenna array , and the K K IoT sensors, which are distributed randomly, use single-antenna to communicate. By sending the energy, the UAV as a hybrid access point , charges the sensors and collects information from them. Then, to manage the time and optimize the energy, the sensors are divided into N groups, so that the UAV equipped with multi-input multi-output (MIMO) technology can serve the sensors in a group, during the total time T T . We provide a simple implementation of the wireless power transfer protocol in the sensors by using the time division multiple access (TDMA) scheme to receive information from the users. In other words, the sensors of each group receive energy from the UAV, when it hovers over the sensors of the previous group, and also when the UAV flies over the previous group to the current group. The sensors of each group send their information to the UAV, when the UAV is hovering over their group. Under these assumptions, we formulate two optimization problems : a sum throughput maximization problem , and a total time minimization problem . Numerical results show that our proposed optimal network provides better performance than the existing networks. In fact, our novel design can serve more sensors at the cost of using more antennas compared to that of the conventional networks.},
  archive      = {J_COMCOM},
  author       = {Leyla Fathollahi and Mahmood Mohassel Feghhi and Mahmoud Atashbar},
  doi          = {10.1016/j.comcom.2023.12.011},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {62-73},
  shortjournal = {Comput. Commun.},
  title        = {Energy optimization for full-duplex wireless-powered IoT networks using rotary-wing UAV with multiple antennas},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobile-IRS assisted next generation UAV communication
networks. <em>COMCOM</em>, <em>215</em>, 51–61. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior research on intelligent reflection surface (IRS)-assisted unmanned aerial vehicle (UAV) communications has focused on a fixed location for the IRS or mounted on a UAV. The assumption that the IRS is located at a fixed position will prohibit mobile users from maximizing many wireless network benefits, such as data rate and coverage. Furthermore, assuming that the IRS is placed on a UAV is impractical for various reasons, including the IRS’s weight and size and wind speed in severe weather. Unlike previous studies, this study assumes a single UAV and an IRS mounted on a mobile ground vehicle (M-IRS) to be deployed in an Internet-of-Things (IoT) 6G wireless network to maximize the average data rate. Such a methodology for providing wireless coverage using an M-IRS assisted UAV system is expected in smart cities. In this paper, we formulate an optimization problem to find an efficient trajectory for the UAV, an efficient path for the M-IRS, and users’ power allocation coefficients that maximize the average data rate for mobile ground users. Due to its intractability, we propose efficient techniques to help find the optimization problem’s solution. First, we show that our dynamic power allocation technique outperforms the fixed power allocation technique in the network average sum rate . Then we employ the individual movement model (Random Waypoint Model) in order to represent the users’ movements inside the coverage area. Finally, we propose an efficient approach using a Genetic Algorithm (GA) to find an efficient trajectory for the UAV and an efficient path for the M-IRS to provide wireless connectivity for mobile users during their movement. We demonstrate through simulations that our methodology can enhance the average data rate by 15% on average compared with the static IRS and by 25% on average compared to without the IRS system.},
  archive      = {J_COMCOM},
  author       = {Hazim Shakhatreh and Ahmad Sawalmeh and Ali H. Alenezi and Sharief Abdel-Razeq and Ala Al-Fuqaha},
  doi          = {10.1016/j.comcom.2023.12.025},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {51-61},
  shortjournal = {Comput. Commun.},
  title        = {Mobile-IRS assisted next generation UAV communication networks},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A UAV-based coverage gap detection and resolution in
cellular networks: A machine-learning approach. <em>COMCOM</em>,
<em>215</em>, 41–50. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of unmanned aerial vehicles (UAVs) to extend the coverage of terrestrial base stations (BSs) in cellular communication systems has been gaining increasing interest in recent years. This is due to the ease of deploying UAV-mounted BSs within relatively short times with low associated costs. In this work, we formulate the problem of deploying UAV-mounted BSs to mitigate the coverage gaps of the terrestrial BSs of cellular networks in some geographic regions. We then devise a technique that provides the optimal bound of the solution to the coverage gap detection and mitigation problem. We also propose a machine-learning (ML) based technique to provide a real-time solution for deploying UAVs to determine and mitigate the coverage gaps. In both solutions, namely, the optimal and ML-based solutions, the deployment of the UAVs is done in such a way that addresses the coverage gaps at the minimum possible cost. Simulation results show that our ML-based technique performs quite closely to the performance of the optimal solution, at a significantly lower complexity, and hence fulfills the real-time requirements of such deployments. It also provides significantly better performance results than a common benchmark solution from the literature.},
  archive      = {J_COMCOM},
  author       = {Ahmed Fahim Mostafa and Mohamed Abdel-Kader and Yasser Gadallah},
  doi          = {10.1016/j.comcom.2023.12.010},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {41-50},
  shortjournal = {Comput. Commun.},
  title        = {A UAV-based coverage gap detection and resolution in cellular networks: A machine-learning approach},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contactless wheat foreign material monitoring and
localization with passive RFID tag arrays. <em>COMCOM</em>,
<em>215</em>, 29–40. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the early stage of harvesting, transportation, and storage, the grain could be mingled with clods from the field, metal pieces from aging machines, and other objects of foreign material, which will greatly influence the grain quality and food security. In this paper, we propose a novel radio frequency (RF) sensing system termed TagSee, which utilizes passive RFID tag arrays to sense the objects buried in wheat. The goal is to simultaneously sense the presence of foreign materials, locate their locations in the 3D space and sense their types, to automatically monitor the stored grain. Specifically, we use RFID received signal strength (RSS) and phase as features for foreign material detection. To design the TagSee system, we first introduce a sensing space division method. Then, an Euclidean Distance Ratio (EDR) algorithm and a heuristic method are proposed to achieve high localization success accuracy. A multi-class SVM method is used to sense different materials properties based on the impact of different foreign materials on signal features. Experimental results show that TagSee can effectively detect foreign materials in stored grain and achieve a centimeter-level localization success accuracy, and can recognize four types of objects in grain.},
  archive      = {J_COMCOM},
  author       = {Erbo Shen and Weidong Yang and Xuyu Wang and Shiwen Mao},
  doi          = {10.1016/j.comcom.2023.12.019},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {29-40},
  shortjournal = {Comput. Commun.},
  title        = {Contactless wheat foreign material monitoring and localization with passive RFID tag arrays},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards heart infarction detection via image-based dataset
and three-stream fusion framework. <em>COMCOM</em>, <em>215</em>, 21–28.
(<a href="https://doi.org/10.1016/j.comcom.2023.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global ageing society intensifies, many elderly people are facing the situation of living alone without care. When sudden illnesses like heart attacks or heart infarctions happen, they might not be able to get prompt and efficient treatment due to not being noticed in time, which can be fatal. To alleviate the above issue, this paper presents a Heart Infarction Image (HII) dataset, an image-based benchmark for heart attack/infarction identification tasks. The dataset contains images of people suffering from heart infarction or normal conditions. In addition, we propose a methodological framework based on this dataset that utilizes convolutional neural networks to identify multidimensional features of potential patients through three network streams in order to quickly and efficiently predict possible heart attack cases and provide early warning. We also present a Guided Adaptive Weight Fusion (GAWF) mechanism in the feature fusion stage. Numerous qualitative and ablation analyses corroborate the necessity of our dataset and framework. Our method achieves an accuracy of 90.01%, indicating that it performs well in recognizing acute heart disease in real-world settings. At the same time, systematic experiments show that using lightweight models under the proposed framework can also achieve good results, which allows our method to be extended to mobile intelligence platforms.},
  archive      = {J_COMCOM},
  author       = {Chuyi Zhong and Dingkang Yang and Shunli Wang and Lihua Zhang},
  doi          = {10.1016/j.comcom.2023.12.021},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {21-28},
  shortjournal = {Comput. Commun.},
  title        = {Towards heart infarction detection via image-based dataset and three-stream fusion framework},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DCSP: A delay and cost-aware service placement and load
distribution algorithm for IoT-based fog networks. <em>COMCOM</em>,
<em>215</em>, 9–20. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The service placement problem in a fog-cloud pertains to the intricate task of ascertaining the optimal placement locations for diverse service replicas that constitute an Internet of Things (IoT) application within a distributed computing architecture. Nonetheless, due to the heterogeneous nature of fog nodes and cloud data centers , the constantly evolving nature of IoT applications, the pressing demand for real-time data processing with minimal latency, and the imperative of cost optimization, service placement remains an arduous and intricate challenge. Heuristic algorithms are widely employed to derive near-optimal solutions. The present study introduces a novel heuristic approach to service placement and load distribution in fog-cloud environments called Delay and Cost-aware Service Placement (DCSP) . This approach takes into account both delay and cost and thereby enables the optimal utilization of resources, delay, and cost for delay-sensitive services. DCSP features continual adjustment of the number of service replicas based on workload and resource availability , ensuring efficient usage of fog and cloud computing resources and optimal delay for each request. The results of simulation experiments that compare various algorithms demonstrate the superiority of DCSP, which outperforms other methods in cost, average delay, and resource utilization . The experimental results not only illustrate the exceptional superiority of our method over MOHGA, MVC, and the Local method in terms of delay and utilization but also demonstrate our efforts to maintain cost at a comparable level. Our method surpasses all of them in utilization, achieving remarkable improvements ranging from 7.4 % to 49.7 %. Furthermore, it outperforms them in terms of delay, with significant enhancements ranging from 2.1 % to 19.3 %.},
  archive      = {J_COMCOM},
  author       = {Sadoon Azizi and Mohammad Shojafar and Pedram Farzin and Javad Dogani},
  doi          = {10.1016/j.comcom.2023.12.016},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {9-20},
  shortjournal = {Comput. Commun.},
  title        = {DCSP: A delay and cost-aware service placement and load distribution algorithm for IoT-based fog networks},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective networking: Enabling effective communications
towards 6G. <em>COMCOM</em>, <em>215</em>, 1–8. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The realization of envisioned 6G use cases involving holographic and multi-sensory communications demand terabits per second data rates and latencies in the range of microseconds for an immersive experience . Concurrently, 6G’s hyper-intelligent IoT use cases require extremely low-latency and reliable communications at the network edge. To address these requirements, communications should be tailored to end-user goals. To this end, we study communication effectiveness, where a sender and receiver harness their computing capabilities and artificial intelligence , to maximize the impact of transmitted messages while sending fewer bits. On our model, the messages can get shorter as locally accumulated knowledge increases the targeted effect of the message. Hence, we describe a framework in which the accumulated knowledge can be aggregated and shared in a distributed manner. On a real-life use case, we showcase the potential reduction in the number of bits transmitted owing to the transferred accumulated knowledge. Finally, we explore future research directions in effective communications, considering technical, economical, and privacy considerations.},
  archive      = {J_COMCOM},
  author       = {Ece Gelal Soyak and Ozgur Ercetin},
  doi          = {10.1016/j.comcom.2023.12.002},
  journal      = {Computer Communications},
  month        = {2},
  pages        = {1-8},
  shortjournal = {Comput. Commun.},
  title        = {Effective networking: Enabling effective communications towards 6G},
  volume       = {215},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient BS sleeping, user association, and resource
allocation in full-duplex ultra-dense networks. <em>COMCOM</em>,
<em>214</em>, 270–283. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full-duplex ultra-dense network (FD-UDN) is a promising technology in 5G mobile networks for handling the increase in network capacity. However, interference management and energy efficiency are two of the most important challenges that must be addressed. With the small base stations (SBSs) densification , employing sleeping strategy coupled with resource management becomes an effective approach to manage interference and power consumption in FD-UDN. To the best of our knowledge, this aspect has not been addressed in previous work. To this end, we develop a framework to optimize the BS sleeping and resource management with the aim of maximizing energy efficiency and maintaining the quality of service (QoS) requirements of users. The problem is formulated as a non-convex mixed-integer non-linear programming problem, which is difficult to handle. Employing the Dinkelbach method, the objective function of the problem is converted to an equivalent parametric subtractive form. Then, the problem is decoupled into two sub-problems: user association and resource allocation , as well as BSs on/off switching. The former is solved using the iterative reweighted l q -norm minimization (IRM) method, and the latter is solved using the Lagrangian dual method and the constrained concave-convex procedure (CCCP). The simulation results demonstrate that the proposed method is more effective than traditional ones in simultaneously improving the EE, reducing power consumption , and keeping fewer SBSs active, especially in high network loads.},
  archive      = {J_COMCOM},
  author       = {Tahere Rahmati and Behrouz Shahgholi Ghahfarokhi},
  doi          = {10.1016/j.comcom.2023.12.012},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {270-283},
  shortjournal = {Comput. Commun.},
  title        = {Energy-efficient BS sleeping, user association, and resource allocation in full-duplex ultra-dense networks},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tag-based self-learning task recommendation for mobile
crowdsensing via collaborative multi-expert system. <em>COMCOM</em>,
<em>214</em>, 260–269. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the powerful sensing capabilities of mobile smart devices , sensing users can complete mobile crowd sensing tasks through smart devices in the Internet of Things . When a user completes a sensing task, the sensing user may want to complete more related items from the previous task as an extended task. However, it is difficult for traditional recommender systems to satisfy sensing user needs for tasks because the recommendation results should be diverse. In this paper, we propose a multi-expert tag recommendation framework (MTRS) with the help of tags for sensing tasks. The main purpose is to predict whether the sensing user wants to extend the task, and provide appropriate related tasks as suggestions. The framework mainly consists of two parts: recommendation migration and recommendation purification. We extract features from multiple aspects. In the information gain expert network, information gain is obtained through the relationship between tags. In the feature interaction expert network, multi-head self-attention is used to model the feature interaction between different feature fields. In similar, the relevance between tags is explicitly emphasized in the degree expert network. Feature information is obtained through different expert networks, and a multi-critic strategy is proposed to judge the importance of each expert network. At the same time, different optimization goals are set for recommendation migration and recommendation purification. In recommendation purification, the user&#39;s satisfaction with task tags and the delay cost of task recommendation are specially considered. Finally, experiments demonstrate the correctness, effectiveness and robustness of the MTRS approach.},
  archive      = {J_COMCOM},
  author       = {Jian Wang and Zhe Zhang and Guosheng Zhao},
  doi          = {10.1016/j.comcom.2023.12.009},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {260-269},
  shortjournal = {Comput. Commun.},
  title        = {Tag-based self-learning task recommendation for mobile crowdsensing via collaborative multi-expert system},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimized multi-hop routing protocol for wireless sensor
network using improved honey badger optimization algorithm for efficient
and secure QoS. <em>COMCOM</em>, <em>214</em>, 244–259. (<a
href="https://doi.org/10.1016/j.comcom.2023.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wireless sensor network (WSN) is a significant technology that might contribute in the industrial revolution. Sensor Nodes (SNs) in WSNs are equipped with a battery as a power source. Energy is the most critical resource for WSNs since the battery cannot be replaced or refilled. Several solutions have been devised and implemented over the years to preserve WSNs , which are a scarce resource. This study suggested a model to solves the energy issue as well as security issue and presents an optimized multi-hop routing in WSNs based on improved Honey Badger Algorithm (I-HBA). This method for multi-hop routing involves two steps: selecting a Cluster Head (CH) and routing the data packets. The I-HBA is used to pick energy-efficient CHs for efficient data transfer . The SNs then send their data through the chosen CH, which forwards it to the base station (BS) using the most efficient number of hops. The suggested I-HBA is used to select the finest hops. The proposed model employs the multi-objective fitness function including eight parameters for effective routing in the WSNs. Furthermore, security conscious multihop routing is carried out by employing a trust model that includes direct and indirect trust, data, integrity and forwarding rate factor. Additionally, the proposed I-HBA based routing protocol compared with the existing protocol such as LEACH, HEED , and PSO ; and the proposed protocol outperforms the existing models. The findings show that for the same number of iterations, the whole quantity of data packets receives by the WSN using the proposed I-HBA based multihop routing protocol is substantially larger than for the PSO, LEACH, and HEED-based models.},
  archive      = {J_COMCOM},
  author       = {Majid Altuwairiqi},
  doi          = {10.1016/j.comcom.2023.08.011},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {244-259},
  shortjournal = {Comput. Commun.},
  title        = {An optimized multi-hop routing protocol for wireless sensor network using improved honey badger optimization algorithm for efficient and secure QoS},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust multimodal federated learning for incomplete
modalities. <em>COMCOM</em>, <em>214</em>, 234–243. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer electronics are continuously collecting multimodal data, such as audio, video, and so on. A multimodal learning mechanism can be adopted to deal with these data. Due to the consideration of privacy protection, some successful attempts at multimodal federated learning (MMFed) have been conducted. However, real-world multimodal data is usually missing modalities, which can significantly affect the accuracy of the global model in MMFed. Effectively fusing and analyzing multimodal data with incompleteness remains a challenging problem. To tackle this problem, we propose a robust Multimodal Federated Learning Framework for Incomplete Modalities ( FedInMM ). More specifically, we design a Long Short-Term Memory (LSTM)-based module to extract the information in the temporal sequence. We dynamically learn a weight map to rescale the feature in each modality and formulate the different contributions of features. And then the content of each modality is further fused to form a uniform representation of all modalities of data. By considering the temporal dependency and intra-relation of multi-modalities automatically through the learning stage, this MMFed framework can efficiently mitigate the effects of missing modalities. By using two multimodal datasets, DEAP and AReM, we have conducted comprehensive experiments by simulating different levels of incompleteness. Experimental results demonstrate that FedInMM outperforms other approaches and can train highly accurate models on datasets comprising different incompleteness patterns, which is more appropriate for integration into a practical multimodal application.},
  archive      = {J_COMCOM},
  author       = {Songcan Yu and Junbo Wang and Walid Hussein and Patrick C.K. Hung},
  doi          = {10.1016/j.comcom.2023.12.003},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {234-243},
  shortjournal = {Comput. Commun.},
  title        = {Robust multimodal federated learning for incomplete modalities},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wi-fi throughput estimation and forecasting for
vehicle-to-infrastructure communication. <em>COMCOM</em>, <em>214</em>,
223–233. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicles increasingly need to connect to external networking infrastructure, to support applications such as over-the-air updates, edge computing , and even autonomous driving . The ubiquity of IEEE 802.11 Wi-Fi makes it ideal for opportunistic vehicular access. However, that ubiquity also creates a problem of choice. In a heterogeneous Wi-Fi environment, where different networks coexist, it becomes important for vehicles to be able to pick the best-performing one. Focusing on delay-insensitive traffic, we equate network performance with throughput. To inform network selection we aim to first estimate current throughput, and then forecast its evolution through time. In order to avoid introducing load onto the network, we estimate throughput using only passively observable variables such as signal strength. We used Symbolic Regression (SR) and an Unscented Kalman Filter (UKF) to develop a computationally inexpensive estimation model — UKF-SR. We trained and tested this model using experimental data featuring 802.11n, ac, and ad networks. UKF-SR proved competitive against more expensive models such as shallow neural networks . To predict future throughput, we explored both general time-series forecasting models such as Autoregressive Integrated Moving Average (ARIMA), and domain-specific ones based on mobility information. The latter clusters historical throughput according to attributes such as vehicle position and direction of movement, using the cluster’s average as the forecast. An evaluation using experimental data showed the mobility-based models to meaningfully outperform general forecasting.},
  archive      = {J_COMCOM},
  author       = {Daniel Teixeira and Rui Meireles and Ana Aguiar},
  doi          = {10.1016/j.comcom.2023.12.005},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {223-233},
  shortjournal = {Comput. Commun.},
  title        = {Wi-fi throughput estimation and forecasting for vehicle-to-infrastructure communication},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedAFR: Enhancing federated learning with adaptive feature
reconstruction. <em>COMCOM</em>, <em>214</em>, 215–222. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a distributed machine learning method where clients train models on local data to ensure that data will not be transmitted to a central server, providing unique advantages in privacy protection. However, in real-world scenarios, data between different clients may be non-Independently and Identically Distributed (non-IID) and imbalanced, leading to discrepancies among local models and impacting the efficacy of global model aggregation. To tackle this issue, this paper proposes a novel framework, FedARF, designed to improve Federated Learning performance by adaptively reconstructing local features during training. FedARF offers a simple reconstruction module for aligning feature representations from various clients, thereby enhancing the generalization capability of cross-client aggregated models. Additionally, to better adapt the model to each client’s data distribution , FedARF employs an adaptive feature fusion strategy for a more effective blending of global and local model information, augmenting the model’s accuracy and generalization performance . Experimental results demonstrate that our proposed Federated Learning method significantly outperforms existing methods in variety image classification tasks, achieving faster model convergence and superior performance when dealing with non-IID data distributions.},
  archive      = {J_COMCOM},
  author       = {Youxin Huang and Shunzhi Zhu and Weizhe Chen and Zhicai Huang},
  doi          = {10.1016/j.comcom.2023.12.007},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {215-222},
  shortjournal = {Comput. Commun.},
  title        = {FedAFR: Enhancing federated learning with adaptive feature reconstruction},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fragmentation of data packets in wireless sensor network
with variable temperature and channel conditions. <em>COMCOM</em>,
<em>214</em>, 201–214. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study formalizes and solves the problem of minimizing the energy dissipated by a node in order to successfully transmit the initial data amount during one round of communication over a wireless channel affected by Rice fading under varying temperature conditions. In this work, the models for the temperature dependence of the parameters in an incoherent frequency shift key (FSK) demodulator are presented and investigated for the first time, taking into account both the frequency separation and the synchronization type of the clock generator. The studied orthogonality coefficients of FSK signals represent internal parameters in the proposed model of the incoherent Rice wireless channel. This model allows the analytical relationship between the bit error probability (BEP) and node heating temperature, signal fading depth in the channel (Rician K-factor), and signal-to-noise ratio (SNR) to be established. Since it is necessary to retransmit the entire data packet when an erroneous bit is detected in a communication system that uses an automatic repeat request , at high BEP values, it is recommended to fragment packets in order to improve the energy efficiency of the nodes. By using the developed models for calculating energy losses during the communication round, it was determined that optimal fragmentation options (OFO) for data packets can be identified within specific ranges of temperature and Rician K-factor, which includes the overhead costs for packeting and retransmission . As a result, OFO matrices of packets in the &quot;temperature – K-factor&quot; ( t 0 – K ) coordinate system were obtained for minimizing the energy loss of nodes at different SNR values. In our example of using TI CC2500 node transceivers for 90-day monitoring of corn vegetation, the t 0 K -Frag algorithm reduced node energy losses by a factor of K = 3.3 compared to the case of transmitting an unfragmented packet of initial length.},
  archive      = {J_COMCOM},
  author       = {Vladimir Fedorenko and Irina Samoylenko and Vladimir Samoylenko},
  doi          = {10.1016/j.comcom.2023.12.001},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {201-214},
  shortjournal = {Comput. Commun.},
  title        = {Fragmentation of data packets in wireless sensor network with variable temperature and channel conditions},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction-based cooperative cache discovery in VANETs for
social networking. <em>COMCOM</em>, <em>214</em>, 184–200. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media traffic is increasingly pervading the Internet. Such traffic is mostly generated by mobile devices, which creates immense traffic load on backhaul links in 5G networks . This load can be mitigated by using vehicular networks as a traffic offloading platform, where intelligent vehicles can act as a valuable asset that can bring the data closer to the requester via edge caching . In this paper, we propose the Prediction-Assisted Cooperative Content Discovery (PACD) scheme. PACD exploits the static and mobile nature of parked and moving vehicles, respectively, to promulgate cached content information into the network via bloom filters . PACD is the first scheme that leverages such information to perform cooperative cache discovery to locate closer replicas to the requester by dynamically predicting the location of caching nodes. The Any Relation Clustering Algorithm (ARCA) is employed to cluster trips based on their route similarity using the XXDice similarity coefficient. Each cluster is then trained using the Mixture Transition Distribution-Probit (MTD-Probit) model to predict the remaining trajectory of vehicles. Using these predictions, PACD tracks all possible data providers and ranks them based on their proximity to the requester, as well as their prediction entropy. Extensive evaluations show that PACD yields significant improvements of up to 86%, 30%, 42%, and 16% in terms of delay, packet delivery ratio , cache hit ratio, and prediction accuracy, respectively, compared to prominent caching and prediction schemes in vehicular networks.},
  archive      = {J_COMCOM},
  author       = {Sara A. Elsayed and Sherin Abdelhamid and Hossam S. Hassanein},
  doi          = {10.1016/j.comcom.2023.11.030},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {184-200},
  shortjournal = {Comput. Commun.},
  title        = {Prediction-based cooperative cache discovery in VANETs for social networking},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable low-cost data transmission in smart grid system.
<em>COMCOM</em>, <em>214</em>, 174–183. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Smart Grid (SG) designates an electrical energy distribution system that automatically and autonomously adapts production to demand. To achieve this balance, the Smart Grid uses a network of sensors and transmission devices. The smart meter, as the key element of the Smart Grid, is an undisputed promise to optimize energy efficiency, but it raises certain questions in terms of data exploitation and privacy protection. Smart meters (SM) deployed in SG systems collect and transmit data to the service provider. Therefore, trust, security, and confidentiality are required in the SG system. Since communications between entities are exposed to attacks, strong protection is needed. In this article, we perform both trust mutual authentication algorithm and soft data transit for the Smart Grid using rotation function. We prove the security of our algorithm against known attacks. To prevent key escrow problems, keys are smoothly updated. In addition, the efficiency of the algorithm against relevant work is demonstrated using real measurements.},
  archive      = {J_COMCOM},
  author       = {Sarra Jebri and Arij Ben Amor and Salah Zidi},
  doi          = {10.1016/j.comcom.2023.12.006},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {174-183},
  shortjournal = {Comput. Commun.},
  title        = {Reliable low-cost data transmission in smart grid system},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Velocity tracking control of nodes for the nonlinear complex
dynamical networks associated with outgoing links subsystem.
<em>COMCOM</em>, <em>214</em>, 167–173. (<a
href="https://doi.org/10.1016/j.comcom.2023.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In general, a complex dynamical network (CDN) can be viewed as a combination of a nodes subsystem(NS) and an outgoing links subsystem (OLS), both of which are coupled with each other. In this paper, a velocity tracking control protocol is used to achieve the velocity tracking control of nodes for the nonlinear CDN with the help of OLS. Firstly, the models of NS and OLS for the CDN are established, and three assumptions for CDN and the definition of the outgoing links vector are given. Further, the control objectives of this article are given. The appropriate control protocol and the scheduled auxiliary targets (SAT) are synthesized to realize the velocity tracking control of nodes for the nonlinear CDN. Finally, a numerical simulation comparison experiment is bench tested to validate the effectiveness of the proposed control protocol and the SAT.},
  archive      = {J_COMCOM},
  author       = {Peitao Gao and Chun Shan and Yinhe Wang and Chihui Liu},
  doi          = {10.1016/j.comcom.2023.12.004},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {167-173},
  shortjournal = {Comput. Commun.},
  title        = {Velocity tracking control of nodes for the nonlinear complex dynamical networks associated with outgoing links subsystem},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RED-SP-CoDel: Random early detection with static priority
scheduling and controlled delay AQM in programmable data planes.
<em>COMCOM</em>, <em>214</em>, 149–166. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging network application paradigms, such as the Tactile Internet, re-emphasize the need for different Quality of Service (QoS) levels. Due to the large packet buffers in the underlying network data plane, Active Queue Management (AQM) is generally required to curtail packet latencies for flows requiring high QoS levels. At the same time, programmable data planes, such as P4, enable packet processing at line-speed, albeit with limited packet processing functionalities. However, the existing AQM mechanisms that support QoS differentiation are too complex to readily run on P4, while the existing AQM mechanisms that run on P4 do generally not support effective QoS differentiation. We address this gap by developing to the best of our knowledge the first AQM mechanism that supports effective QoS differentiation while running on P4. Specifically, we propose SP-CoDel, which combines the well-known Controlled Delay (CoDel) AQM mechanism with Static Priority (SP) scheduling for QoS differentiation. Also, we propose RED-SP-CoDel, which adds a RED AQM component to SP-CoDel so as to make the AQM with priorities essentially parameterless. As a community resource contribution, we substantially extend the existing P4Simulator to the novel fused P4-NS3 Simulator so as to enable the evaluation of packet processing mechanisms through the combined functionalities of P4 device emulation and NS3 packet simulation. Evaluations conducted with the P4 reference switch model in the P4-NS3 Simulator indicate that SP-CoDel and RED-SP-CoDel provide high QoS to high-priority data streams, i.e., significantly reduce latency and packet loss compared to CoDel, while effectively mitigating bufferbloat.},
  archive      = {J_COMCOM},
  author       = {Osel Lhamo and Mingyu Ma and Tung V. Doan and Tobias Scheinert and Giang T. Nguyen and Martin Reisslein and Frank H.P. Fitzek},
  doi          = {10.1016/j.comcom.2023.11.026},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {149-166},
  shortjournal = {Comput. Commun.},
  title        = {RED-SP-CoDel: Random early detection with static priority scheduling and controlled delay AQM in programmable data planes},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A utilization based genetic algorithm for virtual machine
placement in cloud systems. <em>COMCOM</em>, <em>214</em>, 136–148. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increasing demand for cloud computing and related services, cloud providers need to come up with methods and mechanisms that increase the performance, availability and reliability of data centers and cloud systems . Server virtualization is a key component to achieve this, which enables sharing of resources of a single physical machine among multiple virtual machines in a totally isolated manner. Optimizing virtualization has a very significant effect on the overall performance of a cloud computing system . This requires efficient and effective placement of virtual machines into physical machines. Since this is an optimization problem that involves multiple constraints and objectives, we propose a method based on genetic algorithms to place virtual machines into physical servers of a data center. By considering the utilization of machines and node distances, our method, called Utilization Based Genetic Algorithm (UBGA), aims at reducing resource waste, network load, and energy consumption at the same time. We compared our method against several other placement methods in terms of utilization achieved, networking bandwidth consumed, and energy costs incurred, using an open-source, publicly available CloudSim simulator. The results show that our method provides better performance compared to other placement approaches.},
  archive      = {J_COMCOM},
  author       = {Mustafa Can Çavdar and Ibrahim Korpeoglu and Özgür Ulusoy},
  doi          = {10.1016/j.comcom.2023.11.028},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {136-148},
  shortjournal = {Comput. Commun.},
  title        = {A utilization based genetic algorithm for virtual machine placement in cloud systems},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MedGCN: An IoT-edge thrombus graph convolutional network for
accurate prediction and prescription diagnosis of vascular occlusive
diseases from unstructured clinical reports. <em>COMCOM</em>,
<em>214</em>, 123–135. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The timely and accurate prediction and diagnosis of vascular occlusive diseases are pivotal in enhancing patient outcomes. This research addresses this critical healthcare challenge by introducing MedGCN, a novel thrombus graph convolutional network model. MedGCN is specifically designed for the precise prediction of prescriptions and diagnoses based on unstructured clinical diagnostic reports. The model synergistically blends OpenAI’s GPT4 and a uniquely designed Cross Fusion Graph Convolutional Network (CF-GCN) to ensure the meticulous fusion of knowledge. We delve into nine distinct learning tasks, encompassing both prescription and diagnosis and employ a multi-label classification GCN pretraining technique to assess them. Our evaluation underscores MedGCN’s robust predictive prowess across various tasks. By amalgamating cutting-edge AI paradigms with IoT edge computing , this research not only bolsters the efficacy of medical decision-making but also champions patient privacy. The methodologies and findings presented herein hold immense potential for deployment in IoT frameworks, thus proffering swift and precise assistance in medical decision-making and addressing a paramount need in the contemporary healthcare landscape.},
  archive      = {J_COMCOM},
  author       = {Fei Gao and Zhifeng Xiao and Shuo Chen and Richeng Yu and Xiaorong Li},
  doi          = {10.1016/j.comcom.2023.11.001},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {123-135},
  shortjournal = {Comput. Commun.},
  title        = {MedGCN: An IoT-edge thrombus graph convolutional network for accurate prediction and prescription diagnosis of vascular occlusive diseases from unstructured clinical reports},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learning-based approach for video streaming over
fluctuating networks with limited playback buffers. <em>COMCOM</em>,
<em>214</em>, 113–122. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, the surge in applications like social networking and online learning platforms has led to a substantial rise in on-demand video streaming, thus ensuring a seamless user experience holds paramount importance , particularly given the dynamic nature of network conditions. Moreover, numerous service providers are embracing smaller buffer sizes, aiming to reduce bandwidth inefficiencies due to the possibility of users ending video sessions prematurely. However, this transition presents a significant challenge for conventional adaptive bitrate (ABR) algorithms, as they grapple with the task of harmonizing low stalling time, high playback bitrate, and the constraint of a minimized buffer size. In this study, we introduce a novel ABR approach, L 2 2 -ABR, which leverages self-play reinforcement learning to address these complexities. Unlike conventional reward-engineering learning-based ABR strategies that update gradients to maximize linear reward functions, L 2 2 -ABR treats video streaming as a fundamental objective and trains neural networks (NNs) with explicit requirements tailored to video streaming with small playback buffers. Through an extensive series of trace-driven experiments, we showcase that L 2 2 -ABR outperforms existing methods, effectively striking a balance between buffer management and network scenarios without inducing excessive buffer under-runs or overflows. Compared to existing ABR schemes, our method reduces undesirable buffer events, including rebuffering events and buffer full events, while improving the average QoE by up to 71.88% and 75.25% over the HSDPA and FCC datasets, respectively.},
  archive      = {J_COMCOM},
  author       = {Weihe Li and Jiawei Huang and Qichen Su and Wanchun Jiang and Jianxin Wang},
  doi          = {10.1016/j.comcom.2023.11.027},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {113-122},
  shortjournal = {Comput. Commun.},
  title        = {A learning-based approach for video streaming over fluctuating networks with limited playback buffers},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PrivMaskFL: A private masking approach for heterogeneous
federated learning in IoT. <em>COMCOM</em>, <em>214</em>, 100–112. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the arrival of the era of the Internet of Things (IoT), the rapid development of new technologies such as artificial intelligence , big data , and other advanced techniques, data is growing geometrically. These data are prone to form data silos, scattered in various places. Jointly decentralized data applications will accelerate the progress and development of the times, but also face the challenge of data privacy protection. Federated learning (FL), a new branch of distributed machine learning , is trained by collaborative training to obtain global model without direct exposure to local datasets. Some studies have shown that typically federated learning involves a larger number of participants, it can lead to a significant increase in communication overhead , resulting in issues such as higher latency and bandwidth consumption. We suggest masking a subset of diverse participants and allowing the remaining participants to proceed with the next communication round of updates. Our aim is for reducing the communication overhead and improving the convergence performance of the global model on the premise of heterogeneous data privacy protection. We design a private masking approach PrivMaskFL to address two problems. We firstly propose a dynamic participant aggregation masking approach, which adopts the greedy ideology to select the relatively important participants and mask the unimportant ones; secondly, we design an adaptive differential privacy approach, which adaptively stratifies privacy budget according to the characteristics of participant, allocates the budget in a fine-grained stratified level, and adds Gaussian noise reasonably. Specifically, in each communication round, the participant’s model needs to perform local differential privacy noise addition for uplink parameter transmission; the server aggregates to acquire global model, finds a candidate participant subset based on the smaller parameter divergence by using the greedy algorithm approximation for t + 1 t+1 -th communication round for downlink parameter transmission. Subsequently, the privacy budget sequence is divided and granted to the participants of the stratified level, and the Gaussian noise addition of adaptive differential privacy is completed to achieve privacy protection without compromising the model’s usability. In experiments, our approach reduces the communication overhead and improve the convergence performance. Furthermore, our approach achieves higher accuracy and robust variance on both FMNIST and FEMNIST datasets.},
  archive      = {J_COMCOM},
  author       = {Jing Xiong and Hong Zhu},
  doi          = {10.1016/j.comcom.2023.11.022},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {100-112},
  shortjournal = {Comput. Commun.},
  title        = {PrivMaskFL: A private masking approach for heterogeneous federated learning in IoT},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is it possible to extend IPv6? <em>COMCOM</em>,
<em>214</em>, 90–99. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The IPv6 Hop-by-Hop Options and Destination Options Extension Headers have historically faced challenges in deployment due to a lack of router support coupled with concerns around potential for denial-of-service attacks. However, there has been a renewed interest within the standards community both in simplifying their processing, and in using these extension headers for new applications. Through a wide-scale measurement campaign, we show that many autonomous systems in both access networks and the core of the Internet do permit the traversal of packets that include options, and that the path traversal currently depends on the type of network, size of the option and the transport protocol used, but does not usually depend on the type of included option. This is an encouraging result when considering the extensibility of IPv6. We show that packets that include an extension header can also impact the function of load balancing network devices, and present evidence of equipment mis-configuration, noting that a different path to the same destination can result in a different traversal result. Finally, we outline the current deployment challenges and provide recommendations for how extension headers can utilise options to extend IPv6.},
  archive      = {J_COMCOM},
  author       = {Ana Custura and Raffaello Secchi and Elizabeth Boswell and Gorry Fairhurst},
  doi          = {10.1016/j.comcom.2023.10.006},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {90-99},
  shortjournal = {Comput. Commun.},
  title        = {Is it possible to extend IPv6?},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial special issue on AI-driven sensing and computing
for cyber-physical systems. <em>COMCOM</em>, <em>214</em>, 88–89. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Zheng Xu and Neil Yen and Xiaomeng Ma and Vijayan Sugumaran and Yunhuai Liu},
  doi          = {10.1016/j.comcom.2023.11.009},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {88-89},
  shortjournal = {Comput. Commun.},
  title        = {Editorial special issue on AI-driven sensing and computing for cyber-physical systems},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blockchain-based access control system for efficient and
GDPR-compliant personal data management. <em>COMCOM</em>, <em>214</em>,
67–87. (<a href="https://doi.org/10.1016/j.comcom.2023.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New digital technologies generate large amounts of information. This data is processed by Service Providers in order to improve and develop new services and products, but also to fund themselves. However, processing personal data may result in the extraction of sensitive information , which, in turn, may lead to jeopardizing the users’ privacy. To mitigate this significant risk, the European Parliament and Council of the European Union elaborated the General Data Protection Regulation (GDPR). This regulation forces Service Providers to obtain Data Subjects’ explicit consent prior to collecting and processing their personal data. Nevertheless, the GDPR’s legislative text does not define how Service Providers must transparently demonstrate that they already have these consents. Moreover, most individuals do not know the rights they have over their personal data, neither does this regulation provide them with efficient methods to be aware of what third parties are doing with such data. In order to address this situation, we propose a lightweight blockchain-based GDPR-compliant personal data management platform. The new solution provides public access to immutable evidences that reflect the reached agreements between Data Subjects and Service Providers. In this way, Service Providers can effectively demonstrate that they are fulfilling the regulation, and Data Subjects are able to control and manage their personal data according to their legitimate rights. We have implemented the new system, and we have performed a detailed study which includes: GDPR-compliance, provided functionality, security and privacy issues, and the cost in terms of gas and US dollars of the different operations to be run on the blockchain .},
  archive      = {J_COMCOM},
  author       = {Cristòfol Daudén-Esmel and Jordi Castellà-Roca and Alexandre Viejo},
  doi          = {10.1016/j.comcom.2023.11.017},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {67-87},
  shortjournal = {Comput. Commun.},
  title        = {Blockchain-based access control system for efficient and GDPR-compliant personal data management},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MBP: Multi-channel broadcast proxy re-encryption for
cloud-based IoT devices. <em>COMCOM</em>, <em>214</em>, 57–66. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broadcast proxy re-encryption methods extend traditional proxy re-encryption mechanisms, where a single user shares the cloud data with multiple receivers. When the sender has different data to share with different sets of users, the existing broadcast proxy re-encryption schemes allow him/her to calculate distinct re-encryption keys for different groups in terms of additional computation time and overhead. To overcome these issues, we propose a scheme, named MBP, in IoT application scenario that allows the sender to calculate a single re-encryption key for all the groups of users, i.e., IoT devices. We use the multi-channel broadcast encryption concept in the broadcast proxy re-encryption method, so as single re-encryption key calculation and single re-encryption are done for different groups of IoT devices. It reduces the size of the security elements. However, it increases the computation time of the receiver IoT devices at the time of decryption of both the ciphertexts . To address this issue, we use the Rubinstein-Ståhl bargaining game approach. MBP is secure under a selective group chosen-ciphertext attack using the random oracle model . The implementation of MBP shows that it reduces the communication overhead from the data owner to the cloud server and from the cloud server to the receiver than existing algorithms.},
  archive      = {J_COMCOM},
  author       = {Sumana Maiti and Sudip Misra and Ayan Mondal},
  doi          = {10.1016/j.comcom.2023.11.020},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {57-66},
  shortjournal = {Comput. Commun.},
  title        = {MBP: Multi-channel broadcast proxy re-encryption for cloud-based IoT devices},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent wireless sensing driven metaverse: A survey.
<em>COMCOM</em>, <em>214</em>, 46–56. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metaverse seamlessly integrates the real world with the virtual world and allows avatars to carry out rich activities including creation, display, entertainment, social, and trading. It integrates the most fundamental technologies, such as Blockchain , Interaction, Games, Artificial Intelligence, Networks, and the Internet of Things , named BIGANT. Interaction technologies are significant to allow users to interact with virtual entities in physical environments via sensors, such as AR, MR, and VR. However, there are still great challenges regarding how to access the metaverse in a more intelligent, faster, and effective way, especially in capturing human positions and activities. Intelligent wireless sensing technology, integrating AI , can serve as an intelligent, flexible, non-contact way to access the metaverse and expedite the establishment of a bridge between the real physical world and the metaverse. Hence, this paper elaborates on the existing work and discusses potential important trends and hotspots in wireless sensing, especially localization, activity recognition, and pattern analysis. After that, we discussed how intelligent wireless sensing will evolve in the metaverse, together with current challenges and open issues in this topic. Through this overview, we wish readers can better understand how intelligent wireless sensing accelerates the accessing to metaverse and the insights behind the wireless sensing in the metaverse.},
  archive      = {J_COMCOM},
  author       = {Lingjun Zhao and Qinglin Yang and Huakun Huang and Longtao Guo and Shan Jiang},
  doi          = {10.1016/j.comcom.2023.11.024},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {46-56},
  shortjournal = {Comput. Commun.},
  title        = {Intelligent wireless sensing driven metaverse: A survey},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSformer: Enhancing deep learning efficiency for intelligent
IoT. <em>COMCOM</em>, <em>214</em>, 33–45. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of deep learning technology has led to increasing demand for more intelligent, automated, and humanized Internet of Things (IoT) devices. Deep learning models , while endowing IoT devices with the capability to learn higher-level features, concurrently impose more demanding computational and storage prerequisites on the hardware. To tackle the challenge and enable the practical application of deep learning models in IoT devices, we propose a novel efficient Transformer called CSformer, which incorporates intra-layer cluster and inter-layer selection. Intra-layer cluster is performed using a k-means++ based generation algorithm to improve cluster accuracy. To address the issues of information loss caused by clustering, we propose cluster center information enhancement and clustering loss calculation modules. The inter-layer selection strategy selects tokens according to their contribution, consistently diminishes redundancy, and prioritizes the retention of crucial information. By consistently reducing the sequence length , the inter-layer selection significantly improves training speed and reduces the memory occupation of the model. The experimental results indicate that in two common scenarios for intelligent IoT, namely text classification and sequence labeling, CSformer significantly outperforms the baseline models . Specifically, in the text classification task , our model achieves an average 22.66% reduction in memory consumption, a 37.98% decrease in time consumption, and a superior 9.58% performance improvement compared to baseline models across six datasets. Additional experiments substantiate the efficacy of the intra-layer cluster and inter-layer selection modules, as demonstrated through ablation experiments, overall performance, and visualization. The intra-layer cluster module enhances the performance of existing models by achieving more precise clustering and mitigating information loss, leading to significant performance improvements. The inter-layer selection module enhances the efficiency of existing studies by reducing model memory consumption and improving computational efficiency through the selective retention of essential tokens. This can effectively facilitate future research in applying advanced deep learning models to intelligent IoT, expanding the range of application scenarios and tasks within the field of intelligent IoT.},
  archive      = {J_COMCOM},
  author       = {Xu Jia and Han Wu and Ruochen Zhang and Min Peng},
  doi          = {10.1016/j.comcom.2023.11.007},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {33-45},
  shortjournal = {Comput. Commun.},
  title        = {CSformer: Enhancing deep learning efficiency for intelligent IoT},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective based unbiased community identification in
dynamic social networks. <em>COMCOM</em>, <em>214</em>, 18–32. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A network is a topological arrangement of its two basic elements, nodes and edges. Networks in the real world are not static. They tend to evolve with time, causing the set of nodes and edges to alter as well. They consist of several hidden bits of data whose analysis have drawn significant research interest. Identifying groups of similar nodes or edges helps in gaining knowledge about their interaction patterns. These groups are known as communities, which can be disjoint or overlapping. The dynamic nature of the network also impact its current community structure and makes it difficult to keep track of them. The paper presents a multi-objective optimization approach for identifying community structure in a dynamic network. A network is considered as a series of events generated over time, where each event is a new edge introduced at a time. The proposed algorithm uses three objective functions that are inspired from network properties . The community of a node corresponding to an input edge is updated by an algorithm based on its newness. The algorithm uses the Pareto front principle to identify the optimal community. The algorithm is evaluated over 12 datasets and compared to 10 state-of-the-art algorithms. It shows superior performance on real and connected datasets and also performs well for disconnected datasets. The algorithm is evaluated using both accuracy and quality metrics, with the quality metrics slightly outweighing the accuracy metrics.},
  archive      = {J_COMCOM},
  author       = {Sneha Mishra and Shashank Sheshar Singh and Shivansh Mishra and Bhaskar Biswas},
  doi          = {10.1016/j.comcom.2023.11.021},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {18-32},
  shortjournal = {Comput. Commun.},
  title        = {Multi-objective based unbiased community identification in dynamic social networks},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Honey-block: Edge assisted ensemble learning model for
intrusion detection and prevention using defense mechanism in IoT.
<em>COMCOM</em>, <em>214</em>, 1–17. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has gained popularity with interconnected devices and diverse network applications, leading to increased vulnerability of sensitive data to security threats. Many researchers have focused on intrusion detection without considering prevention mechanisms. To overcome these issues, we propose the honeypot and blockchain-based intrusion detection and prevention (HB-IDP) model, in which edge computing is introduced to reduce the latency during communication. Initially, three-fold authentication is performed for entities (users, devices, and gateway) to ensure legitimacy using the camellia encryption algorithm (CEA), which provides secret keys. The proposed datasets (i.e., UNSW-NB15 and BoT-IoT) are pre-processed at the gateway using min–max normalization to reduce redundancy and complexity during feature extraction and classification. Signature-based intrusion detection is performed on the pre-processed data, with known attacks classified into three classes (normal, malicious, and suspicious) using the improved isolation forest (IIF) algorithm. Suspicious data are forwarded for anomaly detection to the edge level; here, a honeypot is deployed to attract the attacker’s patterns. Ensemble learning technique, including multi-layer perceptron (MLP), general adversarial network (GAN), and lightweight convolutional neural Network (LCNN), is applied to classify suspicious packet behaviors. Once intrusions are detected, the proposed work prevents future intrusions by generating reports, which are then encrypted by the CEA algorithm and provided to legitimate users. All transactions (i.e., key generation, report generation, and attacker patterns) are stored in the blockchain . The HB-IDP model’s performance and effectiveness were evaluated using network simulator 3.26 (NS-3.26), showcasing its superiority over existing approaches.},
  archive      = {J_COMCOM},
  author       = {Ernest Ntizikira and Lei Wang and Jenhui Chen and Kiran Saleem},
  doi          = {10.1016/j.comcom.2023.11.023},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {1-17},
  shortjournal = {Comput. Commun.},
  title        = {Honey-block: Edge assisted ensemble learning model for intrusion detection and prevention using defense mechanism in IoT},
  volume       = {214},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging the edge and cloud for V2X-based real-time object
detection in autonomous driving. <em>COMCOM</em>, <em>213</em>, 372–381.
(<a href="https://doi.org/10.1016/j.comcom.2023.11.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental perception is a key element of autonomous driving because the information received from the perception module influences core driving decisions. An outstanding challenge in real-time perception for autonomous driving lies in finding the best trade-off between detection quality and latency. Major constraints on both computation and power must be taken into account for real-time perception in autonomous vehicles. Larger detection models tend to produce the best results but are also slower at runtime. Since the most accurate detectors may not run in real-time locally, we investigate the possibility of offloading computation to edge and cloud platforms, which are less resource-constrained. We create a synthetic dataset to train object detection models and evaluate different offloading strategies. We measure inference and processing times for object detection on real hardware, and we rely on a network simulation framework to estimate data transmission latency. Our study compares different trade-offs between prediction quality and end-to-end delay. Following the existing literature, we aim to perform object detection at a rate of 20Hz. Since sending raw frames over the network implies additional transmission delays, we also explore the use of JPEG and H.265 compression at varying qualities and measure their impact on prediction. We show that models with adequate compression can be run in real-time on the edge/cloud while outperforming local detection performance.},
  archive      = {J_COMCOM},
  author       = {Faisal Hawlader and François Robinet and Raphaël Frank},
  doi          = {10.1016/j.comcom.2023.11.025},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {372-381},
  shortjournal = {Comput. Commun.},
  title        = {Leveraging the edge and cloud for V2X-based real-time object detection in autonomous driving},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-mobile vehicles task offloading for vehicle-edge-cloud
collaboration: A dependency-aware and deep reinforcement learning
approach. <em>COMCOM</em>, <em>213</em>, 359–371. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wide application of edge cloud computing in the Internet of Vehicles (IoV) provides lower latency, more efficient computing power, and more reliable data transmission services for vehicle applications. Realistic vehicle applications frequently consist of multiple tasks with dependencies. Efficiently and quickly scheduling individual tasks for multiple vehicle applications to reduce latency and energy consumption is challenging. Our proposed approach leverages Deep Reinforcement Learning (DRL) to develop a task scheduling strategy that ensures real-time and efficient operations. We maximize the utilization of available resources by harnessing the computational capabilities of vehicles, multiple MEC servers, and a cloud server. Specifically, we model the dependencies of tasks using a Directed Acyclic Graph (DAG) and design dynamically adjustable weights for delay and energy consumption. Transforming the task offloading problem in a vehicle-edge-cloud environment, which considers dependencies, into a Markov Decision Process (MDP) enables us to tackle it effectively. To obtain optimized offloading decisions quickly, we employ Double Deep Q-Network (DDQN) along with specially designed mobility management strategies. A penalty mechanism is introduced in DDQN to impose penalties when the vehicle application is delayed beyond its deadline. Simulation results show that the proposed scheme can significantly decrease the latency and energy consumption of multi-applications compared to the other three schemes and ensure the successful execution of tasks.},
  archive      = {J_COMCOM},
  author       = {Shanchen Pang and Lili Hou and Haiyuan Gui and Xiao He and Teng Wang and Yawu Zhao},
  doi          = {10.1016/j.comcom.2023.11.013},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {359-371},
  shortjournal = {Comput. Commun.},
  title        = {Multi-mobile vehicles task offloading for vehicle-edge-cloud collaboration: A dependency-aware and deep reinforcement learning approach},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum-resistant transport layer security. <em>COMCOM</em>,
<em>213</em>, 345–358. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reliance on asymmetric public key cryptography (PKC) and symmetric encryption for cyber-security in current telecommunication networks is threatened by the emergence of powerful quantum computing technology. This is due to the ability of quantum computers to efficiently solve problems such as factorization or discrete logarithms , which are the basis for classical PKC schemes. Thus, the assumption that communications networks are secure no longer holds true. Quantum Key Distribution (QKD) and post-quantum cryptography (PQC) are the first cyber-security technologies that allow communications to resist the attacks of a quantum computer. To achieve quantum-resistant communications, the aforementioned technologies need to be incorporated into a network security protocol such as Transport Layer Security (TLS). In this paper, we describe and implement two novel, hybrid solutions in which QKD and PQC are combined inside TLS for achieving quantum-resistant authenticated key exchange : Concatenation and Exclusively-OR (XOR). We present the results, in terms of complexity and security enhancement, of integrating state-of-the-art QKD and PQC technologies into a practical, industry-ready TLS implementation. Our findings demonstrate that the adoption of a PQC-only approach enhances the TLS handshake performance by approximately 9 % compared to classical methods. Furthermore, our hybrid PQC-QKD quantum-resistant TLS comes at a performance cost of approximately 117 % during the key establishment process. In return, we substantially augment the security of the handshake, paving the road for the development of future-proof quantum-resistant communication systems based on QKD and PQC.},
  archive      = {J_COMCOM},
  author       = {Carlos Rubio García and Simon Rommel and Sofiane Takarabt and Juan Jose Vegas Olmos and Sylvain Guilley and Philippe Nguyen and Idelfonso Tafur Monroy},
  doi          = {10.1016/j.comcom.2023.11.010},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {345-358},
  shortjournal = {Comput. Commun.},
  title        = {Quantum-resistant transport layer security},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource allocation for cost minimization of a slice broker
in a 5G-MEC scenario. <em>COMCOM</em>, <em>213</em>, 331–344. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fifth generation (5G) of mobile networks may offer a custom logical and virtualized network called network slicing. This virtualization opens a new opportunity to share infrastructure resources and encourage cooperation between several Infrastructure Providers (InPs) to offer tailored network slices for the Slice Tenants (STs). The Slice Broker (SB) is emerging as intermediate entity that purchases resources from the InPs and it offers network slices to the STs. The main challenge of the SB is to jointly decide the purchase of heterogeneous (data and network) resources from multiple InPs and create the slices to meet the various requests from the STs. Being an economical entity, the target of the SB is to maximize its profit by minimizing the costs while satisfying all the ST requests. This paper formulated the SB cost minimization problem and used CPLEX to obtain the optimal solution. The problem formulation considers the realistic scenario that the InPs offer the computing, storage and network resources by using predetermined configurations. Therefore, for each of the computing platform and logical connection, the SB may select one of the configurations. The proposed cost-minimization problem is compared with three alternative problems that have three different objectives: computing platform consolidation, network connection consolidation, and both computing-network consolidation. The computing platform and network connection consolidation are currently the most common approaches for decreasing resource costs. However, the result shows that consolidating computing and network resources fails to reach the actual minimal cost. The proposed problem finds the cheapest solution, which can save at least 30% of the total cost of the other approaches in every evaluated scenario. Moreover, consolidating the number of computing platforms can lead to the most expensive solution, up to 40% higher than the optimal solution of our proposed problem.},
  archive      = {J_COMCOM},
  author       = {Annisa Sarah and Gianfranco Nencioni},
  doi          = {10.1016/j.comcom.2023.11.016},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {331-344},
  shortjournal = {Comput. Commun.},
  title        = {Resource allocation for cost minimization of a slice broker in a 5G-MEC scenario},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-agent federated reinforcement learning-based
optimization of quality of service in various LoRa network slices.
<em>COMCOM</em>, <em>213</em>, 320–330. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The innovations heralded through the implementation of next-generation 5G (Fifth-Generation) networks provide an opportunity for the efficient coexistence of heterogeneous services distributed by a single physical virtualized infrastructure. Indeed, thanks to the possibility of virtualization of network functions implemented in 5G networks , the management of physical resources will be able to become more flexible and users will be able to benefit from a service customization to satisfy their demand in terms of energy efficiency, throughput and communication reliability. However, these advances are not without constraints. The management of physical and virtual resources will become more complex given the number of connected objects which will increase, generating a large volume of data to manage. This therefore requires the implementation of much more intelligent systems in the network controllers in order to guarantee the QoS (Quality of Service) of the communications. Thus, an important axis of research is now oriented towards artificial intelligence techniques, more precisely reinforcement learning to overcome this problem. Driven by this context, we are directing our research towards improving the QoS offered to users of connected objects by proposing an optimization model based on network slicing and federated reinforcement learning in order to minimize energy consumption, maximize user throughputs and reduce latency during communications between LoRa (Long Range) devices. The results obtained by the simulations carried out in a realistic framework clearly demonstrate that our proposal optimizes the traffic in each network slice and also for the individual user .},
  archive      = {J_COMCOM},
  author       = {Eric Ossongo and Moez Esseghir and Leila Merghem-Boulahia},
  doi          = {10.1016/j.comcom.2023.11.015},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {320-330},
  shortjournal = {Comput. Commun.},
  title        = {A multi-agent federated reinforcement learning-based optimization of quality of service in various LoRa network slices},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PiGateway: Real-time granular analysis of smart home network
traffic using p4. <em>COMCOM</em>, <em>213</em>, 309–319. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) makes the home smarter by real-time sensing of the situation and transmission of data to the cloud. As IoT devices continue to expand rapidly, a critical need arises for effective device-type identification methods. Accurate identification of IoT device types is essential for various applications, including network management, security, etc. To identify the type of IoT devices, existing works have proposed various Media Access Control (MAC)-based and Machine Learning (ML) approaches. These solutions lack real-time granular analysis and In-network classification of IoT traffic. In this work, we propose PiGateway: a P4-based gateway to classify the type of IoT devices at a line rate and prioritize the device type. Initially, PiGateway develops a Decision Tree (DT) model and generates flow rules from the model. The developed model is deployed in the PiGateway using two mechanisms. One mechanism is to convert the DT model into Match-Action Tables (MATs), where the tables store the flow rules. Another approach developed an optimized model by deploying the DT model into PiGateway as a mathematical expression. Further, to achieve the In-network classification, the gateway calculates flow-level features dynamically from incoming IoT traffic flow. The feature values are given as input to the deployed DT model for classifying the flow into a specific device type. Subsequently, the flow is prioritized based on device type and Type of Service (ToS) field value. Additionally, our proposed solution is designed and tested in a real-time environment. Through implementation results, we show that the PiGateway can classify the type of IoT devices with 98.61% accuracy and an average CPU usage of 10.68%. Also, the classification time in PiGateway is 0.098ms for 8000 flows using an optimized model. Finally, the proposed system serves as a gateway in a smart home IoT network.},
  archive      = {J_COMCOM},
  author       = {Suvrima Datta and Venkanna U.},
  doi          = {10.1016/j.comcom.2023.11.019},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {309-319},
  shortjournal = {Comput. Commun.},
  title        = {PiGateway: Real-time granular analysis of smart home network traffic using p4},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NextGenV2V: Authenticated V2V communication for next
generation vehicular network using (2, n)-threshold scheme.
<em>COMCOM</em>, <em>213</em>, 296–308. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Securing end-to-end communication in Vehicular Ad-hoc Networks (VANET) is crucial due to zero trust between the communicating entities. Additionally, the underlying communication needs to be lightweight in many cases, especially when vehicles are moving at high speeds or deploying applications like Vehicular Sensor Networks (VSN) and next-generation vehicular networks with densely deployed small-cells and micro-cells. The traditional VANET facilitates two types of communication: Vehicle-to-Infrastructure (V2I) communication to establish trust between a Vehicle User ( V i Vi ) and roadside infrastructure , followed by Vehicle-to-Vehicle communication for information exchange. Unlike existing protocols that encounter security issues and high authentication delay as they require communication with a Trusted Authority (TA) every time V i Vi needs authentication or re-authentication, we present a symmetric key-based an authenticated V2V communication for next generation vehicular network using (2, n)-threshold scheme, called NextGenV2V. In NextGenV2V, authentication of a newly visited V i Vi is performed with the help of TA, but re-authentication of a pre-authenticated V i Vi is done without the support of TA. This significantly reduces communication overhead during re-authentication and improves the overall authentication delay. NextGenV2V is designed in such a way that at least 2 out of n n registered vehicle users must cooperate to compute a shared secret key . This feature ensures improved security, as attackers cannot breach the system’s security unless two or more vehicles are compromised. Furthermore, considering that an attacker cannot physically be present in more than one location simultaneously, the system’s security remains intact even if a current vehicle is compromised. The proposed protocol is also verified to resist known security attacks, both formally and informally. To demonstrate the effectiveness of NextGenV2V, a comparative performance analysis is presented that proves its suitability for next-generation vehicular networks.},
  archive      = {J_COMCOM},
  author       = {Pankaj Kumar and Hari Om},
  doi          = {10.1016/j.comcom.2023.11.014},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {296-308},
  shortjournal = {Comput. Commun.},
  title        = {NextGenV2V: Authenticated V2V communication for next generation vehicular network using (2, n)-threshold scheme},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Service recommendation based on contrastive learning and
multi-task learning. <em>COMCOM</em>, <em>213</em>, 285–295. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service recommendation is an efficient method for service-oriented software that can improve software quality. Applications often require the integration of multiple services to create more powerful and complex functionality while saving software development time. However, the vast number of available candidate Web services can impose a heavy burden on software developers’ selection decisions. The existing service recommendation challenges are mainly come from: (1) the development requirements entered by users are too arbitrary (2) the extreme sparsity of invocation records. To address the above challenges, in this paper, we propose a Service Recommendation method based on Contrastive Learning and Multi-task Learning (SRCLML). Specifically, we utilize the Transformer model to extract the development requirements of users, conduct in-depth mining of text descriptions, and extract features of applications. Next, the features are fed into the DNN model to predict the probability that the service will be selected. Moreover, we add a tag judgment task to make it capable of multi-task learning, through which, the training signal information implied can be used as an inductive bias to improve service recommendation capabilities. Additionally, we build three subgraphs based on the global graph, conduct in-depth mining of historical invocation records based on contrastive learning and graph neural network to extract features of applications and services and calculate application preferences for each service. Finally, we combined the above two to obtain the final recommendation service list. Extensive experiments on real-world datasets demonstrate that our method, SRCLML, outperforms several state-of-the-art comparison methods in the domain of service recommendation.},
  archive      = {J_COMCOM},
  author       = {Ting Yu and Lihua Zhang and Hailin Liu and Hongbing Liu and JiaoJiao Wang},
  doi          = {10.1016/j.comcom.2023.11.018},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {285-295},
  shortjournal = {Comput. Commun.},
  title        = {Service recommendation based on contrastive learning and multi-task learning},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OSF-EIMTC: An open-source framework for standardized
encrypted internet traffic classification. <em>COMCOM</em>,
<em>213</em>, 271–284. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet traffic classification plays a key role in network visibility, Quality of Services (QoS), intrusion detection , Quality of Experience (QoE) and traffic-trend analyses. In order to improve privacy, integrity, confidentiality, and protocol obfuscation, the current traffic is based on encryption protocols, e.g., SSL/TLS. With the increased use of Machine-Learning (ML) and Deep-Learning (DL) models in the literature, comparison between different models and methods has become cumbersome and difficult due to a lack of a standardized framework. In this paper, we propose an open-source framework, named OSF-EIMTC, which can provide the full pipeline of the learning process and simulation reproducibility. From well-known datasets to extracting new and well-known features, it provides implementations of well-known ML and DL models (from the traffic classification literature) as well as experimental test-beds and their evaluation. By providing a standardized platform, OSF-EIMTC enables repeatable, reproducible, and accurate comparisons of both established and novel features and models. As part of our framework evaluation, we demonstrate the reproducibility of a variety of cases where the framework can be of use, utilizing multiple datasets, models, and feature sets. We show analyses of publicly available datasets and invite the community to participate in our open challenges using OSF-EIMTC, fostering collaborative advancements in encrypted traffic classification.},
  archive      = {J_COMCOM},
  author       = {Ofek Bader and Adi Lichy and Amit Dvir and Ran Dubin and Chen Hajaj},
  doi          = {10.1016/j.comcom.2023.10.011},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {271-284},
  shortjournal = {Comput. Commun.},
  title        = {OSF-EIMTC: An open-source framework for standardized encrypted internet traffic classification},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous temporal graph powered DRL algorithm for
channel allocation in maritime IoT systems. <em>COMCOM</em>,
<em>213</em>, 260–270. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In actual maritime Internet of Things systems, the communication environment is characterized by its time-varying nature and the presence of highly heterogeneous network structures. Those attributes present considerable challenges in devising resource allocation strategies. Given the limited availability of frequency resources, designing a reasonable and flexible channel allocation strategy (CAS) is the primary task for meeting diverse and dynamic communication demands. In this paper, a heterogeneous temporal graph powered deep reinforcement learning algorithm is proposed to optimize the CAS to maximize the channel efficiency in a real-world maritime Internet of Things system. Specifically, we build relation-based heterogeneous edges to connect different types of terminal nodes and adopt a time encoding technology to capture the dynamic evolution of communication scenarios over time. The memory and public mailbox modules are constructed as the implementation entities of the information aggregation method based on the attention mechanism . In addition, we develop a corresponding heterogeneous temporal neural network to estimate the real-time resource requirements of target terminals, and subsequently learn the optimal CAS based on the deep reinforcement learning algorithm from the perspective of maximizing the cumulative channel efficiency. Simulation results prove that the proposed algorithm significantly outperforms the other state-of-the-art algorithms in terms of the channel efficiency and generalization ability .},
  archive      = {J_COMCOM},
  author       = {Zongwang Li and Zhuochen Xie and Xiaohe He and Xuwen Liang},
  doi          = {10.1016/j.comcom.2023.11.005},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {260-270},
  shortjournal = {Comput. Commun.},
  title        = {Heterogeneous temporal graph powered DRL algorithm for channel allocation in maritime IoT systems},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Light-PerIChain: Using lightweight scalable blockchain based
on node performance and improved consensus algorithm in IoT systems.
<em>COMCOM</em>, <em>213</em>, 246–259. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the industry and the increase of smart devices in recent years, the Internet of Things has expanded as one of the solutions to meet human needs in smart societies. However, this emerging technology suffers from security vulnerabilities . In recent years, blockchain has attracted much attention due to its decentralization. However, blockchain is computationally expensive, with limited scalability and high overhead costs. Also, designing a secure and robust blockchain for use in the Internet of Things is challenging. In this article, we provide solutions to improve the sharding method to achieve a lightweight and scalable blockchain and enhance the efficiency of the Internet of Things. To assign nodes to shards in the first round, we use the Verifiable Random Function (VRF), and after a few rounds, nodes are assigned to shards according to their point and performance in the network. We use the IBFT consensus algorithm to process the transaction in shards. Unlike other consensus algorithms based on Byzantine fault tolerance , this algorithm requires 2 f + 1 2f+1 replicas instead of 3 f + 1 3f+1 for fault tolerance. Also, in this method, the team leader’s shard is considered, so nodes with the highest performance assign to this shard. If the malicious behavior of the team leader is detected, one of the nodes in the team leader’s shard is replaced by it. For cross-transaction processing, we present an improved two-phase commit method that solves many problems with existing two-phase commit methods. Finally, we analyze our design for efficiency and security. Our qualitative arguments show that this design is resistant to several security attacks and that the scalability and throughput of IoT are increased compared to previous designs.},
  archive      = {J_COMCOM},
  author       = {Fateme Fathi and Mina Baghani and Majid Bayat},
  doi          = {10.1016/j.comcom.2023.11.011},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {246-259},
  shortjournal = {Comput. Commun.},
  title        = {Light-PerIChain: Using lightweight scalable blockchain based on node performance and improved consensus algorithm in IoT systems},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource management in multi-heterogeneous cluster networks
using intelligent intra-clustered federated learning. <em>COMCOM</em>,
<em>213</em>, 236–245. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneous cluster networks (HCN) have recently benefited from federated learning (FL). On distributed data, FL is used to train privacy-preserving models. In heterogeneous networks (HetNet) and the Internet of Things (IoT), FL implementation is challenged by resource optimization, robustness, and security issues. There is a significant risk of data security being compromised by disregarding the nodes’ clustering behavior and quickly varying asynchronous streaming data. Moreover, in HCN-based wireless sensor networks (WSNs), FL enhances asynchronous node performance. Using naturally clustered HCN, distributed nodes train a local and global model collectively. In this paper, we propose a Intra-Clustered FL (ICFL) model. By optimizing computation and communication, ICFL selects heterogeneous FL nodes in each cluster. Despite heterogeneous data , it is highly robust. There are currently no FL frameworks that can handle varying data quality across devices and non-identical distributions. With ICFL, sensitive asynchronous data is not exposed to possible misuse while adapting to changing environments. In addition to being time-efficient, our strategy requires low-power computing nodes. According to our extensive simulation results, ICFL performs better than FedCH in terms of computational performance and provides flexible conditions under which ICFL is more efficient in terms of communication. In extensive testing, ICFL decreased training rounds by 62% and increased accuracy by 6.5%. It can execute evaluations 7.46 times faster than existing models, and its average accuracy has increased by 4.39%. A resource-aware FL system can be successfully implemented in real-time applications according to our research.},
  archive      = {J_COMCOM},
  author       = {Fahad Razaque Mughal and Jingsha He and Nafei Zhu and Saqib Hussain and Zulfiqar Ali Zardari and Ghulam Ali Mallah and Md. Jalil Piran and Fayaz Ali Dharejo},
  doi          = {10.1016/j.comcom.2023.10.026},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {236-245},
  shortjournal = {Comput. Commun.},
  title        = {Resource management in multi-heterogeneous cluster networks using intelligent intra-clustered federated learning},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QoS aware resource allocation for coexistence mechanisms
between eMBB and URLLC: Issues, challenges, and future directions in 5G.
<em>COMCOM</em>, <em>213</em>, 208–235. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {5G NR enables three types of use case scenarios viz. enhance mobile broadband (eMBB), ultra-reliable low latency communication (URLLC), and massive machine type communication (mMTC). The eMBB is suitable for applications that demand higher throughput. Whereas URLLC is suitable for mission-critical applications with stringent requirement of low latency and reliability. The mMTC on the other hand is suitable for machine-to-machine (M2M) communications and massive IoT infrastructures. To meet all these requirements, 5G NR combines eMBB with URLLC services under a unified 5G air interface framework. Most of the coexistence mechanisms between eMBB and URLLC were presented by the researchers with a goal to enhance eMBB throughput with stringent latency and reliability requirements of URLLC services. Formulation of optimal resource scheduling and allocation were found to be the key problems of eMBB and URLLC traffic. This paper investigated the 5G state-of-the-art focused on coexistence mechanisms between eMBB and URLLC traffic for resource scheduling. We followed the PRISMA statement and classified the works to five major classes viz. multiplexing-, QoS-, Machine learning-, Network slicing-, and C-RAN architecture-based approaches. Each work was carefully examined against their methodology and performance metrics. In addition, several key issues, challenges, and future directions were also highlighted to provide detailed insights for researchers working in the field of 5G.},
  archive      = {J_COMCOM},
  author       = {Rajesh Kumar and Deepak Sinwar and Vijander Singh},
  doi          = {10.1016/j.comcom.2023.10.024},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {208-235},
  shortjournal = {Comput. Commun.},
  title        = {QoS aware resource allocation for coexistence mechanisms between eMBB and URLLC: Issues, challenges, and future directions in 5G},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy conserving cost selection for fine-grained
computational offloading in mobile edge computing networks.
<em>COMCOM</em>, <em>213</em>, 199–207. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing networks, total or partial computational offloading of delay sensitive and compute intensive applications to the nearby edge servers is a promising solution to reduce transmission delay and energy consumption of mobile nodes (MNs). However, in computational offloading of an application to edge server, considerations of energy consumption and time delay are not sufficient for the decision of minimum cost of application. In this article, a novel battery dependent cost selection strategy for computational offloading of an application capable for sequential execution of dependent sub tasks is proposed. The strategy considers current battery level of MN in the cost function for optimal selection of offloading policy based on the amount of data, computational resources , and radio resources of the server. Our comprehensive cost function takes into account all the said parameters, calculates all the costs for possible offloading policies, and chooses the policy with minimum cost from a huge dataset. The cost selection is energy efficient for low battery level and prioritizes faster execution for full battery level of MNs. A deep neural network is trained on the generated dataset to minimize the overhead of computations. Simulation results reveal that our proposed strategy outperforms the benchmark strategies in terms of deep neural network accuracy for different sizes of dataset, energy consumption, time delay, and cost for different sizes of applications.},
  archive      = {J_COMCOM},
  author       = {Abdullah Numani and Ziaul Haq Abbas and Ghulam Abbas and Zaiwar Ali},
  doi          = {10.1016/j.comcom.2023.11.012},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {199-207},
  shortjournal = {Comput. Commun.},
  title        = {Energy conserving cost selection for fine-grained computational offloading in mobile edge computing networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DINNRS: A distributed in-network name resolution system for
information-centric networks. <em>COMCOM</em>, <em>213</em>, 188–198.
(<a href="https://doi.org/10.1016/j.comcom.2023.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information-Centric Networking (ICN) is a promising network paradigm designed to address the challenges arising from the inherent ambiguity of IP addresses. ICN prioritizes the content itself as the main factor in network transmission . The Name Resolution System (NRS) is the primary component of the ICN architecture, relying on flat naming. Unfortunately, existing NRS frequently encounters scalability and performance issues. In this paper, we present the Distributed In-Network Name Resolution System (DINNRS), an NRS that leverages the software-defined networking and ICN paradigm. DINNRS offers global-scale name-based routing with high scalability and minimal request delay. Additionally, we propose a modular control domain architecture for easy implementation and an enhanced marked cuckoo filter for fast resolving. Simulation experiments using the ICN emulator demonstrate that our methods result in significant performance gains, with a 61.2% reduction in data acquire latency and a 55.2% lower link load control overhead compared to MDHT.},
  archive      = {J_COMCOM},
  author       = {Zhaolin Ma and Jiali You and Haojiang Deng},
  doi          = {10.1016/j.comcom.2023.11.008},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {188-198},
  shortjournal = {Comput. Commun.},
  title        = {DINNRS: A distributed in-network name resolution system for information-centric networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accuracy enhancement of wi-fi positioning by ranging in
dense UWB networks. <em>COMCOM</em>, <em>213</em>, 179–187. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposed a fusion-based approach of Wi-Fi fingerprint positioning with relatively accurate UWB ranging in dense wireless networks (DWNs) to optimize indoor localization accuracy . Such an optimization was achieved through a proposed objective or “loss” function that minimizes the total positioning errors of the nodes composing a virtual geometric structure with the internode distances preserved during optimization in the DWN. Theoretical analysis and simulation results of DWNs up to 100 nodes revealed that, with at least 6 extra measured distances among as few as 4 participating nodes including the device-under-targeting (DUT) itself, the resulting localization accuracy improvement was proportional to the square root of the number of participating nodes towards the limit of the ranging error. Per experiment outcomes of a 10-node UWB DWN, our Wi-Fi/UWB ensemble reduced the baseline Wi-Fi K-nearest neighbors (KNN) fingerprint localization error by up to 69.8 %, further validating our theoretical and simulation results.},
  archive      = {J_COMCOM},
  author       = {Chao-Hsiang Li and Alexander I-Chi Lai and Ruey-Beei Wu},
  doi          = {10.1016/j.comcom.2023.10.015},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {179-187},
  shortjournal = {Comput. Commun.},
  title        = {Accuracy enhancement of wi-fi positioning by ranging in dense UWB networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Standardization and technology trends of artificial
intelligence for mobile systems. <em>COMCOM</em>, <em>213</em>, 169–178.
(<a href="https://doi.org/10.1016/j.comcom.2023.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, the applications of AI/ML techniques are so pervasive and numerous that they cannot be easily listed. However, some of the most successful and well-known applications include search engines, targeted advertisements, recommendation systems, autonomous vehicles, language translation , image recognition, and large language models . Although introduced more recently, AI/ML applications for mobile systems have already shown potential in improving performance and user experience , automating network management and optimization, reducing overhead and cost, among others. This paper looks into the standardization activities in 3GPP and O-RAN for AI/ML applications, and future technology trends of AI/ML. Though still in its early stages, 3GPP has started to incorporate intelligence into the core network, RAN , and the air-interface. Meanwhile, O-RAN has introduced SMO and RIC for RAN automation and intelligence. Even for the air-interface, where near real-time operation is required, 3GPP is exploring AI/ML use cases in areas such as beam management, CSI feedback enhancements, and positioning, to improve performance and reduce overhead. Federated learning , E2E learning, and explainable AI/ML are emerging as important technology trends in the field of AI/ML for the future. When these future AI/ML technologies are integrated with mobile systems, it is expected that a series of cross-layer protocol stacks for communication could be replaced by an appropriate AI/ML model that can provide more automated, reliable, and accurate inference and actions compared to traditional methods.},
  archive      = {J_COMCOM},
  author       = {Choongil Yeh and Yong-Seouk Choi and Young-Jo Ko and Il-Gyu Kim},
  doi          = {10.1016/j.comcom.2023.11.004},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {169-178},
  shortjournal = {Comput. Commun.},
  title        = {Standardization and technology trends of artificial intelligence for mobile systems},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Key node identification in social networks based on
topological potential model. <em>COMCOM</em>, <em>213</em>, 158–168. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social networks, identifying key nodes in social networks plays a crucial role in preventing and controlling the spread of information. However, the current methods for identifying key nodes have problems such as inaccurate measurement of node value, neglect of mining hidden topology information between nodes, and the calculation of node influence range is not accurate enough etc., and results in low accuracy in identifying key nodes. Aiming at these issues of key node identification, this paper proposes a node influence measurement method based on topological potential model. This method uses information entropy to measure the value of nodes, proposes to apply effective distance to express the relationship between nodes, and evaluates the influence range of each node with the effective distance between the node and its farthest node. Finally, topological potential model is used to measure the importance of nodes, thus improving the accuracy of key node identification. Experiments on multiple datasets show that the proposed method has a better ability to identify key nodes than the compared methods.},
  archive      = {J_COMCOM},
  author       = {Xueqin Zhang and Zhineng Wang and Gang Liu and Yan Wang},
  doi          = {10.1016/j.comcom.2023.11.003},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {158-168},
  shortjournal = {Comput. Commun.},
  title        = {Key node identification in social networks based on topological potential model},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complexity and algorithm of setting optimal location for
data sink in real-time NOMA-based IIoTs. <em>COMCOM</em>, <em>213</em>,
147–157. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time performance is one of the most vital metrics in Non-Orthogonal Multiple Access (NOMA) based Industrial Internet of Things (IIoTs) applications. Since the relative geographic relationship between data sink and wireless sensors affects the transmission parallelism and thus the real-time performance, setting a reasonable location for the data sink is thus a feasible way to high real-time performance for applications where the locations of wireless sensors are fixed. In this paper, we consider how to find an optimal location for the data sink to minimize average access delay. We first formulate the problem and prove it to be NP-Complete by presenting an original reduction proof from classic set partition problem. Second, by tightening the NOMA decoding constraint of the original problem, a heuristic algorithm , which is designed based on Apollonian Circle Theorem and Dilworth Theorem, obtains a feasible location for the data sink. Simulation results reveal that the real-time performance loss of the heuristic algorithm is no more than 50% of the best one. Compared with the classic TDMA scheme, the real-time performance increases by more than 60% for some typical settings, and it can even reach 70% for the linear network topology , owing to the full exploitation of NOMA parallelism .},
  archive      = {J_COMCOM},
  author       = {Chaonong Xu and Yaqi Xie and Fan Wu and Chao Li},
  doi          = {10.1016/j.comcom.2023.10.023},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {147-157},
  shortjournal = {Comput. Commun.},
  title        = {Complexity and algorithm of setting optimal location for data sink in real-time NOMA-based IIoTs},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning for radio resource management of
hybrid energy cellular networks with battery constraints.
<em>COMCOM</em>, <em>213</em>, 135–146. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular networks are facing serious economic and ecological challenges due to the exponential increase in mobile traffic. As a promising direction, mobile operators are equipping base stations with renewable energy and battery systems along with energy efficiency techniques. In this paper, we study cellular networks equipped with batteries and powered by renewable energy sources and the Smart Grid. We exploit reinforcement learning to minimize the grid energy cost and maximize the users’ satisfaction considering variable price of grid energy, traffic variation and renewable energy generation . In contrast to existing studies, we take into consideration both heterogeneity of users and degradation of battery. We propose a Q-learning algorithm that decides the best number of active radio resources considering two cases: with and without battery constraints. Simulation results highlight the importance of imposing constraints on the battery operation. When the battery size is large enough, the battery life is extended with negligible degradation in system performance . In addition, while imposing constraints on the battery may lead to performance degradation on the short term, this is compensated on the long term as shown by simulating the system over one year.},
  archive      = {J_COMCOM},
  author       = {Hussein Al Haj Hassan and Sahar Jaber and Ali El Amine and Abbass Nasser and Loutfi Nuaymi},
  doi          = {10.1016/j.comcom.2023.10.025},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {135-146},
  shortjournal = {Comput. Commun.},
  title        = {Reinforcement learning for radio resource management of hybrid energy cellular networks with battery constraints},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Role of context in determining transfer of risk knowledge in
roundabouts. <em>COMCOM</em>, <em>213</em>, 111–134. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to predict the risk patterns of vehicles in a roundabout shows potential to improve the safety and efficiency of roundabout crossings by connected vehicles. Namely, exit patterns are relevant for vehicles seeking to enter a roundabout in the presence of incoming vehicles. Entering vehicles may take educated decisions on whether to enter the roundabout based on the likelihood of the incoming vehicles not to exit and cause a merging conflict. In previous work, a machine learning model was trained to assess the probability of a vehicle to exit a roundabout based on its observed position relatively to the next exit. Yet, the transferability of the knowledge of exit probability models was not investigated, i.e., whether the knowledge of an existing exit probability model can be accurately transferred in unseen roundabouts, both for model usage and training. In this paper, we compute a metric of similarity of exit probability models trained from eight real roundabouts. In turn, we identify the contextual features of two roundabouts which impact the similarity of their resulting exit probability models, and define three levels of context similarity, i.e., strict, moderate, and low. Lastly, significant accuracy improvements are obtained by constraining the knowledge transfer of exit probability models to roundabouts which feature a similar context. On the one hand, applying exit probability models on distinct roundabouts with a moderately similar context yielded an average accuracy of 80 . 4 ± 4 . 6 % 80.4±4.6% , which is equivalent to the most accurate non-similar models. On the other hand, training a model for an unseen roundabout using exclusively training data extracted from roundabouts with a moderately similar context featured a 80 ± 5 % 80±5% accuracy, which represents a consistent accuracy increase of 8 . 5 ± 4 . 8 % 8.5±4.8% compared with knowledge transfer without context constraints.},
  archive      = {J_COMCOM},
  author       = {Duncan Deveaux and Takamasa Higuchi and Seyhan Uçar and Jérôme Härri and Onur Altintas},
  doi          = {10.1016/j.comcom.2023.10.016},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {111-134},
  shortjournal = {Comput. Commun.},
  title        = {Role of context in determining transfer of risk knowledge in roundabouts},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coalitional game based sub-channel allocation for
full-duplex-enabled mmWave IAB network in B5G. <em>COMCOM</em>,
<em>213</em>, 99–110. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of full-duplex communication (FDC) and millimeter wave integrated access and backhaul (mmWave IAB) is a highly implementable technique in beyond 5G era. However, the technical challenges such as sub-channel allocation and interference coordination must be effectively handled before the attractive benefits of the commercial implementation are enjoyed. In particular, the multiple user interference (MUI) is caused when the same sub-channel is shared by the concurrent links sourced from different transmitters, and the residual self-interference (RSI) is resulted from self-interference cancellation (SIC) in FDC. To this end, a coalitional game based sub-channel allocation mechanism is proposed for the full-duplex-enabled mmWave IAB network to maximize the total actual achievable rate . Firstly, the maximization problem of the sum actual achievable rate is formulated as an integer non-convex and non-linear optimization problem under the constriction of MUI and RSI, which is a non-polynomial hard (NP-Hard) problem, and it is intractable to obtain the optimal solution. Secondly, the formulated problem is transformed as a cooperative game, and a coalitional game based sub-channel allocation (CGSA) algorithm with lower computational complexity is proposed to obtain the sub-optimal solution. Thirdly, the properties of the proposed CGSA algorithm are discussed and analyzed from the aspects of stability and convergence, and it is proved that the final coalition partition formed by the proposed CGSA algorithm is Nash-stable. Fourthly, the proposed CGSA algorithm is compared with the reference algorithms under different scenarios, extensive simulations show that superiorities of the proposed CGSA algorithm are obvious including sum of achievable rate, number-ratio of user equipments with satisfied quality-of-service (QoS)requirement and iterations of algorithm convergence.},
  archive      = {J_COMCOM},
  author       = {Zhongyu Ma and Zijun Wang and Yajing Wang and Xianghong Lin and Qun Guo and Yi Xie},
  doi          = {10.1016/j.comcom.2023.10.027},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {99-110},
  shortjournal = {Comput. Commun.},
  title        = {Coalitional game based sub-channel allocation for full-duplex-enabled mmWave IAB network in B5G},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning based trajectory design and
resource allocation for task-aware multi-UAV enabled MEC networks.
<em>COMCOM</em>, <em>213</em>, 88–98. (<a
href="https://doi.org/10.1016/j.comcom.2023.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing tasks in the air is an important form of mobile edge computing (MEC) to improve the quality of service and enhance network coverage. In this paper, we investigate a multi-UAV cooperative computing model and massive devices access scenario in a service area without infrastructure. There are various types of ground devices with different tasks. Moreover, we consider that the UAV executing tasks of devices need to cache the content that task required. Therefore, we propose a multi-UAV enabled MEC network based on task awareness where each UAV caches some programs to execute tasks offloaded from devices. To minimize completion time, a joint UAV trajectory design, access decision and resource allocation problem is formulated. To address this intractable mixed integer non-linear programming problem, a multi-agent trajectory design and resource allocation (MATR) algorithm is proposed, where the multi-agent deep deterministic policy gradient (MADDPG) is applied. Considering the complexity of high-dimensional continuous action space, we introduce the particle swarm optimization (PSO) algorithm to jointly optimize access decisions, and computation resource allocation to reduce action space. In addition, we discuss the impact of the size of UAV cache space and the location of ground devices on the completion time. Simulation results show that the MATR algorithm can significantly reduce the completion time compared to the baselines.},
  archive      = {J_COMCOM},
  author       = {Zewu Li and Chen Xu and Zhanpeng Zhang and Runze Wu},
  doi          = {10.1016/j.comcom.2023.11.006},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {88-98},
  shortjournal = {Comput. Commun.},
  title        = {Deep reinforcement learning based trajectory design and resource allocation for task-aware multi-UAV enabled MEC networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cellular fault prediction of graphical representation based
on spatio-temporal graph convolutional networks. <em>COMCOM</em>,
<em>213</em>, 78–87. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the 5th generation (5G) networks and the massive amount of data, the operation and maintenance (O&amp;M) mindset needs to change from “post-maintenance” to “pre-prevention”. In the wireless network, the fault has propagation and causality, and a fault in the current timestamp is also one of the factors that affect whether a fault occurs in the subsequent timestamp. Existing fault prediction methods study the time dependencies and inter-measure dependencies of key performance indicators (KPIs) while ignoring the important causal dependencies of faults in cellular networks. Therefore, it is still challenging to study the fault causal dependency in cellular networks. To tackle the above problems, we propose a novel framework for wireless cell fault prediction to deal with the problem of causal dependency. First, we build an undirected graph based on KPIs and fault-related knowledge collected from the base station through graphical representation. Second, we introduce the graphical model to learn the dependency relationship containing different KPIs at synchronous timestamps and the causal relationship between fault codes at asynchronous timestamps. Finally, we employed the attention mechanism of the graphical model to further strengthen the correlation between parameters during the training process. We conduct extensive prediction experiments on fault events (whether a fault occurs) and fault code (which type of fault occurs) tasks on fault datasets of real wireless cells. Experimental results show that our framing method is state-of-the-art and achieves higher accuracy than traditional fault prediction methods.},
  archive      = {J_COMCOM},
  author       = {Bing Qian and Hanlei Xie and Wei Wu and Yan Yang},
  doi          = {10.1016/j.comcom.2023.10.021},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {78-87},
  shortjournal = {Comput. Commun.},
  title        = {Cellular fault prediction of graphical representation based on spatio-temporal graph convolutional networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of machine learning (ML)-based IoT security in
healthcare: A dataset perspective. <em>COMCOM</em>, <em>213</em>, 61–77.
(<a href="https://doi.org/10.1016/j.comcom.2023.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is transforming society by connecting businesses and optimizing systems across industries. Its impact has been felt in healthcare, where it has the potential to revolutionize medical treatment. Conversely, healthcare systems are targeted by attackers and security threats. Malicious activities against such systems intend to compromise privacy and acquire control over internal procedures. In this regard, advanced analytics can enhance these attacks’ detection, mitigation, and prevention and improve overall IoT security. However, the process of producing realistic datasets is complex. There are critical aspects to consider when developing models that can be directly deployed in real environments (e.g., multiple devices, features, and realistic testbed). Thereupon, the main goal of this research is to conduct a review of Machine Learning (ML) solutions for IoT security in healthcare. Furthermore, this review is conducted from a dataset standpoint, focusing on existing datasets, resources, applications, and open challenges. Our primary objective is to highlight the current landscape of datasets for IoT security in healthcare and the immediate requirements for future datasets to support the development of novel approaches.},
  archive      = {J_COMCOM},
  author       = {Euclides Carlos Pinto Neto and Sajjad Dadkhah and Somayeh Sadeghi and Heather Molyneaux and Ali A. Ghorbani},
  doi          = {10.1016/j.comcom.2023.11.002},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {61-77},
  shortjournal = {Comput. Commun.},
  title        = {A review of machine learning (ML)-based IoT security in healthcare: A dataset perspective},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic WiFi indoor positioning based on the multi-scale
metric learning. <em>COMCOM</em>, <em>213</em>, 49–60. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, most of the fingerprint-based positioning methods require the target to stay static in the process of data collection, which are not applicable to the target positioning in a moving state. Common dynamic localization and tracking algorithms such as Kalman Filter (KF) and Particle Filter (PF) need to estimate the position at the current time according to the estimated position at the previous time. If there is a positioning error at a certain time, the following positioning performance will be affected by the cumulative errors. To solve these problems, this paper proposes a dynamic indoor positioning method based on the multi-scale metric learning of the channel state information (CSI), named as CSI-MML. This method can realize dynamic positioning using CSI signals without carrying extra equipment. The proposed model of CSI-MML is trained and tested by constructing few-shot learning tasks, which is mainly composed of two parts: feature extraction and similarity metric. In order to fully extract the effective features of the samples, the attention mechanism is added to the feature extraction module of the network to enhance the model’s ability of extracting significant features. The similarity metric module measures the global similarity and local similarity between samples and fuses the two similarities by a similarity fusion layer. Through the multi-scale metric, we can improve the metric ability of CSI-MML, so as to improve the positioning accuracy of dynamic positioning. The experimental results show that compared with the commonly used dynamic location and tracking algorithms such as KF and PF, the positioning error of the proposed method will not accumulate and the average positioning error is smaller.},
  archive      = {J_COMCOM},
  author       = {Yujie Wang and Ying Wang and Qingqing Liu and Yong Zhang},
  doi          = {10.1016/j.comcom.2023.10.022},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {49-60},
  shortjournal = {Comput. Commun.},
  title        = {Dynamic WiFi indoor positioning based on the multi-scale metric learning},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enabling simulation services for digital twins of 5G/B5G
mobile networks. <em>COMCOM</em>, <em>213</em>, 33–48. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital Twins (DTs) have been proposed as digital replicas of physical entities (e.g., manufacturing equipment), which one can observe in real-time and interact with. Digital Twins of Networks (DTNs) are increasingly being discussed in the literature, as an enabler for efficient data-driven network management and performance-driven network optimization (e.g., to support dynamic reconfiguration , or anticipate the effects of faults). A DTN includes service mapping models , i.e. models that can be fed with acquired data to produce insight on the network itself - e.g., to run what-if scenarios, based on multiple underlying technologies, from Machine Learning to analytical models, e.g. Markov Chains . In this paper we examine the case of DTNs of mobile networks , DTMNs, tailored to 5G and beyond, where issues of dynamic reconfiguration and fault anticipation are critical. We argue that simulation services should be offered by the DTMN in order to allow performance-driven network optimization, and that discrete-event network simulators are ideal instruments to be employed for this purpose. We discuss the challenges that need be addressed to make this happen, e.g., centralized vs. distributed implementation, gathering input from the physical network, security issues and hosting, and we review the possibilities offered by network simulation in terms of what-if analysis, defining the concepts of lockstep and branching analysis . We present a framework to endow a DTMN with simulation services and we exemplify it using Simu5G, a popular 5G/B5G simulation library for OMNeT++, as a reference case study .},
  archive      = {J_COMCOM},
  author       = {Giovanni Nardini and Giovanni Stea},
  doi          = {10.1016/j.comcom.2023.10.017},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {33-48},
  shortjournal = {Comput. Commun.},
  title        = {Enabling simulation services for digital twins of 5G/B5G mobile networks},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A many-objective evolutionary algorithm for solving
computation offloading problems under uncertain communication
conditions. <em>COMCOM</em>, <em>213</em>, 22–32. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computation offloading is a critically important technology in the field of edge computing , enabling improved conditions for task computation. Existing studies on computation offloading largely focus on cooperative offloading and resource allocation . The impact of factors such as uncertainty during task transmission and the different task characteristics on the offloading location is ignored. To address the above issues, this paper designs a many-objective optimization computation offloading model (MaOCO) under uncertainty to cope with offloading requirements. The model formulates channel transmission level strategies and considers five objectives: latency, cost, energy consumption, load balancing, and user satisfaction. A many-objective evolutionary algorithm with deviation selection (MaOEA-DS) is designed to obtain effective offloading strategies. In the environment selection, the deviation is calculated using the concept of variance to choose solutions closer to the origin for the last layer, which further improves the convergence of the algorithm. The simulation results show that MaOEA-DS outperforms other evolutionary algorithms in IGD , GD , SP , and HV .},
  archive      = {J_COMCOM},
  author       = {Qi Li and Zhenyu Shi and Zhaoyu Xue and Zhihua Cui and Yubin Xu},
  doi          = {10.1016/j.comcom.2023.10.020},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {22-32},
  shortjournal = {Comput. Commun.},
  title        = {A many-objective evolutionary algorithm for solving computation offloading problems under uncertain communication conditions},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum to “state-of-the-art solutions of blockchain
technology for data dissemination in smart cities: A comprehensive
review” [comput. Commun. 189 (2022) 120–147]. <em>COMCOM</em>,
<em>213</em>, 21. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_COMCOM},
  author       = {Nur Fadhilah Mohd Shari and Amizah Malip},
  doi          = {10.1016/j.comcom.2023.10.005},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {21},
  shortjournal = {Comput. Commun.},
  title        = {Corrigendum to ‘State-of-the-art solutions of blockchain technology for data dissemination in smart cities: A comprehensive review’ [Comput. commun. 189 (2022) 120–147]},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Route planning model based on multidimensional eigenvector
processing in vehicular fog computing. <em>COMCOM</em>, <em>213</em>,
13–20. (<a href="https://doi.org/10.1016/j.comcom.2023.10.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of information technology, the informationization level of sport tourism has been improved in an all-around way, which makes a large number of data accumulated in the management system of road traffic. However, the traditional association rule algorithm cannot deal with huge data. To make up for the deficiency of predicting suitability from a single perspective, this paper, from the perspective of road daily travel planning, constructs a multi-dimensional characteristic model. Based on eigenvector processing as a research content, by carrying out a multi-dimensional prediction on the whole data set and considering the limitation problem of the influence of time, space, and items on daily travel routes, the invention provides a daily travel suitability road daily travel planning route prediction research method based on multiple dimensions. The invention also extracts the outstanding performance of a network model in a single variable problem based on deep interest and double characteristics. The combination of SVR and GBRT algorithm makes up for the one-sidedness of single perspective prediction. It uses the weighted fusion principle to fuse the results and establishes a multi-dimensional route suitability prediction model under this mode. Experiments verify that the prediction of multivariate dimensional data achieves the desired results. With the help of the dependence between the level and element dimension data, the future route planning trend can be judged. This algorithm compared to the ant colony algorithm and the traditional genetic algorithm increased by 15.6% and 15.1%. The system response time has been increased by more than 60%, which can effectively improve the accuracy of prediction. Therefore, in the VFC environment, the actual user needs can be improved, the planning and management of traffic routes can be guided, and the development of sport tourism systems of Taihang Mountain can be further promoted.},
  archive      = {J_COMCOM},
  author       = {Yakun Gao and Keren Ji and Tian Gao},
  doi          = {10.1016/j.comcom.2023.10.019},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {13-20},
  shortjournal = {Comput. Commun.},
  title        = {Route planning model based on multidimensional eigenvector processing in vehicular fog computing},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cloud native lightweight slice orchestration (CLiSO)
framework. <em>COMCOM</em>, <em>213</em>, 1–12. (<a
href="https://doi.org/10.1016/j.comcom.2023.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-native network functions are becoming promising for 5G and beyond networks. They provide the much needed agility and flexibility that was missing from virtual machines. Though, this paradigm shift of using cloud-native application design principles, containerization, microservices , high resilience, and on-demand scaling has created challenges for legacy orchestration systems . They were designed for handling virtual machine-based network functions. Indeed, network slice orchestration requires interaction with multiple technological domain orchestrators, access, transport, core network, and edge computing . The specifications and existing orchestrators are made on top of the legacy virtual machine based network function orchestration. Hence, this limitation constrains their approach to managing a cloud-native network function. To overcome their challenges, we propose a novel Cloud-native Lightweight Slice Orchestration (CLiSO) framework extending our previously proposed Lightweight edge Slice Orchestration (LeSO) framework. In addition, we present a technology-agnostic and deployment-oriented network slice template. To allow zero-touch management of network slices, our framework provides a concept of Domain Specific Handlers. The framework has been thoroughly evaluated via orchestrating OpenAirInterface container network functions on public and private cloud platforms.},
  archive      = {J_COMCOM},
  author       = {Sagar Arora and Adlen Ksentini and Christian Bonnet},
  doi          = {10.1016/j.comcom.2023.10.010},
  journal      = {Computer Communications},
  month        = {1},
  pages        = {1-12},
  shortjournal = {Comput. Commun.},
  title        = {Cloud native lightweight slice orchestration (CLiSO) framework},
  volume       = {213},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
