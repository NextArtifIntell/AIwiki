<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ICV_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="icv---362">ICV - 362</h2>
<ul>
<li><details>
<summary>
(2024). Camouflaged object detection via location-awareness and
feature fusion. <em>ICV</em>, <em>152</em>, 105339. (<a
href="https://doi.org/10.1016/j.imavis.2024.105339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection aims to completely segment objects immersed in their surroundings from the background. However, existing deep learning methods often suffer from the following shortcomings: (1) They have difficulty in accurately perceiving the target location; (2) The extraction of multi-scale feature is insufficient. To address the above problems, we proposed a camouflaged object detection network(LFNet) based on location-awareness and feature fusion. Specifically, we designed a status location module(SLM) that dynamically captures the structural features of targets across spatial and channel dimensions to achieve accurate segmentation. Beyond that, a residual feature fusion module(RFFM) was devised to address the challenge of insufficient multi-scale feature integration. Experiments conducted on three standard datasets(CAMO,COD10K and NC4K) demonstrate that LFNet achieves significant improvements compared with 15 state-of-the-art methods. The code will be available at https://github.com/ZX123445/LFNet .},
  archive      = {J_ICV},
  author       = {Yanliang Ge and Yuxi Zhong and Junchao Ren and Min He and Hongbo Bi and Qiao Zhang},
  doi          = {10.1016/j.imavis.2024.105339},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105339},
  shortjournal = {Image Vis. Comput.},
  title        = {Camouflaged object detection via location-awareness and feature fusion},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning applications in breast cancer prediction
using mammography. <em>ICV</em>, <em>152</em>, 105338. (<a
href="https://doi.org/10.1016/j.imavis.2024.105338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the second leading cause of cancer-related deaths among women. Early detection of lumps and subsequent risk assessment significantly improves prognosis. In screening mammography, radiologist interpretation of mammograms is prone to high error rates and requires extensive manual effort. To this end, several computer-aided diagnosis methods using machine learning have been proposed for automatic detection of breast cancer in mammography. In this paper, we provide a comprehensive review and analysis of these methods and discuss practical issues associated with their reproducibility. We aim to aid the readers in choosing the appropriate method to implement and we guide them towards this purpose. Moreover, an effort is made to re-implement a sample of the presented methods in order to highlight the importance of providing technical details associated with those methods. Advancing the domain of breast cancer pathology classification using machine learning involves the availability of public databases and development of innovative methods. Although there is significant progress in both areas, more transparency in the latter would boost the domain progress.},
  archive      = {J_ICV},
  author       = {G.M. Harshvardhan and Kei Mori and Sarika Verma and Lambros Athanasiou},
  doi          = {10.1016/j.imavis.2024.105338},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105338},
  shortjournal = {Image Vis. Comput.},
  title        = {Machine learning applications in breast cancer prediction using mammography},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CF-SOLT: Real-time and accurate traffic accident detection
using correlation filter-based tracking. <em>ICV</em>, <em>152</em>,
105336. (<a href="https://doi.org/10.1016/j.imavis.2024.105336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic accident detection using video surveillance is valuable research work in intelligent transportation systems. It is useful for responding to traffic accidents promptly that can avoid traffic jam or prevent secondary accident. In traffic accident detection, tracking occluded vehicles in real-time and accurately is one of the major sticking points for practical applications. In order to improve the tracking of occluded vehicles for traffic accident detection, this paper proposes a simple online tracking scheme with correlation filters (CF-SOLT). The CF-SOLT method utilizes a correlation filter-based auxiliary tracker to assist the main tracker. This auxiliary tracker helps prevent target ID switching caused by occlusion, enabling accurate vehicle tracking in occluded scenes. Based on the tracking results, a precise traffic accident detection algorithm is developed by integrating behavior analysis of both vehicles and pedestrians. The improved accident detection algorithm with the correlation filter-based auxiliary tracker can provide shorter response time, enabling quick identification and detection of traffic accidents. The experiments are conducted on the VisDrone2019, MOT-Traffic and Dataset of accident to evaluate the performances metrics of MOTA, IDF1, FPS, precision, response time and others. The results show that CF-SOLT improves MOTA and IDF1 by 5.3% and 6.7%, accident detection precision by 25%, and reduces response time by 56 s.},
  archive      = {J_ICV},
  author       = {Yingjie Xia and Nan Qian and Lin Guo and Zheming Cai},
  doi          = {10.1016/j.imavis.2024.105336},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105336},
  shortjournal = {Image Vis. Comput.},
  title        = {CF-SOLT: Real-time and accurate traffic accident detection using correlation filter-based tracking},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-negative subspace feature representation for few-shot
learning in medical imaging. <em>ICV</em>, <em>152</em>, 105334. (<a
href="https://doi.org/10.1016/j.imavis.2024.105334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike typical visual scene recognition tasks, where massive datasets are available to train deep neural networks (DNNs), medical image diagnosis using DNNs often faces challenges due to data scarcity. In this paper, we investigate the effectiveness of data-based few-shot learning in medical imaging by exploring different data attribute representations in a low-dimensional space. We introduce different types of non-negative matrix factorization (NMF) in few-shot learning to investigate the information preserved in the subspace resulting from dimensionality reduction, which is crucial to mitigate the data scarcity problem in medical image classification. Extensive empirical studies are conducted in terms of validating the effectiveness of NMF, especially its supervised variants (e.g., discriminative NMF, and supervised and constrained NMF with sparseness), and the comparison with principal component analysis (PCA), i.e., the collaborative representation-based dimensionality reduction technique derived from eigenvectors. With 14 different datasets covering 11 distinct illness categories, thorough experimental results and comparison with related techniques demonstrate that NMF is a competitive alternative to PCA for few-shot learning in medical imaging, and the supervised NMF algorithms are more discriminative in the subspace with greater effectiveness. Furthermore, we show that the part-based representation of NMF, especially its supervised variants, is dramatically impactful in detecting lesion areas in medical imaging with limited samples.},
  archive      = {J_ICV},
  author       = {Keqiang Fan and Xiaohao Cai and Mahesan Niranjan},
  doi          = {10.1016/j.imavis.2024.105334},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105334},
  shortjournal = {Image Vis. Comput.},
  title        = {Non-negative subspace feature representation for few-shot learning in medical imaging},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Channel and spatial enhancement network for human parsing.
<em>ICV</em>, <em>152</em>, 105332. (<a
href="https://doi.org/10.1016/j.imavis.2024.105332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dominant backbones of neural networks for scene parsing consist of multiple stages, where feature maps in different stages often contain varying levels of spatial and semantic information. High-level features convey more semantics and fewer spatial details, while low-level features possess fewer semantics and more spatial details. Consequently, there are semantic-spatial gaps among features at different levels, particularly in human parsing tasks. Many existing approaches directly upsample multi-stage features and aggregate them through addition or concatenation, without addressing the semantic-spatial gaps present among these features. This inevitably leads to spatial misalignment, semantic mismatch, and ultimately misclassification in parsing, especially for human parsing that demands more semantic information and more fine details of feature maps for the reason of intricate textures, diverse clothing styles, and heavy scale variability across different human parts. In this paper, we effectively alleviate the long-standing challenge of addressing semantic-spatial gaps between features from different stages by innovatively utilizing the subtraction and addition operations to recognize the semantic and spatial differences and compensate for them. Based on these principles, we propose the Channel and Spatial Enhancement Network (CSENet) for parsing, offering a straightforward and intuitive solution for addressing semantic-spatial gaps via injecting high-semantic information to lower-stage features and vice versa, introducing fine details to higher-stage features. Extensive experiments on three dense prediction tasks have demonstrated the efficacy of our method. Specifically, our method achieves the best performance on the LIP and CIHP datasets and we also verify the generality of our method on the ADE20K dataset.},
  archive      = {J_ICV},
  author       = {Kunliang Liu and Rize Jin and Yuelong Li and Jianming Wang and Wonjun Hwang},
  doi          = {10.1016/j.imavis.2024.105332},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105332},
  shortjournal = {Image Vis. Comput.},
  title        = {Channel and spatial enhancement network for human parsing},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point cloud segmentation neural network with same-type point
cloud assistance. <em>ICV</em>, <em>152</em>, 105331. (<a
href="https://doi.org/10.1016/j.imavis.2024.105331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes neural network architectures for point cloud segmentation, which leverage prior knowledge derived from same-type point clouds. The approach involves concurrent processing of two point clouds: a target point cloud necessitating segmentation and a labeled same-type point cloud. The labeled point cloud provides preliminary labeling information, assisting in segmenting the target point cloud. A feature combination module is proposed to identify and combine corresponding features across the point clouds. The module augments the feature representation of the target cloud and improves its capacity for object discrimination. Experiments on the ShapeNetPart and S3DIS datasets demonstrate that when integrated into classical network architectures, the proposed approach can achieve improved segmentation performance over the corresponding networks, significantly in some of them.},
  archive      = {J_ICV},
  author       = {Jingxin Lin and Kaifan Zhong and Tao Gong and Xianmin Zhang and Nianfeng Wang},
  doi          = {10.1016/j.imavis.2024.105331},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105331},
  shortjournal = {Image Vis. Comput.},
  title        = {Point cloud segmentation neural network with same-type point cloud assistance},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB-t tracking with frequency hybrid awareness.
<em>ICV</em>, <em>152</em>, 105330. (<a
href="https://doi.org/10.1016/j.imavis.2024.105330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, impressive progress has been made with transformer-based RGB-T trackers due to the transformer’s effectiveness in capturing low-frequency information (i.e., high-level semantic information). However, some studies have revealed that the transformer exhibits limitations in capturing high-frequency information (i.e., low-level texture and edge details), thereby restricting the tracker’s capacity to precisely match target details within the search area. To address this issue, we propose a frequency hybrid awareness modeling RGB-T tracker, abbreviated as FHAT. Specifically, FHAT combines the advantages of convolution and maximum pooling in capturing high-frequency information on the architecture of transformer. In this way, it strengthens the high-frequency features and enhances the model’s perception of detailed information. Additionally, to enhance the complementary effect between the two modalities, the tracker utilizes low-frequency information from both modalities for modality interaction, which can avoid interaction errors caused by inconsistent local details of the multimodality. Furthermore, these high-frequency information and interaction low-frequency information will then be fused, allowing the model to adaptively enhance the frequency features of the modal expression. Through extensive experiments on two mainstream RGB-T tracking benchmarks, our method demonstrates competitive performance.},
  archive      = {J_ICV},
  author       = {Lei Lei and Xianxian Li},
  doi          = {10.1016/j.imavis.2024.105330},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105330},
  shortjournal = {Image Vis. Comput.},
  title        = {RGB-T tracking with frequency hybrid awareness},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-label classification method based on transformer for
deepfake detection. <em>ICV</em>, <em>152</em>, 105319. (<a
href="https://doi.org/10.1016/j.imavis.2024.105319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of hardware and deep learning technologies, existing forgery techniques are capable of more refined facial manipulations, making detection tasks increasingly challenging. Therefore, forgery detection cannot be viewed merely as a traditional binary classification task. To achieve finer forgery detection, we propose a method based on multi-label detection classification capable of identifying the presence of forgery in multiple facial components. Initially, the dataset undergoes preprocessing to meet the requirements of this task. Subsequently, we introduce a Detail-Enhancing Attention Module into the network to amplify subtle forgery traces in shallow feature maps and enhance the network&#39;s feature extraction capabilities. Additionally, we employ a Global–Local Transformer Decoder to improve the network&#39;s ability to focus on local information. Finally, extensive experiments demonstrate that our approach achieves 92.45% mAP and 90.23% mAUC, enabling precise detection of facial components in images, thus validating the effectiveness of our proposed method.},
  archive      = {J_ICV},
  author       = {Liwei Deng and Yunlong Zhu and Dexu Zhao and Fei Chen},
  doi          = {10.1016/j.imavis.2024.105319},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105319},
  shortjournal = {Image Vis. Comput.},
  title        = {A multi-label classification method based on transformer for deepfake detection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAFENet: Semantic-aware feature enhancement network for
unsupervised cross-domain road scene segmentation. <em>ICV</em>,
<em>152</em>, 105318. (<a
href="https://doi.org/10.1016/j.imavis.2024.105318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-domain road scene segmentation has attracted substantial interest because of its capability to perform segmentation on new and unlabeled domains, thereby reducing the dependence on expensive manual annotations. This is achieved by leveraging networks trained on labeled source domains to classify images on unlabeled target domains. Conventional techniques usually use adversarial networks to align inputs from the source and the target in either of their domains. However, these approaches often fall short in effectively integrating information from both domains due to Alignment in each space usually leads to bias problems during feature learning. To overcome these limitations and enhance cross-domain interaction while mitigating overfitting to the source domain, we introduce a novel framework called Semantic-Aware Feature Enhancement Network (SAFENet) for Unsupervised Cross-domain Road Scene Segmentation. SAFENet incorporates the Semantic-Aware Enhancement (SAE) module to amplify the importance of class information in segmentation tasks and uses the semantic space as a new domain to guide the alignment of the source and target domains. Additionally, we integrate Adaptive Instance Normalization with Momentum (AdaIN-M) techniques, which convert the source domain image style to the target domain image style, thereby reducing the adverse effects of source domain overfitting on target domain segmentation performance. Moreover, SAFENet employs a Knowledge Transfer (KT) module to optimize network architecture, enhancing computational efficiency during testing while maintaining the robust inference capabilities developed during training. To further improve the segmentation performance, we further employ Curriculum Learning, a self-training mechanism that uses pseudo-labels derived from the target domain to iteratively refine the network. Comprehensive experiments on three well-known datasets, “Synthia → Cityscapes” and “GTA5 → Cityscapes”, demonstrate the superior performance of our method. In-depth examinations and ablation studies verify the efficacy of each module within the proposed method.},
  archive      = {J_ICV},
  author       = {Dexin Ren and Minxian Li and Shidong Wang and Mingwu Ren and Haofeng Zhang},
  doi          = {10.1016/j.imavis.2024.105318},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105318},
  shortjournal = {Image Vis. Comput.},
  title        = {SAFENet: Semantic-aware feature enhancement network for unsupervised cross-domain road scene segmentation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occlusion-related graph convolutional neural network for
multi-object tracking. <em>ICV</em>, <em>152</em>, 105317. (<a
href="https://doi.org/10.1016/j.imavis.2024.105317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Object Tracking (MOT) has recently been improved by Graph Convolutional Neural Networks (GCNNs) for its good performance in characterizing interactive features. However, GCNNs prefer assigning smaller proportions to node features if a node has more neighbors, presenting challenges in distinguishing objects with similar neighbors which is common in dense scenes. This paper designs an Occlusion-Related GCNN (OR-GCNN) based on which an interactive similarity module is further built. Specifically, the interactive similarity module first uses learnable weights to calculate the edge weights between tracklets and detection objects, which balances the appearance cosine similarity and Intersection over Union (IoU). Then, the module determines the proportion of node features with the help of an occlusion weight comes from a MultiLayer Perceptron (MLP). These occlusion weights, the edge weights, and the node features are then served to our OR-GCNN to obtain interactive features. Finally, by integrating interactive similarity into a common MOT framework, such as BoT-SORT, one gets a tracker that efficiently alleviates the issues in dense MOT task. The experimental results on MOT16 and MOT17 benchmarks show that our model achieves the MOTA of 80.6 and 81.1 and HOTA of 65.3 and 65.1 on MOT16 and MOT17, respectively, which outperforms the state-of-the-art trackers, including ByteTrack, BoT-SORT, GCNNMatch, GNMOT, and GSM.},
  archive      = {J_ICV},
  author       = {Yubo Zhang and Liying Zheng and Qingming Huang},
  doi          = {10.1016/j.imavis.2024.105317},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105317},
  shortjournal = {Image Vis. Comput.},
  title        = {Occlusion-related graph convolutional neural network for multi-object tracking},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TransWild: Enhancing 3D interacting hands recovery in the
wild with IoU-guided transformer. <em>ICV</em>, <em>152</em>, 105316.
(<a href="https://doi.org/10.1016/j.imavis.2024.105316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recovery of 3D interacting hands meshes in the wild (ITW) is crucial for 3D full-body mesh reconstruction, especially when limited 3D annotations are available. The recent ITW interacting hands recovery method brings two hands to a shared 2D scale space and achieves effective learning of ITW datasets. However, they lack the deep exploitation of the intrinsic interaction dynamics of hands. In this work, we propose TransWild, a novel framework for 3D interactive hand mesh recovery that leverages a weight-shared Intersection-of-Union (IoU) guided Transformer for feature interaction. Based on harmonizing ITW and MoCap datasets within a unified 2D scale space, our hand feature interaction mechanism powered by an IoU-guided Transformer enables a more accurate estimation of interacting hands. This innovation stems from the observation that hand detection yields valuable IoU of two hands bounding box, therefore, an IOU-guided Transformer can significantly enrich the Transformer’s ability to decode and integrate these insights into the interactive hand recovery process. To ensure consistent training outcomes, we have developed a strategy for training with augmented ground truth bounding boxes to address inherent variability. Quantitative evaluations across two prominent benchmarks for 3D interacting hands underscore our method’s superior performance. The code will be released after acceptance.},
  archive      = {J_ICV},
  author       = {Wanru Zhu and Yichen Zhang and Ke Chen and Lihua Guo},
  doi          = {10.1016/j.imavis.2024.105316},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105316},
  shortjournal = {Image Vis. Comput.},
  title        = {TransWild: Enhancing 3D interacting hands recovery in the wild with IoU-guided transformer},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SVC: Sight view constraint for robust point cloud
registration. <em>ICV</em>, <em>152</em>, 105315. (<a
href="https://doi.org/10.1016/j.imavis.2024.105315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial to Partial Point Cloud Registration (partial PCR) remains a challenging task, particularly when dealing with a low overlap rate. In comparison to the full-to-full registration task, we find that the objective of partial PCR is still not well-defined, indicating no metric can reliably identify the true transformation. We identify this as the most fundamental challenge in partial PCR tasks. In this paper, instead of directly seeking the optimal transformation, we propose a novel and general Sight View Constraint (SVC) to conclusively identify incorrect transformations, thereby enhancing the robustness of existing PCR methods. Extensive experiments validate the effectiveness of SVC on both indoor and outdoor scenes. On the challenging 3DLoMatch dataset, our approach increases the registration recall from 78% to 82%, achieving the state-of-the-art result. This research also highlights the significance of the decision version problem of partial PCR, which has the potential to provide novel insights into the partial PCR problem. Code will be available at: https://github.com/pppyj-m/SVC .},
  archive      = {J_ICV},
  author       = {Yaojie Zhang and Weijun Wang and Tianlun Huang and Zhiyong Wang and Wei Feng},
  doi          = {10.1016/j.imavis.2024.105315},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105315},
  shortjournal = {Image Vis. Comput.},
  title        = {SVC: Sight view constraint for robust point cloud registration},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CWGA-net: Center-weighted graph attention network for 3D
object detection from point clouds. <em>ICV</em>, <em>152</em>, 105314.
(<a href="https://doi.org/10.1016/j.imavis.2024.105314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The precision of 3D object detection from unevenly distributed outdoor point clouds is critical in autonomous driving perception systems. Current point-based detectors employ self-attention and graph convolution to establish contextual relationships between point clouds; however, they often introduce weakly correlated redundant information, leading to blurred geometric details and false detections. To address this issue, a novel Center-weighted Graph Attention Network (CWGA-Net) has been proposed to fuse geometric and semantic similarities for weighting cross-attention scores, thereby capturing precise fine-grained geometric features. CWGA-Net initially constructs and encodes local graphs between foreground points, establishing connections between point clouds from geometric and semantic dimensions. Subsequently, center-weighted cross-attention is utilized to compute the contextual relationships between vertices within the graph, and geometric and semantic similarities between vertices are fused to weight attention scores, thereby extracting strongly related geometric shape features. Finally, a cross-feature fusion Module is introduced to deeply fuse high and low-resolution features to compensate for the information loss during downsampling. Experiments conducted on the KITTI and Waymo datasets demonstrate that the network achieves superior detection capabilities, outperforming state-of-the-art point-based single-stage methods in terms of average precision metrics while maintaining good speed.},
  archive      = {J_ICV},
  author       = {Jun Shu and Qi Wu and Liang Tan and Xinyi Shu and Fengchun Wan},
  doi          = {10.1016/j.imavis.2024.105314},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105314},
  shortjournal = {Image Vis. Comput.},
  title        = {CWGA-net: Center-weighted graph attention network for 3D object detection from point clouds},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepArUco++: Improved detection of square fiducial markers
in challenging lighting conditions. <em>ICV</em>, <em>152</em>, 105313.
(<a href="https://doi.org/10.1016/j.imavis.2024.105313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fiducial markers are a computer vision tool used for object pose estimation and detection. These markers are highly useful in fields such as industry, medicine and logistics. However, optimal lighting conditions are not always available, and other factors such as blur or sensor noise can affect image quality. Classical computer vision techniques that precisely locate and decode fiducial markers often fail under difficult illumination conditions (e.g. extreme variations of lighting within the same frame). Hence, we propose DeepArUco++, a deep learning-based framework that leverages the robustness of Convolutional Neural Networks to perform marker detection and decoding in challenging lighting conditions. The framework is based on a pipeline using different Neural Network models at each step, namely marker detection, corner refinement and marker decoding. Additionally, we propose a simple method for generating synthetic data for training the different models that compose the proposed pipeline, and we present a second, real-life dataset of ArUco markers in challenging lighting conditions used to evaluate our system. The developed method outperforms other state-of-the-art methods in such tasks and remains competitive even when testing on the datasets used to develop those methods. Code available in GitHub: https://github.com/AVAuco/deeparuco/ .},
  archive      = {J_ICV},
  author       = {Rafael Berral-Soler and Rafael Muñoz-Salinas and Rafael Medina-Carnicer and Manuel J. Marín-Jiménez},
  doi          = {10.1016/j.imavis.2024.105313},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105313},
  shortjournal = {Image Vis. Comput.},
  title        = {DeepArUco++: Improved detection of square fiducial markers in challenging lighting conditions},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAVE: Encoding spatial interactions for vision transformers.
<em>ICV</em>, <em>152</em>, 105312. (<a
href="https://doi.org/10.1016/j.imavis.2024.105312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have achieved impressive performance in visual tasks. Position encoding, which equips vectors (elements of input tokens, queries, keys, or values) with sequence specificity, effectively alleviates the lack of permutation relation in transformers. In this work, we first clarify that both position encoding and additional position-specific operations will introduce positional information when participating in self-attention. On this basis, most existing position encoding methods are equivalent to special affine transformations. However, this encoding method lacks the correlation of vector content interaction. We further propose Spatial Aggregation Vector Encoding (SAVE) that employs transition matrices to recombine vectors. We design two simple yet effective modes to merge other vectors, with each one serving as an anchor. The aggregated vectors control spatial contextual connections by establishing two-dimensional relationships. Our SAVE can be plug-and-play in vision transformers, even with other position encoding methods. Comparative results on three image classification datasets show that the proposed SAVE performs comparably to current position encoding methods. Experiments on detection tasks show that the SAVE improves the downstream performance of transformer-based methods. Code is available at https://github.com/maxiao0234/SAVE .},
  archive      = {J_ICV},
  author       = {Xiao Ma and Zetian Zhang and Rong Yu and Zexuan Ji and Mingchao Li and Yuhan Zhang and Qiang Chen},
  doi          = {10.1016/j.imavis.2024.105312},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105312},
  shortjournal = {Image Vis. Comput.},
  title        = {SAVE: Encoding spatial interactions for vision transformers},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3DPSR: An innovative approach for pose and shape refinement
in 3D human meshes from a single 2D image. <em>ICV</em>, <em>152</em>,
105311. (<a href="https://doi.org/10.1016/j.imavis.2024.105311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of computer vision, 3D human models are gaining a lot of interest in the gaming industry, cloth parsing, avatar creations, and many more applications. In these fields, having a precise 3D human model with accurate shape and pose is crucial for realistic and high-quality results. We proposed an approach called 3DPSR that uses a single 2D image and reconstructs precise 3D human meshes with better alignment of pose and shape. 3DPSR is referred to as 3D P ose and S hape R efinements. 3DPSR contains two modules (mesh deformation using pose-fitting and shape-fitting), in which mesh deformation using shape-fitting acts as a refinement module. Compared to existing methods, the proposed method, 3DPSR, delivers more enhanced MPVE and PA-MPJPE results, as well as more accurate 3D models of humans. 3DPSR significantly outperforms state-of-the-art human mesh reconstruction methods on challenging and standard datasets such as SURREAL, Human3.6M, and 3DPW across different scenarios with complex poses, establishing a new benchmark.},
  archive      = {J_ICV},
  author       = {Mohit Kushwaha and Jaytrilok Choudhary and Dhirendra Pratap Singh},
  doi          = {10.1016/j.imavis.2024.105311},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105311},
  shortjournal = {Image Vis. Comput.},
  title        = {3DPSR: An innovative approach for pose and shape refinement in 3D human meshes from a single 2D image},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text-augmented multi-modality contrastive learning for
unsupervised visible-infrared person re-identification. <em>ICV</em>,
<em>152</em>, 105310. (<a
href="https://doi.org/10.1016/j.imavis.2024.105310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification holds significant implications for intelligent security. Unsupervised methods can reduce the gap of different modalities without labels. Most previous unsupervised methods only train their models with image information, so that the model cannot obtain powerful deep semantic information. In this paper, we leverage CLIP to extract deep text information. We propose a Text–Image Alignment (TIA) module to align the image and text information and effectively bridge the gap between visible and infrared modality. We produce a Local–Global Image Match (LGIM) module to find homogeneous information. Specifically, we employ the Hungarian algorithm and Simulated Annealing (SA) algorithm to attain original information from image features while mitigating the interference of heterogeneous information. Additionally, we design a Changeable Cross-modality Alignment Loss (CCAL) to enable the model to learn modality-specific features during different training stages. Our method performs well and attains powerful robustness by targeted learning. Extensive experiments demonstrate the effectiveness of our approach, our method achieves a rank-1 accuracy that exceeds state-of-the-art approaches by approximately 10% on the RegDB.},
  archive      = {J_ICV},
  author       = {Rui Sun and Guoxi Huang and Xuebin Wang and Yun Du and Xudong Zhang},
  doi          = {10.1016/j.imavis.2024.105310},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105310},
  shortjournal = {Image Vis. Comput.},
  title        = {Text-augmented multi-modality contrastive learning for unsupervised visible-infrared person re-identification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fine-grained semantic oriented embedding set alignment for
text-based person search. <em>ICV</em>, <em>152</em>, 105309. (<a
href="https://doi.org/10.1016/j.imavis.2024.105309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search aims to retrieve images of a person that are highly semantically relevant to a given textual description. The difficulty of this retrieval task is modality heterogeneity and fine-grained matching. Most existing methods only consider the alignment using global features, ignoring the fine-grained matching problem. The cross-modal attention interactions are popularly used for image patches and text markers for direct alignment. However, cross-modal attention may cause a huge overhead in the reasoning stage and cannot be applied in actual scenarios. In addition, it is unreasonable to perform patch-token alignment, since image patches and text tokens do not have complete semantic information. This paper proposes an Embedding Set Alignment (ESA) module for fine-grained alignment. The module can preserve fine-grained semantic information by merging token-level features into embedding sets. The ESA module benefits from pre-trained cross-modal large models, and it can be combined with the backbone non-intrusively and trained in an end-to-end manner. In addition, an Adaptive Semantic Margin (ASM) loss is designed to describe the alignment of embedding sets, instead of adapting a loss function with a fixed margin. Extensive experiments demonstrate that our proposed fine-grained semantic embedding set alignment method achieves state-of-the-art performance on three popular benchmark datasets, surpassing the previous best methods.},
  archive      = {J_ICV},
  author       = {Jiaqi Zhao and Ao Fu and Yong Zhou and Wen-liang Du and Rui Yao},
  doi          = {10.1016/j.imavis.2024.105309},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105309},
  shortjournal = {Image Vis. Comput.},
  title        = {Fine-grained semantic oriented embedding set alignment for text-based person search},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention enhanced machine instinctive vision with
human-inspired saliency detection. <em>ICV</em>, <em>152</em>, 105308.
(<a href="https://doi.org/10.1016/j.imavis.2024.105308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) enables machines to recognize and accurately segment visually prominent regions in images. Despite recent advancements, existing approaches often lack progressive fusion of low and high-level features, effective multi-scale feature handling, and precise boundary detection. Moreover, the robustness of these models under varied lighting conditions remains a concern. To overcome these challenges, we present Attention Enhanced Machine Instinctive Vision framework for SOD. The proposed framework leverages the strategy of Multi-stage Feature Refinement with Optimal Attentions-Driven Framework (MFRNet). The multi-level features are extracted from six stages of the EfficientNet-B7 backbone. This provides effective feature fusions of low and high-level details across various scales at the later stage of the framework. We introduce the Spatial-optimized Feature Attention (SOFA) module, which refines spatial features from three initial-stage feature maps. The extracted multi-scale features from the backbone are passed from the convolution feature transformation and spatial attention mechanisms to refine the low-level information. The SOFA module concatenates and upsamples these refined features, producing a comprehensive spatial representation of various levels. Moreover, the proposed Context-Aware Channel Refinement (CACR) module integrates dilated convolutions with optimized dilation rates followed by channel attention to capture multi-scale contextual information from the mature three layers. Furthermore, our progressive feature fusion strategy combines high-level semantic information and low-level spatial details through multiple residual connections, ensuring robust feature representation and effective gradient backpropagation. To enhance robustness, we train our network with augmented data featuring low and high brightness adjustments, improving its ability to handle diverse lighting conditions. Extensive experiments on four benchmark datasets — ECSSD, HKU-IS, DUTS, and PASCAL-S — validate the proposed framework’s effectiveness, demonstrating superior performance compared to existing SOTA methods in the domain. Code, qualitative results, and trained weights will be available at the link: https://github.com/habib1402/MFRNet-SOD .},
  archive      = {J_ICV},
  author       = {Habib Khan and Muhammad Talha Usman and Imad Rida and JaKeoung Koo},
  doi          = {10.1016/j.imavis.2024.105308},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105308},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention enhanced machine instinctive vision with human-inspired saliency detection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MinoritySalMix and adaptive semantic weight compensation for
long-tailed classification. <em>ICV</em>, <em>152</em>, 105307. (<a
href="https://doi.org/10.1016/j.imavis.2024.105307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world datasets, the widespread presence of a long-tailed distribution often leads models to become overly biased towards majority class samples while ignoring minority class samples. We propose a strategy called MASW (MinoritySalMix and adaptive semantic weight compensation) to improve this problem. First, we propose a data augmentation method called MinoritySalMix (minority-saliency-mixing), which uses significance detection techniques to select significant regions from minority class samples as cropping regions and paste them into the same regions of majority class samples to generate brand new samples, thereby amplifying images containing important regions of minority class samples. Second, in order to make the label value information of the newly generated samples more consistent with the image content of the newly generated samples, we propose an adaptive semantic compensation factor. This factor provides more label value compensation for minority samples based on the different cropping areas, thereby making the new label values closer to the content of the newly generated samples. Improve model performance by generating more accurate new label value information. Finally, considering that some current re-sampling strategies generally lack flexibility in handling class sampling weight allocation and frequently require manual adjustment. We designed an adaptive weight function and incorporated it into the re-sampling strategy to achieve better sampling. The experimental results on three long-tailed datasets show that our method can effectively improve the performance of the model and is superior to most advanced long-tailed methods. Furthermore, we extended MinoritySalMix’s strategy to three balanced datasets for experimentation, and the results indicated that our method surpassed several advanced data augmentation techniques.},
  archive      = {J_ICV},
  author       = {Wu Zeng and Zheng-ying Xiao},
  doi          = {10.1016/j.imavis.2024.105307},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105307},
  shortjournal = {Image Vis. Comput.},
  title        = {MinoritySalMix and adaptive semantic weight compensation for long-tailed classification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated anthropometric measurements from 3D point clouds
of scanned bodies. <em>ICV</em>, <em>152</em>, 105306. (<a
href="https://doi.org/10.1016/j.imavis.2024.105306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anthropometry plays a critical role across numerous sectors, particularly within healthcare and fashion, by facilitating the analysis of the human body structure. The significance of anthropometric data cannot be overstated; it is crucial for assessing nutritional status among children and adults alike, enabling early detection of conditions such as malnutrition, obesity, and being overweight. Furthermore, it is instrumental in creating tailored dietary interventions. This study introduces a novel automated technique for extracting anthropometric measurements from any body part. The proposed method leverages a parametric model to accurately determine the measurement parameters from either an unstructured point cloud or a mesh. We conducted a comprehensive evaluation of our approach by comparing perimetral measurements from over 400 body scans with expert assessments and existing state-of-the-art methods. The results demonstrate that our approach significantly surpasses the current methods for measuring the waist, hip, thigh, chest, and wrist perimeters with exceptional accuracy. These findings indicate the potential of our method to automate anthropometric analysis and offer efficient and accurate measurements for various applications in healthcare and fashion industries.},
  archive      = {J_ICV},
  author       = {Nahuel E. Garcia-D’Urso and Antonio Macia-Lillo and Higinio Mora-Mora and Jorge Azorin-Lopez and Andres Fuster-Guillo},
  doi          = {10.1016/j.imavis.2024.105306},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105306},
  shortjournal = {Image Vis. Comput.},
  title        = {Automated anthropometric measurements from 3D point clouds of scanned bodies},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-object tracking using score-driven hierarchical
association strategy between predicted tracklets and objects.
<em>ICV</em>, <em>152</em>, 105303. (<a
href="https://doi.org/10.1016/j.imavis.2024.105303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine vision is one of the major technologies to guarantee intelligent robots’ human-centered embodied intelligence. Especially in the complex dynamic scene involving multi-person, Multi-Object Tracking (MOT), which can accurately identify and track specific targets, significantly influences intelligent robots’ performance regarding behavior perception and monitoring, autonomous decision-making, and providing personalized humanoid services. In order to solve the problem of targets lost and identity switches caused by the scale variations of objects and frequent overlaps during the tracking process, this paper presents a multi-object tracking method using score-driven hierarchical association strategy between predicted tracklets and objects (ScoreMOT). Firstly, a motion prediction of occluded objects based on bounding box variation (MPOBV) is proposed to estimate the position of occluded objects. MPOBV models the motion state of the object using the bounding box and confidence score. Then, a score-driven hierarchical association strategy between predicted tracklets and objects (SHAS) is proposed to correctly associate them in frequently overlapping scenarios. SHAS associates the predicted tracklets and detected objects with different confidence in different stages. The comparison results with 16 state-of-the-art methods on Multiple Object Tracking Benchmark 20 (MOT20) and DanceTrack datasets are conducted, and ScoreMOT outperforms the compared methods.},
  archive      = {J_ICV},
  author       = {Tianyi Zhao and Guanci Yang and Yang Li and Minglang Lu and Haoran Sun},
  doi          = {10.1016/j.imavis.2024.105303},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105303},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-object tracking using score-driven hierarchical association strategy between predicted tracklets and objects},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature differences reduction and specific features
preserving network for RGB-t salient object detection. <em>ICV</em>,
<em>152</em>, 105302. (<a
href="https://doi.org/10.1016/j.imavis.2024.105302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In RGB-T salient object detection, effective utilization of the different characteristics of RGB and thermal modalities is essential to achieve accurate detection. Most of the previous methods usually only focus on reducing the differences between modalities, which may ignore the specific features that are crucial for salient object detection, leading to suboptimal results. To address the above issue, an RGB-T SOD network that simultaneously considers the reduction of modality differences and the preservation of specific features is proposed. Specifically, we construct a modality differences reduction and specific features preserving module (MDRSFPM) which aims to bridge the gap between modalities and enhance the specific features of each modality. In MDRSFPM, the dynamic vector generated by the interaction of RGB and thermal features is used to reduce modality differences, and then a dual branch is constructed to deal with the RGB and thermal modalities separately, employing a combination of channel-level and spatial-level operations to preserve their respective specific features. In addition, a multi-scale global feature enhancement module (MGFEM) is proposed to enhance global contextual information to provide guidance information for the subsequent decoding stage, so that the model can more easily localize the salient objects. Furthermore, our approach includes a fully fusion and gate module (FFGM) that utilizes dynamically generated importance maps to selectively filter and fuse features during the decoding process. Extensive experiments demonstrate that our proposed model surpasses other state-of-the-art models on three publicly available RGB-T datasets remarkably. Our code will be released at https://github.com/JOOOOKII/FRPNet .},
  archive      = {J_ICV},
  author       = {Qiqi Xu and Zhenguang Di and Haoyu Dong and Gang Yang},
  doi          = {10.1016/j.imavis.2024.105302},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105302},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature differences reduction and specific features preserving network for RGB-T salient object detection},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian error loss function for image smoothing.
<em>ICV</em>, <em>152</em>, 105300. (<a
href="https://doi.org/10.1016/j.imavis.2024.105300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing plays an important role in the fields of image processing and computational photography, and is widely used for a variety of applications. The edge-preserving filters based on global optimization models have attracted widespread attention due to their nice smoothing quality. According to existing research, the edge-preserving capability is strongly correlated to the penalty function used for gradient regularization. By analyzing the edge-stopping function of existing penalties, we demonstrate that existing image smoothing models are not adequately edge-preserving. In this paper, based on a Gaussian error function (ERF), we propose a Gaussian error loss function (ERLF), which shows stronger edge-preserving capability. We embed the proposed loss function into a global optimization model for edge-preserving image smoothing. In addition, we propose an efficient solution based on additive half-quadratic minimization and Fourier-domain optimization that is capable of processing 720P color images (over 20 fps) in real-time on an NVIDIA RTX 3070 GPU. We have experimented with the proposed filter on a number of low-level vision tasks. Both quantitative and qualitative experimental results show that the proposed filter outperforms existing filters. Therefore, it can be practical for real applications.},
  archive      = {J_ICV},
  author       = {Wenzheng Dong and Lanling Zeng and Shunli Ji and Yang Yang},
  doi          = {10.1016/j.imavis.2024.105300},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105300},
  shortjournal = {Image Vis. Comput.},
  title        = {Gaussian error loss function for image smoothing},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). FHLight: A novel method of indoor scene illumination
estimation using improved loss function. <em>ICV</em>, <em>152</em>,
105299. (<a href="https://doi.org/10.1016/j.imavis.2024.105299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In augmented reality tasks, especially in indoor scenes, achieving illumination consistency between virtual objects and real environments is a critical challenge. Currently, mainstream methods are illumination parameters regression and illumination map generation. Among these two categories of methods, few works can effectively recover both high-frequency and low-frequency illumination information within indoor scenes. In this work, we argue that effective restoration of low-frequency illumination information forms the foundation for capturing high-frequency illumination details. In this way, we propose a novel illumination estimation method called FHLight. Technically, we use a low-frequency spherical harmonic irradiance map (LFSHIM) restored by the low-frequency illumination regression network (LFIRN) as prior information to guide the high-frequency illumination generator (HFIG) to restore the illumination map. Furthermore, we suggest an improved loss function to optimize the network training procedure, ensuring that the model accurately restores both low-frequency and high-frequency illumination information within the scene. We compare FHLight with several competitive methods, and the results demonstrate significant improvements in metrics such as RMSE, si-RMSE, and Angular error. In addition, visual experiments further confirm that FHLight is capable of generating scene illumination maps with genuine frequencies, effectively resolving the illumination consistency issue between virtual objects and real scenes. The code is available at https://github.com/WA-tyro/FHLight.git .},
  archive      = {J_ICV},
  author       = {Yang Wang and Ao Wang and Shijia Song and Fan Xie and Chang Ma and Jiawei Xu and Lijun Zhao},
  doi          = {10.1016/j.imavis.2024.105299},
  journal      = {Image and Vision Computing},
  month        = {12},
  pages        = {105299},
  shortjournal = {Image Vis. Comput.},
  title        = {FHLight: A novel method of indoor scene illumination estimation using improved loss function},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum to “STAFFormer: Spatio-temporal adaptive fusion
transformer for efficient 3D human pose estimation” [journal of image
and vision computing volume 149 (2024) 105142]. <em>ICV</em>,
<em>151</em>, 105305. (<a
href="https://doi.org/10.1016/j.imavis.2024.105305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Feng Hao and Fujin Zhong and Yunhe Wang and Hong Yu and Jun Hu and Yan Yang},
  doi          = {10.1016/j.imavis.2024.105305},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105305},
  shortjournal = {Image Vis. Comput.},
  title        = {Corrigendum to “STAFFormer: Spatio-temporal adaptive fusion transformer for efficient 3D human pose estimation” [Journal of image and vision computing volume 149 (2024) 105142]},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A review of recent advances in 3D gaussian splatting for
optimization and reconstruction. <em>ICV</em>, <em>151</em>, 105304. (<a
href="https://doi.org/10.1016/j.imavis.2024.105304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) represents a significant breakthrough in computer graphics and vision, offering an explicit scene representation and novel view synthesis without the reliance on neural networks, unlike Neural Radiance Fields (NeRF). This paper provides a comprehensive survey of recent research on 3DGS optimization and reconstruction, with a particular focus on studies featuring published or forthcoming open-source code. In terms of optimization, the paper examines techniques such as compression, densification, splitting, anti-aliasing, and reflection enhancement. For reconstruction, it explores methods including surface mesh extraction, sparse-view object and scene reconstruction, large-scale scene reconstruction, and dynamic object and scene reconstruction. Through comparative analysis and case studies, the paper highlights the practical advantages of 3DGS and outlines future research directions, offering valuable insights for advancing the field.},
  archive      = {J_ICV},
  author       = {Jie Luo and Tianlun Huang and Weijun Wang and Wei Feng},
  doi          = {10.1016/j.imavis.2024.105304},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105304},
  shortjournal = {Image Vis. Comput.},
  title        = {A review of recent advances in 3D gaussian splatting for optimization and reconstruction},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pyramid quaternion discrete cosine transform based ConvNet
for cancelable face recognition. <em>ICV</em>, <em>151</em>, 105301. (<a
href="https://doi.org/10.1016/j.imavis.2024.105301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current face scanning era can quickly and conveniently attain identity authentication, but face images imply sensitive information simultaneously. Under such context, we introduce a novel cancelable face recognition methodology by using quaternion transform based convolutional network. Firstly, face images in different modalities (e.g., RGB and depth or near-infrared) are encoded into full quaternion matrix for synchronous processing. Based on the designed multiresolution quaternion singular value decomposition, we can obtain pyramid representation. Then they are transformed through random projection for making the process noninvertible. Even if the feature template is compromised, a new one can be generated. Subsequently, a three-stream convolutional network is developed to learn features, where predefined filters are stemmed from quaternion two-dimensional discrete cosine transform basis. Extensive experiments on the TIII-D, NVIE and CASIA datasets have demonstrated that the proposed method obtains competitive performance, also satisfies redistributable and irreversible.},
  archive      = {J_ICV},
  author       = {Zhuhong Shao and Zuowei Zhang and Leding Li and Hailiang Li and Xuanyi Li and Bicao Li and Yuanyuan Shang and Bin Chen},
  doi          = {10.1016/j.imavis.2024.105301},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105301},
  shortjournal = {Image Vis. Comput.},
  title        = {Pyramid quaternion discrete cosine transform based ConvNet for cancelable face recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decision support system for acute lymphoblastic leukemia
detection based on explainable artificial intelligence. <em>ICV</em>,
<em>151</em>, 105298. (<a
href="https://doi.org/10.1016/j.imavis.2024.105298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of acute lymphoblastic leukemia (ALL) via deep learning (DL) has received great interest because of its high accuracy in detecting lymphoblasts without the need for handcrafted feature extraction. However, current DL models, such as convolutional neural networks and vision Transformers, are extremely complex, making them black boxes that perform classification in an obscure way. To compensate for this and increase the explainability of the decisions made by such methods, in this paper, we propose an innovative decision support system for ALL detection that is based on DL and explainable artificial intelligence (XAI). Our approach first introduces causality into the decision with a metric learning approach, enabling a decision to be made by analyzing the most similar images in the database. Second, our method integrates XAI techniques to allow even non-trained personnel to obtain an informed decision by analyzing which regions of the images are most similar and how the samples are organized in the latent space. The results on publicly available ALL databases confirm the validity of our approach in opening the black box while achieving similar or superior accuracy to that of existing approaches.},
  archive      = {J_ICV},
  author       = {Angelo Genovese and Vincenzo Piuri and Fabio Scotti},
  doi          = {10.1016/j.imavis.2024.105298},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105298},
  shortjournal = {Image Vis. Comput.},
  title        = {A decision support system for acute lymphoblastic leukemia detection based on explainable artificial intelligence},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global information regulation network for multimodal
sentiment analysis. <em>ICV</em>, <em>151</em>, 105297. (<a
href="https://doi.org/10.1016/j.imavis.2024.105297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human language is considered multimodal, containing natural language, visual elements, and acoustic signals. Multimodal Sentiment Analysis (MSA) concentrates on the integration of various modalities to capture the sentiment polarity or intensity expressed in human language. Nevertheless, the absence of a comprehensive strategy for processing and integrating multimodal representations results in the inclusion of inaccurate or noisy data from diverse modalities in the ultimate decision-making process, potentially leading to the neglect of crucial information within or across modalities. To address this issue, we propose the Global Information Regulation Network (GIRN), a novel framework designed to regulate information flow and decision-making processes across various stages, ranging from unimodal feature extraction to multimodal outcome prediction. Specifically, before modal fusion stage, we maximize the mutual information between modalities and refine the input signals through random feature erasing, yielding a more robust unimodal representation. In the process of modal fusion, we enhance the traditional Transformer encoder through the gate mechanism and stacked attention to dynamically fuse the target and auxiliary modalities. After modal fusion, cross-hierarchical contrastive learning and decision gate are employed to integrate the valuable information represented in different categories and hierarchies. Extensive experiments conducted on the CMU-MOSI and CMU-MOSEI datasets suggest that our methodology outperforms existing approaches across nearly all criteria.},
  archive      = {J_ICV},
  author       = {Shufan Xie and Qiaohong Chen and Xian Fang and Qi Sun},
  doi          = {10.1016/j.imavis.2024.105297},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105297},
  shortjournal = {Image Vis. Comput.},
  title        = {Global information regulation network for multimodal sentiment analysis},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter efficient finetuning of text-to-image models with
trainable self-attention layer. <em>ICV</em>, <em>151</em>, 105296. (<a
href="https://doi.org/10.1016/j.imavis.2024.105296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel model to efficiently finetune pretrained Text-to-Image models by introducing additional image prompts. The model integrates information from image prompts into the text-to-image (T2I) diffusion process by locking the parameters of the large T2I model and reusing its trainable copy, rather than relying on additional adapters. The trainable copy guides the model by injecting its trainable self-attention features into the original diffusion model, enabling the synthesis of a new specific concept. We also apply Low-Rank Adaptation (LoRA) to restrict the trainable parameters in the self-attention layers. Furthermore, the network is optimized alongside a text embedding that serves as an object identifier to generate contextually relevant visual content. Our model is simple and effective, with a small memory footprint, yet can achieve comparable performance to a fully fine-tuned T2I model in both qualitative and quantitative evaluations.},
  archive      = {J_ICV},
  author       = {Zhuoyuan Li and Yi Sun},
  doi          = {10.1016/j.imavis.2024.105296},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105296},
  shortjournal = {Image Vis. Comput.},
  title        = {Parameter efficient finetuning of text-to-image models with trainable self-attention layer},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Background debiased class incremental learning for video
action recognition. <em>ICV</em>, <em>151</em>, 105295. (<a
href="https://doi.org/10.1016/j.imavis.2024.105295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we tackle class incremental learning (CIL) for video action recognition, a relatively under-explored problem despite its practical importance. Directly applying image-based CIL methods does not work well in the video action recognition setting. We hypothesize the major reason is the spurious correlation between the action and background in video action recognition datasets/models. Recent literature shows that the spurious correlation hampers the generalization of models in the conventional action recognition setting. The problem is even more severe in the CIL setting due to the limited exemplars available in the rehearsal memory. We empirically show that mitigating the spurious correlation between the action and background is crucial to the CIL for video action recognition. We propose to learn background invariant action representations in the CIL setting by providing training videos with diverse backgrounds generated from background augmentation techniques. We validate the proposed method on public benchmarks: HMDB-51, UCF-101, and Something-Something-v2.},
  archive      = {J_ICV},
  author       = {Le Quan Nguyen and Jinwoo Choi and L. Minh Dang and Hyeonjoon Moon},
  doi          = {10.1016/j.imavis.2024.105295},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105295},
  shortjournal = {Image Vis. Comput.},
  title        = {Background debiased class incremental learning for video action recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MATNet: Multilevel attention-based transformers for change
detection in remote sensing images. <em>ICV</em>, <em>151</em>, 105294.
(<a href="https://doi.org/10.1016/j.imavis.2024.105294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image change detection is crucial for natural disaster monitoring and land use change. As the resolution increases, the scenes covered by remote sensing images become more complex, and traditional methods have difficulties in extracting detailed information. With the development of deep learning, the field of change detection has new opportunities. However, existing algorithms mainly focus on the difference analysis between bi-temporal images, while ignoring the semantic information between images, resulting in the global and local information not being able to interact effectively. In this paper, we introduce a new transformer-based multilevel attention network (MATNet), which is capable of extracting multilevel features of global and local information, enabling information interaction and fusion, and thus modeling the global context more effectively. Specifically, we extract multilevel semantic features through the Transformer encoder, and utilize the Feature Enhancement Module (FEM) to perform feature summing and differencing on the multilevel features in order to better extract the local detail information, and thus better detect the changes in small regions. In addition, we employ a multilevel attention decoder (MAD) to obtain information in spatial and spectral dimensions, which can effectively fuse global and local information. In experiments, our method performs excellently on CDD, DSIFN-CD, LEVIR-CD, and SYSU-CD datasets, with F1 scores and OA reaching 95.67%∕87.75%∕90.94%∕86.82% and 98.95%∕95.93%∕99.11%∕90.53% respectively.},
  archive      = {J_ICV},
  author       = {Zhongyu Zhang and Shujun Liu and Yingxiang Qin and Huajun Wang},
  doi          = {10.1016/j.imavis.2024.105294},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105294},
  shortjournal = {Image Vis. Comput.},
  title        = {MATNet: Multilevel attention-based transformers for change detection in remote sensing images},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph construction in hyperbolic space for
automatic image annotation. <em>ICV</em>, <em>151</em>, 105293. (<a
href="https://doi.org/10.1016/j.imavis.2024.105293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic image annotation (AIA) is a fundamental and challenging task in computer vision. Considering the correlations between tags can lead to more accurate image understanding, benefiting various applications, including image retrieval and visual search. While many attempts have been made to incorporate tag correlations in annotation models, the method of constructing a knowledge graph based on external knowledge sources and hyperbolic space has not been explored. In this paper, we create an attributed knowledge graph based on vocabulary, integrate external knowledge sources such as WordNet, and utilize hyperbolic word embeddings for the tag representations. These embeddings provide a sophisticated tag representation that captures hierarchical and complex correlations more effectively, enhancing the image annotation results. In addition, leveraging external knowledge sources enhances contextuality and significantly enriches existing AIA datasets. We exploit two deep learning-based models, the Relational Graph Convolutional Network (R-GCN) and the Vision Transformer (ViT), to extract the input features. We apply two R-GCN operations to obtain word descriptors and fuse them with the extracted visual features. We evaluate the proposed approach using three public benchmark datasets. Our experimental results demonstrate that the proposed architecture achieves state-of-the-art performance across most metrics on Corel5k, ESP Game, and IAPRTC-12.},
  archive      = {J_ICV},
  author       = {Fariba Lotfi and Mansour Jamzad and Hamid Beigy and Helia Farhood and Quan Z. Sheng and Amin Beheshti},
  doi          = {10.1016/j.imavis.2024.105293},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105293},
  shortjournal = {Image Vis. Comput.},
  title        = {Knowledge graph construction in hyperbolic space for automatic image annotation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated grading of diabetic retinopathy and radiomics
analysis on ultra-wide optical coherence tomography angiography scans.
<em>ICV</em>, <em>151</em>, 105292. (<a
href="https://doi.org/10.1016/j.imavis.2024.105292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR), a progressive condition due to diabetes that can lead to blindness, is typically characterized by a number of stages, including non-proliferative (mild, moderate and severe) and proliferative DR. These stages are marked by various vascular abnormalities, such as intraretinal microvascular abnormalities (IRMA), neovascularization (NV), and non-perfusion areas (NPA). Automated detection of these abnormalities and grading the severity of DR are crucial for computer-aided diagnosis. Ultra-wide optical coherence tomography angiography (UW-OCTA) images, a type of retinal imaging, are particularly well-suited for analyzing vascular abnormalities due to their prominence on these images. However, accurate detection of abnormalities and subsequent grading of DR is quite challenging due to noisy data, presence of artifacts, poor contrast and subtle nature of abnormalities. In this work, we aim to develop an automated method for accurate grading of DR severity on UW-OCTA images. Our method consists of various components such as UW-OCTA scan quality assessment, segmentation of vascular abnormalities and grading the scans for DR severity. Applied on publicly available data from Diabetic retinopathy analysis challenge (DRAC 2022), our method shows promising results with a Dice overlap metric and recall values of 0.88 for abnormality segmentation, and the coefficient-of-agreement ( κ κ ) value of 0.873 for DR grading. We also performed a radiomics analysis, and observed that the radiomics features are significantly different for increasing levels of DR severity. This suggests that radiomics could be used for multimodal grading and further analysis of DR, indicating its potential scope in this area.},
  archive      = {J_ICV},
  author       = {Vivek Noel Soren and H.S. Prajwal and Vaanathi Sundaresan},
  doi          = {10.1016/j.imavis.2024.105292},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105292},
  shortjournal = {Image Vis. Comput.},
  title        = {Automated grading of diabetic retinopathy and radiomics analysis on ultra-wide optical coherence tomography angiography scans},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-performance mitosis detection using single-level
feature and hybrid label assignment. <em>ICV</em>, <em>151</em>, 105291.
(<a href="https://doi.org/10.1016/j.imavis.2024.105291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mitosis detection poses a significant challenge in medical image analysis, primarily due to the substantial variability in the appearance and shape of mitotic targets. This paper introduces an efficient and accurate mitosis detection framework, which stands apart from previous mitosis detection techniques with its two key features: Single-Level Feature (SLF) for bounding box prediction and Dense-Sparse Hybrid Label Assignment (HLA) for bounding box matching. The SLF component of our method employs a multi-scale Transformer backbone to capture the global context and morphological characteristics of both mitotic and non-mitotic cells. This information is then consolidated into a single-scale feature map, thereby enhancing the model&#39;s receptive field and reducing redundant detection across various feature maps. In the HLA component, we propose a hybrid label assignment strategy to facilitate the model&#39;s adaptation to mitotic cells of different shapes and positions during training, thereby improving the model&#39;s adaptability to diverse cell morphologies. Our method has been tested on the largest mitosis detection datasets and achieves state-of-the-art (SOTA) performance, with an F1 score of 0.782 on the TUPAC 16 benchmark, and 0.792 with test time augmentation (TTA). Our method also exhibits superior accuracy and faster processing speed compared to previous methods. The source code and pretrained models will be released to facilitate related research.},
  archive      = {J_ICV},
  author       = {Jiangxiao Han and Shikang Wang and Xianbo Deng and Wenyu Liu},
  doi          = {10.1016/j.imavis.2024.105291},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105291},
  shortjournal = {Image Vis. Comput.},
  title        = {High-performance mitosis detection using single-level feature and hybrid label assignment},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-channel network based on occlusion feature
compensation for human pose estimation. <em>ICV</em>, <em>151</em>,
105290. (<a href="https://doi.org/10.1016/j.imavis.2024.105290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is an important technique in computer vision. Existing methods perform well in ideal environments, but there is room for improvement in occluded environments. The specific reasons are that the ambiguity of the features in the occlusion area makes the network pay insufficient attention to it, and the inadequate expressive ability of the features in the occlusion part cannot describe the true keypoint features. To address the occlusion issue, we propose a dual-channel network based on occlusion feature compensation. The dual channels are occlusion area enhancement channel based on convolution and occlusion feature compensation channel based on graph convolution, respectively. In the convolution channel, we propose an occlusion handling enhanced attention mechanism (OHE-attention) to improve the attention to the occlusion area. In the graph convolution channel, we propose a node feature compensation module that eliminates the obstacle features and integrates the shared and private attributes of the keypoints to improve the expressive ability of the node features. We conduct experiments on the COCO2017 dataset, COCO-Wholebody dataset, and CrowdPose dataset, achieving accuracy of 78.7%, 66.4%, and 77.9%, respectively. In addition, a series of ablation experiments and visualization demonstrations verify the performance of the dual-channel network in occluded environments.},
  archive      = {J_ICV},
  author       = {Jiahong Jiang and Nan Xia},
  doi          = {10.1016/j.imavis.2024.105290},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105290},
  shortjournal = {Image Vis. Comput.},
  title        = {A dual-channel network based on occlusion feature compensation for human pose estimation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Landmark-in-facial-component: Towards occlusion-robust
facial landmark localization. <em>ICV</em>, <em>151</em>, 105289. (<a
href="https://doi.org/10.1016/j.imavis.2024.105289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite great efforts in recent years to research robust facial landmark localization methods, occlusion remains a challenge. To tackle this challenge, we propose a model called the Landmark-in-Facial-Component Network (LFCNet). Unlike mainstream models that focus on boundary information, LFCNet utilizes the strong structural constraints inherent in facial anatomy to address occlusion. Specifically, two key modules are designed, a component localization module and an offset localization module. After grouping landmarks based on facial components, the component localization module accomplishes coarse localization of facial components. Offset localization module performs fine localization of landmarks based on the coarse localization results, which can also be seen as delineating the shape of facial components. These two modules form a coarse-to-fine localization pipeline and can also enable LFCNet to better learn the shape constraint of human faces, thereby enhancing LFCNet&#39;s robustness to occlusion. LFCNet achieves 4.82% normalized mean error on occlusion subset of WFLW dataset and 6.33% normalized mean error on Masked 300W dataset. The results demonstrate that LFCNet achieves excellent performance in comparison to state-of-the-art methods, especially on occlusion datasets.},
  archive      = {J_ICV},
  author       = {Xiaoqiang Li and Kaiyuan Wu and Shaohua Zhang},
  doi          = {10.1016/j.imavis.2024.105289},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105289},
  shortjournal = {Image Vis. Comput.},
  title        = {Landmark-in-facial-component: Towards occlusion-robust facial landmark localization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing inherent bias for memory efficient continual
learning: A simple and robust baseline. <em>ICV</em>, <em>151</em>,
105288. (<a href="https://doi.org/10.1016/j.imavis.2024.105288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from continuously evolving data is critical in real-world applications. This type of learning, known as Continual Learning (CL), aims to assimilate new information without compromising performance on prior knowledge. However, learning new information leads to a bias in the network towards recent observations, resulting in a phenomenon known as catastrophic forgetting. The complexity increases in Online Continual Learning (OCL) scenarios where models are allowed only a single pass over data. Existing OCL approaches that rely on replaying exemplar sets are not only memory-intensive when it comes to large-scale datasets but also raise security concerns. While recent dynamic network models address memory concerns, they often present computationally demanding, over-parameterized solutions with limited generalizability. To address this longstanding problem, we propose a novel OCL approach termed “Bias Robust online Continual Learning (BRCL).” BRCL retains all intermediate models generated. These models inherently exhibit a preference for recently learned classes. To leverage this property for enhanced performance, we devise a strategy we describe as ‘utilizing bias to counteract bias.’ This method involves the development of an Inference function that capitalizes on the inherent biases of each model towards the recent tasks. Furthermore, we integrate a model consolidation technique that aligns the first layers of these models, particularly focusing on similar feature representations. This process effectively reduces the memory requirement, ensuring a low memory footprint. Despite the simplicity of the methodology to guarantee expandability to various frameworks, extensive experiments reveal a notable performance edge over leading methods on key benchmarks, getting continual learning closer to matching offline training. (Source code will be made publicly available upon the publication of this paper.)},
  archive      = {J_ICV},
  author       = {Neela Rahimi and Ming Shao},
  doi          = {10.1016/j.imavis.2024.105288},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105288},
  shortjournal = {Image Vis. Comput.},
  title        = {Utilizing inherent bias for memory efficient continual learning: A simple and robust baseline},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PTPFusion: A progressive infrared and visible image fusion
network based on texture preserving. <em>ICV</em>, <em>151</em>, 105287.
(<a href="https://doi.org/10.1016/j.imavis.2024.105287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to provide a more comprehensive image for downstream tasks by highlighting the main target and maintaining rich texture information. Image fusion methods based on deep learning suffer from insufficient multimodal information extraction and texture loss. In this paper, we propose a texture-preserving progressive fusion network (PTPFusion) to extract complementary information from multimodal images to solve these issues. To reduce image texture loss, we design multiple consecutive texture-preserving blocks (TPB) to enhance fused texture. The TPB can enhance the features by using a parallel architecture consisting of a residual block and derivative operators. In addition, a novel cross-channel attention (CCA) fusion module is developed to obtain complementary information by modeling global feature interactions via cross-queries mechanism, followed by information fusion to highlight the feature of the salient target. To avoid information loss, the extracted features at different stages are merged as the output of TPB. Finally, the fused image will be generated by the decoder. Extensive experiments on three datasets show that our proposed fusion algorithm is better than existing state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Yixiang Lu and Weijian Zhang and Dawei Zhao and Yucheng Qian and Davydau Maksim and Qingwei Gao},
  doi          = {10.1016/j.imavis.2024.105287},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105287},
  shortjournal = {Image Vis. Comput.},
  title        = {PTPFusion: A progressive infrared and visible image fusion network based on texture preserving},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning weakly supervised audio-visual violence detection
in hyperbolic space. <em>ICV</em>, <em>151</em>, 105286. (<a
href="https://doi.org/10.1016/j.imavis.2024.105286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the task of weakly supervised audio-visual violence detection has gained considerable attention. The goal of this task is to identify violent segments within multimodal data based on video-level labels. Despite advances in this field, traditional Euclidean neural networks, which have been used in prior research, encounter difficulties in capturing highly discriminative representations due to limitations of the feature space. To overcome this, we propose HyperVD , a novel framework that learns snippet embeddings in hyperbolic space to improve model discrimination. We contribute two branches of fully hyperbolic graph convolutional networks that excavate feature similarities and temporal relationships among snippets in hyperbolic space. By learning snippet representations in this space, the framework effectively learns semantic discrepancies between violent snippets and normal ones. Extensive experiments on the XD-Violence benchmark demonstrate that our method achieves 85.67% AP, outperforming the state-of-the-art methods by a sizable margin.},
  archive      = {J_ICV},
  author       = {Xiao Zhou and Xiaogang Peng and Hao Wen and Yikai Luo and Keyang Yu and Ping Yang and Zizhao Wu},
  doi          = {10.1016/j.imavis.2024.105286},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105286},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning weakly supervised audio-visual violence detection in hyperbolic space},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UIR-ES: An unsupervised underwater image restoration
framework with equivariance and stein unbiased risk estimator.
<em>ICV</em>, <em>151</em>, 105285. (<a
href="https://doi.org/10.1016/j.imavis.2024.105285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imaging faces challenges for enhancing object visibility and restoring true colors due to the absorptive and scattering characteristics of water. Underwater image restoration (UIR) seeks solutions to restore clean images from degraded ones, providing significant utility in downstream tasks. Recently, data-driven UIR has garnered much attention due to the potent expressive capabilities of deep neural networks (DNNs). These DNNs are supervised, relying on a large amount of labeled training samples. However, acquiring such data is expensive or even impossible in real-world underwater scenarios. While recent researches suggest that unsupervised learning is effective in UIR, none of these frameworks consider signal physical priors. In this work, we present a novel physics-inspired unsupervised UIR framework empowered by equivariance and unbiased estimation techniques. Specifically, equivariance stems from the invariance, inherent in natural signals to enhance data-efficient learning. Given that degraded images invariably contain noise, we propose a noise-tolerant loss for unsupervised UIR based on the Stein unbiased risk estimator to achieve an accurate estimation of the data consistency. Extensive experiments on the benchmark UIR datasets, including the UIEB and RUIE datasets, validate the superiority of the proposed method in terms of quantitative scores, visual outcomes, and generalization ability, compared to state-of-the-art counterparts. Moreover, our method demonstrates even comparable performance with the supervised model.},
  archive      = {J_ICV},
  author       = {Jiacheng Zhu and Junjie Wen and Duanqin Hong and Zhanpeng Lin and Wenxing Hong},
  doi          = {10.1016/j.imavis.2024.105285},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105285},
  shortjournal = {Image Vis. Comput.},
  title        = {UIR-ES: An unsupervised underwater image restoration framework with equivariance and stein unbiased risk estimator},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diabetic retinopathy data augmentation and vessel
segmentation through deep learning based three fully convolution neural
networks. <em>ICV</em>, <em>151</em>, 105284. (<a
href="https://doi.org/10.1016/j.imavis.2024.105284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem The eye fundus imaging is used for early diagnosis of most damaging concerns such as diabetic retinopathy, retinal detachments and vascular occlusions. However, the presence of noise, low contrast between background and vasculature during imaging, and vessel morphology lead to uncertain vessel segmentation. Aim This paper proposes a novel retinalblood vessel segmentation method for fundus imaging using a Difference of Gaussian (DoG) filter and an ensemble of three fully convolutional neural network (FCNN) models. Methods A Gaussian filter with standard deviation σ 1 is applied on the preprocessed grayscale fundus image and is subtracted from a similarly applied Gaussian filter with standard deviation σ 2 on the same image. The resultant image is then fed into each of the three fully convolutional neural networks as the input. The FCNN models&#39; output is then passed through a voting classifier, and a final segmented vessel structure is obtained.The Difference of Gaussian filter played an essential part in removing the high frequency details (noise) and thus finely extracted the blood vessels from the retinal fundus with underlying artifacts. Results The total dataset consists of 3832 augmented images transformed from 479 fundus images. The result shows that the proposed method has performed extremely well by achieving an accuracy of 96.50%, 97.69%, and 95.78% on DRIVE, CHASE,and real-time clinical datasets respectively. Conclusion The FCNN ensemble model has demonstrated efficacy in precisely detecting retinal vessels and in the presence of various pathologies and vasculatures.},
  archive      = {J_ICV},
  author       = {Jainy Sachdeva PhD and Puneet Mishra and Deeksha Katoch},
  doi          = {10.1016/j.imavis.2024.105284},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105284},
  shortjournal = {Image Vis. Comput.},
  title        = {Diabetic retinopathy data augmentation and vessel segmentation through deep learning based three fully convolution neural networks},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unbiased scene graph generation via head-tail cooperative
network with self-supervised learning. <em>ICV</em>, <em>151</em>,
105283. (<a href="https://doi.org/10.1016/j.imavis.2024.105283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene Graph Generation (SGG) as a critical task in image understanding, facing the challenge of head-biased prediction caused by the long-tail distribution of predicates. However, current debiased SGG methods can easily prioritize improving the prediction of tail predicates while ignoring the substantial sacrifice of head predicates, leading to a shift from head bias to tail bias. To address this issue, we propose a Head-Tail Cooperative network with self-supervised Learning (HTCL), which achieves unbiased SGG by cooperating head-prefer and tail-prefer predictions through learnable weight parameters. HTCL employs a tail-prefer feature encoder to re-represent predicate features by injecting self-supervised learning, which focuses on the intrinsic structure of features, into the supervised learning of SGG, constraining the representation of predicate features to enhance the distinguishability of tail samples. We demonstrate the effectiveness of our HTCL by applying it to VG150, Open Images V6 and GQA200 datasets. The results show that HTCL achieves higher mean Recall with a minimal sacrifice in Recall and achieves a new state-of-the-art overall performance. Our code is available at https://github.com/wanglei0618/HTCL .},
  archive      = {J_ICV},
  author       = {Lei Wang and Zejian Yuan and Yao Lu and Badong Chen},
  doi          = {10.1016/j.imavis.2024.105283},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105283},
  shortjournal = {Image Vis. Comput.},
  title        = {Unbiased scene graph generation via head-tail cooperative network with self-supervised learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularization by denoising diffusion process meets deep
relaxation in phase. <em>ICV</em>, <em>151</em>, 105282. (<a
href="https://doi.org/10.1016/j.imavis.2024.105282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fourier phase retrieval is one of the representative inverse problems where a signal needs to be recovered using only the measured magnitude of its Fourier transform. Deep learning-based algorithms for solving Fourier phase retrieval have been widely studied. These methods provide better reconstruction than the conventional algorithms, such as alternating projection approaches and convex relaxation methods. However, it is difficult to recover the phase information of 256 × 256 images accurately, and they often cannot provide fine details and textures. Recently, diffusion models have been used to solve Fourier phase retrieval problems. They offer realistic reconstruction results, but due to the nature of generative models, they often create non-existent features in the actual images. To address these issues, we introduced a novel algorithm inspired by regularization by denoising diffusion, a variational diffusion sampling for reconstructing the images from the measurements. In particular, the optimization problem in the convex relaxation approach for phase retrieval is interpreted as an additional constraint during the variational sampling process to estimate the phase from the given Fourier magnitude measurement. The proposed method stands out by leveraging not only pre-trained diffusion models as image priors but also the classical optimization approach as the regularization. This novel combination ensures not just accurate phase reconstruction, but also performance guarantees. Our experiments demonstrate that the proposed algorithm consistently provides state-of-the-art performance across various datasets of 256 × 256 images. We further showed the effectiveness of the new regularization for the performance gain in the phase estimation.},
  archive      = {J_ICV},
  author       = {Eunju Cha},
  doi          = {10.1016/j.imavis.2024.105282},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105282},
  shortjournal = {Image Vis. Comput.},
  title        = {Regularization by denoising diffusion process meets deep relaxation in phase},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dictionary learning based unsupervised neural network for
single image compressed sensing. <em>ICV</em>, <em>151</em>, 105281. (<a
href="https://doi.org/10.1016/j.imavis.2024.105281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of Compressed Sensing (CS), the sparse representation of signals and the advancement of reconstruction algorithms are two critical challenges. However, conventional CS algorithms often fail to sufficiently exploit the structured sparsity present in images and suffer from poor reconstruction quality. Most deep learning-based CS methods are typically trained on large-scale datasets. Obtaining a sufficient number of training sets is challenging in many practical applications and there may be no training sets available at all in some cases. In this paper, a novel deep Dictionary Learning (DL) based unsupervised neural network for single image CS (dubbed DL-CSNet) is proposed. It is an effective trainless neural network that consists of three components and their corresponding loss functions: 1) a DL layer that consists of multi-layer perceptron (MLP) and convolution neural networks (CNN) for latent sparse features extraction with the L1-norm sparsity loss function; 2) an image smoothing layer with the Total Variation (TV) like image smoothing loss function; and 3) a CS acquisition layer for image compression, with the Mean Square Error (MSE) loss function between the original image compression and the reconstructed image compression. In particular, the proposed DL-CSNet is a lightweight and fast model that does not require datasets for training and exhibits a fast convergence speed, making it suitable for deployment in resource-constrained environments. Experiments have demonstrated that the proposed DL-CSNet achieves superior performance compared to traditional CS methods and other unsupervised state-of-the-art deep learning-based CS methods.},
  archive      = {J_ICV},
  author       = {Kuang Luo and Lu Ou and Ming Zhang and Shaolin Liao and Chuangfeng Zhang},
  doi          = {10.1016/j.imavis.2024.105281},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105281},
  shortjournal = {Image Vis. Comput.},
  title        = {A dictionary learning based unsupervised neural network for single image compressed sensing},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ground4Act: Leveraging visual-language model for
collaborative pushing and grasping in clutter. <em>ICV</em>,
<em>151</em>, 105280. (<a
href="https://doi.org/10.1016/j.imavis.2024.105280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge in robotics is to enable robots to transition from visual perception and language understanding to performing tasks such as grasp and assembling objects, bridging the gap between “seeing” and “hearing” to “doing”. In this work, we propose Ground4Act, a two-stage approach for collaborative pushing and grasping in clutter using a visual-language model. In the grounding stage, Ground4Act extracts target features from multi-modal data via visual grounding. In the action stage, it embeds a collaborative pushing and grasping framework to generate the action&#39;s position and direction. Specifically, we propose a DQN-based reinforcement learning pushing policy that uses RGBD images as the state space to determine the push action&#39;s pixel-level coordinates and direction. Additionally, a least squares-based linear fitting grasping policy takes the target mask from the grounding stage as input to achieve efficient grasp. Simulations and real-world experiments demonstrate Ground4Act&#39;s superior performance. The simulation suite, source code, and trained models will be made publicly available.},
  archive      = {J_ICV},
  author       = {Yuxiang Yang and Jiangtao Guo and Zilong Li and Zhiwei He and Jing Zhang},
  doi          = {10.1016/j.imavis.2024.105280},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105280},
  shortjournal = {Image Vis. Comput.},
  title        = {Ground4Act: Leveraging visual-language model for collaborative pushing and grasping in clutter},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new deepfake detection model for responding to perception
attacks in embodied artificial intelligence. <em>ICV</em>, <em>151</em>,
105279. (<a href="https://doi.org/10.1016/j.imavis.2024.105279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied artificial intelligence (AI) represents a new generation of robotics technology combined with artificial intelligence, and it is at the forefront of current research. To reduce the impact of deepfake technology on embodied perception and enhance the security and reliability of embodied AI, this paper proposes a novel deepfake detection model with a new Balanced Contrastive Learning strategy, named BCL. By integrating unsupervised contrastive learning and supervised contrastive learning with deepfake detection, the model effectively extracts the underlying features of fake images from both individual level and category level, thereby leading to more discriminative features. In addition, a Multi-scale Attention Interaction module (MAI) is proposed to enrich the representative ability of features. By cross-fusing the semantic features of different receptive fields of the encoder, more effective deepfake traces can be mined. Finally, extensive experiments demonstrate that our method has good performance and generalization capabilities across intra-dataset, cross-dataset and cross-manipulation scenarios.},
  archive      = {J_ICV},
  author       = {JunShuai Zheng and XiYuan Hu and Chen Chen and YiChao Zhou and DongYang Gao and ZhenMin Tang},
  doi          = {10.1016/j.imavis.2024.105279},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105279},
  shortjournal = {Image Vis. Comput.},
  title        = {A new deepfake detection model for responding to perception attacks in embodied artificial intelligence},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A learnable motion preserving pooling for action
recognition. <em>ICV</em>, <em>151</em>, 105278. (<a
href="https://doi.org/10.1016/j.imavis.2024.105278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using deep neural networks (DNN) for video understanding tasks is expensive in terms of computation cost. Pooling layers in DNN which are widely used in most vision tasks to resize the spatial dimensions play crucial roles in reducing the computation and memory cost. In video-related tasks, pooling layers are also applied, mostly in the spatial dimension only as the standard average pooling in the temporal domain can significantly reduce its performance. This is because conventional temporal pooling degrades the underlying important motion features in consecutive frames. Such a phenomenon is rarely investigated and most state-of-art methods simply do not adopt temporal pooling, leading to enormous computation costs. In this work, we propose a learnable motion-preserving pooling (MPPool) layer that is able to preserve the general motion progression after the pooling. This pooling layer first locates the frames with the strongest motion features and then keeps these crucial features during pooling. Our experiments demonstrate that MPPool not only reduces the computation cost for video data modeling, but also increases the final prediction accuracy on various motion-centric and appearance-centric datasets.},
  archive      = {J_ICV},
  author       = {Tankun Li and Kwok Leung Chan and Tardi Tjahjadi},
  doi          = {10.1016/j.imavis.2024.105278},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105278},
  shortjournal = {Image Vis. Comput.},
  title        = {A learnable motion preserving pooling for action recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFEDC: Dual fusion with enhanced deformable convolution for
medical image segmentation. <em>ICV</em>, <em>151</em>, 105277. (<a
href="https://doi.org/10.1016/j.imavis.2024.105277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the complexity of lesion regions in medical images, current researches relying on CNNs typically employ large-kernel convolutions to expand the receptive field and enhance segmentation quality. However, these convolution methods are hindered by substantial computational requirements and limited capacity to extract contextual and multi-scale information, making it challenging to efficiently segment complex regions. To address this issue, we propose a dual fusion with enhanced deformable convolution network, namely DFEDC, which dynamically adjusts the receptive field and simultaneously integrates multi-scale feature information to effectively segment complex lesion areas and process boundaries. Firstly, we combine global channel and spatial fusion in a serial way, which integrates and reuses global channel attention and fully connected layers to achieve lightweight extraction of channel and spatial information. Additionally, we design a structured deformable convolution (SDC) that structures deformable convolution with inceptions and large kernel attention, and enhances the learning of offsets through parallel fusion to efficiently extract multi-scale feature information. To compensate for the loss of spatial information of SDC, we introduce a hybrid 2D and 3D feature extraction module to transform feature extraction from a single dimension to a fusion of 2D and 3D. Extensive experimental results on the Synapse, ACDC, and ISIC-2018 datasets demonstrate that our proposed DFEDC achieves superior results.},
  archive      = {J_ICV},
  author       = {Xian Fang and Yueqian Pan and Qiaohong Chen},
  doi          = {10.1016/j.imavis.2024.105277},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105277},
  shortjournal = {Image Vis. Comput.},
  title        = {DFEDC: Dual fusion with enhanced deformable convolution for medical image segmentation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LVD-YOLO: An efficient lightweight vehicle detection model
for intelligent transportation systems. <em>ICV</em>, <em>151</em>,
105276. (<a href="https://doi.org/10.1016/j.imavis.2024.105276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle detection is a fundamental component of intelligent transportation systems. However, current algorithms often encounter issues such as high computational complexity, long execution times, and significant resource demands, making them unsuitable for resource-limited environments. To overcome these challenges, we propose LVD-YOLO, a Lightweight Vehicle Detection Model based on YOLO. This model incorporates the EfficientNetv2 network structure as its backbone, which reduces parameters and enhances feature extraction capabilities. By utilizing a bidirectional feature pyramid structure along with a dual attention mechanism, we enable efficient information exchange across feature layers, thereby improving multiscale feature fusion. Additionally, we refine the model&#39;s loss function with SIoU loss to boost regression and prediction performance. Experimental results on the PASCAL VOC and MS COCO datasets show that LVD-YOLO outperforms YOLOv5s, achieving a 0.5% increase in accuracy while reducing FLOPs by 64.6% and parameters by 48.6%. These improvements highlight its effectiveness for use in resource-constrained environments.},
  archive      = {J_ICV},
  author       = {Hao Pan and Shaopeng Guan and Xiaoyan Zhao},
  doi          = {10.1016/j.imavis.2024.105276},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105276},
  shortjournal = {Image Vis. Comput.},
  title        = {LVD-YOLO: An efficient lightweight vehicle detection model for intelligent transportation systems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Corrigendum to “a method of degradation mechanism-based
unsupervised remote sensing image super-resolution” [image and vision
computing, vol 148 (2024), 105108]. <em>ICV</em>, <em>151</em>, 105275.
(<a href="https://doi.org/10.1016/j.imavis.2024.105275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Zhikang Zhao and Yongcheng Wang and Ning Zhang and Yuxi Zhang and Zheng Li and Chi Chen},
  doi          = {10.1016/j.imavis.2024.105275},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105275},
  shortjournal = {Image Vis. Comput.},
  title        = {Corrigendum to “A method of degradation mechanism-based unsupervised remote sensing image super-resolution” [Image and vision computing, vol 148 (2024), 105108]},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AGIL-SwinT: Attention-guided inconsistency learning for face
forgery detection. <em>ICV</em>, <em>151</em>, 105274. (<a
href="https://doi.org/10.1016/j.imavis.2024.105274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face forgery detection (FFD) plays a vital role in maintaining the security and integrity of various information and media systems. Forgery inconsistency caused by manipulation techniques has been proven to be effective for generalizing to the unseen data domain. However, most existing works rely on pixel-level forgery annotations to learn forgery inconsistency. To address the problem, we propose a novel Swin Transformer-based method, AGIL-SwinT, that can effectively learn forgery inconsistency using only video-level labels. Specifically, we first leverage the Swin Transformer to generate the initial mask for the forgery regions. Then, we introduce an attention-guided inconsistency learning module that uses unsupervised learning to learn inconsistency from attention. The learned inconsistency is used to revise the initial mask for enhancing forgery detection. In addition, we introduce a forgery mask refinement module to obtain reliable inconsistency labels for supervising inconsistency learning and ensuring the mask is aligned with the forgery boundaries. We conduct extensive experiments on multiple FFD benchmarks, including intra-dataset, cross-dataset and cross-manipulation testing. The experimental results demonstrate that our method significantly outperforms existing methods and generalizes well to unseen datasets and manipulation categories. Our code is available at https://github.com/woody-xiong/AGIL-SwinT .},
  archive      = {J_ICV},
  author       = {Wuti Xiong and Haoyu Chen and Guoying Zhao and Xiaobai Li},
  doi          = {10.1016/j.imavis.2024.105274},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105274},
  shortjournal = {Image Vis. Comput.},
  title        = {AGIL-SwinT: Attention-guided inconsistency learning for face forgery detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRFormer: A cross-region transformer for shadow removal.
<em>ICV</em>, <em>151</em>, 105273. (<a
href="https://doi.org/10.1016/j.imavis.2024.105273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image shadow removal is a fundamental task in computer vision, which aims to restore damaged signals caused by shadows, thereby improving image quality and scene understanding. Recently, transformers have demonstrated strong capabilities in various applications by capturing global pixel interactions, a capability highly desirable for shadow removal. However, applying transformers to promote shadow removal is non-trivial for the following two reasons: 1) The patchify operation is not suitable for shadow removal due to irregular shadow shapes; 2) Shadow removal only requires one-way interaction from the non-shadow region to the shadow region instead of the common two-way interactions among all pixels in the image. In this paper, we propose a novel C ross- R egion trans F ormer (CRFormer) for shadow removal which differs from existing transformers by only considering the pixel interactions from the non-shadow region to the shadow region without splitting images into patches. This is achieved by a carefully designed region-aware cross-attention mechanism that aggregates the recovered shadow region features, conditioned on the non-shadow region features. Extensive experiments on the ISTD, AISTD, SRD, and Video Shadow Removal datasets demonstrate the superiority of our method compared to other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jin Wan and Hui Yin and Zhenyao Wu and Xinyi Wu and Zhihao Liu and Song Wang},
  doi          = {10.1016/j.imavis.2024.105273},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105273},
  shortjournal = {Image Vis. Comput.},
  title        = {CRFormer: A cross-region transformer for shadow removal},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting certified robustness via an expectation-based
similarity regularization. <em>ICV</em>, <em>151</em>, 105272. (<a
href="https://doi.org/10.1016/j.imavis.2024.105272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A certifiably robust classifier implies the one that is theoretically guaranteed to provide robust predictions against any adversarial attacks under certain conditions. Recent defense methods aim to regularize predictions by ensuring consistency across diverse perturbed samplings around the same sample, thus enhancing the certified robustness of the classifier. However, starting from the visualization of latent representations from classifiers trained with existing defense methods, we observe that noisy samplings of other classes are still easily found near a single sample, undermining the confidence in the neighborhood of inputs required by the certified robustness. Motivated by this observation, a novel training method, namely Expectation-based Similarity Regularization for Randomized Smoothing (ESR-RS), is proposed to optimize the distance between samples utilizing metric learning. To meet the requirement of certified robustness, ESR-RS focuses on the average performance of base classifier, and adopts the expected feature approximated by the average value of multiple Gaussian-corrupted samplings around every sample, to compute similarity scores between samples in the latent space. The metric learning loss is then applied to maximize the representation similarity within the same class and minimize it between different classes. Besides, an adaptive weight correlated with the classification performance is used to control the strength of the proposed similarity regularization. Extensive experiments have verified that our method contributes to stronger certified robustness over multiple defense methods without heavy computational costs.},
  archive      = {J_ICV},
  author       = {Jiawen Li and Kun Fang and Xiaolin Huang and Jie Yang},
  doi          = {10.1016/j.imavis.2024.105272},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105272},
  shortjournal = {Image Vis. Comput.},
  title        = {Boosting certified robustness via an expectation-based similarity regularization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RFSC-net: Re-parameterization forward semantic compensation
network in low-light environments. <em>ICV</em>, <em>151</em>, 105271.
(<a href="https://doi.org/10.1016/j.imavis.2024.105271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although detectors currently perform well in well-light conditions, their accuracy decreases due to insufficient object information. In addressing this issue, we propose the Re-parameterization Forward Semantic Compensation Network (RFSC-Net). We propose the Reparameterization Residual Efficient Layer Aggregation Networks (RSELAN) for feature extraction, which integrates the concepts of re-parameterization and the Efficient Layer Aggregation Networks (ELAN). While focusing on the fusion of feature maps of the same dimension, it also incorporates upward fusion of lower-level feature maps, enhancing the detailed texture information in higher-level features. Our proposed Forward Semantic Compensation Feature Fusion (FSCFF) network reduces interference from high-level to low-level semantic information, retaining finer details to improve detection accuracy in low-light conditions. Experiments on the low-light ExDark and DarkFace datasets show that RFSC-Net improves mAP by 2% on ExDark and 0.5% on DarkFace over the YOLOv8n baseline, without an increase in parameter counts. Additionally, AP50 is enhanced by 2.1% on ExDark and 1.1% on DarkFace, with a mere 3.7 ms detection latency on ExDark.},
  archive      = {J_ICV},
  author       = {Wenhao Zhang and Huiying Xu and Xinzhong Zhu and Yunzhong Si and Yao Dong and Xiao Huang and Hongbo Li},
  doi          = {10.1016/j.imavis.2024.105271},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105271},
  shortjournal = {Image Vis. Comput.},
  title        = {RFSC-net: Re-parameterization forward semantic compensation network in low-light environments},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAV image object detection based on self-attention guidance
and global feature fusion. <em>ICV</em>, <em>151</em>, 105262. (<a
href="https://doi.org/10.1016/j.imavis.2024.105262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) image object detection has garnered considerable attentions in fields such as Intelligent transportation, urban management and agricultural monitoring. However, it suffers from key challenges of the deficiency in multi-scale feature extraction and the inaccuracy when processing complex scenes and small-sized targets in practical applications. To address this challenge, we propose a novel UAV image object detection network based on self-attention guidance and global feature fusion, named SGGF-Net. First, in order to optimizing feature extraction in global perspective and enhancing target localization precision, the global feature extraction module (GFEM) is introduced by exploiting the self-attention mechanism to capture and integrate long-range dependencies within images. Second, a normal distribution-based prior assigner (NDPA) is developed by measuring the resemblance between ground truth and the priors, which improves the precision of target position matching and thus handle the problem of inaccurate localization of small targets. Furthermore, we design an attention-guided ROI pooling module (ARPM) via a deep fusion strategy of multilevel features for optimizing the integration of multi-scale features and improving the quality of feature representation. Finally, experimental results demonstrate the effectiveness of the proposed SGGF-Net approach.},
  archive      = {J_ICV},
  author       = {Jing Bai and Haiyang Hu and Xiaojing Liu and Shanna Zhuang and Zhengyou Wang},
  doi          = {10.1016/j.imavis.2024.105262},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105262},
  shortjournal = {Image Vis. Comput.},
  title        = {UAV image object detection based on self-attention guidance and global feature fusion},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic segmentation of deep endometriosis in the
rectosigmoid using deep learning. <em>ICV</em>, <em>151</em>, 105261.
(<a href="https://doi.org/10.1016/j.imavis.2024.105261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Endometriosis is an inflammatory disease that causes several symptoms, such as infertility and constant pain. While biopsy remains the gold standard for diagnosing endometriosis, imaging tests, particularly magnetic resonance, are becoming increasingly prominent, especially in cases of deep infiltrating disease. However, precise and accurate MRI results require a skilled radiologist. In this study, we employ our built dataset to propose an automated method for classifying patients with endometriosis and segmenting the endometriosis lesion in magnetic resonance images of the rectum and sigmoid colon using image processing and deep learning techniques. Our goals are to assist in the diagnosis, to map the extent of the disease before a surgical procedure, and to help reduce the need for invasive diagnostic methods. This method consists of the following steps: rectosigmoid ROI extraction, image classification, initial lesion segmentation, lesion ROI extraction, and final lesion segmentation. ROI extraction is employed to limit the area while searching for lesions. Using an ensemble of networks, classification of images and patients, with or without endometriosis, achieved accuracies of 87.46% and 96.67%, respectively. One of these networks is a proposed modification of VGG-16. The initial segmentation step produces candidate regions for lesions using TransUnet, achieving a Dice index of 51%. These regions serve as the basis for extracting a new ROI. In the final lesion segmentation, and also using TransUnet, we obtain a Dice index of 65.44%.},
  archive      = {J_ICV},
  author       = {Weslley Kelson Ribeiro Figueredo and Aristófanes Corrêa Silva and Anselmo Cardoso de Paiva and João Otávio Bandeira Diniz and Alice Brandão and Marco Aurelio Pinho Oliveira},
  doi          = {10.1016/j.imavis.2024.105261},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105261},
  shortjournal = {Image Vis. Comput.},
  title        = {Automatic segmentation of deep endometriosis in the rectosigmoid using deep learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFCNet +: Cross-modal dynamic feature contrast net for
continuous sign language recognition. <em>ICV</em>, <em>151</em>,
105260. (<a href="https://doi.org/10.1016/j.imavis.2024.105260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sign language communication, the combination of hand signs and facial expressions is used to convey messages in a fluid manner. Accurate interpretation relies heavily on understanding the context of these signs. Current methods, however, often focus on static images, missing the continuous flow and the story that unfolds through successive movements in sign language. To address this constraint, our research introduces the Dynamic Feature Contrast Net Plus (DFCNet +), a novel model that incorporates both dynamic feature extraction and cross-modal learning. The dynamic feature extraction module of DFCNet + uses dynamic trajectory capture to monitor and record motion across frames and apply key features as an enhancement tool that highlights pixels that are critical for recognizing important sign language movements, allowing the model to follow the temporal variation of the signs. In the cross-modal learning module, we depart from the conventional approach of aligning video frames with textual descriptions. Instead, we adopt a gloss-level alignment, which provides a more detailed match between the visual signals and their corresponding text glosses, capturing the intricate relationship between what is seen and the associated text. The enhanced proficiency of DFCNet + in discerning inter-frame details translates to heightened precision on benchmarks such as PHOENIX14, PHOENIX14-T and CSL-Daily. Such performance underscores its advantage in dynamic feature capture and inter-modal learning compared to conventional approaches to sign language interpretation. Our code is available at https://github.com/fyzjut/DFCNet_Plus .},
  archive      = {J_ICV},
  author       = {Yuan Feng and Nuoyi Chen and Yumeng Wu and Caoyu Jiang and Sheng Liu and Shengyong Chen},
  doi          = {10.1016/j.imavis.2024.105260},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105260},
  shortjournal = {Image Vis. Comput.},
  title        = {DFCNet +: Cross-modal dynamic feature contrast net for continuous sign language recognition},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLAI: Exploration and exploitation based on visual-language
aligned information for robotic object goal navigation. <em>ICV</em>,
<em>151</em>, 105259. (<a
href="https://doi.org/10.1016/j.imavis.2024.105259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object Goal Navigation(ObjectNav) is the task that an agent need navigate to an instance of a specific category in an unseen environment through visual observations within limited time steps. This work plays a significant role in enhancing the efficiency of locating specific items in indoor spaces and assisting individuals in completing various tasks, as well as providing support for people with disabilities. To achieve efficient ObjectNav in unfamiliar environments, global perception capabilities, understanding the regularities of space and semantics in the environment layout are significant. In this work, we propose an explicit-prediction method called VLAI that utilizes visual-language alignment information to guide the agent&#39;s exploration, unlike previous navigation methods based on frontier potential prediction or egocentric map completion, which only leverage visual observations to construct semantic maps, thus failing to help the agent develop a better global perception. Specifically, when predicting long-term goals, we retrieve previously saved visual observations to obtain visual information around the frontiers based on their position on the incrementally built incomplete semantic map. Then, we apply our designed Chat Describer to this visual information to obtain detailed frontier object descriptions. The Chat Describer, a novel automatic-questioning approach deployed in Visual-to-Language, is composed of Large Language Model(LLM) and the visual-to-language model(VLM), which has visual question-answering functionality. In addition, we also obtain the semantic similarity of target object and frontier object categories. Ultimately, by combining the semantic similarity and the boundary descriptions, the agent can predict the long-term goals more accurately. Our experiments on the Gibson and HM3D datasets reveal that our VLAI approach yields significantly better results compared to earlier methods. The code is released at https://github.com/31539lab/VLAI .},
  archive      = {J_ICV},
  author       = {Haonan Luo and Yijie Zeng and Li Yang and Kexun Chen and Zhixuan Shen and Fengmao Lv},
  doi          = {10.1016/j.imavis.2024.105259},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105259},
  shortjournal = {Image Vis. Comput.},
  title        = {VLAI: Exploration and exploitation based on visual-language aligned information for robotic object goal navigation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic deep spare clustering with a dynamic
population-based evolutionary algorithm using reinforcement learning and
transfer learning. <em>ICV</em>, <em>151</em>, 105258. (<a
href="https://doi.org/10.1016/j.imavis.2024.105258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering data effectively remains a significant challenge in machine learning, particularly when the optimal number of clusters is unknown. Traditional deep clustering methods often struggle with balancing local and global search, leading to premature convergence and inefficiency. To address these issues, we introduce ADSC-DPE-RT (Automatic Deep Sparse Clustering with a Dynamic Population-based Evolutionary Algorithm using Reinforcement Learning and Transfer Learning), a novel deep clustering approach. ADSC-DPE-RT builds on Multi-Trial Vector-based Differential Evolution (MTDE), an algorithm that integrates sparse auto-encoding and manifold learning to enable automatic clustering without prior knowledge of cluster count. However, MTDE&#39;s fixed population size can lead to either prolonged computation or premature convergence. Our approach introduces a dynamic population generation technique guided by Reinforcement Learning (RL) and Markov Decision Process (MDP) principles. This allows for flexible adjustment of population size, preventing premature convergence and reducing computation time. Additionally, we incorporate Generative Adversarial Networks (GANs) to facilitate dynamic knowledge transfer between MTDE strategies, enhancing diversity and accelerating convergence towards the global optimum. This is the first work to address the dynamic population issue in deep clustering through RL, combined with Transfer Learning to optimize evolutionary algorithms. Our results demonstrate significant improvements in clustering performance, positioning ADSC-DPE-RT as a competitive alternative to state-of-the-art deep clustering methods.},
  archive      = {J_ICV},
  author       = {Parham Hadikhani and Daphne Teck Ching Lai and Wee-Hong Ong and Mohammad H. Nadimi-Shahraki},
  doi          = {10.1016/j.imavis.2024.105258},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105258},
  shortjournal = {Image Vis. Comput.},
  title        = {Automatic deep spare clustering with a dynamic population-based evolutionary algorithm using reinforcement learning and transfer learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nighttime scene understanding with label transfer scene
parser. <em>ICV</em>, <em>151</em>, 105257. (<a
href="https://doi.org/10.1016/j.imavis.2024.105257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation plays a crucial role in traffic scene understanding, especially in nighttime conditions. This paper tackles the task of semantic segmentation in nighttime scenes. The largest challenge of this task is the lack of annotated nighttime images to train a deep learning-based scene parser. The existing annotated datasets are abundant in daytime conditions but scarce in nighttime due to the high cost. Thus, we propose a novel Label Transfer Scene Parser (LTSP) framework for nighttime scene semantic segmentation by leveraging daytime annotation transfer. Our framework performs segmentation in the dark without training on real nighttime annotated data. In particular, we propose translating daytime images to nighttime conditions to obtain more data with annotation in an efficient way. In addition, we utilize the pseudo-labels inferred from unlabeled nighttime scenes to further train the scene parser. The novelty of our work is the ability to perform nighttime segmentation via daytime annotated labels and nighttime synthetic versions of the same set of images. The extensive experiments demonstrate the improvement and efficiency of our scene parser over the state-of-the-art methods with a similar semi-supervised approach on the benchmark of Nighttime Driving Test dataset. Notably, our proposed method utilizes only one-tenth of the amount of labeled and unlabeled data in comparison with the previous methods. Code is available at https://github.com/danhntd/Label_Transfer_Scene_Parser.git .},
  archive      = {J_ICV},
  author       = {Thanh-Danh Nguyen and Nguyen Phan and Tam V. Nguyen and Vinh-Tiep Nguyen and Minh-Triet Tran},
  doi          = {10.1016/j.imavis.2024.105257},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105257},
  shortjournal = {Image Vis. Comput.},
  title        = {Nighttime scene understanding with label transfer scene parser},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-branch underwater image enhancement network via
multiscale neighborhood interaction attention learning. <em>ICV</em>,
<em>151</em>, 105256. (<a
href="https://doi.org/10.1016/j.imavis.2024.105256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the light scattering and absorption, underwater images inevitably suffer from diverse quality degradation, including color distortion, low contrast, and blurred details. To address the problems, we present a dual-branch convolutional neural network via multiscale neighborhood interaction attention learning for underwater image enhancement. Specifically, the proposed network is trained by an ensemble of locally-aware and globally-aware branches processed in parallel, where the locally-aware branch with stronger representation ability aims to recover high-frequency local details sufficiently, and the globally-aware branch with weaker learning ability aims to prevent information loss in low-frequency global structure effectively. On the other hand, we develop a plug-and-play multiscale neighborhood interaction attention module, which further enhances image quality through appropriate cross-channel interactions with inputs from different receptive fields. Compared with the well-received methods, extensive experiments on both real-world and synthetic underwater images reveal that our proposed network can achieve superior color and contrast enhancement in terms of subjective visual perception and objective evaluation metrics. Ablation study is also conducted to demonstrate the effectiveness of each component in the network.},
  archive      = {J_ICV},
  author       = {Xun Ji and Xu Wang and Na Leng and Li-Ying Hao and Hui Guo},
  doi          = {10.1016/j.imavis.2024.105256},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105256},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-branch underwater image enhancement network via multiscale neighborhood interaction attention learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight hash-directed global perception and
self-calibrated multiscale fusion network for image super-resolution.
<em>ICV</em>, <em>151</em>, 105255. (<a
href="https://doi.org/10.1016/j.imavis.2024.105255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the increase in the depth and width of convolutional neural networks, single image super-resolution (SISR) algorithms have made significant breakthroughs in objective quantitative metrics and subjective visual quality. However, these operations have inevitably caused model inference time to surge. In order to find a balance between model speed and accuracy, we propose a lightweight hash-directed global perception and self-calibrated multiscale fusion network for image Super-Resolution (HSNet) in this paper. The HSNet makes the following two main improvements: first, the Hash-Directed Global Perception module (HDGP) designed in this paper is able to capture the dependencies between features in a global perspective by using the hash encoding to direct the attention mechanism. Second, the Self-Calibrated Multiscale Fusion module (SCMF) proposed in this paper has two independent task branches: the upper branch of the SCMF utilizes the feature fusion module to capture multiscale contextual information, while the lower branch focuses on local details through a small convolutional kernel. These two branches are fused with each other to effectively enhance the network&#39;s multiscale understanding capability. Extensive experimental results demonstrate the remarkable superiority of our approach over other state-of-the-art methods in both subjective visual effects and objective evaluation metrics, including PSNR, SSIM, and computational complexity.},
  archive      = {J_ICV},
  author       = {Zhisheng Cui and Yibing Yao and Shilong Li and Yongcan Zhao and Ming Xin},
  doi          = {10.1016/j.imavis.2024.105255},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105255},
  shortjournal = {Image Vis. Comput.},
  title        = {A lightweight hash-directed global perception and self-calibrated multiscale fusion network for image super-resolution},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diff-STAR: Exploring student-teacher adaptive reconstruction
through diffusion-based generation for image harmonization.
<em>ICV</em>, <em>151</em>, 105254. (<a
href="https://doi.org/10.1016/j.imavis.2024.105254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image harmonization aims to seamlessly integrate foreground and background elements from distinct photos into a visually realistic composite. However, achieving high-quality image composition remains challenging in adjusting color balance, retaining fine details, and ensuring perceptual consistency. This article introduces a novel approach named Diffusion-based Student-Teacher Adaptive Reconstruction (Diff-STAR) to address foreground adjustment by framing it as an image reconstruction task. Leveraging natural photographs for model pretraining eliminates the need for data augmentation within Diff-STAR&#39;s framework. Employing the pre-trained Denoising Diffusion Implicit Model (DDIM) enhances photorealism and fidelity in generating high-quality outputs from reconstructed latent representations. By effectively identifying similarities in low-frequency style and semantic relationships across various regions within latent images, we develop a student-teacher architecture combining Transformer encoders and decoders to predict adaptively masked patches derived through diffusion processes. Evaluated on the public datasets, including iHarmony4 and RealHM, the experiment results confirm Diff-STAR&#39;s superiority over other state-of-the-art approaches based on metrics including Mean Squared Error (MSE) and Peak Signal-to-noise ratio (PSNR).},
  archive      = {J_ICV},
  author       = {An Cao and Gang Shen},
  doi          = {10.1016/j.imavis.2024.105254},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105254},
  shortjournal = {Image Vis. Comput.},
  title        = {Diff-STAR: Exploring student-teacher adaptive reconstruction through diffusion-based generation for image harmonization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D face alignment through fusion of head pose information
and features. <em>ICV</em>, <em>151</em>, 105253. (<a
href="https://doi.org/10.1016/j.imavis.2024.105253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of humans to infer head poses from face shapes, and vice versa, indicates a strong correlation between them. Recent studies on face alignment used head pose information to predict facial landmarks in computer vision tasks. However, many studies have been limited to using head pose information primarily to initialize mean landmarks, as it cannot represent detailed face shapes. To enhance face alignment performance through effective utilization, we introduce a novel approach that integrates head pose information into the feature maps of a face alignment network, rather than simply using it to initialize facial landmarks. Furthermore, the proposed network structure achieves reliable face alignment through a dual-dimensional network. This structure uses multidimensional features such as 2D feature maps and a 3D heatmap to reduce reliance on a single type of feature map and enrich the feature information. We also propose a dense face alignment method through an appended fully connected layer at the end of a dual-dimensional network, trained with sparse face alignment. This method easily trains dense face alignment by directly using predicted keypoints as knowledge and indirectly using semantic information. We experimentally assessed the correlation between the predicted facial landmarks and head pose information, as well as variations in the accuracy of facial landmarks with respect to the quality of head pose information. In addition, we demonstrated the effectiveness of the proposed method through a competitive performance comparison with state-of-the-art methods on the AFLW2000-3D, AFLW, and BIWI datasets. In the evaluation of the face alignment task, we achieved an NME of 3.21 for the AFLW2000-3D and 3.68 for the AFLW dataset.},
  archive      = {J_ICV},
  author       = {Jaehyun So and Youngjoon Han},
  doi          = {10.1016/j.imavis.2024.105253},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105253},
  shortjournal = {Image Vis. Comput.},
  title        = {3D face alignment through fusion of head pose information and features},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient masked feature and group attention network for
stereo image super-resolution. <em>ICV</em>, <em>151</em>, 105252. (<a
href="https://doi.org/10.1016/j.imavis.2024.105252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current stereo image super-resolution methods do not fully exploit cross-view and intra-view information, resulting in limited performance. While vision transformers have shown great potential in super-resolution, their application in stereo image super-resolution is hindered by high computational demands and insufficient channel interaction. This paper introduces an efficient masked feature and group attention network for stereo image super-resolution (EMGSSR) designed to integrate the strengths of transformers into stereo super-resolution while addressing their inherent limitations. Specifically, an efficient masked feature block is proposed to extract local features from critical areas within images, guided by sparse masks. A group-weighted cross-attention module consisting of group-weighted cross-view feature interactions along epipolar lines is proposed to fully extract cross-view information from stereo images. Additionally, a group-weighted self-attention module consisting of group-weighted self-attention feature extractions with different local windows is proposed to effectively extract intra-view information from stereo images. Experimental results demonstrate that the proposed EMGSSR outperforms state-of-the-art methods at relatively low computational costs. The proposed EMGSSR offers a robust solution that effectively extracts cross-view and intra-view information for stereo image super-resolution, bringing a promising direction for future research in high-fidelity stereo image super-resolution. Source codes will be released at https://github.com/jianwensong/EMGSSR .},
  archive      = {J_ICV},
  author       = {Jianwen Song and Arcot Sowmya and Jien Kato and Changming Sun},
  doi          = {10.1016/j.imavis.2024.105252},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105252},
  shortjournal = {Image Vis. Comput.},
  title        = {Efficient masked feature and group attention network for stereo image super-resolution},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot class incremental learning via prompt transfer and
knowledge distillation. <em>ICV</em>, <em>151</em>, 105251. (<a
href="https://doi.org/10.1016/j.imavis.2024.105251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of a model to learn incrementally from very limited data while still retaining knowledge about previously seen classes is called few-shot incremental learning. The challenge of the few-shot learning model is data overfitting while the challenge of incremental learning models is catastrophic forgetting. To address these problems, we propose a distillation algorithm coupled with prompting, which effectively addresses the problem encountered in few-shot class-incremental learning by facilitating the transfer of distilled knowledge from a source to a target prompt. Furthermore, we employ a feature embedding module that monitors the semantic similarity between the input labels and the semantic vectors. This enables the learners to receive additional guidance, thereby mitigating the occurrence of catastrophic forgetting and overfitting. As our third contribution, we introduce an attention-based knowledge distillation method that learns relative similarities between features by creating effective links between teacher and student. This enables the regulation of the distillation intensities of all potential pairs between teacher and student. To validate the effectiveness of our proposed method, we conducted extensive experiments on diverse datasets, including miniImageNet, CIFAR100, and CUB200. The results of these experiments demonstrated that our method achieves state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Feidu Akmel and Fanman Meng and Mingyu Liu and Runtong Zhang and Asebe Teka and Elias Lemuye},
  doi          = {10.1016/j.imavis.2024.105251},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105251},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-shot class incremental learning via prompt transfer and knowledge distillation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distilling OCT cervical dataset with evidential uncertainty
proxy. <em>ICV</em>, <em>151</em>, 105250. (<a
href="https://doi.org/10.1016/j.imavis.2024.105250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based OCT image classification method is of paramount importance for early screening of cervical cancer. For the sake of efficiency and privacy, the emerging data distillation technique becomes a promising way to condense the large-scale original OCT dataset into a much smaller synthetic dataset, without losing much information for network training. However, OCT cervical images often suffer from redundancy, mis-operation and noise, etc. These challenges make it hard to compress as much valuable information as possible into extremely small synthesized dataset. To this end, we design an uncertainty-aware distribution matching based dataset distillation framework (UDM). Precisely, we adopt a pre-trained plug-and-play uncertainty estimation proxy to compute classification uncertainty for each data point in the original and synthetic dataset. The estimated uncertainty allows us to adaptively calculate class-wise feature centers of the original and synthetic data, thereby increasing the importance of typical patterns and reducing the impact of redundancy, mis-operation, and noise, etc. Extensive experiments show that our UDM effectively improves distribution-matching-based dataset distillation under both homogeneous and heterogeneous training scenarios.},
  archive      = {J_ICV},
  author       = {Yuxuan Xiong and Yongchao Xu and Yan Zhang and Bo Du},
  doi          = {10.1016/j.imavis.2024.105250},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105250},
  shortjournal = {Image Vis. Comput.},
  title        = {Distilling OCT cervical dataset with evidential uncertainty proxy},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the synergy between textual identity and visual
signals in human-object interaction. <em>ICV</em>, <em>151</em>, 105249.
(<a href="https://doi.org/10.1016/j.imavis.2024.105249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection task aims to recognize and understand interactions between humans and objects depicted in images. Unlike instance recognition tasks, which focus on isolated objects, HOI detection requires considering various explanatory factors, such as instance identity, spatial relationships, and scene context. However, previous HOI detection methods have primarily relied on local visual cues, often overlooking the vital role of instance identity and thus limiting the performance of models. In this paper, we introduce textual features to expand the definition of HOI representations, incorporating instance identity into the HOI reasoning process. Drawing inspiration from the human activity perception process, we explore the synergy between textual identity and visual signals to leverage various explanatory factors more effectively and enhance HOI detection performance. Specifically, our method extracts HOI explanatory factors using both modal representations. Visual features capture interactive cues, while textual features explicitly denote instance identities within human-object pairs, delineating relevant interaction categories. Additionally, we utilize Contrastive Language-Image Pre-training (CLIP) to enhance the semantic alignment between visual and textual features and design a cross-modal learning module for integrating HOI multimodal information. Extensive experiments on several benchmarks demonstrate that our proposed framework surpasses most existing methods, achieving outstanding performance with a mean average precision (mAP) of 33.95 on the HICO-DET dataset and 63.2 mAP on the V-COCO dataset.},
  archive      = {J_ICV},
  author       = {Pinzhu An and Zhi Tan},
  doi          = {10.1016/j.imavis.2024.105249},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105249},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring the synergy between textual identity and visual signals in human-object interaction},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive graph reasoning network for object detection.
<em>ICV</em>, <em>151</em>, 105248. (<a
href="https://doi.org/10.1016/j.imavis.2024.105248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Transformer-based object detection has achieved leaps and bounds in performance. Nevertheless, these methods still face some problems such as difficulty in detecting heavy occluded objects and tiny objects. Besides, the mainstream object detection paradigms usually deal with region proposals alone, without considering contextual information and the relationships between objects, which results in limited improvement. In this paper, we propose an Adaptive Graph Reasoning Network (AGRN) that explores the relationships between specific objects in an image and mines high-level semantic information via GCN to enrich visual features. Firstly, to enhance the semantic correlation between objects, a cross-scale semantic-aware module is proposed to realize the semantic interaction between feature maps of different scales so as to obtain a cross-scale semantic feature. Secondly, we activate the instance features in the image and combine the cross-scale semantic feature to create a dynamic graph. Finally, guided by the specific semantics, the attention mechanism is introduced to focus on the corresponding critical regions. On the MS-COCO 2017 dataset, our method improves the performance by 3.9% box AP and 3.6% mask AP in object detection and instance segmentation respectively relative to baseline. Additionally, our model has demonstrated exceptional performance on the PASCAL VOC dataset.},
  archive      = {J_ICV},
  author       = {Xinfang Zhong and Wenlan Kuang and Zhixin Li},
  doi          = {10.1016/j.imavis.2024.105248},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105248},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive graph reasoning network for object detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Search and recovery network for camouflaged object
detection. <em>ICV</em>, <em>151</em>, 105247. (<a
href="https://doi.org/10.1016/j.imavis.2024.105247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection aims to accurately identify objects blending into the background. However, existing methods often struggle, especially with small object or multiple objects, due to their reliance on singular strategies. To address this, we introduce a novel Search and Recovery Network (SRNet) using a bionic approach and auxiliary features. SRNet comprises three key modules: the Region Search Module (RSM), Boundary Recovery Module (BRM), and Camouflaged Object Predictor (COP). The RSM mimics predator behavior to locate potential object regions, enhancing object location detection. The BRM refines texture features and recovers object boundaries. The COP fuse multilevel features to predict final segmentation maps. Experimental results on three benchmark datasets show SRNet&#39;s superiority over SOTA models, particularly with small and multiple objects. Notably, SRNet achieves performance improvements without significantly increasing model parameters. Moreover, the method exhibits promising performance in downstream tasks such as defect detection, polyp segmentation and military camouflage detection.},
  archive      = {J_ICV},
  author       = {Guangrui Liu and Wei Wu},
  doi          = {10.1016/j.imavis.2024.105247},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105247},
  shortjournal = {Image Vis. Comput.},
  title        = {Search and recovery network for camouflaged object detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-SF: An encoding-based privacy-preserving
segmentation framework for medical images. <em>ICV</em>, <em>151</em>,
105246. (<a href="https://doi.org/10.1016/j.imavis.2024.105246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is becoming increasingly popular and is being extensively used in the field of medical image analysis. However, the privacy sensitivity of medical data limits the availability of data, which constrains the advancement of medical image analysis and impedes collaboration across multiple centers. To address this problem, we propose a novel encoding-based framework, named Privacy-SF, aimed at implementing privacy-preserving segmentation for medical images. Our proposed segmentation framework consists of three CNN networks: 1) two encoding networks on the client side that encode medical images and their corresponding segmentation masks individually to remove the privacy features, 2) a unique mapping network that analyzes the content of encoded data and learns the mapping from the encoded image to the encoded mask. By sequentially encoding data and optimizing the mapping network, our approach ensures privacy protection for images and masks during both the training and inference phases of medical image analysis. Additionally, to further improve the segmentation performance, we carefully design augmentation strategies specifically for encoded data based on its sequence nature. Extensive experiments conducted on five datasets with different modalities demonstrate excellent performance in privacy-preserving segmentation and multi-center collaboration. Furthermore, the analysis of encoded data and the experiment of model inversion attacks validate the privacy-preserving capability of our approach.},
  archive      = {J_ICV},
  author       = {Long Chen and Li Song and Haiyu Feng and Rediet Tesfaye Zeru and Senchun Chai and Enjun Zhu},
  doi          = {10.1016/j.imavis.2024.105246},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105246},
  shortjournal = {Image Vis. Comput.},
  title        = {Privacy-SF: An encoding-based privacy-preserving segmentation framework for medical images},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GSTGM: Graph, spatial–temporal attention and generative
based model for pedestrian multi-path prediction. <em>ICV</em>,
<em>151</em>, 105245. (<a
href="https://doi.org/10.1016/j.imavis.2024.105245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction in urban environments has emerged as a critical research area with extensive applications across various domains. Accurate prediction of pedestrian trajectories is essential for the safe navigation of autonomous vehicles and robots in pedestrian-populated environments. Effective prediction models must capture both the spatial interactions among pedestrians and the temporal dependencies governing their movements. Existing research primarily focuses on forecasting a single trajectory per pedestrian, limiting its applicability in real-world scenarios characterised by diverse and unpredictable pedestrian behaviours. To address these challenges, this paper introduces the Graph Convolutional Network, Spatial–Temporal Attention, and Generative Model (GSTGM) for pedestrian trajectory prediction. GSTGM employs a spatiotemporal graph convolutional network to effectively capture complex interactions between pedestrians and their environment. Additionally, it integrates a spatial–temporal attention mechanism to prioritise relevant information during the prediction process. By incorporating a time-dependent prior within the latent space and utilising a computationally efficient generative model, GSTGM facilitates the generation of diverse and realistic future trajectories. The effectiveness of GSTGM is validated through experiments on real-world scenario datasets. Compared to the state-of-the-art models on benchmark datasets such as ETH/UCY, GSTGM demonstrates superior performance in accurately predicting multiple potential trajectories for individual pedestrians. This superiority is measured using metrics such as Final Displacement Error (FDE) and Average Displacement Error (ADE). Moreover, GSTGM achieves these results with significantly faster processing speeds.},
  archive      = {J_ICV},
  author       = {Muhammad Haris Kaka Khel and Paul Greaney and Marion McAfee and Sandra Moffett and Kevin Meehan},
  doi          = {10.1016/j.imavis.2024.105245},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105245},
  shortjournal = {Image Vis. Comput.},
  title        = {GSTGM: Graph, spatial–temporal attention and generative based model for pedestrian multi-path prediction},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRENet: Crowd region enhancement network for multi-person 3D
pose estimation. <em>ICV</em>, <em>151</em>, 105243. (<a
href="https://doi.org/10.1016/j.imavis.2024.105243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering multi-person 3D poses from a single image is a challenging problem due to inherent depth ambiguities, including root-relative depth and absolute root depth. Current bottom-up methods show promising potential to mitigate absolute root depth ambiguity through explicitly aggregating global contextual cues. However, these methods treat the entire image region equally during root depth regression, ignoring the negative impact of irrelevant regions. Moreover, they learn shared features for both depths, each of which focuses on different information. This sharing mechanism may result in negative transfer, thus diminishing root depth prediction accuracy. To address these challenges, we present a novel bottom-up method, Crowd Region Enhancement Network (CRENet), incorporating a Feature Decoupling Module (FDM) and a Global Attention Module (GAM). FDM explicitly learns the discriminative feature for each depth through adaptively recalibrating its channel-wise responses and fusing multi-level features, which makes the model focus on each depth prediction separately and thus avoids the adverse effect of negative transfer. GAM highlights crowd regions while suppressing irrelevant regions using the attention mechanism and further refines the attention regions based on the confidence measure about the attention, which is beneficial to learn depth-related cues from informative crowd regions and facilitate root depth estimation. Comprehensive experiments on benchmarks MuPoTS-3D and CMU Panoptic demonstrate that our method outperforms the state-of-the-art bottom-up methods in absolute 3D pose estimation and is applicable to in-the-wild images, which also indicates that learning depth-specific features and suppressing the noise signals can significantly benefit multi-person absolute 3D pose estimation.},
  archive      = {J_ICV},
  author       = {Zhaokun Li and Qiong Liu},
  doi          = {10.1016/j.imavis.2024.105243},
  journal      = {Image and Vision Computing},
  month        = {11},
  pages        = {105243},
  shortjournal = {Image Vis. Comput.},
  title        = {CRENet: Crowd region enhancement network for multi-person 3D pose estimation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pro-ReID: Producing reliable pseudo labels for unsupervised
person re-identification. <em>ICV</em>, <em>150</em>, 105244. (<a
href="https://doi.org/10.1016/j.imavis.2024.105244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mainstream unsupervised person ReIDentification (ReID) is on the basis of the alternation of clustering and fine-tuning to promote the task performance, but the clustering process inevitably produces noisy pseudo labels, which seriously constrains the further advancement of the task performance. To conquer the above concerns, the novel Pro-ReID framework is proposed to produce reliable person samples from the pseudo-labeled dataset to learn feature representations in this work. It consists of two modules: Pseudo Labels Correction (PLC) and Pseudo Labels Selection (PLS). Specifically, we further leverage the temporal ensemble prior knowledge to promote task performance. The PLC module assigns corresponding soft pseudo labels to each sample with control of soft pseudo label participation to potentially correct for noisy pseudo labels generated during clustering; the PLS module associates the predictions of the temporal ensemble model with pseudo label annotations and it detects noisy pseudo labele examples as out-of-distribution examples through the Gaussian Mixture Model (GMM) to supply reliable pseudo labels for the unsupervised person ReID task in consideration of their loss data distribution. Experimental findings validated on three person (Market-1501, DukeMTMC-reID and MSMT17) and one vehicle (VeRi-776) ReID benchmark establish that the novel Pro-ReID framework achieves competitive performance, in particular the mAP on the ambitious MSMT17 that is 4.3% superior to the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Haiming Sun and Shiwei Ma},
  doi          = {10.1016/j.imavis.2024.105244},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105244},
  shortjournal = {Image Vis. Comput.},
  title        = {Pro-ReID: Producing reliable pseudo labels for unsupervised person re-identification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Language conditioned multi-scale visual attention networks
for visual grounding. <em>ICV</em>, <em>150</em>, 105242. (<a
href="https://doi.org/10.1016/j.imavis.2024.105242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual grounding (VG) is a task that requires to locate a specific region in an image according to a natural language expression. Existing efforts on the VG task are divided into two-stage, one-stage and Transformer-based methods, which have achieved high performance. However, most of the previous methods extract visual information at a single spatial scale and ignore visual information at other spatial scales, which makes these models unable to fully utilize the visual information. Moreover, the insufficient utilization of linguistic information, especially failure to capture global linguistic information, may lead to failure to fully understand language expressions, thus limiting the performance of these models. To better address the task, we propose a language conditioned multi-scale visual attention network (LMSVA) for visual grounding, which can sufficiently utilize visual and linguistic information to perform multimodal reasoning, thus improving performance of model. Specifically, we design a visual feature extractor containing a multi-scale layer to get the required multi-scale visual features by expanding the original backbone. Moreover, we exploit pooling the output of the pre-trained Bidirectional Encoder Representations from Transformers (BERT) model to extract sentence-level linguistic features, which can enable the model to capture global linguistic information. Inspired by the Transformer architecture, we present the Visual Attention Layer guided by Language and Multi-Scale Visual Features (VALMS), which is able to better learn the visual context guided by multi-scale visual and linguistic features, and facilitates further multimodal reasoning. Extensive experiments on four large benchmark datasets, including ReferItGame, RefCOCO, RefCOCO + and RefCOCOg, demonstrate that our proposed model achieves the state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Haibo Yao and Lipeng Wang and Chengtao Cai and Wei Wang and Zhi Zhang and Xiaobing Shang},
  doi          = {10.1016/j.imavis.2024.105242},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105242},
  shortjournal = {Image Vis. Comput.},
  title        = {Language conditioned multi-scale visual attention networks for visual grounding},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning facial structural dependency in 3D aligned space
for face alignment. <em>ICV</em>, <em>150</em>, 105241. (<a
href="https://doi.org/10.1016/j.imavis.2024.105241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial structure&#39;s statistical characteristics offer pivotal prior information in facial landmark prediction, forming inter-dependencies among different landmarks. Such inter-dependencies ensure that predictions adhere to the shape distribution typical of natural faces. In challenging scenarios like occlusions or extreme facial poses, this structure becomes indispensable, which can help to predict elusive landmarks based on more discernible ones. While current deep learning methods do capture these landmark dependencies, it&#39;s often an implicit process heavily reliant on vast training datasets. We contest that such implicit modeling approaches fail to manage more challenging situations. In this paper, we propose a new method that harnesses the facial structure and explicitly explores inter-dependencies among facial landmarks in an end-to-end fashion. We propose a Structural Dependency Learning Module (SDLM). It uses 3D face information to map facial features into a canonical UV space, in which the facial structure is explicitly 3D semantically aligned. Besides, to explore the global relationships between facial landmarks, we take advantage of the self-attention mechanism in the image and UV spaces. We name the proposed method Facial Structure-based Face Alignment (FSFA). FSFA reinforces the landmark structure, especially under challenging conditions. Extensive experiments demonstrate that FSFA achieves state-of-the-art performance on the WFLW, 300W, AFLW, and COFW68 datasets.},
  archive      = {J_ICV},
  author       = {Biying Li and Zhiwei Liu and Jinqiao Wang},
  doi          = {10.1016/j.imavis.2024.105241},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105241},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning facial structural dependency in 3D aligned space for face alignment},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probability based dynamic soft label assignment for object
detection. <em>ICV</em>, <em>150</em>, 105240. (<a
href="https://doi.org/10.1016/j.imavis.2024.105240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By defining effective supervision labels for network training, the performance of object detectors can be improved without incurring additional inference costs. Current label assignment strategies generally require two steps: first, constructing a positive sample candidate bag, and then designing labels for these samples. However, the construction of candidate bag of positive samples may result in some noisy samples being introduced into the label assignment process. We explore a single-step label assignment approach: directly generating a probability map as labels for all samples. We design the label assignment approach from the following perspectives: Firstly, it should be able to reduce the impact of noise samples. Secondly, each sample should be treated differently because each one matches the target to a different extent, which assists the network to learn more valuable information from high-quality samples. We propose a probability-based dynamic soft label assignment method. Instead of dividing the samples into positive and negative samples, a probability map, which is calculated based on prediction quality and prior knowledge, is used to supervise all anchor points of the classification branch. The weight of prior knowledge in the labels decreases as the network improves the quality of instance predictions, as a way to reduce noise samples introduced by prior knowledge. By using continuous probability values as labels to supervise the classification branch, the network is able to focus on high-quality samples. As demonstrated in the experiments on the MS COCO benchmark, our label assignment method achieves 40.9% AP in the ResNet-50 under 1x schedule, which improves FCOS performance by approximately 2.0% AP. The code has been available at https://github.com/Liyi4578/PDSLA .},
  archive      = {J_ICV},
  author       = {Yi Li and Sile Ma and Xiangyuan Jiang and Yizhong Luan and Zecui Jiang},
  doi          = {10.1016/j.imavis.2024.105240},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105240},
  shortjournal = {Image Vis. Comput.},
  title        = {Probability based dynamic soft label assignment for object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous image patch attention and pruning for patch
selective transformer. <em>ICV</em>, <em>150</em>, 105239. (<a
href="https://doi.org/10.1016/j.imavis.2024.105239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer models provide superior performance compared to convolutional neural networks for various computer vision tasks but require increased computational overhead with large datasets. This paper proposes a patch selective vision transformer that effectively selects patches to reduce computational costs while simultaneously extracting global and local self-representative patch information to maintain performance. The inter-patch attention in the transformer encoder emphasizes meaningful features by capturing the inter-patch relationships of features, and dynamic patch pruning is applied to the attentive patches using a learnable soft threshold that measures the maximum multi-head importance scores. The proposed patch attention and pruning method provides constraints to exploit dominant feature maps in conjunction with self-attention, thus avoiding the propagation of noisy or irrelevant information. The proposed patch-selective transformer also helps to address computer vision problems such as scale, background clutter, and partial occlusion, resulting in a lightweight and general-purpose vision transformer suitable for mobile devices.},
  archive      = {J_ICV},
  author       = {Sunpil Kim and Gang-Joon Yoon and Jinjoo Song and Sang Min Yoon},
  doi          = {10.1016/j.imavis.2024.105239},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105239},
  shortjournal = {Image Vis. Comput.},
  title        = {Simultaneous image patch attention and pruning for patch selective transformer},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting adversarial samples by noise injection and
denoising. <em>ICV</em>, <em>150</em>, 105238. (<a
href="https://doi.org/10.1016/j.imavis.2024.105238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are highly vulnerable to adversarial examples, leading to significant attention on techniques for detecting them. However, current methods primarily rely on detecting image features for identifying adversarial examples, often failing to address the diverse types and intensities of such examples. We propose a novel adversarial example detection method based on perturbation estimation and denoising to overcome this limitation. We develop an autoencoder to predict the latent adversarial perturbations of samples and select appropriately sized noise based on these predictions to cover the perturbations. Subsequently, we employ a non-blind denoising autoencoder to remove noise and residual perturbations effectively. This approach allows us to eliminate adversarial perturbations while preserving the original information, thus altering the prediction results of adversarial examples without affecting predictions on benign samples. Inconsistencies in predictions before and after processing by the model identify adversarial examples. Our experiments on datasets such as MNIST, CIFAR-10, and ImageNet demonstrate that our method surpasses other advanced detection methods in accuracy.},
  archive      = {J_ICV},
  author       = {Han Zhang and Xin Zhang and Yuan Sun and Lixia Ji},
  doi          = {10.1016/j.imavis.2024.105238},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105238},
  shortjournal = {Image Vis. Comput.},
  title        = {Detecting adversarial samples by noise injection and denoising},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning accurate monocular 3D voxel representation via
bilateral voxel transformer. <em>ICV</em>, <em>150</em>, 105237. (<a
href="https://doi.org/10.1016/j.imavis.2024.105237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based methods for 3D scene perception have been widely explored for autonomous vehicles. However, inferring complete 3D semantic scenes from monocular 2D images is still challenging owing to the 2D-to-3D transformation. Specifically, existing methods that use Inverse Perspective Mapping (IPM) to project image features to dense 3D voxels severely suffer from the ambiguous projection problem. In this research, we present Bilateral Voxel Transformer (BVT), a novel and effective Transformer-based approach for monocular 3D semantic scene completion. BVT exploits a bilateral architecture composed of two branches for preserving the high-resolution 3D voxel representation while aggregating contexts through the proposed Tri-Axial Transformer simultaneously. To alleviate the ill-posed 2D-to-3D transformation, we adopt position-aware voxel queries and dynamically update the voxels with image features through weighted geometry-aware sampling. BVT achieves 11.8 mIoU on the challenging Semantic KITTI dataset, considerably outperforming previous works for semantic scene completion with monocular images. The code and models of BVT will be available on GitHub .},
  archive      = {J_ICV},
  author       = {Tianheng Cheng and Haoyi Jiang and Shaoyu Chen and Bencheng Liao and Qian Zhang and Wenyu Liu and Xinggang Wang},
  doi          = {10.1016/j.imavis.2024.105237},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105237},
  shortjournal = {Image Vis. Comput.},
  title        = {Learning accurate monocular 3D voxel representation via bilateral voxel transformer},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HEDehazeNet: Unpaired image dehazing via enhanced haze
generation. <em>ICV</em>, <em>150</em>, 105236. (<a
href="https://doi.org/10.1016/j.imavis.2024.105236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unpaired image dehazing models based on Cycle-Consistent Adversarial Networks (CycleGAN) typically consist of two cycle branches: dehazing-rehazing branch and hazing-dehazing branch. In these two branches, there is an asymmetry of information in the mutual transformation process between haze images and haze-free images. Previous models tended to focus more on the transformation process from haze images to haze-free images within the dehazing-rehazing branch, overlooking the provision of effective information for the formation of haze images in the hazing-dehazing branch. This oversight results in the production of haze patterns that are both monotonous and simplistic, ultimately impeding the overall performance and generalization capabilities of dehazing networks. In light of this, this paper proposes a novel model called HEDehazeNet (Dehazing Net based on Haze Generation Enhancement), which provides crucial information for the generation process of haze images through a dedicated haze generation enhancement module. This module is capable of producing three distinct modes of transmission maps - random transmission map, simulated transmission map, and mixed transmission maps combining both. Employing these transmission maps to generate hazing images with varying density and patterns provides the dehazing network with a more diverse and dynamically complex set of training samples, thereby enhancing its capacity to handle intricate scenes. Additionally, we made minor modifications to the U-Net, replacing residual blocks with multi-scale parallel convolutional blocks and channel self-attention, to further enhance the network&#39;s performance. Experiments were conducted on both synthetic and real-world datasets, substantiating the superiority of HEDehazeNet over the current state-of-the-art unpaired dehazing models.},
  archive      = {J_ICV},
  author       = {Wentao Li and Deming Fan and Qi Zhu and Zhanjiang Gao and Hao Sun},
  doi          = {10.1016/j.imavis.2024.105236},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105236},
  shortjournal = {Image Vis. Comput.},
  title        = {HEDehazeNet: Unpaired image dehazing via enhanced haze generation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual subspace clustering for spectral-spatial hyperspectral
image clustering. <em>ICV</em>, <em>150</em>, 105235. (<a
href="https://doi.org/10.1016/j.imavis.2024.105235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace clustering supposes that hyperspectral image (HSI) pixels lie in a union vector spaces of multiple sample subspaces without considering their dual space, i.e., spectral space. In this article, we propose a promising dual subspace clustering (DualSC) for improving spectral-spatial HSIs clustering by relaxing subspace clustering. To this end, DualSC simultaneously optimizes row and column subspace-representations of HSI superpixels to capture the intrinsic connection between spectral and spatial information. From the new perspective, the original subspace clustering can be treated as a special case of DualSC that has larger solution space, so tends to finding better sample representation matrix for applying spectral clustering. Besides, we provide theoretical proofs that show the proposed method relaxes the subspace space clustering with dual subspace, and can recover subspace-sparse representation of HSI samples. To the best of our knowledge, this work could be one of the first dual clustering method leveraging sample and spectral subspaces simultaneously. As a result, we conduct several clustering experiments on four canonical data sets, implying that our proposed method with strong interpretability reaches comparable performance and computing efficiency with other state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Shujun Liu},
  doi          = {10.1016/j.imavis.2024.105235},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105235},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual subspace clustering for spectral-spatial hyperspectral image clustering},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced human motion detection with hybrid RDA-WOA-based
RNN and multiple hypothesis tracking for occlusion handling.
<em>ICV</em>, <em>150</em>, 105234. (<a
href="https://doi.org/10.1016/j.imavis.2024.105234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion detection in complex scenarios poses challenges due to occlusions. This paper presents an integrated approach for accurate human motion detections by combining Adapted Canny Edge detection as a preprocessing step, backbone-modified Mask R-CNN for precise segmentation, Hybrid RDA-WOA-based RNN as a classification, and a Multiple-hypothesis model for effective occlusion handling. Adapted Canny Edge detection is utilized as an initial preprocessing step to highlight significant edges in the input image. The resulting edge map enhances object boundaries and highlights structural features, simplifying subsequent processing steps. The improved image is then passed through backbone-modified Mask R-CNN for the pixel-level segmentation of humans. Backbone-modified Mask R-CNN along with IoU, Euclidean Distance, and Z-Score recognizes moving objects in complex scenes exactly. After recognizing moving objects, the optimized Hybrid RDA-WOA-based RNN classifies humans. To handle the self-occlusion, Multiple Hypothesis Tracking (MHT) is used. Real-world situations frequently include occlusions where humans can be partially or completely hidden by objects. The proposed approach integrates a Multiple-hypothesis model into the detection pipeline to address this challenge. Moreover, the proposed human motion detection approach includes an optimized Hybrid RDA-WOA-based RNN trained with 2D representations of 3D skeletal motion. The proposed work was evaluated using the IXMAS, KTH, Weizmann, NTU RGB + D, and UCF101 Datasets. It achieved an accuracy of 98% on the IXMAS, KTH, Weizmann, and UCF101 Datasets and 97.1% on the NTU RGB + D Dataset. The simulation results unveil the superiority of the proposed methodology over the existing approaches.},
  archive      = {J_ICV},
  author       = {Jeba Nega Cheltha and Chirag Sharma and Deepak Prashar and Arfat Ahmad Khan and Seifedine Kadry},
  doi          = {10.1016/j.imavis.2024.105234},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105234},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhanced human motion detection with hybrid RDA-WOA-based RNN and multiple hypothesis tracking for occlusion handling},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box model adaptation for semantic segmentation.
<em>ICV</em>, <em>150</em>, 105233. (<a
href="https://doi.org/10.1016/j.imavis.2024.105233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model adaptation aims to transfer knowledge in pre-trained source models to a new unlabeled dataset. Despite impressive progress, prior methods always need to access the source model and develop data-reconstruction approaches to align the data distributions between target samples and the generated instances, which may raise privacy concerns from source individuals. To alleviate the above problem, we propose a new method in the setting of Black-box model adaptation for semantic segmentation, in which only the pseudo-labels from multiple source domain is required during the adaptation process. Specifically, the proposed method structurally distills the knowledge with multiple classifiers to obtain a customized target model, and then the predictions of target data are refined to fit the target domain with co-regularization. We conduct extensive experiments on several standard datasets, and our method can achieve promising results.},
  archive      = {J_ICV},
  author       = {Zhiheng Zhou and Wanlin Yue and Yinglie Cao and Shifu Shen},
  doi          = {10.1016/j.imavis.2024.105233},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105233},
  shortjournal = {Image Vis. Comput.},
  title        = {Black-box model adaptation for semantic segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDMNet: Spatially dilated multi-scale network for object
detection for drone aerial imagery. <em>ICV</em>, <em>150</em>, 105232.
(<a href="https://doi.org/10.1016/j.imavis.2024.105232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale object detection is a preeminent challenge in computer vision and image processing. Several deep learning models that are designed to detect various objects miss out on the detection capabilities for small objects, reducing their detection accuracies. Intending to focus on different scales, from extremely small to large-sized objects, this work proposes a Spatially Dilated Multi-Scale Network (SDMNet) architecture for UAV-based ground object detection. It proposes a Multi-scale Enhanced Effective Channel Attention mechanism to preserve the object details in the images. Additionally, the proposed model incorporates dilated convolution, sub-pixel convolution, and additional prediction heads to enhance object detection performance specifically for aerial imaging. It has been evaluated on two popular aerial image datasets, VisDrone 2019 and UAVDT, containing publicly available annotated images of ground objects captured from UAV. Different performance metrics, such as precision, recall, mAP, and detection rate, benchmark the proposed architecture with the existing object detection approaches. The experimental results demonstrate the effectiveness of the proposed model for multi-scale object detection with an average precision score of 54.2% and 98.4% for VisDrone and UAVDT datasets, respectively.},
  archive      = {J_ICV},
  author       = {Neeraj Battish and Dapinder Kaur and Moksh Chugh and Shashi Poddar},
  doi          = {10.1016/j.imavis.2024.105232},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105232},
  shortjournal = {Image Vis. Comput.},
  title        = {SDMNet: Spatially dilated multi-scale network for object detection for drone aerial imagery},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A streamlined framework for BEV-based 3D object detection
with prior masking. <em>ICV</em>, <em>150</em>, 105229. (<a
href="https://doi.org/10.1016/j.imavis.2024.105229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of autonomous driving, perception tasks based on Bird&#39;s-Eye-View (BEV) have attracted considerable research attention due to their numerous benefits. Despite recent advancements in performance, efficiency remains a challenge for real-world implementation. In this study, we propose an efficient and effective framework that constructs a spatio-temporal BEV feature from multi-camera inputs and leverages it for 3D object detection. Specifically, the success of our network is primarily attributed to the design of the lifting strategy and a tailored BEV encoder. The lifting strategy is tasked with the conversion of 2D features into 3D representations. In the absence of depth information in the images, we innovatively introduce a prior mask for the BEV feature, which can assess the significance of the feature along the camera ray at a low cost. Moreover, we design a lightweight BEV encoder, which significantly boosts the capacity of this physical-interpretation representation. In the encoder, we investigate the spatial relationships of the BEV feature and retain rich residual information from upstream. To further enhance performance, we establish a 2D object detection auxiliary head to delve into insights offered by 2D object detection and leverage the 4D information to explore the cues within the sequence. Benefiting from all these designs, our network can capture abundant semantic information from 3D scenes and strikes a balanced trade-off between efficiency and performance.},
  archive      = {J_ICV},
  author       = {Qinglin Tong and Junjie Zhang and Chenggang Yan and Dan Zeng},
  doi          = {10.1016/j.imavis.2024.105229},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105229},
  shortjournal = {Image Vis. Comput.},
  title        = {A streamlined framework for BEV-based 3D object detection with prior masking},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OCUCFormer: An over-complete under-complete transformer
network for accelerated MRI reconstruction. <em>ICV</em>, <em>150</em>,
105228. (<a href="https://doi.org/10.1016/j.imavis.2024.105228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many deep learning-based architectures have been proposed for accelerated Magnetic Resonance Imaging (MRI) reconstruction. However, existing encoder-decoder-based popular networks have a few shortcomings: (1) They focus on the anatomy structure at the expense of fine details, hindering their performance in generating faithful reconstructions; (2) Lack of long-range dependencies yields sub-optimal recovery of fine structural details. In this work, we propose an Over-Complete Under-Complete Transformer network (OCUCFormer) which focuses on better capturing fine edges and details in the image and can extract the long-range relations between these features for improved single-coil (SC) and multi-coil (MC) MRI reconstruction. Our model computes long-range relations in the highest resolutions using Restormer modules for improved acquisition and restoration of fine anatomical details. Towards learning in the absence of fully sampled ground truth for supervision, we show that our model trained with under-sampled data in a self-supervised fashion shows a superior recovery of fine structures compared to other works. We have extensively evaluated our network for SC and MC MRI reconstruction on brain, cardiac, and knee anatomies for 4 × 4× and 5 × 5× acceleration factors. We report significant improvements over popular deep learning-based methods when trained in supervised and self-supervised modes. We have also performed experiments demonstrating the strengths of extracting fine details and the anatomical structure and computing long-range relations within over-complete representations. Code for our proposed method is available at: https://github.com/alfahimmohammad/OCUCFormer-main .},
  archive      = {J_ICV},
  author       = {Mohammad Al Fahim and Sriprabha Ramanarayanan and G.S. Rahul and Matcha Naga Gayathri and Arunima Sarkar and Keerthi Ram and Mohanasankar Sivaprakasam},
  doi          = {10.1016/j.imavis.2024.105228},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105228},
  shortjournal = {Image Vis. Comput.},
  title        = {OCUCFormer: An over-complete under-complete transformer network for accelerated MRI reconstruction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coplane-constrained sparse depth sampling and local depth
propagation for depth estimation. <em>ICV</em>, <em>150</em>, 105227.
(<a href="https://doi.org/10.1016/j.imavis.2024.105227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth estimation with sparse reference has emerged recently, and predicts depth map from a monocular image and a set of depth reference samples. Previous works randomly select reference samples by sensors, leading to severe depth bias as this sampling is independent to image semantic and neglects the unbalance of depth distribution in regions. This paper proposes a Coplane-Constrained sparse Depth (CCD) sampling to explore representative reference samples, and design a Local Depth Propagation (LDP) network for complete the sparse point cloud map. This can capture diverse depth information and diffuse the valid points to neighbors with geometry prior. Specifically, we first construct the surface normal map and detect coplane pixels by superpixel segmenting for sampling references, whose depth can be represented by that of superpixel centroid. Then, we introduce local depth propagation to obtain coarse-level depth map with geometric information, which dynamically diffuses the depth from the reference to neighbors based on local planar assumption. Further, we generate the fine-level depth map by devising a pixel-wise focal loss, which imposes the semantic and geometry calibration on pixels with low confidence in coarse-level prediction. Extensive experiments on public datasets demonstrate that our model outperforms SOTA depth estimation and completion methods.},
  archive      = {J_ICV},
  author       = {Jiehua Zhang and Zhiwen Yang and Chuqiao Chen and Hongkui Wang and Tingyu Wang and Chenggang Yan and Yihong Gong},
  doi          = {10.1016/j.imavis.2024.105227},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105227},
  shortjournal = {Image Vis. Comput.},
  title        = {Coplane-constrained sparse depth sampling and local depth propagation for depth estimation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reverse cross-refinement network for camouflaged object
detection. <em>ICV</em>, <em>150</em>, 105218. (<a
href="https://doi.org/10.1016/j.imavis.2024.105218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high intrinsic similarity between camouflaged objects and the background, camouflaged objects often exhibit blurred boundaries, making it challenging to distinguish the boundaries of objects. Existing methods still focus on the overall regional accuracy but not on the boundary quality and are not competent to identify camouflaged objects from the background in complex scenarios. Thus, we propose a novel reverse cross-refinement network called RCR-Net. Specifically, we design a diverse feature enhancement module that simulates the correspondingly expanded receptive fields of the human visual system by using convolutional kernels with different dilation rates in parallel. Also, the boundary attention module is used to reduce the noise of the bottom features. Moreover, a multi-scale feature aggregation module is proposed to transmit the diverse features from pixel-level camouflaged edges to the entire camouflaged object region in a coarse-to-fine manner, which consists of reverse guidance, group guidance, and position guidance. Reverse guidance mines complementary regions and details by erasing already estimated object regions. Group guidance and position guidance integrate different features through simple and effective splitting and connecting operations. Extensive experiments show that RCR-Net outperforms the existing 18 state-of-the-art methods on four widely-used COD datasets. Especially, compared with the existing top-1 model HitNet, RCR-Net significantly improves the performance by ∼ 16.4% (Mean Absolute Error) on the CAMO dataset, showing that RCR-Net could accurately detect camouflaged objects.},
  archive      = {J_ICV},
  author       = {Qian Ye and Yaqin Zhou and Guanying Huo and Yan Liu and Yan Zhou and Qingwu Li},
  doi          = {10.1016/j.imavis.2024.105218},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105218},
  shortjournal = {Image Vis. Comput.},
  title        = {Reverse cross-refinement network for camouflaged object detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Underwater image restoration based on light attenuation
prior and color-contrast adaptive correction. <em>ICV</em>,
<em>150</em>, 105217. (<a
href="https://doi.org/10.1016/j.imavis.2024.105217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imaging is uniquely beset by issues such as color distortion and diminished contrast due to the intricate behavior of light as it traverses water, being attenuated by processes of absorption and scattering. Distinct from traditional underwater image restoration techniques, our methodology uniquely accommodates attenuation coefficients pertinent to diverse water conditions. We endeavor to recover the pristine image by approximating decay rates, focusing particularly on the blue-red and blue-green color channels. Recognizing the inherent ambiguities surrounding water type classifications, we meticulously assess attenuation coefficient ratios for an array of predefined aquatic categories. Each classification results in a uniquely restored image, and an automated selection algorithm is employed to determine the most optimal output, rooted in its color distribution. In tandem, we&#39;ve innovated a color-contrast adaptive correction technique, purposefully crafted to remedy color anomalies in underwater images while simultaneously amplifying contrast and detail fidelity. Extensive trials on benchmark datasets unambiguously highlight our method&#39;s preeminence over six other renowned strategies. Impressively, our methodology exhibits exceptional resilience and adaptability, particularly in scenarios dominated by green background imagery.},
  archive      = {J_ICV},
  author       = {Jianru Li and Xu Zhu and Yuchao Zheng and Huimin Lu and Yujie Li},
  doi          = {10.1016/j.imavis.2024.105217},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105217},
  shortjournal = {Image Vis. Comput.},
  title        = {Underwater image restoration based on light attenuation prior and color-contrast adaptive correction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Source domain prior-assisted segment anything model for
single domain generalization in medical image segmentation.
<em>ICV</em>, <em>150</em>, 105216. (<a
href="https://doi.org/10.1016/j.imavis.2024.105216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based medical image segmentation models often suffer from performance degradation across domains due to domain discrepancies arising from data collected by various healthcare centers. Recent advancements, particularly the Segment Anything Model (SAM), have shown promising generalization abilities in this field, inspiring the development of several SAM-based approaches to address domain discrepancy issue. Nevertheless, these methods seldom exploit the full potential of the rich source domain knowledge to improve segmentation accuracy in unseen target domains. In this paper, we propose a source domain prior-assisted module for generalizable medical image segmentation based on SAM. Specifically, we store diverse features of the source domain data in a memory bank. When applying the model to the target domain, the features of the target domain first match with the features in the memory bank to obtain invaluable prior information. This strategy enables the model to utilize the prior information from the source domain to adapt to the target domain. We validate the proposed method on two widely used medical image segmentation tasks across multiple domains and achieve state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Wenhui Dong and Bo Du and Yongchao Xu},
  doi          = {10.1016/j.imavis.2024.105216},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105216},
  shortjournal = {Image Vis. Comput.},
  title        = {Source domain prior-assisted segment anything model for single domain generalization in medical image segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-scale feature correspondence and pseudo label
retraining strategy for weakly supervised semantic segmentation.
<em>ICV</em>, <em>150</em>, 105215. (<a
href="https://doi.org/10.1016/j.imavis.2024.105215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the performance of semantic segmentation using weakly supervised learning has significantly improved. Weakly supervised semantic segmentation (WSSS) that uses only image-level labels has received widespread attention, it employs Class Activation Maps (CAM) to generate pseudo labels. Compared to traditional use of pixel-level labels, this technique greatly reduces annotation costs by utilizing simpler and more readily available image-level annotations. Besides, due to the local perceptual ability of Convolutional Neural Networks (CNN), the generated CAM cannot activate the entire object area. Researchers have found that this CNN limitation can be compensated for by using Vision Transformer (ViT). However, ViT also introduces an over-smoothing problem. Recent research has made good progress in solving this issue, but when discussing CAM and its related segmentation predictions, it is easy to overlook their intrinsic information and the interrelationships between them. In this paper, we propose a Multi-Scale Feature Correspondence (MSFC) method. Our MSFC can obtain the feature correspondence of CAM and segmentation predictions at different scales, re-extract useful semantic information from them, enhancing the network&#39;s learning of feature information and improving the quality of CAM. Moreover, to further improve the segmentation precision, we design a Pseudo Label Retraining Strategy (PLRS). This strategy refines the accuracy in local regions, elevates the quality of pseudo labels, and aims to enhance segmentation precision. Experimental results on the PASCAL VOC 2012 and MS COCO 2014 datasets show that our method achieves impressive performance among end-to-end WSSS methods.},
  archive      = {J_ICV},
  author       = {Weizheng Wang and Lei Zhou and Haonan Wang},
  doi          = {10.1016/j.imavis.2024.105215},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105215},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale feature correspondence and pseudo label retraining strategy for weakly supervised semantic segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAM-RSP: A new few-shot segmentation method based on segment
anything model and rough segmentation prompts. <em>ICV</em>,
<em>150</em>, 105214. (<a
href="https://doi.org/10.1016/j.imavis.2024.105214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot segmentation (FSS) aims to segment novel classes with a few labeled images. The backbones used in existing methods are pre-trained through classification tasks on the ImageNet dataset. Although these backbones can effectively perceive the semantic categories of images, they cannot accurately perceive the regional boundaries within one image, which limits the model performance. Recently, Segment Anything Model (SAM) has achieved precise image segmentation based on point or box prompts, thanks to its excellent perception of region boundaries within one image. However, it cannot effectively provide semantic information of images. This paper proposes a new few-shot segmentation method that can effectively perceive both semantic categories and regional boundaries. This method first utilizes the SAM encoder to perceive regions and obtain the query embedding. Then the support and query images are input into a backbone pre-trained on ImageNet to perceive semantics and generate a rough segmentation prompt (RSP). This query embedding is combined with the prompt to generate a pixel-level query prototype, which can better match the query embedding. Finally, the query embedding, prompt, and prototype are combined and input into the designed multi-layer prompt transformer decoder, which is more efficient and lightweight, and can provide a more accurate segmentation result. In addition, other methods can be easily combined with our framework to improve their performance. Plenty of experiments on PASCAL-5 i and COCO-20 i under 1-shot and 5-shot settings prove the effectiveness of our method. Our method also achieves new state-of-the-art. Codes are available at https://github.com/Jiaguang-NEU/SAM-RSP .},
  archive      = {J_ICV},
  author       = {Jiaguang Li and Ying Wei and Wei Zhang and Zhenrui Shi},
  doi          = {10.1016/j.imavis.2024.105214},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105214},
  shortjournal = {Image Vis. Comput.},
  title        = {SAM-RSP: A new few-shot segmentation method based on segment anything model and rough segmentation prompts},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved attentive residue multi-dilated network for
thermal noise removal in magnetic resonance images. <em>ICV</em>,
<em>150</em>, 105213. (<a
href="https://doi.org/10.1016/j.imavis.2024.105213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) technology is crucial in the medical field, but the thermal noise in the reconstructed MR images may interfere with the clinical diagnosis. Removing the thermal noise in MR images mainly contains two challenges. First, thermal noise in an MR image obeys Rician distribution, where the statistical features are not consistent in different regions of the image. In this case, conventional denoising methods like spatial convolutional filtering will not be appropriate to deal with it. Second, details and edge information in the image may get damaged while smoothing the noise. This paper proposes a novel deep-learning model to denoise MR images. First, the model learns a binary mask to separate the background and signal regions of the noised image, making the noise left in the signal region obey a unified statistical distribution. Second, the model is designed as an attentive residual multi-dilated network (ARM-Net), composed of a multi-branch structure, and supplemented with a frequency-domain-optimizable discrete cosine transform module. In this way, the deep-learning model will be more effective in removing the noise while maintaining the details of the original image. Furthermore, we have also made improvements on the original ARM-Net baseline to establish a new model called ARM-Net v2, which is more efficient and effective. Experimental results illustrate that over the BraTS 2018 dataset, our method achieves the PSNR of 39.7087 and 32.6005 at noise levels of 5% and 20%, which realizes the state-of-the-art performance among existing MR image denoising methods.},
  archive      = {J_ICV},
  author       = {Bowen Jiang and Tao Yue and Xuemei Hu},
  doi          = {10.1016/j.imavis.2024.105213},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105213},
  shortjournal = {Image Vis. Comput.},
  title        = {An improved attentive residue multi-dilated network for thermal noise removal in magnetic resonance images},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale large kernel convolution and hybrid attention
network for remote sensing image dehazing. <em>ICV</em>, <em>150</em>,
105212. (<a href="https://doi.org/10.1016/j.imavis.2024.105212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing (RS) image dehazing holds significant importance in enhancing the quality and information extraction capability of RS imagery. The enhancement in image dehazing quality has progressively advanced alongside the evolution of convolutional neural network (CNN). Due to the fixed receptive field of CNN, there is insufficient utilization of contextual information on haze features in multi-scale RS images. Additionally, the network fails to adequately extract both local and global information of haze features. In addressing the above problems, in this paper, we propose an RS image dehazing network based on multi-scale large kernel convolution and hybrid attention (MKHANet). The network is mainly composed of multi-scale large kernel convolution (MSLKC) module, hybrid attention (HA) module and feature fusion attention (FFA) module. The MSLKC module fully fuses the multi-scale information of features while enhancing the effective receptive field of the network by parallel multiple large kernel convolutions. To alleviate the problem of uneven distribution of haze and effectively extract the global and local information of haze features, the HA module is introduced by focusing on the importance of haze pixels at the channel level. The FFA module aims to boost the interaction of feature information between the network&#39;s deep and shallow layers. The subjective and objective experimental results on on multiple RS hazy image datasets illustrates that MKHANet surpasses existing state-of-the-art (SOTA) approaches. The source code is available at https://github.com/tohang98/MKHA_Net .},
  archive      = {J_ICV},
  author       = {Hang Su and Lina Liu and Zenghui Wang and Mingliang Gao},
  doi          = {10.1016/j.imavis.2024.105212},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105212},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale large kernel convolution and hybrid attention network for remote sensing image dehazing},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-branch teacher-student with noise-tolerant learning for
domain adaptive nighttime segmentation. <em>ICV</em>, <em>150</em>,
105211. (<a href="https://doi.org/10.1016/j.imavis.2024.105211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While significant progress has been achieved in the field of image semantic segmentation, the majority of research has been primarily concentrated on daytime scenes. Semantic segmentation of nighttime images is equally critical for autonomous driving; however, this task presents greater challenges due to inadequate lighting and difficulties associated with obtaining accurate manual annotations. In this paper, we introduce a novel method called the Dual-Branch Teacher-Student (DBTS) framework for unsupervised nighttime semantic segmentation. Our approach combines domain alignment and knowledge distillation in a mutually reinforcing manner. Firstly, we employ a photometric alignment module to dynamically generate target-like latent images, bridging the appearance gap between the source domain (daytime) and the target domain (nighttime). Secondly, we establish a dual-branch framework, where each branch enhances collaboration between the teacher and student networks. The student network utilizes adversarial learning to align the target domain with another domain (i.e., source or latent domain), while the teacher network generates reliable pseudo-labels by distilling knowledge from the latent domain. Furthermore, recognizing the potential noise present in pseudo-labels, we propose a noise-tolerant learning method to mitigate the risks associated with overreliance on pseudo-labels during domain adaptation. When evaluated on benchmark datasets, the proposed DBTS achieves state-of-the-art performance. Specifically, DBTS, using different backbones, outperforms established baseline models by approximately 25% in mIoU on the Zurich dataset and by over 26% in mIoU on the ACDC dataset, demonstrating the effectiveness of our method in addressing the challenges of domain-adaptive nighttime segmentation.},
  archive      = {J_ICV},
  author       = {Ruiying Chen and Yunan Liu and Yuming Bo and Mingyu Lu},
  doi          = {10.1016/j.imavis.2024.105211},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105211},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-branch teacher-student with noise-tolerant learning for domain adaptive nighttime segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). W-shaped network combined with dual transformers and edge
protection for multi-focus image fusion. <em>ICV</em>, <em>150</em>,
105210. (<a href="https://doi.org/10.1016/j.imavis.2024.105210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a W-shaped network combined with dual transformers and edge protection is proposed for multi-focus image fusion. Different from the traditional Convolutional Neural Network (CNN) fusion method, a heterogeneous encoder network framework is designed for feature extraction, and a decoder is used for feature reconstruction. The purpose of this design is to preserve the local details and edge information of the source image to the maximum extent possible. Specifically, the first encoder uses adaptive average pooling to downsample the source image and extract important features from it. The source image pair for edge detection using the Gaussian Modified Laplace Operator (GMLO) is used as input for the second encoder, and adaptive maximum pooling is employed for downsampling. In addition, the encoder part of the network combines CNN and Transformer to extract both local and global features. By reconstructing the extracted feature information, the final fusion image is obtained. To evaluate the performance of this method, we compared 16 recent multi-focus image fusion methods and conducted qualitative and quantitative analyses. Experimental results on public datasets such as Lytro, MFFW, MFI-WHU, and the real scene dataset HBU-CVMDSP demonstrate that our method can accurately identify the focused and defocused regions of source images. It also preserves the edge details of the source images while extracting the focused regions.},
  archive      = {J_ICV},
  author       = {Hao Zhai and Yun Chen and Yao Wang and Yuncan Ouyang and Zhi Zeng},
  doi          = {10.1016/j.imavis.2024.105210},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105210},
  shortjournal = {Image Vis. Comput.},
  title        = {W-shaped network combined with dual transformers and edge protection for multi-focus image fusion},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UTR: A UNet-like transformer for efficient unsupervised
medical image registration. <em>ICV</em>, <em>150</em>, 105209. (<a
href="https://doi.org/10.1016/j.imavis.2024.105209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing medical image registration algorithms have the problem of low registration accuracy when processing large deformation medical images. In order to improve registration performance and utilize the global context extraction ability of Transformers without causing high computational complexity, a UNet-like Transformer model combining CNN and Transformer was constructed for 3D medical image registration tasks. We use the Efficient Global Local Attention (EGLA) mechanism to construct a Transformer encoder to further address the difficulty of modeling long-distance dependencies in existing medical image registration networks. We leverage the local modeling capabilities of CNN and the long-distance information capture capabilities of Transformer to achieve high-precision registration. The algorithm has undergone detailed validation experiments on two public datasets. The qualitative and quantitative registration results validate the effectiveness of the proposed model.},
  archive      = {J_ICV},
  author       = {Wei Qiu and Lianjin Xiong and Ning Li and Yaobin Wang and Yangsong Zhang},
  doi          = {10.1016/j.imavis.2024.105209},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105209},
  shortjournal = {Image Vis. Comput.},
  title        = {UTR: A UNet-like transformer for efficient unsupervised medical image registration},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual temporal memory network with high-order spatio-temporal
graph learning for video object segmentation. <em>ICV</em>,
<em>150</em>, 105208. (<a
href="https://doi.org/10.1016/j.imavis.2024.105208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Typically, Video Object Segmentation (VOS) always has the semi-supervised setting in the testing phase. The VOS aims to track and segment one or several target objects in the following frames in the sequence, only given the ground-truth segmentation mask in the initial frame. A fundamental issue in VOS is how to best utilize the temporal information to improve the accuracy. To address the aforementioned issue, we provide an end-to-end framework that simultaneously extracts long-term and short-term historical sequential information to current frame as temporal memories. The integrated temporal architecture consists of a short-term and a long-term memory modules. Specifically, the short-term memory module leverages a high-order graph-based learning framework to simulate the fine-grained spatial–temporal interactions between local regions across neighboring frames in a video, thereby maintaining the spatio-temporal visual consistency on local regions. Meanwhile, to relieve the occlusion and drift issues, the long-term memory module employs a Simplified Gated Recurrent Unit (S-GRU) to model the long evolutions in a video. Furthermore, we design a novel direction-aware attention module to complementarily augment the object representation for more robust segmentation. Our experiments on three mainstream VOS benchmarks, containing DAVIS 2017, DAVIS 2016, and Youtube-VOS, demonstrate that our proposed solution provides a fair tradeoff performance between both speed and accuracy.},
  archive      = {J_ICV},
  author       = {Jiaqing Fan and Shenglong Hu and Long Wang and Kaihua Zhang and Bo Liu},
  doi          = {10.1016/j.imavis.2024.105208},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105208},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual temporal memory network with high-order spatio-temporal graph learning for video object segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthetic lidar point cloud generation using deep generative
models for improved driving scene object recognition. <em>ICV</em>,
<em>150</em>, 105207. (<a
href="https://doi.org/10.1016/j.imavis.2024.105207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imbalanced distribution of different object categories poses a challenge for training accurate object recognition models in driving scenes. Supervised machine learning models trained on imbalanced data are biased and easily overfit the majority classes, such as vehicles and pedestrians, which appear more frequently in driving scenes. We propose a novel data augmentation approach for object recognition in lidar point cloud of driving scenes, which leverages probabilistic generative models to produce synthetic point clouds for the minority classes and complement the original imbalanced dataset. We evaluate five generative models based on different statistical principles, including Gaussian mixture model, variational autoencoder, generative adversarial network, adversarial autoencoder and the diffusion model. Experiments with a real-world autonomous driving dataset show that the synthetic point clouds generated for the minority classes by the Latent Generative Adversarial Network result in significant improvement of object recognition performance for both minority and majority classes. The codes are available at https://github.com/AAAALEX-XIANG/Synthetic-Lidar-Generation .},
  archive      = {J_ICV},
  author       = {Zhengkang Xiang and Zexian Huang and Kourosh Khoshelham},
  doi          = {10.1016/j.imavis.2024.105207},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105207},
  shortjournal = {Image Vis. Comput.},
  title        = {Synthetic lidar point cloud generation using deep generative models for improved driving scene object recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DynaSeg: A deep dynamic fusion method for unsupervised image
segmentation incorporating feature similarity and spatial continuity.
<em>ICV</em>, <em>150</em>, 105206. (<a
href="https://doi.org/10.1016/j.imavis.2024.105206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work tackles the fundamental challenge of image segmentation in computer vision, which is crucial for diverse applications. While supervised methods demonstrate proficiency, their reliance on extensive pixel-level annotations limits scalability. We introduce DynaSeg, an innovative unsupervised image segmentation approach that overcomes the challenge of balancing feature similarity and spatial continuity without relying on extensive hyperparameter tuning. Unlike traditional methods, DynaSeg employs a dynamic weighting scheme that automates parameter tuning, adapts flexibly to image characteristics, and facilitates easy integration with other segmentation networks. By incorporating a Silhouette Score Phase, DynaSeg prevents undersegmentation failures where the number of predicted clusters might converge to one. DynaSeg uses CNN-based and pre-trained ResNet feature extraction, making it computationally efficient and more straightforward than other complex models. Experimental results showcase state-of-the-art performance, achieving a 12.2% and 14.12% mIOU improvement over current unsupervised segmentation approaches on COCO-All and COCO-Stuff datasets, respectively. We provide qualitative and quantitative results on five benchmark datasets, demonstrating the efficacy of the proposed approach. Code available at \url{ https://github.com/RyersonMultimediaLab/DynaSeg }},
  archive      = {J_ICV},
  author       = {Boujemaa Guermazi and Riadh Ksantini and Naimul Khan},
  doi          = {10.1016/j.imavis.2024.105206},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105206},
  shortjournal = {Image Vis. Comput.},
  title        = {DynaSeg: A deep dynamic fusion method for unsupervised image segmentation incorporating feature similarity and spatial continuity},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triplet-set feature proximity learning for video anomaly
detection. <em>ICV</em>, <em>150</em>, 105205. (<a
href="https://doi.org/10.1016/j.imavis.2024.105205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of anomalies in videos is a particularly complex visual challenge, given the wide variety of potential real-world events. To address this issue, our paper introduces a unique approach for detecting divergent behavior in surveillance videos, utilizing triplet-loss for video anomaly detection. Our method involves selecting a triplet set of video segments from normal (n) and abnormal (a) data points for deep feature learning. We begin by creating a database of triplet sets of two types: a-a-n and n-n-a. By computing a triplet loss, we model the proximity between n-n chunks and the distance between ‘a’ chunks from the n-n ones. Additionally, we train the deep network to model the closeness of a-a chunks and the divergent behavior of ‘n’ from the a-a chunks. The model acquired in the initial stage can be viewed as a prior, which is subsequently employed for modeling normality. As a result, our method can leverage the advantages of both straightforward classification and normality modeling-based techniques. We also present a data selection mechanism for the efficient generation of triplet sets. Furthermore, we introduce a novel video anomaly dataset, AnoVIL, designed for human-centric anomaly detection. Our proposed method is assessed using the UCF-Crime dataset encompassing all 13 categories, the IIT-H accident dataset, and AnoVIL. The experimental findings demonstrate that our method surpasses the current state-of-the-art approaches. We conduct further evaluations of the performance, considering various configurations such as cross-dataset evaluation, loss functions, siamese structure, and embedding size. Additionally, an ablation study is carried out across different settings to provide insights into our proposed method.},
  archive      = {J_ICV},
  author       = {Kuldeep Marotirao Biradar and Murari Mandal and Sachin Dube and Santosh Kumar Vipparthi and Dinesh Kumar Tyagi},
  doi          = {10.1016/j.imavis.2024.105205},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105205},
  shortjournal = {Image Vis. Comput.},
  title        = {Triplet-set feature proximity learning for video anomaly detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel approach for fast structured light framework using
deep learning. <em>ICV</em>, <em>150</em>, 105204. (<a
href="https://doi.org/10.1016/j.imavis.2024.105204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In structured light 3D imaging, achieving robust and accurate 3D reconstruction with a limited number of fringe patterns remains a challenge. In this study, we introduce SFNet, a symmetric fusion network that designed for high-speed, high-quality 3D surface measurement using just two fringe images. The SFNet employs separate encoders and decoders for each fringe input to estimate its phase. The two generated phase values are then utilized to reconstruct the 3D information. During the training process, we use a refined reference phase which utilizes fringe images with different frequencies. SFNet has the capability to complement the additional frequency information by fusing the feature maps extracted from each encoder. Comparative experiments and ablation studies validate the effectiveness of our proposed method. The dataset is publicly accessible on our project page https://wonhoe-kim.github.io/SFNet/ .},
  archive      = {J_ICV},
  author       = {Won-Hoe Kim and Bongjoong Kim and Hyung-Gun Chi and Jae-Sang Hyun},
  doi          = {10.1016/j.imavis.2024.105204},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105204},
  shortjournal = {Image Vis. Comput.},
  title        = {Novel approach for fast structured light framework using deep learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D-ISRNet: Generating 3D point clouds through image
similarity retrieval in a complex background from a single image.
<em>ICV</em>, <em>150</em>, 105203. (<a
href="https://doi.org/10.1016/j.imavis.2024.105203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing the 3D shape of objects using limited information from a single image is a complex and necessary task. The existing methods for reconstructing point clouds from a single image haven&#39;t effectively addressed the issue of background interference in the input image, thus greatly limited their practical application. To address the underlying issue, this paper proposes a 3D-ISRNet aiming to enhance the accuracy of point cloud reconstruction through image similarity retrieval. The 3D-ISRNet retrieves point clouds similar to the object and employs spatial and channel attention mechanisms for image encoding. This aims to enhance the features of the reconstruction area and improve reconstruction accuracy. After the fusion of retrieved point clouds with image features, the combined data undergoes a symmetric encoder to generate the reconstructed point clouds, thereby alleviating the constraints imposed by the initial point clouds on the reconstruction results. 3D-ISRNet demonstrates favorable results on public and real-world datasets.},
  archive      = {J_ICV},
  author       = {Lianming Chen and Yong Tong and Ning Yang and Yipeng Zuo and Muhammad Ilyas Menhas and Bilal Ahmad and Hui Chen},
  doi          = {10.1016/j.imavis.2024.105203},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105203},
  shortjournal = {Image Vis. Comput.},
  title        = {3D-ISRNet: Generating 3D point clouds through image similarity retrieval in a complex background from a single image},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MPE-DETR: A multiscale pyramid enhancement network for
object detection in low-light images. <em>ICV</em>, <em>150</em>,
105202. (<a href="https://doi.org/10.1016/j.imavis.2024.105202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has broad applications in areas such as autonomous driving, security surveillance, and deep-sea exploration. However, the performance of current detection algorithms significantly decreases due to the loss of detail, increased noise, and color distortion in images under low-light or nighttime conditions. To address this problem, we propose a plug-and-play multiscale pyramid enhancement network (MPENet), which elegantly cascades with RT-DETR to establish an end-to-end framework for low-light object detection, named MPE-DETR. First, MPENet utilizes Gaussian blur to decompose images into Gaussian pyramids and Laplacian pyramids at different resolutions. Specifically, we designed a high-frequency texture enhancement (HTE) module to capture the edge and texture information of images, and a low-frequency noise smoothing (LNS) module to better understand the overall structure of images and capture global-scale features. Additionally, by concatenating the output features of the HTE and LNS modules along the channel dimension, feature fusion across different scales is realized. We conducted experiments on the ExDark and ExDark + LOD datasets, which are designed for low-light object detection. The results indicate that the proposed method achieved an improvement of 2.1% in mAP@.5 compared to that of existing SOTA models on the ExDark dataset, and demonstrated strong generalizability and robustness on the ExDark + LOD dataset. The code and results are available at https://github.com/PZDJL/MPENet .},
  archive      = {J_ICV},
  author       = {Rui Xue and Jialu Duan and Zhengwei Du},
  doi          = {10.1016/j.imavis.2024.105202},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105202},
  shortjournal = {Image Vis. Comput.},
  title        = {MPE-DETR: A multiscale pyramid enhancement network for object detection in low-light images},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visible thermal person re-identification via multi-branch
modality residual complementary learning. <em>ICV</em>, <em>150</em>,
105201. (<a href="https://doi.org/10.1016/j.imavis.2024.105201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-thermal Person Re-identification (VT Re-ID) is a challenging task in all-weather surveillance system. Existing methods concentrate on extracting the modality-shared features, ignoring the discriminative inter-modality complementary features. To tackle this issue, we propose a multi-branch modality residual complementary learning method which consists of the modality residual complementary learning (MRCL) module and the multi-branch feature learning (MBFL) module. The MRCL module can be easily integrated into existing CNN baselines and drive the network to focus on both intra-modality and inter-modality information. On one hand, we adopt the basic two-stream network to obtain the intra-modality features, on the other hand, we capture the inter-modality complementary features within the residual image obtained by cross-modality correlation saliency erasing operation. To handle the intra-modality variations, we employ the MBFL module to capture local spatial features and local channel features, then integrate them with global features to achieve part-to-part and high-level semantic information matching. Finally, the discriminability and robustness of the ultimate representations are enhanced by multi-branch constraint loss learning. Extensive experiments on RegDB and SYSU-MM01 datasets demonstrate the superiority of our proposed method compared with state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Long Chen and Rui Sun and Yiheng Yu and Yun Du and Xudong Zhang},
  doi          = {10.1016/j.imavis.2024.105201},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105201},
  shortjournal = {Image Vis. Comput.},
  title        = {Visible thermal person re-identification via multi-branch modality residual complementary learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Segmentation-aware MRI subsampling for efficient cardiac MRI
reconstruction with reinforcement learning. <em>ICV</em>, <em>150</em>,
105200. (<a href="https://doi.org/10.1016/j.imavis.2024.105200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) scans, though highly detailed and non-invasive, take significantly longer than Computed Tomography (CT) scans and are sensitive to motion during acquisition. Accelerating MRI sampling and improving image reconstruction quality are crucial, especially for dynamic regions like the heart. Existing methods primarily enhance overall image quality but seldom target specific anatomic regions. In this paper, we propose a novel approach that combines segmentation and reinforcement learning to accelerate cardiac MRI sampling and enhance the reconstruction quality of cardiac regions. We design a policy network using reinforcement learning, where the input is a combination of the reconstructed image and the segmented category probability feature map, and the output determines the next k-space line to sample in a Cartesian setup. Retrospective testing on the ACDC (Automated Cardiac Diagnosis Challenge) cardiac segmentation dataset shows that our method significantly improves both the cardiac region and overall image quality compared to variants without segmentation rewards. Our approach ensures dynamically accelerated k-space sampling and surpasses current state-of-the-art reinforcement learning methods in producing diagnostically superior reconstructed cardiac MR images.},
  archive      = {J_ICV},
  author       = {Ruru Xu and Ilkay Oksuz},
  doi          = {10.1016/j.imavis.2024.105200},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105200},
  shortjournal = {Image Vis. Comput.},
  title        = {Segmentation-aware MRI subsampling for efficient cardiac MRI reconstruction with reinforcement learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generous teacher: Good at distilling knowledge for student
learning. <em>ICV</em>, <em>150</em>, 105199. (<a
href="https://doi.org/10.1016/j.imavis.2024.105199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a technique that aims to transfer valuable knowledge from a large, well-trained model (the teacher) to a lightweight model (the student), with the primary goal of improving the student&#39;s performance on a given task. In recent years, mainstream distillation methods have focused on modifying student learning styles, resulting in less attention being paid to the knowledge provided by the teacher. However, upon re-examining the knowledge transferred by the teacher, we find that it still has untapped potential, which is crucial to bridging the performance gap between teachers and students. Therefore, we study knowledge distillation from the teacher&#39;s perspective and introduce a novel teacher knowledge enhancement method termed “Generous Teacher.” The Generous Teacher is a specially trained teacher model that can provide more valuable knowledge for the student model. This is achieved by integrating a standardly trained teacher (Standard Teacher) to assist in the training process of the Generous Teacher. As a result, the Generous Teacher accomplishes the task at hand and assimilates distilled knowledge from the Standard Teacher, effectively adapting to distillation teaching in advance. Specifically, we recognize that non-target class knowledge plays a crucial role in improving the distillation effect for students. To leverage this, we decouple logit outputs and selectively use the Standard Teacher&#39;s non-target class knowledge to enhance the Generous Teacher. By setting the temperature as a multiple of the logit standard deviation, we ensure that the additional knowledge absorbed by the Generous Teacher is more suitable for student distillation. Experimental results on standard benchmarks demonstrate that the Generous Teacher surpasses the Standard Teacher in terms of accuracy when applied to standard knowledge distillation. Furthermore, the Generous Teacher can be seamlessly integrated into existing distillation methods, bringing general improvements at a low additional computational cost. The code will be publicly available at https://github.com/EifelTing/Generous-Teacher .},
  archive      = {J_ICV},
  author       = {Yifeng Ding and Gaoming Yang and Shuting Yin and Ji Zhang and Xianjin Fang and Wencheng Yang},
  doi          = {10.1016/j.imavis.2024.105199},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105199},
  shortjournal = {Image Vis. Comput.},
  title        = {Generous teacher: Good at distilling knowledge for student learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ZSDT: Zero-shot domain translation for real-world
super-resolution. <em>ICV</em>, <em>150</em>, 105198. (<a
href="https://doi.org/10.1016/j.imavis.2024.105198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the significant advancements that deep learning has brought to single image super-resolution (SISR), challenges persist in the real world due to unknown degradation and the lack of ground truth priors, rendering existing fully supervised SISR methods tailored for preset degradation inapplicable in real scenarios. Utilizing image-specific internal priors for zero-shot learning can help mitigate this issue. However, the richness of internal information varies across different images, and existing zero-shot SISR methods that rely solely on internal priors cannot guarantee reconstruction stability. To address the adaptability issues of fully supervised methods and the stability issues of zero-shot methods, this paper proposes a Zero-Shot Domain Translation (ZSDT) framework. Specifically, ZSDT comprises two stages: domain translation and upsampling. In the domain translation stage, an image-specific zero-shot domain translation network is employed to narrow the gap between real and preset degraded images. We design a Gradient Adaptive Coordinate Attention (GACA) module to enhance the domain translation network&#39;s capability to fit internal priors. Concurrently, a Diffusion-Guided Local High-Frequency Data Augmentation (DG-LHDA) algorithm is proposed to improve the network&#39;s ability to restore high-frequency texture details. In the upsampling stage, existing SISR models designed for preset degradation are used to achieve stable, high-quality reconstruction. Extensive qualitative and quantitative evaluations on real and pseudo-real datasets demonstrate that ZSDT is a general SISR framework, highly compatible with existing SISR models for preset degradation, and significantly enhances their performance on real-world low-resolution images. Compared to state-of-the-art zero-shot SISR methods, ZSDT exhibits superior reconstruction performance.},
  archive      = {J_ICV},
  author       = {Mei Yu and Yeting Deng and Jie Gao and Han Jiang and Xuzhou Fu and Xuewei Li and Zhiqiang Liu},
  doi          = {10.1016/j.imavis.2024.105198},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105198},
  shortjournal = {Image Vis. Comput.},
  title        = {ZSDT: Zero-shot domain translation for real-world super-resolution},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise-robust re-identification with triple-consistency
perception. <em>ICV</em>, <em>150</em>, 105197. (<a
href="https://doi.org/10.1016/j.imavis.2024.105197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional re-identification (ReID) methods heavily rely on clean and accurately annotated training data, rendering them susceptible to label noise in real-world scenarios. Although some noise-robust learning methods have been proposed and achieved promising recognition performance, however, most of these methods are designed for the image classification task and they are not suitable in ReID (engaging in the association and matching of objects rather than solely focusing on their identification). To address this problem, in this paper, we propose a Triple-consistency Perception based Noise-robust Re-identification Model (TcP-ReID), in which we make the model mine and focus more on the clean samples and reliable relationships among samples from different perspectives. Specifically, the self-consistency strategy guides the model to emphasize and prioritize clean samples, thereby preventing overfitting to noise labels during the initial stages of model training. Rather than focusing solely on individual samples, the context-consistency loss exploits similarities between samples in the feature space, promoting predictions for each sample to align with those of its nearest neighbors. Moreover, to further enforce the robustness of our model, a Jensen-Shannon divergence based cross-view consistency loss is introduced by encouraging the consistency of samples across different views. Extensive experiments demonstrate the superiority of the proposed TcP-ReID over the competing methods under instance-dependent noise and instance-independent noise. For instance, on the Market1501 dataset, our method achieves 85.8% in rank-1 accuracy and 56.3% in mAP score (5.6% and 8.7% improvements) under instance-independent noise with noise ratio 50% , and similarly 5.7% and 1.4% under instance-dependent label noise.},
  archive      = {J_ICV},
  author       = {Xueping Wang and Zhanpeng Shao and Shixi Luo and Jiazheng Wang and Min Liu and Jianhua Dai and Jun Cheng},
  doi          = {10.1016/j.imavis.2024.105197},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105197},
  shortjournal = {Image Vis. Comput.},
  title        = {Noise-robust re-identification with triple-consistency perception},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised medical image segmentation via cross
teaching between MobileNet and MobileViT. <em>ICV</em>, <em>150</em>,
105196. (<a href="https://doi.org/10.1016/j.imavis.2024.105196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning methods that use a combination of convolutional neural networks and Transformers have shown excellent results in both completely supervised and semi-supervised medical image segmentation tasks. This study provides a simple and effective framework for semi-supervised medical image segmentation by introducing cross teaching between MobileNet and MobileViT. Specifically, we built two different two-path parallel semantic segmentation networks with MobileNet and MobileViT as the main modules. Cross teaching between MobileNet and MobileViT was performed, in which the prediction of each network was used as a pseudo-label to supervise the training of other networks in a direct end-to-end manner. Finally, experiments on two common benchmark tests showed that the proposed framework was superior to six existing semi-supervised learning methods. This shows that our framework can effectively use unlabeled data to improve the performance and is superior to the latest semi-supervised segmentation method. Codes are available at: github.com/yywbkn/MV2-cross-teaching-MobileViT .},
  archive      = {J_ICV},
  author       = {Yuan Yang and Lin Zhang and Lei Ren},
  doi          = {10.1016/j.imavis.2024.105196},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105196},
  shortjournal = {Image Vis. Comput.},
  title        = {Semi-supervised medical image segmentation via cross teaching between MobileNet and MobileViT},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven 2D-EWT based diabetic retinopathy identification
using hybrid neural network. <em>ICV</em>, <em>150</em>, 105194. (<a
href="https://doi.org/10.1016/j.imavis.2024.105194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is one of the primary reason of blindness over the globe. During DR, retina deteriorates continuously with the presence of lesions. It is difficult to detect DR in early stage and manual diagnosis is resource-intensive. To full-fill the requirement, two-dimensional empirical wavelet transform (2D-EWT) and composite neural network based automated DR diagnosis system is developed. The informative part of fundus image is extracted from the given image. The 2D-EWT computes different number of boundaries in magnitude spectrum followed by different modes. Therefore, a data-driven uniform boundary identification method is suggested for DR classification purpose. The acquired decomposed modes are given to the composite neural network for classification. The developed system is assessed on three different datasets MESSIDOR-1, MESSIDOR-2, and AsiaPacific Tele Ophthalmology Society (APTOS) dataset on which proposed method achieved 75%, 83.82%, and 89.28% accuracy, respectively. Efficacy of the proposed system is superior than the compared methods in terms of accuracy, area under the curve, precision, specificity, and sensitivity.},
  archive      = {J_ICV},
  author       = {Amit Rawat and Maheshwari Prasad Singh and Rishi Raj Sharma},
  doi          = {10.1016/j.imavis.2024.105194},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105194},
  shortjournal = {Image Vis. Comput.},
  title        = {Data-driven 2D-EWT based diabetic retinopathy identification using hybrid neural network},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ricci curvature based volumetric segmentation. <em>ICV</em>,
<em>150</em>, 105192. (<a
href="https://doi.org/10.1016/j.imavis.2024.105192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The level set method has played a critical role among many image segmentation approaches. Several edge detectors, such as the gradient, have been applied to its regularisation term. However, traditional edge detectors lack high-order information and are sensitive to image noise. To tackle this problem, we introduce a method to calculate the Ricci curvature, a vital curvature in three-dimensional Riemannian geometry. In addition, we propose incorporating the curvature into the regularisation term. Experiments suggest that our method outperforms the state-of-the-art level set methods and achieves a comparable result with the Swin UNETR and Segment Anything.},
  archive      = {J_ICV},
  author       = {Na Lei and Jisui Huang and Ke Chen and Yuxue Ren and Emil Saucan and Zhenchang Wang and Yuanyuan Shang},
  doi          = {10.1016/j.imavis.2024.105192},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105192},
  shortjournal = {Image Vis. Comput.},
  title        = {Ricci curvature based volumetric segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative feature-driven image replay for continual
learning. <em>ICV</em>, <em>150</em>, 105187. (<a
href="https://doi.org/10.1016/j.imavis.2024.105187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are prone to catastrophic forgetting when trained incrementally on different tasks. Popular incremental learning methods mitigate such forgetting by retaining a subset of previously seen samples and replaying them during the training on subsequent tasks. However, this is not always possible, e.g., due to data protection regulations. In such restricted scenarios, one can employ generative models to replay either artificial images or hidden features to a classifier. In this work, we propose Genifer ( GEN erat I ve FE ature-driven image R eplay), where a generative model is trained to replay images that must induce the same hidden features as real samples when they are passed through the classifier. Our technique therefore incorporates the benefits of both image and feature replay, i.e.: (1) unlike conventional image replay, our generative model explicitly learns the distribution of features that are relevant for classification; (2) in contrast to feature replay, our entire classifier remains trainable; and (3) we can leverage image-space augmentations, which increase distillation performance while also mitigating overfitting during the training of the generative model. We show that Genifer substantially outperforms the previous state of the art for various settings on the CIFAR-100 and CUB-200 datasets. The code is available at: https://github.com/kevthan/feature-driven-image-replay .},
  archive      = {J_ICV},
  author       = {Kevin Thandiackal and Tiziano Portenier and Andrea Giovannini and Maria Gabrani and Orcun Goksel},
  doi          = {10.1016/j.imavis.2024.105187},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105187},
  shortjournal = {Image Vis. Comput.},
  title        = {Generative feature-driven image replay for continual learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video object segmentation based on dynamic perception update
and feature fusion. <em>ICV</em>, <em>150</em>, 105156. (<a
href="https://doi.org/10.1016/j.imavis.2024.105156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current popular video object segmentation algorithms based on memory network indiscriminately update the frame information to the memory pool, fails to make reasonable use of the historical frame information, causing frame information redundancy in the memory pool, resulting in the increase of the computation amount. At the same time, the mask refinement method is relatively rough, resulting in blurred edges of the generated mask. To solve these problems, This paper proposes a video object segmentation algorithm based on dynamic perception update and feature fusion. In order to reasonably utilize the historical frame information, a dynamic perception update module is proposed to selectively update the segmentation frame mask. Meanwhile, a mask refinement module is established to enhance the detail information of the shallow features of the backbone network. This module uses a double kernels fusion block to fuse the different scale information of the features, and finally uses the Laplacian operator to sharpen the edges of the mask. The experimental results show that on the public datasets DAVIS2016, DAVIS2017 and YouTube-VOS 18 , the comprehensive performance of the algorithm in this paper reaches 86.9%, 79.3% and 71.6%, respectively, and the segmentation speed reaches 15FPS on the DAVIS2016 dataset. Compared with many mainstream algorithms in recent years, it has obvious advantages in performance.},
  archive      = {J_ICV},
  author       = {Zhiqiang Hou and Fucheng Li and Jiale Dong and Nan Dai and Sugang Ma and Jiulun Fan},
  doi          = {10.1016/j.imavis.2024.105156},
  journal      = {Image and Vision Computing},
  month        = {10},
  pages        = {105156},
  shortjournal = {Image Vis. Comput.},
  title        = {Video object segmentation based on dynamic perception update and feature fusion},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visionary vigilance: Optimized YOLOV8 for fallen person
detection with large-scale benchmark dataset. <em>ICV</em>,
<em>149</em>, 105195. (<a
href="https://doi.org/10.1016/j.imavis.2024.105195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falls pose a significant risk to elderly people, patients with diseases such as neurological disorders, cardiovascular diseases, and disabled children. This highlights the need for real-time intelligent fall detection (FD) systems for quick relief leading to assisted living. The existing attempts are often based on multimodal approaches which are computationally expensive due to multi-sensor integration. The computer vision (CV) based era for FD needs the deployment of state-of-the-art (SOTA) networks with progressive enhancements to grasp falls effectively. However, CV-based systems often lack the ability to operate efficiently in real-time and the attempts for visual intelligence are usually not integrated at feasible stages of the networks. More importantly, the lack of large-scale well-annotated benchmark datasets limits the ability of FD in challenging and complex environments. To bridge the research gaps, we proposed an enhanced version of YOLOV8 for FD. Our research presents significant contributions by addressing these limitations through three key contributions. Initially, a comprehensive large-scale dataset is introduced which comprises approximately 10,500 image samples with corresponding annotations. The dataset encompasses diverse environmental conditions and scenarios, facilitating the generalization ability for the models. Then, progressive enhancements to the YOLOV8S model are proposed, integrating a focus module in the backbone to optimize feature extraction. Moreover, the convolutional block attention modules (CBAMs) are integrated at the feasible stages of the network to improve spatial and channel contexts for more accurate detection, especially in complex scenes. Finally, an extensive empirical evaluation showcases the superiority of the proposed network over 13 SOTA techniques, substantiated by meticulous benchmarking and qualitative validation across varied environments. The empirical findings and analysis of multiple factors such as model performance, size, and processing time prove that the suggested network displays impressive results. Datasets with annotations, results, and the ways of progressive modifications in the code will be available to the research community at the link https://github.com/habib1402/Fall-Detection-DiverseFall10500},
  archive      = {J_ICV},
  author       = {Habib Khan and Inam Ullah and Mohammad Shabaz and Muhammad Faizan Omer and Muhammad Talha Usman and Mohammed Seghir Guellil and JaKeoung Koo},
  doi          = {10.1016/j.imavis.2024.105195},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105195},
  shortjournal = {Image Vis. Comput.},
  title        = {Visionary vigilance: Optimized YOLOV8 for fallen person detection with large-scale benchmark dataset},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing cervical cancer diagnosis: Integrated
attention-transformer system with weakly supervised learning.
<em>ICV</em>, <em>149</em>, 105193. (<a
href="https://doi.org/10.1016/j.imavis.2024.105193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer screening through cytopathological images poses a significant challenge due to the intricate nature of cancer cells, often resulting in high misdiagnosis rates. This study presents the Integrated Attention-Transformer System (IATS), a pioneering framework designed to enhance the precision and efficiency of cervical cancer cell image analysis, surpassing the capabilities of existing deep learning models. Instead of relying solely on convolutional neural networks (CNNs), IATS leverages the power of transformers, a recently emerged architecture, to holistically capture both global and local features within the images. It employs a multi-pronged approach: Vision Transformer (ViT) module captures the overall spatial context and interactions between cells, providing a crucial understanding of potential cancer patterns. Token-to-token module zooms in on individual cells, meticulously examining subtle malignant features that might be missed by CNNs. SeNet integration with ResNet101 and DenseNet169 refines feature extraction by dynamically analyzing the importance of different features captured by these popular deep learning architectures. SeNet acts like a skilled analyst, prioritizing the most informative features for accurate cancer cell identification. Weighted voting combines the insights from each module, leading to robust and accurate identification, minimizing misdiagnosis risk. The proposed framework achieves an impressive accuracy of 98.44% on Mendeley dataset and 95.88% on SIPaKMeD dataset, outperforming 25 deep learning models, which included Convolutional Neural Network (CNN) and Vision Transformer (VT) models. These results reveal a 2.5% accuracy improvement compared to the best-performing CNN model on the Mendeley dataset. This significant advancement holds the potential to revolutionize cervical cancer screening by substantially reducing misdiagnosis rates and improving patient outcomes. While this study focuses on model performance, future work will explore its computational efficiency and real-world clinical integration to ensure its broader impact on patient care.},
  archive      = {J_ICV},
  author       = {Ashfaque Khowaja and Beiji Zou and Xiaoyan Kui},
  doi          = {10.1016/j.imavis.2024.105193},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105193},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing cervical cancer diagnosis: Integrated attention-transformer system with weakly supervised learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale segmentation net for segregating heterogeneous
brain tumors: Gliomas on multimodal MR images. <em>ICV</em>,
<em>149</em>, 105191. (<a
href="https://doi.org/10.1016/j.imavis.2024.105191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, the 3D volumetric segmentation of heterogeneous brain tumors such as Gliomas- anaplastic astrocytoma, and Glioblastoma Multiforme (GBM) is performed to extract enhancing tumor (ET), whole tumor (WT), and tumor core (TC) regions using T1, T2, and FLAIR images. Therefore, a deep learning-based encoder-decoder architecture named “MS-SegNet” using 3D multi-scale convolutional layers is proposed. The proposed architecture employs multi-scale feature extraction (MS-FE) block the filter size 3 × 3 × 3 to extract confined information like tumor boundary and edges of necrotic part. The filter of size 5 × 5 × 5 focuses on varied features like shape, size, and location of tumor region with edema. The local and global features from different MR modalities are extracted for segmenting thin and meshed boundaries of heterogeneous tumors between anatomical sub-regions like peritumoral edema, enhancing tumor, and necrotic tumor core. The learning parameters on introducing the MS-FE block are reduced to 10 million, which is much less than other architectures like 3D-Unet which takes into consideration 27 million features leading to the consumption of less computational power. A customized loss function is also prophesied based on a combination of dice loss and focal loss along with metrics such as accuracy and Intersection over Union (IoU) i.e. the overlapping of ground truth mask and predicted value for addressing the class imbalance problem. For evaluating the efficacy of the proposed method, four evaluation metrics such as Dice Coefficient (DSC), Sensitivity, Specificity, and Hausdorff95 distance (H95) are employed for analyzing the model&#39;s overall performance. It is observed that the proposed MS-SegNet architecture achieved the DSC of 0.81, 0.91, and 0.83 on BraTS 2020; 0.86, 0.92, and 0.84 on BraTS 2021 for ET, WT, and TC respectively. The developed model is also tested on a real-time dataset collected from the Post Graduate Institute of Medical Education &amp; Research (PGIMER), Chandigarh. The DSC of 0.79, 0.76, and 0.68 for ET, WT, and TC respectively on the real-time dataset. These findings show that deep learning models with enhanced feature extraction capabilities can be readily trained to attain high accuracy in segmenting heterogeneous brain tumors and hold promising results. In the future, other tumor datasets will be explored for the detection and treatment planning of brain tumors to check the effectiveness of the model in real-world healthcare environments.},
  archive      = {J_ICV},
  author       = {Jainy Sachdeva and Deepanshu Sharma and Chirag Kamal Ahuja},
  doi          = {10.1016/j.imavis.2024.105191},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105191},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiscale segmentation net for segregating heterogeneous brain tumors: Gliomas on multimodal MR images},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LDConv: Linear deformable convolution for improving
convolutional neural networks. <em>ICV</em>, <em>149</em>, 105190. (<a
href="https://doi.org/10.1016/j.imavis.2024.105190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks based on convolutional operations have achieved remarkable results in the field of deep learning , but there are two inherent flaws in standard convolutional operations. On the one hand, the convolution operation is confined to a local window, so it cannot capture information from other locations, and its sampled shapes is fixed. On the other hand, the size of the convolutional kernel is fixed to k × × k, which is a fixed square shape, and the number of parameters tends to grow squarely with size. Although Deformable Convolution (Deformable Conv) address the problem of fixed sampling of standard convolutions, the number of parameters also tends to grow in a squared manner, and Deformable Conv do not explore the effect of different initial sample shapes on network performance. In response to the above questions, the Linear Deformable Convolution (LDConv) is explored in this work, which gives the convolution kernel an arbitrary number of parameters and arbitrary sampled shapes to provide richer options for the trade-off between network overhead and performance. In LDConv, a novel coordinate generation algorithm is defined to generate different initial sampled positions for convolutional kernels of arbitrary size. To adapt to changing targets, offsets are introduced to adjust the shape of the samples at each position. LDConv corrects the growth trend of the number of parameters for standard convolution and Deformable Conv to a linear growth. Compared to Deformable Conv, LDConv provides richer choices and can be equivalent to deformable convolution when the number of parameters of LDConv is set to the square of K. Differently, this paper also explores the effect of neural networks by using LDConv with the same size and different initial sampling shapes. LDConv completes the process of efficient feature extraction by irregular convolutional operations and brings more exploration options for convolutional sampled shapes. Object detection experiments on representative datasets COCO2017, VOC 7 + 12, and VisDrone-DET2021 fully demonstrate the advantages of LDConv. LDConv is a plug-and-play convolutional operation that can replace the convolutional operation to improve network performance. The code for the relevant tasks can be found at https://github.com/CV-ZhangXin/LDConv .},
  archive      = {J_ICV},
  author       = {Xin Zhang and Yingze Song and Tingting Song and Degang Yang and Yichen Ye and Jie Zhou and Liming Zhang},
  doi          = {10.1016/j.imavis.2024.105190},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105190},
  shortjournal = {Image Vis. Comput.},
  title        = {LDConv: Linear deformable convolution for improving convolutional neural networks},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label recognition in open driving scenarios based on
bipartite-driven superimposed dynamic graph. <em>ICV</em>, <em>149</em>,
105189. (<a href="https://doi.org/10.1016/j.imavis.2024.105189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-label image recognition task is widely prevalent in real-world scenarios. Overcoming the issue of overlapping and densely packed objects in complex scenes is crucial. For instance, in traffic scenarios, there are overlaps and close proximity among pedestrians, various types of vehicles, and signage. However, a primary obstacle in leveraging label relationships to enhance image classification lies in effectively integrating label semantic topology information with the image data itself. In this paper, we propose a novel framework, the Bipartite-driven Superimposed Dynamic Graph Convolutional Network (Bi-SDNet), augmented with Mapping Alignment Module (MAM) and Semantic Decoupling Module(SDM). Our approach initially decomposes input features into representations capable of discerning category label semantics at multiple scales, facilitated by MAM and SDM modules. Furthermore, through the meticulously designed Superimposed Dynamic Graph, we adeptly capture content-aware category relationships for each image, effectively modeling the relationships between these representations for the final recognition task. We conducted extensive experiments on publicly available benchmark datasets and the traffic scene dataset WZ-traffic. The model achieved an impressive 87.5% mean average precision (mAP) on the MS-COCO dataset and a commendable 91% mAP on the WZ-traffic dataset. Our research introduces novel techniques and significant breakthroughs in this field, furnishing powerful tools for enhancing model performance.},
  archive      = {J_ICV},
  author       = {Xu Wu and Suting Chen},
  doi          = {10.1016/j.imavis.2024.105189},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105189},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-label recognition in open driving scenarios based on bipartite-driven superimposed dynamic graph},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). QSMT-net: A query-sensitive proposal and multi-temporal-span
matching network for video grounding. <em>ICV</em>, <em>149</em>,
105188. (<a href="https://doi.org/10.1016/j.imavis.2024.105188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video grounding task aims to retrieve moments from the videos corresponding to a given textual query. This task poses significant challenges because of the need to comprehend the semantic content of both videos and sentences as well as manage the matching relationship between modalities. Existing approaches struggle to effectively meet this challenge, as they often lack consideration for the diversity in constructing proposals to fit segments from varied scenes and disregard the multi-temporal scale matching relationship between queries and proposals. In this paper, we propose the Query-Sensitive Proposal and Multi-Temporal-Span Matching Network (QSMT-Net), an innovative framework designed to generate more distinctive proposals and to enhance the matching between queries and candidate proposals over varying temporal spans. First, we fortify the connection between modes by instituting fine-grained interactions between video clips and textual words. Subsequently, through a learnable pooling mechanism, we dynamically construct candidate proposals tailored to specific queries, thus implementing a query-sensitive proposal generation strategy. Second, we enhanced the model&#39;s ability to differentiate adjacent candidate proposals through the multi-temporal-span matching network, which facilitated selecting the most accurate proposal results under various time scales. Experiments on three widely used benchmarks, Charades-STA, TACoS and ActivityNet Captions, our approach demonstrated significant improvements over state-of-the-art methods, indicating promising advancements in video grounding.},
  archive      = {J_ICV},
  author       = {Qingqing Wu and Lijun Guo and Rong Zhang and Jiangbo Qian and Shangce Gao},
  doi          = {10.1016/j.imavis.2024.105188},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105188},
  shortjournal = {Image Vis. Comput.},
  title        = {QSMT-net: A query-sensitive proposal and multi-temporal-span matching network for video grounding},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring holistic discriminative representation for
micro-expression recognition via contrastive learning. <em>ICV</em>,
<em>149</em>, 105186. (<a
href="https://doi.org/10.1016/j.imavis.2024.105186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based micro-expression recognition (MER) has been remarkably successful in the affective computing and computer vision communities. However, the most challenging issue that hinders the performance of MER is low intensity. Instead of forcefully transforming the input from micro-expressions to exaggerated micro-expressions by a fixed video motion magnification factor, our approach introduces a sophisticated pretext task with an intensity-agnostic strategy to enhance the discriminative capacity of each micro-expression sample holistically through contrastive transfer learning. This strategy enables us to progressively transfer knowledge and leverage the rich facial expression information from macro-expression samples. In addition, we reconsider that the core of the MER task is to refine and incorporate the instance-level and class-level discriminative features from the initial indistinguishable information. As a result, we jointly merge the two views to learn a holistic-level representation. Simultaneously, to ensure a strong association and guidance between the instance-level view and the class-level view, we maintain their consistency through an alignment loss. The results showed that the proposed method could significantly improve the performance of MER on CASME II, SAMM, SMIC, and CAS(ME) 3 datasets.},
  archive      = {J_ICV},
  author       = {Jie Zhu and Wanyuan He and Feifan Wang and Hongli Chang and Cheng Lu and Yuan Zong},
  doi          = {10.1016/j.imavis.2024.105186},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105186},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring holistic discriminative representation for micro-expression recognition via contrastive learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel facial expression recognition model based on
harnessing complementary features in multi-scale network with attention
fusion. <em>ICV</em>, <em>149</em>, 105183. (<a
href="https://doi.org/10.1016/j.imavis.2024.105183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method for facial expression recognition using the proposed feature complementation and multi-scale attention model with attention fusion (FCMSA-AF). The proposed model consists of four main components: the shallow feature extractor module, parallel structured two-branch multi-scale attention module (MSA), feature complementing module (FCM), and attention fusion and classification module. The MSA module contains multi-scale attention modules in a cascaded fashion in two paths to learn diverse features. The upper and lower paths use left and right multi-scale blocks to extract and aggregate the features at different receptive fields. The attention networks in MSA focus on salient local regions to extract features at granular levels . The FCM uses the correlation between the feature maps in two paths to make the multi-scale attention features complementary to each other. Finally, the complementary features are fused through an attention network to form an informative holistic feature which includes subtle, visually varying regions in similar classes. Hence, complementary and informative features are used in classification to minimize information loss and capture the discriminating finer aspects of facial expression recognition. Experimental evaluation of the proposed model carried out on AffectNet and CK + datasets achieve accuracies of 64.59% and 98.98%, respectively, outperforming some of the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Chakrapani Ghadai and Dipti Patra and Manish Okade},
  doi          = {10.1016/j.imavis.2024.105183},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105183},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel facial expression recognition model based on harnessing complementary features in multi-scale network with attention fusion},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relational-branchformer: Novel framework for audio-visual
speech recognition. <em>ICV</em>, <em>149</em>, 105182. (<a
href="https://doi.org/10.1016/j.imavis.2024.105182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study embraced the state-of-the-art Branchformer series architecture within the realm of automatic speech recognition , supplanting the widely utilized Conformer architecture. This substitution offers an innovative remedy tailored to audio-visual speech recognition tasks. Building upon the Branchformer architecture, enhancements were made, culminating in the proposal of the Relational-Branchformer (R-Branchformer). The convolutional attention relation module was innovatively incorporated to augment the connectivity between the local and global branches by meticulously considering their interrelations and interplays. Consequently, this module facilitates the mutual embedding of local and global contextual information, ultimately leading to a substantial enhancement in model performance. Our model was grounded in the utilization of the connectionist temporal classification (CTC) loss, wherein intermediate CTC losses were incorporated between blocks. Moreover, through the reference and enhancement of the gated interlayer collaboration module, which superseded the inter CTC module, the conditional independence assumption intrinsic to the CTC model was effectively relaxed. As a consequence, this augmentation markedly bolstered the overall performance of our model. Furthermore, the audio-visual output enhancement module was proposed, which adeptly assimilates information from both audio and visual modalities to enrich the representation of audio-visual information. Consequently, the R-Branchformer model achieved remarkable word error rates of 1.7% and 1.5% on the LRS2 and LRS3 test sets, respectively, exemplifying its state-of-the-art performance in audio-visual speech recognition tasks.},
  archive      = {J_ICV},
  author       = {Yewei Xiao and Xuanming Liu and Aosu Zhu and Jian Huang},
  doi          = {10.1016/j.imavis.2024.105182},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105182},
  shortjournal = {Image Vis. Comput.},
  title        = {Relational-branchformer: Novel framework for audio-visual speech recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring global context and position-aware representation
for group activity recognition. <em>ICV</em>, <em>149</em>, 105181. (<a
href="https://doi.org/10.1016/j.imavis.2024.105181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the context and position information in the scene for group activity understanding. Firstly, previous group activity recognition methods strive to reason on individual features without considering the information in the scene. Besides correlations among actors, we argue that integrating the scene context simultaneously can afford us more useful and supplementary cues. Therefore, we propose a new network, termed Contextual Transformer Network (CTN), to incorporate global contextual information into individual representations. In addition, the position of individuals also plays a vital role in group activity understanding. Unlike previous methods that explore correlations among individuals semantically, we propose Clustered Position Embedding (CPE) to integrate the spatial structure of actors and produce position-aware representations. Experimental results on two widely used datasets for sports video and social activity (i.e., Volleyball and Collective Activity datasets) show that the proposed method outperforms state-of-the-art approaches. Especially, when using ResNet-18 as the backbone, our method achieves 93.6/93.9% MCA/MPCA on the Volleyball dataset and 95.4/96.3% MCA/MPCA on the Collective Activity dataset.},
  archive      = {J_ICV},
  author       = {Zexing Du and Qing Wang},
  doi          = {10.1016/j.imavis.2024.105181},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105181},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring global context and position-aware representation for group activity recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantics feature sampling for point-based 3D object
detection. <em>ICV</em>, <em>149</em>, 105180. (<a
href="https://doi.org/10.1016/j.imavis.2024.105180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, 3D object detection is a research hotspot in the field of computer vision . In this paper, we have observed that the commonly used set abstraction module retains excessive irrelevant background information during downsampling, impacting object detection precision. To address this, we propose a mixed sampling method. During point feature extraction, we integrate semantic features into the sampling process, guiding the set abstraction module to sample foreground points. In order to leverage the high-quality 3D proposals generated in the first stage, we have developed a virtual point pooling module aimed at acquiring the features of these proposals. This module facilitates the capture of more comprehensive and resilient ROI features. Experimental results on the KITTI test set show a 3.51% higher Average Precision (AP) compared to the PointRCNN baseline, particularly for moderately challenging car classes, highlighting the effectiveness of our approach.},
  archive      = {J_ICV},
  author       = {Jing-Dong Huang and Ji-Xiang Du and Hong-Bo Zhang and Huai-Jin Liu},
  doi          = {10.1016/j.imavis.2024.105180},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105180},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantics feature sampling for point-based 3D object detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint training strategy of unimodal and multimodal for
multimodal sentiment analysis. <em>ICV</em>, <em>149</em>, 105172. (<a
href="https://doi.org/10.1016/j.imavis.2024.105172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of social media video content, research on multimodal sentiment analysis (MSA) has attracted considerable attention recently. Despite significant progress in MSA, there remains challenges: current research mostly focuses on learning either unimodal features or aspects of multimodal interactions, neglecting the importance of simultaneously considering both unimodal features and intermodal interactions. To address the aforementioned challenges, this paper proposes a fusion strategy called Joint Training of Unimodal and Multimodal (JTUM). Specifically, this strategy combines unimodal label generation module with cross-modal transformer. The unimodal label generation module aims to generate more distinctive labels for each unimodal input, facilitating more effective learning of unimodal representations. Meanwhile, cross-modal transformer is designed to treat each modality as a target modality and optimize it using other modalities as source modalities, thereby learning the interactions between each pair of modalities. By jointly training unimodal and multimodal tasks, our model can focus on individual modality features while learning the interactions between modalities. Finally, to better capture temporal information and make predictions, we also added self-attention transformer as sequence models. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate that JTUM outperforms current main methods.},
  archive      = {J_ICV},
  author       = {Meng Li and Zhenfang Zhu and Kefeng Li and Lihua Zhou and Zhen Zhao and Hongli Pei},
  doi          = {10.1016/j.imavis.2024.105172},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105172},
  shortjournal = {Image Vis. Comput.},
  title        = {Joint training strategy of unimodal and multimodal for multimodal sentiment analysis},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EVA-02: A visual representation for neon genesis.
<em>ICV</em>, <em>149</em>, 105171. (<a
href="https://doi.org/10.1016/j.imavis.2024.105171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open &amp; accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304 M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1 K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1 K, outperforming the previous largest &amp; best open-sourced CLIP with only ∼ 1/6 parameters and ∼ 1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6 M to 304 M parameters, all with impressive performance. To facilitate open access and open research, we release the complete suite of EVA-02 to the community at https://github.com/baaivision/EVA/tree/master/EVA-02 .},
  archive      = {J_ICV},
  author       = {Yuxin Fang and Quan Sun and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
  doi          = {10.1016/j.imavis.2024.105171},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105171},
  shortjournal = {Image Vis. Comput.},
  title        = {EVA-02: A visual representation for neon genesis},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoNPL: Consistency training framework with noise-aware
pseudo labeling for dense pose estimation. <em>ICV</em>, <em>149</em>,
105170. (<a href="https://doi.org/10.1016/j.imavis.2024.105170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense pose estimation faces hurdles due to the lack of costly precise pixel-level IUV labels. Existing methods aim to overcome it by regularizing model outputs or interpolating pseudo labels. However, conventional geometric transformations often fall short, and pseudo labels may introduce unwanted noise, leading to continued challenges in rectifying inaccurate estimations. We introduced a novel Consistency training framework with Noise-aware Pseudo Labeling (CoNPL) to tackle problems in learning from unlabeled pixels. CoNPL employs both weak and strong augmentations in a shared model to enhance robustness against aggressive transformations. To address noisy pseudo labels, CoNPL integrates a Noise-aware Pseudo Labeling (NPL) module, which consists of a Noise-Aware Module (NAM), and Noise-Resistant Learning (NRL) modules. NAM identifies misclassifications and incorrect UV coordinates using binary classification and regression, while NRL dynamically adjusts loss weights to filter out uncertain samples, thereby stabilizing learning from pseudo labels. Our method demonstrates a + 2.0% improvement in AP on the DensePose-COCO benchmark across different networks, achieving state-of-the-art performance. On the Ultrapose and DensePose-Chimps benchmark, our method also demonstrates a + 2.7% and + 3.0% improvement in AP.},
  archive      = {J_ICV},
  author       = {Jiaxiao Wen and Tao Chu and Junyao Sun and Qiong Liu},
  doi          = {10.1016/j.imavis.2024.105170},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105170},
  shortjournal = {Image Vis. Comput.},
  title        = {CoNPL: Consistency training framework with noise-aware pseudo labeling for dense pose estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Event-driven weakly supervised video anomaly detection.
<em>ICV</em>, <em>149</em>, 105169. (<a
href="https://doi.org/10.1016/j.imavis.2024.105169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the observations of human working manners, this work proposes an event-driven method for weakly supervised video anomaly detection. Complementary to the conventional snippet-level anomaly detection, this work designs an event analysis module to predict the event-level anomaly scores as well. It first generates event proposals simply via a temporal sliding window and then constructs a cascaded causal transformer to capture temporal dependencies for potential events of varying durations. Moreover, a dual-memory augmented self-attention scheme is also designed to capture global semantic dependencies for event feature enhancement. The network is learned with a standard multiple instance learning (MIL) loss, together with normal-abnormal contrastive learning losses. During inference, the snippet- and event-level anomaly scores are fused for anomaly detection. Experiments show that the event-level analysis helps to detect anomalous events more continuously and precisely. The performance of the proposed method on three public datasets demonstrates that the proposed approach is competitive with state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Shengyang Sun and Xiaojin Gong},
  doi          = {10.1016/j.imavis.2024.105169},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105169},
  shortjournal = {Image Vis. Comput.},
  title        = {Event-driven weakly supervised video anomaly detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transferable dual multi-granularity semantic excavating for
partially relevant video retrieval. <em>ICV</em>, <em>149</em>, 105168.
(<a href="https://doi.org/10.1016/j.imavis.2024.105168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially Relevant Video Retrieval (PRVR) aims to retrieve partially relevant videos from many unlabeled and untrimmed videos according to the query, which is defined as the multiple instance learning problem. The challenge of PRVR is that it utilizes untrimmed videos, which are much closer to reality. The existing methods excavate video-text semantic consistency information insufficiently and lack the capacity to highlight the semantics of key representations. To tackle these issues, we propose a transferable dual multi-granularity semantic excavating network, called T-D3N, to focus on enhancing the learning of dual-modal representations. Specifically, we first introduce a novel transferable textual semantic learning strategy by designing Adaptive Multi-scale Semantic Mining (AMSM) component to excavate significant textual semantic from multiple perspectives. Second, T-D3N distinguishes the feature differences from the frame-wise perspective to better perform contrastive learning between positive and negative samples in the video feature domain, which can further distance the positive and negative samples and improve the probability of positive samples being retrieved by query. Finally, our model constructs multi-grained video temporal dependencies and conducts cross-grained core feature perception, which enables more sufficient multimodal interactions. Extensive experiments are performed on three benchmarks, i.e., ActivityNet Captions, Charades-STA, and TVR, our T-D3N achieves state-of-the-art results. Furthermore, we also confirm that our model is transferable on a broad range of multimodal tasks such as T2VR, VMR, and MMSum.},
  archive      = {J_ICV},
  author       = {Dingxin Cheng and Shuhan Kong and Bin Jiang and Qiang Guo},
  doi          = {10.1016/j.imavis.2024.105168},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105168},
  shortjournal = {Image Vis. Comput.},
  title        = {Transferable dual multi-granularity semantic excavating for partially relevant video retrieval},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PMANet: Progressive multi-stage attention networks for skin
disease classification. <em>ICV</em>, <em>149</em>, 105166. (<a
href="https://doi.org/10.1016/j.imavis.2024.105166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated skin disease classification is crucial for the timely diagnosis of skin lesions . However, accurate skin disease classification presents a challenge, given the significant intra-class variation and inter-class similarity among different kinds of skin diseases. Previous studies have attempted to address this issue by identifying the most discriminative part of a lesion, but they tend to overlook the interactions between multi-scale features. In this paper, we propose a Progressive Multi-stage Attention Network (PMANet) to enhance the learning of multi-scale discriminative features , so that the model can gradually localize from stable fine-grained to coarse-grained regions in order to improve the accuracy of disease classification. Specifically, we utilize a progressive multi-stage network to supervise feature and classification, thereby fostering multi-scale information and improving the model&#39;s ability to learn intra-class consistent information. Additionally, we propose an enhanced region proposal block that highlights key discriminative features and suppresses background noise of lesions, reinforcing the learning of inter-class discriminative features. Furthermore, we propose a multi-branch feature fusion block that effectively fuses multi-scale lesion features from different stages. Comprehensive experiments conducted on two datasets substantiate the effectiveness and superiority of the proposed method in accurately classifying skin disease.},
  archive      = {J_ICV},
  author       = {Guangzhe Zhao and Chen Zhang and Xueping Wang and Benwang Lin and Feihu Yan},
  doi          = {10.1016/j.imavis.2024.105166},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105166},
  shortjournal = {Image Vis. Comput.},
  title        = {PMANet: Progressive multi-stage attention networks for skin disease classification},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical disentangled representation for image denoising
and beyond. <em>ICV</em>, <em>149</em>, 105165. (<a
href="https://doi.org/10.1016/j.imavis.2024.105165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a typical ill-posed problem due to complex degradation. Leading methods based on normalizing flows have tried to solve this problem with an invertible transformation instead of a deterministic mapping. However, it is difficult to construct feasible bijective mapping to remove spatial-variant noise while recovering fine texture and structure details due to latent ambiguity in inverse problems . Inspired by a common observation that noise tends to appear in the high-frequency part of the image, we propose a fully invertible denoising method that injects the idea of disentangled learning into a general invertible architecture to split noise from the high-frequency part. More specifically, we decompose the noisy image into clean low-frequency and hybrid high-frequency parts with an invertible transformation and then disentangle case-specific noise and high-frequency components in the latent space. In this way, denoising is made tractable by inversely merging noiseless low and high-frequency parts. Furthermore, we construct a flexible hierarchical disentangling framework, which aims to decompose most of the low-frequency image information while disentangling noise from the high-frequency part in a coarse-to-fine manner. Extensive experiments on real image denoising, JPEG compressed artifact removal, and medical low-dose CT image restoration have demonstrated that the proposed method achieves competitive performance on both quantitative metrics and visual quality, with significantly less computational cost.},
  archive      = {J_ICV},
  author       = {Wenchao Du and Hu Chen and Yi Zhang and Hongyu Yang},
  doi          = {10.1016/j.imavis.2024.105165},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105165},
  shortjournal = {Image Vis. Comput.},
  title        = {Hierarchical disentangled representation for image denoising and beyond},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-powered trustable and explainable fall detection system
using transfer learning. <em>ICV</em>, <em>149</em>, 105164. (<a
href="https://doi.org/10.1016/j.imavis.2024.105164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accidental falls pose a significant public health challenge, especially among vulnerable populations. To address this issue, comprehensive research on fall detection and rescue systems is essential. Vision-based technologies, with their promising potential, offer an effective means to detect falls. This research paper presents a cutting-edge fall detection methodology aimed at enhancing individual safety and well-being. The proposed methodology utilizes deep neural networks , leveraging their capabilities to drive advancements in fall detection. To overcome data limitations and computational efficiency concerns, this study employ transfer learning by fine-tuning pre-trained models on large-scale image datasets for fall detection. This approach significantly enhances model performance, enabling better generalization and accuracy, especially in real-time applications with constrained resources. Notably, the methodology achieved an impressive test accuracy of 98.15%. Additionally, the incorporation of Explainable Artificial Intelligence (XAI) techniques is used to ensure transparent and trustworthy decision-making in fall detection using deep learning models , especially in critical healthcare contexts for vulnerable individuals. XAI provides valuable insights into complex model architectures and parameters, enabling a deeper understanding of fall identification patterns . To evaluate the effectiveness of this approach, a rigorous experimentation was conducted using a diverse dataset containing real-world fall and non-fall scenarios. The results demonstrate substantial improvements in both accuracy and interpretability , confirming the superiority of this method over conventional fall detection approaches.},
  archive      = {J_ICV},
  author       = {Aryan Nikul Patel and Ramalingam Murugan and Praveen Kumar Reddy Maddikunta and Gokul Yenduri and Rutvij H. Jhaveri and Yaodong Zhu and Thippa Reddy Gadekallu},
  doi          = {10.1016/j.imavis.2024.105164},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105164},
  shortjournal = {Image Vis. Comput.},
  title        = {AI-powered trustable and explainable fall detection system using transfer learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial immune systems for data augmentation.
<em>ICV</em>, <em>149</em>, 105163. (<a
href="https://doi.org/10.1016/j.imavis.2024.105163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study object detection models and observe that their respective architectures are vulnerable to image distortions such as noise, compression, blur, or snow. We propose alleviating this problem by training the models with antibodies generated using Artificial Immune Systems (AIS) from original training samples (antigens). These antibodies are AIS-distorted antigens at the pixel level through cycles of “select, clone, mutate, select” until an affinity to the antigen is achieved. We then add the antibodies to the antigens, train the models, validate and test them under 15 distortions, and show that our data augmentation approach (AISbod) significantly improved their accuracy without altering their architecture or inference speed. For example, the DINO object detector under the COCO dataset improves by 4% under clean samples, by 6.50% on average over all 15 distortions, by 2.15% under snow, and by 27.60% under impulse noise. Our simulations show that our method performs better under distortions and clean samples than related defense methods and is more consistent across datasets and object detection models. For instance, our method is, on average, 70% better than the closest related method across 15 distortions for the evaluated models under COCO. Moreover, we show that our approach to image classification and object tracking models significantly improves accuracy under distortions. We provide the code of our method and the DINO model trained using our method at https://github.com/moforio/AISbod .},
  archive      = {J_ICV},
  author       = {Mark Ofori-Oduro and Maria Amer},
  doi          = {10.1016/j.imavis.2024.105163},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105163},
  shortjournal = {Image Vis. Comput.},
  title        = {Artificial immune systems for data augmentation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid attention transformer with re-parameterized large
kernel convolution for image super-resolution. <em>ICV</em>,
<em>149</em>, 105162. (<a
href="https://doi.org/10.1016/j.imavis.2024.105162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image super-resolution is a well-established low-level vision task that aims to reconstruct high-resolution images from low-resolution images. Methods based on Transformer have shown remarkable success and achieved outstanding performance in SISR tasks. While Transformer effectively models global information, it is less effective at capturing high frequencies such as stripes that primarily provide local information . Additionally, it has the potential to further enhance the capture of global information. To tackle this, we propose a novel Large Kernel Hybrid Attention Transformer using re-parameterization. It combines different kernel sizes and different steps re-parameterized convolution layers with Transformer to effectively capture global and local information to learn comprehensive features with low-frequency and high-frequency information. Moreover, in order to solve the problem of using batch normalization layer to introduce artifacts in SISR, we propose a new training strategy which is fusing convolution layer and batch normalization layer after certain training epochs. This strategy can enjoy the acceleration convergence effect of batch normalization layer in training and effectively eliminate the problem of artifacts in the inference stage. For re-parameterization of multiple parallel branch convolution layers, adopting this strategy can further reduce the amount of calculation of training. By coupling these core improvements, our LKHAT achieves state-of-the-art performance for single image super-resolution task.},
  archive      = {J_ICV},
  author       = {Zhicheng Ma and Zhaoxiang Liu and Kai Wang and Shiguo Lian},
  doi          = {10.1016/j.imavis.2024.105162},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105162},
  shortjournal = {Image Vis. Comput.},
  title        = {Hybrid attention transformer with re-parameterized large kernel convolution for image super-resolution},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel infrared and visible image fusion algorithm based on
global information-enhanced attention network. <em>ICV</em>,
<em>149</em>, 105161. (<a
href="https://doi.org/10.1016/j.imavis.2024.105161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of infrared and visible images aims to extract and fuse thermal target information and texture details to the fullest extent possible, enhancing the visual understanding capabilities of images for both humans and computers in complex scenes. However, existing methods have difficulties in preserving the comprehensiveness of source image feature information and enhancing the saliency of image texture information. Therefore, we put forward a novel infrared and visible image fusion algorithm based on global information-enhanced attention network (GIEA). Specifically, we develop an attention-guided Transformer module (AGTM) to make sure the fused images have enough global information. This module combines the convolutional neural network and Transformer to perform adequate feature extraction from shallow to deep layers, and utilize the attention network for multi-level feature-guided learning. Then, we build the contrast enhancement module (CENM), which enhances the feature representation and contrast of the image so that the fused image contains significant texture information. Furthermore, our network is driven to fully preserve the texture and structure details of the source images with a loss function that consists of content loss and total variance loss. Numerous experiments demonstrate that our fusion approach outperforms other fusion approaches in both subjective and objective assessments.},
  archive      = {J_ICV},
  author       = {Jia Tian and Dong Sun and Qingwei Gao and Yixiang Lu and Muxi Bao and De Zhu and Dawei Zhao},
  doi          = {10.1016/j.imavis.2024.105161},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105161},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel infrared and visible image fusion algorithm based on global information-enhanced attention network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GDM-depth: Leveraging global dependency modelling for
self-supervised indoor depth estimation. <em>ICV</em>, <em>149</em>,
105160. (<a href="https://doi.org/10.1016/j.imavis.2024.105160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised depth estimation algorithms eschew depth ground truth and employ the convolutional U-Net with a fixed receptive field which confines its focus primarily to nearby spatial distances. These factors obscure adequate supervision during image reconstruction, consequently hindering accurate depth estimation, particularly in complex indoor scenes. The pure transformer framework can perform global modelling to provide more semantic information. However, the cost is significant. To tackle these challenges, we introduce GDM-Depth, which utilizes global dependency modelling to offer more precise depth guidance from the network itself. Initially, we propose integrating learnable tree filters with unary terms, leveraging the structural properties of spanning trees to facilitate efficient long-range interactions. Subsequently, instead of replacing the convolutional framework entirely, we employ the transformer to design a scale-aware global feature extractor, establishing global relationships among local features at various scales, achieving both efficiency and cost-effectiveness. Furthermore, inter-class disparities between depth global and local features are observed. To address this issue, we introduce the global feature injector to further enhance the representation. GDM-Depth&#39;s effectiveness is demonstrated on the NYUv2, ScanNet, and InteriorNet depth datasets, achieving impressive test set performances of 87.2%, 83.1%, and 76.1% in key indicators δ &lt; 0.125 δ&amp;lt;0.125 , respectively.},
  archive      = {J_ICV},
  author       = {Chen Lv and Chenggong Han and Jochen Lang and He Jiang and Deqiang Cheng and Jiansheng Qian},
  doi          = {10.1016/j.imavis.2024.105160},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105160},
  shortjournal = {Image Vis. Comput.},
  title        = {GDM-depth: Leveraging global dependency modelling for self-supervised indoor depth estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FTAN: Frame-to-frame temporal alignment network with
contrastive learning for few-shot action recognition. <em>ICV</em>,
<em>149</em>, 105159. (<a
href="https://doi.org/10.1016/j.imavis.2024.105159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current few-shot action recognition approaches follow the metric learning paradigm , measuring the distance of any sub-sequences (frames, any frame combinations or clips) between different actions for classification. However, this disordered distance metric between action sub-sequences ignores the long-term temporal relations of actions, which may result in significant metric deviations. What&#39;s more, the distance metric suffers from the distinctive temporal distribution of different actions, including intra-class temporal offsets and inter-class local similarity. In this paper, a novel few-shot action recognition framework, Frame-to-frame Temporal Alignment Network ( FTAN ), is proposed to address the above challenges. Specifically, an attention-based temporal alignment ( ATA ) module is devised to calculate the distance between corresponding frames of different actions along the temporal dimension to achieve frame-to-frame temporal alignment. Meanwhile, the Temporal Context module ( TCM ) is proposed to increase inter-class diversity by enriching the frame-level feature representation, and the Frames Cyclic Shift Module ( FCSM ) performs frame-level temporal cyclic shift to reduce intra-class inconsistency. In addition, we present temporal and global contrastive objectives to assist in learning discriminative and class-agnostic visual features. Experimental results show that the proposed architecture achieves state-of-the-art on HMDB51, UCF101, Something-Something V2 and Kinetics-100 datasets.},
  archive      = {J_ICV},
  author       = {Bin Yu and Yonghong Hou and Zihui Guo and Zhiyi Gao and Yueyang Li},
  doi          = {10.1016/j.imavis.2024.105159},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105159},
  shortjournal = {Image Vis. Comput.},
  title        = {FTAN: Frame-to-frame temporal alignment network with contrastive learning for few-shot action recognition},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task disagreement-reducing multimodal sentiment fusion
network. <em>ICV</em>, <em>149</em>, 105158. (<a
href="https://doi.org/10.1016/j.imavis.2024.105158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal sentiment analysis models can effectively capture sentimental commonalities between different modalities and possess high sentimental acquisition capability. However, there are still shortcomings in the model&#39;s analysis and recognition abilities when dealing with samples that exhibit sentimental polarity disagreement between different modalities. Additionally, the dominance of the text modality in multimodal models, particularly those pre-trained with BERT, can hinder the learning of other modalities due to its richer semantic information. This issue becomes particularly pronounced in cases where there is a conflict between multimodal and textual sentimental polarities, often leading to suboptimal analytical results. Besides, the classification ability of each modality is also suppressed by single-task learning. In this paper, We propose a Multi-Task disagreement-Reducing Multimodal Sentiment Fusion Network (MtDr-MSF), designed to enhance the semantic information of non-text modalities and reduce the dominant impact of the textual modality on the model, and to improve the learning capabilities of unimodal networks. We conducted experiments on multimodal sentiment analysis datasets, CMU-MOSI, CMU-MOSEI, and CH-SIMS. The results show that our method outperforms the current SOTA method.},
  archive      = {J_ICV},
  author       = {Wang Zijun and Jiang Naicheng and Chao Xinyue and Sun Bin},
  doi          = {10.1016/j.imavis.2024.105158},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105158},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-task disagreement-reducing multimodal sentiment fusion network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-parallel CNN-transformer fusion network for semantic
change detection. <em>ICV</em>, <em>149</em>, 105157. (<a
href="https://doi.org/10.1016/j.imavis.2024.105157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic change detection (SCD) can recognize the region and the type of changes in remote sensing images. Existing methods are either based on transformer or convolutional neural network (CNN), but due to the size of various ground objects is different, it is necessary to have global modeling ability and local information extraction ability at the same time. Therefore, in this paper we propose a fusion semantic change detection network (FSCD) with both global modeling ability and local information extraction ability by fusing transformer and CNN. A semi-parallel fusion block has also been proposed to construct FSCD. It can not only have global and local features in parallel, but also fuse them as deeply as serial. To better adaptively decide which mechanism is applied to which pixel, we design a self-attention and convolution selection module (ACSM). ACSM is a self-attention mechanism used to selectively combine transformer and CNN. Specifically, the importance of each mechanism is automatically obtained by learning. According to the importance, the mechanism suitable for a pixel is selected, which is better than using either mechanism alone. We evaluate the proposed FSCD on two datasets, and the proposed network has a significant improvement compared with the state-of-the-art network.},
  archive      = {J_ICV},
  author       = {Changzhong Zou and Ziyuan Wang},
  doi          = {10.1016/j.imavis.2024.105157},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105157},
  shortjournal = {Image Vis. Comput.},
  title        = {A semi-parallel CNN-transformer fusion network for semantic change detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An instance-level data balancing method for object detection
via contextual information alignment. <em>ICV</em>, <em>149</em>,
105155. (<a href="https://doi.org/10.1016/j.imavis.2024.105155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imbalance issues in object detection training data , such as in categories, scales, and spatial distribution, result in detection models failing to effectively fit unbalanced data. Our method aims to mitigate the performance disparity caused by data imbalance from the perspective of instance-level augmentation. Firstly, we designed a dynamic data balancing mechanism (DDBM) to develop category expansion rate, scale ratio rules, and spatial distribution indicators to alleviate data imbalance. Then, based on the pixel-level fine-grained context, a fine-grained local object instance augmentation (FLOIA) method is designed to selectively copy the object instance according to the background Mosaic degree. In addition, based on the coarse-grained global context and dynamic balancing mechanism, we proposes a coarse-grained global object instance augmentation (CGOIA) method to establish an object-background association, ensure the alignment of the context information of the object instance and alleviate the data imbalance. We train the proposed instance augmentation-treated datasets on various models, resulting in improved balance across different categories and scales. Additionally, visual analysis validates that this approach balances spatial distributions while conforming to contextual information. Furthermore, this method proves advantageous for training with small-sample datasets.},
  archive      = {J_ICV},
  author       = {Fang Luo and Jiaxing Ma and G.T.S. Ho},
  doi          = {10.1016/j.imavis.2024.105155},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105155},
  shortjournal = {Image Vis. Comput.},
  title        = {An instance-level data balancing method for object detection via contextual information alignment},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GAN-BodyPose: Real-time 3D human body pose data key point
detection and quality assessment assisted by generative adversarial
network. <em>ICV</em>, <em>149</em>, 105144. (<a
href="https://doi.org/10.1016/j.imavis.2024.105144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of deep learning and computer vision, these technologies are becoming increasingly vital in areas like virtual reality, medical diagnosis, and sports training. Existing methods for real-time 3D human body pose keypoint detection and quality assessment face significant challenges such as insufficient detection accuracy, low computational efficiency, and high data quality requirements. To address these challenges, we propose an innovative solution, GAN-BodyPose. This approach integrates 3D convolutional neural networks, self-attention mechanisms, and generative adversarial networks to deliver efficient and accurate detection and assessment in real time. The GAN-BodyPose framework combines 3D-CNN and self-attention for effective feature extraction and keypoint detection, enhanced further by generative adversarial networks for superior data quality and accuracy. Our extensive evaluations using a large-scale 3D human body pose dataset demonstrated that GAN-BodyPose outperforms traditional methods, showing improvements in processing speed (15% faster), accuracy in terms of Mean Per Joint Position Error (reduced by approximately 2.2%), and an Area Under the Curve (AUC) score increased by approximately 9.5% compared to HR-Net and other datasets. Additionally, it achieves lower Floating-Point Operations (FLOPs) by about 9.3%, indicating more efficient computational performance. These advancements underline the potential of our approach to significantly enhance user experiences in virtual reality, motion capture, and other real-time applications. The successful application of GAN-BodyPose promises greater efficiency and precision in fields ranging from game development to medical diagnostics, and robust support for human-computer interaction and gesture recognition. This research represents a substantial contribution to deep learning applications in robot control, decision-making, and broadens the research foundation in these domains.},
  archive      = {J_ICV},
  author       = {Xicheng Zhu and Xinchen Ye},
  doi          = {10.1016/j.imavis.2024.105144},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105144},
  shortjournal = {Image Vis. Comput.},
  title        = {GAN-BodyPose: Real-time 3D human body pose data key point detection and quality assessment assisted by generative adversarial network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STAFFormer: Spatio-temporal adaptive fusion transformer for
efficient 3D human pose estimation. <em>ICV</em>, <em>149</em>, 105142.
(<a href="https://doi.org/10.1016/j.imavis.2024.105142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing two-stage methods for 3D Human Pose Estimation often use 2D poses as input, which are then lifted to obtain 3D representations. This typically involves frame-by-frame estimation, whether in 2D or 3D, resulting in high computational demands unsuitable for edge devices. Due to the continuity of human movement, the differences between adjacent frames can be minimal, a question arises: is frame-by-frame estimation necessary? Previous works demonstrated the feasibility of using Transformer-based models to estimate poses with sparse frames, focusing on either temporal or spatial dependencies but neglecting holistic spatio-temporal correlations. To address this, we introduce the Spatio-Temporal Adaptive Fusion Transformer (STAFFormer). First, STAFFormer recovers dense temporal frames from sparsely sampled ones obtained from a 2D pose estimator through the Temporal Dense Frame Recovery (TDFR) module. This significantly reduces the computational complexity . Second, STAFFormer employs an adaptive fusion attention mechanism , enhancing accuracy by attentively navigating both spatial and temporal dimensions through the Spatio-Temporal Adaptive Fusion (STAF) module. Furthermore, We introduce a kinematic coherence loss to adapt to subtle joint movements, improving pose estimation fidelity. Finally, we explore the possibility of integrating different pre-training strategies using extensive marker-based datasets. Experimental results on challenging datasets show our network achieves state-of-the-art performance with low computational complexity.},
  archive      = {J_ICV},
  author       = {Feng Hao and Fujin Zhong and Hong Yu and Jun Hu and Yan Yang},
  doi          = {10.1016/j.imavis.2024.105142},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105142},
  shortjournal = {Image Vis. Comput.},
  title        = {STAFFormer: Spatio-temporal adaptive fusion transformer for efficient 3D human pose estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M2VAD: Multiview multimodality transformer-based weakly
supervised video anomaly detection. <em>ICV</em>, <em>149</em>, 105139.
(<a href="https://doi.org/10.1016/j.imavis.2024.105139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video Anomaly Detection (VAD) under a weakly supervised setting involves operating with limited video-level annotations. The practical significance of this work plays a crucial role in applications related to surveillance and security. A commonly adopted strategy within existing Weakly Supervised VAD (WS-VAD) methods involves utilizing various modalities as inputs such as audio and video. This integration is motivated by the ability of these modalities to provide abundant discriminative capabilities in addressing the complexities of diverse real-world scenarios. However, the integration of multimodal data poses challenges like misalignment of temporal features and the need for efficient fusion strategies. In response to these challenges, a transformer-based deep architecture based on Multiview and Multimodality WS-VAD named M 2 VAD M2VAD is proposed. The proposed M 2 VAD M2VAD utilizes different views of input with varying durations or resolutions for efficient processing. The Space–Time transformer (ST Transformer) is employed for extracting visual features, while the SpectFormer is utilized for extracting the audio features. To interact and synchronize features from various modalities, a novel Cross-Modality Synchronization (CMS) module is proposed to ensure the coherent representation for strong detection of anomalies. To enhance the overall robustness of the proposed M 2 VAD M2VAD , the anomaly detection module with optimization techniques is proposed to generate anomaly scores. Experimental studies conducted on benchmark datasets demonstrate the superior performance of the M 2 VAD M2VAD , underscoring its resilience and generalization across various views and modalities. Code is available at https://github.com/Shalmiyapaulraj78/Multiview-Multimodality-VAD.git .},
  archive      = {J_ICV},
  author       = {Shalmiya Paulraj and Subramaniyaswamy Vairavasundaram},
  doi          = {10.1016/j.imavis.2024.105139},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105139},
  shortjournal = {Image Vis. Comput.},
  title        = {M2VAD: Multiview multimodality transformer-based weakly supervised video anomaly detection},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EPSViTs: A hybrid architecture for image classification
based on parameter-shared multi-head self-attention. <em>ICV</em>,
<em>149</em>, 105130. (<a
href="https://doi.org/10.1016/j.imavis.2024.105130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, they still suffer from weak local feature extraction, easy loss of channel interaction information in one-dimensional multi-head self-attention modeling, and large number of parameters. This paper proposes a lightweight image classification hybrid architecture named EPSViTs (Efficient Parameter Shared Transformer, EPSViTs). Firstly, a new local feature extraction module is designed to effectively enhance the expression of local features. Secondly, using the parameter sharing approach , a lightweight multi-head self-attention module based on information interaction is designed, which can globally model the image from both spatial and channel dimensions, and mine the potential correlation of the image in space and channel. Extensive experiments are conducted on three public datasets, a subset of ImageNet, Cifar100 and APTOS2019, a private dataset Mushroom66, and the results show that the hybrid architecture EPSViTs proposed in this paper based on parameter sharing for multi-head self-attentive image classification has obvious advantages, especially on a subset of ImageNet to reach 89.18%, which is a 3.8% improvement compared to Edgevits_xxs, verifying the effectiveness of the model.},
  archive      = {J_ICV},
  author       = {Huixian Liao and Xiaosen Li and Xiao Qin and Wenji Wang and Guodui He and Haojie Huang and Xu Guo and Xin Chun and Jinyong Zhang and Yunqin Fu and Zhengyou Qin},
  doi          = {10.1016/j.imavis.2024.105130},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105130},
  shortjournal = {Image Vis. Comput.},
  title        = {EPSViTs: A hybrid architecture for image classification based on parameter-shared multi-head self-attention},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Superpixel conditional generation adversarial network for
CMR artifact correction. <em>ICV</em>, <em>149</em>, 105112. (<a
href="https://doi.org/10.1016/j.imavis.2024.105112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac Magnetic Resonance (CMR) is widely used in diagnosing cardiac diseases for its excellent contrast of cardiovascular features. However, due to the long imaging time of CMR scanning, the patient&#39;s respiration, limb shaking, and heart beating will lead to a certain degree of motion artifacts in the image, seriously degrade the image quality and affect the doctor&#39;s clinical judgment. This paper proposes a superpixel conditional Generative Adversarial Network (spcGAN) based on a conditional Generative Adversarial Network (cGAN) by applying superpixel to both generator and discriminator parts. In the generator section, a generator network based on superpixel segmentation and pooling is proposed for feature extraction at the superpixel level to enhance the reconstruction of image edge texture and structural details. In the discriminator part, superpixel pooling is used to construct a superpixel discriminator. It is fused with the traditional convolutional discriminator to produce a superpixel-based dual discriminator, which makes the discriminator consider the image&#39;s local structure and details. Based on the generator and discriminator structure proposed in this paper, superpixel pooling and edge texturing loss functions are designed for optimization. Adequate ablation experiments and comparison experiments are conducted in terms of experimental results. Three types of objective metrics, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Focus Measurement (Tenengrad), were selected as references. The experimental results show that the effect of removing motion artifacts from authentic CMR images on the three datasets is most significant in the dataset produced in this paper. The results obtained from the fusion between the designed generator, discriminator, and loss function are the most obvious. Compared with the existing methods, the spcGAN proposed in this paper performs better.},
  archive      = {J_ICV},
  author       = {Yueming Zhu and Wei Zheng and Zepeng Ma},
  doi          = {10.1016/j.imavis.2024.105112},
  journal      = {Image and Vision Computing},
  month        = {9},
  pages        = {105112},
  shortjournal = {Image Vis. Comput.},
  title        = {Superpixel conditional generation adversarial network for CMR artifact correction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DASOD: Detail-aware salient object detection. <em>ICV</em>,
<em>148</em>, 105154. (<a
href="https://doi.org/10.1016/j.imavis.2024.105154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) is a challenging task in computer vision . Current SOD approaches have made significant progress, but they fail in challenging scenarios. This paper categorizes the existing challenges in SOD into four groups: images with complex backgrounds, low contrast, transparent objects, and occluded objects. Then, the Detail-Aware Salient Object Detection (DASOD) method is proposed to address these challenging scenarios. To the best of our knowledge, DASOD is the first method that considers mentioned challenging situations together and detects salient objects in images through camouflaged object detection (COD). DASOD has two main stages: 1) pseudo-mask generation and 2) refinement. It first generates a pseudo-mask using the body label and super-resolution technique, then refines the pseudo-mask with the detail map produced by the pseudo-edge generator to detect salient objects with clear boundaries in the pseudo-mask refinement module. This module quantifies uncertainty using the conditional normalizing flows (cFlow) based conditional variational auto-encoder (cVAE) to generate reliable results. Extensive experiments are conducted on six datasets, and the performance of DASOD is compared with 18 state-of-the-art methods. The results demonstrate that DASOD outperforms its competitors and can accurately detect the salient objects when the image background is cluttered and the contrast between foreground and background is low. Also, it effectively detects the transparent and occluded objects in images. It achieves MAE rates of 0.052, 0.033, 0.027, 0.024, 0.059, and 0.088 on DUT-OMRON, DUTS-TE, ECSSD, HKU-IS, PASCAL-S, and SCAS datasets, respectively. All the implementation source codes and results are available at: https://github.com/BaharehAsheghi/DASOD .},
  archive      = {J_ICV},
  author       = {Bahareh Asheghi and Pedram Salehpour and Abdolhamid Moallemi Khiavi and Mahdi Hashemzadeh and Amirhassan Monajemi},
  doi          = {10.1016/j.imavis.2024.105154},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105154},
  shortjournal = {Image Vis. Comput.},
  title        = {DASOD: Detail-aware salient object detection},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating human fall injuries: A novel system utilizing 3D
4-stream convolutional neural networks and image fusion. <em>ICV</em>,
<em>148</em>, 105153. (<a
href="https://doi.org/10.1016/j.imavis.2024.105153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unintentional human falls, especially in seniors, lead to serious injuries, fatalities, and reduced standard of life. Vision-based fall detection methods have demonstrated their usefulness in timely fall response, helping to lessen such injuries. This paper presents an automated vision-based fall detection system that triggers immediate fall reporting. By incorporating human segmentation and image fusion in the pre-processing stage, the system enhances the accuracy of human action classification , thereby ensuring precise fall alerts. It further employs the innovative 4-stream 3D convolutional neural network (4S-3DCNN) model to learn different but consecutive spatial and temporal features. The system processes video input or live surveillance, segmenting human presence every 32 frames using a fine-tuned deep-learning model and applying a three-level image fusion to accentuate movement differences. This technique produces four pre-processed images, input to the 4S-3DCNN model for classification. Consecutive detection of “Falling” and “Fallen” actions triggers an alert for immediate intervention. The original 4S-3DCNN model is an end-to-end trained deep learning model with a fully connected layer serving as a classifier. The research also evaluates the performance of combining the 4S-3DCNN model with Autoencoders and Support Vector Machines (SVM) networks as classifiers. The SVM classifier demonstrated ideal fall detection performance with 100% accuracy using the MCFD, URFD, and Le2i FDD datasets. The proposed system is vital for detecting and preventing falls and reducing healthcare expenses and productivity losses.},
  archive      = {J_ICV},
  author       = {Thamer Alanazi and Khalid Babutain and Ghulam Muhammad},
  doi          = {10.1016/j.imavis.2024.105153},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105153},
  shortjournal = {Image Vis. Comput.},
  title        = {Mitigating human fall injuries: A novel system utilizing 3D 4-stream convolutional neural networks and image fusion},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CrowdAlign: Shared-weight dual-level alignment fusion for
RGB-t crowd counting. <em>ICV</em>, <em>148</em>, 105152. (<a
href="https://doi.org/10.1016/j.imavis.2024.105152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of visible and thermal images has been proven to be effective in improving accuracy for crowd counting in illumination-unconstrained scenes. However, the challenging problem of misalignment in RGB-T image pairs has not been extensively explored in this context. This study aims to address the issue of misalignment between RGB and thermal image pairs to enhance the counting accuracy of cross-modal models. Specifically, we propose CrowdAlign, a cross-modal feature alignment fusion network that utilizes a shared-weight strategy for efficient feature extraction. Additionally, CrowdAlign addresses alignment adjustments through two stages: pre-fusion and post-fusion alignment. For pre-fusion feature extraction, we design a dual-level spatial-semantic parallel alignment module, while for post-fusion feature extraction, a low-frequency feature attention filtering module is developed. This two-stage alignment approach enables cross-modal feature alignment without requiring additional supervision. Experiments on the public benchmarks demonstrate that our method is effective under RGB-T misalignment or dark conditions. We hope CrowdAlign will inspire researchers to focus on and explore the issue of misalignment between RGB image and thermal image for cross-modal crowd counting.},
  archive      = {J_ICV},
  author       = {Weihang Kong and Zepeng Yu and He Li and Liangang Tong and Fengda Zhao and Yang Li},
  doi          = {10.1016/j.imavis.2024.105152},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105152},
  shortjournal = {Image Vis. Comput.},
  title        = {CrowdAlign: Shared-weight dual-level alignment fusion for RGB-T crowd counting},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Omnidirectional image quality assessment with local–global
vision transformers. <em>ICV</em>, <em>148</em>, 105151. (<a
href="https://doi.org/10.1016/j.imavis.2024.105151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rising popularity of omnidirectional images (ODIs) in virtual reality applications , the need for specialized image quality assessment (IQA) methods becomes increasingly critical. Traditional IQA approaches, designed for rectilinear images, often fail to evaluate ODIs accurately due to their 360-degree scene representation. Addressing this, we introduce the Local–Global Transformer for 360-degree Image Quality Assessment (LGT360IQ). This novel framework features dual branches tailored to mimic top-down and bottom-up visual attention mechanisms , adapted for the spherical characteristics of ODIs. The local branch processes tangent viewports from salient regions within the equirectangular projection image, extracting detailed features for granular quality assessment. In parallel, the global branch utilizes a task-dependent token sampling strategy for holistic image feature processing and quality score prediction. This integrated approach combines local and global information, offering an effective IQA method for ODIs. Our extensive evaluation across three benchmark ODI datasets, CVIQ, OIQA, and ODI, demonstrates LGT360IQ superior performance and establishes its role in advancing the field of IQA for omnidirectional images.},
  archive      = {J_ICV},
  author       = {Nafiseh Jabbari Tofighi and Mohamed Hedi Elfkir and Nevrez Imamoglu and Cagri Ozcinar and Aykut Erdem and Erkut Erdem},
  doi          = {10.1016/j.imavis.2024.105151},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105151},
  shortjournal = {Image Vis. Comput.},
  title        = {Omnidirectional image quality assessment with local–global vision transformers},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OFACD: An end-to-end change detection network for small UAVs
remote sensing with viewpoint differences. <em>ICV</em>, <em>148</em>,
105150. (<a href="https://doi.org/10.1016/j.imavis.2024.105150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection using remote sensing images captured by small unmanned aerial vehicles (small UAVs) finds wide applications across various fields. However, there is a challenge when dealing with images captured at the same location by small UAVs at different times, leading to differences in viewpoint. These viewpoint differences present a significant challenge for most change detection methods . In this paper, we propose an end-to-end network, OFACD, designed to simultaneously address the issues of image alignment and change detection. Our network aligns feature maps using estimated optical flow and performs change detection concurrently. This approach enables the network to directly process images with viewpoint differences, effectively improving performance in scenarios with accumulated errors or large viewpoint variations, as well as enhancing throughput by eliminating repetitive feature extraction. Additionally, to fill the gap of the absence of change detection datasets with viewpoint differences and to evaluate our model, we created two change detection datasets with viewpoint differences. Extensive experimental results demonstrate that our method outperforms several state-of-the-art change detection methods in datasets involving viewpoint differences, exhibiting superior throughput and performance.},
  archive      = {J_ICV},
  author       = {Yaxin Dong and Fei Li and Kai Yan and Shen Deng and Tao Wen and Yang Yang},
  doi          = {10.1016/j.imavis.2024.105150},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105150},
  shortjournal = {Image Vis. Comput.},
  title        = {OFACD: An end-to-end change detection network for small UAVs remote sensing with viewpoint differences},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nighttime image semantic segmentation with retinex theory.
<em>ICV</em>, <em>148</em>, 105149. (<a
href="https://doi.org/10.1016/j.imavis.2024.105149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nighttime image semantic segmentation is challenging due to low-light and diverse lighting conditions . A straightforward solution is to first enhance nighttime scene images to resemble daytime scene before performing segmentation. This kind of methods heavily rely on the enhancement quality. Inspired by the Retinex theory for low-light image enhancement, which decomposes an image into reflectance and illumination components, we propose a novel nighttime image segmentation method with Retinex theory (RNightSeg). Our core insight is to obtain high-quality illumination-independent reflectance component to enhance segmentation. Specifically, we apply a decomposition decoder to the backbone network for generating the reflectance component. In addition to the fidelity loss and Total Variation loss for the reflectance component regression, we model the brightening illumination component to enhance the nighttime image and apply the color constancy loss on the enhanced image. This helps to cope with the issue of low-light and diverse lighting in the nighttime scene. Finally, we fuse the reflectance decoder feature with the backbone feature and feed the fused feature to the segmentation decoder. Extensive experimental results on two widely used datasets demonstrate that the proposed RNightSeg achieves superior performance over some state-of-the-art segmentation methods. The code of our implementation is available at https://github.com/sunzc-sunny/RNightSeg .},
  archive      = {J_ICV},
  author       = {Zhichao Sun and Huachao Zhu and Xin Xiao and Yuliang Gu and Yongchao Xu},
  doi          = {10.1016/j.imavis.2024.105149},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105149},
  shortjournal = {Image Vis. Comput.},
  title        = {Nighttime image semantic segmentation with retinex theory},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Erratum to “two-dimensional hybrid incremental learning
(2DHIL) framework for semantic segmentation of skin tissues” [image and
vision computing. vol148 (2024) 105098]. <em>ICV</em>, <em>148</em>,
105148. (<a href="https://doi.org/10.1016/j.imavis.2024.105148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ICV},
  author       = {Muhammad Imran and Muhammad Usman Akram and Mohsin Islam Tiwana and Anum Abdul Salam and Danilo Greco},
  doi          = {10.1016/j.imavis.2024.105148},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105148},
  shortjournal = {Image Vis. Comput.},
  title        = {Erratum to “Two-dimensional hybrid incremental learning (2DHIL) framework for semantic segmentation of skin tissues” [Image and vision computing. vol148 (2024) 105098]},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-dimensional hybrid incremental learning (2DHIL)
framework for semantic segmentation of skin tissues. <em>ICV</em>,
<em>148</em>, 105147. (<a
href="https://doi.org/10.1016/j.imavis.2024.105147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to enhance the robustness and generalization capability of a deep learning transformer model used for segmenting skin carcinomas and tissues through the introduction of incremental learning. Deep learning AI models demonstrate their claimed performance only for tasks and data types for which they are specifically trained. Their performance is severely challenged for the test cases which are not similar to training data thus questioning their robustness and ability to generalize. Moreover, these models require an enormous amount of annotated data for training to achieve desired performance. The availability of large annotated data, particularly for medical applications, is itself a challenge. Despite efforts to alleviate this limitation through techniques like data augmentation, transfer learning, and few-shot training, the challenge persists. To address this, we propose refining the models incrementally as new classes are discovered and more data becomes available, emulating the human learning process. However, deep learning models face the challenge of catastrophic forgetting during incremental training. Therefore, we introduce a two-dimensional hybrid incremental learning framework for segmenting non-melanoma skin cancers and tissues from histopathology images. Our approach involves progressively adding new classes and introducing data of varying specifications to introduce adaptability in the models. We also employ a combination of loss functions to facilitate new learning and mitigate catastrophic forgetting. Our extended experiments demonstrate significant improvements, with an F1 score reaching 91.78, mIoU of 93.00, and an average accuracy of 95%. These findings highlight the effectiveness of our incremental learning strategy in enhancing the robustness and generalization of deep learning segmentation models while mitigating catastrophic forgetting.},
  archive      = {J_ICV},
  author       = {Muhammad Imran and Muhammad Usman Akram and Mohsin Islam Tiwana and Anum Abdul Salam and Taimur Hassan and Danilo Greco},
  doi          = {10.1016/j.imavis.2024.105147},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105147},
  shortjournal = {Image Vis. Comput.},
  title        = {Two-dimensional hybrid incremental learning (2DHIL) framework for semantic segmentation of skin tissues},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive breast cancer diagnosis using ensemble fuzzy
model. <em>ICV</em>, <em>148</em>, 105146. (<a
href="https://doi.org/10.1016/j.imavis.2024.105146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer continues to be a major global health challenge, necessitating reliable diagnostic methods for early detection and improved patient outcomes. This study introduces a novel ensemble fuzzy model for predictive breast cancer diagnosis, integrating multiple deep-learning classifiers with fuzzy logic to enhance decision-making. Traditional diagnostic approaches often struggle with the complexity and heterogeneity of breast cancer data, which this new model addresses through an innovative ensemble technique. The ensemble model combines the strengths of Inception-V4, Inception-ResNet, and Inception V3/V4 + BN , with fuzzy logic for adaptive priority assignment based on confidence scores. The method employs a re-parameterized Gompertz function to assign fuzzy ranks to constituent classifiers, allowing flexible fusion strategies. The proposed model is evaluated on two benchmark breast cancer datasets: the Digital Database for Screening Mammography (DDSM) and the Breast Cancer Histopathological Image Classification (BACH) dataset. It achieves high performance across key metrics, including accuracy, precision, recall, and F1 score, consistently outperforming individual classifiers . On the DDSM dataset, the ensemble fuzzy model attains an accuracy of 0.97, a recall of 0.93, a precision of 0.95, and an F1 score of 0.96. Similarly, on the BACH dataset, the proposed method records an accuracy of 97.05%, a recall of 99.31%, a precision of 95.44%, and an F1 score of 97.37%, demonstrating its robust capability to identify positive instances and maintain a balanced performance. These results highlight the potential of the ensemble fuzzy model to improve breast cancer diagnosis, offering a reliable solution to the inherent challenges in this field.},
  archive      = {J_ICV},
  author       = {Xiaohui Yu and Jingjun Tian and Zhipeng Chen and Yizhen Meng and Jun Zhang},
  doi          = {10.1016/j.imavis.2024.105146},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105146},
  shortjournal = {Image Vis. Comput.},
  title        = {Predictive breast cancer diagnosis using ensemble fuzzy model},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). SMTCNN - a global spatio-temporal texture convolutional
neural network for 3D dynamic texture recognition. <em>ICV</em>,
<em>148</em>, 105145. (<a
href="https://doi.org/10.1016/j.imavis.2024.105145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic textures (DT) are typically 3D videos of physical processes showing statistical regularity but have indeterminate spatial and temporal extent. Existing DT recognition methods usually neglect the global spatio-temporal relationships of DT which reflect the statistical regularities. In this paper, a spatio-temporal texture convolutional neural network (SMTCNN) is proposed for global semantic DT representation. Specifically, SMTCNN describes DT features by learning DT’ temporal motion as well as the sources of the motions and the scenarios where the motion is happening, and accordingly, a motion net and a source net are formulated. In particular, a novel module consisting of expansion and concatenation implementations on deep features is presented, with an arbitrary 2D backbone as input, followed by a new 1D CNN including 4 convolutional, 2 pooling and 2 fully-connected layers to represent the 2D tensors in space–time, by transforming DT descriptors from discrete “words” to global “textures”. A number of comparative experiments on three DT dataset - UCLA, DynTex and DynTex ++ are conducted to demonstrate our approach.},
  archive      = {J_ICV},
  author       = {Liangliang Wang and Lei Zhou and Peidong Liang and Ke Wang and Lianzheng Ge},
  doi          = {10.1016/j.imavis.2024.105145},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105145},
  shortjournal = {Image Vis. Comput.},
  title        = {SMTCNN - a global spatio-temporal texture convolutional neural network for 3D dynamic texture recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indirect deformable image registration using synthetic image
generated by unsupervised deep learning. <em>ICV</em>, <em>148</em>,
105143. (<a href="https://doi.org/10.1016/j.imavis.2024.105143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D image registration is now common in many medical domains. Multimodal registration implies the use of different imaging modalities , which results in lower accuracy compared to monomodal registration. The aim of this study was to propose a novel approach for deformable image registration (DIR) that incorporates an unsupervised deep learning (DL)-based generation step. The objective was to reduce the challenge of multimodal registration to monomodal registration. Two datasets from prostate radiotherapy patients were used to evaluate the proposed method. The first dataset consisted of Computed Tomography (CT)/ Cone Beam Computed Tomography (CBCT) pairs from 23 patients using different CBCT devices. The second dataset included Magnetic Resonance Imaging (MRI)/CT pairs from two different care centers, utilizing different MRI devices (0.35 T MRIdian MR-Linac, 1.5 T GE lightspeed MRI). Following a preprocessing step essential for ensuring DL synthesis accuracy and standardizing the database, synthetic CTs ( sCT reg sCTreg ) were generated using an unsupervised conditional Generative Adversarial Network (cGAN). The generated sCTs from CBCT or MRI were then utilized for deformable registration with CT scans. This registration method was compared to three standard methods: rigid registration, Elastix registration based on BSplines, and VoxelMorph-based registration (applied exclusively to CBCT/CT). The endpoints of comparison were the dice coefficients calculated between delineated structures for both datasets. For both datasets, intermediary sCT generation provided the highest dice coefficients. Dices reached 0.85, 0.85 and 0.75 for the prostate, bladder and rectum for the dataset 1 and 0.90, 0.95 and 0.87 respectively for the dataset 2. When the sCT were not used, dices reached 0.66, 0.78, 0.66 for the dataset 1 and 0.93, 0.87 and 0.84 for the dataset 2. Furthermore, the evaluation of the impact of registration on sCT generation showed that lower Mean Absolute Errors were obtained when the registration was conducted with a sCT. Using unsupervised deep learning to synthesize intermediate sCT has led to improved registration accuracy in radiotherapy applications employing two distinct imaging modalities.},
  archive      = {J_ICV},
  author       = {Cédric Hémon and Blanche Texier and Hilda Chourak and Antoine Simon and Igor Bessières and Renaud de Crevoisier and Joël Castelli and Caroline Lafond and Anaïs Barateau and Jean-Claude Nunes},
  doi          = {10.1016/j.imavis.2024.105143},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105143},
  shortjournal = {Image Vis. Comput.},
  title        = {Indirect deformable image registration using synthetic image generated by unsupervised deep learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-consistency constraints network for noisy facial
expression recognition. <em>ICV</em>, <em>148</em>, 105141. (<a
href="https://doi.org/10.1016/j.imavis.2024.105141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although existing facial expression recognition (FER) methods have achieved great success, their performance degrades significantly under noisy labels caused by low-quality images, ambiguous expressions, and subjective and incorrect labeling. Recent studies have shown that deep neural networks (DNNs) can easily overfit noisy labels, which poses a great challenge to FER task in real-world scenarios. To address this issue, we propose a novel Dual-consistency Constraints Network (DC-Net) to automatically suppress noisy samples during training. Specifically, we first propose a Class Activation Mapping (CAM) Attention Consistency (CAC), which makes the model focus on partially important feature information. As a result, we obtain more robust local feature representations and reduce excessive attention to noisy labels. Then, a Class Feature Consistency (CFC) is designed to encourage the model to focus on the global semantic information of the image. Finally, with the collaboration of the CAC and the CFC, DC-Net can learn robust local and global feature information to prevent the model from learning biased information with noisy labels. We conducted extensive experiments on three field datasets, including RAF-DB, AffectNet, and FERPlus2013. Experimental results show that DC-Net significantly outperforms state-of-the-art noisy labeling methods at different noise rates and generalizes well to other tasks with a large number of classes, such as CIFAR100 and Tiny- ImageNet.},
  archive      = {J_ICV},
  author       = {Haiying Xia and Chunhai Su and Shuxiang Song and Yumei Tan},
  doi          = {10.1016/j.imavis.2024.105141},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105141},
  shortjournal = {Image Vis. Comput.},
  title        = {Dual-consistency constraints network for noisy facial expression recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-branch progressive embedding network for crowd
counting. <em>ICV</em>, <em>148</em>, 105140. (<a
href="https://doi.org/10.1016/j.imavis.2024.105140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is essential for video surveillance and public safety. The performance of counting models has been greatly improved with the rapid development of Convolution neural networks (CNN), while it still suffers interference from complex background and large-scale variation. To relieve the above challenges, this paper proposes a novel Multi-branch Progressive Embedding Network (MPENet) for crowd counting. Specifically, the proposed network mainly consists of two modules named Background Area Filter (BAF) and Sequential Multi-scale Modules (SMM), which are embedded with each other to generate higher-quality density maps. Firstly, the BAF model base on attention mechanism is proposed to distinguish crowd from background, which effectively avoids the model outputting positive predictions in the background region . Meanwhile, a multi-level supervision mechanism is proposed to generate more accurate attention maps. Besides, the SMM module is designed to be progressively embedded with scale context information so that the scale feature will be smooth and continuous. Finally, a novel multi-scale consistency structural loss is proposed to avoid pixel-level isolation due to Euclidean loss. The proposed method significantly improves counting accuracy, achieving Mean Absolute Error (MAE) of 57.6 and 6.9 on ShanghaiTechA and ShanghaiTechB respectively.},
  archive      = {J_ICV},
  author       = {Lifang Zhou and Songlin Rao and Weisheng Li and Bo Hu and Bo Sun},
  doi          = {10.1016/j.imavis.2024.105140},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105140},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-branch progressive embedding network for crowd counting},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Localization of diffusion model-based inpainting through the
inter-intra similarity of frequency features. <em>ICV</em>,
<em>148</em>, 105138. (<a
href="https://doi.org/10.1016/j.imavis.2024.105138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the enhanced abilities of diffusion models have led to more realistic inpainting results, which raises the potential for criminal activity through image forgery. In this study, we explore the detection of inpainted images generated by a diffusion model. We propose a method for inpainting localization using an inter-intra similarity (IIS) module based on image frequency features. The proposed IIS module learns the inter-patch relationship through the learnable frequency filter and subsequently covers the intra-patch relationship through the self-similarity operation. We provide the Diffusion Model Inpainting Dataset (DMID), a benchmark dataset comprising inpainted images using four different diffusion models and three types of masks. Additionally, a test dataset that includes three sampling steps is provided. We validated the effectiveness of our proposed approach by conducting comparative tests with existing forgery detectors using our dataset and testing the robustness of JPEG compression. Additionally, we tested our proposed method on datasets with different sampling step sizes. Our work provides a starting point for research on the detection of inpainting-based forgery using diffusion models. Additionally, by openly releasing the dataset, we offer an opportunity to advance future in-depth research related to forensics.},
  archive      = {J_ICV},
  author       = {Seung-Lee Lee and Minjae Kang and Jong-Uk Hou},
  doi          = {10.1016/j.imavis.2024.105138},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105138},
  shortjournal = {Image Vis. Comput.},
  title        = {Localization of diffusion model-based inpainting through the inter-intra similarity of frequency features},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A puzzle questions form training for self-supervised
skeleton-based action recognition. <em>ICV</em>, <em>148</em>, 105137.
(<a href="https://doi.org/10.1016/j.imavis.2024.105137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposed a novel pretext task to address the skeleton-based video representation for self-supervised action recognition tasks. Instead of exploiting only the whole body, various levels of the skeleton structure (e.g., upper body, lower body, left arm, left leg, right arm, right leg, and torso) are employed to extract essential coarser-grained characteristics. This involves computing statistical representations like motion, orientation, trajectory, and magnitude shift from unlabeled skeleton configurations. Then a learning model is built and trained to yield these statistical representations given the sequence configuration as the input. Our approach is question-driven, where each question acts as a puzzle piece contributing to a deeper understanding of the skeleton joint configuration. It&#39;s inspired by the ability of the cognitive system observed in individuals to hypothesize unseen actions. This is accomplished by posing pertinent questions and envisioning plausible scenarios to recognize the actions taking place. The answers to these devised questions are derived from the statistical representation of skeleton configurations. To this end, we made 44 questions designed to encompass the broadest overview to the finest detail. Our experiments on the NTU RGB-D, NW-UCLA, and PKU-MMD datasets demonstrate outstanding results in action recognition, proving the superiority of our approach in learning discriminative characteristics.},
  archive      = {J_ICV},
  author       = {Oumaima Moutik and Hiba Sekkat and Taha Ait Tchakoucht and Badr El Kari and Ahmed El Hilali Alaoui},
  doi          = {10.1016/j.imavis.2024.105137},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105137},
  shortjournal = {Image Vis. Comput.},
  title        = {A puzzle questions form training for self-supervised skeleton-based action recognition},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video object segmentation by multi-scale attention using
bidirectional strategy. <em>ICV</em>, <em>148</em>, 105136. (<a
href="https://doi.org/10.1016/j.imavis.2024.105136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on semi-supervised video object segmentation (VOS). Recently, several Space‐Time Memory based networks have effectively improved the performance of VOS. However, most methods predict the target object mask forwardly, which causes error propagation to mislead the future frame segmentation. Moreover, the rich multi-scale information of objects needs to be effectively exploited in videos to extract fine-grained multi-scale spatial information. To address these limitations, we present a network with a multi-scale attention module for semi-supervised VOS, which combines a new bidirectional strategy during training. Firstly, we propose the bidirectional strategy in which a backward flow combines the existing standard forward flow. With the strategy, we can rely on the first frame&#39;s ground-truth mask to mitigate the problem of error propagation. Secondly, a multi-scale attention module is designed to extracts multi-scale features by different weights and interacts with information between multi-scale channel attention. Especially the multi-scale attention module can effectively extract the fine-grained mask by the network during the bidirectional training. Experimental results show that our network achieves significant segmentation performance compared to state-of-the-art approaches on the YouTube-VOS and DAVIS datasets.},
  archive      = {J_ICV},
  author       = {Jingxin Wang and Yunfeng Zhang and Fangxun Bao and Yuetong Liu and Qiuyue Zhang and Caiming Zhang},
  doi          = {10.1016/j.imavis.2024.105136},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105136},
  shortjournal = {Image Vis. Comput.},
  title        = {Video object segmentation by multi-scale attention using bidirectional strategy},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active domain adaptation for semantic segmentation via
dynamically balancing domainness and uncertainty. <em>ICV</em>,
<em>148</em>, 105132. (<a
href="https://doi.org/10.1016/j.imavis.2024.105132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active domain adaptation aims to enhance model adaptation performance by annotating a limited number of informative unlabeled target data. Traditional active learning strategies for semantic segmentation often neglect the presence of domain shifts, resulting in suboptimal results in domain adaptation scenarios. In this paper, we present a novel active domain adaptation approach for semantic segmentation that maximizes segmentation performance under domain shifts with a limited number of queried target labels. To recognize the most valuable samples for labeling, we introduce a new acquisition strategy. This strategy leverages a target domainness map to identify the most informative samples for reducing the domain gap and employs region-aware prediction uncertainty to explore ambiguous samples. Meanwhile, to optimize the efficiency of the acquisition strategy, we dynamically adjust the balance between prediction uncertainty and target domainness over the selection rounds. To further bolster adaptation performance, a smooth loss function is employed for the target data, which promotes consistency in local predictions. Extensive experiments on two benchmarks, GTAV → → Cityscapes and SYNTHIA → → Cityscapes, demonstrate that our method surpasses existing active domain adaptation methods for semantic segmentation. Moreover, it achieves comparable results to supervised performance with only 5% annotations in the target domain, validating the effectiveness of our method.},
  archive      = {J_ICV},
  author       = {Siqi Zhang and Lu Zhang and Zhiyong Liu},
  doi          = {10.1016/j.imavis.2024.105132},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105132},
  shortjournal = {Image Vis. Comput.},
  title        = {Active domain adaptation for semantic segmentation via dynamically balancing domainness and uncertainty},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing small object tracking with reversible rescaling
networks. <em>ICV</em>, <em>148</em>, 105131. (<a
href="https://doi.org/10.1016/j.imavis.2024.105131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving domain of visual tracking , the accurate identification and tracking of small objects pose significant challenges due to their minimal pixel presence and detail deficiency. In the field of small object tracking, utilizing deep features derived from models trained on conventionally sized objects frequently leads to a substantial reduction in the fidelity of the representation. This discrepancy often undermines the effectiveness of tracking small objects. To address this challenge, we introduce the Reversible Small Object Tracker (RSTrack). RSTrack innovatively integrates two reversible neural networks into the tracking architecture, specifically tailored for the nuanced representation of small objects. Drawing upon the reversible networks&#39; unique attribute of lossless information propagation, the reversible extraction network in RSTrack is adept at retaining a more granular level of detail pertaining to small objects. In addition, the reversible scaling network can suppress the background and highlight the small objects by enhancing the details of the image, effectively mitigating the issue of diminished object-background discriminability that arises from previously inadequate representational methods. Extensive experiments on four popular small object tracking benchmarks demonstrate our method achieves new state-of-the-art performances.},
  archive      = {J_ICV},
  author       = {Yufei Zha and Xiao Guo and Fan Li and Hangfei Li},
  doi          = {10.1016/j.imavis.2024.105131},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105131},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing small object tracking with reversible rescaling networks},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical slice interaction and multi-layer cooperative
decoding networks for remote sensing image dehazing. <em>ICV</em>,
<em>148</em>, 105129. (<a
href="https://doi.org/10.1016/j.imavis.2024.105129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, U-shaped neural networks have gained widespread application in remote sensing image dehazing and achieved promising performance. However, most of the existing U-shaped dehazing networks neglect the global and local information interaction across layers during the encoding phase, which leads to incomplete utilization of the extracted features for image restoration. Moreover, in the process of image reconstruction, utilizing only the information from the terminal layers of the decoding phase for haze-free image restoration leads to a dilution of semantic information, resulting in color and texture deviations in the dehazed image. To address these issues, We propose a Hierarchical Slice Interaction and Multi-layer Cooperative Decoding Networks for Remote Sensing Image dehazing (HSMD-Net). Specifically, a hierarchical slice information interaction module (HSIIM) is proposed to introduce Intra-layer feature autocorrelation and Inter-layer feature cross-correlation to facilitate global and local information interaction across layers, thereby enhancing the encoding features representation capability and improving the network dehazing performance. Furthermore, a multi-layer cooperative decoding reconstruction module (MCDRM) is proposed to fully utilize feature information in each decoding layer, mitigate semantic information dilution, and improve the network capability to restore image colors and textures. Experimental results demonstrate that our HSMD-Net outperforms several state-of-the-art methods in dehazing on two publicly available datasets. The source code is available at https://github.com/xushouyi1/HSMD-Net .},
  archive      = {J_ICV},
  author       = {Mei Yu and ShouYi Xu and Hang Sun and YueLin Zheng and Wen Yang},
  doi          = {10.1016/j.imavis.2024.105129},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105129},
  shortjournal = {Image Vis. Comput.},
  title        = {Hierarchical slice interaction and multi-layer cooperative decoding networks for remote sensing image dehazing},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modality interactive attention for cross-modality person
re-identification. <em>ICV</em>, <em>148</em>, 105128. (<a
href="https://doi.org/10.1016/j.imavis.2024.105128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The visible-infrared person re-identification (VI-ReID) task is challenging in image retrievals because of the modality gaps between visible and infrared images. Different from the most existing methods which either strive to capture modality invariant features or bridge the two modalities via modal data compensation, the proposed network introduces a modality interactive attention(MIA) module, which aims to establish an interactive relation between modality-shared (MSH) features and modality-specific (MSP) features to narrow down the gap. The attention-driven module explores the relevance score between MSH and MSP features, and takes the score as the modality bridge to fuse the two features, thus introducing the specific feature into the shared one. The subnetworks for extracting the MSP and MSH features are also introduced. Extensive experiments on benchmark datasets show that the proposed VI-ReID method outperforms other state-of-the-art methods. On the large-scale SYSU-MM01 dataset, the proposed method can achieve 83.56% and 85.67% in Rank-1 accuracy and mAP , which is 3.26% and 2.37% higher than the baseline.},
  archive      = {J_ICV},
  author       = {Zilin Zou and Ying Chen},
  doi          = {10.1016/j.imavis.2024.105128},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105128},
  shortjournal = {Image Vis. Comput.},
  title        = {Modality interactive attention for cross-modality person re-identification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occlusion-aware deep convolutional neural network via
homogeneous tanh-transforms for face parsing. <em>ICV</em>,
<em>148</em>, 105120. (<a
href="https://doi.org/10.1016/j.imavis.2024.105120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however, they overlook facial occlusion and ignore some contextual areas outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the lighting phenomena in everyday life, where illumination from four distinct lamps provides a more uniform distribution than a single central light source , we propose a novel homogeneous tanh-transform for image preprocessing, which is made up of four tanh-transforms. These transforms fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information from the surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines information in both Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occluded regions. The network is simple, flexible, and can be trained end-to-end. To facilitate future research of occluded face parsing, we also contribute a new cleaned face parsing dataset. This dataset is manually purified from several academic or industrial datasets, including CelebAMask-HQ, Short-video Face Parsing, and the Helen dataset, and will be made public. Experiments demonstrate that our method surpasses state-of-the-art methods in face parsing under occlusion.},
  archive      = {J_ICV},
  author       = {Jianhua Qiu and Weihua Liu and Chaochao Lin and Jiaojiao Li and Haoping Yu and Said Boumaraf},
  doi          = {10.1016/j.imavis.2024.105120},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105120},
  shortjournal = {Image Vis. Comput.},
  title        = {Occlusion-aware deep convolutional neural network via homogeneous tanh-transforms for face parsing},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video captioning based on dual learning via multiple
reconstruction blocks. <em>ICV</em>, <em>148</em>, 105119. (<a
href="https://doi.org/10.1016/j.imavis.2024.105119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of video captioning, a conventional dual learning scheme involves two tasks: a primal task, which translates frame features into natural language captions, and a dual task, which reconstructs frame features from the generated captions. The dual task serves as a regularization mechanism for the primal task, providing feedback that helps to improve the accuracy of the generated captions. In prior research, it has been demonstrated that the inclusion of dual learning regularization into the architecture of a video captioning model can substantially improve performance. However, it remains an open question whether the performance of such a model can be further enhanced through the incorporation of additional regularizers. In this study, we investigate the use of multiple blocks of primal and dual tasks as additional regularizers in the model. Our experiments on benchmark datasets show that the appropriate number of additional regularizers can further improve the quality of the video captioning model and achieve state-of-the-art results.},
  archive      = {J_ICV},
  author       = {Bahy Helmi Hartoyo Putra and Cheol Jeong},
  doi          = {10.1016/j.imavis.2024.105119},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105119},
  shortjournal = {Image Vis. Comput.},
  title        = {Video captioning based on dual learning via multiple reconstruction blocks},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive attribute distribution similarity for few-shot
learning. <em>ICV</em>, <em>148</em>, 105118. (<a
href="https://doi.org/10.1016/j.imavis.2024.105118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern deep learning has many drawbacks, including a heavy reliance on labeled data. One of the key strategies for solving this problem is few-shot learning (FSL). With just a few labeled samples, FSL seeks to identify previously unknown classes. The majority of works that have been published thus far focus on comparing the features of query samples and support classes, which do not fully utilize the training set&#39;s data and do not help sustain performance improvement . In our study, we compute a new attribute distribution similarity between support classes and a query sample of novel classes using attribute information on the training set. We suggest a fresh approach to three phases to accomplish our objective: 1) A attribute provider harnesses the visual features of the training set to construct attributes. 2) Choosing appropriate attributes for novel classes and enriching attributes to determine how similar novel classes and attributes are to one another. 3) To help with classification, attribute distribution similarity is computed for the first time by creating new correlations between the support classes and the query samples, which increases the accuracy of picture classification. Be aware that our solution won&#39;t make the initial network settings larger. Experiments on inductive FSL tasks demonstrate the usefulness and practicality of our strategy. Specifically, Our method has achieved the highest performance in the 5-way 1-shot task settings on the tiered-ImageNet and CUB 200–2011 datasets, with impressive results of 73.22 % 73.22% and 82.34 % 82.34% respectively.},
  archive      = {J_ICV},
  author       = {Anping Cai and Leiting Chen and Yongqi Chen and Ziyu He and Shuqing Tao and Chuan Zhou},
  doi          = {10.1016/j.imavis.2024.105118},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105118},
  shortjournal = {Image Vis. Comput.},
  title        = {Adaptive attribute distribution similarity for few-shot learning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFSTrack: Dual-stream fusion siamese network for human pose
tracking in videos. <em>ICV</em>, <em>148</em>, 105117. (<a
href="https://doi.org/10.1016/j.imavis.2024.105117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose tracking is a challenging task that involves estimating the human pose and tracking it across multiple frames in a video sequence. In recent years, deep learning-based methods have made significant progress in this field, achieving state-of-the-art performance. However, due to complex background and occlusion among people missed detection and incorrect association matching are still the challenging problems. To address these issues, we adopt a top-down framework to perform human pose tracking in the paper. We propose a human detection prediction recovery module (HDP module) to recover missed detection, and propose a dual-stream fusion Siamese network for human matching (DFSTrack). Specifically, we design a residual graph convolutional block (RGCN block) for spatial position encoding of human keypoints , and use spatial self-attention and temporal cross-attention to design a dual-stream spatial–temporal fusion transformer (DST Transformer). The graph convolutional block and transformer are cascaded to simultaneously obtain information on the spatial and temporal positions of human keypoints, allowing the Siamese network to solve the erroneous human matching. Experimental results on the PoseTrack17 dataset, PoseTrack18 dataset and PoseTrack21 dataset demonstrate that our proposed method outperforms state-of-the-art methods on human pose tracking tasks. Our code and pretrained models are available at https://github.com/yhtian2023/DFSTrack .},
  archive      = {J_ICV},
  author       = {Xiangyang Wang and Yuhui Tian and Fudi Geng and Rui Wang},
  doi          = {10.1016/j.imavis.2024.105117},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105117},
  shortjournal = {Image Vis. Comput.},
  title        = {DFSTrack: Dual-stream fusion siamese network for human pose tracking in videos},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional scale-aware upsampling network for
arbitrary-scale video super-resolution. <em>ICV</em>, <em>148</em>,
105116. (<a href="https://doi.org/10.1016/j.imavis.2024.105116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of video super-resolution (VSR) has significantly improved. However, the current methods only focus on a single scale factor, treating the VSR of different scale factors independently and disregarding video super-resolution of arbitrary-scale factors. To address this issue, we propose a model, the Bidirectional Scale-Aware Upsampling Network for Arbitrary-Scale Video Super-Resolution, which eliminates the need for multiple models for various scale factors. We design a Bidirectional Scale-Aware Upsampling module in the proposed model, consisting of a Bidirectional Scale-Aware Module (BSAM) and a Spatial Pyramid Upsampling section. The BSAM extracts feature for various scale factors and allows feature information of different scales to interact bidirectionally. Additionally, we propose a Spatial Pyramid Loss that optimizes the network based on upsampling and maps the results of different scales to a unified spatial set to find the arbitrary-scale factor&#39;s loss. Along with this, we introduce an Explicit Feature Pyramid module, which uses Spatial Pyramid Upsampling to learn arbitrary-scale factor details explicitly. Finally, we demonstrate the extensibility of the model through a VSR algorithm integration with the Bidirectional Scale-Aware Upsampling, ensuring high-resolution results of arbitrary-scale factors without affecting the performance. Our comprehensive experiments on public benchmarks show promising results for video super-resolution of arbitrary-scale factors.},
  archive      = {J_ICV},
  author       = {Laigan Luo and Benshun Yi and Zhongyuan Wang and Zheng He and Chao Zhu},
  doi          = {10.1016/j.imavis.2024.105116},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105116},
  shortjournal = {Image Vis. Comput.},
  title        = {Bidirectional scale-aware upsampling network for arbitrary-scale video super-resolution},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFG-HCEN: A distinctive-feature guided and hierarchical
channel enhanced network-based infrared and visible image fusion.
<em>ICV</em>, <em>148</em>, 105115. (<a
href="https://doi.org/10.1016/j.imavis.2024.105115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an unsupervised learning approach for the task of infrared and visible image fusion. This approach is called a distinctive-feature guided and hierarchical channel enhanced network-based infrared and visible image Fusion (DFG-HCEN). Instead of using complex fusion rules, DFG-HCEN uses multi-level fusion to achieve fusion results, effectively avoiding information loss during feature extraction. To improve the fusion effect, we designed a distinctive-feature guided module that strengthens the relationship between modules. Moreover, the proposed hierarchical channel enhanced and distinctive-Feature guided module aims to facilitate the fusion framework in efficiently integrating the multilevel complementary features of the source pictures. In addition, we incorporate a hybrid loss method for unsupervised training of the provided DFG-HCEN. The fidelity loss is used to constrain the pixel similarity between the fused result and source images. The application of luminance regularization loss has been shown to be an efficient method for addressing the problem of luminance degradation in fused images. We conducted extensive experiments, including visual examination and quantitative analysis , comparing DFG-HCEN with thirteen other state-of-the-art fusion techniques. The results demonstrate the superiority of DFG-HCEN. Moreover, the extended object detection experiments validate the ability of DFG-HCEN to fully support downstream tasks.},
  archive      = {J_ICV},
  author       = {Lingna Gao and Rencan Nie and Jinde Cao and Gucheng Zhang},
  doi          = {10.1016/j.imavis.2024.105115},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105115},
  shortjournal = {Image Vis. Comput.},
  title        = {DFG-HCEN: A distinctive-feature guided and hierarchical channel enhanced network-based infrared and visible image fusion},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RAD-BNN: Regulating activation distribution for accurate
binary neural network. <em>ICV</em>, <em>148</em>, 105114. (<a
href="https://doi.org/10.1016/j.imavis.2024.105114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The outstanding performance of deep convolutional neural networks comes from their effective extraction and learning ability. Although binary neural networks (BNNs) have the obvious advantages of low storage and high efficiency over their full-precision counterparts on resource-constrained hardware devices, the accuracy degradation brought by binary quantization is still an unavoidable problem. The activation distribution in BNNs is a key factor affecting network performance. To elevate the accuracy of BNNs, in this paper, we propose to regulate the activation distribution to strengthen the representation ability of BNNs. We first propose an Information Entropy enhancement Basic block (IEBlock) to build a competitive baseline model with higher information entropy of output activation distribution. Specifically, we build the IEBlock by deliberately reorganizing the position of the elements in the normal basic block based on a deep analysis of the information flow. After that, we propose a Depth-aware Activation Distribution Amendment (DADA) module, which learns the interdependencies of feature channels to amend the activation distribution with information loss after binary convolution. Extensive experiments demonstrate that our method effectively improves the information entropy of binary activations and elevates the accuracy of BNNs. Our method has outperformed the state-of-the-art methods on CIFAR-10 and ImageNet datasets. Code is available at: https://github.com/tomorrow-rain/RAD-BNN},
  archive      = {J_ICV},
  author       = {Mingyu Yuan and Songwei Pei},
  doi          = {10.1016/j.imavis.2024.105114},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105114},
  shortjournal = {Image Vis. Comput.},
  title        = {RAD-BNN: Regulating activation distribution for accurate binary neural network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced approach for few-shot segmentation via smooth
downsampling mask and label smoothing loss. <em>ICV</em>, <em>148</em>,
105113. (<a href="https://doi.org/10.1016/j.imavis.2024.105113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation aims to segment new categories with only a small number of annotated images. Previous methods mainly focused on exploiting the pixel-level correlation between the support image and the query image, combined with attention-based methods, resulting in significant advancements. In this paper, we introduce a new perspective to enhance few-shot segmentation. We identify that utilizing the bilinear interpolation method to downsample the mask leads to the loss of fine-grained information from the target features. To address this issue, we propose a Smooth Downsampling Mask (SDM) method. The SDM method is designed to retain more effective target semantic features by employing a cascaded downsampling approach with a smooth kernel for mask processing. Additionally, we propose a label smoothing loss to further enhance the performance, which provides direct guidance for low-resolution feature map optimization. Both methods can be used as plug-and-play modules for existing methods. Notably, our proposed method does not involve additional learnable parameters and is computationally efficient, thus achieving painless gains. To validate the effectiveness of our method, we take three publicly available models as baselines and conduct extensive experiments on three public benchmarks PASCAL-5 i , COCO-20 i and FSS-1000, and achieve considerable improvement.},
  archive      = {J_ICV},
  author       = {Hailong Jin and Huiying Li},
  doi          = {10.1016/j.imavis.2024.105113},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105113},
  shortjournal = {Image Vis. Comput.},
  title        = {An enhanced approach for few-shot segmentation via smooth downsampling mask and label smoothing loss},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Person re-identification by utilizing hierarchical spatial
relation reasoning. <em>ICV</em>, <em>148</em>, 105111. (<a
href="https://doi.org/10.1016/j.imavis.2024.105111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have become increasingly popular in person re-identification (re-id) as they enhance the discriminative features while suppressing extraneous or irrelevant details. However, these methods face a potential limitation in overlooking the relationships between semantic regions focusing solely on salient features or pixel-level matching information. As a result, the model performance becomes susceptible to adverse factors such as pose variations, misalignment, and background clutter. To overcome this issue, we propose a novel deep relational learning network (DRLNet) that learns relationship knowledge between semantic regions and “local–global” information to improve discriminative feature learning effectively. We manually establish the relation between local regions using a simple partition strategy that captures global structural information and improves relation-aware attention learning. Dividing images into uniform partitions allows each part-level feature to represent itself and the entire image area, facilitating the learning of more refined and discerning relationships. Our approach&#39;s efficacy is evaluated on three datasets, where experimental results exhibit its superiority. Significantly, our approach achieves a new state-of-the-art Rank-1 accuracy of 84.7%, 80.9%, and 83.5% on CUHK03 (Labeled), CUHK03 (Detected), and MSMT17 datasets, respectively, outperforming current state-of-the-art methods. In conclusion, our proposed DRLNet demonstrates its potential as a highly effective solution to overcoming the limitations of deep learning-based methods by providing superior means of learning discriminative features while incorporating relationships between semantic regions.},
  archive      = {J_ICV},
  author       = {Gengsheng Xie and Hanbing Su and Yong Luo and Wenle Wang and Yugen Yi and Shan Zhong},
  doi          = {10.1016/j.imavis.2024.105111},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105111},
  shortjournal = {Image Vis. Comput.},
  title        = {Person re-identification by utilizing hierarchical spatial relation reasoning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TIE-KD: Teacher-independent and explainable knowledge
distillation for monocular depth estimation. <em>ICV</em>, <em>148</em>,
105110. (<a href="https://doi.org/10.1016/j.imavis.2024.105110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models . To mitigate this, we introduce a novel Teacher-Independent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher&#39;s output, enabling feature-based knowledge distillation solely from the teacher&#39;s response. This approach allows for efficient student learning, leveraging the strengths of feature-based distillation. Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment. The code and pre-trained models related to this research are publicly available. 1},
  archive      = {J_ICV},
  author       = {Sangwon Choi and Daejune Choi and Duksu Kim},
  doi          = {10.1016/j.imavis.2024.105110},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105110},
  shortjournal = {Image Vis. Comput.},
  title        = {TIE-KD: Teacher-independent and explainable knowledge distillation for monocular depth estimation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IIMT-net: Poly-1 weights balanced multi-task network for
semantic segmentation and depth estimation using interactive
information. <em>ICV</em>, <em>148</em>, 105109. (<a
href="https://doi.org/10.1016/j.imavis.2024.105109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation and depth estimation are two basic researchable problems in computer vision . In common, we explore the two tasks separately. However, in some scenes, such as autonomous driving , they need be done at the same time. Meanwhile, there exists interconnected information between two tasks, which can jointly promote the performances of them. Thus, we explore the two tasks based on multi-task learning to jointly train the tasks and gain predictions together. In this paper, we build Interactive Information Multi-Task Network (IIMT-Net) incorporating the information interactive modules, trained with proposed task-balancing strategy. To be specific, we construct the principal part of encoder and decoder based on Transformer to well capture the global information. For better utilization of the task interaction between two tasks, we also add information fusion modules in two sub-decoders. In addition, the task-balancing strategy, Poly-1 weights, is designed as the balance among samples with different degrees of difficulty to ensure the network won&#39;t be biased towards any task severely. The proposed approach&#39;s exceptional performance has been extensively showcased through experimental results on the NYU Depth V2 dataset, the Cityscapes dataset, and the SUN RGB-D dataset. Our model can complete the predictions of semantic segmentation task and depth estimation task together and obtain mIoU values of 46.66 % % on the NYU Depth V2 dataset, 66.37 % % on the Cityscapes dataset, and 49.89 % % on the SUN RGB-D dataset, respectively with rmse values of 0.648, 6.630 and 0.401 for depth estimation task, which outperform most existing methods in multi-task learning.},
  archive      = {J_ICV},
  author       = {Mengfei He and Zhiyou Yang and Guangben Zhang and Yan Long and Huaibo Song},
  doi          = {10.1016/j.imavis.2024.105109},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105109},
  shortjournal = {Image Vis. Comput.},
  title        = {IIMT-net: Poly-1 weights balanced multi-task network for semantic segmentation and depth estimation using interactive information},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A method of degradation mechanism-based unsupervised remote
sensing image super-resolution. <em>ICV</em>, <em>148</em>, 105108. (<a
href="https://doi.org/10.1016/j.imavis.2024.105108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image (RSI) super-resolution (SR) is an efficient and low-cost technique to achieve high-resolution and high-quality reconstruction images. The quality of RSI SR reconstruction is affected by the prior information contained in the degradation model . Therefore, studying how to incorporate more RSI degradation prior into the degradation model is crucial. This article presents an approach to design the degradation model by extracting degradation factors from the perspective of remote sensing imaging mechanisms. It includes two aspects: simulating the atmospheric scattering effect through RGB channel weights downsampling and the comprehensive degradation effect of the remote sensing imaging platform through combined blurring. Furthermore, we proposed a high-performance RSI SR network based on degradation mechanism (RSN-DM), which includes a degrader D D and a generator G G , to employ remote sensing prior fully. We conducted experiments on the UC Merced Land-Use and WPU-RESIS45 datasets, demonstrating that our proposed method is effective. Our method achieves state-of-the-art (SOTA) performance in quantitative evaluation and visual quality. Finally, we apply the proposed degradation model to other networks to further validate the model&#39;s effectiveness. Therefore, the degradation model proposed in this paper can enhance the performance of remote sensing image super-resolution techniques in practical applications.},
  archive      = {J_ICV},
  author       = {Zhikang Zhao and Yongcheng Wang and Ning Zhang and Yuxi Zhang and Zheng Li and Chi Chen},
  doi          = {10.1016/j.imavis.2024.105108},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105108},
  shortjournal = {Image Vis. Comput.},
  title        = {A method of degradation mechanism-based unsupervised remote sensing image super-resolution},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual tracking based on spatiotemporal transformer and
fusion sequences. <em>ICV</em>, <em>148</em>, 105107. (<a
href="https://doi.org/10.1016/j.imavis.2024.105107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, Transformer-based visual tracking methods have exhibited impressive performance. However, despite their widespread adoption, they still have certain limitations. For example, the design of the Transformer framework is somewhat original and redundant, resulting in lower efficiency. In addition, their application methods lack time consideration, and there is a lack of spatiotemporal correlation between tracking video sequences and predicting coordinate sequences, making it difficult to effectively integrate, and the robustness of corresponding tracking templates is insufficient. To address these issues, we propose a new visual tracking method (STFS). Firstly, it introduces a novel Flatten Transformer architecture, which, in comparison to previous modules, offers enhanced efficiency and expressiveness . Secondly, it takes multi frame feature maps and bounding box coordinates as inputs, integrates spatiotemporal information through the spatiotemporal sequence attention module, and provides relevant sequences for historical trend prediction. Finally, it uses diffusion methods to construct tracking templates and improve stability. To verify the performance of the tracker, we conducted experiments on benchmark datasets including GOT-10 K, LaSOT, TrackingNet, VOT2020, OTB100, and UAV123. The results demonstrate that STFS has achieved competitive experimental results.},
  archive      = {J_ICV},
  author       = {Ruixu Wu and Yanli Liu and Xiaogang Wang and Peilin Yang},
  doi          = {10.1016/j.imavis.2024.105107},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105107},
  shortjournal = {Image Vis. Comput.},
  title        = {Visual tracking based on spatiotemporal transformer and fusion sequences},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pedestrian detection in low-light conditions: A
comprehensive survey. <em>ICV</em>, <em>148</em>, 105106. (<a
href="https://doi.org/10.1016/j.imavis.2024.105106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection remains a critical problem in various domains, such as computer vision , surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety . Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies ( i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48 % 48% ) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than 6 % of the works.},
  archive      = {J_ICV},
  author       = {Bahareh Ghari and Ali Tourani and Asadollah Shahbahrami and Georgi Gaydadjiev},
  doi          = {10.1016/j.imavis.2024.105106},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105106},
  shortjournal = {Image Vis. Comput.},
  title        = {Pedestrian detection in low-light conditions: A comprehensive survey},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RagBERT: Relationship-aligned and grammar-wise BERT model
for image captioning. <em>ICV</em>, <em>148</em>, 105105. (<a
href="https://doi.org/10.1016/j.imavis.2024.105105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning has become one of the most popular research problems in the field of artificial intelligence . Although many studies have achieved excellent results, there still are some challenges, for example, cross-modal feature alignment lacks explicit guidance, and model-generated sentences contain grammatical errors. In this paper, we propose a relationship-aligned and grammar-wise BERT model, which integrates a relationship exploration module and a grammar enhancement module into the BERT-based model. Specifically, in the relationship exploration module, to explore relationship tags as anchors to guide semantic alignment, we design a network to calculate the cosine similarity between visual features and word vector information . We construct the grammar enhancement module similarly to the BERT. That means we use two BERT modules in our framework. The first is the main frame for generating captions, and the second is the auxiliary model to determine whether the syntax of the generated caption is correct. To validate the performance of our proposed model, we conduct abundant experiments on the MSCOCO dataset, Flickr30k dataset, and Flickr8k dataset. Experimental results show that our proposed method performs better than state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Hengyou Wang and Kani Song and Xiang Jiang and Zhiquan He},
  doi          = {10.1016/j.imavis.2024.105105},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105105},
  shortjournal = {Image Vis. Comput.},
  title        = {RagBERT: Relationship-aligned and grammar-wise BERT model for image captioning},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wav2NeRF: Audio-driven realistic talking head generation via
wavelet-based NeRF. <em>ICV</em>, <em>148</em>, 105104. (<a
href="https://doi.org/10.1016/j.imavis.2024.105104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Talking head generation is an essential task in various real-world applications such as film making and virtual reality. To this end, recent works focus on the NeRF-based methods that can capture the 3D structural information of faces and generate more natural and vivid talking videos. However, the existing NeRF-based methods fail to accurately generate the audio-synced videos. In this paper, we point out that the previous methods do not consider the audio-visual representations explicitly, which is crucial for precise lip synchronization . Moreover, the existing methods struggle to generate high-frequency details, making the generation results unnatural. To overcome these problems, we propose a novel audio-synced and high-fidelity NeRF-based talking head generation framework, named Wav2NeRF, which learns audio-visual cross-modality representations and employs the wavelet transform for better visual quality. In precise, we adopt a 2D CNN-based neural rendering decoder to a NeRF-based encoder for fast generation of the whole image to employ a new multi-level SyncNet loss for accurate lip synchronization . We also propose a novel cross-attention module to effectively fuse the image and the audio representation. In addition, we integrate the wavelet transform into our framework by proposing the wavelet loss function to enhance high-frequency details. We demonstrate that the proposed method renders realistic and audio-synced talking head videos and shows outstanding performances on average in 4 representative metrics, including PSNR (+ 4.7%), SSIM (+ 2.2%), LMD (+ 51.3%), and SyncNet Confidence (+ 154.7%) compared to the NeRF-based current state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Ah-Hyung Shin and Jae-Ho Lee and Jiwon Hwang and Yoonhyung Kim and Gyeong-Moon Park},
  doi          = {10.1016/j.imavis.2024.105104},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105104},
  shortjournal = {Image Vis. Comput.},
  title        = {Wav2NeRF: Audio-driven realistic talking head generation via wavelet-based NeRF},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environmentally adaptive fast object detection in UAV
images. <em>ICV</em>, <em>148</em>, 105103. (<a
href="https://doi.org/10.1016/j.imavis.2024.105103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting objects in aerial images poses a challenging task due to the presence of numerous small objects and complex environmental information. To address these problems, we propose an efficient detector specifically designed for aerial images, named EAF-YOLOv8, based on YOLOv8-S. In this paper, we introduce a novel backbone network called EAFNet, specifically designed for small object detection. EAFNet consists of the Rapidly Merging Receptive Fields Aggregation Module (RMRFAM) and Multi-Scale Channel Attention (MSCA). The RMRFAM utilizes dilated convolution (DConv) and partial convolution (PConv) to acquire richer receptive fields, capturing more extensive contextual information at higher levels while reducing redundancy in channel information, thereby accelerating inference speed. Furthermore, inspired by denoising tasks, we focus on the feature information surrounding the target background and propose MSCA. MSCA integrates channel attention with an embedded self-attention feature pyramid, extending the feature learning scope to the surrounding environment of the target, beyond the target itself. This approach utilizes enhanced background features to elicit a higher response for small targets, reducing false positives. Experimental results demonstrate that in UAVDT and VisDrone2019, the proposed EAF-YOLOv8 achieves mAP50 scores of 34.3 % 34.3% and 49.7 % 49.7% , respectively. Additionally, EAF-YOLOv8 exhibits high real-time inference speeds of 77.60 77.60 FPS and 55.56 55.56 FPS, showcasing competitive detection performance.},
  archive      = {J_ICV},
  author       = {Mengmei Sang and Shengwei Tian and Long Yu and Guoqi Wang and Yue Peng},
  doi          = {10.1016/j.imavis.2024.105103},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105103},
  shortjournal = {Image Vis. Comput.},
  title        = {Environmentally adaptive fast object detection in UAV images},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LELD: Learn enhancement by learning degradation.
<em>ICV</em>, <em>148</em>, 105102. (<a
href="https://doi.org/10.1016/j.imavis.2024.105102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing low-light images improves both the visibility and quality of the images. Existing methods primarily focus on the enhancement process and heavily rely on the supervised learning strategy , where low/normal-light image pairs are used as the training dataset. In this paper, we propose a novel method called Learn Enhancement by Learning Degradation (LELD) to achieve efficient light adjustment and scene fidelity. We use a carefully designed degradation network (DNet) to guide the enhancement network (ENet). Specifically, the role of DNet is transforming normal-light images into low-light images. For better generalization ability , we employ an unsupervised learning strategy and a generative adversarial network framework. The training is totally dependent on unpaired datasets. Inspired by Retinex theory , we propose a fidelity loss to maintain color and detail during the degradation process . The ENet exhibits a straightforward architecture and achieves efficient enhancement. Experimental results demonstrate the advantages of our method over state-of-the-art methods in terms of visual quality and enhancement efficiency.},
  archive      = {J_ICV},
  author       = {Qintong Li and Yong Ma and Jun Huang and Can Zhang and Zhao Cai},
  doi          = {10.1016/j.imavis.2024.105102},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105102},
  shortjournal = {Image Vis. Comput.},
  title        = {LELD: Learn enhancement by learning degradation},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater image enhancement based on global features and
prior distribution guided. <em>ICV</em>, <em>148</em>, 105101. (<a
href="https://doi.org/10.1016/j.imavis.2024.105101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images often suffer from substantial image blur and color distortion due to the variability of water conditions and the physical location of optical equipment, which significantly impacts the underwater intelligent system&#39;s environmental perception . Standard methods exhibit limited generalization capabilities, leading to considerable performance fluctuations when handling images with uncontrolled degradation. In this research, we leverage global features and the prior distribution of ground truth images to guide our enhancement model, introducing a novel conditional Variational Auto-Encoder-based model, named UWG-VAE, to address these challenges. UWG-VAE enhances model controllability by incorporating prior distribution information and classes of degraded styles into the decoder of the enhancement model. We assess the performance of UWG-VAE in underwater image enhancement tasks across four challenging real underwater image datasets, comparing it to state-of-the-art models. UWG-VAE demonstrates a substantial enhancement in visual quality, with notable improvements in UIQM, UCIQE, and URanker evaluation metrics when compared to existing state-of-the-art models.},
  archive      = {J_ICV},
  author       = {Siqi Lu and Fengxu Guan and Haitao Lai},
  doi          = {10.1016/j.imavis.2024.105101},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105101},
  shortjournal = {Image Vis. Comput.},
  title        = {Underwater image enhancement based on global features and prior distribution guided},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From darkness to clarity: A comprehensive review of
contemporary image shadow removal research (2017–2023). <em>ICV</em>,
<em>148</em>, 105100. (<a
href="https://doi.org/10.1016/j.imavis.2024.105100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The removal of shadows from images is a classic problem in computer vision , aiming to restore the lighting in shadowed areas, thereby reducing the information interference and loss caused by the presence of shadows. In recent years, numerous excellent shadow removal algorithms have emerged, particularly with the rapid development of deep learning technology, which has disrupted traditional physics-based approaches and significantly improved the effectiveness of shadow removal. In this paper, we conduct a comprehensive survey of shadow removal methods published from 2017 to the present. We first introduce background knowledge about image shadow removal, providing detailed explanations of both physics-based and learning-based shadow removal methods. We analyze and compare these algorithms from both quantitative and qualitative perspectives, reassessing all models that provided open-source result sets according to uniform criteria. Additionally, we introduce commonly used datasets and evaluation metrics in the field. Finally, we discuss applications of shadow removal in specific scenarios, along with research challenges and opportunities in this domain.},
  archive      = {J_ICV},
  author       = {Xiujin Zhu and Chee-Onn Chow and Joon Huang Chuah},
  doi          = {10.1016/j.imavis.2024.105100},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105100},
  shortjournal = {Image Vis. Comput.},
  title        = {From darkness to clarity: A comprehensive review of contemporary image shadow removal research (2017–2023)},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-branch dual attention segmentation network for
epiphyte drone images. <em>ICV</em>, <em>148</em>, 105099. (<a
href="https://doi.org/10.1016/j.imavis.2024.105099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring images of epiphytes growing on trees using Unmanned Aerial Vehicles (UAVs) enables botanists to efficiently collect data on these important plant species . Despite the advantages offered by UAVs, challenges such as complex backgrounds, uneven lighting inside the tree canopy, and accessibility issues hinder the acquisition of quality images , resulting in acquiring images datasets of heterogenous quality. AI/Deep Learning algorithms can be used to segment target plants in these images for selecting sampling locations. Existing DL models require large volume of data for training, and they tend to prioritize local features over global ones, impacting segmentation accuracy , particularly on smaller, heterogeneous quality image datasets. To overcome these limitations, we propose a multi-branch dual attention segmentation network designed to effectively handle small datasets with heterogeneous quality. The proposed network incorporates dedicated branches for extracting both global and local features, utilizing spatial and channel attention mechanisms to focus on important regions. Through a fusion process and a decoder with crossed fusion technique, this network effectively combines and enhances features from multiple branches, resulting in improved segmentation performance . Output obtained from the trained model demonstrated major improvements in predicting the boundary regions and class labels, even in close-range, low-light, and zoomed/cropped images. The average Intersection over Union (IoU) scores of the trained model was 5% higher for images acquired close range, 48% higher for images in low-light conditions, and 68% higher for zoomed/cropped images when compared to those obtained from TransUnet, a state-of-the-art vision transformer model trained on epiphyte dataset. The proposed network can be used for segmenting epiphytes in images of heterogeneous quality as well as identifying targets in images acquired in domains such as agriculture and forestry.},
  archive      = {J_ICV},
  author       = {V.V. Sajith Variyar and V. Sowmya and Ramesh Sivanpillai and Gregory K. Brown},
  doi          = {10.1016/j.imavis.2024.105099},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105099},
  shortjournal = {Image Vis. Comput.},
  title        = {A multi-branch dual attention segmentation network for epiphyte drone images},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Two-dimensional hybrid incremental learning (2DHIL)
framework for semantic segmentation of skin tissues. <em>ICV</em>,
<em>148</em>, 105098. (<a
href="https://doi.org/10.1016/j.imavis.2024.105098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to enhance the robustness and generalization capability of a deep learning transformer model used for segmenting skin carcinomas and tissues through the introduction of incremental learning. Deep learning AI models demonstrate their claimed performance only for tasks and data types for which they are specifically trained. Their performance is severely challenged for the test cases which are not similar to training data thus questioning their robustness and ability to generalize. Moreover, these models require an enormous amount of annotated data for training to achieve desired performance. The availability of large annotated data, particularly for medical applications, is itself a challenge. Despite efforts to alleviate this limitation through techniques like data augmentation , transfer learning , and few-shot training, the challenge persists. To address this, we propose refining the models incrementally as new classes are discovered and more data becomes available, emulating the human learning process. However, deep learning models face the challenge of catastrophic forgetting during incremental training. Therefore, we introduce a two-dimensional hybrid incremental learning framework for segmenting non-melanoma skin cancers and tissues from histopathology images. Our approach involves progressively adding new classes and introducing data of varying specifications to introduce adaptability in the models. We also employ a combination of loss functions to facilitate new learning and mitigate catastrophic forgetting. Our extended experiments demonstrate significant improvements, with an F1 score reaching 91.78, mIoU of 93.00, and an average accuracy of 95%. These findings highlight the effectiveness of our incremental learning strategy in enhancing the robustness and generalization of deep learning segmentation models while mitigating catastrophic forgetting.},
  archive      = {J_ICV},
  author       = {Muhammad Imran and Muhammad Usman Akram and Mohsin Islam Tiwana and Anum Abdul Salam and Danilo Greco},
  doi          = {10.1016/j.imavis.2024.105098},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105098},
  shortjournal = {Image Vis. Comput.},
  title        = {Two-dimensional hybrid incremental learning (2DHIL) framework for semantic segmentation of skin tissues},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing consistency in virtual try-on: A novel
diffusion-based approach. <em>ICV</em>, <em>148</em>, 105097. (<a
href="https://doi.org/10.1016/j.imavis.2024.105097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In new technology scenarios, virtual try-on aims to integrate clothing onto body images naturally, enhancing shopping experience by simulating true effect of clothes. As the image resolution increases, we expect to improve the consistency of the result, i.e., to ensure that various elements of the image are harmonized in terms of color, shading, style, and texture in order to achieve a natural visual effect. Many studies based on Generative Adversarial Networks(GANs) struggle with consistency. They encounter challenges in accurately depicting the fabric of target garments, as well as natural shadows and folds, and sometimes exhibit visual discontinuities or inconsistencies. So, we propose a new approach CSD-VTON based on latent diffusion model . Considering that traditional UNet&#39;s computational primitives struggle to capture complex transformation relationships at the pixel level , we address this issue by concatenating the warped cloth images generated by the warping module with the noise image . Additionally, cascade feature extraction module is introduced to extract in-store garment features, which ensures the preservation of texture and details in the target garments. Finally, we incorporate the skip-connection supplementary module to compensate for the reconstruction error. We conducted experiments using the DressCode and VITON-HD datasets to demonstrate the effectiveness and superiority of our approach.},
  archive      = {J_ICV},
  author       = {Chenglin Zhou and Wei Zhang and Zhichao Lian},
  doi          = {10.1016/j.imavis.2024.105097},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105097},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing consistency in virtual try-on: A novel diffusion-based approach},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial attacks and defenses in person search: A
systematic mapping study and taxonomy. <em>ICV</em>, <em>148</em>,
105096. (<a href="https://doi.org/10.1016/j.imavis.2024.105096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person Search aims at retrieving a specific individual (the query) within a collection of whole scene images from diverse, non-overlapping cameras. It has the potential to play a pivotal role in various public safety applications like suspect searching and identifying abandoned luggage owners. Person Search encompasses two Computer Vision challenges: 1. Object Detection, which entails localizing humans in whole scene images, and 2. Person Re-Identification, where the query image is compared with images of detected individuals to establish identification. The critical nature of Person Search underscores the imperative to safeguard it against security threats, such as adversarial attacks , which can result in non-detection or misidentification. While adversarial attacks and defense mechanisms have been extensively studied for both Object Detection and Person Re-Identification, there is a noticeable gap in research concerning Person Search. This work presents a comprehensive Systematic Mapping Study and taxonomy of adversarial attacks and defenses in Person Search, utilizing Parsifal and ChatGPT 4 for in-depth analysis. We highlight the persistent challenges associated with Person Search and discuss prospects for future advancements in addressing its vulnerabilities.},
  archive      = {J_ICV},
  author       = {Eduardo de O. Andrade and Joris Guérin and José Viterbo and Igor Garcia Ballhausen Sampaio},
  doi          = {10.1016/j.imavis.2024.105096},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105096},
  shortjournal = {Image Vis. Comput.},
  title        = {Adversarial attacks and defenses in person search: A systematic mapping study and taxonomy},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MINet: Modality interaction network for unified multi-modal
tracking. <em>ICV</em>, <em>148</em>, 105071. (<a
href="https://doi.org/10.1016/j.imavis.2024.105071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While RGB-based trackers have made impressive progress, they still falter in complex scenarios, necessitating the exploration of multi-modal tracking strategies that leverage auxiliary modalities. However, most existing methods lack sufficient exploration and interaction of complementary information within and between modalities. To address this, we propose a Modality Interaction Network (MINet), a unified framework for multi-modal tracking. It consists of a Modality Representation Module (MRM) and a Memory Query Module (MQM). MRM enforces communications between different modalities by a designed Modality Interaction module (MIM) and fuses multi-modal information by a Modality Fuse Module (MFM) to generate more discriminative representation. MQM maintains historical multi-modal information and builds long-range dependencies between current and historical targets for tracking, which enhances the tracking performance, especially when targets undergo significant deformation and occlusions. To verify efficiency across different multi-modal tracking paradigms, we conduct extensive experiments, including RGB-D, RGB-T, and RGB-E. The experimental results demonstrate that in these multi-modal tracking tasks, the proposed MINet achieves outstanding performance compared to state-of-the-art trackers. Specifically, it outperforms them by 1% in RGB-D, 1.2% in RGB-T, and 1% in RGB-E tracking performance, respectively.},
  archive      = {J_ICV},
  author       = {Shuang Gong and Zhu Teng and Rui Li and Jack Fan and Baopeng Zhang and Jianping Fan},
  doi          = {10.1016/j.imavis.2024.105071},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105071},
  shortjournal = {Image Vis. Comput.},
  title        = {MINet: Modality interaction network for unified multi-modal tracking},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-modal hybrid architectures for gastrointestinal tract
image analysis: A systematic review and futuristic applications.
<em>ICV</em>, <em>148</em>, 105068. (<a
href="https://doi.org/10.1016/j.imavis.2024.105068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper presents an in-depth exploration of gastrointestinal (GI) tract image analysis, particularly emphasizing organ and polyp segmentation . It addresses the inherent challenges posed by the GI tract&#39;s complex anatomy and diverse pathologies, which complicate accurate image analysis. Central to this review is the examination of hybrid computational models that integrate convolutional neural networks (CNNs) and Transformers. This synergy enhances the accuracy of segmenting intricate structures in GI tract imaging, marking a significant advancement in the field. A notable contribution of this review is the systematic categorization and analysis of the latest methodologies in organ and polyp segmentation. It provides a comprehensive overview of various techniques, highlighting their strengths and limitations in addressing the specifications of GI tract imaging. This survey serves as a valuable reference for researchers, outlining current practices and offering insights for future innovations. The review also underscores the critical role of extensive and diverse datasets in advancing GI tract image analysis. It stresses the need for high-quality datasets to effectively train and evaluate emerging models, addressing the broad spectrum of GI tract conditions. Moreover, the review delves into the burgeoning area of Generative AI, exploring its potential to enrich datasets and enhance segmentation models . Future developments in GI tract segmentation will focus on refining hybrid CNN-Transformer models and creating larger, more diverse datasets for better model training. Specialized focus on specific segmentation challenges, like polyp and organ segmentation , is anticipated. The field will explore Generative AI applications for innovative segmentation approaches . Collaborative efforts between technologists and clinicians will enhance practical clinical integration and applicability.},
  archive      = {J_ICV},
  author       = {Praneeth Nemani and Venkata Surya Sundar Vadali and Prathistith Raj Medi and Ashish Marisetty and Satyanarayana Vollala and Santosh Kumar},
  doi          = {10.1016/j.imavis.2024.105068},
  journal      = {Image and Vision Computing},
  month        = {8},
  pages        = {105068},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross-modal hybrid architectures for gastrointestinal tract image analysis: A systematic review and futuristic applications},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLIC: An efficient method for object localization and
classification on edge devices. <em>ICV</em>, <em>147</em>, 105095. (<a
href="https://doi.org/10.1016/j.imavis.2024.105095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Tiny AI , we introduce “You Only Look at Interested Cells” (YOLIC), an efficient method for object localization and classification on edge devices. Through seamlessly blending the strengths of semantic segmentation and object detection, YOLIC provides improved computational efficiency and precision compared to traditional methods. By adopting Cells of Interest for classification instead of individual pixels , YOLIC encapsulates relevant information, reduces computational load , and enables rough object shape inference. Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape. To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell for effectively recognizing overlapping or closely situated objects. This paper presents extensive experiments on multiple datasets to demonstrate that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.},
  archive      = {J_ICV},
  author       = {Kai Su and Yoichi Tomioka and Qiangfu Zhao and Yong Liu},
  doi          = {10.1016/j.imavis.2024.105095},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105095},
  shortjournal = {Image Vis. Comput.},
  title        = {YOLIC: An efficient method for object localization and classification on edge devices},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box reversible adversarial examples with invertible
neural network. <em>ICV</em>, <em>147</em>, 105094. (<a
href="https://doi.org/10.1016/j.imavis.2024.105094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible Adversarial Example (RAE) has been widely researched for its ability to ensure authorized access while preventing unauthorized recognition. Existing RAE schemes focus on Reversible Data Hiding techniques and white-box attacks. However, white-box attacks might be impractical due to the unknown parameters of the target model. Besides, these methods suffer massive loss during the embedding of perturbations, impacting the RAE&#39;s quality. In this paper, we propose I-RAE scheme to generate black-box RAE with minimal loss based on Invertible Neural Network (INN). Specifically, Black-box Attack Flow (BAFlow) is introduced to generate perturbations on a Gaussian distribution that are more easily embeddable. Furthermore, to enhance the embedding capability of RAE, we innovatively treat the embedding of perturbation as an image hiding and propose Perturbation Hiding Network (PHN) to reversibly hide the entire perturbation into the adversarial example. We also implement wavelet high-frequency hiding to reduce the degradation in the visual quality of RAE. Experimental results on the ImageNet and CIFAR-10 datasets demonstrate that I-RAE achieves state-of-the-art black-box attack ability and visual quality.},
  archive      = {J_ICV},
  author       = {Jielun Huang and Guoheng Huang and Xuhui Zhang and Xiaochen Yuan and Fenfang Xie and Chi-Man Pun and Guo Zhong},
  doi          = {10.1016/j.imavis.2024.105094},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105094},
  shortjournal = {Image Vis. Comput.},
  title        = {Black-box reversible adversarial examples with invertible neural network},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). JGULF: Joint global and unilateral local feature network for
micro-expression recognition. <em>ICV</em>, <em>147</em>, 105091. (<a
href="https://doi.org/10.1016/j.imavis.2024.105091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression is a subtle facial movement that is fleeting and manifest in localized areas, making it difficult for the human eye to detect and recognize it. Although algorithms that extract facial features from specific regions or the entire face have shown potential, the classification of micro-expressions using features from symmetrical left and right regions can be challenging in the presence of unilateral movements. This can ultimately affect the performance of micro-expression recognition. To address this issue, we propose a network called Joint Global and Unilateral Local Features (JGULF) for micro-expression recognition. Initially, we employ a Convolutional Neural Network (CNN) and an adjusted Vision Transformer (ViT) model to extract global features from micro-expressions. The local feature extraction module is designed based on global features. The facial features are divided into multiple local regions with varying scales. After that, local feature learning and selection are performed to filter out unilateral local features related to micro-expression movements efficiently. Finally, global and local features are combined to classify micro-expressions. Through comprehensive experimental validation, our algorithm achieves state-of-the-art classification performance on the SMIC, CASMEII, and SAMM micro-expression datasets, demonstrating the effectiveness of combining global features and selecting local features.},
  archive      = {J_ICV},
  author       = {Fengping Wang and Jie Li and Chun Qi and Lin Wang and Pan Wang},
  doi          = {10.1016/j.imavis.2024.105091},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105091},
  shortjournal = {Image Vis. Comput.},
  title        = {JGULF: Joint global and unilateral local feature network for micro-expression recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multitask tensor-based relation network for cloth-changing
person re-identification. <em>ICV</em>, <em>147</em>, 105090. (<a
href="https://doi.org/10.1016/j.imavis.2024.105090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cloth-changing person re-identification (CC-ReID) is more challenging than person re-identification (ReID) because of the unreliability of cloth-relevant features. Existing CC-ReID methods output the unique latent vector feature for the same pedestrian image. However, on the one hand, the latent vector feature generated by the pooling layer will lose more spatial information. On the other hand, the features that they focus on are the same for the same pedestrian image under different comparison pedestrian images, which does not consider the pedestrian pair relation information. For this, we propose a Multitask Tensor-based Relation Network ( MTTRN ) for CC-ReID. In MTTRN , we utilize the augmented cloth-changing identity images, human parsing images and head images to guide the model learning more fine-grained cloth-irrelevant feature cues. We propose a novel channel subspace generator and use the tensor mode-n product to generate the subspace tensor features instead of the vector feature generated by the pooling layer, which can retain more spatial feature information. Furthermore, we assume that the model will focus on different features for the same pedestrian image under different comparison pedestrian images in the same feature subspace or the same pedestrian image pair in different feature subspaces, in which the tensor feature pair relation is considered fully to mine more robust features. Extensive experiments show that our method achieves the state-of-the-art or competitive performance on three CC-ReID benchmark datasets and demonstrate the robustness of our model.},
  archive      = {J_ICV},
  author       = {Jian Xu and Bo Liu and Yanshan Xiao and Fan Cao and Jinghui He},
  doi          = {10.1016/j.imavis.2024.105090},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105090},
  shortjournal = {Image Vis. Comput.},
  title        = {A multitask tensor-based relation network for cloth-changing person re-identification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proactive hybrid learning framework for real-time
multi-vehicle detection in unregulated traffic environments.
<em>ICV</em>, <em>147</em>, 105081. (<a
href="https://doi.org/10.1016/j.imavis.2024.105081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable multi-vehicle detection in unregulated traffic environments is a crucial computer vision task in the development of Intelligent Transportation Systems (ITS). Despite the promising potential of Deep Learning (DL) methods for vehicle detection, the presence of uncertainties such as varying vehicle shapes and sizes, intricate background clutter, and unpredictable vehicle flow contribute to the chaotic unregulated traffic. For real-time vehicle detection in an unregulated traffic environment this paper proposes the Hybrid Learning Multi-Vehicle Detection framework (HL-MVD) based on the C onvolutional M ulti-head A ttention T ransformer D etector (CMATDet). The primary objective of this study is to generate a dataset containing highly informative video frames using a Pool-based Active Learning Strategy (PALS). Additionally, transfer learning will be implemented to train CMATDet to achieve improved accuracy and reduced detection latency. The proposed approach restructures the baseline YOLOv5x and incorporates a Multi-head Attention transformer encoder to effectively extract global features and a Scale Specific Bidirectional Feature Pyramid Network (SS-BiFPN) to facilitate multi-scale feature representation. Simplified Optimal Transport Algorithm with top-q approximation technique (Sim-OTA) is utilized for label assignment approach. Heatmap analysis demonstrated the suitability of the newly generated dataset “AU-INV-P-PALS” for detecting specific Indian native vehicles. The performance of the proposed framework is evaluated on our custom-developed AU-INV-P-PALS vehicle dataset and the IITM-HeTra Dataset 1. In comparison with contemporary detection models, the proposed HL-MVD framework resulted with higher mAP scores (91.1% on mAP@0.5 and 78.3% on mAP@0.5:0.95) for AU-INV-P-PALS. The proposed model demonstrated lower inference latency (8.1 ms), higher precision score (82.7% for IoU = 0.5), and higher recall score (90.8% for IoU = 0.5) than recent deep learning-based detection models in the literature. The top-q approximation technique in the detection head results in a reduced false-positive rate compared to conventional models. Finally, the performance of the proposed framework is tested on CCTV traffic footage captured on city roads in Chennai, India.},
  archive      = {J_ICV},
  author       = {M. Ilamathi and Sabitha Ramakrishnan and Rakhul Kumar Babusankar},
  doi          = {10.1016/j.imavis.2024.105081},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105081},
  shortjournal = {Image Vis. Comput.},
  title        = {Proactive hybrid learning framework for real-time multi-vehicle detection in unregulated traffic environments},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Blind deblurring text images via beltrami regularization.
<em>ICV</em>, <em>147</em>, 105080. (<a
href="https://doi.org/10.1016/j.imavis.2024.105080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a blind image deblurring model based on Beltrami regularization. The existence and uniqueness of the Beltrami model are proved, and we perform a theoretical analysis of the convergence and error estimation of the algorithm for solving the Beltrami regularization model. Meanwhile, we apply a half-quadratic splitting algorithm to solve it and obtain the blur kernel of the blurred image. Furthermore, we get a clean image through some non-blind deblurring algorithms. Finally, we compare this with the state-of-art methods, and our algorithm has specific performance improvements and advantages in the text images. © 2024 Elsevier Ltd. All rights reserved.},
  archive      = {J_ICV},
  author       = {Haijun Gao and Minfu Feng},
  doi          = {10.1016/j.imavis.2024.105080},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105080},
  shortjournal = {Image Vis. Comput.},
  title        = {Blind deblurring text images via beltrami regularization},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth assisted novel view synthesis using few images.
<em>ICV</em>, <em>147</em>, 105079. (<a
href="https://doi.org/10.1016/j.imavis.2024.105079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach to improve the performance of Neural Radiance Fields (NeRF) from limited input views. NeRF has exhibited impressive capabilities in producing photo-realistic renderings when trained on dense input views, but its performance degrades as the number of training views decreases. Our key insight is that the original NeRF lacks geometric regularization and appearance information due to limited inputs, resulting in an over-fitting issue. To address this challenge, we present a novel method: first, a global sampling method with geometric regularization is employed by utilizing warped images as additional pseudo-views, which optimizes the multi-view consistency during the training. Second, we introduce a local patch sampling technique with perceptual regularization to ensure pixel correspondence in appearance. Furthermore, we incorporate depth information for explicit geometry regularization. We evaluate our method on the DTU dataset and LLFF dataset from a different number of inputs. Extensive evaluations demonstrate that our approach outperforms existing benchmarks across various metrics, achieving state-of-the-art results.},
  archive      = {J_ICV},
  author       = {Qian Li and Rao Fu and Fulin Tang},
  doi          = {10.1016/j.imavis.2024.105079},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105079},
  shortjournal = {Image Vis. Comput.},
  title        = {Depth assisted novel view synthesis using few images},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LVP-net: A deep network of learning visual pathway for edge
detection. <em>ICV</em>, <em>147</em>, 105078. (<a
href="https://doi.org/10.1016/j.imavis.2024.105078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based edge detectors typically consist of the encoder and the decoder. To integrate multi-scale features into a global edge map effectively, researchers utilize classification networks such as VGG16 as the encoder and focus on the decoder architecture. In contrast to existing approaches, we propose a novel deep network for edge detection called learning-visual-pathway network (LVP-Net), in which an enhancer-encoder-decoder architecture is designed inspired by the biological visual pathway : the retina/lateral geniculate nucleus → the primary visual cortex (V1) → V2 → V4 → the inferior temporal cortex (IT). To simulate the visual mechanisms along this pathway, we design a feature enhancer network (FENet) that boosts the feature representation capability of the encoder. FENet is combined with VGG16 based on the hierarchical structure of the pathway. Furthermore, inspired by the integration ability of multiple features in IT, we introduce a feedforward fusion module (FFM). Finally, we evaluate LVP-Net on three benchmark datasets, i.e., BSDS500, NYUDv2, and Multicue. Experimental results demonstrate that our method achieves competitive performance compared with most state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Xiao Zhang and Chuan Lin and Fuzhang Li and Yijun Cao and Yongjie Li},
  doi          = {10.1016/j.imavis.2024.105078},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105078},
  shortjournal = {Image Vis. Comput.},
  title        = {LVP-net: A deep network of learning visual pathway for edge detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Giving loss a personal course: Universal loss reweighting to
improve stereo matching via uncertainty guidance. <em>ICV</em>,
<em>147</em>, 105077. (<a
href="https://doi.org/10.1016/j.imavis.2024.105077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although learning-based stereo matching methods have achieved remarkable performance, accurately recovering disparity maps for boundary areas (e.g., thin structures) remains an intractable issue. Existing stereo pipelines usually employ the standard L1 loss function, averaging pixel-wise losses equally despite variable difficulty. This mechanism inevitably overwhelms thin structures due to their limited proportion. In this paper, we propose reweighting the L1 loss to focus on pixels of varying hardness. First, we explicitly model the uncertainty of each pixel to gauge the confidence in its prediction. By aggregating the volume, uncertainty is obtained effortlessly. Second, uncertainty is mapped to weights, which reweight the L1 loss accordingly. The core of our approach lies in leveraging uncertainty to personalize the loss map adjustment, progressively optimizing challenging regions during training. Notably, our method requires no extra parameters or inference computations . Finally, we introduce the Boundary Pixel Error (BPE), a novel metric targeting boundary quality. Extensive experiments with the SceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness and universality of our elegant framework, seamlessly integrating it into existing models as a plug-and-play component, leading to substantial performance improvements .},
  archive      = {J_ICV},
  author       = {Yujun Liu and Xiangchen Zhang and Qiaoqiao Hao and Yang Luo and Jinhe Su and Guorong Cai},
  doi          = {10.1016/j.imavis.2024.105077},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105077},
  shortjournal = {Image Vis. Comput.},
  title        = {Giving loss a personal course: Universal loss reweighting to improve stereo matching via uncertainty guidance},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AM-BQA: Enhancing blind image quality assessment using
attention retractable features and multi-dimensional learning.
<em>ICV</em>, <em>147</em>, 105076. (<a
href="https://doi.org/10.1016/j.imavis.2024.105076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of no-reference image quality assessment (NR-IQA), acquiring pristine source content for reference is often unattainable. This absence of reference presents challenges in accurately estimating perceptual scores due to the diversity and complexity of distortion patterns. To tackle these challenges, we introduce a novel approach named AM-BQA: Enhancing Blind Image Quality Assessment using Attention Retractable Features and Multi-Dimensional Learning, designed to capture and analyze complex patterns. Our method involves several steps. Firstly, we extract crucial and intricate features using a vision transformer . Next, we employ a multi-head transpose attention block with a dual key, incorporating overlap convolution patches and transpose attention into these extracted features. Finally, the attention maps generated by this process pass through an attention retractable block and a weighted multi-head layer to calculate the final quality score. By employing this architecture, we enhance both global and local interactions between complex patches. To validate the effectiveness of our approach, we assess it on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10 K), including both synthetic datasets . Additionally, we conduct experiments on authentic datasets and demonstrate that our model achieves state-of-the-art performance across multiple datasets. The source code and pretrained models are available on this GitHub repository: https://github.com/adhikariastha5/AM-BQA .},
  archive      = {J_ICV},
  author       = {Astha Adhikari and Sang-Woong Lee},
  doi          = {10.1016/j.imavis.2024.105076},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105076},
  shortjournal = {Image Vis. Comput.},
  title        = {AM-BQA: Enhancing blind image quality assessment using attention retractable features and multi-dimensional learning},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Expression-aware neural radiance fields for high-fidelity
talking portrait synthesis. <em>ICV</em>, <em>147</em>, 105075. (<a
href="https://doi.org/10.1016/j.imavis.2024.105075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) have attracted increasing interest in 3D talking portrait synthesis, which is a crucial problem in the field of digital humans and the metaverse. The synthesis of high-fidelity talking portraits remains a challenging task due to the intricacies of capturing and reproducing subtle facial expressions. In this paper, we propose an innovative approach termed Expression-Aware Neural Radiance Fields (EA-NeRF) for the talking portraits synthesis with remarkable realism and expressiveness . Our method leverages the power of NeRF to model complex scene appearance and illumination, while incorporating expression-awareness to accurately capture and reproduce nuanced facial dynamics. Specifically, we introduce a novel Expression-Aware Module (EAM) that enables our model to seamlessly blend between different facial expressions, yielding convincing and natural transitions during synthesis. Moreover, we present a Local–Global Attention Module (LGAM) that dynamically focuses on salient regions of the face, allowing the model to allocate more resources to areas exhibiting significant expression changes. This attention-guided synthesis process enables our model to generate talking portraits with unparalleled realism and expressiveness, accurately preserving fine-grained details and subtle nuances of facial dynamics. Both qualitative and quantitative experimental results demonstrate the effectiveness of our proposed method in generating talking portraits with superior fidelity and expressiveness compared to existing methods.},
  archive      = {J_ICV},
  author       = {Xueping Wang and Tao Ruan and Jun Xu and Xueni Guo and Jiahe Li and Feihu Yan and Guangzhe Zhao and Caiyong Wang},
  doi          = {10.1016/j.imavis.2024.105075},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105075},
  shortjournal = {Image Vis. Comput.},
  title        = {Expression-aware neural radiance fields for high-fidelity talking portrait synthesis},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hourglass cascaded recurrent stereo matching network.
<em>ICV</em>, <em>147</em>, 105074. (<a
href="https://doi.org/10.1016/j.imavis.2024.105074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching acts a crucial role in computer vision and robotics applications . An accurate cost volume and robust disparity regression method are essential for stereo matching of high accuracy. Following GCNet and PSMNet, constructing 4D cost volume and then using the soft argmin method to regress has been dominated. However, it will encounter many difficulties due to the multi-modal distribution of cost volume. One of the reasons for this multi-modal distribution is the occlusion area which not be possible to find a matching region on the reference image and rarely discussed. In this paper, we propose to use global context information could improve the performance of model in occluded regions. Recently, novel recurrent neural network regression methods are proposed, but most of them regress disparity maps from 3D cost volume. In this paper, we propose the new combinatorial paradigm that combine stacked hourglass modules and recurrent neural networks to further aggregate 4D cost volume and regress disparity respectively. The proposed method can be seamlessly integrated into most stereo matching networks , we improved the accuracy by 45% for PSMNet and 38% for GwcNet in our experiment. Experimental results on Scene Flow, KITTI2012, KITTI2015, and ETH3D datasets show our method is competitive. The code is available at: https://github.com/truman1211/HCRnet .},
  archive      = {J_ICV},
  author       = {Tuming Yuan and Jiancheng Hu and ShuangJiang Ou and Weijia Yang and Yafang Hei},
  doi          = {10.1016/j.imavis.2024.105074},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105074},
  shortjournal = {Image Vis. Comput.},
  title        = {Hourglass cascaded recurrent stereo matching network},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PW-NeRF: Progressive wavelet-mask guided neural radiance
fields view synthesis. <em>ICV</em>, <em>147</em>, 105073. (<a
href="https://doi.org/10.1016/j.imavis.2024.105073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) can achieve state-of-the-art new view results when given a sufficient number of training views. However, NeRF&#39;s rendering is based on minimizing photometric consistency loss, and during the optimization process, it can suffer from overfitting due to factors such as lighting and texture, resulting in poor geometric and color reconstruction. The less data there is in the overlapping areas of the images, the greater the impact on the results, such as insufficient data caused by occlusion relationships. In this paper, we observed through experiments that introducing low-frequency images into NeRF during training can quickly obtain approximate geometric structures, which can guide NeRF to achieve more stable view synthesis results. Therefore, we propose a progressive wavelet mask to assist in the training of neural radiance fields. By first constructing a gaussian pyramid for the training images and then applying wavelet decomposition to them, we can obtain a series of low-frequency region masks to guide the neural radiance field to focus on learning low-frequency pixel regions and gradually introduce high-frequency lighting and texture changes. Our experiments on the LLFF and Blender datasets show that using a progressive wavelet mask in NeRF training can achieve more realistic generation effects with almost no additional computational overhead. At the same time, we also tested our method in sparse scenes, where it still performs well in avoiding overfitting.},
  archive      = {J_ICV},
  author       = {Xuefei Han and Zheng Liu and Hai Nan and Kai Zhao and Dongjie Zhao and Xiaodan Jin},
  doi          = {10.1016/j.imavis.2024.105073},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105073},
  shortjournal = {Image Vis. Comput.},
  title        = {PW-NeRF: Progressive wavelet-mask guided neural radiance fields view synthesis},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Social robot in service of the cognitive therapy of elderly
people: Exploring robot acceptance in a real-world scenario.
<em>ICV</em>, <em>147</em>, 105072. (<a
href="https://doi.org/10.1016/j.imavis.2024.105072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aging is a global demographic trend that is leading to an increase in the prevalence of cognitive disorders. Innovative healthcare solutions are needed to meet the growing demand for assistance. Robots equipped with advanced artificial intelligence, sensors and social interaction capabilities offer promising tools to address the challenges of cognitive decline in the elderly. The main objective of this study is to assess the acceptance of social robots in healthcare among elderly people. Specifically, we provide the social robot Furhat with the ability to greet patients and assess their cognitive function through neuropsychological tests. The experiment involves 26 elderly people interacting with the social robot Furhat in a clinical setting. A questionnaire is administered in order to carry out our results to understand the usability, perceived ease of use, perceived usefulness, intention to use and comfort in interaction. In addition, a group discussion highlights some opinions and feedbacks among the participants. Our results suggest that the integration of social robots into cognitive testing for the elderly has a positive outlook.},
  archive      = {J_ICV},
  author       = {Antonella Cavallaro and Francesca Perillo and Marco Romano and Monica Sebillo and Giuliana Vitiello},
  doi          = {10.1016/j.imavis.2024.105072},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105072},
  shortjournal = {Image Vis. Comput.},
  title        = {Social robot in service of the cognitive therapy of elderly people: Exploring robot acceptance in a real-world scenario},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of objects in satellite and aerial imagery using
channel and spatially attentive YOLO-CSL for surveillance. <em>ICV</em>,
<em>147</em>, 105070. (<a
href="https://doi.org/10.1016/j.imavis.2024.105070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel lightweight Rotational Object Detection algorithm is proposed to overcome the shortcomings of conventional computer-vision-aided object detection methods used in Remote Sensing and Surveillance that overlook the variability in size and orientation of objects in satellite and aerial images . This advanced algorithm integrates a branch dedicated to angle prediction and employs the circular smooth label (CSL) method for angle classification. This approach is suitable for scenarios that require detection in rotational boxes. Our work is further distinguished by the introduction of a novel Channel and Spatial Attention (CSA) module, which is seamlessly integrated into the YOLOv5-CSL framework via the C3CS module. This module accentuates pertinent features through both the channel and spatial attention mechanisms. In addition, bicubic interpolation and the GELU activation function were incorporated into the YOLOv5-CSLA model. Our model achieved 57.86 mAP on the challenging DOTA v2 dataset surpassing the second-best method by 0.20 points and simultaneously consuming 11 million fewer parameters and 103 fewer GFLOPs (our model consumes 25 M Params and 54 GFLOPs), justifying its suitability for deployment on a large majority of platforms, as the compute required is a challenge in real-time deployment scenarios .},
  archive      = {J_ICV},
  author       = {Divyansh Chaurasia and B.D.K. Patro},
  doi          = {10.1016/j.imavis.2024.105070},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105070},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of objects in satellite and aerial imagery using channel and spatially attentive YOLO-CSL for surveillance},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Multimodal parallel attention network for medical image
segmentation. <em>ICV</em>, <em>147</em>, 105069. (<a
href="https://doi.org/10.1016/j.imavis.2024.105069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is a crucial aspect of medical image processing , and has been widely used in the detection and clinical diagnosis for brain, lung, liver, heart and other diseases. In this paper, we propose a novel multimodal parallel attention network , called MPA-Net, for medical image segmentation. MPA-Net is divided into two parts. The first part extracts more high-dimensional features by improved network structure, which contains the skip connection, the output of the multimodal parallel attention and the output of the previous upsampling layer. The second part incorporates a multimodal parallel attention mechanism, encompassing feature parallel attention, spatial parallel attention and channel parallel attention. This mechanism facilitates the effective fusion of high-dimensional and low-dimensional features, leading to enhanced context information. Experimental results on Kagglelung dataset, Liver dataset, Cell dataset, Drive dataset and Kvasir-SEG dataset show that MPA-Net has achieved better segmentation performance than that of other baseline methods , on lung, liver, cell contour, retinal vessel and polyps.},
  archive      = {J_ICV},
  author       = {Zhibing Wang and Wenmin Wang and Nannan Li and Shenyong Zhang and Qi Chen and Zhe Jiang},
  doi          = {10.1016/j.imavis.2024.105069},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105069},
  shortjournal = {Image Vis. Comput.},
  title        = {Multimodal parallel attention network for medical image segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matte anything: Interactive natural image matting with
segment anything model. <em>ICV</em>, <em>147</em>, 105067. (<a
href="https://doi.org/10.1016/j.imavis.2024.105067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural image matting algorithms aim to predict the transparency map (alpha-matte) with the trimap guidance. However, the production of trimap often requires significant labor, which limits the widespread application of matting algorithms on a large scale. To address the issue, we propose Matte Anything model (MatAny), an interactive natural image matting model that could produce high-quality alpha-matte with various simple hints. The key insight of MatAny is to generate pseudo trimap automatically with contour and transparency prediction. In our work, we leverage vision foundation models to enhance the performance of natural image matting. Specifically, we use the segment anything model to predict high-quality contour with user interaction and an open-vocabulary detector to predict the transparency of any object. Subsequently, a pre-trained image matting model generates alpha mattes with pseudo trimaps. MatAny is the interactive matting algorithm with the most supported interaction methods and the best performance to date. It consists of orthogonal vision models without any additional training. We evaluate the performance of MatAny against several previous image matting algorithms. MatAny has 58.3% improvement on MSE and 40.6% improvement on SAD compared to the previous image matting methods with simple guidance, achieving new state-of-the-art (SOTA) performance. The source codes and pre-trained models are available at https://github.com/hustvl/Matte-Anything .},
  archive      = {J_ICV},
  author       = {Jingfeng Yao and Xinggang Wang and Lang Ye and Wenyu Liu},
  doi          = {10.1016/j.imavis.2024.105067},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105067},
  shortjournal = {Image Vis. Comput.},
  title        = {Matte anything: Interactive natural image matting with segment anything model},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IGIE-net: Cross-modality person re-identification via
intermediate modality image generation and discriminative information
enhancement. <em>ICV</em>, <em>147</em>, 105066. (<a
href="https://doi.org/10.1016/j.imavis.2024.105066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an RGB image (or an IR image) of a pedestrian , the task of cross-modality person re-identification aims to retrieve images of a specific pedestrian from an IR (or RGB) gallery. However, the huge modality discrepancy between RGB and IR images significantly affects the performance of cross-modality person re-identification. For the purpose of reducing the impact of modality discrepancy and extracting more discriminative pedestrian features, a new cross-modality person re-identification network is proposed by us, which is based on intermediate modality image generation and discriminative information enhancement (IGIE-Net). Specifically, we design an intermediate modality image generation module (IMIGM) to effectively weaken the impact of modality discrepancy, which first extracts the modality-specific and modality-shared information contained in RGB and IR images separately, and then generates intermediate RGB and intermediate IR images by adaptively fusing original RGB and original IR images with their corresponding modality-specific and modality-shared information separately. In addition, we also design a discriminative information enhancement module (DIEM) to obtain more discriminative pedestrian representations by enhancing the discriminative information contained in the deep features of pedestrians. Extensive experiments on the publicly available SYSU-MM01 and RegDB datasets indicate that the performance of IGIE-Net reaches the current advanced level.},
  archive      = {J_ICV},
  author       = {Jiangtao Guo and Haishun Du and Xinxin Hao and Minghao Zhang},
  doi          = {10.1016/j.imavis.2024.105066},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105066},
  shortjournal = {Image Vis. Comput.},
  title        = {IGIE-net: Cross-modality person re-identification via intermediate modality image generation and discriminative information enhancement},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CABnet: A channel attention dual adversarial balancing
network for multimodal image fusion. <em>ICV</em>, <em>147</em>, 105065.
(<a href="https://doi.org/10.1016/j.imavis.2024.105065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion aims to generate informative images by leveraging the distinctive strengths of infrared and visible modalities. These fused images play a crucial role in subsequent downstream tasks, including object detection, recognition, and segmentation. However, complementary information is often difficult to extract. Existing generative adversarial network-based methods generate fused images by modifying the distribution of source images&#39; features to preserve instances and texture details in both infrared and visible images. Nevertheless, these approaches may result in a degradation of the fused image quality when the original image quality is low. Considering the balance of information from different modalities can improve the quality of the fused image. Hence, we introduce CABnet, a Channel Attention dual adversarial Balancing network. CABnet incorporates a channel attention mechanism to capture crucial channel features, thereby, enhancing complementary information. It also employs an adaptive factor to control the mixing distribution of infrared and visible images, which ensures the preservation of instances and texture details during the adversarial process. To enhance efficiency and reduce reliance on manual labeling, our training process adopts a semi-supervised learning strategy. Through qualitative and quantitative experiments across multiple datasets, CABnet surpasses existing state-of-the-art methods in fusion performance, notably achieving a 51.3% enhancement in signal to noise ratio and a 13.4% improvement in standard deviation.},
  archive      = {J_ICV},
  author       = {Le Sun and Mengqi Tang and Ghulam Muhammad},
  doi          = {10.1016/j.imavis.2024.105065},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105065},
  shortjournal = {Image Vis. Comput.},
  title        = {CABnet: A channel attention dual adversarial balancing network for multimodal image fusion},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acute lymphocytic leukemia detection and subtype
classification via extended wavelet pooling based-CNNs and
statistical-texture features. <em>ICV</em>, <em>147</em>, 105064. (<a
href="https://doi.org/10.1016/j.imavis.2024.105064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lymphoblastic leukemia (ALL) is considered the most fatal form of leukemia (also known as blood cancer). It propagates quickly among adults and children and could lead to their death. Early detection of ALL and ALL subtypes is the key factor in selecting effective treatment types and improving survival rates . However, routine diagnostic approaches have several drawbacks. Computer-assisted diagnosis (CAD) is the perfect solution to avoid these challenges and achieve a fast and accurate diagnosis. Current CAD models require enhancement/segmentation processing. Besides, they are either dependent on deep learning (DL) models or handcrafted features along with machine learning. Those CADs that employed DL approaches relied solely on spatial information during the training procedure. However, learning them with spectral temporal and temporal representations could improve performance. Furthermore, integrating deep features from DL models along with handcrafted features can increase the discrimination ability of attributes in medical image classification . This study aims to propose a novel CAD for ALL detection and subtype classification without pre-segmentation or enhancement steps. The proposed CAD extends the conventional DL models of convolutional neural networks by introducing an additional wavelet pooling, accompanied by a dense layer or a long-short-term memory (LSTM) layer, and then a SoftMax layer, acquiring spectral-temporal information along with temporal information. To further improve the framework&#39;s ability to discriminate, the introduced CAD then combines the wavelet-based deep features of every CNN with numerous handcrafted attributes. Afterward, a feature selection methodology is utilized to create a model with limited features and improved accuracy. The performance results show that the novel CAD is capable of achieving 100% ALL detection accuracy, as well as 100% ALL-subtype classification accuracy with just 88 and 146 features. Thus, this CAD can be employed to assist pathologists in the rapid and precise ALL identification and subcategories recognition.},
  archive      = {J_ICV},
  author       = {Omneya Attallah},
  doi          = {10.1016/j.imavis.2024.105064},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105064},
  shortjournal = {Image Vis. Comput.},
  title        = {Acute lymphocytic leukemia detection and subtype classification via extended wavelet pooling based-CNNs and statistical-texture features},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of tuberculosis using customized MobileNet and
transfer learning from chest x-ray image. <em>ICV</em>, <em>147</em>,
105063. (<a href="https://doi.org/10.1016/j.imavis.2024.105063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most contagious diseases in the world, tuberculosis (TB) is brought on by the bacteria Mycobacterium tuberculosis . This hazardous scenario can cause life losses and requires expert doctors and several hours to detect the disease. Using the MobileNet transfer learning model, a computationally lightweight model has been proposed in this study. The optimal model for the diagnosis of tuberculosis has been determined after testing numerous variants on the base model with pre-trained weights. A computationally light transfer learning model is proposed to obtain the maximum overall accuracy of 98.66%. The improvement over the best existing model is quite significant. The transfer learning model (DenseNet) utilized in this existing model is based on a very complex convolutional neural network (CNN), and as a result, the model requires greater amounts of time. The performance of the other existing models is relatively less in comparison to our proposed model, and the methods have a number of other drawbacks. Our goal in this work is to create a more accurate model that requires less computational effort . When compared to previous models, our model has a very less number of trainable parameters, which causes the model to converge more quickly and predict more accurately. Our approach also has the benefit of being simply able to modify its weights when the system is further updated with new datasets. Additionally, because of its lightweight architecture, it can be installed on mobile devices as well as used in web-based applications with ease. To analyze and validate the proposed method, we have collected data from Kaggle and used MC, CHN and NIH datasets.},
  archive      = {J_ICV},
  author       = {Nirupam Shome and Richik Kashyap and Rabul Hussain Laskar},
  doi          = {10.1016/j.imavis.2024.105063},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105063},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of tuberculosis using customized MobileNet and transfer learning from chest X-ray image},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the influence of attention for whole-image
mammogram classification. <em>ICV</em>, <em>147</em>, 105062. (<a
href="https://doi.org/10.1016/j.imavis.2024.105062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention is an important component of modern Convolutional Neural Networks (CNNs) that has been shown to improve baseline model performance for a wide variety of tasks. Attention has shown specific promise in the classification and segmentation of mammograms, but we have a limited understanding of why attention improves performance in these domains. In this paper, we present a robust comparison of different combinations of baseline models and attention methods at two resolutions for whole mammogram classification of masses and calcifications. We find that attention generally helps to improve baseline model performance. However, the extent of improvement is governed by a combination of model architecture and the statistical characteristics of the data. Specifically, we show that high amounts of pooling and model complexity may result in decreased performance for data with high variability. To better understand the effect of attention on mammogram classification, we used LayerCAM, a hierarchical Class Activation Map (CAM) approach, to visualize where the network pays attention in the input image. This research provides statistical evidence that attention can improve the correlation between model performance and LayerCAM activation in the region of interest (ROI). However, these correlations are weak and variable, indicating that improvements in model performance due to attention are not necessarily caused by increased model activation near the ROI. Overall, our work provides novel insights to help guide future efforts in incorporating attention-based mechanisms for mammogram classification.},
  archive      = {J_ICV},
  author       = {Marc Berghouse and George Bebis and Alireza Tavakkoli},
  doi          = {10.1016/j.imavis.2024.105062},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105062},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring the influence of attention for whole-image mammogram classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based efficient diagnosis of periapical
diseases with dental x-rays. <em>ICV</em>, <em>147</em>, 105061. (<a
href="https://doi.org/10.1016/j.imavis.2024.105061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosis of periapical diseases usually requires a lot of time and effort from the dentist to perform a series of examinations and evaluations, although X-rays have been available to assist in visualizing the condition of the apical region of the tooth. To achieve efficient automated detection and diagnosis of periapical diseases (periapical granuloma, periapical abscess, periapical cysts , and condensing osteitis) by dental X-rays, a deep learning-based approach is proposed, which includes an improved Mask R-CNN-based lesion area segmentation network and a neural network classifier based on six texture features extracted from the segmented lesion regions. The results demonstrate that the average pixel accuracy of lesion area segmentation is over 0.97, while the average Dice Similarity Coefficient for segmentation is over 0.95. The accuracy, precision, recall, and F1 score of this method for the diagnosis of periapical diseases are all above 0.99. Precise segmentation and accurate classification of four distinct types of periapical diseases are achieved using this method, making it possible to create automated systems that can assist in the clinical diagnosis of periapical diseases, potentially streamlining the diagnostic process, reducing the workload of dentists, and improving overall patient care.},
  archive      = {J_ICV},
  author       = {Kaixin Wang and Shengben Zhang and Zhiyuan Wei and Xinle Fang and Feng Liu and Min Han and Mi Du},
  doi          = {10.1016/j.imavis.2024.105061},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105061},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning-based efficient diagnosis of periapical diseases with dental X-rays},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CollaborativeBEV: Collaborative bird eye view for
reconstructing crowded environment. <em>ICV</em>, <em>147</em>, 105060.
(<a href="https://doi.org/10.1016/j.imavis.2024.105060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a virtual world for the Metaverse based on real-world data is crucial, yet creating virtual environments for crowded scenes poses challenges in accurately tracking individuals using egocentric wearable cameras due to occlusions caused by crowded pedestrians . To address this, we propose a collaborative perception strategy that leverages multiple agents equipped with multi-view cameras to construct an occupancy map for a crowded environment. To fuse the multi-view perceptions of multiple agents, we propose a Collaborative Bird Eye View fusion network, called CollaborativeBEV (C-BEV), in which, we leverage a depth-based BEV network as a feature extractor, and propose a feature enhancement module to improve perception fusion in overlapping area. A designed loss function is introduced to address data imbalance during training, and a BEV enhancement strategy is proposed to augment the sample pool for training the BEV decoder. Experiment on the Sean2.0 dataset demonstrates that our C-BEV method performs better than the baseline method in terms of a 5.3 % IoU increase. Our code will be released on github. https://github.com/RYaNzzZ1/CollaborativeBEV .},
  archive      = {J_ICV},
  author       = {Jiaxin Zhao and Fangzhou Mu and Yan Lyu},
  doi          = {10.1016/j.imavis.2024.105060},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105060},
  shortjournal = {Image Vis. Comput.},
  title        = {CollaborativeBEV: Collaborative bird eye view for reconstructing crowded environment},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TQRFormer: Tubelet query recollection transformer for action
detection. <em>ICV</em>, <em>147</em>, 105059. (<a
href="https://doi.org/10.1016/j.imavis.2024.105059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial and temporal action detection aims to precisely locate actions while predicting their respective categories. The existing solution, TubeR (Zhao et al., 2022), is designed to directly detect action tubes in videos by recognizing and localizing actions using a unified representation. However, a potential challenge arises during the decoding stage, leading to a gradual decrease in the model&#39;s performance in action detection, specifically in terms of the confidence associated with detected actions. In this paper, we propose TQRFormer: Tubelet Query Recollection Transformer, enabling the subsequent decoder to obtain information from the previous stage. Specifically, we designed Query Recollection Attention to correct errors and output the synthesized results, effectively breaking the limitations of sequential decoding. During the training stage, TubeR (Zhao et al., 2022) generates a limited number of positive sample queries through a one-to-one matching strategy, potentially impacting the effectiveness of training with positive samples. To enhance the quantity of positive samples, we propose a stage matching approach that combines both one-to-many matching and one-to-one matching without additional queries. This approach serves to boost the overall number of positive samples for improved training outcomes. We also propose a more elegant classification head that contains the start and end frames of the small tubes information, eliminating the necessity for a separate action switch. The performance of TQRFormer is superior to previous state-of-the-art technologies on public action detection datasets, including AVA, UCF101–24, JHMDB-21 and MultiSports. The code will available at https://github.com/ykyk000/TQRFormer .},
  archive      = {J_ICV},
  author       = {Xiangyang Wang and Kun Yang and Qiang Ding and Rui Wang and Jinhua Sun},
  doi          = {10.1016/j.imavis.2024.105059},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105059},
  shortjournal = {Image Vis. Comput.},
  title        = {TQRFormer: Tubelet query recollection transformer for action detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal refinement network: Combining dynamic convolution
and multi-scale information for fine-grained action recognition.
<em>ICV</em>, <em>147</em>, 105058. (<a
href="https://doi.org/10.1016/j.imavis.2024.105058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained action recognition is challenging due to the nearly identical context, limited background information , and less distinct inter-class differences compared to coarse-grained actions.Effectively capturing spatio-temporal information is crucial for fine-grained action recognition models. To address the limitations of coarse-grained models in describing spatio-temporal context, we propose a Temporal Refinement Block (TRB) as an efficient component for fine-grained action recognition. The TRB enables our model to effectively model underlying semantics and global dependencies by generating spatial–temporal kernels of different scales and performing fully connected operations within the temporal dimension. Our experiments demonstrate the effectiveness of TRB in learning latent semantics and global dependencies. To further enhance the framework&#39;s performance, we incorporate an enhanced spatio-temporal pyramidal network (TPN) that collects beat information and utilizes dilated convolutions to boost multi-scale features.We refer to the proposed framework as the Temporal Refinement Network, abbreviated as TRN.Our TRN achieves competitive performance on the FineGym and Diving48 benchmarks.},
  archive      = {J_ICV},
  author       = {Jirui Di and Zhengping Hu and Shuai Bi and Hehao Zhang and Yulu Wang and Zhe Sun},
  doi          = {10.1016/j.imavis.2024.105058},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105058},
  shortjournal = {Image Vis. Comput.},
  title        = {Temporal refinement network: Combining dynamic convolution and multi-scale information for fine-grained action recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ASF-YOLO: A novel YOLO model with attentional scale sequence
fusion for cell instance segmentation. <em>ICV</em>, <em>147</em>,
105057. (<a href="https://doi.org/10.1016/j.imavis.2024.105057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASF-YOLO) which combines spatial and scale features for accurate and fast cell instance segmentation . Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multiscale information extraction capability of the network, and the Triple Feature Encoder (TFE) module to fuse feature maps of different scales to increase detailed information. We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TFE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance . Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods. The source code is available at https://github.com/mkang315/ASF-YOLO .},
  archive      = {J_ICV},
  author       = {Ming Kang and Chee-Ming Ting and Fung Fung Ting and Raphaël C.-W. Phan},
  doi          = {10.1016/j.imavis.2024.105057},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105057},
  shortjournal = {Image Vis. Comput.},
  title        = {ASF-YOLO: A novel YOLO model with attentional scale sequence fusion for cell instance segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data augmentation approach that ensures the reliability of
foregrounds in medical image segmentation. <em>ICV</em>, <em>147</em>,
105056. (<a href="https://doi.org/10.1016/j.imavis.2024.105056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is an important task in medical imaging and diagnosis. Data augmentation can substantially improve the accuracy of medical image segmentation when the dataset has a small amount of medical images. However, the data augmentation methods for medical image are usually based on big models that require extensive search space . Furthermore, excessively complex models often have a heavy burden for the general healthcare organization or researcher. To address this problem, we propose a method of data augmentation that is simple to implement even for the general researcher and simple to transplant across various models. Here we introduce our new methods called KeepMask and KeepMix, which can be simply ported to a variety of models and provide high performance. These methods allow data augmentation without any effect on the target organ or lesion and can also be adapted to multi-class segmentation. KeepMask and KeepMix can not only perturb the background of an existing medical image but also add target organs that are not present to it and generate new images based on the image. In this paper, we performed our methods on both binary class datasets and multi-class datasets and obtained better performance. We conducted numerous experiments showing the predicted segmentation images using our proposed methods obtained more accurate boundaries.},
  archive      = {J_ICV},
  author       = {Xiaoqing Liu and Kenji Ono and Ryoma Bise},
  doi          = {10.1016/j.imavis.2024.105056},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105056},
  shortjournal = {Image Vis. Comput.},
  title        = {A data augmentation approach that ensures the reliability of foregrounds in medical image segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HV-YOLOv8 by HDPconv: Better lightweight detectors for small
object detection. <em>ICV</em>, <em>147</em>, 105052. (<a
href="https://doi.org/10.1016/j.imavis.2024.105052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately identifying and localising small objects within images or videos is a critical challenge in the field of computer vision. It is mostly applied in scenarios that require high real-time performance, such as pedestrian detection and autonomous driving scenarios. These tiny targets generally include small objects at long distances, or objects appearing in low-resolution images, due to which it becomes exceptionally difficult to extract effective feature information. Since YOLOv8 with its large downsampling multiplier leads to deeper feature maps that make it difficult to detect tiny objects, we find that the use of residual structures in the convolution module can enhance the accuracy of small object detection. However, this undoubtedly increases the computational cost, so we lightened the convolution module to make it more suitable for practical applications and named it Halved Deep Pointwise Convolution (HDPConv). A cross-level partial module Variety of View Group Shuffle Cross Stage Partial Network (VOV-GSCSP) is also utilised, using a rational architecture as well as multi-scale information fusion, to ensure that the overall model is lightweight while obtaining rich gradient flows. On this basis, we propose a new network lightweight model HV-YOLOv8. In multiple sets of comparative experiments on two datasets (containing several state-of-the-art solutions as well as classical ones), we demonstrate the superiority of HV-YOLOv8, in particular, the accuracy is improved by 1.4% compared to YOLOv8, while the number of parameters and the amount of computation are drastically reduced.},
  archive      = {J_ICV},
  author       = {Wei Wang and Yuanze Meng and Shun Li and Chenghong Zhang},
  doi          = {10.1016/j.imavis.2024.105052},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105052},
  shortjournal = {Image Vis. Comput.},
  title        = {HV-YOLOv8 by HDPconv: Better lightweight detectors for small object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSRNet: Focusing on critical points for depth completion.
<em>ICV</em>, <em>147</em>, 105051. (<a
href="https://doi.org/10.1016/j.imavis.2024.105051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion is an effective method for generating dense depth maps from sparse ones. In recent studies, the majority of points, which we call standard points, often exhibit sub-optimal performance. This issue arises from the need to fit only very few points, termed as challenging points, which consist of noises and regions with discontinuous depth in the ground-truth. On the other hand, traditional evaluations can not recognize this situation, and are dominated by these limited challenging points, whose performance improvements may not significantly benefit related tasks. In contrast, standard points, which are critical for these tasks, are not effectively measured. This discrepancy highlights the need for a more targeted approach and evaluation method for depth completion. In order to solve the above problems, we propose a standard-point-enhancing learning paradigm. This paradigm aims to improve the performance on standard points, which consists of a Cascaded Segmentation-to-Regression Networks (CSRNet) and a Mining L1 loss. CSRNet includes two branches: DSNet and DRNet. DSNet uses segmentation to generate a coarse depth map, providing challenging-point-insensitive information. DRNet adopts a coarse-to-fine approach to learn the residual depth map between the coarse depth map and the ground-truth depth map. In addition, our Mining L1 loss leverages the segmentation results to filter out potential challenging points. This approach allows the network to concentrate more effectively on standard points. Lastly, we introduce the Minimum Error (ME) Curves as a new way to measure the performance of predicted depth maps in a flexible and comprehensive manner, irrespective of whether the points are standard or challenging. Experimental results on the KITTI and NYUDv2 datasets show that our approach significantly improves accuracy on the majority of points.},
  archive      = {J_ICV},
  author       = {Bocen Li and Yifan Wang and Lijun Wang and Huchuan Lu},
  doi          = {10.1016/j.imavis.2024.105051},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105051},
  shortjournal = {Image Vis. Comput.},
  title        = {CSRNet: Focusing on critical points for depth completion},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). G-TRACE: Grouped temporal recalibration for video object
segmentation. <em>ICV</em>, <em>147</em>, 105050. (<a
href="https://doi.org/10.1016/j.imavis.2024.105050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Semi-supervised Video Object Segmentation (SVOS), there is a critical emphasis on enhancing the memory and readout mechanisms for frame matching, especially in relation to temporal dynamics. Current methods predominantly use 2D CNNs for encoding video frames, which unfortunately neglects the crucial aspect of addressing temporal variations in individual frames and their associated masks during the encoding process. One potential solution would be to implement temporal models such as 3D CNNs instead of 2D CNNs , but this significantly increases computational requirements, making it impractical for real-world SVOS applications. In this paper, we introduce the Grouped Temporal Recalibration with Attention for Convolutional Encoders (G-TRACE), a novel plug-and-play module that is compatible with various existing SVOS frameworks. G-TRACE uses hierarchical memory-centric attention and integrates effortlessly with 2D CNNs , offering a novel approach to temporal modeling that operates orthogonally to traditional frame matching methods. Extensive evaluations on four widely-used benchmarks demonstrate that our method consistently delivers significant performance improvements over various baseline models .},
  archive      = {J_ICV},
  author       = {Jiyun Kim and JooHo Kim and Sungeun Hong},
  doi          = {10.1016/j.imavis.2024.105050},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105050},
  shortjournal = {Image Vis. Comput.},
  title        = {G-TRACE: Grouped temporal recalibration for video object segmentation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VAE-GAN3D: Leveraging image-based semantics for 3D zero-shot
recognition. <em>ICV</em>, <em>147</em>, 105049. (<a
href="https://doi.org/10.1016/j.imavis.2024.105049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state of 3D zero-shot recognition falls short in performance when compared to its counterpart in 2D images. A major challenge is the absence of a robust feature extractor for 3D point cloud data. To overcome this challenge, a large dataset needs to be collected, processed, and fed into a deep-learning model capable of generating distinguishable features after training. To this end, we propose VAE-GAN3D, a model that uses a combination of a variational autoencoder (VAE) and a generative adversarial network (GAN) to supplement the small amount of data available for training with synthetic examples of classes. This allows for the underlying data distribution of large and complex data to be learned by the deep VAE network. When combined with a GAN, this network generates synthetic features that exhibit consistency in both seen and unseen classes. Furthermore, we notice that for tasks involving a small domain of classes, the existing text features do not contribute significantly to zero-shot learning. Therefore, we introduce image representation-based semantic features of classes, which improve the performance of zero-shot recognition for 3D objects. We assess the proposed model using three different datasets and present a technique for partitioning the RGB-D object dataset , which contains real-world objects, into seen and unseen classes. Our approach shows promising results in addressing the challenges of 3D zero-shot recognition and presents a novel solution for improving the accuracy of 3D point cloud recognition.},
  archive      = {J_ICV},
  author       = {Md Tahmeed Abdullah and Sejuti Rahman and Shafin Rahman and Md Fokhrul Islam},
  doi          = {10.1016/j.imavis.2024.105049},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105049},
  shortjournal = {Image Vis. Comput.},
  title        = {VAE-GAN3D: Leveraging image-based semantics for 3D zero-shot recognition},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BMFNet: Bifurcated multi-modal fusion network for RGB-d
salient object detection. <em>ICV</em>, <em>147</em>, 105048. (<a
href="https://doi.org/10.1016/j.imavis.2024.105048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning-based RGB-D salient object detection methods have achieved impressive results in the recent years, there are still some issues need to be addressed including multi-modal fusion and multi-level aggregation. In this paper, we propose a bifurcated multi-modal fusion network (BMFNet) to address these two issues cooperatively. First, we design a multi-modal feature interaction (MFI) module to fully capture the complementary information between the RGB and depth features by leveraging the channel attention and spatial attention . Second, unlike the widely used layer-by-layer progressive fusion, we adopt a bifurcated fusion strategy for all the multi-level unimodal and cross-modal features to effectively reduce the gaps between features at different levels. For the intra-group feature aggregation, a multi-modal feature fusion (MFF) module is designed to integrate the intra-group multi-modal features to produce a low-level/high-level saliency feature. For the inter-group aggregation, a multi-scale feature learning (MFL) module is introduced to exploit the contextual interactions between different scales to boost fusion performance. Experimental results on five public RGB-D datasets demonstrate the effectiveness and superiority of our proposed network. The code and prediction maps will be available at https://github.com/ZhangQing0329/BMFNet},
  archive      = {J_ICV},
  author       = {Chenwang Sun and Qing Zhang and Chenyu Zhuang and Mingqian Zhang},
  doi          = {10.1016/j.imavis.2024.105048},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105048},
  shortjournal = {Image Vis. Comput.},
  title        = {BMFNet: Bifurcated multi-modal fusion network for RGB-D salient object detection},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature alignment via mutual mapping for few-shot
fine-grained visual classification. <em>ICV</em>, <em>147</em>, 105032.
(<a href="https://doi.org/10.1016/j.imavis.2024.105032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot fine-grained visual classification aims to identify fine-grained concepts with very few samples, which is widely used in many fields, such as the classification of different species of birds in biological research, and the identification of car models in traffic monitoring . Compared with the common few-shot classification task , it encounters difficulties due to significant variations within each class and small gaps between different categories. To address such problems, previous studies primarily project support samples into the space of query samples and employ metric learning to classify query images into their respective categories. However, we observe that such methods are not effective in resolving inter-class variations. To overcome this limitation, we propose a new feature alignment method based on mutual mapping, which simultaneously considers the discriminative features of new samples and classes. Specifically, besides projecting support samples into the space of query samples for reducing intra-class variations, we also project query samples into the space of support samples to increase inter-class variations. Furthermore, a direct position self-reconstruction module is proposed to utilize the location information of objects and obtain more discriminative features. Extensive experiments on four fine-grained benchmarks demonstrate that our approach is competitive when compared with other state-of-the-art methods, in both 1-shot and 5-shot settings. In the case of 5-shot, our method achieved the best performance on all four datasets, with 92.11%, 85.31%, 96.09%, and 94.64% accuracies on CUB-200-2011, Stanford Dogs, Stanford Cars, and Aircaft, respectively.},
  archive      = {J_ICV},
  author       = {Qin Wu and Tingting Song and Shengnan Fan and Zeda Chen and Kelei Jin and Haojie Zhou},
  doi          = {10.1016/j.imavis.2024.105032},
  journal      = {Image and Vision Computing},
  month        = {7},
  pages        = {105032},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature alignment via mutual mapping for few-shot fine-grained visual classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GLIMS: Attention-guided lightweight multi-scale hybrid
network for volumetric semantic segmentation. <em>ICV</em>,
<em>146</em>, 105055. (<a
href="https://doi.org/10.1016/j.imavis.2024.105055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have become widely adopted for medical image segmentation tasks , demonstrating promising performance. However, the inherent inductive biases in convolutional architectures limit their ability to model long-range dependencies and spatial correlations . While recent transformer-based architectures address these limitations by leveraging self-attention mechanisms to encode long-range dependencies and learn expressive representations, they often struggle to extract low-level features and are highly dependent on data availability. This motivated us for the development of GLIMS, a data-efficient attention-guided hybrid volumetric segmentation network . GLIMS utilizes Dilated Feature Aggregator Convolutional Blocks (DACB) to capture local–global feature correlations efficiently. Furthermore, the incorporated Swin Transformer-based bottleneck bridges the local and global features to improve the robustness of the model. Additionally, GLIMS employs an attention-guided segmentation approach through Channel and Spatial-Wise Attention Blocks (CSAB) to localize expressive features for fine-grained border segmentation. Quantitative and qualitative results on glioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS’ effectiveness in terms of complexity and accuracy. GLIMS demonstrated outstanding performance on BraTS2021 and BTCV datasets, surpassing the performance of Swin UNETR. Notably, GLIMS achieved this high performance with a significantly reduced number of trainable parameters. Specifically, GLIMS has 47.16 M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98 M trainable parameters and 394.84G FLOPs. The code is publicly available at https://github.com/yaziciz/GLIMS .},
  archive      = {J_ICV},
  author       = {Ziya Ata Yazıcı and İlkay Öksüz and Hazım Kemal Ekenel},
  doi          = {10.1016/j.imavis.2024.105055},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105055},
  shortjournal = {Image Vis. Comput.},
  title        = {GLIMS: Attention-guided lightweight multi-scale hybrid network for volumetric semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Starting from the structure: A review of small object
detection based on deep learning. <em>ICV</em>, <em>146</em>, 105054.
(<a href="https://doi.org/10.1016/j.imavis.2024.105054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection, as one of the most fundamental and essential tasks in the field of computer vision , has been the focus of unremitting efforts by researchers, who are committed to modifying the neural network structure in order to improve the accuracy of object detection and expedite task execution. As the application scope continues to expand, small object detection has gradually emerged as a crucial branch in the field of object detection. In this paper, the development history of object detection algorithms is introduced, the concept of small objects is introduced, and the current problems and challenges faced by small object detection are outlined. In this paper, the network structure is disassembled from a macroscopic point of view, and improved algorithms such as enhanced data augmentation , improved feature extraction, superior feature fusion , and refined loss functions are described in detail. Furthermore, the paper explores a series of emerging and improved algorithms for small object detection. It encompasses the introduction of advanced strategies such as unsupervised learning , end-to-end training, density cropping, transfer learning , and anchor-free approaches. The paper provides a comprehensive list of commonly used general-purpose datasets and domain-specific datasets for small object detection tasks, offering performance comparisons of the mentioned improved algorithms. In conclusion, the paper summarizes and provides an outlook on current small object detection algorithms , furnishing the reader with a thorough understanding of the field and insights into future directions.},
  archive      = {J_ICV},
  author       = {Zheng Xiuling and Wang Huijuan and Shang Yu and Chen Gang and Zou Suhua and Yuan Quanbo},
  doi          = {10.1016/j.imavis.2024.105054},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105054},
  shortjournal = {Image Vis. Comput.},
  title        = {Starting from the structure: A review of small object detection based on deep learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight convolutional neural networks with context
broadcast transformer for real-time semantic segmentation. <em>ICV</em>,
<em>146</em>, 105053. (<a
href="https://doi.org/10.1016/j.imavis.2024.105053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing application of embedded mobile devices in various fields, lightweight real-time semantic segmentation systems have attracted more and more attention. Many current methods have successfully reduced the model&#39;s parameters, but they have led to low model accuracy, diminishing their practical value. In recent years, the Transformer architecture has achieved good results in many tasks, effectively capturing long-range dependencies and enhancing accuracy. However, the Transformer is not adept at extracting local features , and the model&#39;s computational cost is generally too high, hindering real-time inference implementation. We propose a lightweight semantic segmentation network called LCBFormer-Net, which embeds Transformer units between asymmetric encoders and decoders to fully leverage their advantages. On the encoder side , we design the Lightweight Multi-Fusion Unit (LMFU) and Partition Grouping Shuffle Channel Attention (PGSCA). The former fully utilizes input features, merging information multiple times through multiple branches and employing depthwise convolutions with dilation rate to further obtain sufficient features. The latter includes a lightweight grouped channel attention, better guide feature extraction. The Lightweight Context Broadcast Transformer (LCB Transformer) is the Transformer unit we designed, with a lightweight structure that significantly reduces GPU memory consumption. It also improves self-attention and feed-forward networks, enhancing the model&#39;s robustness. The decoder includes the Multi-scale Semantic Information Attention Fusion (MSIAF) module, guiding the fusion of features at three different scales and employing a hybrid attention mechanism with both channel and spatial attention to guide feature extraction. LCBFormer-Net achieves good segmentation results with a parameter count of 0.88 M on multiple challenging datasets with diverse scenes.},
  archive      = {J_ICV},
  author       = {Kaidi Hu and Zongxia Xie and Qinghua Hu},
  doi          = {10.1016/j.imavis.2024.105053},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105053},
  shortjournal = {Image Vis. Comput.},
  title        = {Lightweight convolutional neural networks with context broadcast transformer for real-time semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three in one bottom-up framework for simultaneous semantic
segmentation, instance segmentation and classification of multi-organ
nuclei in digital cancer histology. <em>ICV</em>, <em>146</em>, 105047.
(<a href="https://doi.org/10.1016/j.imavis.2024.105047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous segmentation and classification of nuclei in digital histology remains challenging. The highest achieved Panoptic Quality (PQ) remains low due to overlapping nuclei, higher staining and tissue variability, and rough clinical conditions. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. We resolve these issues using a dual attention-based model combined with post-processing in a bottom-up fashion. We use three attention decoder heads, which produce semantic segmentation , edge proposals, and classification maps. We use these outputs to apply post-processing, including controlled watershed and pixel grouping, to produce instance segmentation and classification. Our multi-stage approach utilizes edge proposals and semantic segmentations compared to direct segmentation and classification strategies followed by most generic state-of-the-art methods. Due to this, we demonstrate a significant performance improvement in producing high-quality instance segmentation and nuclei classification. We have achieved a 0.841 Dice score for semantic segmentation, 0.713 bPQ scores for instance segmentation, and 0.633 mPQ for nuclei classification. Furthermore, the framework is less complex compared to the state-of-the-art.},
  archive      = {J_ICV},
  author       = {Ibtihaj Ahmad and Syed Muhammad Israr and Zain Ul Islam},
  doi          = {10.1016/j.imavis.2024.105047},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105047},
  shortjournal = {Image Vis. Comput.},
  title        = {A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-invasive coronary artery disease identification through
the iris and bio-demographic health profile features using stacking
learning. <em>ICV</em>, <em>146</em>, 105046. (<a
href="https://doi.org/10.1016/j.imavis.2024.105046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a non-invasive method for predicting Coronary Artery Disease (CAD) using iris analysis, patient data, and Machine Learning (ML), primarily with iris images . It involved 281 participants, comprising 155 CAD patients and 126 non-patient controls, with eye images and biodemographic data collected at a Cardiology outpatient clinic. The study explored three scenarios: Scenario-I focused on biodemographic data, Scenario-II on iris features, and Scenario-III combined iris images and data. Iris processing included location determination, normalization, and heart region selection, with image enhancement via adaptive histogram equalization . Feature extraction through a 2-level wavelet transform generated 272 attributes, including statistical, Gray Level Co-occurrence Matrix, and Gray Level Run Length Matrix features for eight subcomponents. Correlation-based selection identified the best features, and classification employed ML techniques and incorporated stacking learning to enhance the results. Scenario-I achieved the highest accuracy at 83.57% among all evaluated algorithms. In Scenario-II, the proposed algorithm consistently outperformed others, achieving 94.88% accuracy and strong performance in other metrics, highlighting its effectiveness. In Scenario-III, the algorithm maintained superiority with 96.07% accuracy, specificity, recall, and area under the curve values. The proposed algorithm consistently outperforms other methods across scenarios, indicating its potential for CAD diagnosis, making it a promising choice for future CAD systems. The proposed algorithm presents a novel approach to the preliminary diagnosis of CAD, eliminating the necessity for electrocardiography , echocardiography , or effort tests . It also enables seamless integration into telemedicine systems, allowing for tele-diagnosis to conduct preliminary assessments before routine clinical practice.},
  archive      = {J_ICV},
  author       = {Ferdi Özbilgin and Çetin Kurnaz and Ertan Aydın},
  doi          = {10.1016/j.imavis.2024.105046},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105046},
  shortjournal = {Image Vis. Comput.},
  title        = {Non-invasive coronary artery disease identification through the iris and bio-demographic health profile features using stacking learning},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal fusion of features from decomposed ultrasound RF
data with adaptive weighted ensemble classifier to improve breast lesion
classification. <em>ICV</em>, <em>146</em>, 105045. (<a
href="https://doi.org/10.1016/j.imavis.2024.105045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis plays a crucial role in successful treatment of breast tumors and reduced mortality. In this study, considering the complementary advantages between features and combined with the improvement of classifiers, fusion of optimal complexity and texture features from decomposed ultrasound radio frequency (RF) data is proposed to improve breast lesion classification performance. In this method, three complexity features and four texture features were extracted from the ring regions of interest for breast lesions in all decomposed RF sub-images and their combinations. Selection techniques based on analysis of feature relevance, redundancy, and interaction (FRRI) were used to determine the optimal feature sets (FS–FRRI). Finally, three classifiers with the best performance (weighted k-nearest neighbor, bagged tree , Gaussian Naive Bayes (NB)) were selected based on FS-FRRI. The three classifiers were integrated using the bagging method, and each classifier was adaptively weighted according to the genetic algorithm during the integration to classify breast lesions. The proposed method was evaluated using the Open Access Series of Breast Ultrasonic Data, with 10-fold cross-validation. The experimental results demonstrated that optimal performance was obtained by the FS–FRRI-based adaptive weighted ensemble classifier, with an accuracy of 97%, a sensitivity of 99%, a specificity of 96%, and an area under the receiver operating curve value of 0.97. Fusion of optimal complexity and texture features from decomposed ultrasound RF data with an adaptive weighted ensemble classifier can help improve breast lesion classification performance, which has great potential to assist clinicians in accurate diagnosis of breast lesions.},
  archive      = {J_ICV},
  author       = {Ruihan Yao and Bingbing He and Yufeng Zhang and Zhiyao Li and Jingying Zhu and Xun Lang},
  doi          = {10.1016/j.imavis.2024.105045},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105045},
  shortjournal = {Image Vis. Comput.},
  title        = {Optimal fusion of features from decomposed ultrasound RF data with adaptive weighted ensemble classifier to improve breast lesion classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning with adaptive convolutions for classification
of retinal diseases via optical coherence tomography. <em>ICV</em>,
<em>146</em>, 105044. (<a
href="https://doi.org/10.1016/j.imavis.2024.105044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography (OCT) uses interferometry to capture high-resolution cross-sectional images of the retina to diagnose retinal diseases . Convolutional neural networks (CNNs) have become essential for developing efficient computer-aided diagnostic algorithms , but noisy images can hinder their performance. This study introduces an innovative image preprocessing strategy that involves a new method of representing images to reduce image noise and a new adaptive convolution layer. The adaptive convolution layer aims to replace traditional convolution layers for OCT image classification by relying on local’Feature Content’. The proposed image representation is based on Zeckendorf&#39;s theorem, which states that every positive integer may be split into a unique sum of distinct, non-adjacent Fibonacci numbers. The proposed approach enables the generation of two separate images, known as the ‘base’ and ‘fine,’ where the ‘base’ image is the denoised image . We assessed our methodology by evaluating against ten filters, comprising a Low-pass filter, Gaussian filter, Wiener filter, Wavelet filter, Guided filter, Lee filter, Frost filter, Kuan filter, Detail Preserving Anisotropic Diffusion (DPAD) filter, and Non-Local Means (NLM) filter. In our study, we found that the proposed filter produced the most favorable results in five of the six no-reference parameters (Blur Percent (BP), Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE), Naturalness Image Quality Evaluator (NIQE), Wavelet Variance (WAVV), Wavelet Variance Radial (WAVR)) used to assess the effectiveness of the proposed image enhancement technique. The OCT dataset utilized in the study was compiled by the University of California San Diego. The proposed adaptive convolution layer and its accompanying activation function were tested using seven OCT image classification CNN architectures . The test architectures comprise OctNET, NT-CNN, AOCT-NET, M-CNN, LightOCT, RetiNet, and DeepOCT. Experiments were conducted to assess the impact of the new preprocessing algorithm and the placement of the adaptive convolution layer as a substitute for the conventional convolution layer. Implementing the proposed approaches resulted in accuracy improvements ranging from 0.44% to 2.44% across architectures. Our findings highlight the efficacy of the proposed indirect noise reduction technique and a texture-sensitive adaptive convolution layer.},
  archive      = {J_ICV},
  author       = {Karri Karthik and Manjunatha Mahadevappa},
  doi          = {10.1016/j.imavis.2024.105044},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105044},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning with adaptive convolutions for classification of retinal diseases via optical coherence tomography},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMOVA: Emotion-driven neural volumetric avatar.
<em>ICV</em>, <em>146</em>, 105043. (<a
href="https://doi.org/10.1016/j.imavis.2024.105043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D facial reconstruction is essential for metaverse applications. Traditional mesh-based methods have difficulty rendering photorealistic faces and complex objects. Recent advancements in Neural Radiance Fields (NeRFs) have excelled in representing complex objects. However, they struggle with capturing subtle facial differences, particularly around the eyes and mouth, due to their reliance on RGB value comparisons. To address this, we propose an EMOtion-driven Volumetric Avatar (EMOVA) that utilizes emotional stimuli from images and voices to enhance facial precision. Visual emotional features ensure that the reconstructed face aligns with the input emotion, while auditory features enhance facial details from various viewpoints. Through an attention-based fusion of these features, EMOVA accurately captures and reconstructs faces, even in self-occlusion. EMOVA outperforms the state-of-the-art methods by more than 5.93% in terms of LPIPS for face reconstruction.},
  archive      = {J_ICV},
  author       = {Juheon Hwang and Byung-gyu Kim and Taewan Kim and Heeseok Oh and Jiwoo Kang},
  doi          = {10.1016/j.imavis.2024.105043},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105043},
  shortjournal = {Image Vis. Comput.},
  title        = {EMOVA: Emotion-driven neural volumetric avatar},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AES-net: An adapter and enhanced self-attention guided
network for multi-stage glaucoma classification using fundus images.
<em>ICV</em>, <em>146</em>, 105042. (<a
href="https://doi.org/10.1016/j.imavis.2024.105042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is a progressive eye condition that can lead to permanent vision loss. Therefore, on-time detection of glaucoma is critical for making an effective treatment plan. In recent years, enormous attempts have been made to develop automated glaucoma classification systems using CNNs through images. In contrast, limited methods have been proposed for diagnosing different glaucoma stages. It is mainly owing to the lack of large publicly available labeled datasets. Also, fundus images exhibit a high inter-stage resemblance, redundant features and minute size variations of lesions, making the conventional CNNs difficult to classify multiple stages of glaucoma accurately. To address these challenges, this paper proposes a novel adapter and enhanced self-attention based CNN framework named AES-Net for effective classification of glaucoma stages. In particular, we propose a spatial adapter module on top of the backbone network for learning better feature representations and an enhanced self-attention module (ESAM) to capture global feature correlations among the relevant channels and spatial positions . The ESAM assists in capturing stage-specific and detailed-lesion features from the fundus images. Extensive experiments on two multi-stage glaucoma datasets indicate that our AES-Net surpasses CNN-based existing approaches. The Grad-CAM ++ visualization maps further confirm the effectiveness of our AES-Net.},
  archive      = {J_ICV},
  author       = {Dipankar Das and Deepak Ranjan Nayak and Ram Bilas Pachori},
  doi          = {10.1016/j.imavis.2024.105042},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105042},
  shortjournal = {Image Vis. Comput.},
  title        = {AES-net: An adapter and enhanced self-attention guided network for multi-stage glaucoma classification using fundus images},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous heatmap distillation framework based on
unbiased alignment for lightweight human pose estimation. <em>ICV</em>,
<em>146</em>, 105041. (<a
href="https://doi.org/10.1016/j.imavis.2024.105041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing demand for mobile devices has generated interest in lightweight human pose estimation. Currently, lightweight estimation generally uses heatmap-based methods, which has demonstrated exceptional performance. However, their use of non-differentiable post-processing imposes considerable inference latencies. Conversely, integral-based approaches expedite the inference process by employing a soft-argmax operation but compromise in accuracy. Integrating explicit heatmap knowledge learned using the heatmap-based method into the implicit heatmap generated by the integral-based method, thereby combining the best of both worlds, offers a promising avenue. However, owing to the disparities in supervision and inference processes, the explicit and implicit heatmaps are heterogeneous. Consequently, direct transfer of knowledge presents difficulties in ensuring consistencies in heat value and location. In this paper, we propose a novel Heterogeneous Heatmap Distillation (HHD) framework that effectively tackles these challenges. The framework seamlessly integrates explicit heatmap knowledge that contains high-precision localization information into implicit heatmaps. The framework revolves around an unbiased heatmap alignment scheme encompassing two steps: heterogeneous heatmap normalization and unbiased cropping. Heterogeneous heatmap normalization separately normalizes the output feature maps of both the teacher and student models, alleviating potential heat value bias during the knowledge transfer. Unbiased cropping applies closed-form computation on the normalized teacher and student heatmap to eliminate location bias. Additionally, mirror expansion is implemented to handle potential cases wherein the cropped region extends beyond the image boundary. Extensive experiments demonstrate the efficiency and effectiveness of our methods on the MSCOCO and MPII datasets compared to other integral-based lightweight networks. Our source codes and pre-trained models are available at https://github.com/ducongju/HHD .},
  archive      = {J_ICV},
  author       = {Congju Du and Zhenyu Li and Huijuan Zhao and Shuangjiang He and Li Yu},
  doi          = {10.1016/j.imavis.2024.105041},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105041},
  shortjournal = {Image Vis. Comput.},
  title        = {Heterogeneous heatmap distillation framework based on unbiased alignment for lightweight human pose estimation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring modality enhancement and compensation spaces for
visible-infrared person re-identification. <em>ICV</em>, <em>146</em>,
105040. (<a href="https://doi.org/10.1016/j.imavis.2024.105040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a challenging task in computer vision due to the substantial modality gaps between visible and infrared images. The currently existing approaches can improve performance by addressing cross-modality discrepancies, but they often fail to generate compensation features that fully utilize the unique information present in each modality. Additionally, these methods mainly focus on pixel-level fusion of images, disregarding the challenge of modality misalignment. To address these issues, we propose a novel visible-infrared person re-identification method that explores modality enhancement and compensation spaces to extract more discriminative modality information. Furthermore, we introduce a modality mutual guidance strategy incorporating identity information mutual learning loss and modality-guided alignment loss, which can effectively leverage learned identity-related feature to guide alignment between visible and infrared modalities. Extensive experiments on public datasets demonstrate the significant superiority of our proposed method over existing state-of-the-art approaches.},
  archive      = {J_ICV},
  author       = {Xu Cheng and Shuya Deng and Hao Yu},
  doi          = {10.1016/j.imavis.2024.105040},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105040},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploring modality enhancement and compensation spaces for visible-infrared person re-identification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwinSOD: Salient object detection using swin-transformer.
<em>ICV</em>, <em>146</em>, 105039. (<a
href="https://doi.org/10.1016/j.imavis.2024.105039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Transformer structure has achieved excellent performance in a wide range of applications in computer vision , and Swin-Transformer also shows strong feature representation capabilities. On this basis, we proposed a fusion model SwinSOD for RGB salient object detection . This model used a Swin-Transformer as the encoder to extract hierarchical features, was driven by a multi-head attention mechanism to bridge the gap between hierarchical features, progressively fused adjacent layer feature information under the guidance of global information, and refined the boundaries of saliency objects through the feedback information. Specifically, the Swin-Transformer encoder extracted multi-level features and then recalibrated the channels to optimize intra-layer channel features. The feature fusion module realized feature fusion between each layer under the guidance of global information. In order to clarify the fuzzy boundaries, the second stage feature fusion achieved edge refinement under the guidance of feedback information. The proposed model outperforms state-of-the-art models on five popular SOD datasets, demonstrating the advanced performance of this network. Code released: https://github.com/user-wu/SwinSOD .},
  archive      = {J_ICV},
  author       = {Shuang Wu and Guangjian Zhang and Xuefeng Liu},
  doi          = {10.1016/j.imavis.2024.105039},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105039},
  shortjournal = {Image Vis. Comput.},
  title        = {SwinSOD: Salient object detection using swin-transformer},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attribute discrimination combined with selected sample
dropout for unsupervised domain adaptive person re-identification.
<em>ICV</em>, <em>146</em>, 105038. (<a
href="https://doi.org/10.1016/j.imavis.2024.105038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popular clustering-based Unsupervised Domain Adaptive (UDA) person re-identification (re-ID) does not require additional annotation. However, owing to unsatisfactory feature embedding and imperfect clustering, most existing clustering-based methods suffer from the noise of pseudo labels in the target domain, which will lead to a serious performance degradation . To reduce the negative impact of noisy pseudo labels on training, we put forward an approach named selected sample dropout (SSD) in the training stage, which defines a criterion for evaluating the noise level of pseudo labels. SSD would mine and discard samples with noisy pseudo labels before training, and then all the remaining samples are fed into the network to train. On top of a strong baseline, SSD is proved to be effective. We call the baseline of adding SSD as the selected sample dropout-enhanced teacher-student network (SSD-TSNet). In addition, considering the robustness of pedestrian gender, we use it as auxiliary information in the SSD-TSNet test stage. Specifically, we propose a pedestrian gender attribute discriminator (GAD) to predict gender labels. Based on predictive gender labels, SSD-TSNet could retrieve one person among other persons with the same gender as the person, thus narrowing the search space of re-ID. The proposed SSD-TSNet and GAD are integrated into one framework, and extensive experiments on four widely used UDA benchmark protocols demonstrate its competitive performance. Specifically, our method outperforms the baseline by 11.6% mAP on the Duke-to-Market task, while surpassing the state-of-the-art method by 0.5% mAP on the Market-to-Duke task.},
  archive      = {J_ICV},
  author       = {Chang Liu and Haoqi Li and Xuan He and Guanzhong Yang and Zhiyong Li},
  doi          = {10.1016/j.imavis.2024.105038},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105038},
  shortjournal = {Image Vis. Comput.},
  title        = {Attribute discrimination combined with selected sample dropout for unsupervised domain adaptive person re-identification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image recognition based on lightweight convolutional neural
network: Recent advances. <em>ICV</em>, <em>146</em>, 105037. (<a
href="https://doi.org/10.1016/j.imavis.2024.105037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image recognition is an important task in computer vision with broad applications. In recent years, with the advent of deep learning , lightweight convolutional neural network (CNN) has brought new opportunities for image recognition, which allows high-performance recognition algorithms to run on resource-constrained devices with strong representation and generalization capabilities. This paper first presents an overview of several classical lightweight CNN models . Then, a comprehensive review is provided on recent image recognition techniques using lightweight CNN. According to the strategies applied to optimize image recognition performance, existing methods are classified into three categories: (1) model compression , (2) optimization of lightweight network, and (3) combining Transformer with lightweight network. In addition, some representative methods are tested on three commonly used datasets for performance comparison. Finally, technical challenges and future research trends in this field are discussed.},
  archive      = {J_ICV},
  author       = {Ying Liu and Jiahao Xue and Daxiang Li and Weidong Zhang and Tuan Kiang Chiew and Zhijie Xu},
  doi          = {10.1016/j.imavis.2024.105037},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105037},
  shortjournal = {Image Vis. Comput.},
  title        = {Image recognition based on lightweight convolutional neural network: Recent advances},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Moment preserving tomographic image reconstruction model.
<em>ICV</em>, <em>146</em>, 105036. (<a
href="https://doi.org/10.1016/j.imavis.2024.105036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape descriptors provide valuable prior information in many tomographic image reconstruction methods. Such descriptors include, among others, centroid , circularity , orientation, and elongation. Shape descriptor measures are often analytically expressed as a composition of certain geometric moments. Building upon this fact, this paper suggests preserving the values of a specific geometric moment in the reconstruction process, instead of preserving entire descriptors, as it has been suggested so far. Reconstructions from two natural projection directions (vertical and horizontal) are considered with special attention. The provided theoretical analysis demonstrates that preserving the value of a specific geometric moment, provided as prior information for the reconstruction process, simultaneously ensures the preservation of the true measures of all four abovementioned descriptors. Based on this result, a novel regularized energy minimization reconstruction model is proposed. The minimization task of the new model is solved using gradient-based optimization algorithm. Performance evaluation of the proposed method is supported by experimental results obtained through comparisons with other well-known reconstruction methods.},
  archive      = {J_ICV},
  author       = {Tibor Lukić and Péter Balázs},
  doi          = {10.1016/j.imavis.2024.105036},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105036},
  shortjournal = {Image Vis. Comput.},
  title        = {Moment preserving tomographic image reconstruction model},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Localization-aware logit mimicking for object detection in
adverse weather conditions. <em>ICV</em>, <em>146</em>, 105035. (<a
href="https://doi.org/10.1016/j.imavis.2024.105035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adverse weather conditions would decrease the image quality , leading to a sharp decline in detection accuracy. Most of the researches focus on object detection in fine weather conditions, rather than in adverse weather conditions. Recently, some methods attempt to reduce the gap between degraded images and clean images to improve the detection accuracy in adverse weather conditions. Specifically, these methods usually conduct image restoration and object detection in a sequential way or by joint learning. While these methods can improve detection accuracy to a certain extent, image restoration models may introduce noise or artifacts and increase computational burden, limiting the accuracy and efficiency of object detection in adverse weather conditions. In this paper, we propose a knowledge distillation-based method, Localization-aware Logit Mimicking (LaLM), to improve detection accuracy in adverse weather conditions by reducing the gap between degraded images and clean images at the prediction level, rather than the image level. Moreover, the localization quality is designed as the mimicking target to make the knowledge distillation more effective. Experiments conducted on three popular benchmarks (i.e., RTTS, ExDark, and RID) demonstrate that our LaLM can achieve the state-of-the-art detection accuracy and inference speed in foggy, rainy, and low-light conditions. Code is available at: https://github.com/VIPLab-CQU/LaLM .},
  archive      = {J_ICV},
  author       = {Peiyun Luo and Jing Nie and Jin Xie and Jiale Cao and Xiaohong Zhang},
  doi          = {10.1016/j.imavis.2024.105035},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105035},
  shortjournal = {Image Vis. Comput.},
  title        = {Localization-aware logit mimicking for object detection in adverse weather conditions},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transductive semantic decoupling double variational
inference for few-shot classification. <em>ICV</em>, <em>146</em>,
105034. (<a href="https://doi.org/10.1016/j.imavis.2024.105034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, within the rapidly evolving landscape of deep learning technology, few-shot learning, particularly in few-shot classification, has emerged as an enticing frontier. Despite the notable achievements of deep learning in handling extensive datasets, the task of image classification remains highly demanding when faced with a limited number of annotated samples. To address this challenge, we introduce the Transductive Semantic Decoupling Double Variational Inference (TSDVI), a novel framework that employs two iteratively interacting variational networks to disentangle image information and model distributions. This approach greatly improves the model&#39;s ability to discern inter-class differences, hence enabling more effective separation of features across distinct categories. Our TSDVI approach has been extensively validated through experiments, which have shown significant performance gains. These experiments were conducted on many widely-used datasets such as miniImagenet, tiered-Imagenet, CIFAR-FS, and FC100. Particularly noteworthy is the outstanding performance gain of up to 30% on the 1-shot task within the FC100 dataset. These practical results strongly emphasize the effectiveness of the TSDVI model and its promise in few-shot classification. Code is available at: https://github.com/zjh1015/tsdvi .},
  archive      = {J_ICV},
  author       = {Jinhu Zhang and Shaobo Li and Xingxing Zhang and Zichen Huang and Hui Miao},
  doi          = {10.1016/j.imavis.2024.105034},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105034},
  shortjournal = {Image Vis. Comput.},
  title        = {Transductive semantic decoupling double variational inference for few-shot classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SADGFeat: Learning local features with layer spatial
attention and domain generalization. <em>ICV</em>, <em>146</em>, 105033.
(<a href="https://doi.org/10.1016/j.imavis.2024.105033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local feature plays a pivotal role in various robotic tasks, including 3D reconstruction and visual localization. Although deep learning-based local features have proven superior to their traditional counterparts, they still face challenges in practical applications due to matching failures. These challenges primarily stem from inaccuracies in keypoint localization and the limited robustness of descriptors particularly with significant appearance changes. In this study, we introduce a novel method for local feature learning based on layer spatial attention and domain generalization. Firstly, a keypoint extraction strategy driven by layer spatial attention from high-level feature is proposed to enhance the accuracy of keypoint localization, progressing from coarse to fine granularity . Secondly, a new learning paradigm based on domain generalization is developed to extract local features with resilience against variations in illumination. To enrich the domain diversity within the training dataset, a real-time and lossless Fourier transform-based domain augmentation method is introduced. This method seamlessly integrates into the training process, enhancing the model&#39;s adaptability to varying domains. Additionally, explicit feature alignment-based representation learning is performed, further reinforcing the extraction of domain-invariant local features. Experimental results on public datasets demonstrate that the proposed method achieves state-of-the-art performance across various downstream tasks reliant on local feature matching, such as image matching, 3D reconstruction, and long-term visual localization.},
  archive      = {J_ICV},
  author       = {Wenjing Bai and Yunzhou Zhang and Li Wang and Wei Liu and Jun Hu and Guan Huang},
  doi          = {10.1016/j.imavis.2024.105033},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105033},
  shortjournal = {Image Vis. Comput.},
  title        = {SADGFeat: Learning local features with layer spatial attention and domain generalization},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bilateral regularized optimization model for edge-preserving
image smoothing. <em>ICV</em>, <em>146</em>, 105031. (<a
href="https://doi.org/10.1016/j.imavis.2024.105031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-preserving image smoothing is vital in the field of image processing and computational photography. The state-of-the-art filters based on optimization models have achieved promising performance. However, most of them fail to consider the spatial support in the regularization term, thus limiting the edge-preserving capabilities. In this paper, inspired by the bilateral filter, which consists of a range kernel and a spatial kernel. we propose to leverage bilateral kernel as a penalty function, and embed it into an optimization model for edge-preserving image smoothing. Furthermore, we propose to incorporate an edge-aware weighted scheme in the data term design, which further improves the edge-preserving capability. The bilateral function is non-convex and can be non-trivial to solve. In this paper, we propose a novel iterative solution based on fixed point iteration , where the main burden in each iteration is a bilateral filtering process. We have conducted extensive experiments to evaluate the proposed filter. Experiment results indicate that our filter benefits a variety of image processing tasks. Moreover, we propose an efficient approximation of the proposed filter, which is able to significantly accelerate the filtering process with neglectable sacrifice of smoothing quality.},
  archive      = {J_ICV},
  author       = {Yang Yang and Yue Sun and Wei Gao and Xinyu Wang and Lanling Zeng},
  doi          = {10.1016/j.imavis.2024.105031},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105031},
  shortjournal = {Image Vis. Comput.},
  title        = {Bilateral regularized optimization model for edge-preserving image smoothing},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new multi-picture architecture for learned video
deinterlacing and demosaicing with parallel deformable convolution and
self-attention blocks. <em>ICV</em>, <em>146</em>, 105023. (<a
href="https://doi.org/10.1016/j.imavis.2024.105023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the fact real-world video deinterlacing and demosaicing are well-suited to supervised learning from synthetically degraded data because the degradation models are known and fixed, learned video deinterlacing and demosaicing have received much less attention compared to denoising and super-resolution tasks. We propose a new multi-picture architecture for video deinterlacing or demosaicing by aligning multiple supporting pictures with missing data to a reference picture to be reconstructed, benefiting from both local and global spatio-temporal correlations in the feature space using modified deformable convolution blocks and a novel residual efficient top- k k self-attention (kSA) block, respectively. Separate reconstruction blocks are used to estimate different types of missing data. Our extensive experimental results, on synthetic or real-world datasets, demonstrate that the proposed novel architecture provides superior results that significantly exceed the state-of-the-art for both tasks in terms of PSNR , SSIM, and perceptual quality. Ablation studies are provided to justify and show the benefit of each novel modification made to the deformable convolution and residual efficient kSA blocks. Code is available: https://github.com/KUIS-AI-Tekalp-Research-Group/Video-Deinterlacing .},
  archive      = {J_ICV},
  author       = {Ronglei Ji and A. Murat Tekalp},
  doi          = {10.1016/j.imavis.2024.105023},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105023},
  shortjournal = {Image Vis. Comput.},
  title        = {A new multi-picture architecture for learned video deinterlacing and demosaicing with parallel deformable convolution and self-attention blocks},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TransMix: Crafting highly transferable adversarial examples
to evade face recognition models. <em>ICV</em>, <em>146</em>, 105022.
(<a href="https://doi.org/10.1016/j.imavis.2024.105022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main challenge in deceiving face recognition (FR) models lies in the target model under the black-box setting. Existing works seek to generate adversarial examples to improve the adversarial transferability for black-box attacks. However, the attack performance and quality of the crafted image still have room for improvement. In this work, we propose a novel method called TransMix to improve the transferability of adversarial face examples based on data augmentation . Our approach leverages the mixture of the original image with a mixed sample image that is randomly mixed using images from different identities or the same identities, incorporating information from diverse categories. Then, we perform random transformations N N times to create diverse input patterns, exploiting the gradient from various images and other identities in the same iteration. Extensive experiments conducted on the CelebA dataset demonstrate that TransMix achieves a significantly higher attack success rate against different FR models and Vision Transformers (ViTs), outperforming the best competitor by a large margin of 5.6% and 8.8% when attacking the ViTs using adversarial images generated on the ArcFace model. Our results also confirm that adversarial examples crafted by TransMix exhibit good adversarial transferability against defense models, achieving an attack success rate of 52.3% on the Bit-Red model.},
  archive      = {J_ICV},
  author       = {Yasmeen M. Khedr and Xin Liu and Kun He},
  doi          = {10.1016/j.imavis.2024.105022},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105022},
  shortjournal = {Image Vis. Comput.},
  title        = {TransMix: Crafting highly transferable adversarial examples to evade face recognition models},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ResNet-101 deep learning framework induced transfer
learning strategy for moving object detection. <em>ICV</em>,
<em>146</em>, 105021. (<a
href="https://doi.org/10.1016/j.imavis.2024.105021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is a crucial stage in many visual surveillance systems. The prime objective of any such system is to detect moving objects such that the system could be utilized to face many real-time challenges. In the last few decades, various methods have been developed to detect moving objects. However, the performance of many existing methods needs further improvement for slow, moderate, and fast-moving object detection in videos simultaneously and also for unseen video setups. In this article, a noteworthy effort is made to detect moving objects in complex videos by harnessing the potential of an encoder-decoder-type deep framework, employing a customized ResNet-101 model along with a feature pooling framework (FPF). The proposed algorithm has four-fold innovations including: A pre-trained modified ResNet-101 network with a transfer learning technique is proposed as an encoder to learn the challenging video scene adequately. The proposed encoder network employs a total of twenty three numbers of layers with skip connections making the model less complex. In between the encoder and decoder framework, the FPF module is used that combines a max-pooling layer, a convolutional layer , and multiple convolutional layers with varying sampling rates . This FPM module can preserve multi-scale and multi-dimensional features across different levels accurately. A decoder architecture consisting of stacked convolution layers is implemented to transform the features into image space efficiently. The efficiency of the proposed scheme is corroborated using subjective and objective analysis. The efficiency of the developed model is highlighted through a comparison with thirty-three existing methods, effectively illustrating its superior efficacy.},
  archive      = {J_ICV},
  author       = {Upasana Panigrahi and Prabodh Kumar Sahoo and Manoj Kumar Panda and Ganapati Panda},
  doi          = {10.1016/j.imavis.2024.105021},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105021},
  shortjournal = {Image Vis. Comput.},
  title        = {A ResNet-101 deep learning framework induced transfer learning strategy for moving object detection},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SAKD: Sparse attention knowledge distillation. <em>ICV</em>,
<em>146</em>, 105020. (<a
href="https://doi.org/10.1016/j.imavis.2024.105020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have gained significant interest due to their success in large model scenarios. However, large models often require massive computational resources , which can challenge end devices with limited storage capabilities. Transferring knowledge from big to small models and achieving similar results with limited resources requires further research. Knowledge distillation techniques, which involve using teacher-student models to migrate large model capabilities to small models, have been widely used in model compression and knowledge transfer. In this paper, a novel knowledge distillation approach is proposed, which utilizes the sparse attention mechanism (SAKD). SAKD computes attention using student features as queries and teacher features as key values and performs sparse attention values by random deactivation. Then, this sparse attention value is used to reweight the feature distance of each teacher-student feature pair to avoid negative transfer. Comprehensive experiments demonstrate the effectiveness and generality of our approach. Moreover, our SAKD method outperforms previous state-of-the-art methods on image classification tasks.},
  archive      = {J_ICV},
  author       = {Zhen Guo and Pengzhou Zhang and Peng Liang},
  doi          = {10.1016/j.imavis.2024.105020},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105020},
  shortjournal = {Image Vis. Comput.},
  title        = {SAKD: Sparse attention knowledge distillation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation of large-scale point clouds by
integrating attention mechanisms and transformer models. <em>ICV</em>,
<em>146</em>, 105019. (<a
href="https://doi.org/10.1016/j.imavis.2024.105019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current field of point cloud processing, semantic segmentation of large-scale point clouds remains a challenging problem. Traditional methods often underperform when faced with the complexity and density variations present in large-scale point cloud data. This study introduces an innovative model designed for semantic segmentation of large-scale point clouds. The model integrates a CNN-Transformer-based Context Aggregation Module with a Slot Attention mechanism to enhance the understanding of entity relationships and improve semantic segmentation performance . Furthermore, by flexibly adjusting the weight parameters within the loss function, we have optimized the training process, increasing its flexibility. In our experiments, we compared the proposed model with 55 classical and contemporary point cloud semantic segmentation models , evaluating semantic segmentation performance, computational resource consumption, inference time, and the number of model parameters. The results show a significant improvement in performance on the S3DIS and Semantic3D datasets, specifically achieving an mIoU of 71.53% and an OA of 93.92%. Ablation studies were conducted to further ascertain the contribution of each module to the overall performance of the model. This study presents an innovative approach to overcoming the challenges of large-scale point cloud semantic segmentation, offering significant contributions to the advancement of point cloud processing.},
  archive      = {J_ICV},
  author       = {Tiebiao Yuan and Yangyang Yu and Xiaolong Wang},
  doi          = {10.1016/j.imavis.2024.105019},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105019},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic segmentation of large-scale point clouds by integrating attention mechanisms and transformer models},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning and genetic algorithm-based ensemble model for
feature selection and classification of breast ultrasound images.
<em>ICV</em>, <em>146</em>, 105018. (<a
href="https://doi.org/10.1016/j.imavis.2024.105018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction and selection are important techniques in the classification of medical images. Extraction of key features and selection of relevant features are the preliminary processes that are essential for identifying the shape of an object or diagnosis of a tumor in images. In this study, we conduct a thorough comparison of deep neural networks&#39; performance. The comparison of the networks infers MobileNet as optimal for feature extraction from medical images, where it has minimal parameters and high validation accuracy. For feature selection, we employ the Genetic Algorithm (GA) because of its proficiency in handling high-dimensional and complex feature space . GA&#39;s iterative process aligns well with the unique characteristics of breast ultrasound (BUS) images, which enhances its efficacy in selecting salient features of BUS images. An ensemble model, capitalizing on the collective decision-making capabilities of multiple classifiers based on a weighted voting scheme for classification, is proposed. Empirical evaluations are conducted using two publicly available BUS image datasets, BUSI and UDIAT. The proposed model demonstrates a notable improvement of approximately 4% and 9% in accuracy for the BUSI and UDIAT, respectively. The improved diagnostic accuracy in breast abnormality identification allows for early abnormality diagnosis. This improves treatment outcomes for breast cancer patients and highlights the practical value of the proposed method for improving BUS image categorization.},
  archive      = {J_ICV},
  author       = {Mohsin Furkh Dar and Avatharam Ganivada},
  doi          = {10.1016/j.imavis.2024.105018},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105018},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning and genetic algorithm-based ensemble model for feature selection and classification of breast ultrasound images},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 3D multi-scale CycleGAN framework for generating synthetic
PETs from MRIs for alzheimer’s disease diagnosis. <em>ICV</em>,
<em>146</em>, 105017. (<a
href="https://doi.org/10.1016/j.imavis.2024.105017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel framework for generating synthesized PET images from MRIs to fill in missing PETs and help with Alzheimer&#39;s disease (AD) diagnosis. This framework employs a 3D multi-scale image-to-image CycleGAN architecture for the end-to-end translation of MRI and PET domains together. A hybrid loss function is also proposed to enforce structural similarity while preserving voxel-wise similarity and avoiding blurry images. As shown by the quantitative and visual assessment of the synthesized PETs, this framework is superior to the state-of-the-art. Moreover, using these synthesized PETs helps improve the ternary classification of AD subjects (AD vs. MCI vs. NC). Specifically, assuming an extreme case where none of the subjects has a PET , feeding the classifier with MRIs and their corresponding synthetic PETs results in a more accurate diagnosis than feeding it with just available MRIs. Accordingly, the proposed framework can help improve AD diagnosis, which is the final goal of the current study. Ablation investigation of the proposed multi-scale framework as well as the proposed loss function, is also conducted to study their contribution to the quality of synthesized PETs. Furthermore, other factors, such as stopping criteria, the type of normalization layer, the activation function , and dropouts, are examined, concluding that the appropriate use of these factors can significantly improve the quality of synthesized PETs.},
  archive      = {J_ICV},
  author       = {M. Khojaste-Sarakhsi and Seyedhamidreza Shahabi Haghighi and S.M.T. Fatemi Ghomi and Elena Marchiori},
  doi          = {10.1016/j.imavis.2024.105017},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105017},
  shortjournal = {Image Vis. Comput.},
  title        = {A 3D multi-scale CycleGAN framework for generating synthetic PETs from MRIs for alzheimer&#39;s disease diagnosis},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of dental periapical lesions using retinex based
image enhancement and lightweight deep learning model. <em>ICV</em>,
<em>146</em>, 105016. (<a
href="https://doi.org/10.1016/j.imavis.2024.105016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dental periapical lesions , commonly associated with inflammation around the tooth apex , pose a significant challenge in early diagnosis and treatment. This study introduces a novel approach for the detection of dental periapical lesions through the integration of Retinex-based image enhancement techniques and a lightweight deep learning model. The Retinex algorithm is employed to enhance the radiographic images, addressing issues related to inconsistent illumination and contrast. Subsequently, a tailored lightweight deep learning model is designed to efficiently extract relevant features from the enhanced images. Present methodology leverages a dataset of dental radiographs to train and evaluate the deep learning model, incorporating a diverse range of periapical lesion cases. The model is optimized for computational efficiency while maintaining high accuracy, making it suitable for deployment in resource-constrained environment. To enhance precision in lesion detection, the U-Net segmentation technique has been incorporated, providing a sophisticated approach to delineate and analyze specific areas of interest within the radiographic images. This addition further refines our diagnostic framework, contributing to the robustness of lesion identification. Experimental results demonstrate the effectiveness of the Retinex-based image enhancement in improving the visibility of periapical lesions. The lightweight deep learning model exhibits promising performance in accurately detecting and classifying dental periapical lesions, showcasing its potential for early and efficient diagnosis. The results obtained from the present model are compared with those from Convolutional Neural Network (CNN) as well as with diagnosis of expert practitioners and the model is observed to perform very well. The study contributes to the advancement of computer-aided diagnostic tools in dentistry , offering a scalable and accessible solution for the identification of dental periapical lesions through the fusion of image enhancement and lightweight deep learning techniques.},
  archive      = {J_ICV},
  author       = {Vaishali Latke and Vaibhav Narawade},
  doi          = {10.1016/j.imavis.2024.105016},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105016},
  shortjournal = {Image Vis. Comput.},
  title        = {Detection of dental periapical lesions using retinex based image enhancement and lightweight deep learning model},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation and analysis of feature point detection methods
based on vSLAM systems. <em>ICV</em>, <em>146</em>, 105015. (<a
href="https://doi.org/10.1016/j.imavis.2024.105015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature point detection is identified as a significant issue in the field of computer vision . Quantitative conclusions have been established regarding the performance evaluation of feature point detection using manually annotated datasets. However, these datasets, obtained through affine transformations with preset parameters on images, exhibit limitations in terms of variety, quantity, and challenges, differing from real-world application scenarios. In actual scenes, Vision Simultaneous Localization and Mapping (vSLAM) systems are extensively applied, and the precision of their localization and mapping is directly linked to the performance of feature points. To profoundly understand the performance of feature point detection in practical applications, vSLAM systems are chosen for evaluation in this study. More diverse and challenging datasets are utilized, encompassing real datasets covering variations in lighting, rotation, occlusion, and camera angle changes, as well as synthetic datasets reflecting complex conditions such as time, season, motion patterns, and environmental textures. Based on the evaluation results, the applicability of various feature point detection methods in different environments is thoroughly discussed, and the underlying principles are analyzed (Table 13). The conclusions drawn from this research provide references for the development of new feature point detection methods, the selection of such methods in vSLAM systems, and other related studies in the field of computer vision.},
  archive      = {J_ICV},
  author       = {Chenyang Xie and Qiang Liu and Baojia Chen and Zhiqiang Hao},
  doi          = {10.1016/j.imavis.2024.105015},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105015},
  shortjournal = {Image Vis. Comput.},
  title        = {Evaluation and analysis of feature point detection methods based on vSLAM systems},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WaveletFormerNet: A transformer-based wavelet network for
real-world non-homogeneous and dense fog removal. <em>ICV</em>,
<em>146</em>, 105014. (<a
href="https://doi.org/10.1016/j.imavis.2024.105014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. Moreover, the over-stacking of convolutional blocks might increase the model complexity. In addition to the challenges of obtaining sufficient training data , overfitting can also arise in deep learning techniques for foggy image processing , which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Such a structure reduces computational expenses and improves the effectiveness of the network. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Through extensive experiments on real-world fog datasets, we have demonstrated that our WaveletFormerNet achieves superior performance compared to state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications compared to existing state-of-the-art methods, further confirming our proposed approach&#39;s effectiveness and robustness. Our code is available at https://github.com/shengli666666/WaveletFormerNet .},
  archive      = {J_ICV},
  author       = {Shengli Zhang and Zhiyong Tao and Sen Lin},
  doi          = {10.1016/j.imavis.2024.105014},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105014},
  shortjournal = {Image Vis. Comput.},
  title        = {WaveletFormerNet: A transformer-based wavelet network for real-world non-homogeneous and dense fog removal},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixup mask adaptation: Bridging the gap between input
saliency and representations via attention mechanism in feature mixup.
<em>ICV</em>, <em>146</em>, 105013. (<a
href="https://doi.org/10.1016/j.imavis.2024.105013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent complexity and extensive architecture of deep neural networks often lead to overfitting, compromising their ability to generalize to new, unseen data. One of the regularization techniques, data augmentation , is now considered vital to alleviate this, and mixup, which blends pairs of images and labels, has proven effective in enhancing model generalization. Recently, incorporating saliency in mixups has shown performance gains by retaining salient regions in mixed results. While these methods have become mainstream at the input level, their applications at the feature level remain under-explored. Our observations indicate that outcomes from naive applications of input saliency-based methods did not consistently lead to enhancements in performance. In this paper, we attribute these observations primarily to two challenges: ‘Hard Boundary Issue’ and ‘Saliency Mismatch.’ The Hard Boundary Issue describes a situation where masks with distinct, sharp edges work well at the input level, but lead to unintended distortions in the deeper layers. The Saliency Mismatch points to the disparity between saliency masks generated from input images and the saliency of feature maps. To tackle these challenges, we present a novel method called ‘attention-based mixup mask adaptation’ (MMA). This approach employs an attention mechanism to effectively adapt mixup masks, which are designed to maximize saliency at the input level, for feature augmentation purposes. We reduce the Saliency Mismatch problem by incorporating the spatial significance of the feature map into the mixup mask. Additionally, we address the Hard Boundary Issue by applying softmax to smoothen the adjusted mixup mask. Through comprehensive experiments, we validate our observations and confirm the effectiveness of applying MMA to saliency-aware mixup approaches at the feature level, as evidenced by the performance improvements on multiple benchmarks and the robustness improvements against corruption and deformation.},
  archive      = {J_ICV},
  author       = {Minsoo Kang and Minkoo Kang and Seong-Whan Lee and Suhyun Kim},
  doi          = {10.1016/j.imavis.2024.105013},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105013},
  shortjournal = {Image Vis. Comput.},
  title        = {Mixup mask adaptation: Bridging the gap between input saliency and representations via attention mechanism in feature mixup},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparison of fine-tuning strategies for transfer learning
in medical image classification. <em>ICV</em>, <em>146</em>, 105012. (<a
href="https://doi.org/10.1016/j.imavis.2024.105012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of medical imaging and machine learning, one of the most pressing challenges is the effective adaptation of pre-trained models to specialized medical contexts. Despite the availability of advanced pre-trained models, their direct application to the highly specialized and diverse field of medical imaging often falls short due to the unique characteristics of medical data. This study provides a comprehensive analysis on the performance of various fine-tuning methods applied to pre-trained models across a spectrum of medical imaging domains, including X-ray, MRI , Histology, Dermoscopy, and Endoscopic surgery. We evaluated eight fine-tuning strategies, including standard techniques such as fine-tuning all layers or fine-tuning only the classifier layers, alongside methods such as gradually unfreezing layers, regularization based fine-tuning and adaptive learning rates. We selected three well-established CNN architectures (ResNet-50, DenseNet-121, and VGG-19) to cover a range of learning and feature extraction scenarios. Although our results indicate that the efficacy of these fine-tuning methods significantly varies depending on both the architecture and the medical imaging type, strategies such as combining Linear Probing with Full Fine-tuning resulted in notable improvements in over 50% of the evaluated cases, demonstrating general effectiveness across medical domains. Moreover, Auto-RGN, which dynamically adjusts learning rates, led to performance enhancements of up to 11% for specific modalities. Additionally, the DenseNet architecture showed more pronounced benefits from alternative fine-tuning approaches compared to traditional full fine-tuning. This work not only provides valuable insights for optimizing pre-trained models in medical image analysis but also suggests the potential for future research into more advanced architectures and fine-tuning methods.},
  archive      = {J_ICV},
  author       = {Ana Davila and Jacinto Colan and Yasuhisa Hasegawa},
  doi          = {10.1016/j.imavis.2024.105012},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105012},
  shortjournal = {Image Vis. Comput.},
  title        = {Comparison of fine-tuning strategies for transfer learning in medical image classification},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video anomaly detection based on a multi-layer
reconstruction autoencoder with a variance attention strategy.
<em>ICV</em>, <em>146</em>, 105011. (<a
href="https://doi.org/10.1016/j.imavis.2024.105011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a comprehensive framework for detecting anomalies in videos based on autoencoder (AE). Traditional AE models solely rely on input and final reconstruction, potentially limiting their capacity to fully utilize the intermediate neural network layers . To mitigate this limitation, we introduce a novel approach that concurrently trains the model using corresponding intermediate layers from both the encoder and decoder. This allows the model to capture more intricate features, thus enhancing its anomaly detection capabilities. Furthermore, we introduce a motion loss function that exclusively relies on original video frames rather than optical flow , rendering it more efficient and capable of extracting motion features. Additionally, we have devised a variance attention strategy that is parameter-free and can automatically directs our model&#39;s focus towards moving objects, further boosting the performance of our approach. Our experiments on three public datasets demonstrate the effectiveness and efficiency of our method in identifying abnormal events in complex scenarios. The code is publicly available at https://github.com/lsf2008/multRecLossAEPub .},
  archive      = {J_ICV},
  author       = {Shifeng Li and Yan Cheng and Liang Zhang and Xi Luo and Ruixuan Zhang},
  doi          = {10.1016/j.imavis.2024.105011},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105011},
  shortjournal = {Image Vis. Comput.},
  title        = {Video anomaly detection based on a multi-layer reconstruction autoencoder with a variance attention strategy},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust ensemble person reidentification via orthogonal
fusion with occlusion handling. <em>ICV</em>, <em>146</em>, 105010. (<a
href="https://doi.org/10.1016/j.imavis.2024.105010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occlusion remains one of the major challenges in person reidentification (ReID) due to the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without manually labeling occluded regions, we propose to take an ensemble learning-based approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model uses masked autoencoder (MAE) and global–local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust to occluded regions. Experimental results are reported on several Re-ID datasets to show the effectiveness of our developed ensemble model named orthogonal fusion with occlusion handling (OFOH). Compared to competing methods, the proposed OFOH approach has achieved competent rank-1 and mAP performance.},
  archive      = {J_ICV},
  author       = {Syeda Nyma Ferdous and Xin Li},
  doi          = {10.1016/j.imavis.2024.105010},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105010},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust ensemble person reidentification via orthogonal fusion with occlusion handling},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-dimensional human motion pose recognition algorithm
based on graph convolutional networks. <em>ICV</em>, <em>146</em>,
105009. (<a href="https://doi.org/10.1016/j.imavis.2024.105009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of three-dimensional human motion posture recognition, there are problems such as target loss, inaccurate target positioning, and high computational complexity . This article designs a recognition evaluation algorithm to address these issues. Design a LiteHRNet model for extracting skeleton sequences from action videos, and propose a graph convolutional structure that combines residual networks and attention mechanisms . This network can effectively enhance the expression ability of node key features. Introducing second-order velocity information and spatial position information of joint points to improve positioning accuracy. Improve the TCN and Transformer network models to simultaneously extract local and long-term features throughout the entire model, and more accurately model the temporal correlation between nodes in the entire action sequence. The fusion of Transformer networks can reduce the computational complexity of the model while ensuring its accuracy. The experiment shows that the model has good evaluation performance on multiple datasets.},
  archive      = {J_ICV},
  author       = {Linfang Sun and Ningning Li and Guangfeng Zhao and Gang Wang},
  doi          = {10.1016/j.imavis.2024.105009},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105009},
  shortjournal = {Image Vis. Comput.},
  title        = {A three-dimensional human motion pose recognition algorithm based on graph convolutional networks},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Alignment and fusion for adaptive domain nighttime semantic
segmentation. <em>ICV</em>, <em>146</em>, 105008. (<a
href="https://doi.org/10.1016/j.imavis.2024.105008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of autonomous driving technology, both daytime and nighttime scenes are common. However, due to the poor illumination and difficulty in manual annotation of nighttime images , semantic segmentation of nighttime scenes is more challenging compared to daytime scenes. Therefore, achieving significant progress in nighttime semantic segmentation would greatly enhance the effectiveness of the application of autonomous driving scenarios. In this work, we investigate the problem of domain-adaptive nighttime semantic segmentation (DANSS). The problem aims to learn semantic segmentation in nighttime scenes by leveraging a labeled Cityscapes dataset and unlabeled but roughly aligned day-night image pairs. To address this, we propose an Align and Fusion Network (AAFnet), a network for adaptive domain nighttime semantic segmentation. AAFnet utilizes a novel DAFormer as the backbone to separately compute features for dynamic and static objects during the training process. It also employs methods such as small category sampling and image blending to improve learning effectiveness. Additionally, we propose utilizing local image patches to enhance the results during the testing process. Experimental results demonstrate a significant improvement of 10.4% compared to the previous method, DANNet. Extensive experiments conducted on the Dark Zurich dataset and Nightdriving dataset validate the effectiveness of the proposed approach. Our method outperforms previous backbones and achieves top-ranking results.},
  archive      = {J_ICV},
  author       = {Bao Zhang and Nianmin Yao and Jian Zhao and Yanan Zhang},
  doi          = {10.1016/j.imavis.2024.105008},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105008},
  shortjournal = {Image Vis. Comput.},
  title        = {Alignment and fusion for adaptive domain nighttime semantic segmentation},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatiotemporal motion prediction network based on
multi-level feature disentanglement. <em>ICV</em>, <em>146</em>, 105005.
(<a href="https://doi.org/10.1016/j.imavis.2024.105005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction task is significantly challenged by the intricate scene information and motion variations present in spatiotemporal data. Existing prediction methods struggle to accurately forecast long-term outcomes, particularly for transient motions characterized by notable trends, such as hand lifts, jumps, or vehicle turns. To address these challenges, we introduce a Spatiotemporal Motion Prediction Network based on Multi-level Feature Disentanglement (FDPNet). The model delineates spatiotemporal prediction into two distinct stages: feature disentanglement and motion prediction. We first devise a Multi-level Feature Disentanglement (MFD) model to disentangle the multilayer features of motion within the temporal sequence, encompassing period, trend, and residual components. This disentanglement is based on disentangling spatiotemporal coupling, enabling the network to comprehensively grasp the genuine laws governing motion in the spatiotemporal evolution process. Second, to enhance the prediction accuracy of the network over extended periods, we introduce the Motion Differential Self-Attention LSTM unit (MDSA-LSTM). This unit employs differential operations to extract inter-frame motion trends, elevating the network&#39;s proficiency in capturing spatiotemporal correlations over long distances through an enhanced self-attention mechanism. FDPNet attains state-of-the-art performance on the Moving MNIST, UCF101, KITTI, and Caltech pedestrian datasets. These outcomes substantiate the substantial potential of this research within the realm of spatiotemporal prediction.},
  archive      = {J_ICV},
  author       = {Suting Chen and Yewen Bo and Xu Wu},
  doi          = {10.1016/j.imavis.2024.105005},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {105005},
  shortjournal = {Image Vis. Comput.},
  title        = {A spatiotemporal motion prediction network based on multi-level feature disentanglement},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater image quality optimization: Researches,
challenges, and future trends. <em>ICV</em>, <em>146</em>, 104995. (<a
href="https://doi.org/10.1016/j.imavis.2024.104995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images serve as crucial mediums for conveying marine information. Nevertheless, due to the inherent complexity of the underwater environment , underwater images often suffer from various quality degradation phenomena such as color deviation, low contrast, and non-uniform illumination. These degraded underwater images fail to meet the requirements of underwater computer vision applications. Consequently, effective quality optimization of underwater images is of paramount research and analytical value. Based on whether they rely on underwater physical imaging models, underwater image quality optimization techniques can be categorized into underwater image enhancement and underwater image restoration methods. This paper provides a comprehensive review of underwater image enhancement and restoration algorithms , accompanied by a brief introduction to underwater imaging model. Then, we systematically analyze publicly available underwater image datasets and commonly-used quality assessment methodologies. Furthermore, extensive experimental comparisons are carried out to assess the performance of underwater image optimization algorithms and their practical impact on high-level vision tasks. Finally, the challenges and future development trends in this field are discussed. We hope that the efforts made in this paper will provide valuable references for future research and contribute to the innovative advancement of underwater image optimization.},
  archive      = {J_ICV},
  author       = {Mingjie Wang and Keke Zhang and Hongan Wei and Weiling Chen and Tiesong Zhao},
  doi          = {10.1016/j.imavis.2024.104995},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104995},
  shortjournal = {Image Vis. Comput.},
  title        = {Underwater image quality optimization: Researches, challenges, and future trends},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 3D motion image recognition model based on 3D CNN-GRU
model and attention mechanism. <em>ICV</em>, <em>146</em>, 104991. (<a
href="https://doi.org/10.1016/j.imavis.2024.104991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moving image recognition has become a well-explored problem in computer vision. However, it is difficult for the traditional convolutional neural network (CNN) model to effectively capture timing information in motion. For better use of video sequence features and to improve the accuracy of action recognition, Therefore, this paper proposes a Three-dimensional CNN (3DCNN) model based on Gated Recurrent Unit (GRU) with an attention mechanism. The model leverages 3DCNN for deep feature extraction from video frames, employs GRU to capture the temporal dynamics of feature sequences and incorporates an attention mechanism to emphasize key frames, which improves moving image recognition. Demonstrating superior accuracy in’Cross-Subject’ and’Cross-View’ evaluations, our model surpasses standard benchmarks with accuracies of 83.2% and 87.3% respectively.},
  archive      = {J_ICV},
  author       = {Chen Cheng and Huahu Xu},
  doi          = {10.1016/j.imavis.2024.104991},
  journal      = {Image and Vision Computing},
  month        = {6},
  pages        = {104991},
  shortjournal = {Image Vis. Comput.},
  title        = {A 3D motion image recognition model based on 3D CNN-GRU model and attention mechanism},
  volume       = {146},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Siamese network to assess scanner-related contrast
variability in MRI. <em>ICV</em>, <em>145</em>, 104997. (<a
href="https://doi.org/10.1016/j.imavis.2024.104997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) stands as a noninvasive tool for diagnosing and monitoring various diseases. The flexibility of MRI configuration parameters allows for adaptable imaging sequences, and at the same time poses challenges in terms of reproducibility, as variability in imaging sequences leads to significant differences in image contrast. This is one of the major causes that compromise the reliability of deep learning methods. Since the majority of the literature is focused on documenting the effects of this issue rather than delving into its underlying causes, this work follows a different approach. A Siamese Neural Network (SNN) has been trained to identify the scanner that acquired the input image. Experimental results include the use of Euclidean Distance (ED) and machine learning algorithms trained and tested using the feature vectors generated with the SNN. The results have shown that the proposed method is capable of distinguishing the scanner used for the acquisition with high accuracy. For a comprehensive interpretation of the results, the feature vectors have been dimensionality reduced and visualized with a 3D plot. Finally, the proposed method is sensitive to MR image contrast variability and could be used to detect data-related inconsistencies and provide a mechanism to make users aware of potential issues.},
  archive      = {J_ICV},
  author       = {Matteo Polsinelli and Hongwei Bran Li and Filippo Mignosi and Li Zhang and Giuseppe Placidi},
  doi          = {10.1016/j.imavis.2024.104997},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104997},
  shortjournal = {Image Vis. Comput.},
  title        = {Siamese network to assess scanner-related contrast variability in MRI},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic segmentation using cross-stage feature reweighting
and efficient self-attention. <em>ICV</em>, <em>145</em>, 104996. (<a
href="https://doi.org/10.1016/j.imavis.2024.104996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, vision transformers have demonstrated strong performance in various computer vision tasks . The success of ViTs can be attribute to the ability of capturing long-range dependencies. However, transformer-based approaches often yield segmentation maps with incomplete object structures because of restricted cross-stage information propagation and lack of low-level details. To address these problems, we introduce a CNN-transformer semantic segmentation architecture which adopts a CNN backbone for multi-level feature extraction and a transformer encoder that focuses on global perception learning. Transformer embeddings of all stages are integrated to compute feature weights for dynamic cross-stage feature reweighting. As a result, high-level semantic context and low-level spatial details can be embedded into each stage to preserve multi-level information. An efficient attention-based feature fusion mechanism is developed to combine reweighted transformer embeddings with CNN features to generate segmentation maps with more complete object structure. Different from regular self-attention that has quadratic computational complexity , our efficient self-attention method achieves similar performance with linear complexity. Experimental results on ADE20K and Cityscapes datasets show that the proposed segmentation approach demonstrates superior performance against most state-of-the-art networks.},
  archive      = {J_ICV},
  author       = {Yingdong Ma and Xiaobin Lan},
  doi          = {10.1016/j.imavis.2024.104996},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104996},
  shortjournal = {Image Vis. Comput.},
  title        = {Semantic segmentation using cross-stage feature reweighting and efficient self-attention},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing temporal action localization in an end-to-end
network through estimation error incorporation. <em>ICV</em>,
<em>145</em>, 104994. (<a
href="https://doi.org/10.1016/j.imavis.2024.104994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action localization presents a significant challenge in computer vision , as the development of an efficient method for this task remains elusive. The objective is to identify human activities within untrimmed videos, determining when and which actions occur in each video. While using trimmed videos could potentially resolve the localization problem and enhance classification accuracy , it is impractical for real-world applications as the trimming process itself requires human intervention. This highlights the importance of temporal localization. Due to the availability of several successful approaches for action recognition in trimmed video, conventional multi-stage methods for untrimmed video, commonly employ a network to generate activity proposals, followed by a separate network for classification. These disjoint networks are optimized individually and thus usually vary from the global optimum, leading to less precise candidate action proposals. To address this challenge, we propose a novel end-to-end neural network that utilizes error estimation for precise action localization and recognition in untrimmed videos. The proposed method performs the localization and classification of action instances simultaneously, thereby optimizing the corresponding networks concurrently. To increase the precision of the action proposal boundaries, the Regression module is innovatively utilized as part of the proposed end-to-end network, along with the Evaluation and Classification modules. This module estimates the potential error in proposal time boundaries and enhances the result accuracy. We have conducted experiments on THUMOS 14 and ActivityNet-1.3, which are considered the most challenging datasets for temporal action localization. The novel, yet fairly simple, proposed network achieves remarkable performance improvement compared to the other state-of-the-art methods. This improvement, which is more pronounced in the cases of high temporal intersection with ground truth, is accomplished without requiring extra data or complicated architecture. By incorporating error estimation, we achieved improvement in mean Average Precision (mAP). The proposed approach particularly shines for the localization of challenging activities in the complex and diverse dataset ActivityNet-1.3. For instance, for the “drinking coffee” activity, the mean Average Precision (mAP) was enhanced fivefold compared to the best-reported results.},
  archive      = {J_ICV},
  author       = {Mozhgan Mokari and Khosrow Haj Sadeghi},
  doi          = {10.1016/j.imavis.2024.104994},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104994},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing temporal action localization in an end-to-end network through estimation error incorporation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing open-set domain adaptation through
unknown-filtering multi-classifier adversarial network. <em>ICV</em>,
<em>145</em>, 104993. (<a
href="https://doi.org/10.1016/j.imavis.2024.104993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is a fundamental research problem that aims to address the domain shift issue during the transfer of a model from a labeled source domain to an unlabeled target domain. In traditional domain adaptation scenarios, it is assumed that the class spaces of both the source and target domains are identical. However, real-world applications often entail situations where the target domain contains private classes that are absent in the source domain. Forcing the alignment of these two domains may result in negative transfer. This specific concern is addressed by the emerging field of Open-set Domain Adaptation (OSDA). Previous OSDA methods attempted to align the known classes between the source and target domains while separating the unknown samples. However, these methods are inadequate in effectively discerning instances from the unknown classes. Therefore, we propose Enhancing Open-Set Domain Adaptation through Unknown-Filtering Multi-Classifier Adversarial Network (UFMCAN), which leverages multiple classifiers including a weighted auxiliary classifier, an open-set recognizer, a primary classifier and a three-way domain discriminator through adversarial learning to effectively filter instances from the unknown classes present in the target domain while concurrently aligning the source and target-known distribution. Experiments on extensive benchmarks (Office-31, Office-Home, VisDA-2017 and Office-Mix) demonstrate the superior performance of UFMCAN compared to existing state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Qing Tian and Yi Zhao and Wangyuchen Wu and Jixin Sun},
  doi          = {10.1016/j.imavis.2024.104993},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104993},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing open-set domain adaptation through unknown-filtering multi-classifier adversarial network},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing fall prediction in the elderly people using LBP
features and transfer learning model. <em>ICV</em>, <em>145</em>,
104992. (<a href="https://doi.org/10.1016/j.imavis.2024.104992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an era where the detection and prevention of falls are crucial for the well-being of elderly and vulnerable individuals, achieving high accuracy in identifying such incidents is of utmost importance . This study introduces a modified NASNet , a novel transformer learning model designed for the classification of fall and not fall people based on local binary patterns (LBP) features. The modified NASNet model demonstrates high performance, achieving an accuracy, precision, recall, and F1 score of 99% with LBP features in differentiating fall from not fall. To assess the performance of NASNet, this research work conducted a comparative analysis with other prominent deep learning , and transfer learning models, as well as state-of-the-art machine learning frameworks. The findings indicate NASNet&#39;s enhanced performance in fall detection, highlighting its potential for application in real-time fall detection systems, contributing to a safer environment for individuals at risk. This research sets the stage for enhanced healthcare and assistance for vulnerable populations, addressing a critical concern in the healthcare sector.},
  archive      = {J_ICV},
  author       = {Muhammad Umer and Aisha Ahmed Alarfaj and Ebtisam Abdullah Alabdulqader and Shtwai Alsubai and Lucia Cascone and Fabio Narducci},
  doi          = {10.1016/j.imavis.2024.104992},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104992},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing fall prediction in the elderly people using LBP features and transfer learning model},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MLCapsNet +: A multi-capsule network for the identification
of the HIV ISs along important sequence positions. <em>ICV</em>,
<em>145</em>, 104990. (<a
href="https://doi.org/10.1016/j.imavis.2024.104990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most studied sub-category of the retrovirus is the human immunodeficiency virus (HIV), which is a type of virus of the Retroviridae family. The HIV integration site (HIV IS)/ integration sites (HIV ISs) denote a crucial entity in the entire process of infection and its rebound if there is an interruption in therapy. It determines the steps involved in the formation of latent viral reserve. This work proposes a very deep neural network framework, where each of the layers of the multi-layered network is itself a neural network (NN). The attention mechanism is used for the extraction of the importance of positions in terms of an attention map. This framework will use the Capsule Network along with the attention mechanism to increase the explainability about the presence of local features . Convolutional neural networks (CNNs), which are specialized for image-based recognition and classification, have many drawbacks with one of these being its invariance to translation. This drawback has been overcome by the Capsule Networks. The proposed model also identifies the HIV ISs achieving a performance better than the State-of-the-art methods. Support Vector Machine (SVM), Random Forest (RF) and Logistic Regression (LR) classifiers have been used. Only two state-of-the-art methods are present in the literature that achieve these two goals and a comparison of this work with those two works has been provided here. This work performs way better than the state-of-the-art work in detecting the HIV IS. The comparison is presented here in terms of AUC-ROC, AUC-PR, accuracy, confusion matrix , and F β β score.},
  archive      = {J_ICV},
  author       = {Minakshi Boruah and Ranjita Das},
  doi          = {10.1016/j.imavis.2024.104990},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104990},
  shortjournal = {Image Vis. Comput.},
  title        = {MLCapsNet +: A multi-capsule network for the identification of the HIV ISs along important sequence positions},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient deep learning architecture for effective fire
detection in smart surveillance. <em>ICV</em>, <em>145</em>, 104989. (<a
href="https://doi.org/10.1016/j.imavis.2024.104989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The threat of fire is pervasive, poses significant risks to the environment, and may include potential fatalities, property devastation, and socioeconomic disruption. Successfully mitigating these risks relies on the prompt identification of fires, a process in which soft computing methodologies play a pivotal role . Although, these fire detection methodologies neglected to explore the relationships among fire-indicative features, which are important to enable a model to learn more representative and robust features in remote sensing scenarios. In the context of small fire detection from aerial view using satellite imagery or unmanned arial vehicle (UAVs) presents challenges to capture rich spatial detail , hinder the model ability for accurate fire scene classification. Furthermore, it is significant to manage model complexity effectively to facilitate deployment on UAVs for fast and accurate responses in an emergency situation . To cope with these challenges, we propose an advanced model integrated a modified soft attention mechanism (MSAM) and a 3D convolution operation with a MobileNet architecture to overcome obstacles related to optimising features and controlling model complexity. The MSAM enabling the model to selectively emphasise essential features during the training process which acts as a selective filter. This adaptive attention mechanism enhances sensitivity and allowing the model to prioritise relevant patterns for accurate fire detection. Concurrently, the integration of a 3D convolutional operation extends the model spatial awareness , to capture intricate details across multiple scales, and particularly in small regions observed from aerial viewpoints. Benchmark evaluations of the proposed model over the FD, DFAN, and ADSF datasets reveal superior performance with enhanced accuracy (ACR) compared to existing methods. Our model surpassed the state-of-the-art models with an average ACR improvement of 0.54%, 2.64%, and 1.20% on the FD, ADSF, and DFAN datasets, respectively. Furthermore, the use of an explainable artificial intelligence (XAI) technique enhances the validation of the model visual emphasis on critical regions of the image, providing valuable insights into its decision-making process.},
  archive      = {J_ICV},
  author       = {Hikmat Yar and Zulfiqar Ahmad Khan and Imad Rida and Waseem Ullah and Min Je Kim and Sung Wook Baik},
  doi          = {10.1016/j.imavis.2024.104989},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104989},
  shortjournal = {Image Vis. Comput.},
  title        = {An efficient deep learning architecture for effective fire detection in smart surveillance},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-agnostic progressive saliency map generation for
object detector. <em>ICV</em>, <em>145</em>, 104988. (<a
href="https://doi.org/10.1016/j.imavis.2024.104988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of object detection models across various industries, the interpretability of these detectors has become an important research topic. The interpretability of a detector helps humans understand which areas significantly contribute to the model&#39;s decision. Furthermore, the interpretability enhances the credibility of detectors and helps identify their strengths and weaknesses. Due to the ability to provide intuitive explanations of models, the saliency map has been widely employed in the field of interpreting deep models. Model-agnostic interpretability methods are more general approaches as they treat the model as a black box without considering its internal complexity structure. However, existing model-agnostic interpretability methods often introduce “noise” into saliency maps by applying random masking and fixed masking granularity . This noise reduces the quality and interpretability of the generated saliency maps. To address this challenge and obtain more interpretable saliency maps for object detection models, this paper proposes a model-agnostic progressive saliency map generation method based on a hierarchical framework called MAPSM. In MAPSM, an adaptive masking partition mechanism is introduced to adapt the masking granularity to different object sizes. Additionally, MAPSM employs a saliency-driven mask generation strategy to effectively reduce the “noise”. Utilizing a hierarchical framework, MAPSM progressively discovers and refines the saliency areas of objects, resulting in more interpretable saliency maps. To evaluate the quality of the saliency maps generated by MAPSM, we compare it with other methods in multiple metrics. Experimental results demonstrate that our method produces saliency maps with better quality and interpretability.},
  archive      = {J_ICV},
  author       = {Yicheng Yan and Tong Jiang and Xianfeng Li and Lianpeng Sun and Jinjun Zhu and Jianxin Lin},
  doi          = {10.1016/j.imavis.2024.104988},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104988},
  shortjournal = {Image Vis. Comput.},
  title        = {Model-agnostic progressive saliency map generation for object detector},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integration of ultrasound and mammogram for multimodal
classification of breast cancer using hybrid residual neural network and
machine learning. <em>ICV</em>, <em>145</em>, 104987. (<a
href="https://doi.org/10.1016/j.imavis.2024.104987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is one of the topmost causes of mortality in women all over the world. Early detection and classification of the tumor allow proper treatment of patients and chances of survival. In this article, we propose a hybrid residual neural network (ResNet) and machine learning framework and integrate the features of both mammography (MG) and ultrasound (US) images to perform the multimodal classification of BC images as benign or malignant. The features are extracted automatically from the input images of each modality using the residual neural network from the average pooling layer. Next, the feature level fusion is carried out to obtain a feature vector by combining features of MG &amp; US. Finally, the multimodal classification is performed using the support vector machine (SVM) as a classifier. Experiments are performed on a real-time dataset collected from patients who have undergone both MG and US examinations. The classification accuracy obtained for the multimodal approach with SVM is 99.22%, which is higher than unimodal systems. Results show that the proposed multimodal approach performs better in classifying breast tumors than unimodal mammogram and ultrasound systems.},
  archive      = {J_ICV},
  author       = {Kushangi Atrey and Bikesh Kumar Singh and Narendra Kuber Bodhey},
  doi          = {10.1016/j.imavis.2024.104987},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104987},
  shortjournal = {Image Vis. Comput.},
  title        = {Integration of ultrasound and mammogram for multimodal classification of breast cancer using hybrid residual neural network and machine learning},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature disparity learning for weakly supervised object
localization. <em>ICV</em>, <em>145</em>, 104986. (<a
href="https://doi.org/10.1016/j.imavis.2024.104986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels. As a common WSOL method , adversarial erasing always masks the most discriminative region in the feature space to compel the network to localize more regions of the object. However, with the discriminative region vanishing, the localizer is confused when distinguishing the regions of object from the background. In this paper, we propose a new feature disparity learning (FDL), which encourages the network to learn more distinctive features from the object region with similarity measurement after feature enhancement. Specifically, we first introduce a Spatial Vector Cross Attention (SVCA) module. This module enhances responses in less discriminative region of erased feature maps by reintegrating the spatial distribution of features through the capture of interdependencies among spatial vectors on each channel. Furthermore, we propose a feature complementarity loss to measure the similarity between unerased features and erased features, guiding the network to learn feature disparities caused by adversarial erasing for improved localization and classification. Several experimental studies demonstrate a significant increase in localization performance over the existing state-of-the-art erasing methods on the CUB 200–2011 and ILSVRC 2016 datasets.},
  archive      = {J_ICV},
  author       = {Bingfeng Li and Haohao Ruan and Xinwei Li and Keping Wang},
  doi          = {10.1016/j.imavis.2024.104986},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104986},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature disparity learning for weakly supervised object localization},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible multi-objective particle swarm optimization
clustering with game theory to address human activity discovery fully
unsupervised. <em>ICV</em>, <em>145</em>, 104985. (<a
href="https://doi.org/10.1016/j.imavis.2024.104985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition is a crucial field of study, but current approaches often require ground truth labels, which are not always available. We propose a new method called the Flexible Multi-Objective Particle swarm optimization clustering method based on Game theory (FMOPG), which can identify human activities without any supervision. Unlike traditional clustering methods that require an estimate of the number of clusters and are often inaccurate, FMOPG handles varying cluster numbers with an incremental technique, selecting clusters with good connectivity and separation. We enhance Particle Swarm Optimization (PSO) with mean-shift vectors for faster convergence and better handling of non-spherical clusters. Employing multi-objective optimization and Gaussian mutation, FMOPG provides a range of optimal solutions. We map the optimization problem to game theory to select the best solution based on different criteria. A smart grid-based method is proposed for population initialization, reducing variance and improving reliability. FMOPG outperforms state-of-the-art methods, improving clustering accuracy by 3.65%. Moreover, the incremental technique has improved clustering time by 71.18%.},
  archive      = {J_ICV},
  author       = {Parham Hadikhani and Daphne Teck Ching Lai and Wee-Hong Ong},
  doi          = {10.1016/j.imavis.2024.104985},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104985},
  shortjournal = {Image Vis. Comput.},
  title        = {Flexible multi-objective particle swarm optimization clustering with game theory to address human activity discovery fully unsupervised},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Authenticating and securing healthcare records: A deep
learning-based zero watermarking approach. <em>ICV</em>, <em>145</em>,
104975. (<a href="https://doi.org/10.1016/j.imavis.2024.104975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security in medical records is critical to patient privacy and confidentiality. Digital Patient Records (DPR) hold sensitive information that can reveal a patient&#39;s health status and history. Their unauthorized access or exposure can lead to severe consequences, including identity theft, discrimination, and medical malpractice. Therefore, ensuring proper security measures is critical in protecting DPR and other medical records from breaches or unauthorized access. In this regard, a robust deep learning-based zero-watermarking approach is presented for authenticating and securing healthcare records . The carrier image is initially visibly marked with the hospital logo to identify ownership and prevent illegal duplication and forgery. The image mark is scrambled by applying the step space-filling curve method for improved security. In the final phase, Alexnet is used to extract the features of visibly marked carrier image. Further, NSST and SVD-based zero watermarking is implemented to conceal the scrambled mark within the features of visibly marked carrier images. It is essential for copyright protection since it establishes ownership while preventing the unauthorized use or dissemination of valuable medical research, images, and reports. The proposed framework has exhibited superior versatility, robustness, and imperceptibility compared to existing techniques with a maximum improvement of 47%.},
  archive      = {J_ICV},
  author       = {Ashima Anand and Jatin Bedi and Ashutosh Aggarwal and Muhammad Attique Khan and Imad Rida},
  doi          = {10.1016/j.imavis.2024.104975},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104975},
  shortjournal = {Image Vis. Comput.},
  title        = {Authenticating and securing healthcare records: A deep learning-based zero watermarking approach},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Arbitrary 3D stylization of radiance fields. <em>ICV</em>,
<em>145</em>, 104971. (<a
href="https://doi.org/10.1016/j.imavis.2024.104971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Stylization that creates stylized multi-view images is quite challenging, as it requires not only generating images which align with the desired style but also maintaining consistency across different perspectives. Most previous image style transfer methods focus on the 2D image domain and stylize each view independently, suffering from multi-view inconsistency. To tackle this challenging problem, we build on the neural radiance fields (NeRF) to stylize each 3D scene , as NeRF inherently ensures consistency across multiple perspectives, and has two sub-networks of geometry and appearance where appearance stylization cannot change the geometry. To enable arbitrary style transfer and more explicit and precise style adjustment, we introduce the CLIP model, which allows for style transfer based on either a text prompt or an arbitrary style image. We employ an ensemble of loss functions, of which CLIP loss ensures the similarity between the shared latent embeddings and generated style images, and Mask Loss is to constrain the 3D geometry to avoid non-smooth surface of NeRF. Experimental results demonstrate the effectiveness of our arbitrary 3D stylization generalized across diverse datasets. The proposed method outperforms most image-based and text-based 3D stylization models in terms of style transfer quality, producing pleasing images.},
  archive      = {J_ICV},
  author       = {Sijia Zhang and Ting Liu and Zhuoyuan Li and Yi Sun},
  doi          = {10.1016/j.imavis.2024.104971},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104971},
  shortjournal = {Image Vis. Comput.},
  title        = {Arbitrary 3D stylization of radiance fields},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RGB road scene material segmentation. <em>ICV</em>,
<em>145</em>, 104970. (<a
href="https://doi.org/10.1016/j.imavis.2024.104970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce RGB road scene material segmentation, i.e. , per-pixel segmentation of materials in real-world driving views with pure RGB images , as a novel computer vision task by building a benchmark dataset and by deriving a new method. Our dataset, KITTI-Materials, is based on the well-established KITTI dataset and consists of 1000 frames covering 24 different road scenes of urban/suburban landscapes, carefully annotated with one of 20 material categories for every pixel. It is the first dataset for RGB material segmentation in real driving scenes. Through careful analysis of KITTI-Materials, we identify the extraction and fusion of texture and image context as the key to accurate modeling of road scene material appearance. For this, we introduce R oad scene M aterial S egmentation Net work ( RMSNet ) as a baseline method for this challenging task. RMSNet encodes multi-scale hierarchical features with efficient Transformer layers. We construct the decoder of RMSNet based on a novel efficient self-attention model, which we refer to as SAMixer which adaptively fuses texture and context cues across multiple feature levels. Extensive experiments on KITTI-Materials validate the effectiveness of our RMSNet. We believe our work lays a solid foundation for further studies on RGB road scene material segmentation.},
  archive      = {J_ICV},
  author       = {Sudong Cai and Ryosuke Wakaki and Shohei Nobuhara and Ko Nishino},
  doi          = {10.1016/j.imavis.2024.104970},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104970},
  shortjournal = {Image Vis. Comput.},
  title        = {RGB road scene material segmentation},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient feature pyramid attention network for person
re-identification. <em>ICV</em>, <em>145</em>, 104963. (<a
href="https://doi.org/10.1016/j.imavis.2024.104963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For person re-identification, occlusion, appearance similarity and background clutter have always been challenges. In order to effectively address the challenges, we propose an efficient feature pyramid attention network (FPA-Net), which combines visual features from different levels to focus on both detail features and information. Specifically, we embed a pair of attention mechanisms that complement each other in the backbone network to focus on the discriminant features of person areas. In addition, we designed a novel feature pyramid structure, which propagates the feature information from the cross-level through the top feature to the bottom feature and from the bottom feature to the top feature to supplement the detail information of the feature. Finally, we integrate features form different scales through a lightweight transition block to generate more discriminant features. Our method performed experimental analysis on four datasets: Market-1501, DukeMTMC-ReID, CUHK03(Detected) and MSMT17. A large number of experimental results prove that the performance of the method is significantly better than the existing state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Qian Luo and Jie Shao and Wanli Dang and Chao Wang and Libo Cao and Tao Zhang},
  doi          = {10.1016/j.imavis.2024.104963},
  journal      = {Image and Vision Computing},
  month        = {5},
  pages        = {104963},
  shortjournal = {Image Vis. Comput.},
  title        = {An efficient feature pyramid attention network for person re-identification},
  volume       = {145},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MRFormer: Multiscale retractable transformer for medical
image progressive denoising via noise level estimation. <em>ICV</em>,
<em>144</em>, 104974. (<a
href="https://doi.org/10.1016/j.imavis.2024.104974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clear medical images are important for auxiliary diagnoses, but the images generated by various medical devices inevitably contain considerable noise. Although various models have been proposed for denoising , these methods ignore the fact that different types of medical images have different noise levels , which leads to unsatisfactory test results. In addition, collecting many medical images for training denoising models consumes many material resources. To address these issues, we formulate a progressive denoising architecture that contains preliminary and profound denoising. First, we construct a noise level estimation network to estimate the noise level via self-supervised learning and perform preliminary denoising with a dilated blind-spot network. Second, with the learned noise distribution, we synthesize noisy natural images to construct clean-noisy natural image pairs. Finally, we design a novel medical image denoising model for profound denoising by training these pairs. The proposed three-stage learning scheme and progressive denoising architecture not only solve the problem that the denoising model only adapts to a single noise level but also alleviate the lack of medical image pairs. Moreover, we integrate dense attention and sparse attention to constitute the retractable transformer module in the profound denoising model, which reconciles a wider receptive field and enhances the representation ability of the transformer, s allowing the denoising model to obtain retractable attention on the input feature and capture more local and global receptive fields simultaneously. The results of qualitative and quantitative experiments demonstrate the effectiveness of our method in removing noise at various levels.},
  archive      = {J_ICV},
  author       = {Can Bai and Xianjun Han},
  doi          = {10.1016/j.imavis.2024.104974},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104974},
  shortjournal = {Image Vis. Comput.},
  title        = {MRFormer: Multiscale retractable transformer for medical image progressive denoising via noise level estimation},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camouflaged object detection via cross-level refinement and
interaction network. <em>ICV</em>, <em>144</em>, 104973. (<a
href="https://doi.org/10.1016/j.imavis.2024.104973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of camouflaged object detection (COD) focuses on detecting objects that seamlessly blend into their surroundings. Camouflaged objects pose a substantial challenge in the realm of computer vision due to various factors, including occlusion, limited illumination, and diminutive dimensions. In this paper, we propose a cross-level refinement and interaction network (CRI-Net) to capture camouflaged objects. Specifically, we advance the concept of a semantic amplification module (SAM), which simulates human visual processes through multi-scale parallel convolution in a way of progressive aggregation with the aim of obtaining rich semantic information. Subsequently, we propose a cross-level refinement unit (CRU), which focuses on multivariate information at different levels in an attention-induced manner to facilitate the fusion refinement of features between levels and the exploration of feature similarity. Finally, we design a semantic-texture interaction module (SIM) to facilitate the interaction between high-level semantics and low-level textures while mining rich fine-grained spatial information to improve the integrity of camouflaged objects. By conducting comprehensive experiments on four benchmark camouflaged datasets, our CRI-Net demonstrates significantly superior performance compared to 20 cutting-edge competing methods.},
  archive      = {J_ICV},
  author       = {Yanliang Ge and Junchao Ren and Qiao Zhang and Min He and Hongbo Bi and Cong Zhang},
  doi          = {10.1016/j.imavis.2024.104973},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104973},
  shortjournal = {Image Vis. Comput.},
  title        = {Camouflaged object detection via cross-level refinement and interaction network},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-axis interactive multidimensional attention network
for vehicle re-identification. <em>ICV</em>, <em>144</em>, 104972. (<a
href="https://doi.org/10.1016/j.imavis.2024.104972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning fine-grained discriminative information is essential to address the challenges of small inter-class differences and large intra-class differences in vehicle re-identification (Re-ID). Attentional mechanism is often used to capture important global information in images rather than fine-grained discriminative information. Studies have shown that the multi-axis interaction of information can enhance the feature representation ability of networks. This paper explores how to use the multi-axis interaction of information to facilitate more effective learning of attention and how to capture important detailed information in local regions. We propose a multi-axis interactive multidimensional attention network (MIMA-Net) for vehicle Re-ID. The network allows information to interact on multiple axes and calibrates the weight distribution of features from multiple dimensions to learn subtle discriminative information in vehicle parts/regions. The window-channel attention module (W-CAM) in MIMA-Net facilitates the learning of channel attention by interacting first across locations and then across channels, while the channel group-spatial attention module (CG-SAM) facilitates the learning of spatial attention by interacting first across channels and then across locations. These two modules perform window partitioning in a priori manner and channel semantic aggregation in an adaptive manner to learn discriminative semantic features in parts, respectively. These two approaches complement each other to strengthen the feature representation ability of MIMA-Net. Extensive experiments on three large public datasets, VeRi-776, VehicleID, and VERI-Wild, verify the effectiveness of our MIMA-Net and show that our method achieves state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Xiyu Pang and Yanli Zheng and Xiushan Nie and Yilong Yin and Xi Li},
  doi          = {10.1016/j.imavis.2024.104972},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104972},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-axis interactive multidimensional attention network for vehicle re-identification},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved skin lesion detection solution using multi-step
preprocessing features and NASNet transfer learning model. <em>ICV</em>,
<em>144</em>, 104969. (<a
href="https://doi.org/10.1016/j.imavis.2024.104969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis has shown its potential for accurate detection of various diseases like skin lesion . Skin lesion has been recognized as a challenging task since manual identification through visual analysis of images can be inefficient, tedious, and error-prone. Although automatic diagnosis approaches are used to overcome this challenge, it is crucial to address problems such as variations in the size of images, presence of hairs in images, unsatisfactory schemes of colors, ruler markers, low-contrast, and differences in dimensions of lesions, and gel bubbles. Researchers in the field of dermatology pigmented lesion classification have proposed different methodologies to confront this issue. Specifically, they have focused on the binary classification problem of distinguishing Melanocytic lesions from normal ones. In this research, the dataset “MNIST HAM10000” is utilized, published by International Skin Image Collaboration, and contains data about 07 different skin cancer types. Moreover, in this research, we have focused on image preprocessing and skin lesion detection with NASNet model. Experimental reuslts demonstrated the superiority of the proposed model, which achieves an accuracy of 99.85%. This accomplishment has been made possible with the utilization of data augmentation techniques and multi-step image processing methods with the proposed NasNET model.},
  archive      = {J_ICV},
  author       = {Abdulaziz Altamimi and Fadwa Alrowais and Hanen Karamti and Muhammad Umer and Lucia Cascone and Imran Ashraf},
  doi          = {10.1016/j.imavis.2024.104969},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104969},
  shortjournal = {Image Vis. Comput.},
  title        = {An improved skin lesion detection solution using multi-step preprocessing features and NASNet transfer learning model},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear circumference-based robust ellipse detection in
low-SNR images. <em>ICV</em>, <em>144</em>, 104968. (<a
href="https://doi.org/10.1016/j.imavis.2024.104968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to present an effective method to detect ellipses on images with low signal-to-noise ratios (SNR). Firstly, we analyze four major interferences caused by low SNR images. A nonlinear circumference-based ellipse detection method is proposed, which uses a nonlinear circumference to vote on the parameter space constructed from input images and a spatial hierarchical search strategy to find the optimum ellipse parameters. The nonlinear circumference is designed to describe the possibility that selected edge points will generate a given ellipse. Experiments on six low SNR datasets show that our method outperforms five existing methods in terms of recall, precision and F-measure. Furthermore, a definition based on the nonlinear circumference is proposed to quantify SNR of synthetic images with ellipses. Experimental results demonstrate that our method can detect ellipses on images close to 0.2 dB while state-of-the-art methods can almost only reach 1.2 dB.},
  archive      = {J_ICV},
  author       = {Zhuoran Wang and Jianjun Yi and Hongkai Ding and Fei Zeng and Jinzhen Mu and Bin Wu},
  doi          = {10.1016/j.imavis.2024.104968},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104968},
  shortjournal = {Image Vis. Comput.},
  title        = {Nonlinear circumference-based robust ellipse detection in low-SNR images},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep feature fusion network with global context and
cross-dimensional dependencies for classification of mild cognitive
impairment from brain MRI. <em>ICV</em>, <em>144</em>, 104967. (<a
href="https://doi.org/10.1016/j.imavis.2024.104967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objectives : The accurate identification of people with Mild Cognitive Impairment (MCI) who may develop Alzheimer&#39;s disease (AD) holds significant importance in facilitating timely intervention and treatment. However, current classification methods have yet to be effective due to the subtle nature of the features involved. This research aims to improve the performance of the MCI classification by enhancing the feature representation in brain MRI. We propose an integrated model that combines Group Shuffle Depth-wise Convolution (GSDW), Global Context Network (GCN), Hybrid Multi-Focus Attention Block (HMAB), and EfficientNet-B0 architecture for MCI classification. This model extracts fine-grained features, contextual information, and long-range dependencies and aggregates low-level details with high-level context information to learn discriminative features . Our comprehensive evaluation demonstrates significant improvements in the classification performance for both progressive MCI (pMCI) and stable MCI (sMCI), surpassing existing approaches. The proposed model achieved a notable accuracy of 77.2%. This study introduces a novel feature fusion technique that combines global contextual representations and cross-dimensional dependencies to enhance classification results . These findings highlight the potential of the proposed framework for early identification and intervention in individuals at risk of cognitive decline.},
  archive      = {J_ICV},
  author       = {T. Illakiya and R. Karthik and For the Alzheimer&#39;s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.imavis.2024.104967},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104967},
  shortjournal = {Image Vis. Comput.},
  title        = {A deep feature fusion network with global context and cross-dimensional dependencies for classification of mild cognitive impairment from brain MRI},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved YOLOv7 models based on modulated deformable
convolution and swin transformer for object detection in fisheye images.
<em>ICV</em>, <em>144</em>, 104966. (<a
href="https://doi.org/10.1016/j.imavis.2024.104966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the wide view field, the fisheye camera can get much more visual information. Thus, it is widely used in the field of computer vision . However, projection is often required for fisheye images to be used for object detection. Meanwhile, the projection will lead to distortion in fisheye images, and the discontinuous image edges will make the objects incomplete. Fisheye images are characterized by objects that are large near and small far. These problems are still challenges for the existing advanced object detector YOLOv7. Therefore, in this paper, we propose an improved YOLOv7 model. First, Modulated Deformable Convolution is introduced into the YOLOv7 model to automatically adapt to distortion changes of distorted objects in fisheye images. It not only adjusts the sampling position of the convolutional kernel but also further extends the deformation range. The improved model can efficiently extract features of distorted and edge-discontinuous objects. In addition, fisheye images are characterized by objects close to the fisheye lens being large, while objects farther away from the fisheye lens will be smaller. To further optimize the detection performance of small objects in fisheye images, Swin Transformer is also introduced into the YOLOv7 model, and Swin Transformer Block with Window Multi-head Self-Attention (W-MSA) Effectively enhances Network Local Perception. Finally, our proposed model achieves up to 2.4% improvement in mAP compared to the original YOLOv7 model on the ERP-360 dataset. Also, the proposed model achieves the best results compared to other state-of-the-art object detection methods for equirectangular projection images. On the VOC-360 dataset, our proposed model improves the mAP by up to 5.9% compared to the original YOLOv7 model. The experimental results show that the proposed models achieve good results for object detection in both fisheye images and equirectangular projection images. The ERP-360 dataset, source code and pre-trained models for related tasks can be found at https://github.com/xiaoxiaomichong/ERP-360dataset .},
  archive      = {J_ICV},
  author       = {Jie Zhou and Degang Yang and Tingting Song and Yichen Ye and Xin Zhang and Yingze Song},
  doi          = {10.1016/j.imavis.2024.104966},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104966},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved YOLOv7 models based on modulated deformable convolution and swin transformer for object detection in fisheye images},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image captioning: Semantic selection unit with stacked
residual attention. <em>ICV</em>, <em>144</em>, 104965. (<a
href="https://doi.org/10.1016/j.imavis.2024.104965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic information and attention mechanism play important roles in the task of image captioning. Semantic information can strengthen the relationship between images and languages, while attention operation can steer the relevant regions spatially in the image. However, in most current works, semantic attributes are always confined to be learned from pairs of images and sentences, which ignore to fully utilize more semantic attributes and the structure information of sentences, thus limit the variety of sentences to be generated. Meanwhile, current attention models usually lack the ability to learn the positional information in an explicit way during attention generation, and have the problem of vanishing gradient in the training process. This paper proposes a Semantic Selection Unit (SSU) and a Stacked Residual Attention (SRA) to remedy these drawbacks. Specifically, the SSU is designed to capture selectively semantic information from expanding attributes or guidance sentences. With the help of expanding vocabulary and the structure information in sentences, the SSU can improve the quality of the generated sentences. The SRA is constructed to solve the problem of positional information missing and vanishing gradient problem during attention generation. Architecturally, the SSU and SRA work together in a jointed framework with end-to-end learning for image captioning. Extensive experiments have been conducted on the public dataset of the MS COCO, achieving 139.7 CIDEr score on the test set.},
  archive      = {J_ICV},
  author       = {Lifei Song and Fei Li and Ying Wang and Yu Liu and Yuanhua Wang and Shiming Xiang},
  doi          = {10.1016/j.imavis.2024.104965},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104965},
  shortjournal = {Image Vis. Comput.},
  title        = {Image captioning: Semantic selection unit with stacked residual attention},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-object tracking with adaptive measurement noise and
information fusion. <em>ICV</em>, <em>144</em>, 104964. (<a
href="https://doi.org/10.1016/j.imavis.2024.104964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is a challenging task in computer vision that aims to estimate the trajectories of multiple objects in a video sequence. Observation-Centric SORT (OCSORT) is a pure motion-based MOT algorithm that uses the Kalman filter as the motion model and three observation-centric techniques: Re-Update, Momentum and Recovery, to enhance the data association. However, OCSORT is limited by camera motion error, constant measurement noise and lack of appearance information. In this paper, we propose three methods to address these limitations and improve the performance of OCSORT. First, we use Enhanced Correlation Coefficient Maximization (ECC) to compensate for the camera motion between adjacent frames. Second, we adjust the measurement noise scale for the Kalman filter according to the detection confidence. Third, we introduce a deep visual feature model to extract appearance information and propose a method to effectively use both motion and appearance information. The proposed method first filters out the inappropriate appearance information based on motion information and then combines the filtered appearance information with the motion information by minimization. We evaluate our algorithm on three MOT benchmarks: MOT17, MOT20 and DanceTrack. The results show that our algorithm achieves state-of-the-art performance on all datasets, especially on DanceTrack, where the objects have highly nonlinear motion and frequent occlusion. Compared to OCSORT, our algorithm improves Higher Order Tracking Accuracy (HOTA) by 1.1%, 0.8%, and 3.5%, and ID F1 Score (IDF1) by 1.7%, 1.9%, and 4.3% on MOT17, MOT20 and DanceTrack, respectively.},
  archive      = {J_ICV},
  author       = {Xi Huang and Yinwei Zhan},
  doi          = {10.1016/j.imavis.2024.104964},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104964},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-object tracking with adaptive measurement noise and information fusion},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). C2F: An effective coarse-to-fine network for video
summarization. <em>ICV</em>, <em>144</em>, 104962. (<a
href="https://doi.org/10.1016/j.imavis.2024.104962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of video summarization is to develop a concise and condensed summary that accurately captures the original video content. The methods currently used to summarize supervised videos and consider the task a sequence-to-sequence problem. However, modeling the order of long videos presents three challenges: (1) capturing both local and global relationships simultaneously is challenging; (2) the boundaries of video highlight segments are often incorrectly located, indicating that semantic integrity is incomplete; (3) efficient relation computing is difficult to do well. We design a novel coarse-to-fine network (C2F) for video summarization adapted to the multi-level semantic video structure, thus addressing these limitations. The multiscale representation scheme initially captures different scales of temporal relationships for the coarse classification results ; Meanwhile, the action-wise proposal module is intended to provide the fine prediction of importance scores and regress the temporal locations of key-frames. In addition, a loss function is proposed to identify local differences among frames and analyze combinations of various loss functions. Extensive experimental results on two benchmark datasets have demonstrated that the proposed C2F achieves significant performance compared with state-of-the-art methods, and performs satisfactorily in efficient relation computing. For example, on the TVSum dataset, we improve the F F -score from 69.4% to 72.8% by 3.4%. Furthermore, C2F includes 4.7 M parameters, accounting for only 10.7% of the parameters used in the SASUM model.},
  archive      = {J_ICV},
  author       = {Ye Jin and Xiaoyan Tian and Zhao Zhang and Peng Liu and Xianglong Tang},
  doi          = {10.1016/j.imavis.2024.104962},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104962},
  shortjournal = {Image Vis. Comput.},
  title        = {C2F: An effective coarse-to-fine network for video summarization},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaze analysis: A survey on its applications. <em>ICV</em>,
<em>144</em>, 104961. (<a
href="https://doi.org/10.1016/j.imavis.2024.104961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The examination of ocular movements has a wide range of applications due to the current developments in sensors that are now able to collect this biometric . This type of investigation is known as “gaze analysis”. The gaze has successfully examined a subject&#39;s physical and mental status in the past. As a result, over the last few decades, a large and diverse amount of literature on this subject has been generated and presented. The aim of this study is to collect and debate current gaze analysis methods based on their application field. Due to the context-specific needs for performance and efficiency, the eye movements under research are frequently evaluated from completely distinct perspectives. As a result, a collection of data, methods, and discussions ranging from the medical community to virtual and augmented reality , as well as human computer interface and remote learning , has been produced. In addition to providing a peek of novel observation on the issue of gaze analysis, the gaps between and within areas are also discussed to provide points for researchers to pursue.},
  archive      = {J_ICV},
  author       = {Carmen Bisogni and Michele Nappi and Genoveffa Tortora and Alberto Del Bimbo},
  doi          = {10.1016/j.imavis.2024.104961},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104961},
  shortjournal = {Image Vis. Comput.},
  title        = {Gaze analysis: A survey on its applications},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust visual tracking via modified harris hawks
optimization. <em>ICV</em>, <em>144</em>, 104959. (<a
href="https://doi.org/10.1016/j.imavis.2024.104959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its outstanding efficiency and high precision, Harris hawks optimization (HHO for short) is suitable for solving the problem of visual target tracking under conditions of occlusion, deformation, rotation and in other complicated tracking scenes. A visual target tracker based on HHO is proposed in this study. To further promote the efficiency and stability of the standard HHO method and reduce the probability of the iteration falling into local optima and algorithm prematurity, in this study we propose an improved method called Super-HHO and apply it to visual target tracking . Compared with standard HHO, Super-HHO is superior due to its parameter optimization and updating strategy. We first optimize the random parameters of HHO via chaos theory to avoid frequent repeated exploration of the feasible region . Next, we design a nonlinear renewal strategy for the escape energy, which solves the problem in traditional HHO in which the fixed escape energy cannot accurately reflect the real hunting process of Harris hawks. Mutation strategies are also designed for the locations of the prey and the hunters to improve the optimization ability and eliminate the risk of falling into local extremes. In addition, a frame scale adjustment method model is developed to address the the issue in which the use of a size-fixed tracking frame makes it easy to include too many invalid features, which reduces the efficiency. Finally, we use the OTB2015, and VOT2018 tracking evaluation datasets, which contain hundreds of visual sequences and more than 10 complex interference scenes to conduct a qualitative analysis, a quantitative analysis and a statistical analysis of ours and other classic trackers, and to effectively test and compare the success ratio, precision and stability of each tracker. The proposed method was also compared with other classic trackers using classic large-scale benchmarks such as LaSOT and TrackingNet. Experimental data prove that ours performs well in terms of robustness, precision and efficiency.},
  archive      = {J_ICV},
  author       = {Yuqi Xiao and Yongjun Wu},
  doi          = {10.1016/j.imavis.2024.104959},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104959},
  shortjournal = {Image Vis. Comput.},
  title        = {Robust visual tracking via modified harris hawks optimization},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PTET: A progressive token exchanging transformer for
infrared and visible image fusion. <em>ICV</em>, <em>144</em>, 104957.
(<a href="https://doi.org/10.1016/j.imavis.2024.104957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating complementary information from different modalities is one of the key challenges in image fusion. Most of the existing deep learning-based methods still rely on a one-off fusion layer to integrate the features extracted from two modalities into one. Such an information interaction pattern only considers significant feature integration but neglects the removal of hazardous information that is widely present in the source images. To overcome these limitations, we propose a progressive token exchanging Transformer for infrared and visible image fusion, named PTET. Different from the one-time fusion layer, we devise a progressive token exchange strategy to gradually transfer features from source images and remove harmful information simultaneously. A predictor is utilized to assess the saliency of Transformer tokens from both modalities. Afterwards, an exchanger is designed to perform beneficial token transfer and insignificant token elimination. Through the cascading layers, our network enhances the feature of fusion branch in a progressive manner. Innovative exchange loss and rank loss are introduced to constrain the fusion network. Extensive experiments on MSRS and LLVIP datasets demonstrate the superiority of our PTET compared to nine state-of-the-art alternatives. Visualization of token exchanging strategy and ablation study reveals the effectiveness of our designs.},
  archive      = {J_ICV},
  author       = {Jun Huang and Ziang Chen and Yong Ma and Fan Fan and Linfeng Tang and Xinyu Xiang},
  doi          = {10.1016/j.imavis.2024.104957},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104957},
  shortjournal = {Image Vis. Comput.},
  title        = {PTET: A progressive token exchanging transformer for infrared and visible image fusion},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention guided multi-level feature aggregation network for
camouflaged object detection. <em>ICV</em>, <em>144</em>, 104953. (<a
href="https://doi.org/10.1016/j.imavis.2024.104953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) aims to identify objects that are visually blended into their highly similar surroundings, which is an extremely complex and challenging visual task in real-world scenarios, and has recently attracted increasing research interest in the field of computer vision due to its valuable applications. The existing deep learning based methods of COD have the following problems: 1) the ambiguous boundary of the camouflaged objects in prediction map, 2) an inaccurate detection of the camouflaged object with accurate and complete structure details. To this end, an attention guided multi-level feature aggregation network is proposed for this task, which is based on three key designs. First, by embedding spatial pyramid attention (SPA) in ResNet-like backbone network , better multi-level features are extracted to identify the camouflaged objects with complete internal details. Second, an edge context module is designed to make full use of edge information , which highlights camouflaged object structure and generates accurate edge localization of COD. Third, the feature aggregation module based on local attention is used to fuse these enhanced multi-level features and edge context cues, which consists of two major components: the iterative Attentional Feature Fusion (iAFF) module and the Dual-branch Global Context Module (DGCM). Compared with the existing 16 state-of-the-art methods, extensive experiments on four widely-used benchmark datasets under four authoritative evaluation metrics illustrate that the proposed method is very beneficial to the COD task.},
  archive      = {J_ICV},
  author       = {Anzhi Wang and Chunhong Ren and Shuang Zhao and Shibiao Mu},
  doi          = {10.1016/j.imavis.2024.104953},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104953},
  shortjournal = {Image Vis. Comput.},
  title        = {Attention guided multi-level feature aggregation network for camouflaged object detection},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POSER: POsed vs spontaneous emotion recognition using
fractal encoding. <em>ICV</em>, <em>144</em>, 104952. (<a
href="https://doi.org/10.1016/j.imavis.2024.104952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition from facial expressions is a fundamental human ability that can be harnessed and transferred to machines. The ability to differentiate between spontaneous and posed emotions holds significant importance in various domains, including behavioral biometrics , forensics, and security. This paper introduces a novel method, called POsed vs Spontaneous Emotion Recognition (POSER), which leverages a modified version of the Partitioned Iterated Functions System (PIFS) to obtain a Fractal Encoding. This encoding is used for the first time as facial features to train a machine learning approach for the classification of emotions as either spontaneous or posed. Furthermore, by adapting the original architecture, we demonstrate the effectiveness of these features in distinguishing seven different emotions in controlled as well as wild environments, within a framework referred to as POSER-EMO. Experimental results are presented on the SPOS and DISFA + datasets for the first classification problem, where POSER outperforms the state of the art, and on the CK + and SFEW datasets for the second classification problem.},
  archive      = {J_ICV},
  author       = {Carmen Bisogni and Lucia Cascone and Michele Nappi and Chiara Pero},
  doi          = {10.1016/j.imavis.2024.104952},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104952},
  shortjournal = {Image Vis. Comput.},
  title        = {POSER: POsed vs spontaneous emotion recognition using fractal encoding},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-depth branch network for efficient image
super-resolution. <em>ICV</em>, <em>144</em>, 104949. (<a
href="https://doi.org/10.1016/j.imavis.2024.104949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A longstanding challenge in Super-Resolution (SR) is how to efficiently enhance high-frequency details in Low-Resolution (LR) images while maintaining semantic coherence. This is particularly crucial in practical applications where SR models are often deployed on low-power devices. To address this issue, we propose an innovative asymmetric SR architecture featuring Multi-Depth Branch Module (MDBM). These MDBMs contain branches of different depths, designed to capture high- and low-frequency information simultaneously and efficiently. The hierarchical structure of MDBM allows the deeper branch to gradually accumulate fine-grained local details under the contextual guidance of the shallower branch. We visualize this process using feature maps, and further demonstrate the rationality and effectiveness of this design using proposed novel Fourier spectral analysis methods. Moreover, our model exhibits more significant spectral differentiation between branches than existing branch networks. This suggests that MDBM reduces feature redundancy and offers a more effective method for integrating high- and low-frequency information. Extensive qualitative and quantitative evaluations on various datasets show that our model can generate structurally consistent and visually realistic HR images. It achieves state-of-the-art (SOTA) results at a very fast inference speed. Our code is available at https://github.com/thy960112/MDBN .},
  archive      = {J_ICV},
  author       = {Huiyuan Tian and Li Zhang and Shijian Li and Min Yao and Gang Pan},
  doi          = {10.1016/j.imavis.2024.104949},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104949},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-depth branch network for efficient image super-resolution},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature decoupling and interaction network for defending
against adversarial examples. <em>ICV</em>, <em>144</em>, 104931. (<a
href="https://doi.org/10.1016/j.imavis.2024.104931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, it was found that deep neural networks (DNNs) are susceptible to adversarial input perturbations. Most defense strategies adopt the denoising method based on preprocessing, which mitigates the impacts of adversarial perturbations on DNNs by learning the distributions of nonadversarial datasets and projecting adversarial inputs into the learned nonadversarial manifolds. However, existing defense strategies commonly focus on reconstructing clean images while ignoring the role of adversarial perturbations, which results in the reconstructed images failing to achieve the visual quality and classification accuracy of the original clean images, and the induced adversarial robustness improvement is limited. This paper proposes a feature decoupling-interaction network (FDIN), which introduces the concepts of clean features and adversarial features to separate the two kinds of features from the input adversarial examples (AEs) in a feature decoupling-interaction manner. The clean features are used to reconstruct the input image so that it is infinitely close to the original clean image, and the adversarial features are used to reconstruct the adversarial perturbations. Adversarial perturbations are removed from the adversarial examples across multiple cross cycles to improve further the reconstructed image&#39;s visual quality and classification accuracy . The features of the original clean image are used as prior knowledge to guide the network to learn the clean features of the adversarial examples and improve the classification accuracy of the model on the clean examples. In addition, a classification loss function based on the Carlini &amp; Wagner (CW) attack algorithm is used instead of the conventional cross-entropy loss function to improve the adversarial robustness of the FDIN. The experimental results show that the proposed method achieves better defense performance than the current state-of-the-art methods on both standard tests and various attack tests and even exceeds the test accuracy of the target classifier on the original test set.},
  archive      = {J_ICV},
  author       = {Weidong Wang and Zhi Li and Shuaiwei Liu and Li Zhang and Jin Yang and Yi Wang},
  doi          = {10.1016/j.imavis.2024.104931},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104931},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature decoupling and interaction network for defending against adversarial examples},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A point-2s reinforcement learning biomimetic model for
estimating and analyzing human 3D motion posture. <em>ICV</em>,
<em>144</em>, 104927. (<a
href="https://doi.org/10.1016/j.imavis.2024.104927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid progress of computer vision and artificial intelligence , the accuracy of estimating and analyzing human body movements and postures has always been a highly focused research field. However, current methods still have some shortcomings in accurately estimating the pose of 3D human movements. This study aims to propose an effective method to accurately estimate the motion posture of 3D human activities using new technologies of deep learning and neural networks. Firstly, based on previous research, this paper analyzes the problems and challenges of insufficient accuracy in current 3D human motion pose estimation methods, limited capture of 3D spatial information in deep video data, and inability to capture subtle motion details. Then, in response to the problem of disorder in point clouds, an innovative SMPL human Point-2 s reinforcement learning framework was constructed using the VIBE network to estimate the pose of RGB in the NTU dataset. 24 Point-2 s biomimetic algorithm joints were sampled from a distance using the FPS of PointNet ++in SMPL, and an index was established based on relative positions to ensure that other joint points are in the same position. Finally, these joint points were input into the 2 s-AGCN network to construct a complete Point-2 s model. The research results indicate that the proposed Point-2 s model has achieved good results in accurately estimating the motion posture of 3D human activities, and effectively solves the problem of disorder in point clouds converted from deep videos. Compared to traditional methods, this research model has significantly improved accuracy and stability. The practical application of this method will provide useful references and guidance for research and method development in related fields.},
  archive      = {J_ICV},
  author       = {Yubo Wang},
  doi          = {10.1016/j.imavis.2024.104927},
  journal      = {Image and Vision Computing},
  month        = {4},
  pages        = {104927},
  shortjournal = {Image Vis. Comput.},
  title        = {A point-2s reinforcement learning biomimetic model for estimating and analyzing human 3D motion posture},
  volume       = {144},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BPMB: BayesCNNs with perturbed multi-branch structure for
robust facial expression recognition. <em>ICV</em>, <em>143</em>,
104960. (<a href="https://doi.org/10.1016/j.imavis.2024.104960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wild Facial Expression Recognition (FER) task has been a long-standing challenge due to the various forms of uncertainty exist in expression data. When expression data is fed into a convolutional neural network (CNN), the model&#39;s estimated parameters also become uncertain. This uncertainty gives rise to concerns regarding the reliability of the recognition results. To quantify these uncertainties and achieve robust performance in the presence of noisy data, this paper introduces a novel model for Wild Facial Expression Recognition: the Bayesian Convolutional Neural Network with Perturbed Multi-Branch Structure (BPMB). This model aims to address uncertainty issues, enabling the network&#39;s decisions to become more deterministic with increasing training accuracy. Specifically, BPMB incorporates variational inference (VI) to introduce a probability distribution for the weights. A variational approximation to the true posterior is derived using Bayes by Backprop, involving two convolution operations : one for classification and another for quantifying uncertainty. Furthermore, an exploration is conducted into a lightweight multi-branch structure that leverages Dropout as a random generator to introduce perturbations during the training process, enhancing the model&#39;s robustness while extracting deep features. Extensive experiments validate the superiority of the proposed BPMB algorithm over the majority of existing mainstream algorithms on three widely utilized wild datasets.},
  archive      = {J_ICV},
  author       = {Shuaishi Liu and Dongxu Zhao and Zhongbo Sun and Yuekun Chen},
  doi          = {10.1016/j.imavis.2024.104960},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104960},
  shortjournal = {Image Vis. Comput.},
  title        = {BPMB: BayesCNNs with perturbed multi-branch structure for robust facial expression recognition},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Explicit knowledge transfer of graph-based correlation
distillation and diversity data hallucination for few-shot object
detection. <em>ICV</em>, <em>143</em>, 104958. (<a
href="https://doi.org/10.1016/j.imavis.2024.104958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of few-shot object detection has seen marked improvement through fine-tuning paradigms. However, existing methods often depend on shared parameters to implicitly transfer knowledge without explicit induction. This results in novel-class representations that are easily confused with similar base classes and poorly suited to diverse patterns of variation in the truth distribution. In view of this, the present paper focuses on mining transferable base-class knowledge, which is further subdivided into inter-class correlation and intra-class diversity. First, we design a graph to dynamically capture the relationship between base and novel class representations, and then introduce distillation techniques to tackle the shortage of correlation knowledge in few-shot labels. Furthermore, an efficient diversity knowledge transfer module based on the data hallucination is proposed, which can adaptively disentangle class-independent variation patterns from base-class features and generate additional trainable hallucinated instances for novel classes. Experiments on VOC and COCO datasets confirmed that our proposed method effectively reduces the reliance on novel-class samples and demonstrates superior performance compared to other state-of-the-art baseline methods .},
  archive      = {J_ICV},
  author       = {Meng Wang and Yang Wang and Haipeng Liu},
  doi          = {10.1016/j.imavis.2024.104958},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104958},
  shortjournal = {Image Vis. Comput.},
  title        = {Explicit knowledge transfer of graph-based correlation distillation and diversity data hallucination for few-shot object detection},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Foreground and background separated image style transfer
with a single text condition. <em>ICV</em>, <em>143</em>, 104956. (<a
href="https://doi.org/10.1016/j.imavis.2024.104956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image-based style transfer requires additional reference style images, making it less user-friendly. Text-based methods are more convenient but suffer from issues like slow generation, unclear content, and poor quality. In this work, we propose a new style transfer method SA2-CS (means Semantic-Aware and Salient Attention CLIPStyler), which is based on the Comparative Language Image Pretraining (CLIP) model and a salient object detection network. Masks obtained from the salient object detection network are utilized to guide the style transfer process, and various strategies are employed to optimize according to different masks. Adequate experiments with diverse content images and style text descriptions were conducted, demonstrating our method&#39;s advantages: the network is easily trainable and converges rapidly; it achieves stable, superior generation results compared to other methods. Our approach addresses over-stylization issues in the foreground, enhances foreground-background contrast, and enables precise control over style transfer in various semantic regions.},
  archive      = {J_ICV},
  author       = {Yue Yu and Jianming Wang and Nengli Li},
  doi          = {10.1016/j.imavis.2024.104956},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104956},
  shortjournal = {Image Vis. Comput.},
  title        = {Foreground and background separated image style transfer with a single text condition},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio-visual saliency prediction with multisensory
perception and integration. <em>ICV</em>, <em>143</em>, 104955. (<a
href="https://doi.org/10.1016/j.imavis.2024.104955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio-visual saliency prediction (AVSP) is a task that aims to model human attention patterns in the perception of auditory and visual scenes. Given the challenges associated with perceiving and combining multi-modal saliency features from videos, this paper presents a multi-sensory framework for AVSP. This framework is designed to extract audio, motion and image saliency features and integrate them effectively, which can then serve as a general architecture for the AVSP task. To obtain multi-sensory information, we develop a three-stream encoder that extracts audio, motion and image saliency features. In particular, we utilize a pre-trained encoder with knowledge related to image saliency to extract saliency features for each frame. The image saliency features are then incorporated with motion features using a spatial attention module. For motion features, 3D convolutional neural networks (CNNs) like S3D are commonly used in AVSP models. However, these networks are unable to effectively capture the global motion relationship in videos. To tackle this problem, we incorporate Transformer- and MLP-based motion encoders into the AVSP models. To learn joint audio-visual representations, an audio-visual fusion block is exploited to enhance the correlation between audio and visual motion features under the supervision of a cosine similarity loss in a self-supervised manner. Finally, a multi-stage decoder integrates audio, motion and image saliency features to generate the final saliency map. We evaluate our methods on six audio-visual eye-tracking datasets. Experimental results demonstrate that our method achieves compelling performance compared to the state-of-the-art methods. The source code is available at https://github.com/oraclefina/MSPI .},
  archive      = {J_ICV},
  author       = {Jiawei Xie and Zhi Liu and Gongyang Li and Yingjie Song},
  doi          = {10.1016/j.imavis.2024.104955},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104955},
  shortjournal = {Image Vis. Comput.},
  title        = {Audio-visual saliency prediction with multisensory perception and integration},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-probability sampling network based on anomaly pedestrian
trajectory discrimination for pedestrian trajectory prediction.
<em>ICV</em>, <em>143</em>, 104954. (<a
href="https://doi.org/10.1016/j.imavis.2024.104954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction in first-person view is an important support for achieving fully automated driving in cities. However, existing pedestrian trajectory prediction methods still have significant shortcomings in terms of pedestrian trajectory diversity, dynamic scene constraints, and dependence on long-term trajectory prediction. We proposes a non-probability sampling network based on pedestrian trajectory anomaly recognition (ADsampler) to predict multiple possible future pedestrian trajectories. First, by incorporating pose and optical flow information, ADsampler models the multi-dimensional motion characteristics of pedestrians based on observed trajectory information and discriminates trajectory states . The sampling range in the Gaussian latent space is determined based on the recognition results. Next, velocity and yaw information of the car are introduced to model the car&#39;s motion state. A subtraction fusion network is employed to remove redundant image feature constraints in highly dynamic scenes. Finally, ADsampler utilizes a novel trajectory decoding network that combines the position encoding capability of GRU with the long-term dependency capturing ability of Transformer to decode and predict the fused features. we evaluate our model on crowded videos in the public datasets JAAD, PIE, ETH and UCY. Experiments demonstrate that the proposed method outperforms state-of-the-art approaches in prediction accuracy.},
  archive      = {J_ICV},
  author       = {Quankai Liu and Haifeng Sang and Jinyu Wang and Wangxing Chen and Yulong Liu},
  doi          = {10.1016/j.imavis.2024.104954},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104954},
  shortjournal = {Image Vis. Comput.},
  title        = {Non-probability sampling network based on anomaly pedestrian trajectory discrimination for pedestrian trajectory prediction},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple object detection and tracking from drone videos
based on GM-YOLO and multi-tracker. <em>ICV</em>, <em>143</em>, 104951.
(<a href="https://doi.org/10.1016/j.imavis.2024.104951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking in drone videos is a vital vision task with broad application prospects, but most trackers use spatial or appearance clues alone to correlate detections. Our proposed Multi-Tracker uses a novel similarity measure that combines position and appearance information. We designed the GM-YOLO network to provide high-quality detections as input to Multi-Tracker. Add a Coordinate Attention mechanism and a weighted Bidirectional Feature Pyramid Network structure to the Backbone, each feature point&#39;s effective receptive field is modeled as a Gaussian distribution . To accurately obtain the motion and appearance features of the object, the adaptive noise covariance Kalman filter is used to get the position information, MB-OSNet network is designed to use global features to learn contour information to retrieve images from a wider field of view while incorporating Part-Level elements that contain more fine-grained data. Finally, the motion and appearance features are jointly compared to realize multi object tracking. The performance of the GM-YOLO object detector and the Multi-Tracker was verified on the VisDrone MOT and UAVDT datasets.},
  archive      = {J_ICV},
  author       = {Yubin Yuan and Yiquan Wu and Langyue Zhao and Huixian Chen and Yao Zhang},
  doi          = {10.1016/j.imavis.2024.104951},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104951},
  shortjournal = {Image Vis. Comput.},
  title        = {Multiple object detection and tracking from drone videos based on GM-YOLO and multi-tracker},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CVAD-GAN: Constrained video anomaly detection via generative
adversarial network. <em>ICV</em>, <em>143</em>, 104950. (<a
href="https://doi.org/10.1016/j.imavis.2024.104950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of abnormal behavior in video sequences is a fundamental and challenging problem for intelligent video surveillance systems . However, the existing state-of-the-art Video Anomaly Detection (VAD) methods are computationally expensive and lack the desired robustness in real-world scenarios. The contemporary VAD methods cannot detect the fundamental features absent during training, which usually results in a high false positive rate while testing. To this end, we propose a Constrained Generative Adversarial Network (CVAD-GAN) for real-time VAD. Adding white Gaussian noise to the input video frame with constrained latent space of CVAD-GAN improves its fine-grained features learning from the normal video frames. Also, the dilated convolution layers and skip-connection preserve the information across layers to understand the broader context of complex video scenes in real-time. Our proposed approach achieves a higher Area Under Curve (AUC) score and a lower Equal Error Rate (EER) with enhanced computational efficiency than the existing state-of-the-art VAD methods. CVAD-GAN achieves an AUC and EER score of 98.0% and 6.0% on UCSD Peds1, 97.8% and 7.0% on UCSD Peds2, 94.0% and 8.1% on CUHK Avenue, and 76.2% and 21.7% on ShanghaiTech dataset, respectively. Also, it detects 63 and 19 abnormal events, with false alarms of 3 and 1, respectively, on the Subway-Entry and Subway-Exit datasets. The source code to replicate the results of the proposed CVAD-GAN is available at https://github.com/Rituraj-ksi/CVAD-GAN .},
  archive      = {J_ICV},
  author       = {Rituraj Singh and Anikeit Sethi and Krishanu Saini and Sumeet Saurav and Aruna Tiwari and Sanjay Singh},
  doi          = {10.1016/j.imavis.2024.104950},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104950},
  shortjournal = {Image Vis. Comput.},
  title        = {CVAD-GAN: Constrained video anomaly detection via generative adversarial network},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote sensing scene classification using multi-domain
sematic high-order network. <em>ICV</em>, <em>143</em>, 104948. (<a
href="https://doi.org/10.1016/j.imavis.2024.104948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, convolutional neural networks (CNNs), which obtain powerful deep features in an end-to-end manner, have achieved powerful performance in remote sensing scene classification. However, the average or maximum pooling operations defined in the spatial domain and coarser-resolution features with high levels cannot extract reliable features and clear boundaries for small-scale targets in remote sensing scene imagery. This paper attempts to address these problems and proposes a multi-domain sematic high-order network for scene classification, named MSHNet. First, wavelet-spatial and detachable pooling blocks defined in the wavelet and spatial domains are inserted at the end of the convolutional block to learn the features in a more structural fusion manner. Second, multi-scale and multi-resolution semantic embedding modules are proposed to take full advantage of the complementary information and effectively maintain the spatial structures of learned deep features. Third, we employ a factorized bilinear coding approach to obtain compact and discriminative second-order features. MSHNet is thoroughly evaluated on two publicly available benchmarks, i.e., AID (Aerial Image Dataset) and NWPU-RESISC45 (Northwestern Polytechnical University-Remote Sensing Image Scene Classification 45). The extensive results illustrate that our MSHNet is competitive with other related multi-scale deep neural networks .},
  archive      = {J_ICV},
  author       = {Yuanyuan Lu and Yanhui Zhu and Hao Feng and Yang Liu},
  doi          = {10.1016/j.imavis.2024.104948},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104948},
  shortjournal = {Image Vis. Comput.},
  title        = {Remote sensing scene classification using multi-domain sematic high-order network},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ECT: Fine-grained edge detection with learned cause tokens.
<em>ICV</em>, <em>143</em>, 104947. (<a
href="https://doi.org/10.1016/j.imavis.2024.104947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we tackle the challenging fine-grained edge detection task, which refers to predicting specific edges caused by reflectance, illumination, normal, and depth changes, respectively. Prior methods exploit multi-scale convolutional networks , which are limited in three aspects: (1) Convolutions are local operators while identifying the cause of edge formation requires looking at far away pixels. (2) Priors specific to edge cause are fixed in prediction heads. (3) Using separate networks for generic and fine-grained edge detection, and the constraint between them may be violated . To address these three issues, we propose a two-stage transformer-based network sequentially predicting generic edges and fine-grained edges, which has a global receptive field thanks to the attention mechanism . The prior knowledge of edge causes is formulated as four learnable cause tokens in a cause-aware decoder design. Furthermore, to encourage the consistency between generic edges and fine-grained edges, an edge aggregation and alignment loss is exploited. We evaluate our method on the public benchmark BSDS-RIND and several newly derived benchmarks, and achieve new state-of-the-art results. Our code, data, and models are publicly available at https://github.com/Daniellli/ECT.git .},
  archive      = {J_ICV},
  author       = {Shaocong Xu and Xiaoxue Chen and Yuhang Zheng and Guyue Zhou and Yurong Chen and Hongbin Zha and Hao Zhao},
  doi          = {10.1016/j.imavis.2024.104947},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104947},
  shortjournal = {Image Vis. Comput.},
  title        = {ECT: Fine-grained edge detection with learned cause tokens},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gated contextual transformer network for multi-modal retinal
image clinical description generation. <em>ICV</em>, <em>143</em>,
104946. (<a href="https://doi.org/10.1016/j.imavis.2024.104946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating semantically meaningful and coherent clinical description for the diagnosis of retinal images has been a challenging task for both Computer Vision and Natural Language Processing domains. This is mainly due to the fact that the clinical descriptions generated by the language model are completely dependent on the type of retinal image representations learned by the vision model. This work investigates and proposes a unified approach to integrate multi-modal retinal image visual representations with corresponding clinical keyword embeddings which can aid the language model to learn the clinical semantics and generate lengthy, coherent clinical descriptions accurately. Our proposed approach, named the Gated Contextual Transformer Network, comprises two attention-based encoders that learn semantically discriminative attention-based representations from retinal images and clinical keywords, along with a Transformer Network for clinical description generation. The first encoder leverages a pre-trained Convolutional Neural Network (VGG19) and a Gated Contextual Attention module to learn discriminative attention-based representations from the multi-modal retinal images. The second encoder incorporates an Embedding layer and an Attention module to learn attention-based clinical keyword embeddings. The Transformer network consists of a fusion encoder that attentively integrates retinal image visual features with clinical keyword embeddings and a decoder that is responsible for generating semantically meaningful and coherent clinical descriptions. Our experimental studies on the benchmark DeepEyeNet dataset demonstrate that the proposed approach successfully generates clinical descriptions from multi-modal retinal images, meeting the standards of ophthalmologists. To support our claim, we provide qualitative and quantitative evaluations of the proposed approach. This includes reporting BLUE, CIDEr, and ROUGE scores for the predicted descriptions, as well as employing Visual Explanation for Clinical Description Generation.},
  archive      = {J_ICV},
  author       = {Nagur Shareef Shaik and Teja Krishna Cherukuri},
  doi          = {10.1016/j.imavis.2024.104946},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104946},
  shortjournal = {Image Vis. Comput.},
  title        = {Gated contextual transformer network for multi-modal retinal image clinical description generation},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating prior knowledge into a bibranch pyramid network
for medical image segmentation. <em>ICV</em>, <em>143</em>, 104945. (<a
href="https://doi.org/10.1016/j.imavis.2024.104945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation is crucial for obtaining accurate diagnoses, and while convolutional neural network (CNN)-based methods have made strides in recent years, they struggle with modeling long-range dependencies. Transformer-based methods improve this task but require more computational resources. The segment anything model (SAM) can generate pixel-level segmentation results for natural images using sparse manual prompts, but it performs poorly on low-contrast, noisy ultrasound images. To address this issue, we propose a new medical image segmentation network architecture that integrates transformer components, CNN modules, and an SAM encoder into a unified framework. This allows us to simultaneously capture both long-range dependencies and local features. Additionally, we incorporate the image features extracted from the SAM model as prior knowledge to achieve further improved segmentation accuracy with limited training data. To reduce the imposed computational stress, we employ an axial attention mechanism to approximate a transformer&#39;s effects by expanding the receptive field. Instead of replacing the transformer components with lightweight attention modules, our model is divided into a global branch and a local branch. The global branch extracts context features with the transformer components, while the local branch processes patch tokens with the axial attention mechanism. We also construct an image pyramid to excavate internal statistics and multiscale representations to obtain more accurate segmentation regions. This bibranch pyramid transformer (Bi-BPT) architecture is effective and robust for medical image segmentation, surpassing other related segmentation network architectures. The experimental results obtained on various medical image datasets demonstrate its effectiveness.},
  archive      = {J_ICV},
  author       = {Xianjun Han and Tiantian Li and Can Bai and Hongyu Yang},
  doi          = {10.1016/j.imavis.2024.104945},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104945},
  shortjournal = {Image Vis. Comput.},
  title        = {Integrating prior knowledge into a bibranch pyramid network for medical image segmentation},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep hybrid manifold for image set classification.
<em>ICV</em>, <em>143</em>, 104935. (<a
href="https://doi.org/10.1016/j.imavis.2024.104935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of the data volume of image sets, which contain more information than a single image, has attracted increasing attention from researchers. Image set data are often described as covariance matrices or linear subspaces, and the unique geometries they span are symmetric positive definite (SPD) manifolds and Grassmann manifolds, respectively. Image set data are often described as covariance matrices or linear subspaces, and the distinctive geometries they span are symmetric positive definite (SPD) manifold and Grassmann manifold, respectively. However, most studies focus on a single manifold and ignore the useful information of the another manifold. Based on this, we propose a new Deep Hybrid Manifold Network (DHMNet). The DHMNet consists of backbone network, stackable Hybrid Manifold AutoEncoder (HMAE) and,Maximum Fusion Module (MFM). The image set data is modeled through SPD manifold and Grassmann manifold. The modeled data is input into the backbone network composed of SPDNet and GrNet for initial feature extraction, and the output manifold data are input into HMAEs. The HMAE effectively extracts and hybridizes complementary information from different manifolds and has the ability to generate deep representations with rich structural semantic information. For the three image datasets used, DHMNet with two HMAEs improves the classification accuracy by 3.83–5.76% over the classical SPDNet, and even reaches the best when compared to other models, with the best performance on the First Person Hand Action (FPHA) dataset for skeleton-based hand action recognition.},
  archive      = {J_ICV},
  author       = {Xianhua Zeng and Jueqiu Guo and Yifan Wei and Yang Zhuo},
  doi          = {10.1016/j.imavis.2024.104935},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104935},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep hybrid manifold for image set classification},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computer vision and deep learning meet plankton: Milestones
and future directions. <em>ICV</em>, <em>143</em>, 104934. (<a
href="https://doi.org/10.1016/j.imavis.2024.104934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planktonic organisms play a pivotal role within aquatic ecosystems , serving as the foundation of the aquatic food chain while also playing a critical role in climate regulation and the production of oxygen. In recent years, the advent of automated systems for capturing in-situ images has led to a huge influx of plankton images, making manual classification impractical. This, at the same time, has opened up opportunities for the application of machine learning and deep learning solutions. This paper undertakes an extensive analysis of the broad range of computer vision techniques and methodologies that have emerged to facilitate the automatic analysis of small- to large-scale datasets containing plankton images. By focusing on different computer vision tasks, we present findings and limitations in order to offer a comprehensive overview of the current state-of-the-art, while also pinpointing the open challenges that demand further research and attention.},
  archive      = {J_ICV},
  author       = {Massimiliano Ciranni and Vittorio Murino and Francesca Odone and Vito Paolo Pastore},
  doi          = {10.1016/j.imavis.2024.104934},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104934},
  shortjournal = {Image Vis. Comput.},
  title        = {Computer vision and deep learning meet plankton: Milestones and future directions},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EMNet: Edge-guided multi-level network for salient object
detection in low-light images. <em>ICV</em>, <em>143</em>, 104933. (<a
href="https://doi.org/10.1016/j.imavis.2024.104933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) has achieved remarkable performance in well-lit scenes. However, when generalized to low-light scenes, the performance of SOD shows significant decrease owing to more challenging conditions such as weak brightness, low contrast, and poor signal-to-noise ratio. To address this issue, we propose a novel edge-guided and multi-level network (EMNet) for SOD in low light images, which learns robust multiscale region features by optimizing the boundaries of salient objects and employing a multi-stage cascaded strategy. To be more specific, the proposed Edge Feature Highlight (EFH) module can establish mapping relationships at different scales and fuse the outputs obtained from pairs of different branches for extracting accurate boundary information. Secondly, Multi-layer Feature Fusion (MFF) module is proposed for combining multi-scale deep features with salient edge cues, using stepwise fusion for effective integration of deep features. Finally, we employ a coarse-to-fine way for iterative prediction to generate high-quality saliency maps. We conducted comprehensive experiments on LLI dataset, and the results demonstrate that the proposed method achieves the best performance in terms of five evaluation metrics , where an average improvement of max F β Fβ , ω F β ωFβ , E m Em , S m Sm , and MAE outperforms state-of-the-art method by 7.20%, 12.06%, 4.62%, 5.00%, and 36.34%, respectively.},
  archive      = {J_ICV},
  author       = {Lianghu Jing and Bo Wang},
  doi          = {10.1016/j.imavis.2024.104933},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104933},
  shortjournal = {Image Vis. Comput.},
  title        = {EMNet: Edge-guided multi-level network for salient object detection in low-light images},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-branch residual image semantic segmentation combined
with inverse weight gated-control. <em>ICV</em>, <em>143</em>, 104932.
(<a href="https://doi.org/10.1016/j.imavis.2024.104932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The loss of pixel-level information in the multi-class segmentation task based on the U-net model results in unclear boundaries and low semantic segmentation accuracy. Aiming at this, a deep multi-branch residual Unet (IWG-MRUN) with fused inverse weight gated-control is proposed to improve the quality of image semantic segmentation. Specifically, we first introduce a deep multi-branch residual module, which used parallel convolution mode to capture the contextual feature to extract the detailed features of the input image at a deeper level. Then, we adopt an inverse weight gated-control module to enhance the diversity of up-sampling information by counterclockwise transmitting attention horizontally to improve the restoration accuracy of up-sampled image pixels. Finally, to obtain finer granularity features from low spatial resolution images, we adopt the different receptive field pyramid attention mechanisms at the highest level of the U-shaped encoder to capture high-level context information at different scales, thereby improving the accuracy of semantic segmentation. The experimental results show that the segmentation accuracy of the proposed algorithm reaches 91.80% and the CCE loss is reduced to 0.21. When compared to the Unet, BiSeNet, DeeplabV3 + and U-net + BLR model, the pixel accuracy of semantic segmentation is improved by 15.0%, 1.98%, 0.9% and 6.5%, respectively. The semantic segmentation model proposed in this paper provides an end-to-end semantic segmentation capability with the enriched finer granularity features of the target boundary and realizes the accurate segmentation of the objects in different categories.},
  archive      = {J_ICV},
  author       = {Haicheng Qu and Xiaona Wang and Ying Wang and Yao Chen},
  doi          = {10.1016/j.imavis.2024.104932},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104932},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-branch residual image semantic segmentation combined with inverse weight gated-control},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Person search over security video surveillance systems using
deep learning methods: A review. <em>ICV</em>, <em>143</em>, 104930. (<a
href="https://doi.org/10.1016/j.imavis.2024.104930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search has become one of the most critical and challenging applications in today&#39;s video surveillance systems . It helps in locating a person in surveillance videos , which is plausible only with advanced deep learning models , large scale datasets and high compute power GPUs. This survey features exhaustive analysis of deep learning based person search through image, textual and attributes based description. The image based person search is reviewed based on aspects such as region proposal consideration, feature representation, context information and compute complexity. The text based person search is reviewed based on the aspects of feature representation and alignment. The attribute based person search is reviewed based on the aspect of high level and low level feature representation. The paper summarizes more than 100 research works and provides future perspectives for enhancements with the objective of guiding and facilitating the development of better solutions in future. We believe that this exclusive review on deep learning based person search for video surveillance security systems will facilitate better systematization.},
  archive      = {J_ICV},
  author       = {S. Irene and A. John Prakash and V. Rhymend Uthariaraj},
  doi          = {10.1016/j.imavis.2024.104930},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104930},
  shortjournal = {Image Vis. Comput.},
  title        = {Person search over security video surveillance systems using deep learning methods: A review},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Point-level feature learning based on vision transformer for
occluded person re-identification. <em>ICV</em>, <em>143</em>, 104929.
(<a href="https://doi.org/10.1016/j.imavis.2024.104929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is challenging due to the presence of variations in pose and occlusion, which significantly impact the matching of visual features across different camera views and pose considerable difficulty for accurate person re-identification. This paper proposes a novel method for occluded person re-identification by introducing point-level feature learning based on vision transformers . Our approach utilizes a pose estimator to detect the keypoints of the human body and employs these points to locate intermediate features. These intermediate features of keypoints are input to a pose-based transformer branch to learn point-level features. Then, we design a part-based transformer branch to learn part-level features that capture visual features of different image parts, further enhancing the discriminative power of the learned features. Additionally, we employ a global branch to learn the global-level feature by treating the person&#39;s image as a single entity. Finally, we integrate point-level, part-level, and global-level features to represent a person&#39;s features. The experimental results on occluded and partial person re-identification datasets demonstrate the effectiveness of our proposed approach in improving re-identification. Our approach shows potential for improving person re-identification in scenarios with occlusion and pose variations.},
  archive      = {J_ICV},
  author       = {Hua Gao and Chenchen Hu and Guang Han and Jiafa Mao and Wei Huang and Qiu Guan},
  doi          = {10.1016/j.imavis.2024.104929},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104929},
  shortjournal = {Image Vis. Comput.},
  title        = {Point-level feature learning based on vision transformer for occluded person re-identification},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning methods for single camera based clinical
in-bed movement action recognition. <em>ICV</em>, <em>143</em>, 104928.
(<a href="https://doi.org/10.1016/j.imavis.2024.104928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many clinical applications involve in-bed patient activity monitoring, from intensive care and neuro-critical infirmary, to semiology-based epileptic seizure diagnosis support or sleep monitoring at home, which require accurate recognition of in-bed movement actions from video streams. The major challenges of clinical application arise from the domain gap between common in-the-lab and clinical scenery (e.g. viewpoint, occlusions, out-of-domain actions), the requirement of minimally intrusive monitoring to already existing clinical practices (e.g. non-contact monitoring), and the significantly limited amount of labeled clinical action data available. Focusing on one of the most demanding in-bed clinical scenarios - semiology-based epileptic seizure classification – this review explores the challenges of video-based clinical in-bed monitoring, reviews video-based action recognition trends, monocular 3D MoCap, and semiology-based automated seizure classification approaches . Moreover, provides a guideline to take full advantage of transfer learning for in-bed action recognition for quantified, evidence-based clinical diagnosis support. The review suggests that an approach based on 3D MoCap and skeleton-based action recognition, strongly relying on transfer learning, could be advantageous for these clinical in-bed action recognition problems. However, these still face several challenges, such as spatio-temporal stability, occlusion handling, and robustness before realizing the full potential of this technology for routine clinical usage.},
  archive      = {J_ICV},
  author       = {Tamás Karácsony and László Attila Jeni and Fernando De la Torre and João Paulo Silva Cunha},
  doi          = {10.1016/j.imavis.2024.104928},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104928},
  shortjournal = {Image Vis. Comput.},
  title        = {Deep learning methods for single camera based clinical in-bed movement action recognition},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recent advances in deterministic human motion prediction: A
review. <em>ICV</em>, <em>143</em>, 104926. (<a
href="https://doi.org/10.1016/j.imavis.2024.104926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the rapid advancement of deep learning and the advent of extensive human motion datasets have significantly enhanced the prominence of human motion prediction technology . This article presents an overview of prevalent model architectures within this field, critically examining their advantages and drawbacks. It methodically reviews recent research breakthroughs, offering in-depth analyses of significant works. Additionally, the paper provides a comprehensive survey of current methodologies, widely used datasets, and standard evaluation metrics in human motion prediction. In closing, we highlight some limitations in the field and propose potential research directions to aid its further development in human motion prediction.},
  archive      = {J_ICV},
  author       = {Tenghao Deng and Yan Sun},
  doi          = {10.1016/j.imavis.2024.104926},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104926},
  shortjournal = {Image Vis. Comput.},
  title        = {Recent advances in deterministic human motion prediction: A review},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). The impact of introducing textual semantics on item
instance retrieval with highly similar appearance: An empirical study.
<em>ICV</em>, <em>143</em>, 104925. (<a
href="https://doi.org/10.1016/j.imavis.2024.104925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation plays an important role in image instance retrieval (IIR). In practical applications, we find that items of different categories but highly similar in appearance are easy to become the objects of incorrect retrieval. We analyze that extracting features from the appearance dimension alone may cause objects with similar appearance to have smaller similar distances in feature space . But the appearance is not the only factor that determines whether the item is the same, and the difference in the shooting angle will also amplify the appearance difference of the same item in the image. In this paper, through detailed empirical study, we verify a conjecture that by introducing text semantics and fusing it with appearance features, the similarity distance of falsely retrieved objects in feature space can be corrected, thus improving the retrieval effectiveness of image instance retrieval tasks in highly similar appearance data. We introduce textual semantics for image instances based on the image-text cross-modal model. Specifically, we enhance the proportion of appearance similar items based on three open-source datasets (Products-10 k, RP2k and Stanford products) of item instances, and add multi-angle image samples of the same item to enlarge the difference of the same item. Subsequently, we have embarked on baseline experiments for appearance features and textual features from the perspectives of shooting angle similarity and visual character similarity, to explore the advantages of multiple strategies for fusing textual semantics with appearance features. Then, we examine the effect of our method on fine-grained item instance retrieval methods with state-of-the-art. Resultantly, taking mean Average Precision (mAP) as the quantitative metric and averaging experimental results, our method has an obvious improvement over the appearance and textual baselines, where the improvement of appearance feature baselines is generally more obvious than that of textual feature baselines (e.g., in our expanded RP2k dataset, from the perspective of shooting angle similarity, the mAP of the appearance feature baseline is nearly 19.62, the textual feature baseline is 32.45, our method is 43.19. From perspective of visual character similarity, the values are 27.14, 43.59, 54.76, respectively). Moreover, our methods outperform the state-of-the-art fine-grained item instance retrieval methods with improvements of nearly 13.05 % 13.05% and 22.49 % 22.49% on expanded Products-10 k and RP2k, respectively.},
  archive      = {J_ICV},
  author       = {Bo Li and Jiansheng Zhu and Linlin Dai and Hui Jing and Zhizheng Huang},
  doi          = {10.1016/j.imavis.2024.104925},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104925},
  shortjournal = {Image Vis. Comput.},
  title        = {The impact of introducing textual semantics on item instance retrieval with highly similar appearance: An empirical study},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depth awakens: A depth-perceptual attention fusion network
for RGB-d camouflaged object detection. <em>ICV</em>, <em>143</em>,
104924. (<a href="https://doi.org/10.1016/j.imavis.2024.104924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings. However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment. The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects. Therefore, we propose a novel depth-perception attention fusion network that leverages the depth map as an auxiliary input to enhance the network&#39;s ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images. The network uses a trident-branch encoder to extract chromatic and depth information and their communications. Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps. To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features. Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection. The code will be available at https://github.com/xinran-liu00/DAF-Net .},
  archive      = {J_ICV},
  author       = {Xinran Liu and Lin Qi and Yuxuan Song and Qi Wen},
  doi          = {10.1016/j.imavis.2024.104924},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104924},
  shortjournal = {Image Vis. Comput.},
  title        = {Depth awakens: A depth-perceptual attention fusion network for RGB-D camouflaged object detection},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). R2-trans: Fine-grained visual categorization with redundancy
reduction. <em>ICV</em>, <em>143</em>, 104923. (<a
href="https://doi.org/10.1016/j.imavis.2024.104923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories, whose main challenge is the large intraclass diversities and subtle inter-class differences. Existing FGVC methods usually select discriminant regions found by a trained model, which is prone to neglect other potential discriminant information. On the other hand, the massive interactions between the sequence of image patches in ViT make the resulting class token contain lots of redundant information, which may also impact FGVC performance. In this paper, we present a novel approach for FGVC, which can simultaneously make use of partial yet sufficient discriminative information in environmental cues and also compress the redundant information in class-token with respect to the target. Specifically, our model calculates the ratio of high-weight regions in a batch, adaptively adjusts the masking threshold, and achieves moderate extraction of background information in the input space. Moreover, we also use the Information Bottleneck (IB) approach to guide our network to learn a minimum sufficient representations in the feature space . Experimental results on three widely-used benchmark datasets verify that our approach can achieve better performance than other state-of-the-art approaches and baseline models . The code of our model is available at: https://github.com/SYe-hub/R-2-Trans .},
  archive      = {J_ICV},
  author       = {Shuo Ye and Shujian Yu and Yu Wang and Xinge You},
  doi          = {10.1016/j.imavis.2024.104923},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104923},
  shortjournal = {Image Vis. Comput.},
  title        = {R2-trans: Fine-grained visual categorization with redundancy reduction},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shadow detection using a cross-attentional dual-decoder
network with self-supervised image reconstruction features.
<em>ICV</em>, <em>143</em>, 104922. (<a
href="https://doi.org/10.1016/j.imavis.2024.104922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection is a challenging problem in computer vision due to the high variability in lighting conditions , object shapes, and scene layouts. Despite the positive results achieved by some existing technologies, the problem becomes particularly challenging with complex and heterogeneous images where shadow-casting objects coexist and shadows can have different depths, scales, and morphologies. As a result, more advanced and accurate solutions are still needed to deal with this type of complexities. To address these challenges, this paper proposes a novel deep learning model , called the Cross-Attentional Dual Decoder Network (CADDN), to improve shadow detection by using fine-grained image reconstruction features. Unlike other existing methods, the CADDN uses an innovative encoder-decoder architecture with two decoder segments that work together to reconstruct the input images and their corresponding shadow masks. In this way, the features used to reconstruct the original input image can be used to support the shadow detection process itself. The proposed model also incorporates a cross-attention mechanism to weight the most relevant features for detecting shadows and skip connections with noise to improve the quality of the transferred features. The experimental results, including several benchmark image datasets and state-of-the-art detection methods, demonstrate the suitability of the presented approach for detecting shadows in computer vision applications .},
  archive      = {J_ICV},
  author       = {Ruben Fernandez-Beltran and Angélica Guzmán-Ponce and Rafael Fernandez and Jian Kang and Ginés García-Mateos},
  doi          = {10.1016/j.imavis.2024.104922},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104922},
  shortjournal = {Image Vis. Comput.},
  title        = {Shadow detection using a cross-attentional dual-decoder network with self-supervised image reconstruction features},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature attention fusion network for occluded person
re-identification. <em>ICV</em>, <em>143</em>, 104921. (<a
href="https://doi.org/10.1016/j.imavis.2024.104921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occluded Person re-identification (ReID) is a person retrieval task which aims to match occluded person images with the holistic image. In this paper, we propose a novel framework by using person key-points estimation and attention mechanism based on occluded person Re-identification, which is used to get discriminative features and robust alignment. We use a CNN backbone and a key-points estimation model to extract semantic local features and global features. In this process, the features extracted by the backbone network contain a lot of noise. Therefore, the Feature Attention Module (FAM) was built and applied to the backbone network to enable the network to better extract foreground information. Since the currently used baseline does not achieve very good results in occlusion scenarios, the authors considered using ConvNeXt instead of Resnet. In addition, the network adds the attention of the spatial and channel after the last convolution block, and gets the person feature with little background information . Most multi-level feature aggregation methods treat feature maps on different levels equally and use simple local operations for feature fusion , which neglects the long-distance connection among feature maps. FAM uses attention feature as query to perform second-order information propagation from the source feature map. The attention feature is computed based on the compatibility of the source feature map with the attention feature map. The feature attention module connects the attention features with the features at all levels, so as to make better use of the relationship among the features, and greatly reduce the influence of the background noise of the picture. Our method achieves 55.9% and 79.1% Rank-1 scores on the datasets of Occluded-Duke and Occluded-REID. Meanwhile, our proposed framework has 85.1% of the Rank-1 scores on partial-REID, and is superior to other methods on Partial-iLIDS datasets. Finally, our method is close to the most advanced method in Holistic Datasets.},
  archive      = {J_ICV},
  author       = {Xuyao Zhuang and Dan Wei and Danyang Liang and Lei Jiang},
  doi          = {10.1016/j.imavis.2024.104921},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104921},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature attention fusion network for occluded person re-identification},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Drone-NeRF: Efficient NeRF based 3D scene reconstruction for
large-scale drone survey. <em>ICV</em>, <em>143</em>, 104920. (<a
href="https://doi.org/10.1016/j.imavis.2024.104920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural rendering has garnered substantial attention owing to its capacity for creating realistic 3D scenes. However, its applicability to extensive scenes remains challenging, with limitations in effectiveness. In this work, we propose the Drone-NeRF framework to enhance the efficient reconstruction of unbounded large-scale scenes suited for drone oblique photography using Neural Radiance Fields (NeRF). Our approach involves dividing the scene into uniform sub-blocks based on camera position and depth visibility. Sub-scenes are trained in parallel using NeRF, then merged for a complete scene. We refine the model by optimizing camera poses and guiding NeRF with a uniform sampler. Integrating chosen samples enhances accuracy. A hash-coded fusion MLP accelerates density representation, yielding RGB and Depth outputs. Our framework accounts for sub-scene constraints, reduces parallel-training noise, handles shadow occlusion, and merges sub-regions for a polished rendering result. Moreover, our framework can be enhanced through the integration of semantic scene division, ensuring consistent allocation of identical objects to the same sub-block for improved object integrity and rendering performance. This Drone-NeRF framework demonstrates promising capabilities in addressing challenges related to scene complexity, rendering efficiency, and accuracy in drone-obtained imagery.},
  archive      = {J_ICV},
  author       = {Zhihao Jia and Bing Wang and Changhao Chen},
  doi          = {10.1016/j.imavis.2024.104920},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104920},
  shortjournal = {Image Vis. Comput.},
  title        = {Drone-NeRF: Efficient NeRF based 3D scene reconstruction for large-scale drone survey},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view daily action recognition based on hooke balanced
matrix and broad learning system. <em>ICV</em>, <em>143</em>, 104919.
(<a href="https://doi.org/10.1016/j.imavis.2024.104919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Daily action recognition is a challenging task in computer vision , and so the multi-layer methods are proposed recently. However, the feature concatenation strategy in multi-view clustering can be regarded as equal-scale feature fusion and ignores the information difference between views. To deal with this problem, we firstly propose the multi-view feature fusion strategy, which constructs Hooke balanced matrix to complete multi-view unsupervised clustering in preparation for more discriminative motion atoms. Secondly, we build the coverage detection network model based on the broad learning system (BLS) to mine the relationship between the features and labels of motion atoms and obtain more accurate labels of motion atoms. Finally, the experimental results based on the WVU dataset, the NTU RGB-D 120 dataset and the N-UCLA dataset show that the proposed UVS-H-BLS method has state-of-the-art performance, compared with the classic methods such as iDT, MoFAP, JLMF, FGCN, MVMLR, and UVS.},
  archive      = {J_ICV},
  author       = {Zhigang Liu and Bingshuo Lu and Yin Wu and Chunlei Gao},
  doi          = {10.1016/j.imavis.2024.104919},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104919},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-view daily action recognition based on hooke balanced matrix and broad learning system},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-based human re-identification: Which covariates are
actually (the most) important? <em>ICV</em>, <em>143</em>, 104917. (<a
href="https://doi.org/10.1016/j.imavis.2024.104917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human re-identification (re-ID) is nowadays among the most popular topics in computer vision , due to the increasing importance given to safety/security in modern societies. Being expected to sun in totally uncontrolled data acquisition settings (e.g., visual surveillance) automated re-ID not only depends on various factors that may occur in non-controlled data acquisition settings, but - most importantly - performance varies with respect to different subject features (e.g., gender, height, ethnicity, clothing, and action being performed), which may result in highly biased and undesirable automata . While many efforts have been putted in increase the robustness of identification to uncontrolled settings, a systematic assessment of the actual variations in performance with respect to each subject feature remains to be done. Accordingly, the contributions of this paper are threefold: 1) we report the correlation between the performance of three state-of-the-art re-ID models and different subject features; 2) we discuss the most concerning features and report valuable insights about the roles of the various features in re-ID performance, which can be used to develop more effective and unbiased re-ID systems; and 3) we leverage the concept of biometric menagerie , in order to identify the groups of individuals that typically fall into the most common menagerie families (e.g., goats, lambs, and wolves). Our findings not only contribute to a better understanding of the factors affecting re-ID performance, but also may offer practical guidance for researchers and practitioners concerned on human re-identification development.},
  archive      = {J_ICV},
  author       = {Kailash Hambarde and Hugo Proença},
  doi          = {10.1016/j.imavis.2024.104917},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104917},
  shortjournal = {Image Vis. Comput.},
  title        = {Image-based human re-identification: Which covariates are actually (the most) important?},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noisy label facial expression recognition via face-specific
label distribution learning. <em>ICV</em>, <em>143</em>, 104901. (<a
href="https://doi.org/10.1016/j.imavis.2024.104901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncertainty in Facial expression recognition (FER) data is caused by factors such as ambiguous facial expressions, low-resolution facial images , the subjectivity of the annotator or subject of expression, and compound expression. FER has been steadily evolving with the advent of deep learning , but the label inconsistency problem due to the uncertainty in large FER datasets is one of the challenges in FER. Noisy labels in the label inconsistency problem adversely affect emotion recognition results. In this paper, we propose a Face-Specific Label Distribution Learning (FSLDL) method, which encourages deep networks to predict the actual emotion distribution of the input itself, rather than the noisy single label. Under the assumption that the emotion distribution of a specific sample is similar to the emotion distribution of the augmented sample from a different viewpoint, we generate a new target label distribution using facial expression-specific augmented samples. In order to generate a sophisticated target label distribution, the importance weights of the augmented samples are extracted and multiplied by each predicted emotion distribution. In addition, we compensate for the lack of information in the target label distribution by calculating the uncertainty of the provided label, and use it for model training. Finally, we make a more robust model by adding a rank regularization loss function for the importance weights and a discriminative loss function for the feature vectors. Representative experiments demonstrated that FSLDL outperforms the state-of-the-art on FER datasets such as RAF-DB, AffectNet, and SFEW. In addition, we demonstrated the effectiveness of the proposed method through various noisy label injection experiments.},
  archive      = {J_ICV},
  author       = {Hyunuk Shin and Bokyeung Lee and Bonhwa Ku and Hanseok Ko},
  doi          = {10.1016/j.imavis.2024.104901},
  journal      = {Image and Vision Computing},
  month        = {3},
  pages        = {104901},
  shortjournal = {Image Vis. Comput.},
  title        = {Noisy label facial expression recognition via face-specific label distribution learning},
  volume       = {143},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing lung abnormalities diagnosis using hybrid
DCNN-ViT-GRU model with explainable AI: A deep learning approach.
<em>ICV</em>, <em>142</em>, 104918. (<a
href="https://doi.org/10.1016/j.imavis.2024.104918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a novel approach called DCNN-ViT-GRU, which combines deep Convolutional Neural Networks (CNNs) with Gated Recurrent Units (GRUs) and the Vision Transformer (ViT) model for the accurate detection and classification of lung abnormalities. By leveraging the strengths of both CNNs and the ViT model, our architecture automatically extracts meaningful features from lung images, leading to improved diagnostic capabilities. The DCNN-ViT-GRU model utilizes a combination of deep CNN and GRU layers, allowing it to effectively capture local and global patterns. This comprehensive feature representation enhances the model&#39;s ability to identify various abnormalities in lung images, including lung cancer, COVID-19, and pneumonia. To further enhance the interpretability and transparency of our model, we integrate Explainable Artificial Intelligence (XAI) techniques, including LIME and SHAP . This integration provides valuable insights into the decision-making process of the DCNN-ViT-GRU model, enabling clinicians to understand and validate the predictions made by the model. We evaluated the performance of our proposed approach on diverse datasets containing cases of lung abnormalities. Through cross-validation, our DCNN-ViT-GRU model achieved impressive weighted mean accuracy of 99% and 99.86% for two distinct datasets, demonstrating its superior performance. Furthermore, in hold-out validation on separate datasets, the model achieved accuracies of 99.09% and 99.87%, respectively. Integrating the XAI techniques enhances the interpretability of the DCNN-ViT-GRU model and provides clinicians with valuable insights regarding the factors contributing to the diagnosis of lung abnormalities. This approach presents a promising solution for accurate and interpretable lung abnormalities detection and classification, with potential implications for improved patient care and treatment planning.},
  archive      = {J_ICV},
  author       = {Md Khairul Islam and Md Mahbubur Rahman and Md Shahin Ali and S.M. Mahim and Md Sipon Miah},
  doi          = {10.1016/j.imavis.2024.104918},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104918},
  shortjournal = {Image Vis. Comput.},
  title        = {Enhancing lung abnormalities diagnosis using hybrid DCNN-ViT-GRU model with explainable AI: A deep learning approach},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weakly supervised point cloud semantic segmentation with the
fusion of heterogeneous network features. <em>ICV</em>, <em>142</em>,
104916. (<a href="https://doi.org/10.1016/j.imavis.2024.104916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised point cloud segmentation has emerged as a prominent research area to address the problem of manual annotation costs. A crucial challenge in weakly supervised point cloud segmentation is the implicit augmentation of the total amount of supervision signals. In this article, we propose a novel method that utilizes the fusion of features from different networks to enhance the supervision signals. Specifically, we utilize a deep Encoder-Decoder network to capture high-level semantic features of labeled points, while a shallow Encoder network captures multi-scale detail features of labeled data. By combining these two heterogeneous networks , we acquire richer feature representations that implicitly enhance the supervision signal.Furthermore, we introduce scene-level and instance-level contrast to enhance feature representations in both coarse-grained and fine-grained manners, thus further boosting the supervisory signal. To validate the effectiveness of our approach, we conducted experiments on the large-scale indoor scene dataset, S3DIS, and the outdoor datasets, Toronto3D and Semantic3D, achieving convincing results.},
  archive      = {J_ICV},
  author       = {Yingchun Niu and Jianqin Yin},
  doi          = {10.1016/j.imavis.2024.104916},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104916},
  shortjournal = {Image Vis. Comput.},
  title        = {Weakly supervised point cloud semantic segmentation with the fusion of heterogeneous network features},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSTPNet: Long short-term perception network for dynamic
facial expression recognition in the wild. <em>ICV</em>, <em>142</em>,
104915. (<a href="https://doi.org/10.1016/j.imavis.2024.104915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-the-wild dynamic facial expression recognition (DFER) is a very challenging task, and previous methods based on convolutional neural networks (CNNs), recurrent neural networks (RNNs), or Transformers emphasize the extraction of either short-term temporal information or long-term temporal information from facial video sequences. Different from existing methods, this paper proposes a long short-term perceptimon network (LSTPNet) for dynamic facial expression recognition, taking into account the joint perception of the above two temporal cues to benefit the DFER task. Specifically, we propose a long short-term temporal Transformer (LSTformer) which can perceive both long-term and short-term temporal information effectively. In addition, we introduce a temporal channel excitation (TCE) module extended from the previous notable efficient channel attention (ECA) module, in order to establish temporal attention for intermediate features within the backbone network , and obtain more temporally representative features. Experimental results on three benchmark datasets demonstrate the state-of-the-art performance of the proposed LSTPNet. The code will be available at https://github.com/LLFabiann/LSTPNet/ .},
  archive      = {J_ICV},
  author       = {Chengcheng Lu and Yiben Jiang and Keren Fu and Qijun Zhao and Hongyu Yang},
  doi          = {10.1016/j.imavis.2024.104915},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104915},
  shortjournal = {Image Vis. Comput.},
  title        = {LSTPNet: Long short-term perception network for dynamic facial expression recognition in the wild},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variation-aware semantic image synthesis. <em>ICV</em>,
<em>142</em>, 104914. (<a
href="https://doi.org/10.1016/j.imavis.2024.104914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image synthesis (SIS) aims to produce photorealistic images aligning to given conditional semantic layout and has witnessed a significant improvement in recent years. Although the diversity in image-level has been discussed heavily, class-level mode collapse widely exists in current algorithms. Therefore, we declare a new requirement for SIS to achieve more photorealistic images, variation-aware, which consists of inter- and intra-class variation. The inter-class variation is the diversity between different semantic classes while the intra-class variation stresses the diversity inside one class. Through analysis, we find that current algorithms elusively embrace the inter-class variation but the intra-class variation is still not enough. Further, we introduce two simple methods to achieve variation-aware semantic image synthesis (VASIS) with a higher intra-class variation, semantic noise and position code. We combine our method with several state-of-the-art algorithms and the experimental result shows that our models generate more natural images and achieve slightly better FIDs and/or mIoUs than the counterparts. Our codes and models will be publicly available.},
  archive      = {J_ICV},
  author       = {Mingle Xu and Jaehwan Lee and Sook Yoon and Hyongsuk Kim and Dong Sun Park},
  doi          = {10.1016/j.imavis.2024.104914},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104914},
  shortjournal = {Image Vis. Comput.},
  title        = {Variation-aware semantic image synthesis},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three dimensional tracking of rigid objects in motion using
2D optical flows. <em>ICV</em>, <em>142</em>, 104913. (<a
href="https://doi.org/10.1016/j.imavis.2024.104913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and tracking objects in image sequences is paramount for any video analytic. While object detectors have become increasingly robust, motion estimates based on the straightforward approach of running a detector and linking detections are prone to bounding box noise. An algorithm for the monocular estimation of the 3D motion of rigid objects is presented, combining an object detector, minimal camera calibration, and 2D optical flows observed on the image plane. The algorithm utilizes the 2D Delaunay triangulation over geometrically consistent optical flow tracks to identify regions common to the same object. The algorithm is evaluated on the special case of image sequences of vehicles on roads. Experiments on BrnoCompSpeed dataset show that speed estimates from both detector-based tracking (mean error of 1.62 km/h) and optical flow based tracking (mean error of 2.19 km/h) perform competitively on straight roads. An extensive empirical evaluation is also conducted on a new dataset containing synthetic and real world scenes that also include vehicle trajectories involving rotation. A naive baseline bounding box track based method obtained a mean error of 5.29 km/h for vehicle speed, but optical flow tracks performed significantly better with mean error of 1.65 km/h on the new dataset including rotation. With the new method, video analytics such as vehicle speed estimation and lane change detection can obtain precise information about the 3D trajectory of rigid objects in motion.},
  archive      = {J_ICV},
  author       = {Ramesh Marikhu and Matthew N. Dailey and Mongkol Ekpanyapong},
  doi          = {10.1016/j.imavis.2024.104913},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104913},
  shortjournal = {Image Vis. Comput.},
  title        = {Three dimensional tracking of rigid objects in motion using 2D optical flows},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EESSO: Exploiting extreme and smooth signals via
omni-frequency learning for text-based person retrieval. <em>ICV</em>,
<em>142</em>, 104912. (<a
href="https://doi.org/10.1016/j.imavis.2024.104912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing methods manage to tackle the problem of text-based person retrieval from the spatial-wise perspective. In this paper, we manage to address the problem of this task from a novel perspective, namely, the frequency-wise perspective. To this end, we propose to E xploit E xtreme and S mooth S ignals via O mni-frequency learning ( EESSO ) through a jointly optimized multi-stream architecture. It consists of a Spatial Information Stream (SIS), an Extreme Signal Stream (ESS) and a Smooth Signal Stream (SSS). EESSO aims to excavate the complementary effect between spatial-wise and frequency-wise features, so as to achieve a superior performance. A novel Uncertainty-Guided Mutual Learning Mechanism (UG-MLM) is utilized during training, which not only enables the three streams to communicate with and learn from each other, but also models the data-related heteroscedastic uncertainty as a weight for knowledge transference, and hence enables each stream to adaptively allocate knowledge from the other two streams. A large number of experiments are carried out on the widely-used CUHK-PEDES, RSTPReid and ICFG-PEDES datasets to verify the effectiveness of EESSO. Through the experimental results, it may not be hard to find that EESSO has achieved the state-of-the-art performance in supervised , weakly supervised and cross-domain text-based person retrieval settings.},
  archive      = {J_ICV},
  author       = {Jingyi Xue and Zijie Wang and Guan-Nan Dong and Aichun Zhu},
  doi          = {10.1016/j.imavis.2024.104912},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104912},
  shortjournal = {Image Vis. Comput.},
  title        = {EESSO: Exploiting extreme and smooth signals via omni-frequency learning for text-based person retrieval},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Speech driven video editing via an audio-conditioned
diffusion model. <em>ICV</em>, <em>142</em>, 104911. (<a
href="https://doi.org/10.1016/j.imavis.2024.104911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking inspiration from recent developments in visual generative tasks using diffusion models , we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronised without relying on intermediate structural representations such as facial landmarks or a 3D face model . We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single-speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audio-driven video editing. All code, datasets, and models used as part of this work are made publicly available here: https://danbigioi.github.io/DiffusionVideoEditing/ .},
  archive      = {J_ICV},
  author       = {Dan Bigioi and Shubhajit Basak and Michał Stypułkowski and Maciej Zieba and Hugh Jordan and Rachel McDonnell and Peter Corcoran},
  doi          = {10.1016/j.imavis.2024.104911},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104911},
  shortjournal = {Image Vis. Comput.},
  title        = {Speech driven video editing via an audio-conditioned diffusion model},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel approach for breast cancer detection using optimized
ensemble learning framework and XAI. <em>ICV</em>, <em>142</em>, 104910.
(<a href="https://doi.org/10.1016/j.imavis.2024.104910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is a common and highly lethal ailment. It stands as the second leading contributor to cancer-related deaths in women worldwide. The timely identification of this condition is of utmost importance in mitigating mortality rates . This research paper presents a novel framework for the precise identification of BC, utilising a combination of image and numerical data features with explainable Artificial Intelligence (XAI). The utilisation of the U-NET transfer learning model is employed for image-based prediction. Additionally, an ensemble model is constructed by integrating characteristics from a customised convolutional neural network (CNN) model with an ensemble comprising random forest (RF) and support vector machine (SVM). The experiments aim to evaluate the influence of original features compared to convoluted features. A comparative analysis is carried out to assess the efficacy of various classifiers in accurately detecting BC, utilising the Wisconsin dataset. The model under consideration exhibits promising capabilities in enhancing BC diagnosis, with a remarkable accuracy rate of 99.99%. The present study contributes to the advancement of BC diagnosis by introducing a novel strategy based on machine learning and discussing the interpretation of the variables using XAI. The primary objective of this approach is to get a notable level of precision, hence facilitating the early and reliable identification of BC. Ultimately, the implementation of this approach is expected to enhance patient outcomes.},
  archive      = {J_ICV},
  author       = {Raafat M. Munshi and Lucia Cascone and Nazik Alturki and Oumaima Saidani and Amal Alshardan and Muhammad Umer},
  doi          = {10.1016/j.imavis.2024.104910},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104910},
  shortjournal = {Image Vis. Comput.},
  title        = {A novel approach for breast cancer detection using optimized ensemble learning framework and XAI},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based illumination transform for
devignetting photographs of dermatological lesions. <em>ICV</em>,
<em>142</em>, 104909. (<a
href="https://doi.org/10.1016/j.imavis.2024.104909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photographs of skin lesions taken with standard digital cameras (macroscopic images) have gained wide acceptance in dermatology. However, uneven background lighting caused by nonstandard image acquisition negatively impacts lesion segmentation and diagnosis. To address this, we propose an automated illumination equalization method based on a counter exponential transform (IECET). A modified residual network (ResNet) regressor is used to automate the selection of the operational parameter of the IECET. The regressor is designed by modifying the final fully-connected layer of the baseline ResNet-50 model. The modified fully-connected layer is coupled to a regression layer in the modified ResNet regressor. A prior knowledge base is created to train the modified ResNet regressor. For this, a set of corrupted images are generated by simulating uneven background illumination on pristine images. The knowledge base is created by including pairs of value components obtained from the HSV color space version of the corrupted macroscopic images and ideal operational parameter values that maximize the peak signal-to-noise ratio (PSNR) between the pristine images and the IECET outputs. We evaluated segmentation accuracies of the deep threshold prediction network (DTP-Net), DeepLabV3 +, fully convolutional network (FCN), and U-Net on the corrupted macroscopic images and output images of the IECET. The DTP-Net, DeepLabV3 +, FCN, and U-Net exhibited Dice similarity coefficient (DSC) of 0.71 ± ± 0.26, 0.85 ± ± 0.15, 0.75 ± ± 0.22, and 0.66 ± ± 0.28 on corrupted images and 0.81 ± ± 0.17, 0.87 ± ± 0.12, 0.79 ± ± 0.18, and 0.79 ± ± 0.15, on the outputs of the IECET. Increase in DSC proves the ability of the IECET to improve the performance of deep learning models used to segment skin lesions on macroscopic images.},
  archive      = {J_ICV},
  author       = {Vipin Venugopal and Malaya Kumar Nath and Justin Joseph and M. Vipin Das},
  doi          = {10.1016/j.imavis.2024.104909},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104909},
  shortjournal = {Image Vis. Comput.},
  title        = {A deep learning-based illumination transform for devignetting photographs of dermatological lesions},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-shot lip-based biometric authentication: Extending
behavioral features with authentication phrase information.
<em>ICV</em>, <em>142</em>, 104900. (<a
href="https://doi.org/10.1016/j.imavis.2024.104900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lip-based biometric authentication (LBBA) is an authentication method based on a person&#39;s lip movements during speech in the form of video data. LBBA can utilize both physical and behavioral characteristics of lip movements without requiring any additional sensory equipment apart from an RGB camera. Current approaches employ deep siamese neural networks trained with one-shot learning to generate embedding vectors from lip movement features. However, most of these approaches don&#39;t discriminate against speech content which makes them vulnerable to video replay attacks. Moreover, there is a lack of comprehensive analysis regarding the impact of distinct lip characteristics or difficult dataset phrases with significant word overlap on the performance of authentication in one-shot approaches. To address this, we introduce the GRID-CCP dataset and train a siamese neural network using 3D convolutions and recurrent neural network layers to additionally discriminate against speech content. For loss calculation, we propose a custom triplet loss function for efficient and customizable batch-wise hard-negative mining. Our experimental results, using an open-set protocol, demonstrate a False Acceptance Rate ( FAR ) of 3.2% and a False Rejection Rate ( FRR ) of 3.8% on the test set of the GRID-CCP dataset. Finally, we conduct an analysis to assess the influence and discriminative power of behavioral and physical features in LBBA.},
  archive      = {J_ICV},
  author       = {Brando Koch and Ratko Grbić},
  doi          = {10.1016/j.imavis.2024.104900},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104900},
  shortjournal = {Image Vis. Comput.},
  title        = {One-shot lip-based biometric authentication: Extending behavioral features with authentication phrase information},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Appearance flow estimation for online virtual clothing
warping via optimal feature linear assignment. <em>ICV</em>,
<em>142</em>, 104899. (<a
href="https://doi.org/10.1016/j.imavis.2024.104899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clothing warping to spatially align source garments with the corresponding body parts is crucial in clothing media tasks such as virtual try-on and pose-guided person generation. Recent pioneering work has utilized flow fields with additional dimensions of freedom to simulate clothing warping flexibly. However, current methods for estimating appearance flow typically rely on calculating just the local cost volume which contains multiple noisy matching points, potentially leading to a mismatch between clothing and body parts. To address this issue, we propose a novel appearance flow estimation network(Warping-Flow) for clothing warping based on optimal features linear assignment. Specially, two remarkable contributions are made to improve feature matching precision. First, a local context feature aggregation module is proposed to enhance the semantic feature distinction of the source cloth and target pose . Second, Warping-Flow estimates a hard attention mask through the cost volume to filter irrelevant features, followed by the optimal linear assignment algorithm to normalize the cost volume to a discrete permutation matrix that explicitly models the most contributing bipartite matches. Experiments conducted on the VITON and VITON-HD datasets demonstrate that Warping-Flow outperforms existing state-of-the-art algorithms, particularly in cases involving complex clothing deformation. Furthermore, Warping-Flow can serve as a plug-in to improve existing garment media technologies.},
  archive      = {J_ICV},
  author       = {Kexin Sun and Jing Tao and Peng Zhang and Jie Zhang},
  doi          = {10.1016/j.imavis.2024.104899},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104899},
  shortjournal = {Image Vis. Comput.},
  title        = {Appearance flow estimation for online virtual clothing warping via optimal feature linear assignment},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SDSDet: A real-time object detector for small, dense,
multi-scale remote sensing objects. <em>ICV</em>, <em>142</em>, 104898.
(<a href="https://doi.org/10.1016/j.imavis.2024.104898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection in remote sensing images (RSIs) plays a crucial role in aerial and satellite image analysis. Existing methods lack the capability to effectively detect small and multi-scale objects in RSIs. Consequently, achieving an optimal trade-off between speed and accuracy remains unattainable. Extensive investigation reveals that state-of-the-art detectors have largely overlooked two critical aspects: Spatial artifacts from convolution operations and gradient confusion caused by neighboring levels in the Feature Pyramid Network. To address the first problem, we propose adopting a non-reorganized patch-embedding layer in the downsampling stage and a dual-path learning network (DPLNet) as the backbone, which can effectively mitigate the adverse effects of the edge pixel feature bias in feature maps. Additionally, using DPLNet as the backbone network can minimize costs while learning the intrinsic feature information of objects in RSI. For the second aspect, we propose a neighbor-erasing module with only one gradient flow (OGF-NEM). This module utilizes deep features to erase large objects to highlight small objects in shallow features and changes the backpropagation path to prevent the backflow of unreasonable gradients and the erosion of information from neighbor scales. Thus, a novel detector, called SDSDet, is proposed, which achieves excellent performance for small, dense, and multi-scale objects in RSIs. We have conducted exhaustive experiments on DOTA and MS COCO datasets. Specifically, the SDSDet achieves 42.8% AP on DOTA and 33.3% AP on MS COCO, together with nearly 4.87 M model size and 95 FPS.},
  archive      = {J_ICV},
  author       = {Jinping Liu and Kunyi Zheng and Xianyi Liu and Pengfei Xu and Ying Zhou},
  doi          = {10.1016/j.imavis.2024.104898},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104898},
  shortjournal = {Image Vis. Comput.},
  title        = {SDSDet: A real-time object detector for small, dense, multi-scale remote sensing objects},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting classifier inter-level features for efficient
out-of-distribution detection. <em>ICV</em>, <em>142</em>, 104897. (<a
href="https://doi.org/10.1016/j.imavis.2023.104897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning approaches have achieved state-of-the-art performance in a wide range of applications. Most often, however, it is falsely assumed that samples at inference follow a similar distribution as the training data. This assumption impairs models&#39; ability to handle Out-of-Distribution (OOD) data during deployment. While several OOD detection approaches mostly focus on outputs of the last layer, we propose a novel mechanism that exploits features extracted from intermediate layers of a deep classifier. Specifically, we train an off-the-shelf auxiliary network using features of early layers to learn distinctive representations that improve OOD detection. The proposed network can be appended to any classification model without imposing any modification to its original architecture. Additionally, the mechanism does not require access to OOD data during training. We evaluate the performance of the mechanism on a variety of backbone architectures and datasets for near-OOD and far-OOD scenarios. The results demonstrate improvements in OOD detection compared to other state-of-the-art approaches. In particular, our proposed mechanism improves AUROC by 14.2% and 8.3% in comparison to the strong OOD baseline method, and by 3.2% and 3.9% in comparison to the second-best performing approach, on CIFAR-10 and CIFAR-100 datasets respectively.},
  archive      = {J_ICV},
  author       = {Jamil Fayyad and Kashish Gupta and Navid Mahdian and Dominique Gruyer and Homayoun Najjaran},
  doi          = {10.1016/j.imavis.2023.104897},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104897},
  shortjournal = {Image Vis. Comput.},
  title        = {Exploiting classifier inter-level features for efficient out-of-distribution detection},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Universal domain adaptation from multiple black-box sources.
<em>ICV</em>, <em>142</em>, 104896. (<a
href="https://doi.org/10.1016/j.imavis.2023.104896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Black-box domain adaptation (B2DA) is a practical unsupervised domain adaptation (UDA) setting, in which only an interface to source model is available due to privacy or security concerns. It aims to transfer source knowledge with no source data or model, but only queries for target samples from the source interface. Previous B2DA methods commonly assume a single source interface and shared label space across domains. Whereas in real tasks, there are usually multiple source interfaces with label shift from the target domain, how to effectively combine them without target ground-truth has not been fully investigated yet so far. We term such setting as Universal Domain Adaptation from Multiple Black-box sources (Uni-MBDA), and propose a new Universal multi-source Black-Box method (Um2B) through Dual-Attention on both source domains and classes. In Um2B, adaptive domain attention is first introduced to seek the optimal combination of source interfaces, whose performance is no worse than the single best one. Second, to address the label shift across domains, adaptive class attention is adopted in adaptation to suppress the private source classes, then automatically identify target unknowns and separate them apart from the known classes. Empirical results over benchmark datasets show the effectiveness of Um2B, and in most cases, it performs no worse than the best single-source.},
  archive      = {J_ICV},
  author       = {Yunyun Wang and Jian Mao and Cong Zou and Xinyang Kong},
  doi          = {10.1016/j.imavis.2023.104896},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104896},
  shortjournal = {Image Vis. Comput.},
  title        = {Universal domain adaptation from multiple black-box sources},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SGF3D: Similarity-guided fusion network for 3D object
detection. <em>ICV</em>, <em>142</em>, 104895. (<a
href="https://doi.org/10.1016/j.imavis.2023.104895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The representation of pseudo point cloud can significantly improve the precision of 3D object detection. However, existing pseudo point cloud-based methods typically fuse the processed features through coarse concatenation, which ignores the consistency between the point cloud and pseudo point cloud features. The inconsistency of features in different modal data can lead to detection bias. In this paper, we propose a novel pseudo point cloud-based network called SGF3D, which utilizes a cross-modal attention module cross-modal attention fusion (CMAF) to fuse point cloud and pseudo point cloud features. It can better learn the cross-modal similarity of output features, enabling the detection box to fit better with the target. We also designed a region of interest (RoI) head similarity attention head (SAH) to utilize the overlooked similarity to optimize training without increasing the complexity of the network. By using CMAF and SAH, the proposed method can obtain more accurate bounding boxes. Extensive experiments on KITTI dataset demonstrate that the proposed method can achieve competitive results. Training code and well trained weights are available at https://github.com/ChunZheng2022/SGF3D .},
  archive      = {J_ICV},
  author       = {Chunzheng Li and Gaihua Wang and Qian Long and Zhengshu Zhou},
  doi          = {10.1016/j.imavis.2023.104895},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104895},
  shortjournal = {Image Vis. Comput.},
  title        = {SGF3D: Similarity-guided fusion network for 3D object detection},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Top-tuning: A study on transfer learning for an efficient
alternative to fine tuning for image classification with fast kernel
methods. <em>ICV</em>, <em>142</em>, 104894. (<a
href="https://doi.org/10.1016/j.imavis.2023.104894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impressive performance of deep learning architectures is associated with a massive increase in model complexity. Millions of parameters need to be tuned, with training and inference time scaling accordingly, together with energy consumption. But is massive fine-tuning always necessary? In this paper, focusing on image classification , we consider a simple transfer learning approach exploiting pre-trained convolutional features as input for a fast-to-train kernel method . We refer to this approach as top-tuning since only the kernel classifier is trained on the target dataset. In our study, we perform more than 3000 training processes focusing on 32 small to medium-sized target datasets, a typical situation where transfer learning is necessary. We show that the top-tuning approach provides comparable accuracy with respect to fine-tuning, with a training time between one and two orders of magnitude smaller. These results suggest that top-tuning is an effective alternative to fine-tuning in small/medium datasets, being especially useful when training time efficiency and computational resources saving are crucial.},
  archive      = {J_ICV},
  author       = {Paolo Didier Alfano and Vito Paolo Pastore and Lorenzo Rosasco and Francesca Odone},
  doi          = {10.1016/j.imavis.2023.104894},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104894},
  shortjournal = {Image Vis. Comput.},
  title        = {Top-tuning: A study on transfer learning for an efficient alternative to fine tuning for image classification with fast kernel methods},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCTrans: Self-align and cross-align transformer for few-shot
segmentation. <em>ICV</em>, <em>142</em>, 104893. (<a
href="https://doi.org/10.1016/j.imavis.2023.104893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot Semantic Segmentation (FSS) refers to train a segmentation model that can be generalized to novel categories with limited labeled images. One challenge of FSS is spatial inconsistency between support and query images, e.g., appearance and texture. Most existing methods are only committed to utilizing the semantic-level prototypes of support images to guide mask predictions. These methods, nevertheless, only focus on the most discriminate regions of the object rather than holonomic feature representations. Besides, another question exists that the lack of interaction between paired support and query images. In this paper, we propose a self-align and cross-align transformer (SCTrans) to remedy the above limitations. Specifically, we design a feature fusion module (FFM) to incorporate low-level information from the query branch into mid-level semantic features , boosting the semantic representations of query images. In addition, a feature alignment module is designed to bidirectionally propagate semantic information from support to query images conditioned on more representative support and query features, increasing both intra-class similarities and inter-class differences. Extensive experiments on PASCAL-5i and COCO-20i show that our SCTrans significantly advances the state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Jun Ding and Zhen Zhang and QiYu Wang and HuiBin Wang},
  doi          = {10.1016/j.imavis.2023.104893},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104893},
  shortjournal = {Image Vis. Comput.},
  title        = {SCTrans: Self-align and cross-align transformer for few-shot segmentation},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EFDCNet: Encoding fusion and decoding correction network for
RGB-d indoor semantic segmentation. <em>ICV</em>, <em>142</em>, 104892.
(<a href="https://doi.org/10.1016/j.imavis.2023.104892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a crucial task in vision measurement systems that involves understanding and segmenting different objects and regions within an image. Over the years, numerous RGB-D semantic segmentation methods have been developed, leveraging the encoder-decoder architecture to achieve outstanding performance. However, existing methods have two main problems that constrain further performance improvement . Firstly, in the encoding stage, existing methods have a weak ability to fuse cross-modal information, and low-quality depth maps can easily lead to poor feature representation. Secondly, in the decoding stage, the upsampling of high-level semantic information may cause the loss of contextual information, and low-level features from the encoder may bring noises to the decoder through skip connections. To solve these issues, we propose a novel Encoding Fusion and Decoding Correction Network (EFDCNet) for RGB-D indoor semantic segmentation. First, in the encoding stage of EFDCNet, we focus on extracting valuable information from low-quality depth maps, and employ a channel-wise filter to select informative depth features. Additionally, we establish the global dependencies between RGB and depth features via the self-attention mechanism to enhance the cross-modal feature interactions, extracting discriminant and powerful features. Then, in the decoding stage of EFDCNet, we use the highest-level information as semantic guidance to compensate for the upsampling information and filter out noise from the low-level encoder features propagated through the skip connections to the decoder. Extensive experiments conducted on two widely-used RGB-D indoor semantic segmentation datasets demonstrate that the proposed EFDCNet surpasses the performance of relevant state-of-the-art methods. The code is available at https://github.com/ Mark9010/EFDCNet},
  archive      = {J_ICV},
  author       = {Jianlin Chen and Gongyang Li and Zhijiang Zhang and Dan Zeng},
  doi          = {10.1016/j.imavis.2023.104892},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104892},
  shortjournal = {Image Vis. Comput.},
  title        = {EFDCNet: Encoding fusion and decoding correction network for RGB-D indoor semantic segmentation},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving multi-focus image fusion through noisy image and
feature difference network. <em>ICV</em>, <em>142</em>, 104891. (<a
href="https://doi.org/10.1016/j.imavis.2023.104891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited by optical constraints, the acquisition of a comprehensive set of multi-focus images remains a challenge. Leveraging the intrinsic accuracy of focused images, the concept of multi-focus image fusion emerges. However, distinguishing between focused and defocused regions, despite their visual similarities, is complex due to the absence of direct numerical indicators. This study unveils a novel insight: noisy source images lead to more substantial information loss in focus areas than in defocused regions. This observation prompts us to exploit this discrepancy by introducing noise to the source images. Motivated by this discovery, we introduce the Feature Difference Network (DDMF) for Multi-Focus Image Fusion (MFIF), aiming to leverage the differences present within feature dimensions. The pioneering DDMF approach incorporates the diffusion process from the Denoising Diffusion Probabilistic Models , employing it as a mechanism to introduce Gaussian noise to source images. Furthermore, the denoising process enhances feature representation. This equips DDMF to effectively capture hidden differences within features, allowing precise categorization of each pixel. Our extensive experimental evaluation underscores the prowess of DDMF. Through both subjective visual assessment and objective evaluation metrics, DDMF emerges as a frontrunner, surpassing established state-of-the-art MFIF methods. ARTICLE INFO.},
  archive      = {J_ICV},
  author       = {Xin Pan and Hao Zhai and You Yang and Lianhua Chen and Anyu Li},
  doi          = {10.1016/j.imavis.2023.104891},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104891},
  shortjournal = {Image Vis. Comput.},
  title        = {Improving multi-focus image fusion through noisy image and feature difference network},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature extraction and representation learning of 3D point
cloud data. <em>ICV</em>, <em>142</em>, 104890. (<a
href="https://doi.org/10.1016/j.imavis.2023.104890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional point cloud data serves as a critical source of information in various real-world application domains, such as computer vision , robotics, geographic information systems , and medical image processing . Due to the discrete and unordered nature of point clouds, applying 2D image feature extractors directly to the extraction of 3D point cloud features is challenging. Therefore, we propose a novel variational feature component extraction method called PointFEA. This paper aims to research and propose a series of methods to enhance the feature extraction and representation learning of 3D point cloud data. Firstly, in terms of feature extraction, local neighborhood encoding is combined with the local latent representation of point clouds to obtain more correlated point cloud features. Secondly, in the domain of point cloud representation learning, the multi-scale representation learning method maps point cloud data into a high-dimensional space to better capture critical features and adapt to different granularities of point cloud data. Lastly, features of different dimensions are input into a cross-fusion transformer to obtain local attention coefficients. We validate our methods on commonly used point cloud datasets, and the experiments demonstrate the effectiveness of our approach, achieving accuracies of 94.8% on ModelNet40 and 89.1% on ScanObjectNN.},
  archive      = {J_ICV},
  author       = {Hongying Si and Xianyong Wei},
  doi          = {10.1016/j.imavis.2023.104890},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104890},
  shortjournal = {Image Vis. Comput.},
  title        = {Feature extraction and representation learning of 3D point cloud data},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camera-aware re-identification feature for multi-target
multi-camera tracking. <em>ICV</em>, <em>142</em>, 104889. (<a
href="https://doi.org/10.1016/j.imavis.2023.104889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-target multi-camera tracking is an important research topic in intelligent surveillance to achieve re-identification (Re-ID) of moving targets across different cameras. However, Re-ID faces significant challenges owing to variations in cameras/viewpoints, making it difficult to learn discriminative feature representations for targets captured from different cameras/viewpoints. A new pipeline is introduced by balancing the across-cameras sample feature space in camera-aware Re-ID framework. Specifically, different sampling strategies play a crucial role on the performance under the same baseline and identify the feature discrepancy between cameras/viewpoints as a crucial factor. This proposed sampling strategy is called camera equalization sampling to learn enhanced feature disparities, which balances cameras under identity rather than randomly sampling in a batch. The sampled images are combined with non visual cues(cameras position encoding) to reduce intra class variance . For the mechanism of camera equalization sampling, the improved camera centric loss function can better reduce the negative impact of individual samples and provide stable learnable features.Our proposed method consists of three modules. (i) Camera equalization (CE) ensures that each batch collects at least one image from each camera for every identity, thereby enabling robust features. (ii) Camera position embedding (CPE) mitigates the scene bias caused by different cameras/viewpoints by encoding camera indices. (iii) Camera Center triplet loss(CCL) based on CE improves higher robustness to outliers and noisy labels. We demonstrated the proposed method&#39;s effectiveness on popular datasets such as DukeMTMC-reID, Market-1501, and MSMT17, achieving state-of-the-art performance.},
  archive      = {J_ICV},
  author       = {Jinlong Zhu and Qingliang Li and Changbo Gao and Yu Ge and Ke Xu},
  doi          = {10.1016/j.imavis.2023.104889},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104889},
  shortjournal = {Image Vis. Comput.},
  title        = {Camera-aware re-identification feature for multi-target multi-camera tracking},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance metric-based learning for long-tail object
detection. <em>ICV</em>, <em>142</em>, 104888. (<a
href="https://doi.org/10.1016/j.imavis.2023.104888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent success of general object detection, almost all models perform unsatisfactorily on long-tail datasets. The main cause of performance degradation is the imbalance in the number of positive samples between categories. The traditional approaches can lead to distortion of the classification feature space , which in turn can seriously affect the classification ability of the network. To address the above issues, we propose a novel distance metric-based learning approach for long-tail object detection (LTDL) in this paper. Specifically, we directly use the feature space as the optimization target, thus allowing clearer decision boundaries between classes. In order to optimize the decision boundary, we adjust the intra-class and inter-class distances by Margin Module (MAM). Meanwhile, to further exploit the information provided by the dataset, we introduce supervised information of labels for distance weighting using the Semantic Module (SEM). In addition, to protect the learning of tail samples and optimize the classifier, we propose a Distance-based Equilibrium Loss (DEL). Extensive experiments conducted on the LVIS benchmark have demonstrated the strength of our proposed approach. The experimental results show that our method improves the baseline by 2.9% AP. And our best model can outperform almost all other representative methods.},
  archive      = {J_ICV},
  author       = {Mingwen Shao and Zilu Peng},
  doi          = {10.1016/j.imavis.2023.104888},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104888},
  shortjournal = {Image Vis. Comput.},
  title        = {Distance metric-based learning for long-tail object detection},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on efficient detection network method for remote
sensing images based on self attention mechanism. <em>ICV</em>,
<em>142</em>, 104884. (<a
href="https://doi.org/10.1016/j.imavis.2023.104884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing images are widely used in aerial drones, satellites, and other fields. However, traditional object detection methods face challenges of low efficiency and accuracy due to the diverse scales, numerous target types, and complex backgrounds of remote sensing images. To address these issues, this paper proposes a remote sensing image detection network based on self-attention, which adaptively learns the relative importance of pixels to achieve more precise and efficient object detection in remote sensing images, meeting the demands of large-scale remote sensing image processing . Firstly, through end-to-end training, the object detection network is optimized as a whole to directly output detection results from the input remote sensing images, eliminating the need for additional intermediate steps. This not only reduces the impact of information loss and inconsistency but also simplifies the entire detection process. Secondly, we integrate the Faster R-CNN architecture as the foundation, combining region extraction and object classification into a unified process. Lastly, we embed self-attention mechanisms at different levels of the Faster R-CNN to progressively extract multi-scale and multi-level feature information, enhancing the network&#39;s ability to learn the correlation of information from different positions in the image, automatically capturing the relationships between objects, and improving the accuracy of object detection. This significantly reduces redundant computation, making it more efficient for large-scale remote sensing image processing. Experimental verification demonstrates that this approach outperforms traditional methods in terms of detection accuracy and efficiency, better addressing the particularities of remote sensing images, and providing an efficient and precise solution for aerial drone and satellite image processing. Remote sensing image detection has become one of the research hotspots in the field of remote sensing, holding significant theoretical significance and practical application value.},
  archive      = {J_ICV},
  author       = {Jing Li and XiaoMeng Wei},
  doi          = {10.1016/j.imavis.2023.104884},
  journal      = {Image and Vision Computing},
  month        = {2},
  pages        = {104884},
  shortjournal = {Image Vis. Comput.},
  title        = {Research on efficient detection network method for remote sensing images based on self attention mechanism},
  volume       = {142},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A locally weighted, correlated subdomain adaptive network
employed to facilitate transfer learning. <em>ICV</em>, <em>141</em>,
104887. (<a href="https://doi.org/10.1016/j.imavis.2023.104887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation has found extensive applications in diverse fields, including transfer learning , deep learning , and image processing , to effectively address the challenge of mismatched data distributions. Nevertheless, persistent challenges such as limited transfer data and the occurrence of negative transfer continue to exist. These issues not only increase the complexity of model learning but also affect the accuracy of the transfer results. To tackle these challenges, this paper introduces a novel approach called the Related Weighted Subdomain Adaptive Network (RWSAN). This network constructs related subdomain clusters in the source domain to encompass a wider spectrum of associated information. It aims to align target domain subdomains with the related subdomain clusters in the source domain as closely as possible, thus increasing the number of transferable samples from the source domain and solving insufficient transferable samples in some adaptations. To address the issue of negative transfer arising from irrelevant subdomains in the source domain during the transfer process, a related weighted maximum mean discrepancy method is introduced. This method optimizes transfer weights, minimizes the feature distribution disparity between the target domain and the source domain, and mitigates the negative impact of irrelevant subdomains on the target domain, thereby enhancing the accuracy of domain adaptation during transfer. To evaluate the proposed RWSAN method, comprehensive performance tests were conducted using widely used red-green-blue (RGB) and hyperspectral image (HIS) image datasets. The results demonstrate that the RWSAN method effectively resolves insufficient transfer data and negative transfer, proving its high reliability. Additionally, it exhibits excellent performance in error analysis, network convergence analysis, data visualization, and distribution difference.},
  archive      = {J_ICV},
  author       = {Tuo Xu and Bing Han and Jie Li and Yuefan Du},
  doi          = {10.1016/j.imavis.2023.104887},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104887},
  shortjournal = {Image Vis. Comput.},
  title        = {A locally weighted, correlated subdomain adaptive network employed to facilitate transfer learning},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved SiamCAR with ranking-based pruning and optimization
for efficient UAV tracking. <em>ICV</em>, <em>141</em>, 104886. (<a
href="https://doi.org/10.1016/j.imavis.2023.104886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV tracking is a burgeoning task with vast application prospects in various fields such as agriculture, navigation, and public safety. However, the computational constraints and limited processing speed of drones hinder the deployment of Siamese tracking algorithms. In order to better apply tracking algorithms to drone devices, this paper proposes a novel Ranking-Based SiamCAR (RB-SiamCAR) tracker. The RB-SiamCAR tracker achieves model compression by utilizing a ranking-based filter pruning method, which sorts the filters in the backbone network based on their importance and prunes the filters with low ranks for efficient feature extraction. Additionally, considering existing Siamese trackers overlook the correlation between positive and negative samples and the coherence between classification and localization, we introduce two ranking-based losses. The classification ranking loss ensures that the ranking of positive samples is higher than that of hard negative samples, allowing the tracker to successfully select foreground samples without being fooled by distractors. The IoU-guided ranking loss aims to align the classification with the intersection over union (IoU) of the corresponding localizations of positive samples, enabling well-localized predictions to be represented by high classification confidence. To further enhance the tracking performance, we employ an effective channel attention module (ECA) that allows the network to automatically learn and focus on the most important channels for capturing more discriminative features . Experimental evaluations on UAV tracking benchmarks demonstrate that the proposed RB-SiamCAR outperforms existing state-of-the-art trackers. It is noteworthy that our RB-SiamCAR achieves an impressive tracking speed of nearly 100 fps. The experimental results validate its effectiveness and efficiency in UAV tracking applications.},
  archive      = {J_ICV},
  author       = {Xiaoqiang Jin and Dawei Zhang and Qiner Wu and Xin Xiao and Pengsen Zhao and Zhonglong Zheng},
  doi          = {10.1016/j.imavis.2023.104886},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104886},
  shortjournal = {Image Vis. Comput.},
  title        = {Improved SiamCAR with ranking-based pruning and optimization for efficient UAV tracking},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale conditional reconstruction generative
adversarial network. <em>ICV</em>, <em>141</em>, 104885. (<a
href="https://doi.org/10.1016/j.imavis.2023.104885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial network has become the factual standard for high-quality image synthesis . However, modeling the distribution of complex datasets (e.g. ImageNet and COCO-Stuff) remains challenging in unsupervised approaches. This is partly due to the imbalance between the generator and the discriminator during training, the discriminator easily defeats the generator because of special views. In this paper, we propose a model called multi-scale conditional reconstruction GAN (MS-GAN). The core concept of MS-GAN is to model the local density implicitly using different scales of instance conditions. Instance conditions are extracted from the target images via a self-supervised learning model. In addition, we alignment the semantic features of the observed instances by adding an additional reconstruction loss to the generator. Our MS-GAN can aggregate instance features at different scales and maximize semantic features. This allows the generator to learn additional comparative knowledge from instance features, leading to a better feature representation, thus improving the performance of the generation task. Experimental results on the ImageNet dataset and the COCO-Stuff dataset show that our method matches or exceeds the original performance in both FID and IS scores compared to the IC-GAN framework. Additionally, our precision score on the ImageNet dataset improved from 74.2% to 79.9%.},
  archive      = {J_ICV},
  author       = {Yanming Chen and Jiahao Xu and Zhulin An and Fuzhen Zhuang},
  doi          = {10.1016/j.imavis.2023.104885},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104885},
  shortjournal = {Image Vis. Comput.},
  title        = {Multi-scale conditional reconstruction generative adversarial network},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust direct linear transformation for camera pose
estimation using points. <em>ICV</em>, <em>141</em>, 104883. (<a
href="https://doi.org/10.1016/j.imavis.2023.104883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera pose estimation from 3D points and their 2D projections, known as the perspective- n -point (P n P) problem, is a fundamental task in computer vision . In this paper, we propose a robust direct linear transformation method for the PnP problem. The novelty lies in that a plane constraint based on the pinhole camera model is introduced, and an over-constrained linear equation system is obtained by merging the linear form of both the measured point correspondences and the plane constraint. As a result, our method successfully reduces the minimal number of point correspondences required by the linear solver to 4, and yields a unique solution for 4-point, 5-point, and n-point pose estimation. Experiment results, using both synthetic and real data, show that our method is substantially faster, and offers the accuracy and precision comparable with that of state-of-the-art methods.},
  archive      = {J_ICV},
  author       = {Ping Wang and Boqiao Jiao and Pengpeng Yao and Xiaoyuan Wei and Aihua Zhang},
  doi          = {10.1016/j.imavis.2023.104883},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104883},
  shortjournal = {Image Vis. Comput.},
  title        = {A robust direct linear transformation for camera pose estimation using points},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross channel weight sharing for image classification.
<em>ICV</em>, <em>141</em>, 104872. (<a
href="https://doi.org/10.1016/j.imavis.2023.104872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypercomplex convolutional neural networks (HCNNs) have recently been used to improve deep learning architectures due to their ability to share weights across input channels and thus improve the cohesiveness of learned representations within the layers. Much work has been done to study the effect of hypercomplex representation in CNNs. However, to date, none of these models have used fully hypercomplex architectures, meaning that some of the layers in these earlier networks used conventional real-valued calculations that did not engage cross-channel weight sharing. Here, we introduce and study full hypercomplex CNNs by ensuring all layers perform hypercomplex calculations. For the earlier HCNNs, the real-valued layers are found in: (I) in the front end of the networks, (II) in the back end of the networks, and (III) in the residual blocks of the networks. The present research examines the performance of HCNNs when the modules mentioned above are replaced with hypercomplex equivalents. These representational networks have outperformed and shown state-of-the-art results compared to previous hypercomplex models (on some specific datasets). A disadvantage of these representational networks is that they consume high computational costs. To reduce this cost, novel separable hypercomplex networks (SHNNs) are proposed. They are created by factoring a quaternion convolutional module into two consecutive separable vectormap convolutional modules. As the successive layers of deep HCNNs perform local hierarchical grouping on increasingly abstract features, these groupings are responsible for the long-distance interaction problems found in the dense layers. To handle these problems, researchers have applied attention mechanisms in hypercomplex space. This paper offers a perspective on the basic concepts of representational networks and attention-based representational networks with the focus on (1) reviewing deep full HCNNs, including hypercomplex-based fully connected dense layer; (2) analyzing hypercomplex-based separable concept; (3) describing how the hypercomplex spatial layer of residual bottleneck block is replaced with an attention layer; and (4) providing a comprehensive summary comparing the performance of recent deep HCNNs, deep convolutional neural networks , and original attention based networks. We compare the performance of the networks in terms of accuracy and the number of trainable parameters on several image classification datasets.},
  archive      = {J_ICV},
  author       = {Nazmul Shahadat and Anthony S. Maida},
  doi          = {10.1016/j.imavis.2023.104872},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104872},
  shortjournal = {Image Vis. Comput.},
  title        = {Cross channel weight sharing for image classification},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer multi-source knowledge via scale-aware online
domain adaptation in depth estimation for autonomous driving.
<em>ICV</em>, <em>141</em>, 104871. (<a
href="https://doi.org/10.1016/j.imavis.2023.104871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the challenging online monocular depth adaptation task that aims to train an initial depth estimation model in a source domain and continuously adapt the model against a constantly changing target domain. Due to the high cost of real-world data collection and the camera-dependent nature of the depth estimation, previous works tend to simulate the environment using a virtual-world dataset for training (e.g. Virtual KITTI), and employ a real-world dataset for testing (e.g. KITTI, which shares the same camera settings) and are therefore vulnerable to novel domains with unknown statistics. We propose a meta-learning-based online domain adaptation framework that can leverage multi-source domain to transfer the learned knowledge from the virtual world to the real world better with the metrically accurate scale. Our learn-to-adapt algorithm mimics domain shifts during training by creating fictitious testing domains and incorporating a meta-optimization objective for optimizing the performance of the testing domains after updating the training domains in each mini-batch step. The algorithm is augmented with gradient surgery to alleviate unreliable optimization of inconsistent regions. To facilitate multi-source domain training and testing, we introduce a camera conversion technique for transforming images and depth cues from different camera settings to a unified one. During online adaptation, we also apply the exact statistics of the test-time input to the network&#39;s normalization layers to ensure a more robust adaptation. Extensive experiments demonstrate that our method can robustly adapt and outperform the virtual-to-real state-of-the-art methods on the standard KITTI Eigen benchmark of both full-length videos and isolated frames, a feat never attempted before, as well as generalizing to other real-world datasets without retraining.},
  archive      = {J_ICV},
  author       = {Phan Thi Huyen Thanh and Minh Quan Viet Bui and Duc Dung Nguyen and Tran Vu Pham and Truong Vinh Truong Duy and Natori Naotake},
  doi          = {10.1016/j.imavis.2023.104871},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104871},
  shortjournal = {Image Vis. Comput.},
  title        = {Transfer multi-source knowledge via scale-aware online domain adaptation in depth estimation for autonomous driving},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The triple refinement of self-paced learning style for
unsupervised cross-domain person re-identification. <em>ICV</em>,
<em>141</em>, 104870. (<a
href="https://doi.org/10.1016/j.imavis.2023.104870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised cross-domain person re-identification based on clustering typically involves generating pseudo-labels for all training data . However, some data are prone to generating incorrect pseudo-labels in the early stages of training, which leads to a decrease in the accuracy of the model. Based on the idea of self-paced learning, from easy to difficult and from a few to many samples, we propose the Triple Refinement of Self-paced Learning style (TRSL) method to solve this problem. Firstly, we use the density-based clustering algorithm DBSCAN to perform the first data refinement on the unlabeled data . Then, in the second refinement step, we calculate the challenge value by exploring the distance distribution between the clustering data and the centroids to refine the data. In the third refinement step, we calculate the credibility value of the data by comparing the k-nearest neighbor ranking lists of the local and global features of the images, use the credibility value to recall the discarded data from the dual refinement process, and perform a third filtering on the retained data. Finally, the processed data is input into a pair of mutually beneficial learning models for training. On top of a strong baseline, our method performs better than the recent state-of-the-art methods in some evaluation metrics .},
  archive      = {J_ICV},
  author       = {Shuren Zhou and Nanfang Lei and Jiarui Zhou and Jiasi Xiong and Jianming Zhang},
  doi          = {10.1016/j.imavis.2023.104870},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104870},
  shortjournal = {Image Vis. Comput.},
  title        = {The triple refinement of self-paced learning style for unsupervised cross-domain person re-identification},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot classification with multisemantic information
fusion network. <em>ICV</em>, <em>141</em>, 104869. (<a
href="https://doi.org/10.1016/j.imavis.2023.104869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric-based methods aim to predict class labels by computing the similarity between samples using distance functions, which is the mainstream approach to few-shot learning. However, the limited representational space of feature vectors and appearance variations among congenetic samples still present challenges. We propose a Multisemantic Information Fusion Network (MIFN) to address these problems. A Lower-level Feature generator (LF-generator), which is an unsupervised module, adaptively activates high-response regions of objects to introduce discriminative semantic details. Meanwhile, a Higher-level Feature extractor (HF-extractor) learns global semantic information with human cognition to minimise the impact of appearance variations. We integrate the coarse outputs of these two modules, which complement each other to jointly promote more precise predictions. Furthermore, considering the importance of prototypes, we redefine the sampling strategy of the triplet loss and utilise it as an auxiliary loss to sharpen the decision boundary at the prototype level, facilitating subsequent classification. Our experimental results demonstrate the competitiveness of our approach in both general few-shot classification ( mini -ImageNet and tiered -ImageNet) and cross-domain problems (CUB, Caltech-101, Stanford-dogs, and Stanford-cars) with minimal bells and whistles.},
  archive      = {J_ICV},
  author       = {Ruixuan Gao and Han Su and Shitala Prasad and Peisen Tang},
  doi          = {10.1016/j.imavis.2023.104869},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104869},
  shortjournal = {Image Vis. Comput.},
  title        = {Few-shot classification with multisemantic information fusion network},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RBGAN: Realistic-generation and balanced-utility GAN for
face de-identification. <em>ICV</em>, <em>141</em>, 104868. (<a
href="https://doi.org/10.1016/j.imavis.2023.104868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of computer vision technologies and surveillance devices has led to numerous studies on vision, but it also raises privacy concerns, especially regarding facial identity information . Protecting the privacy of face identity information has prompted the development of face de-identification, which faces two main challenges. The first challenge is the realism of the generated images, while the second challenge is finding the proper balance between privacy protection and preserving data utility. To address these challenges, we propose a face de-identification model called Realistic-Generation and Balanced-Utility GAN (RBGAN). RBGAN includes a disentangled module and a symmetric-consistency-guided generation module, which aim to preserve multiple attributes while generating realistic de-identified images. We design an attribute-specialized encoder to obtain the disentangled representation of identity and attribute to balance privacy protection and data utility preservation. To generate realistic de-identified images, we incorporate symmetric consistency and face reconstruction into the face de-identification process using the Generative Adversarial Network (GAN). The disentangled module can extract richer representation, and the symmetric-consistency-guided generation module can construct more facial details. As a result, our proposed RBGAN can generate de-identified face images that closely resemble real faces and are difficult to detect by visual systems. Experiments conducted on benchmark face datasets demonstrate the effectiveness of our model in generating realistic images while preserving multiple attributes.},
  archive      = {J_ICV},
  author       = {Yaofang Zhang and Yuchun Fang and Yiting Cao and Jiahua Wu},
  doi          = {10.1016/j.imavis.2023.104868},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104868},
  shortjournal = {Image Vis. Comput.},
  title        = {RBGAN: Realistic-generation and balanced-utility GAN for face de-identification},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ROBUSfT: Robust real-time shape-from-template, a c ++
library. <em>ICV</em>, <em>141</em>, 104867. (<a
href="https://doi.org/10.1016/j.imavis.2023.104867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking the 3D shape of a deforming object using only monocular 2D vision is a challenging problem. This is because one should (i) infer the 3D shape from a 2D image, which is a severely underconstrained problem, and (ii) implement the whole solution pipeline in real time. The pipeline typically requires feature detection and matching, mismatch filtering, 3D shape inference and feature tracking algorithms. We propose ROBUSfT , a conventional pipeline based on a template containing the object&#39;s rest shape, texture map and deformation law . ROBUSfT is ready-to-use, wide-baseline, capable of handling large deformations , fast up to 30 fps, free of training, and robust against partial occlusions and discontinuities. It outperforms the state-of-the-art methods in challenging video datasets. ROBUSfT is implemented as a publicly available C ++ library. We provide the code, a tutorial on how to use it, and a supplementary video of our experiments at https://github.com/mrshetab/ ROBUSfT .},
  archive      = {J_ICV},
  author       = {Mohammadreza Shetab-Bushehri and Miguel Aranda and Erol Özgür and Youcef Mezouar and Adrien Bartoli},
  doi          = {10.1016/j.imavis.2023.104867},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104867},
  shortjournal = {Image Vis. Comput.},
  title        = {ROBUSfT: Robust real-time shape-from-template, a c ++ library},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Open-set face recognition with maximal entropy and
objectosphere loss. <em>ICV</em>, <em>141</em>, 104862. (<a
href="https://doi.org/10.1016/j.imavis.2023.104862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-set face recognition characterizes a scenario where unknown individuals, unseen during the training and enrollment stages, appear on operation time. This work concentrates on watchlists, an open-set task that is expected to operate at a low false-positive identification rate and generally includes only a few enrollment samples per identity. We introduce a compact adapter network that benefits from additional negative face images when combined with distinct cost functions , such as Objectosphere Loss (OS) and the proposed Maximal Entropy Loss (MEL). MEL modifies the traditional Cross-Entropy loss in favor of increasing the entropy for negative samples and attaches a penalty to known target classes in pursuance of gallery specialization. The proposed approach adopts pre-trained deep neural networks (DNNs) for face recognition as feature extractors. Then, the adapter network takes deep feature representations and acts as a substitute for the output layer of the pre-trained DNN in exchange for an agile domain adaptation . Promising results have been achieved following open-set protocols for three different datasets: LFW, IJB-C, and UCCS as well as state-of-the-art performance when supplementary negative data is properly selected to fine-tune the adapter network.},
  archive      = {J_ICV},
  author       = {Rafael Henrique Vareto and Yu Linghu and Terrance Edward Boult and William Robson Schwartz and Manuel Günther},
  doi          = {10.1016/j.imavis.2023.104862},
  journal      = {Image and Vision Computing},
  month        = {1},
  pages        = {104862},
  shortjournal = {Image Vis. Comput.},
  title        = {Open-set face recognition with maximal entropy and objectosphere loss},
  volume       = {141},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
