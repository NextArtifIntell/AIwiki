<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---261">CVIU - 261</h2>
<ul>
<li><details>
<summary>
(2024). Monocular depth estimation with boundary attention mechanism
and shifted window adaptive bins. <em>CVIU</em>, <em>249</em>, 104220.
(<a href="https://doi.org/10.1016/j.cviu.2024.104220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation is a classic research topic in computer vision. In recent years, development of Convolutional Neural Networks (CNNs) has facilitated significant breakthroughs in this field. However, there still exist two challenges: (1) The network struggles to effectively fuse edge features in the feature fusion stage, which ultimately results in the loss of structure or boundary distortion of objects in the scene. (2) Classification based studies typically depend on Transformers for global modeling, a process that often introduces substantial computational complexity overhead as described in Equation 2. In this paper, we propose two modules to address the aforementioned issues. The first module is the Boundary Attention Module (BAM), which leverages the attention mechanism to enhance the ability of the network to perceive object boundaries during the feature fusion stage. In addition, to mitigate the computational complexity overhead resulting from predicting adaptive bins, we propose a Shift Window Adaptive Bins (SWAB) module to reduce the amount of computation in global modeling. The proposed method is evaluated on three public datasets, NYU Depth V2, KITTI and SUNRGB-D, and demonstrates state-of-the-art (SOTA) performance.},
  archive      = {J_CVIU},
  author       = {Hengjia Hu and Mengnan Liang and Congcong Wang and Meng Zhao and Fan Shi and Chao Zhang and Yilin Han},
  doi          = {10.1016/j.cviu.2024.104220},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104220},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Monocular depth estimation with boundary attention mechanism and shifted window adaptive bins},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UUD-fusion: An unsupervised universal image fusion approach
via generative diffusion model. <em>CVIU</em>, <em>249</em>, 104218. (<a
href="https://doi.org/10.1016/j.cviu.2024.104218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image fusion is a classical problem in the field of image processing whose solutions are usually not unique. The common image fusion methods can only generate a fixed fusion result based on the source image pairs. They tend to be applicable only to a specific task and have high computational costs. Hence, in this paper, a two-stage unsupervised universal image fusion with generative diffusion model is proposed, termed as UUD-Fusion. For the first stage, a strategy based on the initial fusion results is devised to offload the computational effort. For the second stage, two novel sampling algorithms based on generative diffusion model are designed. The fusion sequence generation algorithm (FSGA) searches for a series of solutions in the solution space by iterative sampling. The fusion image enhancement algorithm (FIEA) greatly improves the quality of the fused images. Qualitative and quantitative evaluations of multiple datasets with different modalities demonstrate the great versatility and effectiveness of UUD-Fusion. It is capable of solving different fusion problems, including multi-focus image fusion task, multi-exposure image fusion task, infrared and visible fusion task, and medical image fusion task. The proposed approach is superior to current state-of-the-art methods. Our code is publicly available at https://github.com/xiangxiang-wang/UUD-Fusion .},
  archive      = {J_CVIU},
  author       = {Xiangxiang Wang and Lixing Fang and Junli Zhao and Zhenkuan Pan and Hui Li and Yi Li},
  doi          = {10.1016/j.cviu.2024.104218},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104218},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UUD-fusion: An unsupervised universal image fusion approach via generative diffusion model},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous image denoising and completion through
convolutional sparse representation and nonlocal self-similarity.
<em>CVIU</em>, <em>249</em>, 104216. (<a
href="https://doi.org/10.1016/j.cviu.2024.104216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low rank matrix approximation (LRMA) has been widely studied due to its capability of approximating original image from the degraded image. According to the characteristics of degraded images, image denoising and image completion have become research objects. Existing methods are usually designed for a single task. In this paper, focusing on the task of simultaneous image denoising and completion, we propose a weighted low rank sparse representation model and the corresponding efficient algorithm based on LRMA. The proposed method integrates convolutional analysis sparse representation (ASR) and nonlocal statistical modeling to maintain local smoothness and nonlocal self-similarity (NLSM) of natural images. More importantly, we explore the alternating direction method of multipliers (ADMM) to solve the above inverse problem efficiently due to the complexity of simultaneous image denoising and completion. We conduct experiments on image completion for partial random samples and mask removal with different noise levels. Extensive experiments on four datasets, i.e., Set12, Kodak, McMaster, and CBSD68, show that the proposed method prevents the transmission of noise while completing images and has achieved better quantitative results and human visual quality compared to 17 methods. The proposed method achieves (1.9%, 1.8%, 4.2%, and 3.7%) gains in average PSNR and (4.2%, 2.9%, 6.7%, and 6.6%) gains in average SSIM over the sub-optimal method across the four datasets, respectively. We also demonstrate that our method can handle the challenging scenarios well. Source code is available at https://github.com/weimin581/demo_CSRNS .},
  archive      = {J_CVIU},
  author       = {Weimin Yuan and Yuanyuan Wang and Ruirui Fan and Yuxuan Zhang and Guangmei Wei and Cai Meng and Xiangzhi Bai},
  doi          = {10.1016/j.cviu.2024.104216},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104216},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Simultaneous image denoising and completion through convolutional sparse representation and nonlocal self-similarity},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate prototype representation for domain-generalized
incremental learning. <em>CVIU</em>, <em>249</em>, 104215. (<a
href="https://doi.org/10.1016/j.cviu.2024.104215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models often suffer from catastrophic forgetting when fine-tuned with samples of new classes. This issue becomes even more challenging when there is a domain shift between training and testing data. In this paper, we address the critical yet less explored Domain-Generalized Class-Incremental Learning (DGCIL) task. We propose a DGCIL approach designed to memorize old classes, adapt to new classes, and reliably classify objects from unseen domains. Specifically, our loss formulation maintains classification boundaries while suppressing domain-specific information for each class. Without storing old exemplars, we employ knowledge distillation and estimate the drift of old class prototypes as incremental training progresses. Our prototype representations are based on multivariate Normal distributions, with means and covariances continually adapted to reflect evolving model features, providing effective representations for old classes. We then sample pseudo-features for these old classes from the adapted Normal distributions using Cholesky decomposition. Unlike previous pseudo-feature sampling strategies that rely solely on average mean prototypes, our method captures richer semantic variations. Experiments on several benchmarks demonstrate the superior performance of our method compared to the state of the art.},
  archive      = {J_CVIU},
  author       = {Can Peng and Piotr Koniusz and Kaiyu Guo and Brian C. Lovell and Peyman Moghadam},
  doi          = {10.1016/j.cviu.2024.104215},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104215},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multivariate prototype representation for domain-generalized incremental learning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A MLP architecture fusing RGB and CASSI for computational
spectral imaging. <em>CVIU</em>, <em>249</em>, 104214. (<a
href="https://doi.org/10.1016/j.cviu.2024.104214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coded Aperture Snapshot Spectral Imaging (CASSI) system offers significant advantages in dynamically acquiring hyper-spectral images compared to traditional measurement methods. However, it faces the following challenges: (1) Traditional masks rely on random patterns or analytical design, limiting CASSI’s performance improvement. (2) Existing CASSI reconstruction algorithms do not fully utilize RGB information. (3) High-quality reconstruction algorithms are often slow and limited to offline scene reconstruction. To address these issues, this paper proposes a new MLP architecture, Spectral–Spatial MLP (SSMLP), which replaces the transformer structure with a network using CASSI measurements and RGB as multimodal inputs. This maintains reconstruction quality while significantly improving reconstruction speed. Additionally, we constructed a teacher-student network (SSMLP with a teacher, SSMLP-WT) to transfer the knowledge learned from a large model to a smaller network, further enhancing the smaller network’s accuracy. Extensive experiments show that SSMLP matches the performance of transformer-based structures in spectral image reconstruction while improving inference speed by at least 50%. The reconstruction quality of SSMLP-WT is further improved by knowledge transfer without changing the network, and the teacher boosts the performance by 0.92 dB (44.73 dB vs. 43.81 dB).},
  archive      = {J_CVIU},
  author       = {Zeyu Cai and Ru Hong and Xun Lin and Jiming Yang and YouLiang Ni and Zhen Liu and Chengqian Jin and Feipeng Da},
  doi          = {10.1016/j.cviu.2024.104214},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104214},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A MLP architecture fusing RGB and CASSI for computational spectral imaging},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GCN and transformer complementary network for
skeleton-based action recognition. <em>CVIU</em>, <em>249</em>, 104213.
(<a href="https://doi.org/10.1016/j.cviu.2024.104213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolution Networks (GCNs) have been widely used in skeleton-based action recognition. Although there are significant progress, the inherent limitation still lies in the restricted receptive field of GCN, hindering its ability to extract global dependencies effectively. And the joints that are structurally separated can also have strong correlation. Previous works rarely explore local and global correlations of joints, leading to insufficiently model the complex dynamics of skeleton sequences. To address this issue, we propose a GCN and Transformer complementary network (GTC-Net) that allows parallel communications between GCN and Transformer domains. Specifically, we introduce a graph convolution and self-attention combined module (GAM), which can effectively leverage the complementarity of GCN and self-attention to perceive local and global dependencies of joints for the human body. Furthermore, in order to address the problems of long-term sequence ordering and position detection, we design a position-aware module (PAM), which can explicitly capture the ordering information and unique identity information for body joints of skeleton sequence. Extensive experiments on NTU RGB+D 60 and NTU RGB+D 120 datasets are conducted to evaluate our proposed method. The results demonstrate that our method can achieve competitive results on both datasets.},
  archive      = {J_CVIU},
  author       = {Xuezhi Xiang and Xiaoheng Li and Xuzhao Liu and Yulong Qiao and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2024.104213},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104213},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A GCN and transformer complementary network for skeleton-based action recognition},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A large corpus for the recognition of greek sign language
gestures. <em>CVIU</em>, <em>249</em>, 104212. (<a
href="https://doi.org/10.1016/j.cviu.2024.104212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition (SLR) from videos constitutes a captivating problem in gesture recognition, requiring the interpretation of hand movements, facial expressions, and body postures. The complexity of sign formation, signing variability among signers, and the technical hurdles of visual detection and tracking render SLR a challenging task. At the same time, the scarcity of large-scale SLR datasets, which are critical for developing robust data-intensive deep-learning SLR models, exacerbates these issues. In this article, we introduce a multi-signer video corpus of Greek Sign Language (GSL), which is the largest GSL database to date, serving as a valuable resource for SLR research. This corpus comprises an extensive RGB+D video collection that conveys rich lexical content in a multi-modal fashion, encompassing three subsets: (i) isolated signs; (ii) continuous signing; and (iii) continuous alphabet fingerspelling of words. Moreover, we introduce a comprehensive experimental setup that paves the way for more accurate and robust SLR solutions. In particular, except for the multi-signer (MS) and signer-independent (SI) settings, we employ a signer-adapted (SA) experimental paradigm, facilitating a comprehensive evaluation of system performance across various scenarios. Further, we provide three baseline SLR systems for isolated signs, continuous signing, and continuous fingerspelling. These systems leverage cutting-edge methods in deep learning and sequence modeling to capture the intricate temporal dynamics inherent in sign gestures. The models are evaluated on the three corpus subsets, setting their state-of-the-art recognition benchmark. The SL-ReDu GSL corpus, including its recommended experimental frameworks, is publicly available at https://sl-redu.e-ce.uth.gr/corpus .},
  archive      = {J_CVIU},
  author       = {Katerina Papadimitriou and Galini Sapountzaki and Kyriaki Vasilaki and Eleni Efthimiou and Stavroula-Evita Fotinea and Gerasimos Potamianos},
  doi          = {10.1016/j.cviu.2024.104212},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104212},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A large corpus for the recognition of greek sign language gestures},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 3D scene generation for zero-shot learning using ChatGPT
guided language prompts. <em>CVIU</em>, <em>249</em>, 104211. (<a
href="https://doi.org/10.1016/j.cviu.2024.104211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning in the realm of 3D point cloud data remains relatively unexplored compared to its 2D image counterpart. This domain introduces fresh challenges due to the absence of robust pre-trained feature extraction models. To tackle this, we introduce a prompt-guided method for 3D scene generation and supervision, enhancing the network’s ability to comprehend the intricate relationships between seen and unseen objects. Initially, we utilize basic prompts resembling scene annotations generated from one or two point cloud objects. Recognizing the limited diversity of basic prompts, we employ ChatGPT to expand them, enriching the contextual information within the descriptions. Subsequently, leveraging these descriptions, we arrange point cloud objects’ coordinates to fabricate augmented 3D scenes. Lastly, employing contrastive learning, we train our proposed architecture end-to-end, utilizing pairs of 3D scenes and prompt-based captions. We posit that 3D scenes facilitate more efficient object relationships than individual objects, as demonstrated by the effectiveness of language models like BERT in contextual understanding. Our prompt-guided scene generation method amalgamates data augmentation and prompt-based annotation, thereby enhancing 3D ZSL performance. We present ZSL and generalized ZSL results on both synthetic (ModelNet40, ModelNet10, and ShapeNet) and real-scanned (ScanOjbectNN) 3D object datasets. Furthermore, we challenge the model by training with synthetic data and testing with real-scanned data, achieving state-of-the-art performance compared to existing 2D and 3D ZSL methods in the literature. Codes and models are available at: https://github.com/saharahmadisohraviyeh/ChatGPT_ZSL_3D .},
  archive      = {J_CVIU},
  author       = {Sahar Ahmadi and Ali Cheraghian and Townim Faisal Chowdhury and Morteza Saberi and Shafin Rahman},
  doi          = {10.1016/j.cviu.2024.104211},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104211},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {3D scene generation for zero-shot learning using ChatGPT guided language prompts},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reverse stable diffusion: What prompt was used to generate
this image? <em>CVIU</em>, <em>249</em>, 104210. (<a
href="https://doi.org/10.1016/j.cviu.2024.104210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image diffusion models have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we study the task of predicting the prompt embedding given an image generated by a generative diffusion model. We consider a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise ( i . e . that are better aligned). We conduct experiments on the DiffusionDB data set, predicting text prompts from images generated by Stable Diffusion. In addition, we make an interesting discovery: training a diffusion model on the prompt generation task can make the model generate images that are much better aligned with the input prompts, when the model is directly reused for text-to-image generation. Our code is publicly available for download at https://github.com/CroitoruAlin/Reverse-Stable-Diffusion .},
  archive      = {J_CVIU},
  author       = {Florinel-Alin Croitoru and Vlad Hondru and Radu Tudor Ionescu and Mubarak Shah},
  doi          = {10.1016/j.cviu.2024.104210},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104210},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Reverse stable diffusion: What prompt was used to generate this image?},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Invisible backdoor attack with attention and steganography.
<em>CVIU</em>, <em>249</em>, 104208. (<a
href="https://doi.org/10.1016/j.cviu.2024.104208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the development and widespread application of deep neural networks (DNNs), backdoor attacks have posed new security threats to the training process of DNNs. Backdoor attacks on neural networks undermine the security and trustworthiness of DNNs by implanting hidden, unauthorized triggers, leading to benign behavior on clean samples while exhibiting malicious behavior on samples containing backdoor triggers. Existing backdoor attacks typically employ triggers that are sample-agnostic and identical for each sample, resulting in poisoned images that lack naturalness and are ineffective against existing backdoor defenses. To address these issues, this paper proposes a novel stealthy backdoor attack, where the backdoor trigger is dynamic and specific to each sample. Specifically, we leverage spatial attention on images and pre-trained models to obtain dynamic triggers, which are then injected using an encoder–decoder network. The design of the injection network benefits from recent advances in steganography research. To demonstrate the effectiveness of the proposed steganographic network, we design two backdoor attack modes named ASBA and ATBA, where ASBA utilizes the steganographic network for attack, while ATBA is a backdoor attack without steganography. Subsequently, we conducted attacks on Deep Neural Networks (DNNs) using four standard datasets. Our extensive experiments show that ASBA surpasses ATBA in terms of stealthiness and resilience against current defensive measures. Furthermore, both ASBA and ATBA demonstrate superior attack efficiency.},
  archive      = {J_CVIU},
  author       = {Wenmin Chen and Xiaowei Xu and Xiaodong Wang and Huasong Zhou and Zewen Li and Yangming Chen},
  doi          = {10.1016/j.cviu.2024.104208},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104208},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Invisible backdoor attack with attention and steganography},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diffusion models for counterfactual explanations.
<em>CVIU</em>, <em>249</em>, 104207. (<a
href="https://doi.org/10.1016/j.cviu.2024.104207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Counterfactual explanations have demonstrated promising results as a post-hoc framework to improve the explanatory power of image classifiers. Herein, this paper proposes DiME, a method that allows the generation of counterfactual images using the latest diffusion models. The proposed method uses a guided generative diffusion process to exploit the gradients of the target classifier to generate counterfactual explanations of the input instances. Furthermore, we examine present strategies for assessing spurious correlations and expand the assessment methods by presenting a novel measure, Correlation Difference, which is more efficient at detecting such correlations. The provided work includes a comprehensive ablation study and a thorough experimental validation demonstrating that the proposed algorithm outperforms previous state-of-the-art results on the CelebA, CelebAHQ and BDD100k datasets.},
  archive      = {J_CVIU},
  author       = {Guillaume Jeanneret and Loïc Simon and Frédéric Jurie},
  doi          = {10.1016/j.cviu.2024.104207},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104207},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Diffusion models for counterfactual explanations},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NeRFtrinsic four: An end-to-end trainable NeRF jointly
optimizing diverse intrinsic and extrinsic camera parameters.
<em>CVIU</em>, <em>249</em>, 104206. (<a
href="https://doi.org/10.1016/j.cviu.2024.104206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel view synthesis using neural radiance fields (NeRF) is the state-of-the-art technique for generating high-quality images from novel viewpoints. Existing methods require a priori knowledge about extrinsic and intrinsic camera parameters. This limits their applicability to synthetic scenes, or real-world scenarios with the necessity of a preprocessing step. Current research on the joint optimization of camera parameters and NeRF focuses on refining noisy extrinsic camera parameters and often relies on the preprocessing of intrinsic camera parameters. Further approaches are limited to cover only one single camera intrinsic. To address these limitations, we propose a novel end-to-end trainable approach called NeRFtrinsic Four. We utilize Gaussian Fourier features to estimate extrinsic camera parameters and dynamically predict varying intrinsic camera parameters through the supervision of the projection error. Our approach outperforms existing joint optimization methods on LLFF and BLEFF. In addition to these existing datasets, we introduce a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic Four is a step forward in joint optimization NeRF-based view synthesis and enables more realistic and flexible rendering in real-world scenarios with varying camera parameters.},
  archive      = {J_CVIU},
  author       = {Hannah Schieber and Fabian Deuser and Bernhard Egger and Norbert Oswald and Daniel Roth},
  doi          = {10.1016/j.cviu.2024.104206},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104206},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {NeRFtrinsic four: An end-to-end trainable NeRF jointly optimizing diverse intrinsic and extrinsic camera parameters},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M3A: A multimodal misinformation dataset for media
authenticity analysis. <em>CVIU</em>, <em>249</em>, 104205. (<a
href="https://doi.org/10.1016/j.cviu.2024.104205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of various generative models, misinformation in news media becomes more deceptive and easier to create, posing a significant problem. However, existing datasets for misinformation study often have limited modalities, constrained sources, and a narrow range of topics. These limitations make it difficult to train models that can effectively combat real-world misinformation. To address this, we propose a comprehensive, large-scale Multimodal Misinformation dataset for Media Authenticity Analysis ( M 3 A ), featuring broad sources and fine-grained annotations for topics and sentiments. To curate M 3 A , we collect genuine news content from 60 renowned news outlets worldwide and generate fake samples using multiple techniques. These include altering named entities in texts, swapping modalities between samples, creating new modalities, and misrepresenting movie content as news. M 3 A contains 708K genuine news samples and over 6M fake news samples, spanning text, images, audio, and video. M 3 A provides detailed multi-class labels, crucial for various misinformation detection tasks, including out-of-context detection and deepfake detection. For each task, we offer extensive benchmarks using state-of-the-art models, aiming to enhance the development of robust misinformation detection systems.},
  archive      = {J_CVIU},
  author       = {Qingzheng Xu and Huiqiang Chen and Heming Du and Hu Zhang and Szymon Łukasik and Tianqing Zhu and Xin Yu},
  doi          = {10.1016/j.cviu.2024.104205},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104205},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {M3A: A multimodal misinformation dataset for media authenticity analysis},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image compressive sensing reconstruction via nonlocal
low-rank residual-based ADMM framework. <em>CVIU</em>, <em>249</em>,
104204. (<a href="https://doi.org/10.1016/j.cviu.2024.104204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The nonlocal low-rank (LR) modeling has proven to be an effective approach in image compressive sensing (CS) reconstruction, which starts by clustering similar patches using the nonlocal self-similarity (NSS) prior into nonlocal image group and then imposes an LR penalty on each nonlocal image group. However, most existing methods only approximate the LR matrix directly from the degraded nonlocal image group, which may lead to suboptimal LR matrix approximation and thus obtain unsatisfactory reconstruction results. In this paper, we propose a novel nonlocal low-rank residual (NLRR) approach for image CS reconstruction, which progressively approximates the underlying LR matrix by minimizing the LR residual. To do this, we first use the NSS prior to obtaining a good estimate of the original nonlocal image group, and then the LR residual between the degraded nonlocal image group and the estimated nonlocal image group is minimized to derive a more accurate LR matrix. To ensure the optimization is both feasible and reliable, we employ an alternative direction multiplier method (ADMM) to solve the NLRR-based image CS reconstruction problem. Our experimental results show that the proposed NLRR algorithm achieves superior performance against many popular or state-of-the-art image CS reconstruction methods, both in objective metrics and subjective perceptual quality.},
  archive      = {J_CVIU},
  author       = {Junhao Zhang and Kim-Hui Yap and Lap-Pui Chau and Ce Zhu},
  doi          = {10.1016/j.cviu.2024.104204},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104204},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image compressive sensing reconstruction via nonlocal low-rank residual-based ADMM framework},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end tracking framework via multi-view and temporal
feature aggregation. <em>CVIU</em>, <em>249</em>, 104203. (<a
href="https://doi.org/10.1016/j.cviu.2024.104203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view pedestrian tracking has frequently been used to cope with the challenges of occlusion and limited fields-of-view in single-view tracking. However, there are few end-to-end methods in this field. Many existing algorithms detect pedestrians in individual views, cluster projected detections in a top view and then track them. The others track pedestrians in individual views and then associate the projected tracklets in a top view. In this paper, an end-to-end framework is proposed for multi-view tracking, in which both multi-view and temporal aggregations of feature maps are applied. The multi-view aggregation projects the per-view feature maps to a top view, uses a transformer encoder to output encoded feature maps and then uses a CNN to calculate a pedestrian occupancy map. The temporal aggregation uses another CNN to estimate position offsets from the encoded feature maps in consecutive frames. Our experiments have demonstrated that this end-to-end framework outperforms the state-of-the-art online algorithms for multi-view pedestrian tracking.},
  archive      = {J_CVIU},
  author       = {Yihan Yang and Ming Xu and Jason F. Ralph and Yuchen Ling and Xiaonan Pan},
  doi          = {10.1016/j.cviu.2024.104203},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104203},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An end-to-end tracking framework via multi-view and temporal feature aggregation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Region-aware image-based human action retrieval with
transformers. <em>CVIU</em>, <em>249</em>, 104202. (<a
href="https://doi.org/10.1016/j.cviu.2024.104202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action understanding is a fundamental and challenging task in computer vision. Although there exists tremendous research on this area, most works focus on action recognition, while action retrieval has received less attention. In this paper, we focus on the neglected but important task of image-based action retrieval which aims to find images that depict the same action as a query image. We establish benchmarks for this task and set up important baseline methods for fair comparison. We present a Transformer-based model that learns rich action representations from three aspects: the anchored person, contextual regions, and the global image. A fusion transformer is designed to model the relationships among different features and effectively fuse them into an action representation. Experiments on both the Stanford-40 and PASCAL VOC 2012 Action datasets show that the proposed method significantly outperforms previous approaches for image-based action retrieval.},
  archive      = {J_CVIU},
  author       = {Hongsong Wang and Jianhua Zhao and Jie Gui},
  doi          = {10.1016/j.cviu.2024.104202},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104202},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Region-aware image-based human action retrieval with transformers},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MT-DSNet: Mix-mask teacher–student strategies and dual
dynamic selection plug-in module for fine-grained image recognition.
<em>CVIU</em>, <em>249</em>, 104201. (<a
href="https://doi.org/10.1016/j.cviu.2024.104201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fine-grained image recognition (FGIR) task aims to classify and distinguish subtle differences between subcategories with visually similar appearances, such as bird species and the makes or models of vehicles. However, subtle interclass differences and significant intraclass variances lead to poor model recognition performance. To address these challenges, we developed a mixed-mask teacher–student cooperative training strategy. A mixed masked image is generated and embedded into a knowledge distillation network by replacing one image’s visible marker with another’s masked marker. Collaborative reinforcement between teachers and students is used to improve the recognition performance of the network. We chose the classic transformer architecture as a baseline to better explore the contextual relationships between features. Additionally, we suggest a dual dynamic selection plug-in for choosing features with discriminative capabilities in the spatial and channel dimensions and filter out irrelevant interference information to efficiently handle background and noise features in fine-grained images. The proposed feature suppression module is used to enhance the differences between different features, thereby motivating the network to mine more discriminative features. We validated our method using two datasets: CUB-200-2011 and Stanford Cars. The experimental results show that the proposed MT-DSNet can significantly improve the feature representation for FGIR tasks. Moreover, by applying it to different fine-grained networks, the FGIR accuracy can be improved without changing the original network structure. We hope that this work provides a promising approach for improving the feature representation of networks in the future.},
  archive      = {J_CVIU},
  author       = {Hongchun Lu and Min Han},
  doi          = {10.1016/j.cviu.2024.104201},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104201},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MT-DSNet: Mix-mask teacher–student strategies and dual dynamic selection plug-in module for fine-grained image recognition},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WGS-YOLO: A real-time object detector based on YOLO
framework for autonomous driving. <em>CVIU</em>, <em>249</em>, 104200.
(<a href="https://doi.org/10.1016/j.cviu.2024.104200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The safety and reliability of autonomous driving depends on the precision and efficiency of object detection systems. In this paper, a refined adaptation of the YOLO architecture (WGS-YOLO) is developed to improve the detection of pedestrians and vehicles. Specifically, its information fusion is enhanced by incorporating the Weighted Efficient Layer Aggregation Network (W-ELAN) module, an innovative dynamic weighted feature fusion module using channel shuffling. Meanwhile, the computational demands and parameters of the proposed WGS-YOLO are significantly reduced by employing the Space-to-Depth Convolution (SPD-Conv) and the Grouped Spatial Pyramid Pooling (GSPP) modules that have been strategically designed. The performance of our model is evaluated with the BDD100k and DAIR-V2X-V datasets. In terms of mean Average Precision ( mAP 0 . 5 ), the proposed model outperforms the baseline Yolov7 by 12%. Furthermore, extensive experiments are conducted to verify our analysis and the model’s robustness across diverse scenarios.},
  archive      = {J_CVIU},
  author       = {Shiqin Yue and Ziyi Zhang and Ying Shi and Yonghua Cai},
  doi          = {10.1016/j.cviu.2024.104200},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104200},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {WGS-YOLO: A real-time object detector based on YOLO framework for autonomous driving},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial network for semi-supervised image
captioning. <em>CVIU</em>, <em>249</em>, 104199. (<a
href="https://doi.org/10.1016/j.cviu.2024.104199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional supervised image captioning methods usually rely on a large number of images and paired captions for training. However, the creation of such datasets necessitates considerable temporal and human resources. Therefore, we propose a new semi-supervised image captioning algorithm to solve this problem. The proposed method uses a generative adversarial network to generate images that match captions, and uses these generated images and captions as new training data. This avoids the error accumulation problem when generating pseudo captions with autoregressive method and the network can directly perform backpropagation. At the same time, in order to ensure the correlation between the generated images and captions, we introduced the CLIP model for constraints. The CLIP model has been pre-trained on a large amount of image–text data, so it shows excellent performance in semantic alignment of images and text. To verify the effectiveness of our method, we validate on MSCOCO offline “Karpathy” test split. Experiment results show that our method can significantly improve the performance of the model when using 1% paired data, with the CIDEr score increasing from 69.5% to 77.7%. This shows that our method can effectively utilize unlabeled data for image caption tasks.},
  archive      = {J_CVIU},
  author       = {Xu Liang and Chen Li and Lihua Tian},
  doi          = {10.1016/j.cviu.2024.104199},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104199},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Generative adversarial network for semi-supervised image captioning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperspectral image classification with token fusion on GPU.
<em>CVIU</em>, <em>249</em>, 104198. (<a
href="https://doi.org/10.1016/j.cviu.2024.104198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images capture material nuances with spectral data, vital for remote sensing. Transformer has become a mainstream approach for tackling the challenges posed by high-dimensional hyperspectral data with complex structures. However, a major challenge they face when processing hyperspectral images is the presence of a large number of redundant tokens, which leads to a significant increase in computational load, adding to the model’s computational burden and affecting inference speed. Therefore, we propose a token fusion algorithm tailored to the operational characteristics of the hyperspectral image and pure transformer network, aimed at enhancing the final accuracy and throughput of the model. The token fusion algorithm introduces a token merging step between the attention mechanism and the multi-layer perceptron module in each Transformer layer. Experiments on four hyperspectral image datasets demonstrate that our token fusion algorithm can significantly improve inference speed without any training, while only causing a slight decrease in the pure transformer network’s classification accuracy.},
  archive      = {J_CVIU},
  author       = {He Huang and Sha Tao},
  doi          = {10.1016/j.cviu.2024.104198},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104198},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hyperspectral image classification with token fusion on GPU},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PMGNet: Disentanglement and entanglement benefit mutually
for compositional zero-shot learning. <em>CVIU</em>, <em>249</em>,
104197. (<a href="https://doi.org/10.1016/j.cviu.2024.104197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compositional zero-shot learning (CZSL) aims to model compositions of two primitives (i.e., attributes and objects) to classify unseen attribute-object pairs. Most studies are devoted to integrating disentanglement and entanglement strategies to circumvent the trade-off between contextuality and generalizability. Indeed, the two strategies can mutually benefit when used together. Nevertheless, they neglect the significance of developing mutual guidance between the two strategies. In this work, we take full advantage of guidance from disentanglement to entanglement and vice versa. Additionally, we propose exploring multi-scale feature learning to achieve fine-grained mutual guidance in a progressive framework. Our approach, termed Progressive Mutual Guidance Network (PMGNet), unifies disentanglement–entanglement representation learning, allowing them to learn from and teach each other progressively in one unified model. Furthermore, to alleviate overfitting recognition on seen pairs, we adopt a relaxed cross-entropy loss to train PMGNet, without an increase of time and memory cost. Extensive experiments on three benchmarks demonstrate that our method achieves distinct improvements, reaching state-of-the-art performance. Moreover, PMGNet exhibits promising performance under the most challenging open-world CZSL setting, especially for unseen pairs.},
  archive      = {J_CVIU},
  author       = {Yu Liu and Jianghao Li and Yanyi Zhang and Qi Jia and Weimin Wang and Nan Pu and Nicu Sebe},
  doi          = {10.1016/j.cviu.2024.104197},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104197},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PMGNet: Disentanglement and entanglement benefit mutually for compositional zero-shot learning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semantic segmentation method integrated convolutional
nonlinear spiking neural model with transformer. <em>CVIU</em>,
<em>249</em>, 104196. (<a
href="https://doi.org/10.1016/j.cviu.2024.104196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a critical task in computer vision, with significant applications in areas like autonomous driving and medical imaging. Transformer-based methods have gained considerable attention recently because of their strength in capturing global information. However, these methods often sacrifice detailed information due to the lack of mechanisms for local interactions. Similarly, convolutional neural network (CNN) methods struggle to capture global context due to the inherent limitations of convolutional kernels. To overcome these challenges, this paper introduces a novel Transformer-based semantic segmentation method called NSNPFormer, which leverages the nonlinear spiking neural P (NSNP) system—a computational model inspired by the spiking mechanisms of biological neurons. The NSNPFormer employs an encoding–decoding structure with two convolutional NSNP components and a residual connection channel. The convolutional NSNP components facilitate nonlinear local feature extraction and block-level feature fusion. Meanwhile, the residual connection channel helps prevent the loss of feature information during the decoding process. Evaluations on the ADE20K and Pascal Context datasets show that NSNPFormer achieves mIoU scores of 53.7 and 58.06, respectively, highlighting its effectiveness in semantic segmentation tasks.},
  archive      = {J_CVIU},
  author       = {Siyan Sun and Wenqian Yang and Hong Peng and Jun Wang and Zhicai Liu},
  doi          = {10.1016/j.cviu.2024.104196},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104196},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A semantic segmentation method integrated convolutional nonlinear spiking neural model with transformer},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel image inpainting method based on a modified
lengyel–epstein model. <em>CVIU</em>, <em>249</em>, 104195. (<a
href="https://doi.org/10.1016/j.cviu.2024.104195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of digital images, developing advanced algorithms that can accurately reconstruct damaged images while maintaining high visual quality is crucial. Traditional image restoration algorithms often struggle with complex structures and details, while recent deep learning methods, though effective, face significant challenges related to high data dependency and computational costs. To resolve these challenges, we propose a novel image inpainting model, which is based on a modified Lengyel–Epstein (LE) model. We discretize the modified LE model by using an explicit Euler algorithm. A series of restoration experiments are conducted on various image types, including binary images, grayscale images, index images, and color images. The experimental results demonstrate the effectiveness and robustness of the method, and even under complex conditions of noise interference and local damage, the proposed method can exhibit excellent repair performance. To quantify the fidelity of these restored images, we use the peak signal-to-noise ratio (PSNR), a widely accepted metric in image processing. The calculation results further demonstrate the applicability of our model across different image types. Moreover, by evaluating CPU time, our method can achieve ideal repair results within a remarkably brief duration. The proposed method validates significant potential for real-world applications in diverse domains of image restoration and enhancement.},
  archive      = {J_CVIU},
  author       = {Jian Wang and Mengyu Luo and Xinlei Chen and Heming Xu and Junseok Kim},
  doi          = {10.1016/j.cviu.2024.104195},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104195},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel image inpainting method based on a modified Lengyel–Epstein model},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight cross-modal transformer for RGB-d salient object
detection. <em>CVIU</em>, <em>249</em>, 104194. (<a
href="https://doi.org/10.1016/j.cviu.2024.104194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Transformer-based RGB-D salient object detection (SOD) models have pushed the performance to a new level. However, they come at the cost of consuming abundant resources, including memory and power, thus hindering their real-life applications. To remedy this situation, a novel lightweight cross-modal Transformer (LCT) for RGB-D SOD will be presented in this paper. Specifically, LCT will first reduce its parameters and computational costs by employing a middle-level feature fusion structure and taking a lightweight Transformer as the backbone. Then, with the aid of Transformers, it will compensate for performance degradation by effectively capturing the cross-modal and cross-level complementary information from the multi-modal input images. To this end, a cross-modal enhancement and fusion module (CEFM) with a lightweight channel-wise cross attention block (LCCAB) will be designed to capture the cross-modal complementary information effectively but with fewer costs. A bi-directional multi-level feature interaction module (Bi-MFIM) with a lightweight spatial-wise cross attention block (LSCAB) will be designed to capture the cross-level complementary context information. By virtue of CEFM and Bi-MFIM, the performance degradation caused by parameter reduction can be well compensated, thus boosting the performances. By doing so, our proposed model has only 2.8M parameters with 7.6G FLOPs and runs at 66 FPS. Furthermore, experimental results on several benchmark datasets show that our proposed model can achieve competitive or even better results than other models. Our code will be released on https://github.com/nexiakele/lightweight-cross-modal-Transformer-LCT-for-RGB-D-SOD .},
  archive      = {J_CVIU},
  author       = {Nianchang Huang and Yang Yang and Qiang Zhang and Jungong Han and Jin Huang},
  doi          = {10.1016/j.cviu.2024.104194},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104194},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lightweight cross-modal transformer for RGB-D salient object detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MultiSubjects: A multi-subject video dataset for
single-person basketball action recognition from basketball gym.
<em>CVIU</em>, <em>249</em>, 104193. (<a
href="https://doi.org/10.1016/j.cviu.2024.104193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision technology is becoming a research focus in the field of basketball. Despite the abundance of datasets centered on basketball games, there remains a significant gap in the availability of a large-scale, multi-subject, and fine-grained dataset for the recognition of basketball actions in real-world sports scenarios, particularly for amateur players. Such datasets are crucial for advancing the application of computer vision tasks in the real world. To address this gap, we deployed multi-view cameras in a civilian basketball gym, constructed a real basketball data acquisition platform, and acquired a challenging multi-subject video dataset, named MultiSubjects. The MultiSubjects v1.0 dataset features a variety of ages, body types, attire, genders, and basketball actions, providing researchers with a high-quality and diverse resource of basketball action data. We collected a total of 1,000 distinct subjects from video data between September and December 2023, classified and labeled three basic basketball actions, and assigned a unique identity ID to each subject, provided a total of 6,144 video clips, 436,460 frames, and labeled 6,144 instances of actions with clear temporal boundaries using 436,460 human body bounding boxes. Additionally, complete frame-wise skeleton keypoint coordinates for the entire action are provided. We used some representative video action recognition algorithms as well as skeleton-based action recognition algorithms on the MultiSubjects v1.0 dataset and analyzed the results. The results confirm that the quality of our dataset surpasses that of popular video action recognition datasets, it also presents that skeleton-based action recognition remains a challenging task. The link to our dataset is: https://huggingface.co/datasets/Henu-Software/Henu-MultiSubjects .},
  archive      = {J_CVIU},
  author       = {Zhijie Han and Wansong Qin and Yalu Wang and Qixiang Wang and Yongbin Shi},
  doi          = {10.1016/j.cviu.2024.104193},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104193},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MultiSubjects: A multi-subject video dataset for single-person basketball action recognition from basketball gym},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple but effective vision transformer framework for
visible–infrared person re-identification. <em>CVIU</em>, <em>249</em>,
104192. (<a href="https://doi.org/10.1016/j.cviu.2024.104192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of visible–infrared person re-identification (VI-ReID), the acquisition of a robust visual representation is paramount. Existing approaches predominantly rely on convolutional neural networks (CNNs), which are guided by intricately designed loss functions to extract features. In contrast, the vision transformer (ViT), a potent visual backbone, has often yielded subpar results in VI-ReID. We contend that the prevailing training methodologies and insights derived from CNNs do not seamlessly apply to ViT, leading to the underutilization of its potential in VI-ReID. One notable limitation is ViT’s appetite for extensive data, exemplified by the JFT-300M dataset, to surpass CNNs. Consequently, ViT struggles to transfer its knowledge from visible to infrared images due to inadequate training data. Even the largest available dataset, SYSU-MM01, proves insufficient for ViT to glean a robust representation of infrared images. This predicament is exacerbated when ViT is trained on the smaller RegDB dataset, where slight data flow modifications drastically affect performance—a stark contrast to CNN behavior. These observations lead us to conjecture that the CNN-inspired paradigm impedes ViT’s progress in VI-ReID. In light of these challenges, we undertake comprehensive ablation studies to shed new light on ViT’s applicability in VI-ReID. We propose a straightforward yet effective framework, named “Idformer”, to train a high-performing ViT for VI-ReID. Idformer serves as a robust baseline that can be further enhanced with carefully designed techniques akin to those used for CNNs. Remarkably, our method attains competitive results even in the absence of auxiliary information, achieving 78.58%/76.99% Rank-1/mAP on the SYSU-MM01 dataset, as well as 96.82%/91.83% Rank-1/mAP on the RegDB dataset. The code will be made publicly accessible.},
  archive      = {J_CVIU},
  author       = {Yudong Li and Sanyuan Zhao and Jianbing Shen},
  doi          = {10.1016/j.cviu.2024.104192},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104192},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A simple but effective vision transformer framework for visible–infrared person re-identification},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Found missing semantics: Supplemental prototype network for
few-shot semantic segmentation. <em>CVIU</em>, <em>249</em>, 104191. (<a
href="https://doi.org/10.1016/j.cviu.2024.104191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation alleviates the problem of massive data requirements and high costs in semantic segmentation tasks. By learning from support set, few-shot semantic segmentation can segment new classes. However, existing few-shot semantic segmentation methods suffer from information loss during the process of mask average pooling. To address this problem, we propose a supplemental prototype network (SPNet). The SPNet aggregates the lost information from global prototypes to create a supplemental prototype, which enhances the segmentation performance for the current class. In addition, we utilize mutual attention to enhance the similarity between the support and the query feature maps, allowing the model to better identify the target to be segmented. Finally, we introduce a Self-correcting auxiliary, which utilizes the data more effectively to improve segmentation accuracy. We conducted extensive experiments on PASCAL-5i and COCO-20i, which demonstrated the effectiveness of SPNet. And our method achieved state-of-the-art results in the 1-shot and 5-shot semantic segmentation settings.},
  archive      = {J_CVIU},
  author       = {Chen Liang and Shuang Bai},
  doi          = {10.1016/j.cviu.2024.104191},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104191},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Found missing semantics: Supplemental prototype network for few-shot semantic segmentation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BundleMoCap++: Efficient, robust and smooth motion capture
from sparse multiview videos. <em>CVIU</em>, <em>249</em>, 104190. (<a
href="https://doi.org/10.1016/j.cviu.2024.104190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Producing smooth and accurate motions from sparse videos without requiring specialized equipment and markers is a long-standing problem in the research community. Most approaches typically involve complex processes such as temporal constraints, multiple stages combining data-driven regression and optimization techniques, and bundle solving over temporal windows. These increase the computational burden and introduce the challenge of hyperparameter tuning for the different objective terms. In contrast, BundleMoCap++ offers a simple yet effective approach to this problem. It solves the motion in a single stage, eliminating the need for temporal smoothness objectives while still delivering smooth motions without compromising accuracy. BundleMoCap++ outperforms the state-of-the-art without increasing complexity. Our approach is based on manifold interpolation between latent keyframes. By relying on a local manifold smoothness assumption and appropriate interpolation schemes, we efficiently solve a bundle of frames using two or more latent codes. Additionally, the method is implemented as a sliding window optimization and requires only the first frame to be properly initialized, reducing the overall computational burden. BundleMoCap++’s strength lies in achieving high-quality motion capture results with fewer computational resources. To do this efficiently, we propose a novel human pose prior that focuses on the geometric aspect of the latent space, modeling it as a hypersphere, allowing for the introduction of sophisticated interpolation techniques. We also propose an algorithm for optimizing the latent variables directly on the learned manifold, improving convergence and performance. Finally, we introduce high-order interpolation techniques adapted for the hypersphere, allowing us to increase the solving temporal window, enhancing performance and efficiency.},
  archive      = {J_CVIU},
  author       = {Georgios Albanis and Nikolaos Zioulis and Kostas Kolomvatsos},
  doi          = {10.1016/j.cviu.2024.104190},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104190},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {BundleMoCap++: Efficient, robust and smooth motion capture from sparse multiview videos},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring event-based human pose estimation with 3D event
representations. <em>CVIU</em>, <em>249</em>, 104189. (<a
href="https://doi.org/10.1016/j.cviu.2024.104189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is a fundamental and appealing task in computer vision. Although traditional cameras are commonly applied, their reliability decreases in scenarios under high dynamic range or heavy motion blur, where event cameras offer a robust solution. Predominant event-based methods accumulate events into frames, ignoring the asynchronous and high temporal resolution that is crucial for distinguishing distinct actions. To address this issue and to unlock the 3D potential of event information, we introduce two 3D event representations: the Rasterized Event Point Cloud (RasEPC) and the Decoupled Event Voxel (DEV). The RasEPC aggregates events within concise temporal slices at identical positions, preserving their 3D attributes along with statistical information, thereby significantly reducing memory and computational demands. Meanwhile, the DEV representation discretizes events into voxels and projects them across three orthogonal planes, utilizing decoupled event attention to retrieve 3D cues from the 2D planes. Furthermore, we develop and release EV-3DPW, a synthetic event-based dataset crafted to facilitate training and quantitative analysis in outdoor scenes. Our methods are tested on the DHP19 public dataset, MMHPSD dataset, and our EV-3DPW dataset, with further qualitative validation via a derived driving scene dataset EV-JAAD and an outdoor collection vehicle. Our code and dataset have been made publicly available at https://github.com/MasterHow/EventPointPose .},
  archive      = {J_CVIU},
  author       = {Xiaoting Yin and Hao Shi and Jiaan Chen and Ze Wang and Yaozu Ye and Kailun Yang and Kaiwei Wang},
  doi          = {10.1016/j.cviu.2024.104189},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104189},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploring event-based human pose estimation with 3D event representations},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FTM: The face truth machine—hand-crafted features from
micro-expressions to support lie detection. <em>CVIU</em>, <em>249</em>,
104188. (<a href="https://doi.org/10.1016/j.cviu.2024.104188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work deals with the delicate task of lie detection from facial dynamics. The proposed Face Truth Machine (FTM) is an intelligent system able to support a human operator without any special equipment. It can be embedded in the present infrastructures for forensic investigation or whenever it is required to assess the trustworthiness of responses during an interview. Due to its flexibility and its non-invasiveness, it can overcome some limitations of present solutions. Of course, privacy issues may arise from the use of such systems, as often underlined nowadays. However, it is up to the utilizer to take these into account and make fair use of tools of this kind. The paper will discuss particular aspects of the dynamic analysis of face landmarks to detect lies. In particular, it will delve into the behavior of the features used for detection and how these influence the system’s final decision. The novel detection system underlying the Face Truth Machine is able to analyze the subject’s expressions in a wide range of poses. The results of the experiments presented testify to the potential of the proposed approach and also highlight the very good results obtained in cross-dataset testing, which usually represents a challenge for other approaches.},
  archive      = {J_CVIU},
  author       = {Maria De Marsico and Giordano Dionisi and Donato Francesco Pio Stanco},
  doi          = {10.1016/j.cviu.2024.104188},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104188},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FTM: The face truth Machine—Hand-crafted features from micro-expressions to support lie detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). For a semiotic AI: Bridging computer vision and visual
semiotics for computational observation of large scale facial image
archives. <em>CVIU</em>, <em>249</em>, 104187. (<a
href="https://doi.org/10.1016/j.cviu.2024.104187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks are creating a digital world in which the cognitive, emotional, and pragmatic value of the imagery of human faces and bodies is arguably changing. However, researchers in the digital humanities are often ill-equipped to study these phenomena at scale. This work presents FRESCO (Face Representation in E-Societies through Computational Observation), a framework designed to explore the socio-cultural implications of images on social media platforms at scale. FRESCO deconstructs images into numerical and categorical variables using state-of-the-art computer vision techniques, aligning with the principles of visual semiotics. The framework analyzes images across three levels: the plastic level, encompassing fundamental visual features like lines and colors; the figurative level, representing specific entities or concepts; and the enunciation level, which focuses particularly on constructing the point of view of the spectator and observer. These levels are analyzed to discern deeper narrative layers within the imagery. Experimental validation confirms the reliability and utility of FRESCO, and we assess its consistency and precision across two public datasets. Subsequently, we introduce the FRESCO score, a metric derived from the framework’s output that serves as a reliable measure of similarity in image content.},
  archive      = {J_CVIU},
  author       = {Lia Morra and Antonio Santangelo and Pietro Basci and Luca Piano and Fabio Garcea and Fabrizio Lamberti and Massimo Leone},
  doi          = {10.1016/j.cviu.2024.104187},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104187},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {For a semiotic AI: Bridging computer vision and visual semiotics for computational observation of large scale facial image archives},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional temporal and frame-segment attention for
sparse action segmentation of figure skating. <em>CVIU</em>,
<em>249</em>, 104186. (<a
href="https://doi.org/10.1016/j.cviu.2024.104186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action segmentation is a task for understanding human activities in long-term videos. Most of the efforts have been focused on dense-frame action, which relies on strong correlations between frames. However, in the figure skating scene, technical actions are sparsely shown in the video. This brings new challenges: a large amount of redundant temporal information leads to weak frame correlation. To end this, we propose a Bidirectional Temporal and Frame-Segment Attention Module (FSAM). Specifically, we propose an additional reverse-temporal input stream to enhance frame correlation, learned by fusing bidirectional temporal features. In addition, the proposed FSAM contains a Multi-stage segment-aware GCN and decoder interaction module, aiming to learn the correlation between segment features across time domains and integrate embeddings between frame and segment representations. To evaluate our approach, we propose the Figure Skating Sparse Action Segmentation (FSSAS) dataset: The dataset comprises 100 samples of the Olympic figure skating final and semi-final competition, with more than 50 different men and women athletes. Extensive experiments show that our method achieves an accuracy of 87.75 and an edit score of 90.18 on the FSSAS dataset.},
  archive      = {J_CVIU},
  author       = {Yanchao Liu and Xina Cheng and Yuan Li and Takeshi Ikenaga},
  doi          = {10.1016/j.cviu.2024.104186},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104186},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bidirectional temporal and frame-segment attention for sparse action segmentation of figure skating},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty guided test-time training for face forgery
detection. <em>CVIU</em>, <em>249</em>, 104185. (<a
href="https://doi.org/10.1016/j.cviu.2024.104185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of generative image modeling poses security risks of spreading unreal visual information, even though those techniques make a lot of applications possible in positive aspects. To provide alerts and maintain a secure social environment, forgery detection has been an urgent and crucial solution to deal with this situation and try to avoid any negative effects, especially for human faces, owing to potential severe results when malicious creators spread disinformation widely. In spite of the success of recent works w.r.t. model design and feature engineering, detecting face forgery from novel image creation methods or data distributions remains unresolved, because well-trained models are typically not robust to the distribution shift during test-time. In this work, we aim to alleviate the sensitivity of an existing face forgery detector to new domains, and then boost real-world detection under unknown test situations. In specific, we leverage test examples, selected by uncertainty values, to fine-tune the model before making a final prediction. Therefore, it leads to a test-time training based approach for face forgery detection, that our framework incorporates an uncertainty-driven test sample selection with self-training to adapt a classifier onto target domains. To demonstrate the effectiveness of our framework and compare with previous methods, we conduct extensive experiments on public datasets, including FaceForensics++, Celeb-DF-v2, ForgeryNet and DFDC. Our results clearly show that the proposed framework successfully improves many state-of-the-art methods in terms of better overall performance as well as stronger robustness to novel data distributions.},
  archive      = {J_CVIU},
  author       = {Pengxiang Xu and Yang He and Jian Yang and Shanshan Zhang},
  doi          = {10.1016/j.cviu.2024.104185},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104185},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Uncertainty guided test-time training for face forgery detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance-based loss function for deep feature space learning
of convolutional neural networks. <em>CVIU</em>, <em>249</em>, 104184.
(<a href="https://doi.org/10.1016/j.cviu.2024.104184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have been on the forefront of neural network research in recent years. Their breakthrough performance in fields such as image classification has gathered efforts in the development of new CNN-based architectures, but recently more attention has been directed to the study of new loss functions. Softmax loss remains the most popular loss function due mainly to its efficiency in class separation, but the function is unsatisfactory in terms of intra-class compactness. While some studies have addressed this problem, most solutions attempt to refine softmax loss or combine it with other approaches. We present a novel loss function based on distance matrices (LDMAT), softmax independent, that maximizes interclass distance and minimizes intraclass distance. The loss function operates directly on deep features, allowing their use on arbitrary classifiers. LDMAT minimizes the distance between two distance matrices, one constructed with the model’s deep features and the other calculated from the labels. The use of a distance matrix in the loss function allows a two-dimensional representation of features and imposes a fixed distance between classes, while improving intra-class compactness. A regularization method applied to the distance matrix of labels is also presented, that allows a degree of relaxation of the solution and leads to a better spreading of features in the separation space. Efficient feature extraction was observed on datasets such as MNIST, CIFAR10 and CIFAR100.},
  archive      = {J_CVIU},
  author       = {Eduardo S. Ribeiro and Lourenço R.G. Araújo and Gabriel T.L. Chaves and Antônio P. Braga},
  doi          = {10.1016/j.cviu.2024.104184},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104184},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Distance-based loss function for deep feature space learning of convolutional neural networks},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LCMA-net: A light cross-modal attention network for streamer
re-identification in live video. <em>CVIU</em>, <em>249</em>, 104183.
(<a href="https://doi.org/10.1016/j.cviu.2024.104183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of the we-media industry, streamers have increasingly incorporated inappropriate content into live videos to attract traffic and pursue interests. Blacklisted streamers often forge their identities or switch platforms to continue streaming, causing significant harm to the online environment. Consequently, streamer re-identification (re-ID) has become of paramount importance. Streamer biometrics in live videos exhibit multimodal characteristics, including voiceprints, faces, and spatiotemporal information, which complement each other. Therefore, we propose a light cross-modal attention network (LCMA-Net) for streamer re-ID in live videos. First, the voiceprint, face, and spatiotemporal features of the streamer are extracted by RawNet-SA, Π Π -Net, and STDA-ResNeXt3D, respectively. We then design a light cross-modal pooling attention (LCMPA) module, which, combined with a multilayer perceptron (MLP), aligns and concatenates different modality features into multimodal features within the LCMA-Net. Finally, the streamer is re-identified by measuring the similarity between these multimodal features. Five experiments were conducted on the StreamerReID dataset, and the results demonstrated that the proposed method achieved competitive performance. The dataset and code are available at https://github.com/BJUT-AIVBD/LCMA-Net .},
  archive      = {J_CVIU},
  author       = {Jiacheng Yao and Jing Zhang and Hui Zhang and Li Zhuo},
  doi          = {10.1016/j.cviu.2024.104183},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104183},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LCMA-net: A light cross-modal attention network for streamer re-identification in live video},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Efficient degradation representation learning network for
remote sensing image super-resolution. <em>CVIU</em>, <em>249</em>,
104182. (<a href="https://doi.org/10.1016/j.cviu.2024.104182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancements in convolutional neural networks have led to significant progress in image super-resolution (SR) techniques. Nevertheless, it is crucial to acknowledge that current SR methods operate under the assumption of bicubic downsampling as a degradation factor in low-resolution (LR) images and train models accordingly. However, this approach does not account for the unknown degradation patterns present in real-world scenes. To address this problem, we propose an efficient degradation representation learning network (EDRLN). Specifically, we adopt a contrast learning approach, which enables the model to distinguish and learn various degradation representations in realistic images to obtain critical degradation information. We also introduce streamlined and efficient pixel attention to strengthen the feature extraction capability of the model. In addition, we optimize our model with mutual affine convolution layers instead of ordinary convolution layers to make it more lightweight while minimizing performance loss. Experimental results on remote sensing and benchmark datasets show that our proposed EDRLN exhibits good performance for different degradation scenarios, while the lightweight version minimizes the performance loss as much as possible. The Code will be available at: https://github.com/Leilei11111/EDRLN .},
  archive      = {J_CVIU},
  author       = {Xuan Wang and Lijun Sun and Jinglei Yi and Yongchao Song and Qiang Zheng and Abdellah Chehri},
  doi          = {10.1016/j.cviu.2024.104182},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104182},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient degradation representation learning network for remote sensing image super-resolution},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bridging the gap between object detection in close-up and
high-resolution wide shots. <em>CVIU</em>, <em>249</em>, 104181. (<a
href="https://doi.org/10.1016/j.cviu.2024.104181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have seen a significant rise in gigapixel-level image/video capture systems and benchmarks with high-resolution wide (HRW) shots. Different from close-up shots like MS COCO, the higher resolution and wider field of view raise new research and application problems, such as how to perform accurate and efficient object detection with such large input in low-power edge devices like UAVs. There are several unique challenges in HRW shots. (1) Sparse information: the objects of interest cover less area. (2) Various scale: there is 10 to 100 × × object scale change in one single image. (3) Incomplete objects: the sliding window strategy to handle the large input leads to truncated objects at the window edge. (4) Multi-scale information: it is unclear how to use multi-scale information in training and inference. Consequently, directly using a close-up detector leads to inaccuracy and inefficiency. In this paper, we systematically investigate this problem and bridge the gap between object detection in close-up and HRW shots, by introducing a novel sparse architecture that can be integrated with common networks like ConvNet and Transformer. It leverages alternative sparse learning to complementarily fuse coarse-grained and fine-grained features to (1) adaptively extract valuable information from (2) different object scales. We also propose a novel Cross-window Non-Maximum Suppression (C-NMS) algorithm to (3) improve the box merge from different windows. Furthermore, we propose a (4) simple yet effective multi-scale training and inference strategy to improve accuracy. Experiments on two benchmarks with HRW shots, PANDA and DOTA-v1.0, demonstrate that our methods significantly improve accuracy (up to 5.8%) and speed (up to 3 × × ) over SotAs, for both ConvNet or Transformer based detectors, on edge devices. Our code is open-sourced and available at https://github.com/liwenxi/SparseFormer .},
  archive      = {J_CVIU},
  author       = {Wenxi Li and Yuchen Guo and Jilai Zheng and Haozhe Lin and Chao Ma and Lu Fang and Xiaokang Yang},
  doi          = {10.1016/j.cviu.2024.104181},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104181},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bridging the gap between object detection in close-up and high-resolution wide shots},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial attention for human-centric visual understanding: An
information bottleneck method. <em>CVIU</em>, <em>249</em>, 104180. (<a
href="https://doi.org/10.1016/j.cviu.2024.104180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selective visual attention mechanism in the Human Visual System (HVS) restricts the amount of information that reaches human visual awareness, allowing the brain to perceive high-fidelity natural scenes in real-time with limited computational cost. This selectivity acts as an “Information Bottleneck (IB)” that balances information compression and predictive accuracy. However, such information constraints are rarely explored in the attention mechanism for deep neural networks (DNNs). This paper introduces an IB-inspired spatial attention module for DNNs, which generates an attention map by minimizing the mutual information (MI) between the attentive content and the input while maximizing that between the attentive content and the output. We develop this IB-inspired attention mechanism based on a novel graphical model and explore various implementations of the framework. We show that our approach can yield attention maps that neatly highlight the regions of interest while suppressing the backgrounds, and are interpretable for the decision-making of the DNNs. To validate the effectiveness of the proposed IB-inspired attention mechanism, we apply it to various computer vision tasks including image classification, fine-grained recognition, cross-domain classification, semantic segmentation, and object detection. Extensive experiments demonstrate that it bootstraps standard DNN structures quantitatively and qualitatively for these tasks.},
  archive      = {J_CVIU},
  author       = {Qiuxia Lai and Yongwei Nie and Yu Li and Hanqiu Sun and Qiang Xu},
  doi          = {10.1016/j.cviu.2024.104180},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104180},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatial attention for human-centric visual understanding: An information bottleneck method},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Specular highlight removal using quaternion transformer.
<em>CVIU</em>, <em>249</em>, 104179. (<a
href="https://doi.org/10.1016/j.cviu.2024.104179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specular highlight removal is a very important issue, because specular highlight reflections in images with illumination changes can give very negative effects on various computer vision and image processing tasks. Numerous state-of-the-art networks for the specular removal use convolutional neural networks (CNN), which cannot learn global context effectively. They capture spatial information while overlooking 3D structural correlation information of an RGB image. To address this problem, we introduce a specular highlight removal network based on Quaternion transformer (QformerSHR), which employs a transformer architecture based on Quaternion representation. In particular, a depth-wise separable Quaternion convolutional layer (DSQConv) is proposed to enhance computational performance of QformerSHR, while efficiently preserving the structural correlation of an RGB image by utilizing the Quaternion representation. In addition, a Quaternion transformer block (QTB) based on DSQConv learns global context. As a result, QformerSHR consisting of DSQConv and QTB performs the specular removal from natural and text image datasets effectively. Experimental results demonstrate that it is significantly more effective than state-of-the-art networks for the specular removal, in terms of both quantitative performance and subjective quality.},
  archive      = {J_CVIU},
  author       = {The Van Le and Jin Young Lee},
  doi          = {10.1016/j.cviu.2024.104179},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104179},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Specular highlight removal using quaternion transformer},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient feature reuse distillation network for
lightweight image super-resolution. <em>CVIU</em>, <em>249</em>, 104178.
(<a href="https://doi.org/10.1016/j.cviu.2024.104178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent research, single-image super-resolution (SISR) using deep Convolutional Neural Networks (CNN) has seen significant advancements. While previous methods excelled at learning complex mappings between low-resolution (LR) and high-resolution (HR) images, they often required substantial computational and memory resources. We propose the Efficient Feature Reuse Distillation Network (EFRDN) to alleviate these challenges. EFRDN primarily comprises Asymmetric Convolutional Distillation Modules (ACDM), incorporating the Multiple Self-Calibrating Convolution (MSCC) units for spatial and channel feature extraction. It includes an Asymmetric Convolution Residual Block (ACRB) to enhance the skeleton information of the square convolution kernel and a Feature Fusion Lattice Block (FFLB) to convert low-order input signals into higher-order representations. Introducing a Transformer module for global features, we enhance feature reuse and gradient flow, improving model performance and efficiency. Extensive experimental results demonstrate that EFRDN outperforms existing methods in performance while conserving computing and memory resources.},
  archive      = {J_CVIU},
  author       = {Chunying Liu and Guangwei Gao and Fei Wu and Zhenhua Guo and Yi Yu},
  doi          = {10.1016/j.cviu.2024.104178},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104178},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An efficient feature reuse distillation network for lightweight image super-resolution},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer fusion for indoor RGB-d semantic segmentation.
<em>CVIU</em>, <em>249</em>, 104174. (<a
href="https://doi.org/10.1016/j.cviu.2024.104174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing geometric cues with visual appearance is an imperative theme for RGB-D indoor semantic segmentation. Existing methods commonly adopt convolutional modules to aggregate multi-modal features, paying little attention to explicitly leveraging the long-range dependencies in feature fusion. Therefore, it is challenging for existing methods to accurately segment objects with large-scale variations. In this paper, we propose a novel transformer-based fusion scheme, named TransD-Fusion, to better model contextualized awareness. Specifically, TransD-Fusion consists of a self-refinement module, a calibration scheme with cross-interaction, and a depth-guided fusion. The objective is to first improve modality-specific features with self- and cross-attention, and then explore the geometric cues to better segment objects sharing a similar visual appearance. Additionally, our transformer fusion benefits from a semantic-aware position encoding which spatially constrains the attention to neighboring pixels. Extensive experiments on RGB-D benchmarks demonstrate that the proposed method performs well over the state-of-the-art methods by large margins.},
  archive      = {J_CVIU},
  author       = {Zongwei Wu and Zhuyun Zhou and Guillaume Allibert and Christophe Stolz and Cédric Demonceaux and Chao Ma},
  doi          = {10.1016/j.cviu.2024.104174},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104174},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transformer fusion for indoor RGB-D semantic segmentation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pyramid transformer-based triplet hashing for robust visual
place recognition. <em>CVIU</em>, <em>249</em>, 104167. (<a
href="https://doi.org/10.1016/j.cviu.2024.104167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing is being used to approximate nearest neighbor search for large-scale image recognition problems. However, CNN architectures have dominated similar applications. We present a Pyramid Transformer-based Triplet Hashing architecture to handle large-scale place recognition challenges in this study, leveraging the capabilities of Vision Transformer (ViT). For feature representation, we create a Siamese Pyramid Transformer backbone. We present a multi-scale feature aggregation technique to learn discriminative features for scale-invariant features. In addition, we observe that binary codes suitable for place recognition are sub-optimal. To overcome this issue, we use a self-restraint triplet loss deep learning network to create compact hash codes, further increasing recognition accuracy. To the best of our knowledge, this is the first study to use a triplet loss deep learning network to handle the deep hashing learning problem. We do extensive experiments on four difficult place datasets: KITTI, Nordland, VPRICE, and EuRoC. The experimental findings reveal that the suggested technique performs at the cutting edge of large-scale visual place recognition challenges.},
  archive      = {J_CVIU},
  author       = {Zhenyu Li and Pengjie Xu},
  doi          = {10.1016/j.cviu.2024.104167},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104167},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pyramid transformer-based triplet hashing for robust visual place recognition},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable video transformer for full-frame video prediction.
<em>CVIU</em>, <em>249</em>, 104166. (<a
href="https://doi.org/10.1016/j.cviu.2024.104166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have shown success in many low-level computer vision tasks. However, existing ViT models are limited by their high computation and memory cost when generating high-resolution videos for tasks like video prediction. This paper presents a scalable video transformer for full-frame video predication. Specifically, we design a backbone transformer block for our video transformer. This transformer block decouples the temporal and channel features to reduce the computation cost when processing large-scale spatial–temporal video features. We use transposed attention to focus on the channel dimension instead of the spatial window to further reduce the computation cost. We also design a Global Shifted Multi-Dconv Head Transposed Attention module (GSMDTA) for our transformer block. This module is built upon two key ideas. First, we design a depth shift module to better incorporate the cross-channel or temporal information from video features. Second, we introduce a global query mechanism to capture global information to handle large motion for video prediction. This new transformer block enables our video transformer to predict a full frame from multiple past frames at the resolution of 1024 × 512 with 12 GB VRAM. Experiments on various video prediction benchmarks demonstrate that our method with only RGB input outperforms state-of-the-art methods that require additional data, like segmentation maps and optical flows. Our method exceeds the state-of-the-art RGB-only methods by a large margin (1.2 dB) in PSNR. Our method is also faster than state-of-the-art video prediction transformers.},
  archive      = {J_CVIU},
  author       = {Zhan Li and Feng Liu},
  doi          = {10.1016/j.cviu.2024.104166},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104166},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Scalable video transformer for full-frame video prediction},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triple-stream commonsense circulation transformer network
for image captioning. <em>CVIU</em>, <em>249</em>, 104165. (<a
href="https://doi.org/10.1016/j.cviu.2024.104165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional image captioning methods only have a local perspective at the dataset level, allowing them to explore dispersed information within individual images. However, the lack of a global perspective prevents them from capturing common characteristics among similar images. To address the limitation, this paper introduces a novel T riple-stream C ommonsense C irculating T ransformer N etwork (TCCTN). It incorporates contextual stream into the encoder, combining enhanced channel stream and spatial stream for comprehensive feature learning. The proposed commonsense-aware contextual attention (CCA) module queries commonsense contextual features from the dataset, obtaining global contextual association information by projecting grid features into the contextual space. The pure semantic channel attention (PSCA) module leverages compressed spatial domain for channel pooling, focusing on attention weights of pure channel features to capture inherent semantic features. The region spatial attention (RSA) module enhances spatial concepts in semantic learning by incorporating region position information. Furthermore, leveraging the complementary differences among the three features, TCCTN introduces the mixture of experts strategy to enhance the unique discriminative ability of features and promote their integration in textual feature learning. Extensive experiments on the MS-COCO dataset demonstrate the effectiveness of contextual commonsense stream and the superior performance of TCCTN.},
  archive      = {J_CVIU},
  author       = {Jianchao Li and Wei Zhou and Kai Wang and Haifeng Hu},
  doi          = {10.1016/j.cviu.2024.104165},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104165},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Triple-stream commonsense circulation transformer network for image captioning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VIDF-net: A voxel-image dynamic fusion method for 3D object
detection. <em>CVIU</em>, <em>249</em>, 104164. (<a
href="https://doi.org/10.1016/j.cviu.2024.104164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, multi-modal fusion methods have shown excellent performance in the field of 3D object detection, which select the voxel centers and globally fuse with image features across the scene. However, these approaches exist two issues. First, The distribution of voxel density is highly heterogeneous due to the discrete volumes. Additionally, there are significant differences in the features between images and point clouds. Global fusion does not take into account the correspondence between these two modalities, which leads to the insufficient fusion. In this paper, we propose a new multi-modal fusion method named Voxel-Image Dynamic Fusion (VIDF). Specifically, VIDF-Net is composed of the Voxel Centroid Mapping module (VCM) and the Deformable Attention Fusion module (DAF). The Voxel Centroid Mapping module is used to calculate the centroid of voxel features and map them onto the image plane, which can locate the position of voxel features more effectively. We then use the Deformable Attention Fusion module to dynamically calculates the offset of each voxel centroid from the image position and combine these two modalities. Furthermore, we propose Region Proposal Network with Channel-Spatial Aggregate to combine channel and spatial attention maps for improved multi-scale feature interaction. We conduct extensive experiments on the KITTI dataset to demonstrate the outstanding performance of proposed VIDF network. In particular, significant improvements have been observed in the Hard categories of Cars and Pedestrians, which shows the significant effectiveness of our approach in dealing with complex scenarios.},
  archive      = {J_CVIU},
  author       = {Xuezhi Xiang and Dianang Li and Xi Wang and Xiankun Zhou and Yulong Qiao},
  doi          = {10.1016/j.cviu.2024.104164},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104164},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {VIDF-net: A voxel-image dynamic fusion method for 3D object detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delving into CLIP latent space for video anomaly
recognition. <em>CVIU</em>, <em>249</em>, 104163. (<a
href="https://doi.org/10.1016/j.cviu.2024.104163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We tackle the complex problem of detecting and recognising anomalies in surveillance videos at the frame level, utilising only video-level supervision. We introduce the novel method A n o m a l y C L I P AnomalyCLIP , the first to combine Vision and Language Models (VLMs), such as CLIP, with multiple instance learning for joint video anomaly detection and classification. Our approach specifically involves manipulating the latent CLIP feature space to identify the normal event subspace, which in turn allows us to effectively learn text-driven directions for abnormal events. When anomalous frames are projected onto these directions, they exhibit a large feature magnitude if they belong to a particular class. We also leverage a computationally efficient Transformer architecture to model short- and long-term temporal dependencies between frames, ultimately producing the final anomaly score and class prediction probabilities. We compare A n o m a l y C L I P AnomalyCLIP against state-of-the-art methods considering three major anomaly detection benchmarks, i.e. ShanghaiTech, UCF-Crime, and XD-Violence, and empirically show that it outperforms baselines in recognising video anomalies. Project website and code are available at https://lucazanella.github.io/AnomalyCLIP/ .},
  archive      = {J_CVIU},
  author       = {Luca Zanella and Benedetta Liberatori and Willi Menapace and Fabio Poiesi and Yiming Wang and Elisa Ricci},
  doi          = {10.1016/j.cviu.2024.104163},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104163},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Delving into CLIP latent space for video anomaly recognition},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human–object interaction detection algorithm based on graph
structure and improved cascade pyramid network. <em>CVIU</em>,
<em>249</em>, 104162. (<a
href="https://doi.org/10.1016/j.cviu.2024.104162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of insufficient use of human–object interaction (HOI) information and spatial location information in images, we propose a human–object​ interaction detection network based on graph structure and improved cascade pyramid. This network is composed of three branches, namely, graph branch, human–object branch and human pose branch. In graph branch, we propose a Graph-based Interactive Feature Generation Algorithm (GIFGA) to address the inadequate utilization of interaction information. GIFGA constructs an initial dense graph model by taking humans and objects as nodes and their interaction relationships as edges. Then, by traversing each node, the graph model is updated to generate the final interaction features. In human pose branch, we propose an Improved Cascade Pyramid Network (ICPN) to tackle the underutilization of spatial location information. ICPN extracts human pose features and maps both the object bounding boxes and extracted human pose maps onto the global feature map to capture the most discriminative interaction-related region features within the global context. Finally, the features from the three branches are fed into a Multi-Layer Perceptron (MLP) for fusion and then classified for recognition. Experimental results demonstrate that our network achieves mAP of 54.93% and 28.69% on the V-COCO and HICO-DET datasets, respectively.},
  archive      = {J_CVIU},
  author       = {Qing Ye and Xiuju Xu and Rui Li and Yongmei Zhang},
  doi          = {10.1016/j.cviu.2024.104162},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104162},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human–object interaction detection algorithm based on graph structure and improved cascade pyramid network},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HBANet: A hybrid boundary-aware attention network for
infrared and visible image fusion. <em>CVIU</em>, <em>249</em>, 104161.
(<a href="https://doi.org/10.1016/j.cviu.2024.104161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion is an extensively investigated problem in infrared image processing, aiming to extract useful information from source images. However, the automatic fusion of these images presents a significant challenge due to the large domain difference and ambiguous boundaries. In this article, we propose a novel image fusion approach based on hybrid boundary-aware attention, termed HBANet, which models global dependencies across the image and leverages boundary-wise prior knowledge to supplement local details. Specifically, we design a novel mixed boundary-aware attention module that is capable of leveraging spatial information to the fullest extent and integrating long dependencies across different domains. To preserve the integrity of texture and structural information, we introduced a sophisticated loss function that comprises structure, intensity, and variation losses. Our method has been demonstrated to outperform state-of-the-art methods in terms of both visual and quantitative metrics, in our experiments on public datasets. Furthermore, our approach also exhibits great generalization capability, achieving satisfactory results in CT and MRI image fusion tasks.},
  archive      = {J_CVIU},
  author       = {Xubo Luo and Jinshuo Zhang and Liping Wang and Dongmei Niu},
  doi          = {10.1016/j.cviu.2024.104161},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104161},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {HBANet: A hybrid boundary-aware attention network for infrared and visible image fusion},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating optical flow: A comprehensive review of the state
of the art. <em>CVIU</em>, <em>249</em>, 104160. (<a
href="https://doi.org/10.1016/j.cviu.2024.104160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical flow estimation is a crucial task in computer vision that provides low-level motion information. Despite recent advances, real-world applications still present significant challenges. This survey provides an overview of optical flow techniques and their application. For a comprehensive review, this survey covers both classical frameworks and the latest AI-based techniques. In doing so, we highlight the limitations of current benchmarks and metrics, underscoring the need for more representative datasets and comprehensive evaluation methods. The survey also highlights the importance of integrating industry knowledge and adopting training practices optimized for deep learning-based models. By addressing these issues, future research can aid the development of robust and efficient optical flow methods that can effectively address real-world scenarios.},
  archive      = {J_CVIU},
  author       = {Andrea Alfarano and Luca Maiano and Lorenzo Papa and Irene Amerini},
  doi          = {10.1016/j.cviu.2024.104160},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104160},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Estimating optical flow: A comprehensive review of the state of the art},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agglomerator++: Interpretable part-whole hierarchies and
latent space representations in neural networks. <em>CVIU</em>,
<em>249</em>, 104159. (<a
href="https://doi.org/10.1016/j.cviu.2024.104159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks achieve outstanding results in a large variety of tasks, often outperforming human experts. However, a known limitation of current neural architectures is the poor accessibility in understanding and interpreting the network’s response to a given input. This is directly related to the huge number of variables and the associated non-linearities of neural models, which are often used as black boxes. This lack of transparency, particularly in crucial areas like autonomous driving, security, and healthcare, can trigger skepticism and limit trust, despite the networks’ high performance. In this work, we want to advance the interpretability in neural networks. We present Agglomerator++, a framework capable of providing a representation of part-whole hierarchies from visual cues and organizing the input distribution to match the conceptual-semantic hierarchical structure between classes. We evaluate our method on common datasets, such as SmallNORB, MNIST, FashionMNIST, CIFAR-10, and CIFAR-100, showing that our solution delivers a more interpretable model compared to other state-of-the-art approaches. Our code is available at https://mmlab-cv.github.io/Agglomeratorplusplus/ .},
  archive      = {J_CVIU},
  author       = {Zeno Sambugaro and Nicola Garau and Niccoló Bisagno and Nicola Conci},
  doi          = {10.1016/j.cviu.2024.104159},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104159},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Agglomerator++: Interpretable part-whole hierarchies and latent space representations in neural networks},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CAFNet: Context aligned fusion for depth completion.
<em>CVIU</em>, <em>249</em>, 104158. (<a
href="https://doi.org/10.1016/j.cviu.2024.104158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth completion aims at reconstructing a dense depth from sparse depth input, frequently using color images as guidance. The sparse depth map lacks sufficient contexts for reconstructing focal contexts such as the shape of objects. The RGB images contain redundant contexts including details useless for reconstruction, which reduces the efficiency of focal context extraction. The unaligned contextual information from these two modalities poses a challenge to focal context extraction and further fusion, as well as the accuracy of depth completion. To optimize the utilization of multimodal contextual information, we explore a novel framework: Context Aligned Fusion Network (CAFNet). CAFNet comprises two stages: the context-aligned stage and the full-scale stage. In the context-aligned stage, CAFNet downsamples input RGB-D pairs to the scale, at which multimodal contextual information is adequately aligned for feature extraction in two encoders and fusion in CF modules. In the full-scale stage, feature maps with fused multimodal context from the previous stage are upsampled to the original scale and subsequentially fused with full-scale depth features by the GF module utilizing a dynamic masked fusion strategy. Ultimately, accurate dense depth maps are reconstructed, leveraging the GF module’s resultant features. Experiments conducted on indoor and outdoor benchmark datasets show that the CAFNet produces results comparable to state-of-the-art methods while effectively reducing computational costs.},
  archive      = {J_CVIU},
  author       = {Zhichao Fu and Anran Wu and Shuwen Yang and Tianlong Ma and Liang He},
  doi          = {10.1016/j.cviu.2024.104158},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104158},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CAFNet: Context aligned fusion for depth completion},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight convolutional neural network-based feature
extractor for visible images. <em>CVIU</em>, <em>249</em>, 104157. (<a
href="https://doi.org/10.1016/j.cviu.2024.104157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature extraction networks (FENs), as the first stage in many computer vision tasks, play critical roles. Previous studies regarding FENs employed deeper and wider networks to attain higher accuracy, but their approaches were memory-inefficient and computationally intensive. Here, we present an accurate and lightweight feature extractor (RoShuNet) for visible images based on ShuffleNetV2. The provided improvements are threefold. To make ShuffleNetV2 compact without degrading its feature extraction ability, we propose an aggregated dual group convolutional module; to better aid the channel interflow process, we propose a γ γ -weighted shuffling module; to further reduce the complexity and size of the model, we introduce slimming strategies. Classification experiments demonstrate the state-of-the-art (SOTA) performance of RoShuNet, which yields an increase in accuracy and reduces the complexity and size of the model compared to those of ShuffleNetV2. Generalization experiments verify that the proposed method is also applicable to feature extraction tasks in semantic segmentation and multiple-object tracking scenarios, achieving comparable accuracy to that of other approaches with more memory and greater computational efficiency. Our method provides a novel perspective for designing lightweight models.},
  archive      = {J_CVIU},
  author       = {Xujie He and Jing Jin and Yu Jiang and Dandan Li},
  doi          = {10.1016/j.cviu.2024.104157},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104157},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A lightweight convolutional neural network-based feature extractor for visible images},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast differential network with adaptive reference sample
for gaze estimation. <em>CVIU</em>, <em>249</em>, 104156. (<a
href="https://doi.org/10.1016/j.cviu.2024.104156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most non-invasive gaze estimation methods do not consider the inter-individual differences in anatomical structure, but directly regress the gaze direction from the appearance image information, which limits the accuracy of individual-independent gaze estimation networks. In addition, existing gaze estimation methods tend to consider only how to improve the model’s generalization performance, ignoring the crucial issue of efficiency, which leads to bulky models that are difficult to deploy and have questionable cost-effectiveness in practical use. This paper makes the following contributions: (1) A differential network for gaze estimation using adaptive reference samples is proposed, which can adaptively select reference samples based on scene and individual characteristics. (2) The knowledge distillation is used to transfer the knowledge structure of robust teacher networks into lightweight networks so that our networks can execute quickly and at low computational cost, dramatically increasing the prospect and value of applying gaze estimation. (3) Integrating the above innovations, a novel fast differential neural network (Diff-Net) named FDAR-Net is constructed and achieved excellent results on MPIIGaze, UTMultiview and EyeDiap.},
  archive      = {J_CVIU},
  author       = {Jiahui Hu and Yonghua Lu and Xiyuan Ye and Qiang Feng and Lihua Zhou},
  doi          = {10.1016/j.cviu.2024.104156},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104156},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A fast differential network with adaptive reference sample for gaze estimation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deformable surface reconstruction via riemannian metric
preservation. <em>CVIU</em>, <em>249</em>, 104155. (<a
href="https://doi.org/10.1016/j.cviu.2024.104155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the pose of an object from a monocular image is a fundamental inverse problem in computer vision. Due to its ill-posed nature, solving this problem requires incorporating deformation priors. In practice, many materials do not perceptibly shrink or extend when manipulated, constituting a reliable and well-known prior. Mathematically, this translates to the preservation of the Riemannian metric. Neural networks offer the perfect playground to solve the surface reconstruction problem as they can approximate surfaces with arbitrary precision and allow the computation of differential geometry quantities. This paper presents an approach for inferring continuous deformable surfaces from a sequence of images, which is benchmarked against several techniques and achieves state-of-the-art performance without the need for offline training. Being a method that performs per-frame optimization, our method can refine its estimates, contrary to those based on performing a single inference step. Despite enforcing differential geometry constraints at each update, our approach is the fastest of all the tested optimization-based methods.},
  archive      = {J_CVIU},
  author       = {Oriol Barbany and Adrià Colomé and Carme Torras},
  doi          = {10.1016/j.cviu.2024.104155},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104155},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deformable surface reconstruction via riemannian metric preservation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bypass network for semantics driven image paragraph
captioning. <em>CVIU</em>, <em>249</em>, 104154. (<a
href="https://doi.org/10.1016/j.cviu.2024.104154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image paragraph captioning aims to describe a given image with a sequence of coherent sentences. Most existing methods model the coherence through the topic transition that dynamically infers a topic vector from preceding sentences. However, these methods still suffer from immediate or delayed repetitions in generated paragraphs because (i) the entanglement of syntax and semantics distracts the topic vector from attending pertinent visual regions; (ii) there are few constraints or rewards for learning long-range transitions. In this paper, we propose a bypass network that separately models semantics and linguistic syntax of preceding sentences. Specifically, the proposed model consists of two main modules, i.e. a topic transition module and a sentence generation module. The former takes previous semantic vectors as queries and applies attention mechanism on regional features to acquire the next topic vector, which reduces immediate repetition by eliminating linguistics. The latter decodes the topic vector and the preceding syntax state to produce the following sentence. To further reduce delayed repetition in generated paragraphs, we devise a replacement-based reward for the REINFORCE training. Comprehensive experiments on the widely used benchmark demonstrate the superiority of the proposed model over the state of the art for coherence while maintaining high accuracy.},
  archive      = {J_CVIU},
  author       = {Qi Zheng and Chaoyue Wang and Dadong Wang},
  doi          = {10.1016/j.cviu.2024.104154},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104154},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bypass network for semantics driven image paragraph captioning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AWADA: Foreground-focused adversarial learning for
cross-domain object detection. <em>CVIU</em>, <em>249</em>, 104153. (<a
href="https://doi.org/10.1016/j.cviu.2024.104153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection networks have achieved impressive results, but it can be challenging to replicate this success in practical applications due to a lack of relevant data specific to the task. Typically, additional data sources are used to support the training process. However, the domain gaps between these data sources present a challenge. Adversarial image-to-image style transfer is often used to bridge this gap, but it is not directly connected to the object detection task and can be unstable. We propose AWADA, a framework that combines attention-weighted adversarial domain adaptation connecting style transfer and object detection. By using object detector proposals to create attention maps for foreground objects, we focus the style transfer on these regions and stabilize the training process. Our results demonstrate that AWADA can reach state-of-the-art unsupervised domain adaptation performance in three commonly used benchmarks.},
  archive      = {J_CVIU},
  author       = {Maximilian Menke and Thomas Wenzel and Andreas Schwung},
  doi          = {10.1016/j.cviu.2024.104153},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104153},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AWADA: Foreground-focused adversarial learning for cross-domain object detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A convex kullback–leibler optimization for semi-supervised
few-shot learning. <em>CVIU</em>, <em>249</em>, 104152. (<a
href="https://doi.org/10.1016/j.cviu.2024.104152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning has achieved great success in many fields, thanks to its requirement of limited number of labeled data. However, most of the state-of-the-art techniques of few-shot learning employ transfer learning, which still requires massive labeled data to train a meta-learning system. To simulate the human learning mechanism, a deep model of few-shot learning is proposed to learn from one, or a few examples. First of all in this paper, we analyze and note that the problem with representative semi-supervised few-shot learning methods is getting stuck in local optimization and the negligence of intra-class compactness problem. To address these issue, we propose a novel semi-supervised few-shot learning method with Convex Kullback–Leibler, hereafter referred to as CKL, in which KL divergence is employed to achieve global optimum solution by optimizing a strictly convex functions to perform clustering; whereas sample selection strategy is employed to achieve intra-class compactness. In training, the CKL is optimized iteratively via deep learning and expectation–maximization algorithm. Intensive experiments have been conducted on three popular benchmark data sets, take miniImagenet data set for example, our proposed CKL achieved 76.83% and 85.78% under 5-way 1-shot and 5-way 5-shot, the experimental results show that this method significantly improves the classification ability of few-shot learning tasks and obtains the start-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Yukun Liu and Zhaohui Luo and Daming Shi},
  doi          = {10.1016/j.cviu.2024.104152},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104152},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A convex Kullback–Leibler optimization for semi-supervised few-shot learning},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HAD-net: An attention u-based network with hyper-scale
shifted aggregating and max-diagonal sampling for medical image
segmentation. <em>CVIU</em>, <em>249</em>, 104151. (<a
href="https://doi.org/10.1016/j.cviu.2024.104151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate extraction of regions of interest (ROI) with variable shapes and scales is one of the primary challenges in medical image segmentation. Current U-based networks mostly aggregate multi-stage encoding outputs as an improved multi-scale skip connection. Although this design has been proven to provide scale diversity and contextual integrity, there remain several intuitive limits: (i) the encoding outputs are resampled to the same size simply, which destruct the fine-grained information. The advantages of utilization of multiple scales are insufficient. (ii) Certain redundant information proportional to the feature dimension size is introduced and causes multi-stage interference. And (iii) the precision of information delivery relies on the up-sampling and down-sampling layers, but guidance on maintaining consistency in feature locations and trends between them is lacking. To improve these situations, this paper proposed a U-based CNN network named HAD-Net, by assembling a new hyper-scale shifted aggregating module (HSAM) paradigm and progressive reusing attention (PRA) for skip connections, as well as employing a novel pair of dual-branch parameter-free sampling layers, i.e. max-diagonal pooling (MDP) and max-diagonal un-pooling (MDUP). That is, the aggregating scheme additionally combines five subregions with certain offsets in the shallower stage. Since the lower scale-down ratios of subregions enrich scales and fine-grain context. Then, the attention scheme contains a partial-to-global channel attention (PGCA) and a multi-scale reusing spatial attention (MRSA), it builds reusing connections internally and adjusts the focus on more useful dimensions. Finally, MDP and MDUP are explored in pairs to improve texture delivery and feature consistency, enhancing information retention and avoiding positional confusion. Compared to state-of-the-art networks, HAD-Net has achieved comparable and even better performances with Dice of 90.13%, 81.51%, and 75.43% for each class on BraTS20, 89.59% Dice and 98.56% AUC on Kvasir-SEG, as well as 82.17% Dice and 98.05% AUC on DRIVE. The scheme of HSAM+PRA+MDP+MDUP has been proven to be a remarkable improvement and leaves room for further research.},
  archive      = {J_CVIU},
  author       = {Junding Sun and Yabei Li and Xiaosheng Wu and Chaosheng Tang and Shuihua Wang and Yudong Zhang},
  doi          = {10.1016/j.cviu.2024.104151},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104151},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {HAD-net: An attention U-based network with hyper-scale shifted aggregating and max-diagonal sampling for medical image segmentation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M-adapter: Multi-level image-to-video adaptation for video
action recognition. <em>CVIU</em>, <em>249</em>, 104150. (<a
href="https://doi.org/10.1016/j.cviu.2024.104150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing size of visual foundation models, training video models from scratch has become costly and challenging. Recent attempts focus on transferring frozen pre-trained Image Models (PIMs) to video fields by tuning inserted learnable parameters such as adapters and prompts. However, these methods require saving PIM activations for gradient calculations, leading to limited savings of GPU memory. In this paper, we propose a novel parallel branch that adapts the multi-level outputs of the frozen PIM for action recognition. It avoids passing gradients through the PIMs, thus naturally owning much lower GPU memory footprints. The proposed adaptation branch consists of hierarchically combined multi-level output adapters (M-adapters), comprising a fusion module and a temporal module. This design digests the existing discrepancies between the pre-training task and the target task with lower training costs. We show that when using larger models or on scenarios with higher demands for temporal modelling, the proposed method performs better than those with the full-parameter tuning manner. Finally, despite only tuning fewer parameters, our method achieves superior or comparable performance against current state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Rongchang Li and Tianyang Xu and Xiao-Jun Wu and Linze Li and Xiao Yang and Zhongwei Shen and Josef Kittler},
  doi          = {10.1016/j.cviu.2024.104150},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104150},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {M-adapter: Multi-level image-to-video adaptation for video action recognition},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Action-conditioned contrastive learning for 3D human pose
and shape estimation in videos. <em>CVIU</em>, <em>249</em>, 104149. (<a
href="https://doi.org/10.1016/j.cviu.2024.104149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this research is to estimate 3D human pose and shape in videos, which is a challenging task due to the complex nature of the human body and the wide range of possible pose and shape variations. This problem also poses difficulty in finding a satisfactory solution due to the trade-off between the accuracy and temporal consistency of the estimated 3D pose and shape. Thus previous researches have prioritized one objective over the other. In contrast, we propose a novel approach called the action-conditioned mesh recovery (ACMR) model, which improves accuracy without compromising temporal consistency by leveraging human action information. Our ACMR model outperforms existing methods that prioritize temporal consistency in terms of accuracy, while also achieving comparable temporal consistency with other state-of-the-art methods. Significantly, the action-conditioned learning process occurs only during training, requiring no additional resources at inference time, thereby enhancing performance without increasing computational demands.},
  archive      = {J_CVIU},
  author       = {Inpyo Song and Moonwook Ryu and Jangwon Lee},
  doi          = {10.1016/j.cviu.2024.104149},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104149},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Action-conditioned contrastive learning for 3D human pose and shape estimation in videos},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LightSOD: Towards lightweight and efficient network for
salient object detection. <em>CVIU</em>, <em>249</em>, 104148. (<a
href="https://doi.org/10.1016/j.cviu.2024.104148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent emphasis has been on achieving rapid and precise detection of salient objects, which presents a challenge for resource-constrained edge devices because the current models are too computationally demanding for deployment. Some recent research has prioritized inference speed over accuracy to address this issue. In response to the inherent trade-off between accuracy and efficiency, we introduce an innovative framework called LightSOD, with the primary objective of achieving a balance between precision and computational efficiency. LightSOD comprises several vital components, including the spatial-frequency boundary refinement module (SFBR), which utilizes wavelet transform to restore spatial loss information and capture edge features from the spatial-frequency domain. Additionally, we introduce a cross-pyramid enhancement module (CPE), which utilizes adaptive kernels to capture multi-scale group-wise features in deep layers. Besides, we introduce a group-wise semantic enhancement module (GSRM) to boost global semantic features in the topmost layer. Finally, we introduce a cross-aggregation module (CAM) to incorporate channel-wise features across layers, followed by a triple features fusion (TFF) that aggregates features from coarse to fine levels. By conducting experiments on five datasets and utilizing various backbones, we have demonstrated that LSOD achieves competitive performance compared with heavyweight cutting-edge models while significantly reducing computational complexity.},
  archive      = {J_CVIU},
  author       = {Ngo-Thien Thu and Hoang Ngoc Tran and Md. Delowar Hossain and Eui-Nam Huh},
  doi          = {10.1016/j.cviu.2024.104148},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104148},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LightSOD: Towards lightweight and efficient network for salient object detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DBMHT: A double-branch multi-hypothesis transformer for 3D
human pose estimation in video. <em>CVIU</em>, <em>249</em>, 104147. (<a
href="https://doi.org/10.1016/j.cviu.2024.104147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of 3D human poses from monocular videos presents a significant challenge. The existing methods face the problems of deep ambiguity and self-occlusion. To overcome these problems, we propose a Double-Branch Multi-Hypothesis Transformer (DBMHT). In detail, we utilize a Double-Branch architecture to capture temporal and spatial information and generate multiple hypotheses. To merge these hypotheses, we adopt a lightweight module to integrate spatial and temporal representations. The DBMHT can not only capture spatial information from each joint in the human body and temporal information from each frame in the video but also merge multiple hypotheses that have different spatio-temporal information. Comprehensive evaluation on two challenging datasets (i.e. Human3.6M and MPI-INF-3DHP) demonstrates the superior performance of DBMHT, marking it as a robust and efficient approach for accurate 3D HPE in dynamic scenarios. The results show that our model surpasses the state-of-the-art approach by 1.9% MPJPE with ground truth 2D keypoints as input.},
  archive      = {J_CVIU},
  author       = {Xuezhi Xiang and Xiaoheng Li and Weijie Bao and Yulong Qiao and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2024.104147},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104147},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DBMHT: A double-branch multi-hypothesis transformer for 3D human pose estimation in video},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class probability space regularization for semi-supervised
semantic segmentation. <em>CVIU</em>, <em>249</em>, 104146. (<a
href="https://doi.org/10.1016/j.cviu.2024.104146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation achieves fine-grained scene parsing in any scenario, making it one of the key research directions to facilitate the development of human visual attention mechanisms. Recent advancements in semi-supervised semantic segmentation have attracted considerable attention due to their potential in leveraging unlabeled data. However, existing methods only focus on exploring the knowledge of unlabeled pixels with high certainty prediction. Their insufficient mining of low certainty regions of unlabeled data results in a significant loss of supervisory information. Therefore, this paper proposes the C lass P robability S pace R egularization ( CPSR ) approach to further exploit the potential of each unlabeled pixel. Specifically, we first design a class knowledge reshaping module to regularize the probability space of low certainty pixels, thereby transforming them into high certainty ones for supervised training. Furthermore, we propose a tail probability suppression module to suppress the probabilities of tailed classes, which facilitates the network to learn more discriminative information from the class probability space. Extensive experiments conducted on the PASCAL VOC2012 and Cityscapes datasets prove that our method achieves state-of-the-art performance without introducing much computational overhead. Code is available at https://github.com/MKSAQW/CPSR .},
  archive      = {J_CVIU},
  author       = {Jianjian Yin and Shuai Yan and Tao Chen and Yi Chen and Yazhou Yao},
  doi          = {10.1016/j.cviu.2024.104146},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104146},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Class probability space regularization for semi-supervised semantic segmentation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acoustic features analysis for explainable machine
learning-based audio spoofing detection. <em>CVIU</em>, <em>249</em>,
104145. (<a href="https://doi.org/10.1016/j.cviu.2024.104145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of synthetic voice generation and audio manipulation technologies poses significant challenges, raising societal and security concerns due to the risks of impersonation and the proliferation of audio deepfakes. This study introduces a lightweight machine learning (ML)-based framework designed to effectively distinguish between genuine and spoofed audio recordings. Departing from conventional deep learning (DL) approaches, which mainly rely on image-based spectrogram features or learning-based audio features, the proposed method utilizes a diverse set of hand-crafted audio features – such as spectral, temporal, chroma, and frequency-domain features – to enhance the accuracy of deepfake audio content detection. Through extensive evaluation and experiments on three well-known datasets, ASVSpoof2019, FakeAVCelebV2, and an In-The-Wild database, the proposed solution demonstrates robust performance and a high degree of generalization compared to state-of-the-art methods. In particular, our method achieved 89% accuracy on ASVSpoof2019, 94.5% on FakeAVCelebV2, and 94.67% on the In-The-Wild database. Additionally, the experiments performed on explainability techniques clarify the decision-making processes within ML models, enhancing transparency and identifying crucial features essential for audio deepfake detection.},
  archive      = {J_CVIU},
  author       = {Carmen Bisogni and Vincenzo Loia and Michele Nappi and Chiara Pero},
  doi          = {10.1016/j.cviu.2024.104145},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104145},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Acoustic features analysis for explainable machine learning-based audio spoofing detection},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal transformer with language modality distillation
for early pedestrian action anticipation. <em>CVIU</em>, <em>249</em>,
104144. (<a href="https://doi.org/10.1016/j.cviu.2024.104144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language-vision integration has become an increasingly popular research direction within the computer vision field. In recent years, there has been a growing recognition of the importance of incorporating linguistic information into visual tasks, particularly in domains such as action anticipation. This integration allows anticipation models to leverage textual descriptions to gain deeper contextual understanding, leading to more accurate predictions. In this work, we focus on pedestrian action anticipation, where the objective is the early prediction of pedestrians’ future actions in urban environments. Our method relies on a multi-modal transformer model that encodes past observations and produces predictions at different anticipation times, employing a learned mask technique to filter out redundancy in the observed frames. Instead of relying solely on visual cues extracted from images or videos, we explore the impact of integrating textual information in enriching the input modalities of our pedestrian action anticipation model. We investigate various techniques for generating descriptive captions corresponding to input images, aiming to enhance the anticipation performance. Evaluation results on available public benchmarks demonstrate the effectiveness of our method in improving the prediction performance at different anticipation times compared to previous works. Additionally, incorporating the language modality in our anticipation model proved significant improvement, reaching a 29.5% increase in the F1 score at 1-second anticipation and a 16.66% increase at 4-second anticipation. These results underscore the potential of language-vision integration in advancing pedestrian action anticipation in complex urban environments.},
  archive      = {J_CVIU},
  author       = {Nada Osman and Guglielmo Camporese and Lamberto Ballan},
  doi          = {10.1016/j.cviu.2024.104144},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104144},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-modal transformer with language modality distillation for early pedestrian action anticipation},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous fake media detection: Adapting deepfake detectors
to new generative techniques. <em>CVIU</em>, <em>249</em>, 104143. (<a
href="https://doi.org/10.1016/j.cviu.2024.104143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative techniques continue to evolve at an impressively high rate, driven by the hype about these technologies. This rapid advancement severely limits the application of deepfake detectors, which, despite numerous efforts by the scientific community, struggle to achieve sufficiently robust performance against the ever-changing content. To address these limitations, in this paper, we propose an analysis of two continuous learning techniques on a Short and a Long sequence of fake media. Both sequences include a complex and heterogeneous range of deepfakes (generated images and videos) from GANs, computer graphics techniques, and unknown sources. Our experiments show that continual learning could be important in mitigating the need for generalizability. In fact, we show that, although with some limitations, continual learning methods help to maintain good performance across the entire training sequence. For these techniques to work in a sufficiently robust way, however, it is necessary that the tasks in the sequence share similarities. In fact, according to our experiments, the order and similarity of the tasks can affect the performance of the models over time. To address this problem, we show that it is possible to group tasks based on their similarity. This small measure allows for a significant improvement even in longer sequences. This result suggests that continual techniques can be combined with the most promising detection methods, allowing them to catch up with the latest generative techniques. In addition to this, we propose an overview of how this learning approach can be integrated into a deepfake detection pipeline for continuous integration and continuous deployment (CI/CD). This allows you to keep track of different funds, such as social networks, new generative tools, or third-party datasets, and through the integration of continuous learning, allows constant maintenance of the detectors.},
  archive      = {J_CVIU},
  author       = {Francesco Tassone and Luca Maiano and Irene Amerini},
  doi          = {10.1016/j.cviu.2024.104143},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104143},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Continuous fake media detection: Adapting deepfake detectors to new generative techniques},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MoMa: Skinned motion retargeting using masked pose modeling.
<em>CVIU</em>, <em>249</em>, 104141. (<a
href="https://doi.org/10.1016/j.cviu.2024.104141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion retargeting requires to carefully analyze the differences in both skeletal structure and body shape between source and target characters. Existing skeleton-aware and shape-aware approaches can deal with such differences, but they struggle when the source and target characters exhibit significant dissimilarities in both skeleton (like joint count and bone length) and shape (like geometry and mesh properties). In this work we introduce MoMa, a novel approach for skinned motion retargeting which is both skeleton and shape-aware. Our skeleton-aware module learns to retarget animations by recovering the differences between source and target using a custom transformer-based auto-encoder coupled with a spatio-temporal masking strategy. The auto-encoder can transfer the motion between input and target skeletons by reconstructing the masked skeletal differences using shared joints as a reference point. Surpassing the limitations of previous approaches, we can also perform retargeting between skeletons with a varying number of leaf joints. Our shape-aware module incorporates a novel face-based optimizer that adapts skeleton positions to limit collisions between body parts. In contrast to conventional vertex-based methods, our face-based optimizer excels in resolving surface collisions within a body shape, resulting in more accurate retargeted motions. The proposed architecture outperforms the state-of-the-art results on the Mixamo dataset, both quantitatively and qualitatively. Our code is available at: [Github link upon acceptance, see supplementary materials].},
  archive      = {J_CVIU},
  author       = {Giulia Martinelli and Nicola Garau and Niccoló Bisagno and Nicola Conci},
  doi          = {10.1016/j.cviu.2024.104141},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104141},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MoMa: Skinned motion retargeting using masked pose modeling},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Targeted adversarial attack on classic vision pipelines.
<em>CVIU</em>, <em>249</em>, 104140. (<a
href="https://doi.org/10.1016/j.cviu.2024.104140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks are susceptible to adversarial attacks. End-to-end differentiability of deep networks provides the analytical formulation which has aided in proliferation of diverse adversarial attacks. On the contrary, handcrafted pipelines (local feature matching, bag-of-words based place recognition, and visual tracking) consist of intuitive approaches and perhaps lack end-to-end formal description. In this work, we show that classic handcrafted pipelines are also susceptible to adversarial attacks. We propose a novel targeted adversarial attack for multiple well-known handcrafted pipelines and datasets. Our attack is able to match an image with any given target image which can be completely different from the original image. Our approach manages to attack simple (image registration) as well as sophisticated multi-stage (place recognition (FAB-MAP), visual tracking (ORB-SLAM3)) pipelines. We outperform multiple baselines over different public datasets (Places, KITTI and HPatches). Our analysis shows that although vulnerable, achieving true imperceptibility is harder in case of targeted attack on handcrafted pipelines. To this end, we propose a stealthy attack where the noise is perceptible but appears benign. In order to assist the community in further examining the weakness of popular handcrafted pipelines we release our code.},
  archive      = {J_CVIU},
  author       = {Kainat Riaz and Muhammad Latif Anjum and Wajahat Hussain and Rohan Manzoor},
  doi          = {10.1016/j.cviu.2024.104140},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104140},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Targeted adversarial attack on classic vision pipelines},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodality-guided visual-caption semantic enhancement.
<em>CVIU</em>, <em>249</em>, 104139. (<a
href="https://doi.org/10.1016/j.cviu.2024.104139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captions generated with single modality, e.g. video clips, often suffer from insufficient event discovery and inadequate scene description. Therefore, this paper aims to improve the quality of captions by addressing these issues through the integration of multi-modal information. Specifically, We first construct a multi-modal dataset and introduce the triplet annotations of video, audio and text, fostering a comprehensive exploration about the associations between different modalities. Build upon this, We propose to explore the collaborative perception of audio and visual concepts to mitigate inaccuracies and incompleteness in captions in vision-based benchmarks by incorporating audio-visual perception priors. To achieve this, we extract effective semantic features from visual and auditory modalities, bridge the semantic gap between audio-visual modalities and text, and form a more precise knowledge graph multimodal coherence checking and information pruning mechanism. Exhaustive experiments demonstrate that the proposed approach surpasses existing methods and generalizes well with the assistance of ChatGPT.},
  archive      = {J_CVIU},
  author       = {Nan Che and Jiang Liu and Fei Yu and Lechao Cheng and Yuxuan Wang and Yuehua Li and Chenrui Liu},
  doi          = {10.1016/j.cviu.2024.104139},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104139},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multimodality-guided visual-caption semantic enhancement},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VADS: Visuo-adaptive DualStrike attack on visual question
answer. <em>CVIU</em>, <em>249</em>, 104137. (<a
href="https://doi.org/10.1016/j.cviu.2024.104137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) is a fundamental task in computer vision and natural language process fields. The adversarial vulnerability of VQA models is crucial for their reliability in real-world applications. However, current VQA attacks are mainly focused on the white-box and transfer-based settings, which require the attacker to have full or partial prior knowledge of victim VQA models. Besides that, query-based VQA attacks require a massive amount of query times, which the victim model may detect. In this paper, we propose the Visuo-Adaptive DualStrike (VADS) attack, a novel adversarial attack method combining transfer-based and query-based strategies to exploit vulnerabilities in VQA systems. Unlike current VQA attacks focusing on either approach, VADS leverages a momentum-like ensemble method to search potential attack targets and compress the perturbation. After that, our method employs a query-based strategy to dynamically adjust the weight of perturbation per surrogate model. We evaluate the effectiveness of VADS across 8 VQA models and two datasets. The results demonstrate that VADS outperforms existing adversarial techniques in both efficiency and success rate. Our code is available at: https://github.com/stevenzhang9577/VADS .},
  archive      = {J_CVIU},
  author       = {Boyuan Zhang and Jiaxu Li and Yucheng Shi and Yahong Han and Qinghua Hu},
  doi          = {10.1016/j.cviu.2024.104137},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104137},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {VADS: Visuo-adaptive DualStrike attack on visual question answer},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DSU-GAN: A robust frontal face recognition approach based on
generative adversarial network. <em>CVIU</em>, <em>249</em>, 104128. (<a
href="https://doi.org/10.1016/j.cviu.2024.104128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition technology is widely used in different areas, such as entrance guard, payment etc . However, little attention has been given to non-positive faces recognition, especially model training and the quality of the generated images. To this end, a novel robust frontal face recognition approach based on generative adversarial network (DSU-GAN) is proposed in this paper. A mechanism of consistency loss is presented in deformable convolution proposed in the generator-encoder to avoid additional computational overhead and the problem of overfitting. In addition, a self-attention mechanism is presented in generator–encoder to avoid information overloading and construct the long-term dependencies at the pixel level. To balance the capability between the generator and discriminator, a novelf discriminator architecture based U-Net is proposed. Finally, the single-way discriminator is improved through a new up-sampling module. Experiment results demonstrate that our proposal achieves an average Rank-1 recognition rate of 95.14% on the Multi-PIE face dataset in dealing with the multi-pose. In addition, it is proven that our proposal has achieved outstanding performance in recent benchmarks conducted on both IJB-A and IJB-C.},
  archive      = {J_CVIU},
  author       = {Deyu Lin and Huanxin Wang and Xin Lei and Weidong Min and Chenguang Yao and Yuan Zhong and Yong Liang Guan},
  doi          = {10.1016/j.cviu.2024.104128},
  journal      = {Computer Vision and Image Understanding},
  month        = {12},
  pages        = {104128},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DSU-GAN: A robust frontal face recognition approach based on generative adversarial network},
  volume       = {249},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Embedding AI ethics into the design and use of computer
vision technology for consumer’s behaviour understanding. <em>CVIU</em>,
<em>248</em>, 104142. (<a
href="https://doi.org/10.1016/j.cviu.2024.104142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) techniques are becoming more and more sophisticated showing the potential to deeply understand and predict consumer behaviour in a way to boost the retail sector; however, retail-sensitive considerations underpinning their deployment have been poorly explored to date. This paper explores the application of AI technologies in the retail sector, focusing on their potential to enhance decision-making processes by preventing major ethical risks inherent to them, such as the propagation of bias and systems’ lack of explainability. Drawing on recent literature on AI ethics, this study proposes a methodological path for the design and the development of trustworthy, unbiased, and more explainable AI systems in the retail sector. Such framework grounds on European (EU) AI ethics principles and addresses the specific nuances of retail applications. To do this, we first examine the VRAI framework, a deep learning model used to analyse shopper interactions, people counting and re-identification, to highlight the critical need for transparency and fairness in AI operations. Second, the paper proposes actionable strategies for integrating high-level ethical guidelines into practical settings, and particularly, to mitigate biases leading to unfair outcomes in AI systems and improve their explainability. By doing so, the paper aims to show the key added value of embedding AI ethics requirements into AI practices and computer vision technology to truly promote technically and ethically robust AI in the retail domain.},
  archive      = {J_CVIU},
  author       = {Simona Tiribelli and Benedetta Giovanola and Rocco Pietrini and Emanuele Frontoni and Marina Paolanti},
  doi          = {10.1016/j.cviu.2024.104142},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104142},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Embedding AI ethics into the design and use of computer vision technology for consumer’s behaviour understanding},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CRML-net: Cross-modal reasoning and multi-task learning
network for tooth image segmentation. <em>CVIU</em>, <em>248</em>,
104138. (<a href="https://doi.org/10.1016/j.cviu.2024.104138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data from a single modality may suffer from noise, low contrast, or other imaging limitations that affect the model’s accuracy. Furthermore, due to the limited amount of data, most models trained on single-modality data tend to overfit the training set and perform poorly on out-of-domain data. Therefore, in this paper, we propose a network named Cross-Modal Reasoning and Multi-Task Learning Network (CRML-Net), which combines cross-modal reasoning and multi-task learning, aiming to leverage the complementary information between different modalities and tasks to enhance the model’s generalization ability and accuracy. Specifically, CRML-Net consists of two stages. In the first stage, our network extracts a new morphological information modality from the original image and then performs cross-modal fusion with the original modality image, aiming to leverage the morphological information to enhance the model’s robustness to out-of-domain datasets. In the second stage, based on the output of the previous stage, we introduce a multi-task learning mechanism, aiming to improve the model’s performance on unseen data by sharing surface detail information from auxiliary tasks. We validated our method on a publicly available tooth cone beam computed tomography dataset. Our evaluation demonstrates that our method outperforms state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Yingda Lyu and Zhehao Liu and Yingxin Zhang and Haipeng Chen and Zhimin Xu},
  doi          = {10.1016/j.cviu.2024.104138},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104138},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CRML-net: Cross-modal reasoning and multi-task learning network for tooth image segmentation},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). URINet: Unsupervised point cloud rotation invariant
representation learning via semantic and structural reasoning.
<em>CVIU</em>, <em>248</em>, 104136. (<a
href="https://doi.org/10.1016/j.cviu.2024.104136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many rotation-invariant networks have been proposed to alleviate the interference caused by point cloud arbitrary rotations. These networks have demonstrated powerful representation learning capabilities. However, most of those methods rely on costly manually annotated supervision for model training. Moreover, they fail to reason the structural relations and lose global information. To address these issues, we present an unsupervised method for achieving comprehensive rotation invariant representations without human annotation. Specifically, we propose a novel encoder–decoder architecture named URINet, which learns a point cloud representation by combining local semantic and global structural information, and then reconstructs the input without rotation perturbation. In detail, the encoder is a two-branch network where the graph convolution based structural branch models the relationships among local regions to learn global structural knowledge and the semantic branch learns rotation invariant local semantic features. The two branches derive complementary information and explore the point clouds comprehensively. Furthermore, to avoid the self-reconstruction ambiguity brought by uncertain poses, a bidirectional alignment is proposed to measure the quality of reconstruction results without orientation knowledge. Extensive experiments on downstream tasks show that the proposed method significantly surpasses existing state-of-the-art methods on both synthetic and real-world datasets.},
  archive      = {J_CVIU},
  author       = {Qiuxia Wu and Kunming Su},
  doi          = {10.1016/j.cviu.2024.104136},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104136},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {URINet: Unsupervised point cloud rotation invariant representation learning via semantic and structural reasoning},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The shading isophotes: Model and methods for lambertian
planes and a point light. <em>CVIU</em>, <em>248</em>, 104135. (<a
href="https://doi.org/10.1016/j.cviu.2024.104135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structure-from-Motion (SfM) and Shape-from-Shading (SfS) are complementary classical approaches to 3D vision. Broadly speaking, SfM exploits geometric primitives from textured surfaces and SfS exploits pixel intensity from the shading image. We propose an approach that exploits virtual geometric primitives extracted from the shading image, namely the level-sets, which we name shading isophotes. Our approach thus combines the strength of geometric reasoning with the rich shading information. We focus on the case of untextured Lambertian planes of unknown albedo lit by an unknown Point Light Source (PLS) of unknown intensity. We derive a comprehensive geometric model showing that the unknown scene parameters are in general all recoverable from a single image of at least two planes. We propose computational methods to detect the isophotes, to reconstruct the scene parameters in closed-form and to refine the results densely using pixel intensity. Our methods thus estimate light source, plane pose and camera pose parameters for untextured planes, which cannot be achieved by the existing approaches. We evaluate our model and methods on synthetic and real images.},
  archive      = {J_CVIU},
  author       = {Damien Mariyanayagam and Adrien Bartoli},
  doi          = {10.1016/j.cviu.2024.104135},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104135},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {The shading isophotes: Model and methods for lambertian planes and a point light},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetrical siamese network for pose-guided person
synthesis. <em>CVIU</em>, <em>248</em>, 104134. (<a
href="https://doi.org/10.1016/j.cviu.2024.104134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose-Guided Person Image Synthesis (PGPIS) aims to generate a realistic person image that preserves the appearance of the source person while adopting the target pose. Various appearances and drastic pose changes make this task highly challenging. Due to the insufficient utilization of paired data, existing models face difficulties in accurately preserving the source appearance details and high-frequency textures in the generated images. Meanwhile, although current popular AdaIN-based methods are advantageous in handling drastic pose changes, they struggle to capture diverse clothing shapes imposed by the limitation of global feature statistics. To address these issues, we propose a novel Symmetrical Siamese Network (SSNet) for PGPIS, which consists of two synergistic symmetrical generative branches that leverage prior knowledge of paired data to comprehensively exploit appearance details. For feature integration, we propose a Style Matching Module (SMM) to transfer multi-level region appearance styles and gradient information to the desired pose for enriching the high-frequency textures. Furthermore, to overcome the limitation of global feature statistics, a Spatial Attention Module (SAM) is introduced to complement the SMM for capturing clothing shapes. Extensive experiments show the effectiveness of our SSNet, achieving state-of-the-art results on public datasets. Moreover, our SSNet can also edit the source appearance attributes, making it versatile in wider application scenarios.},
  archive      = {J_CVIU},
  author       = {Quanwei Yang and Lingyun Yu and Fengyuan Liu and Yun Song and Meng Shao and Guoqing Jin and Hongtao Xie},
  doi          = {10.1016/j.cviu.2024.104134},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104134},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Symmetrical siamese network for pose-guided person synthesis},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audio–visual deepfake detection using articulatory
representation learning. <em>CVIU</em>, <em>248</em>, 104133. (<a
href="https://doi.org/10.1016/j.cviu.2024.104133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in generative artificial intelligence have made it easier to manipulate auditory and visual elements, highlighting the critical need for robust audio–visual deepfake detection methods. In this paper, we propose an articulatory representation-based audio–visual deepfake detection approach, ART-AVDF . First, we devise an audio encoder to extract articulatory features that capture the physical significance of articulation movement, integrating with a lip encoder to explore audio–visual articulatory correspondences in a self-supervised learning manner. Then, we design a multimodal joint fusion module to further explore inherent audio–visual consistency using the articulatory embeddings. Extensive experiments on the DFDC, FakeAVCeleb, and DefakeAVMiT datasets demonstrate that ART-AVDF obtains a significant performance improvement compared to many deepfake detection models.},
  archive      = {J_CVIU},
  author       = {Yujia Wang and Hua Huang},
  doi          = {10.1016/j.cviu.2024.104133},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104133},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Audio–visual deepfake detection using articulatory representation learning},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RSTC: Residual swin transformer cascade to approximate
taylor expansion for image denoising. <em>CVIU</em>, <em>248</em>,
104132. (<a href="https://doi.org/10.1016/j.cviu.2024.104132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional denoising methods establish mathematical models by employing different priors, which can achieve preferable results but they are usually time-consuming and their outputs are not adaptive on regularization parameters. While the success of end-to-end deep learning denoising strategies depends on a large amount of data and lacks a theoretical interpretability. In order to address the above problems, this paper proposes a novel image denoising method, namely Residual Swin Transformer Cascade (RSTC), based on Taylor expansion. The key procedures of our RSTC are specified as follows: Firstly, we discuss the relationship between image denoising model and Taylor expansion, as well as its adjacent derivative parts. Secondly, we use a lightweight deformable convolutional neural network to estimate the basic layer of Taylor expansion and a residual network where swin transformer block is selected as a backbone for pursuing the solution of the derivative layer. Finally, the results of the two networks contribute to the approximation solution of Taylor expansion. In the experiments, we firstly test and discuss the selection of network parameters to verify its effectiveness. Then, we compare it with existing advanced methods in terms of visualization and quantification, and the results show that our method has a powerful generalization ability and performs better than state-of-the-art denoising methods on performance improvement and structure preservation.},
  archive      = {J_CVIU},
  author       = {Jin Liu and Yang Yang and Biyun Xu and Hao Yu and Yaozong Zhang and Qian Li and Zhenghua Huang},
  doi          = {10.1016/j.cviu.2024.104132},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104132},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {RSTC: Residual swin transformer cascade to approximate taylor expansion for image denoising},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual cross perception network with texture and boundary
guidance for camouflaged object detection. <em>CVIU</em>, <em>248</em>,
104131. (<a href="https://doi.org/10.1016/j.cviu.2024.104131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is a task needs to segment objects that subtly blend into their surroundings effectively. Edge and texture information of the objects can be utilized to reveal the edges of camouflaged objects and detect texture differences between camouflaged objects and the surrounding environment. However, existing methods often fail to fully exploit the advantages of these two types of information. Considering this, our paper proposes an innovative Dual Cross Perception Network (DCPNet) with texture and boundary guidance for camouflaged object detection. DCPNet consists of two essential modules, namely Dual Cross Fusion Module (DCFM) and the Subgroup Aggregation Module (SAM). DCFM utilizes attention techniques to emphasize the information that exists in edges and textures by cross-fusing features of the edge, texture, and basic RGB image, which strengthens the ability to capture edge information and texture details in image analysis. SAM gives varied weights to low-level and high-level features in order to enhance the comprehension of objects and scenes of various sizes. Several experiments have demonstrated that DCPNet outperforms 13 state-of-the-art methods on four widely used assessment metrics.},
  archive      = {J_CVIU},
  author       = {Yaming Wang and Jiatong Chen and Xian Fang and Mingfeng Jiang and Jianhua Ma},
  doi          = {10.1016/j.cviu.2024.104131},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104131},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dual cross perception network with texture and boundary guidance for camouflaged object detection},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DDGPnP: Differential degree graph based PnP solution to
handle outliers. <em>CVIU</em>, <em>248</em>, 104130. (<a
href="https://doi.org/10.1016/j.cviu.2024.104130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing external relationships for outlier removal in the perspective-n-point problem are generally spatial coherence among the neighbor correspondences. In the situation of high noise or spatially incoherent distributions, pose estimation is relatively inaccurate due to a small number of detected inliers. To address these problems, this paper explores the globally coherent external relationships for outlier removal and pose estimation. To this end, the differential degree graph (DDG) is proposed to employ the intersection angles between rays of correspondences to handle outliers. Firstly, a pair of two degree graphs are constructed to establish the external relationships between 3D-2D correspondences in the world and camera coordinates. Secondly, the DDG is estimated through subtracting the two degree graphs and operating binary operation with a degree threshold. Besides, this paper mathematically proves that the maximum clique of the DDG represents the inliers. Thirdly, a novel vertice degree based method is put forward to extract the maximum clique from DDG for outlier removal. Besides, this paper proposes a novel pipeline of DDG based PnP solution, i.e. DDGPnP, to achieve accurate pose estimation. Experiments demonstrate the superiority and effectiveness of the proposed method in the aspects of outlier removal and pose estimation by comparison with the state of the arts. Especially for the high noise situation, the DDGPnP method can achieve not only accurate pose but also a large number of correct correspondences.},
  archive      = {J_CVIU},
  author       = {Zhichao Cui and Zeqi Chen and Chi Zhang and Gaofeng Meng and Yuehu Liu and Xiangmo Zhao},
  doi          = {10.1016/j.cviu.2024.104130},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104130},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DDGPnP: Differential degree graph based PnP solution to handle outliers},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An egocentric video and eye-tracking dataset for visual
search in convenience stores. <em>CVIU</em>, <em>248</em>, 104129. (<a
href="https://doi.org/10.1016/j.cviu.2024.104129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an egocentric video and eye-tracking dataset, comprised of 108 first-person videos of 36 shoppers searching for three different products (orange juice, KitKat chocolate bars, and canned tuna) in a convenience store, along with the frame-centered eye fixation locations for each video frame. The dataset also includes demographic information about each participant in the form of an 11-question survey. The paper describes two applications using the dataset — an analysis of eye fixations during search in the store, and a training of a clustered saliency model for predicting saliency of viewers engaged in product search in the store. The fixation analysis shows that fixation duration statistics are very similar to those found in image and video viewing, suggesting that similar visual processing is employed during search in 3D environments and during viewing of imagery on computer screens. A clustering technique was applied to the questionnaire data, which resulted in two clusters being detected. Based on these clusters, personalized saliency prediction models were trained on the store fixation data, which provided improved performance in prediction saliency on the store video data compared to state-of-the art universal saliency prediction methods.},
  archive      = {J_CVIU},
  author       = {Yinan Wang and Sansitha Panchadsaram and Rezvan Sherkati and James J. Clark},
  doi          = {10.1016/j.cviu.2024.104129},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104129},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An egocentric video and eye-tracking dataset for visual search in convenience stores},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep video compression based on long-range temporal context
learning. <em>CVIU</em>, <em>248</em>, 104127. (<a
href="https://doi.org/10.1016/j.cviu.2024.104127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video compression allows for efficient storage and transmission of data, benefiting imaging and vision applications, e.g. computational imaging, photography, and displays by delivering high-quality videos. To exploit more informative contexts of video, we propose DVCL, a novel D eep V ideo C ompression based on L ong-range Temporal Context Learning. Aiming at high coding performance, this new compression paradigm makes full use of long-range temporal correlations derived from multiple reference frames to learn richer contexts. Motion vectors (MVs) are estimated to represent the motion relations of videos. By employing MVs, a long-range temporal context learning (LTCL) module is presented to extract context information from multiple reference frames, such that a more accurate and informative temporal contexts can be learned and constructed. The long-range temporal contexts serve as conditions and generate the predicted frames by contextual encoder and decoder. To address the challenge of imbalanced training, we develop a multi-stage training strategy to ensure the whole DVCL framework is trained progressively and stably. Extensive experiments demonstrate the proposed DVCL achieves the highest objective and subjective quality, while maintaining relatively low complexity. Specifically, 25.30% and 45.75% bitrate savings on average can be obtained than x265 codec at the same PSNR and MS-SSIM, respectively.},
  archive      = {J_CVIU},
  author       = {Kejun Wu and Zhenxing Li and You Yang and Qiong Liu},
  doi          = {10.1016/j.cviu.2024.104127},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104127},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep video compression based on long-range temporal context learning},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved high dynamic range imaging using multi-scale
feature flows balanced between task-orientedness and accuracy.
<em>CVIU</em>, <em>248</em>, 104126. (<a
href="https://doi.org/10.1016/j.cviu.2024.104126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has made it possible to accurately generate high dynamic range (HDR) images from multiple images taken at different exposure settings, largely owing to advancements in neural network design. However, generating images without artifacts remains difficult, especially in scenes with moving objects. In such cases, issues like color distortion, geometric misalignment, or ghosting can appear. Current state-of-the-art network designs address this by estimating the optical flow between input images to align them better. The parameters for the flow estimation are learned through the primary goal, producing high-quality HDR images. However, we find that this ”task-oriented flow” approach has its drawbacks, especially in minimizing artifacts. To address this, we introduce a new network design and training method that improve the accuracy of flow estimation. This aims to strike a balance between task-oriented flow and accurate flow. Additionally, the network utilizes multi-scale features extracted from the input images for both flow estimation and HDR image reconstruction. Our experiments demonstrate that these two innovations result in HDR images with fewer artifacts and enhanced quality.},
  archive      = {J_CVIU},
  author       = {Qian Ye and Masanori Suganuma and Takayuki Okatani},
  doi          = {10.1016/j.cviu.2024.104126},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104126},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improved high dynamic range imaging using multi-scale feature flows balanced between task-orientedness and accuracy},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial attention inference model for cascaded siamese
tracking with dynamic residual update strategy. <em>CVIU</em>,
<em>248</em>, 104125. (<a
href="https://doi.org/10.1016/j.cviu.2024.104125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target representation is crucial for visual tracking. Most Siamese-based trackers try their best to establish target models by using various deep networks. However, they neglect the exploration of correlation among features, which leads to the inability to learn more representative features. In this paper, we propose a spatial attention inference model for cascaded Siamese tracking with dynamic residual update strategy. First, a spatial attention inference model is constructed. The model fuses interlayer multi-scale features generated by dilation convolution to enhance the spatial representation ability of features. On this basis, we use self-attention to capture interaction between target and context, and use cross-attention to aggregate interdependencies between target and background. The model infers potential feature information by exploiting the correlations among features for building better appearance models. Second, a cascaded localization-aware network is introduced to bridge a gap between classification and regression. We propose an alignment-aware branch to resample and learn object-aware features from the predicted bounding boxes for obtaining localization confidence, which is used to correct the classification confidence by weighted integration. This cascaded strategy alleviates the misalignment problem between classification and regression. Finally, a dynamic residual update strategy is proposed. This strategy utilizes the Context Fusion Network (CFNet) to fuse the templates of historical and current frames to generate the optimal templates. Meanwhile, we use a dynamic threshold function to determine when to update by judging the tracking results. The strategy uses temporal context to fully explore the intrinsic properties of the target, which enhances the adaptability to changes in the target’s appearance. We conducted extensive experiments on seven tracking benchmarks, including OTB100, UAV123, TC128, VOT2016, VOT2018, GOT10k and LaSOT, to validate the effectiveness of our proposed algorithm.},
  archive      = {J_CVIU},
  author       = {Huanlong Zhang and Mengdan Liu and Xiaohui Song and Yong Wang and Guanglu Yang and Rui Qi},
  doi          = {10.1016/j.cviu.2024.104125},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104125},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Spatial attention inference model for cascaded siamese tracking with dynamic residual update strategy},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep unsupervised shadow detection with curriculum learning
and self-training. <em>CVIU</em>, <em>248</em>, 104124. (<a
href="https://doi.org/10.1016/j.cviu.2024.104124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection is undergoing a rapid and remarkable development along with the wide use of deep neural networks. Benefiting from a large number of training images annotated with strong pixel-level ground-truth masks, current deep shadow detectors have achieved state-of-the-art performance. However, it is expensive and time-consuming to provide the pixel-level ground-truth mask for each training image. Considering that, this paper proposes the first unsupervised deep shadow detection framework, which consists of an initial pseudo label generation (IPG) module, a curriculum learning (CL) module and a self-training (ST) module. The supervision signals used in our learning framework are generated from several existing traditional unsupervised shadow detectors, which usually contain a lot of noisy information. Therefore, each module in our unsupervised framework is dedicated to reduce the adverse influence of noisy information on model training. Specifically, the IPG module combines different traditional unsupervised shadow maps to obtain their complementary shadow information. After obtaining the initial pseudo labels, the CL module and the ST module will be used in conjunction to gradually learn new shadow patterns and update the qualities of pseudo labels simultaneously. Extensive experimental results on various benchmark datasets demonstrate that our deep shadow detector not only outperforms the traditional unsupervised shadow detection methods by a large margin but also achieves comparable results with some recent state-of-the-art fully-supervised deep shadow detection methods.},
  archive      = {J_CVIU},
  author       = {Qiang Zhang and Hongyuan Guo and Guanghe Li and Tianlu Zhang and Qiang Jiao},
  doi          = {10.1016/j.cviu.2024.104124},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104124},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep unsupervised shadow detection with curriculum learning and self-training},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for detecting fighting behavior based on key
points of human skeletal posture. <em>CVIU</em>, <em>248</em>, 104123.
(<a href="https://doi.org/10.1016/j.cviu.2024.104123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting fights from videos and images in public surveillance places is an important task to limit violent criminal behavior. Real-time detection of violent behavior can effectively ensure the personal safety of pedestrians and further maintain public social stability. Therefore, in this paper, we aim to detect real-time violent behavior in videos. We propose a novel neural network model framework based on human pose key points, called Real-Time Pose Net (RTPNet). Utilize the pose extractor (YOLO-Pose) to extract human skeleton features, and classify video level violent behavior based on the 2DCNN model (ACTION-Net). Utilize appearance features and inter frame correlation to accurately detect fighting behavior. We have also proposed a new image dataset called VIMD (Violence Image Dataset), which includes images of fighting behavior collected online and captured independently. After training on the dataset, the network can effectively identify skeletal features from videos and locate fighting movements. The dataset is available on GitHub ( https://github.com/ChinaZhangPeng/Violence-Image-Dataset ). We also conducted experiments on four datasets, including Hockey-Fight, RWF-2000, Surveillance Camera Fight, and AVD dataset. These experimental results showed that RTPNet outperformed the most advanced methods in the past, achieving an accuracy of 99.4% on the Hockey-Fight dataset, 93.3% on the RWF-2000 dataset, and 93.4% on the Surveillance Camera Fight dataset, 99.3% on the AVD dataset. And with speeds capable of reaching 33fps, state-of-the-art results are achieved with faster speeds. In addition, RTPNet can also have good detection performance in violent behavior in complex backgrounds.},
  archive      = {J_CVIU},
  author       = {Peng Zhang and Xinlei Zhao and Lijia Dong and Weimin Lei and Wei Zhang and Zhaonan Lin},
  doi          = {10.1016/j.cviu.2024.104123},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104123},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A framework for detecting fighting behavior based on key points of human skeletal posture},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual cross-enhancement network for highly accurate
dichotomous image segmentation. <em>CVIU</em>, <em>248</em>, 104122. (<a
href="https://doi.org/10.1016/j.cviu.2024.104122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing image segmentation tasks mainly focus on segmenting objects with specific characteristics, such as salient, camouflaged, and meticulous objects, etc. However, the research of highly accurate Dichotomous Image Segmentation (DIS) combining these tasks has just started and still faces problems such as insufficient information interaction between layers and incomplete integration of high-level semantic information and low-level detailed features. In this paper, a new dual cross-enhancement network (DCENet) for highly accurate DIS is proposed, which mainly consists of two new modules: a cross-scaling guidance (CSG) module and a semantic cross-transplantation (SCT) module. Specifically, the CSG module adopts the adjacent-layer cross-scaling guidance method, which can efficiently interact with the multi-scale features of the adjacent layers extracted; the SCT module uses dual-branch features to complement each other. Moreover, in the way of transplantation, the high-level semantic information of the low-resolution branch is used to guide the low-level detail features of the high-resolution branch, and the features of different resolution branches are effectively fused. Finally, experimental results on the challenging DIS5K benchmark dataset show that the proposed network outperforms the 9 state-of-the-art (SOTA) networks in 5 widely used evaluation metrics. In addition, the ablation experiments also demonstrate the effectiveness of the cross-scaling guidance module and the semantic cross-transplantation module.},
  archive      = {J_CVIU},
  author       = {Hongbo Bi and Yuyu Tong and Pan Zhang and Jiayuan Zhang and Cong Zhang},
  doi          = {10.1016/j.cviu.2024.104122},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104122},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dual cross-enhancement network for highly accurate dichotomous image segmentation},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning model for simultaneous recognition of
quantitative and qualitative emotion using visual and bio-sensing data.
<em>CVIU</em>, <em>248</em>, 104121. (<a
href="https://doi.org/10.1016/j.cviu.2024.104121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of emotions heavily relies on important factors such as human facial expressions and physiological signals, including electroencephalogram and electrocardiogram. In literature, emotion recognition is investigated quantitatively (while estimating valance, arousal, and dominance) and qualitatively (while predicting discrete emotions like happiness, sadness, anger, surprise, and so on). Current methods utilize a combination of visual data and bio-sensing information to create recognition systems that incorporate multiple modes (quantitative/qualitative). Nevertheless, these methods necessitate extensive expertise in specific domains and intricate preprocessing procedures, and consequently, they are unable to fully leverage the inherent advantages of end-to-end deep learning techniques. Moreover, methods usually aim to recognize either qualitative or quantitative emotions. Although both kinds of emotions are significantly co-related, previous methods do not simultaneously recognize qualitative and quantitative emotions. In this paper, a novel deep end-to-end framework named DeepVADNet is introduced, specifically designed for the purpose of multi-modal emotion recognition. The proposed framework leverages deep learning techniques to effectively extract crucial face appearance features as well as bio-sensing features, predicting both qualitative and quantitative emotions in a single forward pass. In this study, we employ the CRNN architecture to extract face appearance features, while the ConvLSTM model is utilized to extract spatio-temporal information from visual data (videos). Additionally, we utilize the Conv1D model for processing physiological signals (EEG, EOG, ECG, and GSR) as this approach deviates from conventional manual techniques that involve traditional manual methods for extracting features based on time and frequency domains. After enhancing the feature quality by fusing both modalities, we use a novel method employing quantitative emotion to predict qualitative emotions accurately. We perform extensive experiments on the DEAP and MAHNOB-HCI datasets, achieving state-of-the-art quantitative emotion recognition results of 98.93%/6e-4 and 89.08%/0.97 (mean classification accuracy/MSE) in both datasets, respectively. Also, for the qualitative emotion recognition task, we achieve 82.71% mean classification accuracy on the MAHNOB-HCI dataset. The code and evaluation can be accessed at: https://github.com/I-Man-H/DeepVADNet.git},
  archive      = {J_CVIU},
  author       = {Iman Hosseini and Md Zakir Hossain and Yuhao Zhang and Shafin Rahman},
  doi          = {10.1016/j.cviu.2024.104121},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104121},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep learning model for simultaneous recognition of quantitative and qualitative emotion using visual and bio-sensing data},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coarse-to-fine mechanisms mitigate diffusion limitations on
image restoration. <em>CVIU</em>, <em>248</em>, 104118. (<a
href="https://doi.org/10.1016/j.cviu.2024.104118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the remarkable performance of diffusion models in various vision tasks. However, for image restoration that aims to recover clear images with sharper details from given degraded observations, diffusion-based methods may fail to recover promising results due to inaccurate noise estimation. Moreover, simple constraining noises cannot effectively learn complex degradation information, which subsequently hinders the model capacity. To solve the above problems, we propose a coarse-to-fine diffusion Transformer (C2F-DFT) to mitigate diffusion limitations mentioned before on image restoration. Specifically, the proposed C2F-DFT contains diffusion self-attention (DFSA) and diffusion feed-forward network (DFN) within a new coarse-to-fine training mechanism. The DFSA and DFN with embedded diffusion steps respectively capture the long-range diffusion dependencies and learn hierarchy diffusion representation to guide the restoration process in different time steps. In the coarse training stage, our C2F-DFT estimates noises and then generates the final clean image by a sampling algorithm. To further improve the restoration quality, we propose a simple yet effective fine training pipeline. It first exploits the coarse-trained diffusion model with fixed steps to generate restoration results, which then would be constrained with corresponding ground-truth ones to optimize the models to remedy the unsatisfactory results affected by inaccurate noise estimation. Extensive experiments show that C2F-DFT significantly outperforms diffusion-based restoration method IR-SDE and achieves competitive performance compared with Transformer-based state-of-the-art methods on 3 tasks, including image deraining, image deblurring, and real image denoising. The source codes and visual results are available at https://github.com/wlydlut/C2F-DFT .},
  archive      = {J_CVIU},
  author       = {Liyan Wang and Qinyu Yang and Cong Wang and Wei Wang and Zhixun Su},
  doi          = {10.1016/j.cviu.2024.104118},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104118},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Coarse-to-fine mechanisms mitigate diffusion limitations on image restoration},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MKP-net: Memory knowledge propagation network for
point-supervised temporal action localization in livestreaming.
<em>CVIU</em>, <em>248</em>, 104109. (<a
href="https://doi.org/10.1016/j.cviu.2024.104109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standardized regulation of livestreaming is an important element of cyberspace governance. Temporal action localization (TAL) can localize the occurrence of specific actions to better understand human activities. Due to the short duration and inconspicuous boundaries of human-specific actions, it is very cumbersome to obtain sufficient labeled data for training in untrimmed livestreaming. The point-supervised approach requires only a single-frame annotation for each action instance and can effectively balance cost and performance. Therefore, we propose a memory knowledge propagation network (MKP-Net) for point-supervised temporal action localization in livestreaming, including (1) a plug-and-play memory module is introduced to model prototype features of foreground actions and background knowledge using point-level annotations, (2) the memory knowledge propagation mechanism is used to generate discriminative feature representation in a multi-instance learning pipeline, and (3) localization completeness learning is performed by designing a dual optimization loss for refining and localizing temporal actions. Experimental results show that our method achieves 61.4% and 49.1% SOTAs on THUMOS14 and self-built BJUT-PTAL datasets, respectively, with an inference speed of 711 FPS.},
  archive      = {J_CVIU},
  author       = {Lin Chen and Jing Zhang and Yian Zhang and Junpeng Kang and Li Zhuo},
  doi          = {10.1016/j.cviu.2024.104109},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104109},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MKP-net: Memory knowledge propagation network for point-supervised temporal action localization in livestreaming},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deconfounded hierarchical multi-granularity classification.
<em>CVIU</em>, <em>248</em>, 104108. (<a
href="https://doi.org/10.1016/j.cviu.2024.104108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical multi-granularity classification (HMC) assigns labels at varying levels of detail to images using a structured hierarchy that categorizes labels from coarse to fine, such as [“Suliformes”, “Fregatidae”, “Frigatebird”]. Traditional HMC methods typically integrate hierarchical label information into either the model’s architecture or its loss function. However, these approaches often overlook the spurious correlations between coarse-level semantic information and fine-grained labels, which can lead models to rely on these non-causal relationships for making predictions. In this paper, we adopt a causal perspective to address the challenges in HMC, demonstrating how coarse-grained semantics can serve as confounders in fine-grained classification. To comprehensively mitigate confounding bias in HMC, we introduce a novel framework, Deconf-HMC, which consists of three main components: (1) a causal-inspired label prediction module that combines fine-level features with coarse-level prediction outcomes to determine the appropriate labels at each hierarchical level; (2) a representation disentanglement module that minimizes the mutual information between representations of different granularities; and (3) an adversarial training module that restricts the predictive influence of coarse-level representations on fine-level labels, thereby aiming to eliminate confounding bias. Extensive experiments on three widely used datasets demonstrate the superiority of our approach over existing state-of-the-art HMC methods.},
  archive      = {J_CVIU},
  author       = {Ziyu Zhao and Leilei Gan and Tao Shen and Kun Kuang and Fei Wu},
  doi          = {10.1016/j.cviu.2024.104108},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104108},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deconfounded hierarchical multi-granularity classification},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end pedestrian trajectory prediction via efficient
multi-modal predictors. <em>CVIU</em>, <em>248</em>, 104107. (<a
href="https://doi.org/10.1016/j.cviu.2024.104107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory prediction plays a key role in understanding human behavior and guiding autonomous driving. It is a difficult task due to the multi-modal nature of human motion. Recent advances have mainly focused on modeling this multi-modality, either by using implicit generative models or explicit pre-defined anchors. However, the former is limited by the sampling problem, while the latter introduces strong prior to the data, both of which require extra tricks to achieve better performance. To address these issues, we propose a simple yet effective framework called Efficient Multi-modal Predictors (EMP), which casts off the generative paradigm and predicts multi-modal trajectories in an end-to-end style. It is achieved by combining a set of parallel predictors with a model error based sparse selector. During training, the entire set of parallel multi-modal predictors will converge into disjoint subsets, with each subset specializing in one mode, thus obtaining multi-modal prediction with no human prior and reducing the problems of above two genres. Experiments on SDD/ETH-UCY/NBA datasets show that EMP achieves state-of-the-art performance with the highest inference speed. Additionally, we show that by replacing multi-modal modules with EMP, state-of-the-art works outperform their baselines, which further validate the versatility of EMP. Moreover, we formally prove that EMP can alleviate the problem of modal collapse and has a low test error bound.},
  archive      = {J_CVIU},
  author       = {Qi Wu and Sanping Zhou and Le Wang and Liushuai Shi and Yonghao Dong and Gang Hua},
  doi          = {10.1016/j.cviu.2024.104107},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104107},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {End-to-end pedestrian trajectory prediction via efficient multi-modal predictors},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DHS-DETR: Efficient DETRs with dynamic head switching.
<em>CVIU</em>, <em>248</em>, 104106. (<a
href="https://doi.org/10.1016/j.cviu.2024.104106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection Transformer (DETR) and its variants have emerged a new paradigm to object detection, but their high computational cost hinders practical applications. By investigating their essential components, we found that the transformer-based head usually occupies a significant amount of computation. Through further comparing heavy and light transformer heads, we observed that both heads produced satisfactory results for easy images while showing a noticeable difference for hard images. Inspired by these findings, we propose a dynamic head switching (DHS) strategy to dynamically select the proper head for each image at inference for a better balance of efficiency and accuracy. Specifically, our DETR model incorporates multiple heads with different computational complexity and a lightweight module which selects proper heads for given images. This module is optimized to maximize detection accuracy while adhering to the overall computational budget limitations. To minimize the potential accuracy drop when executing the lighter heads, we propose online head distillation (OHD) to improve the accuracy of the lighter heads with the help of the heavier head. Extensive experiments on the MS COCO dataset validated the effectiveness of the proposed method, which demonstrated a better accuracy–efficiency trade-off compared to the baseline using static heads.},
  archive      = {J_CVIU},
  author       = {Hang Chen and Chufeng Tang and Xiaolin Hu},
  doi          = {10.1016/j.cviu.2024.104106},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104106},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DHS-DETR: Efficient DETRs with dynamic head switching},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint pyramidal perceptual attention and hierarchical
consistency constraint for gaze estimation. <em>CVIU</em>, <em>248</em>,
104105. (<a href="https://doi.org/10.1016/j.cviu.2024.104105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eye gaze provides valuable cues about human intent, making gaze estimation a hot topic. Extracting multi-scale information has recently proven effective for gaze estimation in complex scenarios. However, existing methods for estimating gaze based on multi-scale features tend to focus only on information from single-level feature maps. Furthermore, information across different scales may also lack relevance. To address these issues, we propose a novel joint pyramidal perceptual attention and hierarchical consistency constraint (PaCo) for gaze estimation. The proposed PaCo consists of two main components: pyramidal perceptual attention module (PPAM) and hierarchical consistency constraint (HCC). Specifically, PPAM first extracts multi-scale spatial features using a pyramid structure, and then aggregates information from coarse granularity to fine granularity. In this way, PPAM enables the model to simultaneously focus on both the eye region and facial region at multiple scales. Then, HCC makes constrains consistency on low-level and high-level features, aiming to enhance the gaze semantic consistency between different feature levels. With the combination of PPAM and HCC, PaCo can learn more discriminative features in complex situations. Extensive experimental results show that PaCo achieves significant performance improvements on challenging datasets such as Gaze360, MPIIFaceGaze, and RT-GENE,reducing errors to 10.27 ° ° , 3.23 ° ° , 6.46 ° ° , respectively.},
  archive      = {J_CVIU},
  author       = {Haiying Xia and Zhuolin Gong and Yumei Tan and Shuxiang Song},
  doi          = {10.1016/j.cviu.2024.104105},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104105},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint pyramidal perceptual attention and hierarchical consistency constraint for gaze estimation},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFCT: Multi-frequency cascade transformers for no-reference
SR-IQA. <em>CVIU</em>, <em>248</em>, 104104. (<a
href="https://doi.org/10.1016/j.cviu.2024.104104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution image reconstruction techniques have advanced quickly, leading to the generation of a sizable number of super-resolution images using different super-resolution techniques. Nevertheless, accurately assessing the quality of super-resolution images remains a formidable challenge. This paper introduces a novel Multi-Frequency Cascade Transformers (MFCT) for evaluating super-resolution image quality (SR-IQA). In the first step, we develop a unique Frequency-Divided Module (FDM) to transform the super-resolution images into three different frequency bands. Subsequently, the Cascade Transformer Blocks (CAF) incorporating hierarchical self-attention mechanisms are employed to capture cross-window features for quality perception. Ultimately, the image quality scores from different frequency bands are fused to derive the overall image quality score. The experimental results show that, on the chosen SR-IQA databases, the proposed MFCT-based SR-IQA method can consistently outperforms all the compared Image Quality Assessment (IQA) models. Furthermore, a collection of thorough ablation studies demonstrates that, when compared to other earlier rivals, the newly proposed approach exhibits impressive generalization ability. The code will be available at https://github.com/kbzhang0505/MFCT .},
  archive      = {J_CVIU},
  author       = {Dandan Fan and Kaibing Zhang and Hui Li and Longgang Ren and Guang Shi},
  doi          = {10.1016/j.cviu.2024.104104},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104104},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MFCT: Multi-frequency cascade transformers for no-reference SR-IQA},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascaded UNet for progressive noise residual prediction for
structure-preserving video denoising. <em>CVIU</em>, <em>248</em>,
104103. (<a href="https://doi.org/10.1016/j.cviu.2024.104103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prominence of high-quality video services has become so substantial that by 2030, it is estimated that approximately 80% of internet traffic will consist of videos. On the contrary, video denoising remains a relatively unexplored and intricate field, presenting more substantial challenges compared to image denoising. Many published deep learning video denoising algorithms typically rely on simple, efficient single encoder–decoder networks, but they have inherent limitations in preserving intricate image details and effectively managing noise information propagation for noise residue modelling. In response to these challenges, the proposed work introduces an innovative approach; in terms of utilization of cascaded UNets for progressive noise residual prediction in video denoising. This multi-stage encoder–decoder architecture is meticulously designed to accurately predict noise residual maps, thereby preserving the locally fine details within video content as represented by SSIM. The proposed network has undergone extensive end-to-end training from scratch without explicit motion compensation to reduce complexity. In terms of the more rigorous SSIM metric, the proposed network outperformed all video denoising methods while maintaining a comparable PSNR.},
  archive      = {J_CVIU},
  author       = {Abhijeet Pimpale and Kishor Bhurchandi},
  doi          = {10.1016/j.cviu.2024.104103},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104103},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cascaded UNet for progressive noise residual prediction for structure-preserving video denoising},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image semantic segmentation of indoor scenes: A survey.
<em>CVIU</em>, <em>248</em>, 104102. (<a
href="https://doi.org/10.1016/j.cviu.2024.104102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey provides a comprehensive evaluation of various deep learning-based segmentation architectures. It covers a wide range of models, from traditional ones like FCN and PSPNet to more modern approaches like SegFormer and FAN. In addition to assessing the methods in terms of segmentation accuracy, we propose to also evaluate the methods in terms of temporal consistency and corruption vulnerability. Most of the existing surveys on semantic segmentation focus on outdoor datasets. In contrast, this survey focuses on indoor scenarios to enhance the applicability of segmentation methods in this specific domain. Furthermore, our evaluation consists of a performance analysis of the methods in prevalent real-world segmentation scenarios that pose particular challenges. These complex situations involve scenes impacted by diverse forms of noise, blur corruptions, camera movements, optical aberrations, among other factors. By jointly exploring the segmentation accuracy, temporal consistency, and corruption vulnerability in challenging real-world situations, our survey offers insights that go beyond existing surveys, facilitating the understanding and development of better image segmentation methods for indoor scenes.},
  archive      = {J_CVIU},
  author       = {Ronny Velastegui and Maxim Tatarchenko and Sezer Karaoglu and Theo Gevers},
  doi          = {10.1016/j.cviu.2024.104102},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104102},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image semantic segmentation of indoor scenes: A survey},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opti-CAM: Optimizing saliency maps for interpretability.
<em>CVIU</em>, <em>248</em>, 104101. (<a
href="https://doi.org/10.1016/j.cviu.2024.104101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods based on class activation maps (CAM) provide a simple mechanism to interpret predictions of convolutional neural networks by using linear combinations of feature maps as saliency maps. By contrast, masking-based methods optimize a saliency map directly in the image space or learn it by training another network on additional data. In this work we introduce Opti-CAM, combining ideas from CAM-based and masking-based approaches. Our saliency map is a linear combination of feature maps, where weights are optimized per image such that the logit of the masked image for a given class is maximized. We also fix a fundamental flaw in two of the most common evaluation metrics of attribution methods. On several datasets, Opti-CAM largely outperforms other CAM-based approaches according to the most relevant classification metrics. We provide empirical evidence supporting that localization and classifier interpretability are not necessarily aligned.},
  archive      = {J_CVIU},
  author       = {Hanwei Zhang and Felipe Torres and Ronan Sicre and Yannis Avrithis and Stephane Ayache},
  doi          = {10.1016/j.cviu.2024.104101},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104101},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Opti-CAM: Optimizing saliency maps for interpretability},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bidirectional brain image translation using transfer
learning from generic pre-trained models. <em>CVIU</em>, <em>248</em>,
104100. (<a href="https://doi.org/10.1016/j.cviu.2024.104100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain imaging plays a crucial role in the diagnosis and treatment of various neurological disorders, providing valuable insights into the structure and function of the brain. Techniques such as magnetic resonance imaging (MRI) and computed tomography (CT) enable non-invasive visualization of the brain, aiding in the understanding of brain anatomy, abnormalities, and functional connectivity. However, cost and radiation dose may limit the acquisition of specific image modalities, so medical image synthesis can be used to generate required medical images without actual addition. CycleGAN and other GANs are valuable tools for generating synthetic images across various fields. In the medical domain, where obtaining labeled medical images is labor-intensive and expensive, addressing data scarcity is a major challenge. Recent studies propose using transfer learning to overcome this issue. This involves adapting pre-trained CycleGAN models, initially trained on non-medical data, to generate realistic medical images. In this work, transfer learning was applied to the task of MR-CT image translation and vice versa using 18 pre-trained non-medical models, and the models were fine-tuned to have the best result. The models’ performance was evaluated using four widely used image quality metrics: Peak-signal-to-noise-ratio, Structural Similarity Index, Universal Quality Index, and Visual Information Fidelity. Quantitative evaluation and qualitative perceptual analysis by radiologists demonstrate the potential of transfer learning in medical imaging and the effectiveness of the generic pre-trained model. The results provide compelling evidence of the model’s exceptional performance, which can be attributed to the high quality and similarity of the training images to actual human brain images. These results underscore the significance of carefully selecting appropriate and representative training images to optimize performance in brain image analysis tasks.},
  archive      = {J_CVIU},
  author       = {Fatima Haimour and Rizik Al-Sayyed and Waleed Mahafza and Omar S. Al-Kadi},
  doi          = {10.1016/j.cviu.2024.104100},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104100},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Bidirectional brain image translation using transfer learning from generic pre-trained models},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Invisible gas detection: An RGB-thermal cross attention
network and a new benchmark. <em>CVIU</em>, <em>248</em>, 104099. (<a
href="https://doi.org/10.1016/j.cviu.2024.104099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of various chemical gases in industrial processes necessitates effective measures to prevent their leakage during transportation and storage, given their high toxicity. Thermal infrared-based computer vision detection techniques provide a straightforward approach to identify gas leakage areas. However, the development of high-quality algorithms has been challenging due to the low texture in thermal images and the lack of open-source datasets. In this paper, we present the R GB- T hermal C ross A ttention N etwork (RT-CAN), which employs an RGB-assisted two-stream network architecture to integrate texture information from RGB images and gas area information from thermal images. Additionally, to facilitate the research of invisible gas detection, we introduce Gas-DB, an extensive open-source gas detection database including about 1.3K well-annotated RGB-thermal images with eight variant collection scenes. Experimental results demonstrate that our method successfully leverages the advantages of both modalities, achieving state-of-the-art (SOTA) performance among RGB-thermal methods, surpassing single-stream SOTA models in terms of accuracy, Intersection of Union (IoU), and F2 metrics by 4.86%, 5.65%, and 4.88%, respectively. The code and data can be found at https://github.com/logic112358/RT-CAN .},
  archive      = {J_CVIU},
  author       = {Jue Wang and Yuxiang Lin and Qi Zhao and Dong Luo and Shuaibao Chen and Wei Chen and Xiaojiang Peng},
  doi          = {10.1016/j.cviu.2024.104099},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104099},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Invisible gas detection: An RGB-thermal cross attention network and a new benchmark},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). View-aligned pixel-level feature aggregation for 3D shape
classification. <em>CVIU</em>, <em>248</em>, 104098. (<a
href="https://doi.org/10.1016/j.cviu.2024.104098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view 3D shape classification, which identifies a 3D shape based on its 2D views rendered from different viewpoints, has emerged as a promising method of shape understanding. A key building block in these methods is cross-view feature aggregation. However, existing methods dominantly follow the “extract-then-aggregate” pipeline for view-level global feature aggregation, leaving cross-view pixel-level feature interaction under-explored. To tackle this issue, we develop a “fuse-while-extract” pipeline, with a novel View-aligned Pixel-level Fusion (VPF) module to fuse cross-view pixel-level features originating from the same 3D part. We first reconstruct the 3D coordinate of each feature via the rasterization results, then match and fuse the features via spatial neighbor searching. Incorporating the proposed VPF module with ResNet18 backbone, we build a novel view-aligned multi-view network, which conducts feature extraction and cross-view fusion alternatively. Extensive experiments have demonstrated the effectiveness of the VPF module as well as the excellent performance of the proposed network.},
  archive      = {J_CVIU},
  author       = {Yong Xu and Shaohui Pan and Ruotao Xu and Haibin Ling},
  doi          = {10.1016/j.cviu.2024.104098},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104098},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {View-aligned pixel-level feature aggregation for 3D shape classification},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UC-former: A multi-scale image deraining network using
enhanced transformer. <em>CVIU</em>, <em>248</em>, 104097. (<a
href="https://doi.org/10.1016/j.cviu.2024.104097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While convolutional neural networks (CNN) have achieved remarkable performance in single image deraining tasks, it is still a very challenging task due to CNN’s limited receptive field and the unreality of the output image. In this paper, UC-former, an effective and efficient U-shaped architecture based on transformer for image deraining was presented. In UC-former, there are two core designs to avoid heavy self-attention computation and inefficient communications across encoder and decoder. First, we propose a novel channel across Transformer block, which computes self-attention between channels. It significantly reduces the computational complexity of high-resolution rain maps while capturing global context. Second, we propose a multi-scale feature fusion module between the encoder and decoder to combine low-level local features and high-level non-local features. In addition, we employ depth-wise convolution and H-Swish non-linear activation function in Transformer Blocks to enhance rain removal authenticity. Extensive experiments indicate that our method outperforms the state-of-the-art deraining approaches on synthetic and real-world rainy datasets.},
  archive      = {J_CVIU},
  author       = {Weina Zhou and Linhui Ye},
  doi          = {10.1016/j.cviu.2024.104097},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104097},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UC-former: A multi-scale image deraining network using enhanced transformer},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-dimensional attention-aided transposed ConvBiLSTM
network for hyperspectral image super-resolution. <em>CVIU</em>,
<em>248</em>, 104096. (<a
href="https://doi.org/10.1016/j.cviu.2024.104096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral (HS) image always suffers from the deficiency of low spatial resolution, compared with conventional optical image types, which has limited its further applications in remote sensing areas. Therefore, HS image super-resolution (SR) techniques are broadly employed in order to observe finer spatial structures while preserving the spectra of ground covers. In this paper, a novel multi-dimensional attention-aided transposed convolutional long-short term memory (LSTM) network is proposed for single HS image super-resolution task. The proposed network employs the convolutional bi-directional LSTM for the purpose of local and non-local spatial–spectral feature explorations, and transposed convolution for the purpose of image amplification and reconstruction. Moreover, a multi-dimensional attention module is proposed, aiming to capture the salient features on spectral, channel, and spatial dimensions, simultaneously, to further improve the learning abilities of network. Experiments on four commonly-used HS images demonstrate the effectiveness of this approach, compared with several state-of-the-art deep learning-based SR methods.},
  archive      = {J_CVIU},
  author       = {Xiaochen Lu and Yuting Pan and Yuan Liu and Lei Zhang and Yajun Li},
  doi          = {10.1016/j.cviu.2024.104096},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104096},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-dimensional attention-aided transposed ConvBiLSTM network for hyperspectral image super-resolution},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Challenges and solutions for vision-based hand gesture
interpretation: A review. <em>CVIU</em>, <em>248</em>, 104095. (<a
href="https://doi.org/10.1016/j.cviu.2024.104095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture is one of the most efficient and natural interfaces in current human–computer interaction (HCI) systems. Despite the great progress achieved in hand gesture-based HCI, perceiving or tracking the hand pose from images remains challenging. In the past decade, several challenges have been indicated and explored, such as incomplete data issue, the requirement of large-scale annotated dataset, and 3D hand pose estimation from monocular RGB image; however, there is a lack of surveys to provide comprehensive collection and analysis for these challenges and corresponding solutions. To this end, this paper devotes effort to the general challenges of hand gesture interpretation techniques in HCI systems based on visual sensors and elaborates on the corresponding solutions in current state-of-the-arts, which can provide a systematic reminder for practical problems of hand gesture interpretation. Moreover, this paper provides informative cues for recent datasets to further point out the inherent differences and connections among them, such as the annotation of objects and the number of hands, which is important for conducting research yet ignored by previous reviews. In retrospect of recent developments, this paper also conjectures what the future work will concentrate on, from the perspectives of both hand gesture interpretation and dataset construction.},
  archive      = {J_CVIU},
  author       = {Kun Gao and Haoyang Zhang and Xiaolong Liu and Xinyi Wang and Liang Xie and Bowen Ji and Ye Yan and Erwei Yin},
  doi          = {10.1016/j.cviu.2024.104095},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104095},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Challenges and solutions for vision-based hand gesture interpretation: A review},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural image re-exposure. <em>CVIU</em>, <em>248</em>,
104094. (<a href="https://doi.org/10.1016/j.cviu.2024.104094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images and videos often suffer from issues such as motion blur, video discontinuity, or rolling shutter artifacts. Prior studies typically focus on designing specific algorithms to address individual issues. In this paper, we highlight that these issues, albeit differently manifested, fundamentally stem from sub-optimal exposure processes. With this insight, we propose a paradigm termed re-exposure, which resolves the aforementioned issues by performing exposure simulation. Following this paradigm, we design a new architecture, which constructs visual content representation from images and event camera data, and performs exposure simulation in a controllable manner. Experiments demonstrate that, using only a single model, the proposed architecture can effectively address multiple visual issues, including motion blur, video discontinuity, and rolling shutter artifacts, even when these issues co-occur.},
  archive      = {J_CVIU},
  author       = {Xinyu Zhang and Hefei Huang and Xu Jia and Dong Wang and Lihe Zhang and Bolun Zheng and Wei Zhou and Huchuan Lu},
  doi          = {10.1016/j.cviu.2024.104094},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104094},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Neural image re-exposure},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uni MS-PS: A multi-scale encoder-decoder transformer for
universal photometric stereo. <em>CVIU</em>, <em>248</em>, 104093. (<a
href="https://doi.org/10.1016/j.cviu.2024.104093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photometric Stereo (PS) addresses the challenge of reconstructing a three-dimensional (3D) representation of an object by estimating the 3D normals at all points on the object’s surface. This is achieved through the analysis of at least three photographs, all taken from the same viewpoint but with distinct lighting conditions. This paper introduces a novel approach for Universal PS, i.e., when both the active lighting conditions and the ambient illumination are unknown. Our method employs a multi-scale encoder–decoder architecture based on Transformers that allows to accommodates images of any resolutions as well as varying number of input images. We are able to scale up to very high resolution images like 6000 pixels by 8000 pixels without losing performance and maintaining a decent memory footprint. Moreover, experiments on publicly available datasets establish that our proposed architecture improves the accuracy of the estimated normal field by a significant factor compared to state-of-the-art methods. Code and dataset available at: https://clement-hardy.github.io/Uni-MS-PS/index.html .},
  archive      = {J_CVIU},
  author       = {Clément Hardy and Yvain Quéau and David Tschumperlé},
  doi          = {10.1016/j.cviu.2024.104093},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104093},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Uni MS-PS: A multi-scale encoder-decoder transformer for universal photometric stereo},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LLAFN-generator: Learnable linear-attention with
fast-normalization for large-scale image captioning. <em>CVIU</em>,
<em>248</em>, 104088. (<a
href="https://doi.org/10.1016/j.cviu.2024.104088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, although Transformer has widespread application in the field of computer vision, the quadratic complexity of its Self-Attention hindered the processing in large-scale image captioning task. Therefore, in this paper, we propose a Learnable Linear-Attention with Fast-Normalization for Large-Scale Image Captioning (dubbed as LLAFN-Generator). Firstly, it introduces a Learnable Linear-Attention (LLA) module to solve the weight score learning of large-scale images, which is simply implemented through two linear layers and greatly reduces the computation complexity. Meanwhile, the Fast-Normalization (FN) method is employed in the Learnable Linear-Attention instead of the original Softmax function to improve the computational speed. Additionally, the feature enhancement module be used to compensate for the shallow, fine-grained information in order to enhance the feature representation of the model. Finally, extensive experiments on the MS COCO dataset show that the computational complexity is reduced by 30% and the parameter is reduced by 20% on models of the same size, with the performance metrics BLEU_1 and CIDEr increasing by 1.2% and 3.6%, respectively.},
  archive      = {J_CVIU},
  author       = {Xiaobao Yang and Xi Tian and Junsheng Wu and Xiaochun Yang and Sugang Ma and Xinman Qi and Zhiqiang Hou},
  doi          = {10.1016/j.cviu.2024.104088},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104088},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LLAFN-generator: Learnable linear-attention with fast-normalization for large-scale image captioning},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning depth-aware decomposition for single image
dehazing. <em>CVIU</em>, <em>248</em>, 104069. (<a
href="https://doi.org/10.1016/j.cviu.2024.104069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing under deficient data is an ill-posed and challenging problem. Most existing methods tackle this task by developing either CycleGAN-based hazy-to-clean translation or physical-based haze decomposition. However, geometric structure is often not effectively incorporated in their straightforward hazy-clean projection framework, which might incur inaccurate estimation in distant areas. In this paper, we rethink the image dehazing task and propose a depth-aware perception framework, DehazeDP , for robust haze decomposition on deficient data. Our DehazeDP is insthe pired by Diffusion Probabilistic Model to form an end-to-end training pipeline that seamlessly ines the hazy image generation with haze disentanglement. Specifically, in the forward phase, the haze is added to a clean image step-by-step according to the depth distribution. Then, in the reverse phase, a unified U-Net is used to predict the haze and recover the clean image progressively. Extensive experiments on public datasets demonstrate that the proposed DehazeDP performs favorably against state-of-the-art approaches. We release the code and models at https://github.com/stallak/DehazeDP .},
  archive      = {J_CVIU},
  author       = {Yumeng Kang and Lu Zhang and Ping Hu and Yu Liu and Huchuan Lu and You He},
  doi          = {10.1016/j.cviu.2024.104069},
  journal      = {Computer Vision and Image Understanding},
  month        = {11},
  pages        = {104069},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning depth-aware decomposition for single image dehazing},
  volume       = {248},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced local distribution learning for real image
super-resolution. <em>CVIU</em>, <em>247</em>, 104092. (<a
href="https://doi.org/10.1016/j.cviu.2024.104092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous work has shown that CNN-based local distribution learning can efficiently reconstruct high-resolution images, but with limited performance improvement against complex degraded images. In this paper, we propose an enhanced local distribution learning framework, called ELDRN, which successfully generalizes local distribution learning to realistic images whose degradation process is complex and unknowable. The cores of our ELDRN are the parallel attention block and dilated neighborhood sampling. The former mines discriminative features at both spatial and channel levels, that is, parameters for constructing local distributions, thus improving the robustness of distributions to real degradation patterns. To deal with the fact that the reference range of the target sub-pixel is not exactly equal to its neighborhood, we explicitly increase the sampling density, i.e. , fusing more sampled pixels to produce the target sub-pixel. Experiments conducted on RealSR dataset illustrate that our ELDRN outperforms recent learning-based SISR methods and reconstructs visually-pleasant high-quality images.},
  archive      = {J_CVIU},
  author       = {Yaoqi Sun and Quan Chen and Wen Xu and Aiai Huang and Chenggang Yan and Bolun Zheng},
  doi          = {10.1016/j.cviu.2024.104092},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104092},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhanced local distribution learning for real image super-resolution},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UAHOI: Uncertainty-aware robust interaction learning for HOI
detection. <em>CVIU</em>, <em>247</em>, 104091. (<a
href="https://doi.org/10.1016/j.cviu.2024.104091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on Human–Object Interaction (HOI) detection, addressing the challenge of identifying and understanding the interactions between humans and objects within a given image or video frame. Spearheaded by Detection Transformer (DETR), recent developments lead to significant improvements by replacing traditional region proposals by a set of learnable queries. However, despite the powerful representation capabilities provided by Transformers, existing Human–Object Interaction (HOI) detection methods still yield low confidence levels when dealing with complex interactions and are prone to overlooking interactive actions. To address these issues, we propose a novel approach UAHOI , Uncertainty-aware Robust Human–Object Interaction Learning that explicitly estimates prediction uncertainty during the training process to refine both detection and interaction predictions. Our model not only predicts the HOI triplets but also quantifies the uncertainty of these predictions. Specifically, we model this uncertainty through the variance of predictions and incorporate it into the optimization objective, allowing the model to adaptively adjust its confidence threshold based on prediction variance. This integration helps in mitigating the adverse effects of incorrect or ambiguous predictions that are common in traditional methods without any hand-designed components, serving as an automatic confidence threshold. Our method is flexible to existing HOI detection methods and demonstrates improved accuracy. We evaluate UAHOI on two standard benchmarks in the field: V-COCO and HICO-DET, which represent challenging scenarios for HOI detection. Through extensive experiments, we demonstrate that UAHOI achieves significant improvements over existing state-of-the-art methods, enhancing both the accuracy and robustness of HOI detection.},
  archive      = {J_CVIU},
  author       = {Mu Chen and Minghan Chen and Yi Yang},
  doi          = {10.1016/j.cviu.2024.104091},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104091},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UAHOI: Uncertainty-aware robust interaction learning for HOI detection},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subtle signals: Video-based detection of infant
non-nutritive sucking as a neurodevelopmental cue. <em>CVIU</em>,
<em>247</em>, 104081. (<a
href="https://doi.org/10.1016/j.cviu.2024.104081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-nutritive sucking (NNS), which refers to the act of sucking on a pacifier, finger, or similar object without nutrient intake, plays a crucial role in assessing healthy early development. In the case of preterm infants, NNS behavior is a key component in determining their readiness for feeding. In older infants, the characteristics of NNS behavior offer valuable insights into neural and motor development. Additionally, NNS activity has been proposed as a potential safeguard against sudden infant death syndrome (SIDS). However, the clinical application of NNS assessment is currently hindered by labor-intensive and subjective finger-in-mouth evaluations. Consequently, researchers often resort to expensive pressure transducers for objective NNS signal measurement. To enhance the accessibility and reliability of NNS signal monitoring for both clinicians and researchers, we introduce a vision-based algorithm designed for non-contact detection of NNS activity using baby monitor footage in natural settings. Our approach involves a comprehensive exploration of optical flow and temporal convolutional networks, enabling the detection and amplification of subtle infant-sucking signals. We successfully classify short video clips of uniform length into NNS and non-NNS periods. Furthermore, we investigate manual and learning-based techniques to piece together local classification results, facilitating the segmentation of longer mixed-activity videos into NNS and non-NNS segments of varying duration. Our research introduces two novel datasets of annotated infant videos, including one sourced from our clinical study featuring 18 infant subjects and 183 h of overnight baby monitor footage. Additionally, we incorporate a second, shorter dataset obtained from publicly available YouTube videos. Our NNS action recognition algorithm achieves an impressive 95.8% accuracy in binary classification, based on 960 2.5-s balanced NNS versus non-NNS clips from our clinical dataset. We also present results for a subset of clips featuring challenging video conditions. Moreover, our NNS action segmentation algorithm achieves an average precision of 93.5% and an average recall of 92.9% across 30 heterogeneous 60-s clips from our clinical dataset.},
  archive      = {J_CVIU},
  author       = {Shaotong Zhu and Michael Wan and Sai Kumar Reddy Manne and Elaheh Hatamimajoumerd and Marie J. Hayes and Emily Zimmerman and Sarah Ostadabbas},
  doi          = {10.1016/j.cviu.2024.104081},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104081},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Subtle signals: Video-based detection of infant non-nutritive sucking as a neurodevelopmental cue},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-to-image translation based face photo de-meshing using
GANs. <em>CVIU</em>, <em>247</em>, 104080. (<a
href="https://doi.org/10.1016/j.cviu.2024.104080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing face photo de-meshing methods have accomplished promising results; there are certain quality problems with these methods like the inpainted regions would appear blurry and unpleasant boundaries becoming visible. Such artifacts cause generated face photos unreal. Therefore, we propose an effective image-to-image translation framework called Face De-meshing Using Generative Adversarial Networks (De-mesh GANs). The De-mesh GANs is a two-stage model: (i) binary mask generating module, is a three convolution layers-based encoder–decoder network architecture that automatically generates a binary mask for the meshed region, and (ii) face photo de-meshing module, is a GANs-based network that eliminates the mesh mask and synthesizes the meshed area. An arrangement of careful losses (reconstruction loss, adversarial loss, and perceptual loss) is used to reassure the better quality of the de-mesh face photos. To facilitate the training of the proposed model, we have designed a dataset of clean/corrupted photo pairs using the CelebA dataset. Qualitative and quantitative evaluations of the De-mesh GANs on real-world corrupted face photo images show better performance than the previously proposed face photo de-meshing models. Furthermore, we also offer the ablation study for performance assessment of the additional network i.e., perceptual network.},
  archive      = {J_CVIU},
  author       = {Abdul Jabbar and Muhammad Assam and Muhammad Arslan and Madiha Bukhsh and Muhammad Shoib Amin and Yazeed Yasin Ghadi and Nisreen Innab and Masoud Alajmi and Mamyrbayev Orken and Salgozha Indira and Hend Khalid Alkahtan},
  doi          = {10.1016/j.cviu.2024.104080},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104080},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image-to-image translation based face photo de-meshing using GANs},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-light image enhancement based on cell vibration energy
model and lightness difference. <em>CVIU</em>, <em>247</em>, 104079. (<a
href="https://doi.org/10.1016/j.cviu.2024.104079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light image enhancement algorithms play a crucial role in revealing details obscured by darkness in images and substantially improving overall image quality. However, existing methods often suffer from issues like color or lightness distortion and possess limited scalability. In response to these challenges, we introduce a novel low-light image enhancement algorithm leveraging a cell vibration energy model and lightness difference. Initially, a new low-light image enhancement framework is proposed, building upon a comprehensive understanding and analysis of the cell vibration energy model and its statistical properties. Subsequently, to achieve pixel-level multi-lightness difference adjustment and exert control over the lightness level of each pixel independently, a lightness difference adjustment strategy is introduced utilizing Weibull distribution and linear mapping. Furthermore, to expand the adaptive range of the algorithm, we consider the disparities between HSV space and RGB space. Two enhanced image output modes are designed, accompanied by a thorough analysis and deduction of the relevant image layer mapping formulas. Finally, to enhance the reliability of experimental results, certain image faults in the SICE database are rectified using the feature matching method. Experimental results showcase the superiority of the proposed algorithm over twelve state-of-the-art algorithms. The resource code of this article will be released at https://github.com/leixiaozhou/CDEGmethod .},
  archive      = {J_CVIU},
  author       = {Xiaozhou Lei and Zixiang Fei and Wenju Zhou and Huiyu Zhou and Minrui Fei},
  doi          = {10.1016/j.cviu.2024.104079},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104079},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Low-light image enhancement based on cell vibration energy model and lightness difference},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artifact feature purification for cross-domain detection of
AI-generated images. <em>CVIU</em>, <em>247</em>, 104078. (<a
href="https://doi.org/10.1016/j.cviu.2024.104078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of AIGC, the fast development of visual content generation technologies, such as diffusion models, brings potential security risks to our society. Existing generated image detection methods suffer from performance drops when faced with out-of-domain generators and image scenes. To relieve this problem, we propose Artifact Purification Network (APN) to facilitate the artifact extraction from generated images through the explicit and implicit purification processes. For the explicit one, a suspicious frequency-band proposal method and a spatial feature decomposition method are proposed to extract artifact-related features. For the implicit one, a training strategy based on mutual information estimation is proposed to further purify the artifact-related features. The experiments are conducted in two settings. Firstly, we perform a cross-generator evaluation, wherein detectors trained using data from one generator are evaluated on data generated by other generators. Secondly, we conduct a cross-scene evaluation, wherein detectors trained for a specific domain of content (e.g., ImageNet) are assessed on data collected from another domain (e.g., LSUN-Bedroom). Results show that for cross-generator detection, the average accuracy of APN is 5 . 6 % ∼ 16 . 4 % 5.6%∼16.4% higher than the previous 11 methods on the GenImage dataset and 1 . 7 % ∼ 50 . 1 % 1.7%∼50.1% on the DiffusionForensics dataset. For cross-scene detection, APN maintains its high performance. Via visualization analysis, we find that the proposed method can extract diverse forgery patterns and condense the forgery information diluted in irrelated features. We also find that the artifact features APN focuses on across generators and scenes are global and diverse. The code will be available at https://github.com/RichardSunnyMeng/APN-official-codes .},
  archive      = {J_CVIU},
  author       = {Zheling Meng and Bo Peng and Jing Dong and Tieniu Tan and Haonan Cheng},
  doi          = {10.1016/j.cviu.2024.104078},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104078},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Artifact feature purification for cross-domain detection of AI-generated images},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing image generation with denoising diffusion
probabilistic model and ConvNeXt-v2: A novel approach for enhanced
diversity and quality. <em>CVIU</em>, <em>247</em>, 104077. (<a
href="https://doi.org/10.1016/j.cviu.2024.104077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving domain of image generation, the availability of sufficient data is crucial for effective model training. However, obtaining a large dataset is often challenging. Medical imaging, industrial monitoring, and self-driving cars are among the applications that require high-fidelity image generation from limited or single data points. The paper proposes a novel approach for increasing the diversity of images generated from a single input image by combining a Denoising Diffusion Probabilistic Model (DDPM) with the ConvNeXt-V2 architecture. This technique addresses the issue of limited data availability by utilizing single images using the BSD and Places365 datasets, significantly increasing the ability of the model through different conditions. The research greatly enhances the image quality by including Global Response Normalization (GRN) and Sigmoid-Weighted Linear Units (SiLU) in the DDPM. In-depth analyses and comparisons with the existing State-of-the-art (SOTA) models highlight the model’s effectiveness, which shows higher experimental results. Achievements include a Pixel Diversity score of 0.87±0.1, an LPIPS Diversity score of 0.42±0.03, and a SIFID for Patch Distribution of 0.046±0.02, along with notable NIQE and RECO scores. These findings indicate the exceptional ability of the model to generate a wide range of high-quality images, exhibiting significant advancement over existing State-of-the-art models in the field of image generation.},
  archive      = {J_CVIU},
  author       = {Ayushi Verma and Tapas Badal and Abhay Bansal},
  doi          = {10.1016/j.cviu.2024.104077},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104077},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Advancing image generation with denoising diffusion probabilistic model and ConvNeXt-v2: A novel approach for enhanced diversity and quality},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EnsCLR: Unsupervised skeleton-based action recognition via
ensemble contrastive learning of representation. <em>CVIU</em>,
<em>247</em>, 104076. (<a
href="https://doi.org/10.1016/j.cviu.2024.104076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is a key research area in video understanding, beneficial from its compact and efficient motion information. To relieve from the burden of expensive and laborious data annotation, unsupervised approaches, particularly contrastive learning, have been widely employed to extract action representations from unlabeled data. In this paper, we propose an Ensemble framework for Contrastive Learning of Representation (EnsCLR) to preform unsupervised skeleton-based action recognition. Concretely, Queue Extension method is devised to generate discriminative representation by aggregating the ensemble information from multiple pipelines. Furtherly, Ensemble Nearest Neighbors Mining (ENNM) method is utilized to excavate the most similar samples from the unlabeled data as positive samples, which alleviates the false-negative samples problem caused by the disregard of category label. The experiments with extensive evaluation protocols show that EnsCLR outperforms previous state-of-the-art methods on NTU60, NTU120, and PKU-MMD datasets.},
  archive      = {J_CVIU},
  author       = {Kun Wang and Jiuxin Cao and Biwei Cao and Bo Liu},
  doi          = {10.1016/j.cviu.2024.104076},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104076},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {EnsCLR: Unsupervised skeleton-based action recognition via ensemble contrastive learning of representation},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Object discriminability re-extraction for distractor-aware
visual object tracking. <em>CVIU</em>, <em>247</em>, 104075. (<a
href="https://doi.org/10.1016/j.cviu.2024.104075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The similar distractor problem is one of the most difficult challenges for Siamese-based trackers. Since they formulate the visual tracking task as a similar matching problem, these trackers involve an essential problem that they are sensitive to the intra-class and inter-class instances with similar appearance confusion. To solve the problem, we propose an object discriminability re-extraction network (ODR-Net) for distractor-aware visual object tracking. The network first mines similar distractors from existing tracking information with a distractor capture module, and then re-extracts discriminative features to redetect the target from distractors with a discriminative feature re-extraction module. It solves the distractor problem in the decoding phase of a tracker and can be considered as a general block that applied to existing Siamese trackers to tackle the similar distractor problem. To demonstrate the effectiveness of the proposed method, extensive experiments and comparisons with state-of-the-art trackers are conducted on a variety of large-scale benchmark datasets, including GOT-10k, LaSOT, OTB-2015, TrackingNet, VOT2020, VOT2021, and VOT2022. Without bells and whistles, our ODR-Net achieves leading performance with a real-time speed.},
  archive      = {J_CVIU},
  author       = {Ying Cui and Qiang Cheng and Dongyan Guo and Xiangjie Kong and Zhenhua Wang and Jianhua Zhang},
  doi          = {10.1016/j.cviu.2024.104075},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104075},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Object discriminability re-extraction for distractor-aware visual object tracking},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightning fast video anomaly detection via multi-scale
adversarial distillation. <em>CVIU</em>, <em>247</em>, 104074. (<a
href="https://doi.org/10.1016/j.cviu.2024.104074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a very fast frame-level model for anomaly detection in video, which learns to detect anomalies by distilling knowledge from multiple highly accurate object-level teacher models. To improve the fidelity of our student, we distill the low-resolution anomaly maps of the teachers by jointly applying standard and adversarial distillation, introducing an adversarial discriminator for each teacher to distinguish between target and generated anomaly maps. We conduct experiments on three benchmarks (Avenue, ShanghaiTech, UCSD Ped2), showing that our method is over 7 times faster than the fastest competing method, and between 28 and 62 times faster than object-centric models, while obtaining comparable results to recent methods. Our evaluation also indicates that our model achieves the best trade-off between speed and accuracy, due to its previously unheard-of speed of 1480 FPS. In addition, we carry out a comprehensive ablation study to justify our architectural design choices. Our code is freely available at: https://github.com/ristea/fast-aed .},
  archive      = {J_CVIU},
  author       = {Florinel-Alin Croitoru and Nicolae-Cătălin Ristea and Dana Dăscălescu and Radu Tudor Ionescu and Fahad Shahbaz Khan and Mubarak Shah},
  doi          = {10.1016/j.cviu.2024.104074},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104074},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lightning fast video anomaly detection via multi-scale adversarial distillation},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SHOWMe: Robust object-agnostic hand-object 3D reconstruction
from RGB video. <em>CVIU</em>, <em>247</em>, 104073. (<a
href="https://doi.org/10.1016/j.cviu.2024.104073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of detailed hand-object 3D reconstruction from monocular video with unknown objects , for applications where the required accuracy and level of detail is important, e.g. object hand-over in human–robot collaboration, or manipulation and contact point analysis. While the recent literature on this topic is promising, the accuracy and generalization abilities of existing methods are still lacking. This is due to several limitations, such as the assumption of known object class or model for a small number of instances, or over-reliance on off-the-shelf keypoint and structure-from-motion methods for object-relative viewpoint estimation, prone to complete failure with previously unobserved, poorly textured objects or hand-object occlusions. To address previous method shortcomings, we present a 2-stage pipeline superseding state-of-the-art (SotA) performance on several metrics. First, we robustly retrieve viewpoints relying on a learned pairwise camera pose estimator trainable with a low data regime, followed by a globalized Shonan pose averaging. Second, we simultaneously estimate detailed 3D hand-object shapes and refine camera poses using a differential renderer-based optimizer. To better assess the out-of-distribution abilities of existing methods, and to showcase our methodological contributions, we introduce the new SHOWMe benchmark dataset with 96 sequences annotated with poses, millimetric textured 3D shape scans, and parametric hand models, introducing new object and hand diversity. Remarkably, we show that our method is able to reconstruct 100% of these sequences as opposed to SotA Structure-from-Motion (SfM) or hand-keypoint-based pipelines, and obtains reconstructions of equivalent or better precision when existing methods do succeed in providing a result. We hope these contributions lead to further research under harder input assumptions. The dataset can be downloaded at https://download.europe.naverlabs.com/showme .},
  archive      = {J_CVIU},
  author       = {Anilkumar Swamy and Vincent Leroy and Philippe Weinzaepfel and Fabien Baradel and Salma Galaaoui and Romain Brégier and Matthieu Armando and Jean-Sebastien Franco and Grégory Rogez},
  doi          = {10.1016/j.cviu.2024.104073},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104073},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SHOWMe: Robust object-agnostic hand-object 3D reconstruction from RGB video},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-domain awareness for compressed deepfake videos
detection over social networks guided by common mechanisms between
artifacts. <em>CVIU</em>, <em>247</em>, 104072. (<a
href="https://doi.org/10.1016/j.cviu.2024.104072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The viral spread of massive deepfake videos over social networks has caused serious security problems. Despite the remarkable advancements achieved by existing deepfake detection algorithms, deepfake videos over social networks are inevitably influenced by compression factors. This causes deepfake detection performance to be limited by the following challenging issues: (a) interfering with compression artifacts, (b) loss of feature information, and (c) aliasing of feature distributions. In this paper, we analyze the common mechanism between compression artifacts and deepfake artifacts, revealing the structural similarity between them and providing a reliable theoretical basis for enhancing the robustness of deepfake detection models against compression. Firstly, based on the common mechanism between artifacts, we design a frequency domain adaptive notch filter to eliminate the interference of compression artifacts on specific frequency bands. Secondly, to reduce the sensitivity of deepfake detection models to unknown noise, we propose a spatial residual denoising strategy. Thirdly, to exploit the intrinsic correlation between feature vectors in the frequency domain branch and the spatial domain branch, we enhance deepfake features using an attention-based feature fusion method. Finally, we adopt a multi-task decision approach to enhance the discriminative power of the latent space representation of deepfakes, achieving deepfake detection with robustness against compression. Extensive experiments show that compared with the baseline methods, the detection performance of the proposed algorithm on compressed deepfake videos has been significantly improved. In particular, our model is resistant to various types of noise disturbances and can be easily combined with baseline detection models to improve their robustness.},
  archive      = {J_CVIU},
  author       = {Yan Wang and Qindong Sun and Dongzhu Rong and Rong Geng},
  doi          = {10.1016/j.cviu.2024.104072},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104072},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-domain awareness for compressed deepfake videos detection over social networks guided by common mechanisms between artifacts},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision and structured-language pretraining for cross-modal
food retrieval. <em>CVIU</em>, <em>247</em>, 104071. (<a
href="https://doi.org/10.1016/j.cviu.2024.104071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-Language Pretraining (VLP) and Foundation models have been the go-to recipe for achieving SoTA performance on general benchmarks. However, leveraging these powerful techniques for more complex vision-language tasks, such as cooking applications, with more structured input data, is still little investigated. In this work, we propose to leverage these techniques for structured-text based computational cuisine tasks. Our strategy, dubbed VLPCook, first transforms existing image-text pairs to image and structured-text pairs. This allows to pretrain our VLPCook model using VLP objectives adapted to the structured data of the resulting datasets, then finetuning it on downstream computational cooking tasks. During finetuning, we also enrich the visual encoder, leveraging pretrained foundation models ( e.g. CLIP) to provide local and global textual context. VLPCook outperforms current SoTA by a significant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset. We conduct further experiments on VLP to validate their importance, especially on the Recipe1M+ dataset. Finally, we validate the generalization of the approach to other tasks ( i.e , Food Recognition) and domains with structured text such as the Medical domain on the ROCO dataset. The code will be made publicly available.},
  archive      = {J_CVIU},
  author       = {Mustafa Shukor and Nicolas Thome and Matthieu Cord},
  doi          = {10.1016/j.cviu.2024.104071},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104071},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Vision and structured-language pretraining for cross-modal food retrieval},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modality adaptation via feature difference learning for
depth human parsing. <em>CVIU</em>, <em>247</em>, 104070. (<a
href="https://doi.org/10.1016/j.cviu.2024.104070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of human parsing, depth data offers unique advantages over RGB data due to its illumination invariance and geometric detail, which motivates us to explore human parsing with only depth input. However, depth data is challenging to collect at scale due to the specialized equipment required. In contrast, RGB data is readily available in large quantities, presenting an opportunity to enhance depth-only parsing models with semantic knowledge learned from RGB data. However, fully finetuning the RGB-pretrained encoder leads to high training costs and inflexible domain generalization, while keeping the encoder frozen suffers from a large RGB-depth modality gap and restricts the parsing performance. To alleviate the limitations of these naive approaches, we introduce a Modality Adaptation pipeline via Feature Difference Learning (MAFDL) which leverages the RGB knowledge to facilitate depth human parsing. A Difference-Guided Depth Adapter (DGDA) is proposed within MAFDL to learn the feature differences between RGB and depth modalities, adapting depth features into RGB feature space to bridge the modality gap. Furthermore, we also design a Feature Alignment Constraint (FAC) to impose explicit alignment supervision at pixel and batch levels, making the modality adaptation more comprehensive. Extensive experiments on the NTURGBD-Parsing-4K dataset show that our method surpasses previous state-of-the-art approaches.},
  archive      = {J_CVIU},
  author       = {Shaofei Huang and Tianrui Hui and Yue Gong and Fengguang Peng and Yuqiang Fang and Jingwei Wang and Bin Ma and Xiaoming Wei and Jizhong Han},
  doi          = {10.1016/j.cviu.2024.104070},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104070},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Modality adaptation via feature difference learning for depth human parsing},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classroom teacher action recognition based on
spatio-temporal dual-branch feature fusion. <em>CVIU</em>, <em>247</em>,
104068. (<a href="https://doi.org/10.1016/j.cviu.2024.104068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classroom teaching action recognition task refers to recognizing and understanding teacher action through video temporal and spatial information. Due to complex backgrounds and significant occlusions, recognizing teacher action in the classroom environment poses substantial challenges. In this study, we propose a classroom teacher action recognition approach based on a spatio-temporal dual-branch feature fusion architecture, where the core task involves utilizing continuous human keypoint heatmap information and single-frame image information. Specifically, we fuse features from two modalities to propose a method combining image spatial information with temporal human keypoint heatmap information for teacher action recognition. Our approach ensures recognition accuracy while reducing the model’s parameters and computational complexity. Additionally, we constructed a teacher action dataset (CTA) in a real classroom environment, comprising 12 action categories, 13k+ video segments, and a total duration exceeding 15 h. The experimental results on the CTA dataset validate the effectiveness of our proposed method. Our research explores action recognition tasks in real complex classroom environments, providing a technical framework for classroom teaching intelligent analysis.},
  archive      = {J_CVIU},
  author       = {Di Wu and Jun Wang and Wei Zou and Shaodong Zou and Juxiang Zhou and Jianhou Gan},
  doi          = {10.1016/j.cviu.2024.104068},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104068},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Classroom teacher action recognition based on spatio-temporal dual-branch feature fusion},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pseudo initialization based few-shot class incremental
learning. <em>CVIU</em>, <em>247</em>, 104067. (<a
href="https://doi.org/10.1016/j.cviu.2024.104067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-Shot Class Incremental Learning (FSCIL) aims to recognize sequentially arriving new classes without catastrophic forgetting old classes. The incremental new classes only contain very few labeled examples for updating the model, which causes overfitting problem. Current popular reserving embedding space method Forward Compatible Training preserves feature space for novel classes. Base class is pushed away from the most similar virtual class, preparing for the incoming novel classes. However, this can lead to pushing the base class to other similar virtual classes. In this paper, we propose a novel FSCIL method in order to overcome the aforementioned problem. Specifically, our core idea is pushing base classes away from the most similar top-K virtual classes to reserve feature space and provide pseudo initialization for the incoming novel classes. To further encourage learning new classes without forgetting, an additional regularization is applied to limit the extent of model updating. Extensive experiments are conducted on CUB200, CIFAR100 and mini-ImageNet, illustrating the performance of our proposed method. The results show that our method outperforms the state-of-the-art method and achieves significant improvement.},
  archive      = {J_CVIU},
  author       = {Mingwen Shao and Xinkai Zhuang and Lixu Zhang and Wangmeng Zuo},
  doi          = {10.1016/j.cviu.2024.104067},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104067},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pseudo initialization based few-shot class incremental learning},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced dual contrast representation learning with cell
separation and merging for breast cancer diagnosis. <em>CVIU</em>,
<em>247</em>, 104065. (<a
href="https://doi.org/10.1016/j.cviu.2024.104065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer remains a prevalent malignancy impacting a substantial number of individuals globally. In recent times, there has been a growing trend of combining deep learning methods with breast cancer diagnosis. Nevertheless, this integration encounters challenges, including limited data availability, class imbalance, and the absence of fine-grained labels to safeguard patient privacy and accommodate experience-dependent detection. To address these issues, we propose an effective framework by a dual contrast representation learning with a cell separation and merging strategy. The proposed algorithm comprises three main components: the cell separation and merging part, the dual contrast representation learning part, and the multi-category classification part. The cell separation and merging part takes an unpaired set of histopathological images as input and produces two sets of separated image layers, through the exploration of latent semantic information using SAM. Subsequently, these separated image layers are utilized to generate two new unpaired histopathological images via a cell separation and merging approach based on the linear superimposition model, with an inpainting network being employed to refine image details. Thus the class imbalance problem is alleviated and the data size is enlarged for a sufficient CNN training. The second part introduces a dual contrast representation learning framework for these generated images, with one branch designed for the positive samples (tumor cells) and the other for the negative samples (normal cells). The contrast learning network effectively minimizes the distance between two generated positive samples while maximizing the similarity of intra-class images to enhance feature representation. Leveraging the facilitated feature representation acquired from the dual contrast representation learning part, a pre-trained classifier is further fine-tuned to predict breast cancer categories. Extensive quantitative and qualitative experimental results validates the superiority of our proposed method compared to other state-of-the-art methods on the BreaKHis dataset in terms of four measurement metrics.},
  archive      = {J_CVIU},
  author       = {Yang Liu and Yiqi Zhu and Zhehao Gu and Jinshan Pan and Juncheng Li and Ming Fan and Lihua Li and Tieyong Zeng},
  doi          = {10.1016/j.cviu.2024.104065},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104065},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhanced dual contrast representation learning with cell separation and merging for breast cancer diagnosis},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit and explicit commonsense for multi-sentence video
captioning. <em>CVIU</em>, <em>247</em>, 104064. (<a
href="https://doi.org/10.1016/j.cviu.2024.104064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing dense or paragraph video captioning approaches rely on holistic representations of videos, possibly coupled with learned object/action representations, to condition hierarchical language decoders. However, they fundamentally lack the commonsense knowledge of the world required to reason about progression of events, causality, and even the function of certain objects within a scene. To address this limitation we propose a novel video captioning Transformer-based model, that takes into account both implicit (visuo-lingual and purely linguistic) and explicit (knowledge-base) commonsense knowledge. We show that these forms of knowledge, in isolation and in combination, enhance the quality of produced captions. Further, inspired by imitation learning, we propose a new task of instruction generation, where the goal is to produce a set of linguistic instructions from a video demonstration of its performance. We formalize the task using the ALFRED dataset generated using an AI2-THOR environment. While instruction generation is conceptually similar to paragraph captioning, it differs in the fact that it exhibits stronger object persistence, as well as spatially-aware and causal sentence structure. We show that our commonsense knowledge enhanced approach produces significant improvements on this task (up to 57% in METEOR and 8.5% in CIDEr), as well as the state-of-the-art result on more traditional video captioning in the ActivityNet Captions dataset.},
  archive      = {J_CVIU},
  author       = {Shih-Han Chou and James J. Little and Leonid Sigal},
  doi          = {10.1016/j.cviu.2024.104064},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104064},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Implicit and explicit commonsense for multi-sentence video captioning},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-label image classification using adaptive graph
convolutional networks: From a single domain to multiple domains.
<em>CVIU</em>, <em>247</em>, 104062. (<a
href="https://doi.org/10.1016/j.cviu.2024.104062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive graph-based approach for multi-label image classification. Graph-based methods have been largely exploited in the field of multi-label classification, given their ability to model label correlations. Specifically, their effectiveness has been proven not only when considering a single domain but also when taking into account multiple domains. However, the topology of the used graph is not optimal as it is pre-defined heuristically. In addition, consecutive Graph Convolutional Network (GCN) aggregations tend to destroy the feature similarity. To overcome these issues, an architecture for learning the graph connectivity in an end-to-end fashion is introduced. This is done by integrating an attention-based mechanism and a similarity-preserving strategy. The proposed framework is then extended to multiple domains using an adversarial training scheme. Numerous experiments are reported on well-known single-domain and multi-domain benchmarks. The results demonstrate that our approach achieves competitive results in terms of mean Average Precision (mAP) and model size as compared to the state-of-the-art. The code will be made publicly available.},
  archive      = {J_CVIU},
  author       = {Inder Pal Singh and Enjie Ghorbel and Oyebade Oyedotun and Djamila Aouada},
  doi          = {10.1016/j.cviu.2024.104062},
  journal      = {Computer Vision and Image Understanding},
  month        = {10},
  pages        = {104062},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-label image classification using adaptive graph convolutional networks: From a single domain to multiple domains},
  volume       = {247},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A proxy-data-based hierarchical adversarial patch generation
method. <em>CVIU</em>, <em>246</em>, 104066. (<a
href="https://doi.org/10.1016/j.cviu.2024.104066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current training data-dependent physical attacks have limited applicability to privacy-critical situations when attackers lack access to neural networks’ training data. To address this issue, this paper presents a hierarchical adversarial patch generation framework considering data privacy, utilizing proxy datasets while assuming that the training data is blinded. In the upper layer, Average Patch Saliency ( APS ) is introduced as a quantitative metric to determine the best proxy dataset for patch generation from a set of publicly available datasets. In the lower layer, Expectation of Transformation Plus ( EoT+ ) method is developed to generate patches while accounting for perturbing background simulation and sensitivity alleviation. Evaluation results obtained in digital settings show that the proposed proxy-data-based framework achieves comparable targeted attack results to the data-dependent benchmark method. Finally, the framework’s validity is comprehensively evaluated in the physical world, where the corresponding experimental videos and code can be found at here .},
  archive      = {J_CVIU},
  author       = {Jiawei Liu and Xun Gong and Tingting Wang and Yunfeng Hu and Hong Chen},
  doi          = {10.1016/j.cviu.2024.104066},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104066},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A proxy-data-based hierarchical adversarial patch generation method},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised network for low-light traffic image
enhancement based on deep noise and artifacts removal. <em>CVIU</em>,
<em>246</em>, 104063. (<a
href="https://doi.org/10.1016/j.cviu.2024.104063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent transportation system (ITS), detecting vehicles and pedestrians in low-light conditions is challenging due to the low contrast between objects and the background. Recently, many works have enhanced low-light images using deep learning-based methods, but these methods require paired images during training, which are impractical to obtain in real-world traffic scenarios. Therefore, we propose a self-supervised network (SSN) for low-light traffic image enhancement that can be trained without paired images. To avoid amplifying noise and artifacts in the processed image during enhancement, we first proposed a denoising net to reduce the noise and artifacts in the input image. Then the processed image can be enhanced by the enhancement net. Considering the compression of the traffic image, we designed an artifacts removal net to improve the quality of the enhanced image. We proposed several effective and differential losses to make SSN trainable with low-light images only. To better integrate the extracted features from different levels in the network, we also proposed an attention module named the multi-head non-local block. In experiments, we evaluated SSN and other low-light image enhancement methods on two low-light traffic image sets: the Berkeley Deep Drive (BDD) dataset and the Hong Kong night-time multi-class vehicle (HK) dataset. The results indicated that SSN significantly improves upon other methods in visual comparison and some blind image quality metrics. We also conducted comparisons on classical ITS tasks like vehicle detection on the images enhanced by SSN and other methods, which further verified its effectiveness.},
  archive      = {J_CVIU},
  author       = {Houwang Zhang and Kai-Fu Yang and Yong-Jie Li and Leanne Lai-Hang Chan},
  doi          = {10.1016/j.cviu.2024.104063},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104063},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised network for low-light traffic image enhancement based on deep noise and artifacts removal},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Camouflaged object segmentation with prior via two-stage
training. <em>CVIU</em>, <em>246</em>, 104061. (<a
href="https://doi.org/10.1016/j.cviu.2024.104061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The camouflaged object segmentation (COS) task aims to segment objects visually embedded within the background. Existing models usually rely on prior information as an auxiliary means to identify camouflaged objects. However, low-quality priors and the singular guidance form hinder the effective utilization of prior information. To address these issues, we propose a novel approach for prior generation and guidance, named prior-guided transformer (PGT). For prior generation, we design a prior generation subnetwork consisting of a Transformer backbone and simple convolutions to obtain higher-quality priors at a lower cost. In addition, to fully exploit the backbone’s understanding capabilities of the camouflage characteristics, a novel two-stage training method is proposed to achieve the backbone’s deep supervision. For prior guidance, we design a prior guidance modules (PGM), with distinct space token mixers to respectively capture global dependencies of location priors and local details of boundary priors. Additionally, we introduce a cross-level prior in the form of features to facilitate inter-level communication of backbone features. Extensive experiments have been conducted and experimental results illustrate the effectiveness and superiority of our method. The code is available at https://github.com/Ray3417/PGT .},
  archive      = {J_CVIU},
  author       = {Rui Wang and Caijuan Shi and Changyu Duan and Weixiang Gao and Hongli Zhu and Yunchao Wei and Meiqin Liu},
  doi          = {10.1016/j.cviu.2024.104061},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104061},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Camouflaged object segmentation with prior via two-stage training},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ModelNet-o: A large-scale synthetic dataset for
occlusion-aware point cloud classification. <em>CVIU</em>, <em>246</em>,
104060. (<a href="https://doi.org/10.1016/j.cviu.2024.104060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, 3D point cloud classification has made significant progress with the help of many datasets. However, these datasets do not reflect the incomplete nature of real-world point clouds caused by occlusion , which limits the practical application of current methods. To bridge this gap, we propose ModelNet-O, a large-scale synthetic dataset of 123,041 samples that emulates real-world point clouds with self-occlusion caused by scanning from monocular cameras. ModelNet-O is 10 times larger than existing datasets and offers more challenging cases to evaluate the robustness of existing methods. Our observation on ModelNet-O reveals that well-designed sparse structures can preserve structural information of point clouds under occlusion , motivating us to propose a robust point cloud processing method that leverages a critical point sampling (CPS) strategy in a multi-level manner. We term our method PointMLS. Through extensive experiments, we demonstrate that our PointMLS achieves state-of-the-art results on ModelNet-O and competitive results on regular datasets such as ModelNet40 and ScanObjectNN, and we also demonstrate its robustness and effectiveness. Code available: https://github.com/fanglaosi/ModelNet-O_PointMLS .},
  archive      = {J_CVIU},
  author       = {Zhongbin Fang and Xia Li and Xiangtai Li and Shen Zhao and Mengyuan Liu},
  doi          = {10.1016/j.cviu.2024.104060},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104060},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ModelNet-O: A large-scale synthetic dataset for occlusion-aware point cloud classification},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skeleton cluster tracking for robust multi-view multi-person
3D human pose estimation. <em>CVIU</em>, <em>246</em>, 104059. (<a
href="https://doi.org/10.1016/j.cviu.2024.104059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-view 3D human pose estimation task relies on 2D human pose estimation for each view; however, severe occlusion, truncation, and human interaction lead to incorrect 2D human pose estimation for some views. The traditional “Matching-Lifting-Tracking” paradigm amplifies the incorrect 2D human pose into an incorrect 3D human pose, which significantly challenges the robustness of multi-view 3D human pose estimation. In this paper, we propose a novel method that tackles the inherent difficulties of the traditional paradigm. This method is rooted in the newly devised “Skeleton Pooling-Clustering-Tracking (SPCT)” paradigm. It initiates a 2D human pose estimation for each perspective. Then a symmetrical dilated network is created for skeleton pool estimation. Upon clustering the skeleton pool, we introduce and implement an innovative tracking method that is explicitly designed for the SPCT paradigm. The tracking method refines and filters the skeleton clusters, thereby enhancing the robustness of the multi-person 3D human pose estimation results. By coupling the skeleton pool with the tracking refinement process, our method obtains high-quality multi-person 3D human pose estimation results despite severe occlusions that produce erroneous 2D and 3D estimates. By employing the proposed SPCT paradigm and a computationally efficient network architecture, our method outperformed existing approaches regarding robustness on the Shelf, 4D Association, and CMU Panoptic datasets, and could be applied in practical scenarios such as markerless motion capture and animation production.},
  archive      = {J_CVIU},
  author       = {Zehai Niu and Ke Lu and Jian Xue and Jinbao Wang},
  doi          = {10.1016/j.cviu.2024.104059},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104059},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Skeleton cluster tracking for robust multi-view multi-person 3D human pose estimation},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Confidence sharing adaptation for out-of-domain human pose
and shape estimation. <em>CVIU</em>, <em>246</em>, 104051. (<a
href="https://doi.org/10.1016/j.cviu.2024.104051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose and shape estimation is often impacted by distribution bias in real-world scenarios due to factors such as bone length, camera parameters, background, and occlusion. To address this issue, we propose the Confidence Sharing Adaptation (CSA) algorithm, which corrects model bias using unlabeled images from the test domain before testing. However, the lack of annotation constraints in the adaptive training process poses a significant challenge, making it susceptible to model collapse. CSA utilizes a decoupled dual-branch learning framework to provide pseudo-labels and remove noise samples based on the confidence scores of the inference results. By sharing the most confident prior knowledge between the dual-branch networks, CSA effectively mitigates distribution bias. CSA is also remarkably adaptable to severely occluded scenes, thanks to two auxiliary techniques: a self-attentive parametric regressor that ensures robustness to occlusion of local body parts and a rendered surface texture loss that regulates the relationship between occlusion of human joint positions. Evaluation results show that CSA successfully adapts to scenarios beyond the training domain and achieves state-of-the-art performance on both occlusion-specific and general benchmarks. Code and pre-trained models are available for research at https://github.com/bodymapper/csa.git},
  archive      = {J_CVIU},
  author       = {Tianyi Yue and Keyan Ren and Yu Shi and Hu Zhao and Qingyun Bian},
  doi          = {10.1016/j.cviu.2024.104051},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104051},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Confidence sharing adaptation for out-of-domain human pose and shape estimation},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-driven diffusion for sign language production with
gloss-pose latent spaces alignment. <em>CVIU</em>, <em>246</em>, 104050.
(<a href="https://doi.org/10.1016/j.cviu.2024.104050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign Language Production (SLP) aims to translate spoken language into visual sign language sequences. The most challenging process in SLP is the transformation of a sequence of sign glosses into corresponding sign poses (G2P). Existing approaches on G2P mainly focus on constructing mappings of sign language glosses to frame-level sign pose representations, while neglecting gloss is just a weak annotation of the sequence of sign poses. To address this problem, this paper proposes the semantic-driven diffusion model with gloss-pose latent spaces alignment (SDD-GPLA) for G2P. G2P is divided into two phases. In the first phase, we design the gloss-pose latent spaces alignment (GPLA) to model the sign pose latent representations with glosses dependency. In the second phase, we propose semantic-driven diffusion (SDD) with supervised pose reconstruction guidance as a mapping between the gloss and sign poses latent features. In addition, we propose the sign pose decoder ( Decoder p Decoderp ) to progressively generate high-resolution sign poses from latent sign pose features and to guide the SDD training process. We evaluated SDD-GPLA on a self-collected dataset of Daily Chinese Sign Language (DCSL) and a public dataset called RWTH-Phoenix-Weather-2014T. Compared with the state-of-the-art G2P methods, we obtain at least 22.9% and 2.3% improvement in WER scores on the above two datasets, respectively.},
  archive      = {J_CVIU},
  author       = {Sheng Chen and Qingshan Wang and Qi Wang},
  doi          = {10.1016/j.cviu.2024.104050},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104050},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic-driven diffusion for sign language production with gloss-pose latent spaces alignment},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lv-adapter: Adapting vision transformers for visual
classification with linear-layers and vectors. <em>CVIU</em>,
<em>246</em>, 104049. (<a
href="https://doi.org/10.1016/j.cviu.2024.104049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large pre-trained models based on Vision Transformers (ViTs) contain nearly billions of parameters, demanding substantial computational resources and storage space. This restricts their transferability across different tasks. Recent approaches try to use adapter fine-tuning to address this drawback. However, there is still potential to improve the number of tunable parameters and the accuracy in these methods. To address this challenge, we propose an adapter fine-tuning module called Lv-Adapter, which consists of a linear layer and vector. This module enables targeted parameter fine-tuning of pretrained models by learning both the prior knowledge of pre-trained task and the information from downstream specific task, to adapt to various downstream tasks in image and video tasks while transfer learning. Compared to full fine-tuning methods, Lv-Adapter has several appealing advantages. Firstly, by adding only about 3% extra parameters to ViT, Lv-Adapter achieves comparable accuracy to full fine-tuning methods and even significantly surpasses them on action recognition benchmarks. Secondly, Lv-Adapter is a lightweight module that can be plug-and-play in different transformer models due to its simplicity. Finally, to validate these claims, extensive experiments were conducted on five image and video datasets in this study, providing evidence for the effectiveness of Lv-Adapter. When only 3.5% of the extra parameters are updated, it respectively achieves a relative boost of about 13% and 24% compared to the fully fine-tuned model on SSv2 and HMDB51.},
  archive      = {J_CVIU},
  author       = {Guangyi Xu and Junyong Ye and Xinyuan Liu and Xubin Wen and Youwei Li and Jingjing Wang},
  doi          = {10.1016/j.cviu.2024.104049},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104049},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lv-adapter: Adapting vision transformers for visual classification with linear-layers and vectors},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised monocular depth estimation with
self-distillation and dense skip connection. <em>CVIU</em>,
<em>246</em>, 104048. (<a
href="https://doi.org/10.1016/j.cviu.2024.104048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation (MDE) is crucial in a wide range of applications, including robotics, autonomous driving and virtual reality. Self-supervised monocular depth estimation has emerged as a promising MDE approach without requiring hard-to-obtain depth labels during training, and multi-scale photometric loss is widely used for self-supervised monocular depth estimation as the self-supervised signal. However, multi-photometric loss is a weak training signal and might disturb the good intermediate features representation. In this paper, we propose a successive depth map self-distillation(SDM-SD) loss, which combines with the single-scale photometric loss to replace the multi-scale photometric loss. Moreover, considering that multi-stage feature representations are essential for dense prediction tasks such as depth estimation, we also propose a dense skip connection, which can efficiently fuse the intermediate features of the encoder and fully utilize them in each stage of the decoder in our encoder–decoder architecture. By applying successive depth map self-distillation loss and dense skip connection, our proposed method can achieve state-of-the-art performance on the KITTI benchmark, and exhibit the best generalization ability on the challenging indoor dataset NYUv2 dataset.},
  archive      = {J_CVIU},
  author       = {Xuezhi Xiang and Wei Li and Yao Wang and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2024.104048},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104048},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised monocular depth estimation with self-distillation and dense skip connection},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identity-preserving editing of multiple facial attributes by
learning global edit directions and local adjustments. <em>CVIU</em>,
<em>246</em>, 104047. (<a
href="https://doi.org/10.1016/j.cviu.2024.104047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic facial attribute editing using pre-trained Generative Adversarial Networks (GANs) has attracted a great deal of attention and effort from researchers in recent years. Due to the high quality of face images generated by StyleGANs, much work has focused on the StyleGANs’ latent space and the proposed methods for facial image editing. Although these methods have achieved satisfying results for manipulating user-intended attributes, they have not fulfilled the goal of preserving the identity, which is an important challenge. We present ID-Style, a new architecture capable of addressing the problem of identity loss during attribute manipulation. The key components of ID-Style include a Learnable Global Direction (LGD) module, which finds a shared and semi-sparse direction for each attribute, and an Instance-Aware Intensity Predictor (IAIP) network, which finetunes the global direction according to the input instance. Furthermore, we introduce two losses during training to enforce the LGD and IAIP to find semi-sparse semantic directions that preserve the identity of the input instance. Despite reducing the size of the network by roughly 95% as compared to similar state-of-the-art works, ID-Style outperforms baselines by 10% and 7% in identity preserving metric (FRS) and average accuracy of manipulation (mACC), respectively.},
  archive      = {J_CVIU},
  author       = {Najmeh Mohammadbagheri and Fardin Ayar and Ahmad Nickabadi and Reza Safabakhsh},
  doi          = {10.1016/j.cviu.2024.104047},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104047},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Identity-preserving editing of multiple facial attributes by learning global edit directions and local adjustments},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). POTLoc: Pseudo-label oriented transformer for
point-supervised temporal action localization. <em>CVIU</em>,
<em>246</em>, 104044. (<a
href="https://doi.org/10.1016/j.cviu.2024.104044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to the creation of incomplete action proposals. This paper proposes POTLoc, a P seudo-label O riented T ransformer for weakly-supervised Action Loc alization utilizing only point-level annotation. POTLoc is designed to identify and track continuous action structures via a self-training strategy. The base model begins by generating action proposals solely with point-level supervision. These proposals undergo refinement and regression to enhance the precision of the estimated action boundaries, which subsequently results in the production of ‘pseudo-labels’ to serve as supplementary supervisory signals. The architecture of the model integrates a transformer with a temporal feature pyramid to capture video snippet dependencies and model actions of varying duration. The pseudo-labels, providing information about the coarse locations and boundaries of actions, assist in guiding the transformer for enhanced learning of action dynamics. POTLoc outperforms the state-of-the-art point-supervised methods on THUMOS’14 and ActivityNet-v1.2 datasets.},
  archive      = {J_CVIU},
  author       = {Elahe Vahdani and Yingli Tian},
  doi          = {10.1016/j.cviu.2024.104044},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104044},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {POTLoc: Pseudo-label oriented transformer for point-supervised temporal action localization},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DHBSR: A deep hybrid representation-based network for blind
image super resolution. <em>CVIU</em>, <em>246</em>, 104034. (<a
href="https://doi.org/10.1016/j.cviu.2024.104034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super resolution involves enhancing the spatial resolution of low-quality images and improving their visual quality. As in many real-life situations, the image degradation process is unknown, performing the task of image super resolution in a blind manner is of paramount importance. Deep neural networks provide high performances for the task of blind image super resolution, in view of their end-to-end learning capability between the low-resolution images and their ground truth versions. Generally speaking, deep blind image super resolution networks initially estimate the parameters of the image degradation process, such as blurring kernel, and then use them for super-resolving the low-resolution images. In this paper, we develop a novel deep learning-based scheme for the task of blind image super resolution, in which the idea of leveraging the hybrid representations is utilized. Specifically, we employ the deterministic and stochastic representations of the blurring kernel parameters to train a deep blind super resolution network in an effective manner. The results of extensive experiments prove the effectiveness of various ideas used in the development of the proposed deep blind image super resolution network.},
  archive      = {J_CVIU},
  author       = {Alireza Esmaeilzehi and Farshid Nooshi and Hossein Zaredar and M. Omair Ahmad},
  doi          = {10.1016/j.cviu.2024.104034},
  journal      = {Computer Vision and Image Understanding},
  month        = {9},
  pages        = {104034},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DHBSR: A deep hybrid representation-based network for blind image super resolution},
  volume       = {246},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Font transformer for few-shot font generation.
<em>CVIU</em>, <em>245</em>, 104043. (<a
href="https://doi.org/10.1016/j.cviu.2024.104043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic font generation is of great benefit to improving the efficiency of font designers. Few-shot font generation aims to generate new fonts from a few reference samples, and has recently attracted a lot of attention from researchers. This is valuable but challenging, especially for ideograms with high diversity and complex structures. Existing models based on convolutional neural networks (CNNs) struggle to generate glyphs with accurate font style and stroke details in the few-shot setting. This paper proposes the TransFont, exploiting the long-range dependency modeling ability of the Vision Transformer (ViT) for few-shot font generation. For the first time, we empirically show that the ViT is better at glyph image generation than CNNs. Furthermore, based on the observation of the high redundancy in the glyph feature map, we introduce the glyph self-attention module for mitigating the quadratic computational and memory complexity of the pixel-level glyph image generation, along with several new techniques, i.e., multi-head multiple sampling, yz axis convolution, and approximate relative position bias. Extensive experiments on two Chinese font libraries show the superiority of our method over existing CNN-based font generation models, the proposed TransFont generates glyph images with more accurate font style and stroke details.},
  archive      = {J_CVIU},
  author       = {Xu Chen and Lei Wu and Yongliang Su and Lei Meng and Xiangxu Meng},
  doi          = {10.1016/j.cviu.2024.104043},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104043},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Font transformer for few-shot font generation},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Text to image synthesis with multi-granularity feature aware
enhancement generative adversarial networks. <em>CVIU</em>,
<em>245</em>, 104042. (<a
href="https://doi.org/10.1016/j.cviu.2024.104042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing complex images from text presents challenging. Compared to autoregressive and diffusion model-based methods, Generative Adversarial Network-based methods have significant advantages in terms of computational cost and generation efficiency yet remain two limitations: first, these methods often refine all features output from the previous stage indiscriminately, without considering these features are initialized gradually during the generation process; second, the sparse semantic constraints provided by the text description are typically ineffective for refining fine-grained features. These issues complicate the balance between generation quality, computational cost and inference speed. To address these issues, we propose a Multi-granularity Feature Aware Enhancement GAN (MFAE-GAN), which allows the refinement process to match the order of different granularity features being initialized. Specifically, MFAE-GAN (1) samples category-related coarse-grained features and instance-level detail-related fine-grained features at different generation stages based on different attention mechanisms in Coarse-grained Feature Enhancement (CFE) and Fine-grained Feature Enhancement (FFE) to guide the generation process spatially, (2) provides denser semantic constraints than textual semantic information through Multi-granularity Features Adaptive Batch Normalization (MFA-BN) in the process of refining fine-grained features, and (3) adopts a Global Semantics Preservation (GSP) to avoid the loss of global semantics when sampling features continuously. Extensive experimental results demonstrate that our MFAE-GAN is competitive in terms of both image generation quality and efficiency.},
  archive      = {J_CVIU},
  author       = {Pei Dong and Lei Wu and Ruichen Li and Xiangxu Meng and Lei Meng},
  doi          = {10.1016/j.cviu.2024.104042},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104042},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Text to image synthesis with multi-granularity feature aware enhancement generative adversarial networks},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving semantic video retrieval models by training with a
relevance-aware online mining strategy. <em>CVIU</em>, <em>245</em>,
104035. (<a href="https://doi.org/10.1016/j.cviu.2024.104035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To retrieve a video via a multimedia search engine, a textual query is usually created by the user and then used to perform the search. Recent state-of-the-art cross-modal retrieval methods learn a joint text–video embedding space by using contrastive loss functions, which maximize the similarity of positive pairs while decreasing that of the negative pairs. Although the choice of these pairs is fundamental for the construction of the joint embedding space, the selection procedure is usually driven by the relationships found within the dataset: a positive pair is commonly formed by a video and its own caption, whereas unrelated video-caption pairs represent the negative ones. We hypothesize that this choice results in a retrieval system with limited semantics understanding, as the standard training procedure requires the system to discriminate between groundtruth and negative even though there is no difference in their semantics. Therefore, differently from the previous approaches, in this paper we propose a novel strategy for the selection of both positive and negative pairs which takes into account both the annotations and the semantic contents of the captions. By doing so, the selected negatives do not share semantic concepts with the positive pair anymore, and it is also possible to discover new positives within the dataset. Based on our hypothesis, we provide a novel design of two popular contrastive loss functions, and explore their effectiveness on four heterogeneous state-of-the-art approaches. The extensive experimental analysis conducted on four datasets, EPIC-Kitchens-100, MSR-VTT, MSVD, and Charades, validates the effectiveness of the proposed strategy, observing, e.g., more than +20% nDCG on EPIC-Kitchens-100. Furthermore, these results are corroborated with qualitative evidence both supporting our hypothesis and explaining why the proposed strategy effectively overcomes it.},
  archive      = {J_CVIU},
  author       = {Alex Falcon and Giuseppe Serra and Oswald Lanz},
  doi          = {10.1016/j.cviu.2024.104035},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104035},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving semantic video retrieval models by training with a relevance-aware online mining strategy},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Digital image defogging using joint retinex theory and
independent component analysis. <em>CVIU</em>, <em>245</em>, 104033. (<a
href="https://doi.org/10.1016/j.cviu.2024.104033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The images captured under adverse weather conditions suffer from poor visibility and contrast problems. Such images are not suitable for computer vision analysis and similar applications. Therefore, image defogging/dehazing is one of the most intriguing topics. In this paper, a new, fast, and robust defogging/de-hazing algorithm is proposed by combining the Retinex theory with independent component analysis, which performs better than existing algorithms. Initially, the foggy image is decomposed into two components: reflectance and luminance. The former is computed using the Retinex theory, while the latter is obtained by decomposing the foggy image into parallel and perpendicular components of air-light. Finally, the defogged image is obtained by applying Koschmieder’s law. Simulation results demonstrate the absence of halo effects and the presence of high-resolution images. The simulation results also confirm the effectiveness of the proposed method when compared to other conventional techniques in terms of NIQE, FADE, SSIM, PSNR, AG, CIEDE2000, r ̄ r̄ , and implementation time. All foggy and defogged results are available in high quality at the following link: https://drive.google.com/file/d/1OStXrfzdnF43gr6PAnBd8BHeThOfj33z/view?usp=drive_link .},
  archive      = {J_CVIU},
  author       = {Hossein Noori and Mohammad Hossein Gholizadeh and Hossein Khodabakhshi Rafsanjani},
  doi          = {10.1016/j.cviu.2024.104033},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104033},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Digital image defogging using joint retinex theory and independent component analysis},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complete contextual information extraction for
self-supervised monocular depth estimation. <em>CVIU</em>, <em>245</em>,
104032. (<a href="https://doi.org/10.1016/j.cviu.2024.104032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning methods are increasingly important for monocular depth estimation since they do not require ground-truth data during training. Although existing methods have achieved great success for better monocular depth estimation based on Convolutional Neural Networks (CNNs), the limited receptive field of CNNs usually is insufficient to effectively model the global information, e.g., relationship between foreground and background or relationship among objects, which are crucial for accurately capturing scene structure. Recently, some studies based on Transformers have attracted significant interest in computer vision. However, duo to the lack of spatial locality bias, they may fail to model the local information, e.g., fine-grained details with an image. To tackle these issues, we propose a novel self-supervised learning framework by incorporating the advantages of both the CNNs and Transformers so as to model the complete contextual information for high-quality monocular depth estimation. Specifically, the proposed method mainly includes two branches, where the Transformer branch is considered to capture the global information while the Convolution branch is exploited to preserve the local information. We also design a rectangle convolution module with pyramid structure to perceive the semi-global information, e.g. thin objects, along the horizontal and vertical directions within an image. Moreover, we propose a shape refinement module by learning the affinity matrix between pixel and its neighborhood to obtain accurate geometrical structure of scenes. Extensive experiments evaluated on KITTI, Cityscapes and Make3D dataset demonstrate that the proposed method achieves the competitive result compared with the state-of-the-art self-supervised monocular depth estimation methods and shows good cross-dataset generalization ability.},
  archive      = {J_CVIU},
  author       = {Dazheng Zhou and Mingliang Zhang and Xianjie Gao and Youmei Zhang and Bin Li},
  doi          = {10.1016/j.cviu.2024.104032},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104032},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Complete contextual information extraction for self-supervised monocular depth estimation},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Take a prior from other tasks for severe blur removal.
<em>CVIU</em>, <em>245</em>, 104027. (<a
href="https://doi.org/10.1016/j.cviu.2024.104027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recovering clear structures from severely blurry inputs is a huge challenge due to the detail loss and ambiguous semantics. Although segmentation maps can help deblur facial images, their effectiveness is limited in complex natural scenes because they ignore the detailed structures necessary for deblurring. Furthermore, direct segmentation of blurry images may introduce error propagation. To alleviate the semantic confusion and avoid error propagation, we propose utilizing high-level vision tasks, such as classification, to learn a comprehensive prior for severe blur removal. We propose a feature learning strategy based on knowledge distillation, which aims to learn the priors with global contexts and sharp local structures. To integrate the priors effectively, we propose a semantic prior embedding layer with multi-level aggregation and semantic attention. We validate our method on natural image deblurring benchmarks by introducing the priors to various models, including UNet and mainstream deblurring baselines, to demonstrate its effectiveness and generalization ability. The results show that our approach outperforms existing methods on severe blur removal with our plug-and-play semantic priors.},
  archive      = {J_CVIU},
  author       = {Pei Wang and Yu Zhu and Danna Xue and Qingsen Yan and Jinqiu Sun and Sung-eui Yoon and Yanning Zhang},
  doi          = {10.1016/j.cviu.2024.104027},
  journal      = {Computer Vision and Image Understanding},
  month        = {8},
  pages        = {104027},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Take a prior from other tasks for severe blur removal},
  volume       = {245},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight all-focused light field rendering.
<em>CVIU</em>, <em>244</em>, 104031. (<a
href="https://doi.org/10.1016/j.cviu.2024.104031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel real-time method for high-quality view interpolation from light field. The proposal is a lightweight method, which can be used with consumer GPU, reaching same or better quality than existing methods, in a shorter time, with significantly smaller memory requirements. Light field belongs to image-based rendering methods that can produce realistic images without computationally demanding algorithms. The novel view is synthesized from multiple input images of the same scene, captured at different camera positions. Standard rendering techniques, such as rasterization or ray-tracing, are limited in terms of quality, memory footprint, and speed. Light field rendering methods often produce unwanted artifacts resembling ghosting or blur in certain parts of the scene due to unknown geometry of the scene. The proposed method estimates the geometry for each pixel as an optimal focusing distance to mitigate the artifacts. The focusing distance determines which pixels from the input images are mixed to produce the final view. State-of-the-art methods use a constant-step pixel matching scan that iterates over a range of focusing distances. The scan searches for a distance with the smallest color dispersion of the contributing pixels, assuming that they belong to the same spot in the scene. The paper proposes an optimal scanning strategy of the focusing range, an improved color dispersion metric, and other minor improvements, such as sampling block size adjustment, out-of-bounds sampling, and filtering. Experimental results show that the proposal uses less resources, achieves better visual quality, and is significantly faster than existing light field rendering methods. The proposal is 8 × 8× faster than the methods in the same category. The proposal uses only four closest views from the light field data and reduces the necessary data transfer. Existing methods often require the full light field grid, which is typically 8 × 8 images large. Additionally, a new 4K light field dataset, containing scenes of various types, was created and published. An optimal novel method for light field acquisition is also proposed and used to create the dataset.},
  archive      = {J_CVIU},
  author       = {Tomáš Chlubna and Tomáš Milet and Pavel Zemčík},
  doi          = {10.1016/j.cviu.2024.104031},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104031},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lightweight all-focused light field rendering},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Other tokens matter: Exploring global and local features of
vision transformers for object re-identification. <em>CVIU</em>,
<em>244</em>, 104030. (<a
href="https://doi.org/10.1016/j.cviu.2024.104030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from images captured at different places and times. Recently, object Re-ID has achieved great success with the advances of Vision Transformers (ViT). However, the effects of the global–local relation have not been fully explored in Transformers for object Re-ID. In this work, we first explore the influence of global and local features of ViT and then further propose a novel Global–Local Transformer (GLTrans) for high-performance object Re-ID. We find that the features from last few layers of ViT already have a strong representational ability, and the global and local information can mutually enhance each other. Based on this fact, we propose a Global Aggregation Encoder (GAE) to utilize the class tokens of the last few Transformer layers and learn comprehensive global features effectively. Meanwhile, we propose the Local Multi-layer Fusion (LMF) which leverages both the global cues from GAE and multi-layer patch tokens to explore the discriminative local representations. Extensive experiments demonstrate that our proposed method achieves superior performance on four object Re-ID benchmarks. The code is available at https://github.com/AWangYQ/GLTrans .},
  archive      = {J_CVIU},
  author       = {Yingquan Wang and Pingping Zhang and Dong Wang and Huchuan Lu},
  doi          = {10.1016/j.cviu.2024.104030},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104030},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Other tokens matter: Exploring global and local features of vision transformers for object re-identification},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An unsupervised multi-focus image fusion method via
dual-channel convolutional network and discriminator. <em>CVIU</em>,
<em>244</em>, 104029. (<a
href="https://doi.org/10.1016/j.cviu.2024.104029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge in multi-focus image fusion tasks lies in accurately preserving the complementary information from the source images in the fused image. However, existing datasets often lack ground truth images, making it difficult for some full-reference loss functions (such as SSIM) to effectively participate in model training, thereby further affecting the performance of retaining source image details. To address this issue, this paper proposes an unsupervised dual-channel dense convolutional method, DCD, for multi-focus image fusion. DCD designs Patch processing blocks specifically for the fusion task, which segment the source image pairs into equally sized patches and evaluate their information to obtain a reconstructed image and a set of adaptive weight coefficients. The reconstructed image is used as the reference image, enabling unsupervised methods to utilize full-reference loss functions in training and overcoming the challenge of lacking labeled data in the training set. Furthermore, considering that the human visual system (HVS) is more sensitive to brightness than color, DCD trains the dual-channel network using both RGB images and their luminance components. This allows the network to focus more on the brightness information while preserving the color and gradient details of the source images, resulting in fused images that are more compatible with the HVS. The adaptive weight coefficients obtained through the Patch processing blocks are also used to determine the degree of preservation of the brightness information in the source images. Finally, comparative experiments on different datasets also demonstrate the superior performance of DCD in terms of fused image quality compared to other methods.},
  archive      = {J_CVIU},
  author       = {Lixing Fang and Xiangxiang Wang and Junli Zhao and Zhenkuan Pan and Hui Li and Yi Li},
  doi          = {10.1016/j.cviu.2024.104029},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104029},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An unsupervised multi-focus image fusion method via dual-channel convolutional network and discriminator},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). De2Net: Under-display camera image restoration with feature
deconvolution and kernel decomposition. <em>CVIU</em>, <em>244</em>,
104028. (<a href="https://doi.org/10.1016/j.cviu.2024.104028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the under-display camera (UDC) system provides an effective solution for notch-free full-screen displays, it inevitably causes severe image quality degradation due to the diffraction phenomenon. Recent methods have achieved decent performance with deep neural networks, yet the characteristic of the point spread function (PSF) is less studied. In this paper, considering the large support and spatial inconsistency of PSF, we propose De 2 2 Net for UDC image restoration with feature de convolution and kernel de composition. In terms of feature deconvolution, we introduce Wiener deconvolution as a preliminary process, which alleviates feature entanglement caused by the large PSF support. Besides, the deconvolution kernel can be learned from training images, eliminating the tedious PSF-obtaining process. As for kernel decomposition, we observe regular patterns for PSFs at different positions. Thus, with a kernel prediction network (KPN) deployed for handling the spatial inconsistency problem, we improve it from two aspects, i.e. , (i) decomposing the predicted kernels into a set of bases and weights, (ii) decomposing kernels into groups with different dilation rates. These modifications largely improve the receptive field under certain memory limits. Extensive experiments on three commonly used UDC datasets show that De 2 2 Net outperforms existing methods both quantitatively and qualitatively. Source code and pre-trained models are available at https://github.com/HyZhu39/De2Net .},
  archive      = {J_CVIU},
  author       = {Hangyan Zhu and Shaohui Liu and Ming Liu and Zifei Yan and Wangmeng Zuo},
  doi          = {10.1016/j.cviu.2024.104028},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104028},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {De2Net: Under-display camera image restoration with feature deconvolution and kernel decomposition},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditioning diffusion models via attributes and semantic
masks for face generation. <em>CVIU</em>, <em>244</em>, 104026. (<a
href="https://doi.org/10.1016/j.cviu.2024.104026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have shown impressive results in generating realistic images of faces. GANs managed to generate high-quality, high-fidelity images when conditioned on semantic masks, but they still lack the ability to diversify their output. Diffusion models partially solve this problem and are able to generate diverse samples given the same condition. This paper introduces a novel strategy for enhancing diffusion models through multi-conditioning, harnessing cross-attention mechanisms to utilize multiple feature sets, ultimately enabling the generation of high-quality and controllable images. The proposed method extends previous approaches by introducing conditioning on both attributes and semantic masks, ensuring finer control over the generated face images. In order to improve the training time and the generation quality, the impact of applying perceptual-focused loss weighting into the latent space instead of the pixel space is also investigated. The proposed solution has been evaluated on the CelebA-HQ dataset, and it can generate realistic and diverse samples while allowing for fine-grained control over multiple attributes and semantic regions. Experiments on the DeepFashion dataset have also been performed in order to analyze the capability of the proposed model to generalize to different domains. In addition, an ablation study has been conducted to evaluate the impact of different conditioning strategies on the quality and diversity of the generated images.},
  archive      = {J_CVIU},
  author       = {Giuseppe Lisanti and Nico Giambi},
  doi          = {10.1016/j.cviu.2024.104026},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104026},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Conditioning diffusion models via attributes and semantic masks for face generation},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised learning of rotation-invariant 3D point set
features using transformer and its self-distillation. <em>CVIU</em>,
<em>244</em>, 104025. (<a
href="https://doi.org/10.1016/j.cviu.2024.104025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invariance against rotations of 3D objects is an important property in analyzing 3D point set data. Conventional 3D point set DNNs having rotation invariance typically obtain accurate 3D shape features via supervised learning by using labeled 3D point sets as training samples. However, due to the rapid increase in 3D point set data and the high cost of labeling, a framework to learn rotation-invariant 3D shape features from numerous unlabeled 3D point sets is required. This paper proposes a novel self-supervised learning framework for acquiring accurate and rotation-invariant 3D point set features at object-level. Our proposed lightweight DNN architecture decomposes an input 3D point set into multiple global-scale regions, called tokens, that preserve the spatial layout of partial shapes composing the 3D object. We employ a self-attention mechanism to refine the tokens and aggregate them into an expressive rotation-invariant feature per 3D point set. Our DNN is effectively trained by using pseudo-labels generated by a self-distillation framework. To facilitate the learning of accurate features, we propose to combine multi-crop and cut-mix data augmentation techniques to diversify 3D point sets for training. Through a comprehensive evaluation, we empirically demonstrate that, (1) existing rotation-invariant DNN architectures designed for supervised learning do not necessarily learn accurate 3D shape features under a self-supervised learning scenario, and (2) our proposed algorithm learns rotation-invariant 3D point set features that are more accurate than those learned by existing algorithms.},
  archive      = {J_CVIU},
  author       = {Takahiko Furuya and Zhoujie Chen and Ryutarou Ohbuchi and Zhenzhong Kuang},
  doi          = {10.1016/j.cviu.2024.104025},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104025},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised learning of rotation-invariant 3D point set features using transformer and its self-distillation},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware feature stylization for domain
generalization. <em>CVIU</em>, <em>244</em>, 104016. (<a
href="https://doi.org/10.1016/j.cviu.2024.104016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizing to out-of-distribution (OOD) data is a challenging task for existing deep learning approaches. This problem largely comes from the common but often incorrect assumption of statistical learning algorithms that the source and target data come from the same i.i.d. distribution. To tackle the limited variability of domains available during training, as well as domain shifts at test time, numerous approaches for domain generalization have focused on generating samples from new domains. Recent studies on this topic suggest that feature statistics from instances of different domains can be mixed to simulate synthesized images from a novel domain. While this simple idea achieves state-of-art results on various domain generalization benchmarks, it ignores structural information which is key to transferring knowledge across different domains. In this paper, we leverage the ability of humans to recognize objects using solely their structural information (prominent region contours) to design a Structural-Aware Feature Stylization method for domain generalization. Our method improves feature stylization based on mixing instance statistics by enforcing structural consistency across the different style-augmented samples. This is achieved via a multi-task learning model which classifies original and augmented images while also reconstructing their edges in a secondary task. The edge reconstruction task helps the network preserve image structure during feature stylization, while also acting as a regularizer for the classification task. Through quantitative comparisons, we verify the effectiveness of our method upon existing state-of-the-art methods on PACS, VLCS, OfficeHome, DomainNet and Digits-DG. The implementation is available at this repository .},
  archive      = {J_CVIU},
  author       = {Milad Cheraghalikhani and Mehrdad Noori and David Osowiechi and Gustavo A. Vargas Hakim and Ismail Ben Ayed and Christian Desrosiers},
  doi          = {10.1016/j.cviu.2024.104016},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104016},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Structure-aware feature stylization for domain generalization},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view cognition with path search for one-shot part
labeling. <em>CVIU</em>, <em>244</em>, 104015. (<a
href="https://doi.org/10.1016/j.cviu.2024.104015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagram is an abstract form of visual expression in the field of education, which is often used to express complex phenomena and convey logic relationships. In recent years, tasks such as diagram classification and textbook question answering have attracted attention and become a new benchmark for evaluating the complex reasoning ability of models. However, due to the lack of large corpora and the abstract and sparse visual expressions, it is difficult for research methods on natural images to achieve good results on diagrams. In order to solve the above challenges, the researchers consider using the one-shot setting for limited samples challenge and using part labeling to enhance the learning of relational structures. By definition, the one-shot part labeling task is to label multiple parts of an object in the query diagram given only a single support diagram of that category. Under this setting, we propose the Automated Search Multi-view Matching Network (Auto-MMN) which simulating human cognitive methods and process of set-to-set matching problem. We define three views operations based on the attention mechanism and multiplex graph, including the learning of global visual features (global–local view), the interaction between neighboring parts (local–local view), and the comparison of counterparts (cross-local view). We propose a novel learning path search technology to adaptively plan paths for the above three views, which can also increase the generalization performance of the model. We evaluate the Auto-MMN on three different datasets, that is, image-to-image, diagram-to-diagram, and image-to-diagram part labeling scenarios. Extensive experiments show that our model significantly outperforms other baselines on different scenarios and both the multi-view operations and the learning path search produce excellent results. We open source the core code in https://github.com/WayneWong97/Auto-MMN .},
  archive      = {J_CVIU},
  author       = {Shaowei Wang and Lingling Zhang and Tao Qin and Jun Liu and Yifei Li and Qianying Wang and Qinghua Zheng},
  doi          = {10.1016/j.cviu.2024.104015},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104015},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-view cognition with path search for one-shot part labeling},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TFUT: Task fusion upward transformer model for multi-task
learning on dense prediction. <em>CVIU</em>, <em>244</em>, 104014. (<a
href="https://doi.org/10.1016/j.cviu.2024.104014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based advancements have shown great promise in solving multi-task learning on dense prediction tasks. Well-designed task interaction modules of these methods further improve the performances by effectively transferring contextual information between tasks. However, many of these methods do not leverage the target task to guide contextual information from the source task. We propose the Task Fusion Upward Transformer (TFUT) model for multi-task learning on dense prediction. To facilitate task interaction, we introduce the Asymmetric Cross Task Interaction module, which utilizes asymmetric transmission in attention. During similarity calculations, the model leverages the target task to guide the expression of contextual information from the source task, ensuring effective transmission of the context information. In order to avoid the loss of detail and the discontinuity of gradient in upsampling, the Upward Transformer Decoder is designed to extract and align multi-scale features using multi-level convolution. The effectiveness of the proposed model has been demonstrated through experiments on the NYUD-v2 dataset and the PASCAL Context dataset. The experimental results show that this model has achieved optimal performance in various single task and multi-task scenarios.},
  archive      = {J_CVIU},
  author       = {Zewei Xin and Shalayiding Sirejiding and Yuxiang Lu and Yue Ding and Chunlin Wang and Tamam Alsarhan and Hongtao Lu},
  doi          = {10.1016/j.cviu.2024.104014},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104014},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TFUT: Task fusion upward transformer model for multi-task learning on dense prediction},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CTM: Cross-time temporal module for fine-grained action
recognition. <em>CVIU</em>, <em>244</em>, 104013. (<a
href="https://doi.org/10.1016/j.cviu.2024.104013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic contextual attribute information in the time dimension is the key to fine-grained action recognition. Temporal contextual relationships cannot be captured by conventional 2D CNNs; good local time can be obtained by the 3D CNNs, but the 3D CNNs are computationally intensive and lack capability for global time. A parallel cross-time temporal module-CTM is proposed in this article, which aims to efficiently capture dynamic contextual information of both local and global temporal dimensions. With our study, we think that the 2D CNNs can better mine temporal features to enrich the contextual relationships of temporal dimensions. The CTM can be embedded into any existing 2D CNNs baseline in a plug-and-play manner, yielding a feature framework that can capture complex spatio-temporal modeling (CTNet) with a tiny additional computational cost. In the extensive validation experiments on three datasets(i.e., SomethingV1&amp;V2, Jester, Diving48), both action recognition accuracy and runtime inference speed are obviously better than existing temporal contextual baseline optimization schemes with similar computational cost complexity, when the CTM embedded into any 2D CNNs framework to enhance the baseline.},
  archive      = {J_CVIU},
  author       = {Huifang Qian and Jialun Zhang and Jianping Yi and Zhenyu Shi and Yimin Zhang},
  doi          = {10.1016/j.cviu.2024.104013},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104013},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CTM: Cross-time temporal module for fine-grained action recognition},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Static graph convolution with learned temporal and
channel-wise graph topology generation for skeleton-based action
recognition. <em>CVIU</em>, <em>244</em>, 104012. (<a
href="https://doi.org/10.1016/j.cviu.2024.104012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) are widely used in skeleton-based action recognition. It is known that the graph topology is a vital part in GCNs, and different kinds of graph topologies have been proposed for skeleton-based action recognition, mostly based on a predefined topology and a dynamically learned one. The predefined topology is based on the human intuition for skeleton (the connectivity of joints) and has not been investigated whether it is optimal. In this paper, we focus on investigating this static graph topology and propose to generate a learned static graph topology for skeleton. To be specific, a temporal frame-wise and channel-wise topology-based GCNs (TC-GCNs) are developed, where, instead of using a predefined topology by human, a topology is learned for skeleton-based action recognition. The TC-GCNs consist of generating a temporal frame-wise topology and a channel-wise topology to formulate the relationship of skeleton joints in the temporal dimension and channel dimension, respectively. The proposed method can be integrated with the conventional dynamic topology by replacing the predefined graph topology with our generated one. Experimental results show that our method with learned static graph achieves better performance than the predefined graph and dynamic graph on three widely used benchmarks, namely the NTU-RGB+D, NTU-RGB+D 120 and UAV-Human.},
  archive      = {J_CVIU},
  author       = {Chuankun Li and Shuai Li and Yanbo Gao and Lijuan Zhou and Wanqing Li},
  doi          = {10.1016/j.cviu.2024.104012},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104012},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Static graph convolution with learned temporal and channel-wise graph topology generation for skeleton-based action recognition},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FusionDiff: A unified image fusion network based on
diffusion probabilistic models. <em>CVIU</em>, <em>244</em>, 104011. (<a
href="https://doi.org/10.1016/j.cviu.2024.104011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces FusionDiff, a novel unified end-to-end image fusion network based on diffusion probabilistic models. The proposed method addresses the data dependency issue present in current unified image fusion approaches. Initially, the method pre-fuses the source images after diffusion. Subsequently, a noise prediction network forecasts the noise applied to the pre-fused image. During this stage, the Spatially-Adaptive Constraint layer is employed to restrict the fusion image, thereby maximizing the preservation of respective source image features. Finally, in the reverse process, skip-sampling is employed to effectively overcome the computational time drawback of traditional diffusion probabilistic models while maintaining high-quality image generation. Compared to other diffusion model-based image fusion methods, our approach stands out for its structural simplicity and ease of training, serving as a unified image fusion method. Furthermore, compared to other unified image fusion methods, our proposed network fully leverages the generalization of the diffusion probabilistic model, achieving adaptive feature extraction by constraining the inverse process of source images. Qualitative and quantitative experimental results across four classic image fusion tasks demonstrate the superiority of our method over state-of-the-art unified image fusion methods in recent years.},
  archive      = {J_CVIU},
  author       = {Zefeng Huang and Shen Yang and Jin Wu and Lei Zhu and Jin Liu},
  doi          = {10.1016/j.cviu.2024.104011},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104011},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FusionDiff: A unified image fusion network based on diffusion probabilistic models},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recognizing facial expressions based on pyramid multi-head
grid and spatial attention network. <em>CVIU</em>, <em>244</em>, 104010.
(<a href="https://doi.org/10.1016/j.cviu.2024.104010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) is garnered considerable interest in the field of computer vision. Being a challenging task, it faces some key problems such as inter-class similarity, intra-class variability, and environment sensitivity. Typically, the traditional Convolutional Neural Networks (CNN) are limited by their locality and thus have difficulty learning long-range dependencies between elements in the image, which leads to decreased performance. A innovative expression analysis system that relies on a pyramid multi-head grid and spatial attention network (PMAN) is presented to address these issues. The PMAN is divided into two stages: the initial feature extraction stage, in which the correlations between various facial zones are learned using Multi-head Grid Attention (MGA), and the deep feature learning stage, in which Multi-head Spatial Attention (MSA) is employed in order to improve the model’s global attention to facial features. In addition, a unique feature pyramid design is implemented at the deep feature learning stage to diminish the network’s sensitivity to face image size. The experiments show that the PMAN performs significantly not only better than the existing methods in terms of CK+, RAF-DB, FER+, and AffectNet but also achieves 100% accuracy on the CK+ dataset without using pre-trained models.},
  archive      = {J_CVIU},
  author       = {Jianyang Zhang and Wei Wang and Xiangyu Li and Yanjiang Han},
  doi          = {10.1016/j.cviu.2024.104010},
  journal      = {Computer Vision and Image Understanding},
  month        = {7},
  pages        = {104010},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Recognizing facial expressions based on pyramid multi-head grid and spatial attention network},
  volume       = {244},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor robust PCA with nonconvex and nonlocal
regularization. <em>CVIU</em>, <em>243</em>, 104007. (<a
href="https://doi.org/10.1016/j.cviu.2024.104007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor robust principal component analysis (TRPCA) is a classical way for low-rank tensor recovery, which minimizes the convex surrogate of tensor rank by shrinking each tensor singular value equally. However, for real-world visual data, large singular values represent more significant information than small singular values. In this paper, we propose a nonconvex TRPCA (N-TRPCA) model based on the tensor adjustable logarithmic norm. Unlike TRPCA, our N-TRPCA can adaptively shrink small singular values more and shrink large singular values less. In addition, TRPCA assumes that the whole data tensor is of low rank. This assumption is hardly satisfied in practice for natural visual data, restricting the capability of TRPCA to recover the edges and texture details from noisy images and videos. To this end, we integrate nonlocal self-similarity into N-TRPCA, and further develop a nonconvex and nonlocal TRPCA (NN-TRPCA) model. Specifically, similar nonlocal patches are grouped as a tensor and then each group tensor is recovered by our N-TRPCA. Since the patches in one group are highly correlated, all group tensors have strong low-rank property, leading to an improvement of recovery performance. Experimental results demonstrate that the proposed NN-TRPCA outperforms existing TRPCA methods in visual data recovery. The demo code is available at https://github.com/qguo2010/NN-TRPCA .},
  archive      = {J_CVIU},
  author       = {Xiaoyu Geng and Qiang Guo and Shuaixiong Hui and Ming Yang and Caiming Zhang},
  doi          = {10.1016/j.cviu.2024.104007},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104007},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Tensor robust PCA with nonconvex and nonlocal regularization},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary search via channel attention based parameter
inheritance and stochastic uniform sampled training. <em>CVIU</em>,
<em>243</em>, 104000. (<a
href="https://doi.org/10.1016/j.cviu.2024.104000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks’ performance is significantly affected by their architecture. Different algorithms have been developed to aid in automating the network structure selection. Recently, evolutionary neural architecture search (ENAS) methods have gained increasing number of attentions due to their excellent global search capability. Nevertheless, evolutionary neural architecture can be computationally expensive since improving evolution typically involves many performance evaluations. This study introduces a framework utilizing evolutionary search for a lightweight channel attention convolutional network. It leverages stochastic uniform sampled and parameter inheritance to address these challenges. The lightweight channel attention method is applied to the model to make it more capable of extracting features while using fewer parameters. In this framework, parent individuals undergo stochastic sampling and receive training repeatedly on each mini-batch of the training dataset. The strategy of parameter inheritance enables the assessment of offspring individuals’ fitness without fully training the offspring candidate network. Compared to existing methods on standard-bench (CIFAR-10, CIFAR-100, ImageNet) and real-world datasets(NEU-CLS, Chest Xray2017), our model demonstrated superior search efficiency by several orders of magnitude. Our experiments showed that Stochastic uniform sampled evolutionary Network (SUSE-Net) achieved the lowest error rate among state-of-the-art NAS models, with 2.55% for CIFAR-10, 14.8% for CIFAR-100, and 23.2% for ImageNet dataset. This results indicate that the proposed algorithm is not only more computationally efficient, but also competitive with existing methods in terms of learning performance.},
  archive      = {J_CVIU},
  author       = {Yugang Liao and Junqing Li and Shuwei Wei and Xiumei Xiao},
  doi          = {10.1016/j.cviu.2024.104000},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {104000},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Evolutionary search via channel attention based parameter inheritance and stochastic uniform sampled training},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Head pose estimation with uncertainty and an application to
dyadic interaction detection. <em>CVIU</em>, <em>243</em>, 103999. (<a
href="https://doi.org/10.1016/j.cviu.2024.103999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the visual focus of attention of people in a scene is a fundamental cue to understand social interactions from videos. Gaze direction is ideal for determining eye contact, a basic cue of non-verbal communication, but it is not always easy to recognize. Head direction is a well-known proxy of gaze direction, more robust to the variability of the scene, thus offering a valuable alternative. In this work, we consider HHP-net, a method for estimating the head direction from single frames based on a heteroscedastic neural network to estimate people’s head pose from a minimal set of head key points. We formulate the problem as a multi-task regression, to predict the pose as a triplet of Euler angles from the output of a 2D pose estimator. HHP-net also provides a measure of the aleatoric heteroscedastic uncertainties associated with the angles, through an ad-hoc loss function we introduce. In a thorough experimental analysis, we show that our model is efficient and effective compared with the state of the art, with only ∼ ∼ 2 degrees of degradation in the worst case counterbalanced by a space occupation ∼ ∼ 12 times smaller. We also show the beneficial effects of uncertainty on interpretability. Finally, we discuss the robustness of our method to input variability, showing that it can be seen as a plug-in to different pose estimators. As a proof-of-concept, we address social interaction analysis, with an algorithm to detect dyadic interactions in images.},
  archive      = {J_CVIU},
  author       = {Federico Figari Tomenotti and Nicoletta Noceti and Francesca Odone},
  doi          = {10.1016/j.cviu.2024.103999},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103999},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Head pose estimation with uncertainty and an application to dyadic interaction detection},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-guided-based image matting via boundary detection.
<em>CVIU</em>, <em>243</em>, 103998. (<a
href="https://doi.org/10.1016/j.cviu.2024.103998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing automatic matting methods tend to directly obtain the alpha mattes from the RGB image using semantic segmentation networks, and relying solely on segmentation to achieve high-quality estimation is usually unrealistic. To address this issue, we propose a multi-guided-based image matting (MGBMatting) model that utilizes boundary information and semantic features as comprehensive and sufficient guidance, increasing attention to the unknown regions in the trimap, which is often a challenging aspect of matting tasks. The boundary-extracted module we introduced in MGBMatting effectively enhances the boundary features of the foreground. In addition, the boundary optimization module effectively achieves spatial consistency in features as well as enhances the representational power of the features. Further, the dual-stream encoder increases the possibility of capturing both local features and long-range feature dependencies. We use two widely used image matting datasets, Composition-1k and Distinctions-646, to evaluate our MGBMatting. Extensive experimentation demonstrates that the proposed MGBMatting yields significant performance.},
  archive      = {J_CVIU},
  author       = {Guilin Yao and Anming Sun},
  doi          = {10.1016/j.cviu.2024.103998},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103998},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-guided-based image matting via boundary detection},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The 2023 video similarity dataset and challenge.
<em>CVIU</em>, <em>243</em>, 103997. (<a
href="https://doi.org/10.1016/j.cviu.2024.103997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a dataset, benchmark, and challenge for the problem of video copy tracing. There are two related tasks: determining whether a query video shares content with a reference video (“detection”) and temporally localizing the shared content within each video (“localization”). The benchmark is designed to evaluate methods on these two tasks. It simulates a realistic needle-in-haystack setting, where the majority of both query and reference videos are “distractors” containing no copied content. We propose an accuracy metric for both tasks. The associated challenge imposes computing resource restrictions that reflect real-world settings. We also analyze the results and methods of the top submissions to the challenge. The dataset, baseline methods, and evaluation code are publicly available and were discussed at the Visual Copy Detection Workshop (VCDW) at CVPR’23. We provide reference code for evaluation and baselines at: https://github.com/facebookresearch/vsc2022 .},
  archive      = {J_CVIU},
  author       = {Ed Pizzi and Giorgos Kordopatis-Zilos and Hiral Patel and Gheorghe Postelnicu and Sugosh Nagavara Ravindra and Akshay Gupta and Symeon Papadopoulos and Giorgos Tolias and Matthijs Douze},
  doi          = {10.1016/j.cviu.2024.103997},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103997},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {The 2023 video similarity dataset and challenge},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel camera calibration method based on known rotations
and translations. <em>CVIU</em>, <em>243</em>, 103996. (<a
href="https://doi.org/10.1016/j.cviu.2024.103996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is often difficult to align the camera optical center with the rotation center of the motion mechanism during active vision calibration, and this dis-alignment could lead to inaccuracy of the constraint equations and calibration results. To circumvent such issues, this paper proposes a novel method for active vision camera calibration and facilitates its real-world implementation. In this method, the rotational motion axis is assumed not necessarily to pass through the camera’s optical center, and the constraint equations are established by correlating image matched points before and after camera motions. The displacements caused by rotation are incorporated into the constraint equations to improve the accuracy of calibration. Numerical simulation and experiments are conducted. The feasibility of this proposed method is justified by using both synthetic and experimental data, and the results show that this method could estimate camera intrinsic parameters with high accuracy. Furthermore, it is found that small rotation angles can better the calibration accuracy even under the influence of external noises. This study suggests that this novel method has great potential for visual and image applications.},
  archive      = {J_CVIU},
  author       = {Zhanfei Chen and Xuelong Si and Dan Wu and Fengnian Tian and Zhenxing Zheng and Renfu Li},
  doi          = {10.1016/j.cviu.2024.103996},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103996},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel camera calibration method based on known rotations and translations},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GaitSCM: Causal representation learning for gait
recognition. <em>CVIU</em>, <em>243</em>, 103995. (<a
href="https://doi.org/10.1016/j.cviu.2024.103995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition is a promising biometric technology that aims to identify the target subject via walking pattern. Most existing appearance-based methods focus on learning discriminative spatio-temporal representations from gait silhouettes. However, these methods pay less attention to probing the causality between identity factors and identity labels, which often mislead the model to learn gait representations that are susceptible to identity-irrelevant factors. In this paper, we attribute the cause that leads to the decline of model generalization under different external conditions to identity-irrelevant factors. We formulate the causalities among the identity factors, identity-irrelevant factors, and identity labels as a structural causal model (SCM). We accordingly propose a novel gait recognition framework named GaitSCM to learn covariate invariant gait representations, which is mainly composed of three components, including feature extraction module, feature disentanglement module, and backdoor adjustment. Specifically, we design a feature extractor with regard to the movement patterns of different body parts to learn fine-grained gait motion features, and then present a two-branch feature decoupling module to disentangle identity features and identity-irrelevant features with the aid of the classification confusion loss. To relieve the negative effect of identity-irrelevant factors, we develop a backdoor adjustment strategy to eliminate spurious associations between identity and identity-irrelevant features, which further facilitates the proposed framework to generate more powerful identity representations. Extensive experiments conducted on two public datasets validate the effectiveness of our method. The average Rank-1 can reach 93.2% and 90.4% on CASIA-B and OU-MVLP datasets, respectively, which verifies the superiority of GaitSCM. Source code is released at: https://github.com/HuoweiCode/GaitSCM .},
  archive      = {J_CVIU},
  author       = {Wei Huo and Ke Wang and Jun Tang and Nian Wang and Dong Liang},
  doi          = {10.1016/j.cviu.2024.103995},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103995},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GaitSCM: Causal representation learning for gait recognition},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SlowFastFormer for 3D human pose estimation. <em>CVIU</em>,
<em>243</em>, 103992. (<a
href="https://doi.org/10.1016/j.cviu.2024.103992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D human pose estimation in videos aims at locating the human joints in the 3D space given a temporal sequence. Motion information and skeleton context are two significant elements for pose estimation in videos. In this paper, we propose a SlowFastFormer (slow-fast transformer) network where two branches with different input rates are composed to encode these two different kinds of context. For the slow branch, skeleton context is well learned at a higher frame rate. For the fast branch, motion information is captured at a lower frame rate. Through these two branches, different kinds of context are encoded separately. We fuse these two branches at a later stage to fully utilize the skeleton context and motion information. Afterwards, a blending module is developed to promote the message exchange among multiple branches. In the blending stage, different kinds of context information are exchanged and feature representation is enhanced consequently. Lastly, a hierarchical supervision scheme is tailored where predictions of different levels are inferred in a progressive manner. Our approach achieves competitive performance with lower computation complexity on several benchmarks, i.e., Human3.6M, MPI-INF-3DHP and HumanEva-I.},
  archive      = {J_CVIU},
  author       = {Lu Zhou and Yingying Chen and Jinqiao Wang},
  doi          = {10.1016/j.cviu.2024.103992},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103992},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SlowFastFormer for 3D human pose estimation},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning single and multi-scene camera pose regression with
transformer encoders. <em>CVIU</em>, <em>243</em>, 103982. (<a
href="https://doi.org/10.1016/j.cviu.2024.103982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary state-of-the-art localization methods perform feature matching against a structured scene model or learn to regress the scene 3D coordinates. The resulting matches between 2D query pixels and 3D scene coordinates are used to estimate the camera pose using PnP and RANSAC, requiring the camera intrinsics for both the query and reference images. An alternative approach is to directly regress the camera pose from the query image. Although less accurate, absolute camera pose regression does not require any additional information at inference time and is typically lightweight and fast. Recently, Transformers were proposed for learning multi-scene camera pose regression, employing encoders to attend to spatially varying deep features while using decoders to embed multiple scene queries at once. In this work, we show that Transformer Encoders can aggregate and extract task-informative latent representations for learning both single- and multi- scene camera pose regression, without Transformer-Decoders. Our approach is shown to reduce the runtime and memory of previous Transformer-based multi-scene solutions, while comparing favorably with contemporary pose regression schemes and achieving state-of-the-art accuracy on multiple indoor and outdoor regression benchmarks. In particular, to the best of our knowledge, our approach is the first absolute regression approach to attain sub-meter average accuracy across outdoor scenes. We make our code publicly available at: https://github.com/yolish/transposenet .},
  archive      = {J_CVIU},
  author       = {Yoli Shavit and Ron Ferens and Yosi Keller},
  doi          = {10.1016/j.cviu.2024.103982},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103982},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning single and multi-scene camera pose regression with transformer encoders},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain-aware triplet loss in domain generalization.
<em>CVIU</em>, <em>243</em>, 103979. (<a
href="https://doi.org/10.1016/j.cviu.2024.103979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the considerable advances in deep learning for object recognition, there are still several factors that hinder the performance of deep learning models. One of these factors is domain shift, which occurs due to variations in the distribution of the testing and training data. This paper addresses the issue of compact feature clustering in domain generalization, with the aim of optimizing the embedding space from multi-domain data. Specifically, we propose a domain-aware triplet loss for domain generalization, which not only facilitates clustering of similar semantic features but also disperses features that arise from the domain. Unlike previous methods that focus on aligning distributions, our algorithm disperses domain information in the embedding space. Our approach is based on the assumption that embedding features can be clustered based on domain information, which is supported mathematically and empirically in this paper. Furthermore, in our investigation of feature clustering in domain generalization, we observe that the factors that influence the convergence of metric learning loss in domain generalization are more significant than the pre-defined domains. To address this issue, we utilize two methods to normalize the embedding space and reduce the internal covariate shift of the embedding features. Our ablation study illustrates the effectiveness of our algorithm. Additionally, our experiments on benchmark datasets, including PACS, VLCS, and Office-Home, demonstrate that our method outperforms related approaches that focus on domain discrepancy. Notably, our results on RegnetY-16GF are substantially better than state-of-the-art methods on the benchmark datasets. Our code is available at https://github.com/workerbcd/DCT .},
  archive      = {J_CVIU},
  author       = {Kaiyu Guo and Brian C. Lovell},
  doi          = {10.1016/j.cviu.2024.103979},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103979},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Domain-aware triplet loss in domain generalization},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visual tracking in camera-switching outdoor sport videos:
Benchmark and baselines for skiing. <em>CVIU</em>, <em>243</em>, 103978.
(<a href="https://doi.org/10.1016/j.cviu.2024.103978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skiing is a globally popular winter sport discipline with a rich history of competitive events. This domain offers ample opportunities for the application of computer vision to enhance the understanding of athletes’ performances. However, this potential has remained relatively untapped in comparison to other sports, primarily due to the limited availability of dedicated research studies and datasets. The present paper takes a significant stride towards bridging these gaps. It conducts a comprehensive examination of skier appearance tracking in videos capturing their entire performance—an essential step for more advanced performance analyses. To implement this investigation, we introduce SkiTB, the largest and most annotated dataset tailored for computer vision applications in skiing. We subject a range of visual object tracking algorithms to rigorous testing, including both well-established methodologies and a novel skier-specific baseline algorithm. The results yield valuable insights into the suitability of various tracking techniques for vision-based skiing analysis and into the generalization of state-of-the-art algorithms to complex target behaviors and conditions set by winter outdoor environments. To foster further development, we make SkiTB, the associated code, and the obtained results accessible through https://machinelearning.uniud.it/datasets/skitb .},
  archive      = {J_CVIU},
  author       = {Matteo Dunnhofer and Christian Micheloni},
  doi          = {10.1016/j.cviu.2024.103978},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103978},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Visual tracking in camera-switching outdoor sport videos: Benchmark and baselines for skiing},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scribble-based complementary graph reasoning network for
weakly supervised salient object detection. <em>CVIU</em>, <em>243</em>,
103977. (<a href="https://doi.org/10.1016/j.cviu.2024.103977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current salient object detection (SOD) methods rely heavily on accurate pixel-level annotations. To reduce the annotation workload, some scribble-based methods have emerged. Recent works address the sparse scribble annotations by introducing auxiliary information and enhancing local features. However, the impact of long-range dependence between pixels on energy propagation and model performance has not been explored in this field. In this paper, we propose a novel complementary graph reasoning network (CGRNet), which globally infers relationships between salient regions by building graph representations. Specifically, we introduce a dual-stream cross-interactive graph reasoning pipeline to model high-level representations and incorporate efficient graph cooperation unit (GCU) to adaptively select complementary components from the representations. Additionally, considering the lack of structural information in scribble data, we design an edge-oriented module (EOM) to explicitly mine boundary semantics. Finally, we propose a dense fusion strategy (DFS) to aggregate multi-source semantics in a multi-guidance manner for obtaining complete global information. Experimental and visual results on five benchmarks demonstrate the superiority of our proposed CGRNet.},
  archive      = {J_CVIU},
  author       = {Shuang Liang and Zhiqi Yan and Chi Xie and Hongming Zhu and Jiewen Wang},
  doi          = {10.1016/j.cviu.2024.103977},
  journal      = {Computer Vision and Image Understanding},
  month        = {6},
  pages        = {103977},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Scribble-based complementary graph reasoning network for weakly supervised salient object detection},
  volume       = {243},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploiting multimodal synthetic data for egocentric
human-object interaction detection in an industrial scenario.
<em>CVIU</em>, <em>242</em>, 103984. (<a
href="https://doi.org/10.1016/j.cviu.2024.103984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we tackle the problem of Egocentric Human-Object Interaction (EHOI) detection in an industrial setting. To overcome the lack of public datasets in this context, we propose a pipeline and a tool for generating synthetic images of EHOIs paired with several annotations and data signals (e.g., depth maps or segmentation masks). Using the proposed pipeline, we present EgoISM-HOI a new multimodal dataset composed of synthetic EHOI images in an industrial environment with rich annotations of hands and objects. To demonstrate the utility and effectiveness of synthetic EHOI data produced by the proposed tool, we designed a new method that predicts and combines different multimodal signals to detect EHOIs in RGB images. Our study shows that exploiting synthetic data to pre-train the proposed method significantly improves performance when tested on real-world data. Moreover, to fully understand the usefulness of our method, we conducted an in-depth analysis in which we compared and highlighted the superiority of the proposed approach over different state-of-the-art class-agnostic methods. To support research in this field, we publicly release the datasets, source code, and pre-trained models at https://iplab.dmi.unict.it/egoism-hoi .},
  archive      = {J_CVIU},
  author       = {Rosario Leonardi and Francesco Ragusa and Antonino Furnari and Giovanni Maria Farinella},
  doi          = {10.1016/j.cviu.2024.103984},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {103984},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploiting multimodal synthetic data for egocentric human-object interaction detection in an industrial scenario},
  volume       = {242},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cascade transformers with dynamic attention for video
question answering. <em>CVIU</em>, <em>242</em>, 103983. (<a
href="https://doi.org/10.1016/j.cviu.2024.103983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual question answering (VQA) has become a hot study topic with challenging motivation of correctly answering the videos or images questions in recent years. However, the existing VQA model mostly aimed at answering questions about images and performed poorly in the video question answering (VideoQA) domain. VideoQA needs to simultaneously consider the correlations between video frames and the dynamic information of multiple objects in video. Therefore, we propose a novel Cascade Transformers with Dynamic Attention for Video Question Answering (CTDA-QA), which aims to simultaneously solve the above considerations. Specifically, the proposed CTDA-QA model utilizes multiple transformers structure to encode videos for reasoning complex spatial and temporal information, which is different from the previous recurrent neural network methods. Besides, in order to effectively capture the dynamic information from various scenarios in videos, a flexible attention module has been proposed to explore the essential relations between objects in a dynamic timeline. Finally, to avoid spurious answers and fully explore the cross-modal relationships, a mixed-supervised learning strategy is designed for optimizing the reasoning tasks. The experiments on several benchmark video question–answer datasets clearly verify the performance and effectiveness of CTDA-QA, which contains the results in contrast to the state-of-the-art methods. Besides, the provided ablation study and visualization results further reveal the potential of CTDA-QA.},
  archive      = {J_CVIU},
  author       = {Yimin Jiang and Tingfei Yan and Mingze Yao and Huibing Wang and Wenzhe Liu},
  doi          = {10.1016/j.cviu.2024.103983},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {103983},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cascade transformers with dynamic attention for video question answering},
  volume       = {242},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FAM: Improving columnar vision transformer with feature
attention mechanism. <em>CVIU</em>, <em>242</em>, 103981. (<a
href="https://doi.org/10.1016/j.cviu.2024.103981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformer has garnered outstanding performance in visual tasks due to its capability for global modeling of image information. However, during the self-attention computation of image tokens, a common issue of attention map homogenization arises, impacting the final performance of the model as attention maps propagate through feature maps layer by layer. In this research, we propose a token-based approach to adjust the output of attention sub-layer, focusing on the feature dimensions, to address the homogenization problem. Furthermore, different network architectures exhibit variations in their approaches to modeling image features. Specifically, Vision Transformers excel at modeling long-range relationships, while convolutional neural networks possess local receptive fields. Therefore, this paper introduces a plug-and-play convolutional operator-based component, integrated into the Vision Transformer, to validate the impact of structural enhancements on model performance. Experimental results on image recognition and adversarial attack tasks respectively demonstrate the effectiveness and robustness of the two proposed methods. Additionally, the analysis of information entropy on the feature maps of the model’s final layer indicates that the improved model exhibits higher information richness, making it more conducive to the classifier’s discriminative capabilities.},
  archive      = {J_CVIU},
  author       = {Lan Huang and Xingyu Bai and Jia Zeng and Mengqiang Yu and Wei Pang and Kangping Wang},
  doi          = {10.1016/j.cviu.2024.103981},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {103981},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FAM: Improving columnar vision transformer with feature attention mechanism},
  volume       = {242},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end dense video grounding via parallel regression.
<em>CVIU</em>, <em>242</em>, 103980. (<a
href="https://doi.org/10.1016/j.cviu.2024.103980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video grounding aims to localize the corresponding moment in an untrimmed video given a sentence description. Existing methods often address this task in an indirect “one-to-many” way, i.e., predicting more than one proposal for one sentence description, by casting it as a propose-and-match or fusion-and-detection problem. Solving these surrogate problems often requires sophisticated label assignment during training and hand-crafted removal of near-duplicate results. Meanwhile, existing works typically focus on sparse video grounding with a single sentence as input, which could result in ambiguous localization due to its unclear description. In this paper, we tackle a new problem of dense video grounding, by simultaneously localizing multiple moments with a paragraph as input. From a perspective on video grounding as language-conditioned regression, we present an end-to-end parallel decoding paradigm by re-purposing a Transformer-alike architecture (PRVG). The key design in our PRVG is to use languages as queries, and regress only one temporal boundary for each sentence based on language-modulated visual representations. Thanks to its simplicity in design, our PRVG framework predicts in a “one-to-one” manner, getting rid of complicated label assignment during training and allowing for efficient inference without any post-processing technique. In addition, we devise a robust proposal-level attention loss to guide the training of PRVG, which is invariant to moment duration and contributes to model convergence. We perform experiments on two benchmarks, namely ActivityNet Captions and TACoS, demonstrating the superiority of PRVG. We also perform in-depth studies to investigate the effectiveness of the parallel regression paradigm on video grounding.},
  archive      = {J_CVIU},
  author       = {Fengyuan Shi and Weilin Huang and Limin Wang},
  doi          = {10.1016/j.cviu.2024.103980},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {103980},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {End-to-end dense video grounding via parallel regression},
  volume       = {242},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Background no more: Action recognition across domains by
causal interventions. <em>CVIU</em>, <em>242</em>, 103975. (<a
href="https://doi.org/10.1016/j.cviu.2024.103975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim to recognize actions under an appearance distribution shift between a source training domain and a target test domain. To enable such video domain generalization, our key idea is to intervene on the action to remove the confounding effect of the domain background on the class label using causal inference. Towards this, we propose to learn a causally debiased model on a source domain that intervenes on the action through three possible D o Do -operators, which separate the action and background. To better align the source and target distributions, we also introduce a test-time action intervention. Experiments on two challenging video domain generalization benchmarks reveal that causal inference is a promising tool for action recognition as it already achieves state-of-the-art results on Kinetics2Mimetics, the benchmark with the largest domain shift.},
  archive      = {J_CVIU},
  author       = {Sarah Rastegar and Hazel Doughty and Cees G.M. Snoek},
  doi          = {10.1016/j.cviu.2024.103975},
  journal      = {Computer Vision and Image Understanding},
  month        = {5},
  pages        = {103975},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Background no more: Action recognition across domains by causal interventions},
  volume       = {242},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple contrastive learning in a self-supervised manner for
robust visual question answering. <em>CVIU</em>, <em>241</em>, 103976.
(<a href="https://doi.org/10.1016/j.cviu.2024.103976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent observations have revealed that Visual Question Answering models are susceptible to learning the spurious correlations formed by dataset biases, i.e., the language priors, instead of the intended solution. For instance, given a question and a relative image, some VQA systems are prone to provide the frequently occurring answer in the dataset while disregarding the image content. Such a preferred tendency has caused them to be brittle in real-world settings, harming the robustness of VQA models. We experimentally found that conventional VQA methods often confuse negative samples that with identical questions but different images, which results in the generation of linguistic bias. In this paper, we propose a simple contrastive learning scheme, namely SCLSM, to mitigate the above issues in a self-supervised manner. We construct several special negative samples and introduce a debiasing-aware contrastive learning approach to help the model learn more discriminative multimodal features, thus improving the ability of debiasing. The SCLSM is compatible with numerous VQA baselines. Experimental results on the widely-used public datasets VQA-CP v2 and VQA v2 validate the effectiveness of our proposed model.},
  archive      = {J_CVIU},
  author       = {Shuwen Yang and Luwei Xiao and Xingjiao Wu and Junjie Xu and Linlin Wang and Liang He},
  doi          = {10.1016/j.cviu.2024.103976},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103976},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Simple contrastive learning in a self-supervised manner for robust visual question answering},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SpATr: MoCap 3D human action recognition based on spiral
auto-encoder and transformer network. <em>CVIU</em>, <em>241</em>,
103974. (<a href="https://doi.org/10.1016/j.cviu.2024.103974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent technological advancements have significantly expanded the potential of human action recognition through harnessing the power of 3D data. This data provides a richer understanding of actions, including depth information that enables more accurate analysis of spatial and temporal characteristics. In this context, We study the challenge of 3D human action recognition. Unlike prior methods, that rely on sampling 2D depth images, skeleton points, or point clouds, often leading to substantial memory requirements and the ability to handle only short sequences, we introduce a novel approach for 3D human action recognition, denoted as SpATr (Spiral Auto-encoder and Transformer Network), specifically designed for fixed-topology mesh sequences. The SpATr model disentangles space and time in the mesh sequences. A lightweight auto-encoder, based on spiral convolutions, is employed to extract spatial geometrical features from each 3D mesh. These convolutions are lightweight and specifically designed for fix-topology mesh data. Subsequently, a temporal transformer, based on self-attention, captures the temporal context within the feature sequence. The self-attention mechanism enables long-range dependencies capturing and parallel processing, enabling scalability for long sequences. The proposed method is evaluated on three prominent 3D human action datasets: Babel, MoVi, and BMLrub, from the Archive of Motion Capture As Surface Shapes (AMASS). Our results analysis demonstrates the competitive performance of our SpATr model in 3D human action recognition while maintaining efficient memory usage. The code and the training results are publicly available at https://github.com/h-bouzid/spatr .},
  archive      = {J_CVIU},
  author       = {Hamza Bouzid and Lahoucine Ballihi},
  doi          = {10.1016/j.cviu.2024.103974},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103974},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {SpATr: MoCap 3D human action recognition based on spiral auto-encoder and transformer network},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning key lines for multi-object tracking. <em>CVIU</em>,
<em>241</em>, 103973. (<a
href="https://doi.org/10.1016/j.cviu.2024.103973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most online multi-object tracking methods utilize bounding boxes and center points inherited from detectors as the base models to represent targets. Limited performance is obtained with these base models alone for tracking. Complex networks are generally applied on top to extract high-level discriminative features such as appearance embeddings and motion predictions for data association. However, the weakness in the feature representation of bounding boxes and center points degrades the tracking performance. In this paper, we propose a novel base model that represents targets with key lines for tracking, which can provide discriminative features and accurate target affinity measurements. Besides, we use the proposed key lines to select low-scored detections and unmatched tracks to recover missed targets and enhance identity consistency. Based on this, we apply the proposed line-based modeling strategy to existing trackers and propose a line-based Cascade Tracking algorithm to associate targets in three stages, and very competitive results are achieved on MOTChallenge benchmarks. Extensive experiments with improved performances demonstrate the effectiveness and generalization of key lines in providing discriminative features and enhancing tracking performance.},
  archive      = {J_CVIU},
  author       = {Yi-Fan Li and Hong-Bing Ji and Xi Chen and Yong-Liang Yang and Yu-Kun Lai},
  doi          = {10.1016/j.cviu.2024.103973},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103973},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning key lines for multi-object tracking},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combinational sign language recognition. <em>CVIU</em>,
<em>241</em>, 103972. (<a
href="https://doi.org/10.1016/j.cviu.2024.103972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Sign Language Recognition (SLR) suffers from the scale limitation of SL datasets, which may lead to over-fitting in narrow context and application. In this paper, to solve the problem, we for the first time propose a Combinational Sign Language Recognition (CombSLR) framework, which can serve as an augmentation to extend existing datasets by combining continuous videos (called Template) and isolated videos (called Entity). The CombSLR framework is trained on combinational SL data (T &amp; E) and applied on continuous SL data. However, due to the unknown combination location and context inconsistency between any T-E pair, naively inserting E into T is infeasible. To tackle this issue, we propose a simple yet effective method named EinT, which contains two main modules: (1) Location Candidate Prediction, to produce a reliable insertion location considering the inter-frame relationship and make the network end-to-end trainable; (2) Feature Insertion via Context Passing, to eliminate context inconsistency between T and E feature. EinT can be easily compatible with the existing SLR models to effectively implement data augmentation at the feature level during training stage. We conduct extensive experiments on multiple publicly available sign language datasets, e.g., CCLS, CSL+DEVISIGN-D and CSL-Daily+DEVISIGN-D. The experimental results show the CombSLR can significantly promote existing SLR methods, e.g., averagely improving by 15.1% on CCLS dataset and 6.4% on CSL dataset for WER metric, which demonstrates the superiority of CombSLR framework.},
  archive      = {J_CVIU},
  author       = {Liqing Gao and Wei Feng and Fan Lyu and Liang Wan},
  doi          = {10.1016/j.cviu.2024.103972},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103972},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Combinational sign language recognition},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalized federated learning for person
re-identification. <em>CVIU</em>, <em>241</em>, 103969. (<a
href="https://doi.org/10.1016/j.cviu.2024.103969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of Person Re-identification (ReID), addressing the demands of practical applications in diverse and uncontrollable unseen domains necessitates a focus on Domain Generalization (DG). However, when tackling DG for human-related tasks, the growing awareness of privacy introduces new challenges. Privacy concerns often prevent the sharing of local datasets for global learning, and this limitation in data sharing can impair the generalization ability. Therefore, it becomes imperative to address domain generalization under the constraint of privacy protection. This paper delves into a novel and challenging domain generalization problem that incorporates privacy concerns. We propose a new generalizable ReID network that integrates decentralized learning from non-shared private training data. To mitigate domain variations among clients, we introduce a dynamic aggregation strategy for learning a domain-invariant server model. This strategy adaptively weights clients, guided by domain-invariance principles. To ensure generalization ability with limited client data, we present a domain compensation network. This network augments fictitious domains in the model design to simulate unseen testing situations. In the process of generating fictitious domains, we integrate diversity to avoid meaningless generations, and we constrain fidelity to preserve discrimination. Extensive experiments demonstrate the effectiveness of our method in enhancing generalization ability and privacy protection. Our approach achieves competitive performance on multiple widely used benchmarks.},
  archive      = {J_CVIU},
  author       = {Fangyi Liu and Mang Ye and Bo Du},
  doi          = {10.1016/j.cviu.2024.103969},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103969},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Domain generalized federated learning for person re-identification},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring using jigsaw puzzles for out-of-distribution
detection. <em>CVIU</em>, <em>241</em>, 103968. (<a
href="https://doi.org/10.1016/j.cviu.2024.103968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection involves binary classification whether the given data is from outside the training data or not. Previous studies proposed outlier exposure (OE) that trains the model on an outlier dataset designed to represent potential future OOD data, thereby enhancing OOD detection performance. However, obtaining an outlier dataset representing all possible future OOD data can be challenging, and such dataset may be unavailable in some cases. This study proposes a novel approach to expose the model to jigsaw puzzles generated from training images as the outlier data. Specifically, the model is trained to have a low LogitNorm for given jigsaw puzzles. We argue that jigsaw puzzles can effectively represent future OOD data because they contain similar background information as the in-distribution data but with their semantic information destroyed. Our experimental results demonstrate that our approach outperforms previous competitive OOD detection methods and effectively detects semantically shifted OOD examples. Our code is available at https://github.com/gist-ailab/jigsaw-training-OOD .},
  archive      = {J_CVIU},
  author       = {Yeonguk Yu and Sungho Shin and Minhwan Ko and Kyoobin Lee},
  doi          = {10.1016/j.cviu.2024.103968},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103968},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Exploring using jigsaw puzzles for out-of-distribution detection},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying model uncertainty for semantic segmentation of
fluorine-19 MRI using stochastic gradient MCMC. <em>CVIU</em>,
<em>241</em>, 103967. (<a
href="https://doi.org/10.1016/j.cviu.2024.103967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluorine-19 ( 19 F) MRI is an emerging theranostic tool for studying diseases and treatments simultaneously, particularly in challenging neuroinflammatory conditions. However, the low signal-to-noise ratio (SNR) of 19 F MRI necessitates computational methods to reliably detect 19 F signal regions and segment these from the background. In this study, we demonstrate that Bayesian fully convolutional neural networks provide a means to increase sensitivity in 19 F MRI and simultaneously provide estimates of data uncertainty. While our model effectively denoises the data, uncertain areas remain, particularly in boundary regions of the foreground. The uncertainty estimates are beneficial in preventing overconfident downstream analysis on noisy data and providing crucial information for rectifying prediction errors. Our results demonstrate that our model significantly outperforms other commonly used methods for 19 F MRI signal detection in terms of sensitivity, while also providing valuable uncertainty estimates.},
  archive      = {J_CVIU},
  author       = {Masoumeh Javanbakhat and Ludger Starke and Sonia Waiczies and Christoph Lippert},
  doi          = {10.1016/j.cviu.2024.103967},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103967},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Quantifying model uncertainty for semantic segmentation of fluorine-19 MRI using stochastic gradient MCMC},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey on fast dense video segmentation techniques.
<em>CVIU</em>, <em>241</em>, 103959. (<a
href="https://doi.org/10.1016/j.cviu.2024.103959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation aims at classifying image pixels according to given categories. Deep learning approaches have proven to be very effective for this task. However, extensions to video content are more challenging, typically requiring more complex architectures, given the temporal constraints and the additional data that video introduces. At the same time, video application tend to necessitate real-time, or at least interactive performances: self-driving cars, industrial applications, or live broadcasting to name a few, imposing even stronger constraints to video methods. In recent years, considerable efforts have been made in addressing these somewhat opposing challenges. In this survey, we explore the solutions proposed to improve the quality and accuracy of video segmentation, as well as the different techniques that can be employed to improve the efficiency of such approaches, in particular in terms of inference time. Finally, we briefly describe the datasets related to the semantic video segmentation task and the challenges involved.},
  archive      = {J_CVIU},
  author       = {Quentin Monnier and Tania Pouli and Kidiyo Kpalma},
  doi          = {10.1016/j.cviu.2024.103959},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103959},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Survey on fast dense video segmentation techniques},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAEDAY: MAE for few- and zero-shot AnomalY-detection.
<em>CVIU</em>, <em>241</em>, 103958. (<a
href="https://doi.org/10.1016/j.cviu.2024.103958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose using Masked Auto-Encoder (MAE), a transformer model self-supervisedly trained on image inpainting, for anomaly detection (AD). Assuming anomalous regions are harder to reconstruct compared with normal regions. MAEDAY is the first image-reconstruction-based anomaly detection method that utilizes a pre-trained model, enabling its use for Few-Shot Anomaly Detection (FSAD). We also show the same method works surprisingly well for the novel tasks of Zero-Shot AD (ZSAD) and Zero-Shot Foreign Object Detection (ZSFOD), where no normal samples are available.},
  archive      = {J_CVIU},
  author       = {Eli Schwartz and Assaf Arbelle and Leonid Karlinsky and Sivan Harary and Florian Scheidegger and Sivan Doveh and Raja Giryes},
  doi          = {10.1016/j.cviu.2024.103958},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103958},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MAEDAY: MAE for few- and zero-shot AnomalY-detection},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer-based assignment decision network for multiple
object tracking. <em>CVIU</em>, <em>241</em>, 103957. (<a
href="https://doi.org/10.1016/j.cviu.2024.103957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data association is a crucial component for any multiple object tracking (MOT) method that follows the tracking-by-detection paradigm. To generate complete trajectories such methods employ a data association process to establish assignments between detections and existing targets during each timestep. Recent data association approaches try to solve either a multi-dimensional linear assignment task or a network flow minimization problem or tackle it via multiple hypotheses tracking. However, during inference an optimization step that computes optimal assignments is required for every sequence frame inducing additional complexity to any given solution. To this end, in the context of this work we introduce Transformer-based Assignment Decision Network (TADN) that tackles data association without the need of any explicit optimization during inference. In particular, TADN can directly infer assignment pairs between detections and active targets in a single forward pass of the network. We have integrated TADN in a rather simple MOT framework, designed a novel training strategy for efficient end-to-end training and demonstrated the high potential of our approach for online visual tracking-by-detection MOT on several popular benchmarks, i.e. MOT17, MOT20 and UA-DETRAC. Our proposed approach demonstrates strong performance in most evaluation metrics despite its simple nature as a tracker lacking significant auxiliary components such as occlusion handling or re-identification. The implementation of our method is publicly available at https://github.com/psaltaath/tadn-mot .},
  archive      = {J_CVIU},
  author       = {Athena Psalta and Vasileios Tsironis and Konstantinos Karantzalos},
  doi          = {10.1016/j.cviu.2024.103957},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103957},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transformer-based assignment decision network for multiple object tracking},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Re-scoring using image-language similarity for few-shot
object detection. <em>CVIU</em>, <em>241</em>, 103956. (<a
href="https://doi.org/10.1016/j.cviu.2024.103956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot object detection, which focuses on detecting novel objects with few labels, is an emerging challenge in the community. Recent studies show that adapting a pre-trained model or modified loss function can improve performance. In this paper, we explore leveraging the power of Contrastive Language-Image Pre-training (CLIP) and hard negative classification loss in low data setting. Specifically, we propose Re-scoring using Image-language Similarity for Few-shot object detection (RISF) which extends Faster R-CNN by introducing Calibration Module using CLIP (CM-CLIP) and Background Negative Re-scale Loss (BNRL). The former adapts CLIP, which performs zero-shot classification, to re-score the classification scores of a detector using image-class similarities, the latter is modified classification loss considering the punishment for fake backgrounds as well as confusing categories on a generalized few-shot object detection dataset. Extensive experiments on MS-COCO and PASCAL VOC show that the proposed RISF substantially outperforms the state-of-the-art approaches. Code is available at: https://github.com/INFINIQ-AI1/RISF},
  archive      = {J_CVIU},
  author       = {Min Jae Jung and Seung Dae Han and Joohee Kim},
  doi          = {10.1016/j.cviu.2024.103956},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103956},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Re-scoring using image-language similarity for few-shot object detection},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-scene network: A novel baseline with self-rectifying
loss for weakly supervised video anomaly detection. <em>CVIU</em>,
<em>241</em>, 103955. (<a
href="https://doi.org/10.1016/j.cviu.2024.103955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video anomaly detection in surveillance systems with only video-level labels ( i.e. weakly supervised ) is challenging. This is due to (i) the complex integration of a large variety of scenarios including human and scene-based anomalies characterized by subtle or sharp spatio-temporal cues in real-world videos and (ii) non-optimal optimization between normal and anomaly instances under weak supervision. In this paper, we propose a Human-Scene Network to learn discriminative representations by capturing both subtle and strong cues in a dissociative manner. In addition, a self-rectifying loss is proposed that dynamically computes the pseudo-temporal annotations from video-level labels for optimizing the Human-Scene Network effectively. The proposed Human-Scene Network optimized with self-rectifying loss is validated on three publicly available datasets i.e. UCF-Crime, ShanghaiTech, and IITB-Corridor, outperforming recently reported state-of-the-art approaches on five out of the six scenarios considered.},
  archive      = {J_CVIU},
  author       = {Snehashis Majhi and Rui Dai and Quan Kong and Lorenzo Garattoni and Gianpiero Francesca and François Brémond},
  doi          = {10.1016/j.cviu.2024.103955},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103955},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human-scene network: A novel baseline with self-rectifying loss for weakly supervised video anomaly detection},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Video frame-wise explanation driven contrastive learning for
procedural text generation. <em>CVIU</em>, <em>241</em>, 103954. (<a
href="https://doi.org/10.1016/j.cviu.2024.103954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Procedural text generation from visual observation of instructional videos, such as assembling, biochemical experiments, and cooking, is an essential task for scene understanding and real-world applications. The major difference from general captioning tasks is two-fold: it has a flow of material combination in instructional steps, and the materials change their state through action-involved manipulations. However, existing works do not adequately address both two issues. To this end, this paper proposes a procedural text generation framework, namely XCL4PTG , with V ideo F rame-wise e X planation driven C ontrastive L earning ( VFXCL ) module and A ction F used M aterial R epresentation L earning ( AFMRL ) module, generating a procedural text from the step’s frame sequence of an instructional video. The VFXCL utilizes an explanation method to determine the frame’s importance in a step’s frame sequence and derive the positive and negative sequences for self-supervised contrastive learning, aiming at enhancing step representation learning for capturing the inter-step differences; The AFMRL leverages identified actions and materials to update material states after manipulations, which contributes to step representation learning via intra-step action fused material state tracking. By integrating the two modules, they collaboratively extract the information essential for the decoder to accurately generate procedural text. The experimental results show the effectiveness of the proposed framework, which outperforms state-of-the-art video procedural text generation models.},
  archive      = {J_CVIU},
  author       = {Zhihao Wang and Lin Li and Zhongwei Xie and Chuanbo Liu},
  doi          = {10.1016/j.cviu.2024.103954},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103954},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video frame-wise explanation driven contrastive learning for procedural text generation},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simplifying open-set video domain adaptation with
contrastive learning. <em>CVIU</em>, <em>241</em>, 103953. (<a
href="https://doi.org/10.1016/j.cviu.2024.103953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an effort to reduce annotation costs in action recognition, unsupervised video domain adaptation methods have been proposed that aim to adapt a predictive model from a labelled dataset (i.e., source domain) to an unlabelled dataset (i.e., target domain). In this work we address a more realistic scenario, called open-set video domain adaptation (OUVDA), where the target dataset contains “unknown” semantic categories that are not shared with the source. The challenge lies in aligning the shared classes of the two domains while separating the shared classes from the unknown ones. In this work we propose to address OUVDA with an unified contrastive learning framework that learns discriminative and well-clustered features. We also propose a video-oriented temporal contrastive loss that enables our method to better cluster the feature space by exploiting the freely available temporal information in video data. We show that discriminative feature space facilitates better separation of the unknown classes, and thereby allows us to use a simple similarity based score to identify them. We conduct thorough experimental evaluation on multiple OUVDA benchmarks and show the effectiveness of our proposed method against the prior art.},
  archive      = {J_CVIU},
  author       = {Giacomo Zara and Victor Guilherme Turrisi da Costa and Subhankar Roy and Paolo Rota and Elisa Ricci},
  doi          = {10.1016/j.cviu.2024.103953},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103953},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Simplifying open-set video domain adaptation with contrastive learning},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisiting coarse-to-fine strategy for low-light image
enhancement with deep decomposition guided training. <em>CVIU</em>,
<em>241</em>, 103952. (<a
href="https://doi.org/10.1016/j.cviu.2024.103952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous coarse-to-fine strategies typically spend equal effort in feature extraction and feature reconstruction, and gradually improve the brightness of images from bottom to top, resulting in computational resources not being well consumed for restoration. In this paper, we propose a new deep framework for Robust and Fast Low-Light Image Enhancement, dubbed RFLLIE. Specifically, we first use a lightweight CNN encoder consisting of a few convolutional layers and pooling layers to form a feature pyramid for restoration. Then, a coarse-to-fine recovery module, which consists of cascaded depth blocks and well-designed spatial attention layers as well as progressive dilation Resblocks, is proposed for feature aggregation and global-to-local restoration. As such, our RFLLIE is formed as a light-head and heavy-tail architecture that focuses more on feature reconstruction rather than extraction. Additionally, we propose a decomposition-guided restoration loss based on the Retinex theory that adopts the “enhancement before decomposition” strategy instead of the commonly used “decomposition before enhancement” to further improve the contrast and suppress noise. Extensive experiments demonstrate that our method outperforms the existing state-of-the-art methods both quantitatively and visually, and achieves a better trade-off between performance and efficiency. Our code will be available at https://github.com/JianghaiSCU/RFLLIE .},
  archive      = {J_CVIU},
  author       = {Hai Jiang and Yang Ren and Songchen Han},
  doi          = {10.1016/j.cviu.2024.103952},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103952},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Revisiting coarse-to-fine strategy for low-light image enhancement with deep decomposition guided training},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethink arbitrary style transfer with transformer and
contrastive learning. <em>CVIU</em>, <em>241</em>, 103951. (<a
href="https://doi.org/10.1016/j.cviu.2024.103951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arbitrary style transfer holds widespread attention in research and boasts numerous practical applications. The existing methods, which either employ cross-attention to incorporate deep style attributes into content attributes or use adaptive normalization to adjust content features, fail to generate high-quality stylized images. In this paper, we introduce an innovative technique to improve the quality of stylized images. Firstly, we propose Style Consistency Instance Normalization (SCIN), a method to refine the alignment between content and style features. In addition, we have developed an Instance-based Contrastive Learning (ICL) approach designed to understand the relationships among various styles, thereby enhancing the quality of the resulting stylized images. Recognizing that VGG networks are more adept at extracting classification features and need to be better suited for capturing style features, we have also introduced the Perception Encoder (PE) to capture style features. Extensive experiments demonstrate that our proposed method generates high-quality stylized images and effectively prevents artifacts compared with the existing state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Zhanjie Zhang and Jiakai Sun and Guangyuan Li and Lei Zhao and Quanwei Zhang and Zehua Lan and Haolin Yin and Wei Xing and Huaizhong Lin and Zhiwen Zuo},
  doi          = {10.1016/j.cviu.2024.103951},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103951},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Rethink arbitrary style transfer with transformer and contrastive learning},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised multi-scale semantic consistency
regularization for unsupervised image-to-image translation.
<em>CVIU</em>, <em>241</em>, 103950. (<a
href="https://doi.org/10.1016/j.cviu.2024.103950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised image-to-image translation aims to learn a domain mapping function that preserves the semantics of an input image while adapting its style to target domains without paired data. However, if there is a large semantic mismatch between the source and target domains, current methods often suffer from semantics distortion. Based on dense self-supervised representation learning, a novel Multi-Scale Semantic Consistency Regularization (MSSCR) is presented to alleviate the semantic distortion and enable the generator to produce images with realistic local semantics and consistent structures. Both local and global multi-scale representations are learned by the MSSCR during training the different layers of a discriminator. Concretely, MSSCR operates by sliding a fixed-size window over the overlapping region between a pair of views cropped from a single real image, aligning these areas with their corresponding multi-scale representation regions extracted from the discriminator, and then maximizing the similarity of representations between positive pairs. Qualitative and quantitative experiments demonstrate the superiority of MSSCR on image-to-image translation and image generation tasks.},
  archive      = {J_CVIU},
  author       = {Heng Zhang and Yi-Jun Yang and Wei Zeng},
  doi          = {10.1016/j.cviu.2024.103950},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103950},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised multi-scale semantic consistency regularization for unsupervised image-to-image translation},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based multimodal image matching. <em>CVIU</em>,
<em>241</em>, 103949. (<a
href="https://doi.org/10.1016/j.cviu.2024.103949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for matching multimodal image patches using a multiscale Transformer-Encoder that focuses on the feature maps of a Siamese CNN. It effectively combines multiscale image embeddings while improving task-specific and appearance-invariant image cues. We also introduce a residual attention architecture that allows for end-to-end training by using a residual connection. To the best of our knowledge, this is the first successful use of the Transformer-Encoder architecture in multimodal image matching. We motivate the use of task-specific multimodal descriptors by achieving new state-of-the-art accuracy on both multimodal and unimodal benchmarks, and demonstrate the quantitative and qualitative advantages of our approach over state-of-the-art unimodal image matching methods in multimodal matching. Our code is shared here: Code .},
  archive      = {J_CVIU},
  author       = {Aviad Moreshet and Yosi Keller},
  doi          = {10.1016/j.cviu.2024.103949},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103949},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Attention-based multimodal image matching},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep parametric retinex decomposition model for low-light
image enhancement. <em>CVIU</em>, <em>241</em>, 103948. (<a
href="https://doi.org/10.1016/j.cviu.2024.103948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured under low light conditions often suffer from various degradations. The Retinex models are highly effective in enhancing low-light images. The analytical optimization models are interpretable but inflexible to various scenes. The data-driven learning models are flexible to various scenes but less interpretable. To reconcile the advantages of both, we propose a parametric Retinex model with pixel-wise varying parameters. Then we unroll its iterative algorithm into an unfolding network so that the parameters can be learned. We call it Deep Parametric REtinex Decomposition (DPRED). Based on the Retinex decomposition, we present a novel network for low-light image enhancement, also called DPRED. The whole network comprises three modules: parametric Retinex decomposition, enhancement and refinement. The first two modules operate on the V channel in the HSV space, avoiding color deviation. The refinement module aims to remove noise in the enhanced RGB image . Extensive experiments demonstrate the proposed method is effective in low-light image enhancement and it significantly outperforms recent baselines.},
  archive      = {J_CVIU},
  author       = {Xiaofang Li and Weiwei Wang and Xiangchu Feng and Min Li},
  doi          = {10.1016/j.cviu.2024.103948},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103948},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep parametric retinex decomposition model for low-light image enhancement},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards efficient image and video style transfer via
distillation and learnable feature transformation. <em>CVIU</em>,
<em>241</em>, 103947. (<a
href="https://doi.org/10.1016/j.cviu.2024.103947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the recent rapid development of neural style transfer, existing style transfer methods are still somewhat inefficient or have a large model size, which limits their application on computational resource limited devices. The major problem lies in that they usually adopt a pre-trained VGG-19 backbone which is relatively large or the feature transformation module is computationally heavy. To address above problems, we propose a DIstillation based Style Transfer framework (called DIST) in conjunction with an efficient feature transformation module for arbitrary image and video style transfer. The distillation module can lead to a highly compressed backbone network , which is 15.95 × × smaller than the VGG-19 based backbone. The proposed feature transformation is capable of transforming the content features in an extremely efficient feed forward pass. For video style transfer, the above framework is further combined with a temporal consistency regularization loss. Extensive experiments show that the proposed method is superior over the state-of-the-art image and video style transfer methods, even with a much smaller model size.},
  archive      = {J_CVIU},
  author       = {Jing Huo and Meihao Kong and Wenbin Li and Jing Wu and Yu-Kun Lai and Yang Gao},
  doi          = {10.1016/j.cviu.2024.103947},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103947},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Towards efficient image and video style transfer via distillation and learnable feature transformation},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing video anomaly detection with learnable memory
network: A new approach to memory-based auto-encoders. <em>CVIU</em>,
<em>241</em>, 103946. (<a
href="https://doi.org/10.1016/j.cviu.2024.103946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of video anomaly detection is to detect anomalous events in a video sequence. In an unsupervised setting, enhancing detection accuracy hinges on the ability to learn normal features during the training phase and subsequently generate large errors when abnormal video frames are encountered during the testing phase. The transformer is an innovative neural network that utilizes a self-attention mechanism to extract intrinsic features, thereby proving more effective in extracting normal features. When paired with convolutional neural networks (CNNs), known for their proficiency in local information extraction, this hybrid architecture becomes particularly adept at handling numerous vision tasks. However, research exploring the full potential of such a hybrid architecture network for video anomaly detection is still in its early stages. In this paper, we introduce a novel approach to integrating transformers and CNNs for video anomaly detection. Here, the transformer functions as a memory module (TransMem) that processes latent features and incorporates them into CNN-based autoencoders (AEs). This approach significantly reduces computational complexity compared to directly processing video frames. Moreover, unlike other similarity-based memory methods, the proposed memory module is learnable. TransMem is a lightweight, plug-and-play module that can be seamlessly integrated into other complex frameworks to further enhance detection accuracy. Extensive experiments have demonstrated the effectiveness of our proposed method.},
  archive      = {J_CVIU},
  author       = {Zhiqiang Wang and Xiaojing Gu and Xingsheng Gu and Jingyu Hu},
  doi          = {10.1016/j.cviu.2024.103946},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103946},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhancing video anomaly detection with learnable memory network: A new approach to memory-based auto-encoders},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GMC: A general framework of multi-stage context learning and
utilization for visual detection tasks. <em>CVIU</em>, <em>241</em>,
103944. (<a href="https://doi.org/10.1016/j.cviu.2024.103944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various contextual information has been employed by many approaches for visual detection tasks. However, most of the existing approaches only focus on specific context for specific tasks. In this paper, GMC, a general framework is proposed for multistage context learning and utilization, with various deep network architectures for various visual detection tasks. The GMC framework encompasses three stages: preprocessing, training, and post-processing. In the preprocessing stage, the representation of local context is enhanced by utilizing commonly used labeling standards. During the training stage, semantic context information is fused with visual information, leveraging prior knowledge from the training dataset to capture semantic relationships. In the post-processing stage, general topological relations and semantic masks for stuff are incorporated to enable spatial context reasoning between objects. The proposed framework provides a comprehensive and adaptable solution for context learning and utilization in visual detection scenarios. The framework offers flexibility with user-defined configurations and provide adaptability to diverse network architectures and visual detection tasks, offering an automated and streamlined solution that minimizes user effort and inference time in context learning and reasoning. Experimental results on the visual detection tasks, for storefront object detection, pedestrian detection and COCO object detection, demonstrate that our framework outperforms previous state-of-the-art detectors and transformer architectures. The experiments also demonstrate that three contextual learning components can not only be applied individually and in combination, but can also be applied to various network architectures, and its flexibility and effectiveness in various detection scenarios.},
  archive      = {J_CVIU},
  author       = {Xuan Wang and Hao Tang and Zhigang Zhu},
  doi          = {10.1016/j.cviu.2024.103944},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103944},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GMC: A general framework of multi-stage context learning and utilization for visual detection tasks},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Space–time recurrent memory network. <em>CVIU</em>,
<em>241</em>, 103943. (<a
href="https://doi.org/10.1016/j.cviu.2024.103943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have recently been popular for learning and inference in the spatial–temporal domain. However, their performance relies on storing and applying attention to the feature tensor of each frame in video. Hence, their space and time complexity increase linearly as the length of video grows, which could be very costly for long videos. We propose a novel visual memory network architecture for the learning and inference problem in the spatial–temporal domain. We maintain a fixed set of memory slots in our memory network and propose an algorithm based on Gumbel-Softmax to learn an adaptive strategy to update this memory. Finally, this architecture is benchmarked on the video object segmentation (VOS) and video prediction problems. We demonstrate that our memory architecture achieves state-of-the-art results, outperforming transformer-based methods on VOS and other recent methods on video prediction while maintaining constant memory capacity independent of the sequence length.},
  archive      = {J_CVIU},
  author       = {Hung Nguyen and Chanho Kim and Fuxin Li},
  doi          = {10.1016/j.cviu.2024.103943},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103943},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Space–time recurrent memory network},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MERLIN-seg: Self-supervised despeckling for label-efficient
semantic segmentation. <em>CVIU</em>, <em>241</em>, 103940. (<a
href="https://doi.org/10.1016/j.cviu.2024.103940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing satellites acquire a continuous stream of data on a daily basis. As most of those data are unlabeled, the development of algorithms requiring weak supervision is of paramount importance. In this paper, we show that the need for annotation for Synthetic Aperture Radar data can be reduced by coupling a despeckling task (self-supervised) and a segmentation task (supervised). The proposed self-supervised learning framework, called MERLIN-Seg, has been trained for building footprint extraction and achieves favorable performances even with 1% of annotated data. We show that conditioning the network on despeckling without labels is beneficial for supervised segmentation. Our experiments demonstrate that the joint training of the two tasks achieves better performances than a vanilla segmentation network in terms of IoU, F1 score, and accuracy on both simulated and real SAR images.},
  archive      = {J_CVIU},
  author       = {Emanuele Dalsasso and Clément Rambour and Nicolas Trouvé and Nicolas Thome},
  doi          = {10.1016/j.cviu.2024.103940},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103940},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MERLIN-seg: Self-supervised despeckling for label-efficient semantic segmentation},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the coherency of quantitative evaluation of visual
explanations. <em>CVIU</em>, <em>241</em>, 103934. (<a
href="https://doi.org/10.1016/j.cviu.2024.103934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have shown an increased development of methods for justifying the predictions of neural networks through visual explanations. These explanations usually take the form of heatmaps which assign a saliency (or relevance) value to each pixel of the input image that expresses how relevant the pixel is for the prediction of a label. Complementing this development, evaluation methods have been proposed to assess the “goodness” of such explanations. On the one hand, some of these methods rely on synthetic datasets . However, this introduces the weakness of having limited guarantees regarding their applicability on more realistic settings. On the other hand, some methods rely on metrics for objective evaluation. However the level to which some of these evaluation methods perform with respect to each other is uncertain. Taking this into account, we conduct a comprehensive study on a subset of the ImageNet-1k validation set where we evaluate a number of different commonly-used explanation methods following a set of evaluation methods. We complement our study with sanity checks on the studied evaluation methods as a means to investigate their reliability and the impact of characteristics of the explanations on the evaluation methods. Results of our study suggest that there is a lack of coherency on the grading provided by some of the considered evaluation methods. Moreover, we have identified some characteristics of the explanations, e.g. sparsity , which can have a significant effect on the performance.},
  archive      = {J_CVIU},
  author       = {Benjamin Vandersmissen and José Oramas},
  doi          = {10.1016/j.cviu.2024.103934},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103934},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {On the coherency of quantitative evaluation of visual explanations},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Local to global purification strategy to realize
collaborative camouflaged object detection. <em>CVIU</em>, <em>241</em>,
103932. (<a href="https://doi.org/10.1016/j.cviu.2024.103932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of camouflaged object detection is to detect objects in images that are not easily perceived by human eyes. Aiming at the problems of low recognition performance and unsatisfied texture information extraction in the complex environment in the current camouflaged object detection algorithms , we propose to improve the accuracy by simultaneously detecting a group of images containing the same camouflaged category. Therefore, we put forward a novel method termed local to global purification network (LGPNet) for collaborative camouflaged object detection. Our method comprises two main modules: the Local Detail Mining module (LDM) and the Global Intra-group Feature Extraction module (GIFE). The LDM is designed to exploit diversified detail information via different adaptive kernels and receptive field mechanisms locally, and the GIFE module is invented for feature enhancement and multi-level information aggregation. Specifically, the GIFE first utilizes channel attention and spatial attention mechanisms to enhance high-level semantic information and then aggregates the intra-group characteristics by level. Extensive experiments on CoCOD8K dataset and 4 COD benchmark datasets illustrate the effectiveness and superiority of our method compared to SOTAs.},
  archive      = {J_CVIU},
  author       = {Jinghui Tong and Yaqiu Bi and Cong Zhang and Hongbo Bi and Ye Yuan},
  doi          = {10.1016/j.cviu.2024.103932},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103932},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Local to global purification strategy to realize collaborative camouflaged object detection},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PPformer: Using pixel-wise and patch-wise cross-attention
for low-light image enhancement. <em>CVIU</em>, <em>241</em>, 103930.
(<a href="https://doi.org/10.1016/j.cviu.2024.103930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transformer-based methods have shown strong competition compared to CNN-based methods on the low-light image enhancement task, by employing the self-attention for feature extraction. Transformer-based methods perform well in modeling long-range pixel dependencies, which are essential for low-light image enhancement to achieve better lighting, natural colors, and higher contrast. However, the high computational cost of self-attention limits its development in low-light image enhancement, while some works struggle to balance accuracy and computational cost. In this work, we propose a lightweight and effective network based on the proposed pixel-wise and patch-wise cross-attention mechanism, PPformer, for low-light image enhancement. PPformer is a CNN-transformer hybrid network that is divided into three parts: local-branch, global-branch, and Dual Cross-Attention. Each part plays a vital role in PPformer. Specifically, the local-branch extracts local structural information using a stack of Wide Enhancement Modules , and the global-branch provides the refining global information by Cross Patch Module and Global Convolution Module. Besides, different from self-attention, we use extracted global semantic information to guide modeling dependencies between local and non-local. According to calculating Dual Cross-Attention, the PPformer can effectively restore images with better color consistency, natural brightness and contrast. Benefiting from the proposed dual cross-attention mechanism, PPformer effectively captures the dependencies in both pixel and patch levels for a full-size feature map. Extensive experiments on eleven real-world benchmark datasets show that PPformer achieves better quantitative and qualitative results than previous state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Jiachen Dang and Yong Zhong and Xiaolin Qin},
  doi          = {10.1016/j.cviu.2024.103930},
  journal      = {Computer Vision and Image Understanding},
  month        = {4},
  pages        = {103930},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PPformer: Using pixel-wise and patch-wise cross-attention for low-light image enhancement},
  volume       = {241},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal adaptive feature pyramid network for action
detection. <em>CVIU</em>, <em>240</em>, 103945. (<a
href="https://doi.org/10.1016/j.cviu.2024.103945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting actions in videos has become a prominent research task due to its wide application. In addition to recognizing action category, this task also needs to localize the start time and end time of each action instance, which requires the model to have high temporal modeling capability. Moreover, the duration between each action instance is often different and highly variable. Although previous works have made attempts to address this difficulty, it is still a persistent problem. To further address the difficulty, we propose an action detection network using temporal feature pyramid, which can collect data using cameras and predict precise action categories and localizations. Specifically, we introduce a temporal adaptive module , which mixes self-attention and 1D convolution to flexibly adjust the temporal receptive field to improve the temporal modeling ability for different actions. We also propose a channel adaptive module to adjust channel weights and suppress useless information. We then propose the Temporal Adaptive Feature Pyramid Network (TAFPN) by integrating the two modules to adaptively extract multi-scale temporal information. We also improve the traditional parallel head into a unified head by stacking channel adaptive modules to simplify the network structure. Experimental results on the THUMOS14 dataset and ActivityNet1.3 dataset show that our method is competitive with state-of-the-art methods, which proves the effectiveness of our method.},
  archive      = {J_CVIU},
  author       = {Xuezhi Xiang and Hang Yin and Yulong Qiao and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2024.103945},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103945},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Temporal adaptive feature pyramid network for action detection},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CPRNC: Channels pruning via reverse neuron crowding for
model compression. <em>CVIU</em>, <em>240</em>, 103942. (<a
href="https://doi.org/10.1016/j.cviu.2024.103942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is an efficient technique for model compression , removing redundant parts of a convolutional neural network with minor degradation in classification accuracy . Previous criteria of channel pruning ignore neurons’ intrinsic relationship and the high correlation with input samples. Inspired by the visual crowding phenomenon in neuroscience, this paper presents a novel channel pruning method via reverse neuron crowding, dubbed CPRNC, to address this issue. First, CPRNC involves a neuron crowding degree measure (NCDM) module, which builds the relationship model among all artificial neurons by observing their crowding behaviors. Subsequently, each channel’s importance is evaluated by the crowding degree of corresponding channels. Considering that the channel importance is affected by the characteristic of input samples, CPRNC designs a neuron crowding degree recalibrate (NCDR) module. NCDR emphasizes discriminative samples to recalibrate the channel priority list generated by NCDM, further enhancing the precision of the pruning criterion. Experimental results show that CPRNC achieves performance that competes with state-of-the-art pruning methods, including dynamic channel pruning and learning-based pruning. For example, we prune ResNet-50 with 56.7% FLOPs on the large-scale dataset ImageNet1K with only a 0.19% decrease in accuracy. At low pruning rates, CPRNC achieves lossless compression , e.g. , the pruned ResNet-56 on CIFAR-10 increases accuracy by 0.13% over the baseline model at 56.3% FLOPs reduction.},
  archive      = {J_CVIU},
  author       = {Pingfan Wu and Hengyi Huang and Han Sun and Dong Liang and Ningzhong Liu},
  doi          = {10.1016/j.cviu.2024.103942},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103942},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CPRNC: Channels pruning via reverse neuron crowding for model compression},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic-aware transformer for shadow detection.
<em>CVIU</em>, <em>240</em>, 103941. (<a
href="https://doi.org/10.1016/j.cviu.2024.103941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shadow detection is significant for scene understanding. Ambiguities in a shadow image, such as shadow-like non-shadow regions and shadow regions with non-shadow patterns, are still very challenging for prevalent CNN-based methods. This work attempts to alleviate this problem from a new perspective of shape semantics, and then proposes a Semantic-aware Transformer (SaT) in a multi-task learning manner. Concretely, we first propose a shadow detection network based on the recent progress of Transformer architecture, allowing us to capture significant global interactions between contexts. Next, we design a multi-task learning framework, combining shadow supervision and semantic supervision to perform a semantic-aware shadow detection. Finally, we introduce a simple yet effective information buffer unit to overcome the gradient signal conflict from multi-task learning. Experimental results on three public benchmark datasets (i.e., ISTD, SBU, and UCF) show that our SaT can effectively detect ambiguous cases and achieve state-of-the-art results.},
  archive      = {J_CVIU},
  author       = {Kai Zhou and Jing-Long Fang and Wen Wu and Yan-Li Shao and Xing-Qi Wang and Dan Wei},
  doi          = {10.1016/j.cviu.2024.103941},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103941},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic-aware transformer for shadow detection},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised deep learning of foreground objects from
low-rank and sparse dataset. <em>CVIU</em>, <em>240</em>, 103939. (<a
href="https://doi.org/10.1016/j.cviu.2024.103939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground object identification can be considered as anomaly detection in a redundant background. This paper proposes unsupervised deep learning of foreground objects on the basis of the prior knowledge about spatio-temporal sparseness and low-rankness of foreground objects and background scenes. The proposed framework trains a U-Net model to encode and decode the sparse foreground objects in batches of input images with low-rank backgrounds, by minimizing a combination of nuclear and ℓ 1 ℓ1 norms as a loss function. This approach is similar to background subtraction based on robust principal component analysis (RPCA): an iterative method that detects sparse foreground objects as outliers while learning the principal components of the linearly dependent background. In contrast, the proposed method is advantageous over RPCA in that once the U-Net model has learned enough features common to the foreground objects, it can robustly detect them from any single image regardless of the low-rankness and sparseness. The U-Net also enables online object segmentation with much less computational expense than that of RPCA. These advantages are illustrated with background subtraction in video surveillance. It is also shown that the proposed method can build up a well-generalized cell segmentation model from only a few dozen unannotated training images.},
  archive      = {J_CVIU},
  author       = {Keita Takeda and Tomoya Sakai},
  doi          = {10.1016/j.cviu.2024.103939},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103939},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Unsupervised deep learning of foreground objects from low-rank and sparse dataset},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fourier analysis on robustness of graph convolutional neural
networks for skeleton-based action recognition. <em>CVIU</em>,
<em>240</em>, 103936. (<a
href="https://doi.org/10.1016/j.cviu.2024.103936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using Fourier analysis, we explore the robustness and vulnerability of graph convolutional neural networks (GCNs) for skeleton-based action recognition. We adopt a joint Fourier transform (JFT), a combination of the graph Fourier transform (GFT) and the discrete Fourier transform (DFT), to examine the robustness of adversarially-trained GCNs against adversarial attacks and common corruptions. Experimental results with the NTU RGB+D dataset reveal that adversarial training does not introduce a robustness trade-off between adversarial attacks and low-frequency perturbations, which typically occurs during image classification based on convolutional neural networks . This finding indicates that adversarial training is a practical approach to enhancing robustness against adversarial attacks and common corruptions in skeleton-based action recognition. Furthermore, we find that the Fourier approach cannot explain vulnerability against skeletal part occlusion corruption, which highlights its limitations. These findings extend our understanding of the robustness of GCNs, potentially guiding the development of more robust learning methods for skeleton-based action recognition.},
  archive      = {J_CVIU},
  author       = {Nariki Tanaka and Hiroshi Kera and Kazuhiko Kawamoto},
  doi          = {10.1016/j.cviu.2024.103936},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103936},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fourier analysis on robustness of graph convolutional neural networks for skeleton-based action recognition},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LandmarkBreaker: A proactive method to obstruct DeepFakes
via disrupting facial landmark extraction. <em>CVIU</em>, <em>240</em>,
103935. (<a href="https://doi.org/10.1016/j.cviu.2024.103935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent development of Deep Neural Networks (DNN) has significantly increased the realism of AI-synthesized faces, with the most notable examples being the DeepFakes . In particular, DeepFake can synthesize the face of the target subject from the face of another subject, while retaining the same face attributes. With the increased number of social media portals, DeepFake videos rapidly spread through the Internet, causing a broad negative impact on society. Recent countermeasures to combat DeepFake focus on detection, a passive defense that is not able to prevent or slow down the generation of DeepFakes. Therefore in this paper, we focus on proactive defense and describe a new method named LandmarkBreaker , which is the first dedicated solution to obstruct the generation of DeepFake videos by disrupting facial landmark extraction, inspired by the observation that facial landmark extraction is an indispensable step for face alignment required in DeepFake synthesis. To disrupt facial landmark extraction, we design adversarial perturbations meticulously by optimizing a loss function in an iterative manner. Furthermore, we develop LandmarkBreaker++ , which can further reduce the perceptibility of adversarial perturbations using a gradient clipping and face masking strategy. We validate our method on three state-of-the-art facial landmark extractors and investigate the defense performance on a recent Celeb-DF dataset, which demonstrates the efficacy of our method in obstructing the generation of DeepFake videos.},
  archive      = {J_CVIU},
  author       = {Yuezun Li and Pu Sun and Honggang Qi and Siwei Lyu},
  doi          = {10.1016/j.cviu.2024.103935},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103935},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LandmarkBreaker: A proactive method to obstruct DeepFakes via disrupting facial landmark extraction},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel slime mold algorithm for grayscale and color image
contrast enhancement. <em>CVIU</em>, <em>240</em>, 103933. (<a
href="https://doi.org/10.1016/j.cviu.2024.103933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image enhancement is a key step in image pre-processing. To address the problem of low quality and visual effect of images under low illumination conditions , this paper proposes an image enhancement method with hyperbolic oscillation factor and quadratic interpolation of slime mold algorithm (SSMA) in non-complete beta function dynamically looking to adjust the grayscale curve. The new strategy mainly proposes three different improvement strategies for the problem that the classical slime mold algorithm has low convergence accuracy and can easily fall into local optimum. Experimenting the proposed SSMA with other conventional and latest algorithms on the CEC2017 benchmark function and low-illumination standard dataset. The experimental results show that the convergence accuracy and convergence speed of SSMA are better than other algorithms. The proposed SSMA-optimized image enhancement algorithm effectively enhances the image brightness while preserving more details of the image, which is significantly better than other algorithms for image enhancement.},
  archive      = {J_CVIU},
  author       = {Guoyuan Ma and Xiaofeng Yue and Juan Zhu and Zeyuan Liu and Zongheng Zhang and Yuan Zhou and Chang Li},
  doi          = {10.1016/j.cviu.2024.103933},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103933},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel slime mold algorithm for grayscale and color image contrast enhancement},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lmser-pix2seq: Learning stable sketch representations for
sketch healing. <em>CVIU</em>, <em>240</em>, 103931. (<a
href="https://doi.org/10.1016/j.cviu.2024.103931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketch healing aims to recreate a complete sketch from the corrupted one. Sketches are abstract and sparse, making it difficult for neural networks to learn high-quality representations of sketches that include colors, textures, and other details. This presents a significant challenge for sketch healing. The features extracted from the corrupted sketch may be inconsistent with the ones from the corresponding full sketch. In this paper, we present Lmser-pix2seq to learn stable sketch representations against the missing information by employing a Least mean square error reconstruction (Lmser) block, which falls into encoder–decoder paradigm. Taking as input a corrupted sketch, the Lmser encoder computes the embeddings of structural patterns of the input, while the decoder reconstructs the complete sketch from the embeddings. We build bi-directional skip connections between the encoder and the decoder in our Lmser block. The feedback connections enable recurrent paths to receive more information about the reconstructed sketch produced by the decoder, which helps the encoder extract stable sketch features. The features captured by the Lmser block are eventually fed into a recurrent neural network decoder to recreate the sketches. We also find that compared with the vanilla convolutional neural networks , our gated multilayer perceptron (gMLP) block based network captures the long-range dependence of different regions in the sketch image, automatically learns the relationship between patches and extracts the individual-specific features from the sketch more effectively. Experimental results show that our Lmser-pix2seq outperforms the state-of-the-art methods in sketch healing, especially when the sketches are heavily masked or corrupted.},
  archive      = {J_CVIU},
  author       = {Tengjie Li and Sicong Zang and Shikui Tu and Lei Xu},
  doi          = {10.1016/j.cviu.2024.103931},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103931},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Lmser-pix2seq: Learning stable sketch representations for sketch healing},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TECD_attention: Texture-enhanced and cross-domain attention
modeling for visual place recognition. <em>CVIU</em>, <em>240</em>,
103929. (<a href="https://doi.org/10.1016/j.cviu.2024.103929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition (VPR) is a challenging task for visual computing in the field of robot navigation . However, most of the existing methods fail to learn the most salient features of place images by simple CNN feature or popular Transformer feature due to the inconsistency problem commonly existing in VPR datasets, which limits the robustness and interpretability of the model. In addition, existing state-of-the-art methods only capture general features of original places with multi-scale CNN or transformer features and ignore texture characteristics existing in place images, resulting in suboptimal recognition performance. To cope with the above issues, we propose a novel visual place recognition network , named Texture-enhanced Cross-domain Attention Transformer (TECD_Attention). Specially, a cross-attention Transformer is first used for fusing deep attentive local and global features to improve the multi-scale feature representation of the recognition model. Second, a texture-enhanced cross-domain attention block is designed to construct the final feature descriptor by fusing texture features and attentive local–global features. Then, a tripled loss function is used for matching top-ranked reference places from the place database to a query place. Last, effective and efficient place re-ranking is achieved by training an adapted weakly supervised re-ranking network relying on the similarity computing between the query place and the top-ranked places. Our approach is carried out in extensive experiments on four challenging datasets. Our model has achieved 96.2%, 94.6%, 95.9%, and 96.8% average recall based on top 1% Candidate scenario on Tokyo 24/7, Pitts250k, VPRiCE, and SUN397 datasets, respectively. Therefore, Compared with the existing state-of-the-art VPR methods, TECD_Attention performs superior on robot place recognition in challenging environments. Hence, we can conclude that this is a robust model for robot visual place recognition in challenging environments.},
  archive      = {J_CVIU},
  author       = {Zhenyu Li and Zhenbiao Dong},
  doi          = {10.1016/j.cviu.2024.103929},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103929},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {TECD_Attention: Texture-enhanced and cross-domain attention modeling for visual place recognition},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GradPaint: Gradient-guided inpainting with diffusion models.
<em>CVIU</em>, <em>240</em>, 103928. (<a
href="https://doi.org/10.1016/j.cviu.2024.103928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising Diffusion Probabilistic Models (DDPMs) have recently achieved remarkable results in conditional and unconditional image generation . The pre-trained models can be adapted without further training to different downstream tasks, by guiding their iterative denoising process at inference time to satisfy additional constraints. For the specific task of image inpainting , the current guiding mechanism relies on copying-and-pasting the known regions from the input image at each denoising step. However, diffusion models are strongly conditioned by the initial random noise, and therefore struggle to harmonize predictions inside the inpainting mask with the real parts of the input image, often producing results with unnatural artifacts. Our method, dubbed GradPaint, steers the generation towards a globally coherent image. At each step in the denoising process, we leverage the model’s “denoised image estimation” by calculating a custom loss measuring its coherence with the masked input image. Our guiding mechanism uses the gradient obtained from backpropagating this loss through the diffusion model itself. GradPaint generalizes well to diffusion models trained on various datasets, improving upon current state-of-the-art supervised and unsupervised methods . Our code will be made available upon publication.},
  archive      = {J_CVIU},
  author       = {Asya Grechka and Guillaume Couairon and Matthieu Cord},
  doi          = {10.1016/j.cviu.2024.103928},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103928},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {GradPaint: Gradient-guided inpainting with diffusion models},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing image-based facial expression recognition through
muscle activation-based facial feature extraction. <em>CVIU</em>,
<em>240</em>, 103927. (<a
href="https://doi.org/10.1016/j.cviu.2024.103927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a non-intrusive method to estimate facial muscle activity from images, diverging from conventional electrode-based approaches. Our methodology capitalizes on an inclusive set of features encompassing a diverse range of facial muscles, often overlooked in research, thus significantly expanding the scope of analyzing muscle activity within facial expressions. Our method is based on the standard 68-point face landmark and extends it by identifying the interactions of these muscles when a person performs a specific facial expression. These interactions are recorded in a feature vector, which is used by three classifiers , Linear Discriminant (LD), Support Vector Machine (SVM) and Multi-layer Perceptron (MLP) to classify six facial expressions (anger, disgust, fear, happiness, neutrality, and sadness). The method’s validation is conducted with three databases: FACES, KDEF, and JAFFE. These databases present different challenges; the first contains faces of individuals from three age ranges and both genders displaying facial expressions. On one hand, the KDEF database contains images of both genders but only within the range of young adults, whereas JAFFE comprises solely female faces. Additionally, JAFFE presents another drawback, representing only 10% of the images contained in FACES. In all these cases, our method yields excellent results, occasionally achieving 100% classification, especially in the young category. It is also noteworthy that the best results were obtained in the classification of facial expressions in female faces, suggesting that women tend to be more expressive. The method was thoroughly evaluated, and the results demonstrate the robustness of the approach, showcasing its good performance across three different databases and also for faces of individuals across various age groups. Numerical metrics, including accuracy, precision, recall, and F1 score, are presented along with confusion matrices .},
  archive      = {J_CVIU},
  author       = {Manuel A. Solis-Arrazola and Raul E. Sanchez-Yañez and Carlos H. Garcia-Capulin and Horacio Rostro-Gonzalez},
  doi          = {10.1016/j.cviu.2024.103927},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103927},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Enhancing image-based facial expression recognition through muscle activation-based facial feature extraction},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emerging image generation with flexible control of perceived
difficulty. <em>CVIU</em>, <em>240</em>, 103919. (<a
href="https://doi.org/10.1016/j.cviu.2023.103919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging images (EI) are two-tone and contain a number of discrete speckles. If certain speckles are appropriately organized together, we will perceive a meaningful object, which reflects the closed-loop information processing of human visual cognition. EIs hold significant application value. They can be used in studies of perceptual organization in cognitive psychology. Additionally, they can also serve as a CAPTCHA mechanism to distinguish humans from bots in the field of network security . Both applications require a method for generating EIs that can flexibly adjust the perceived difficulty. Although universal style transfer (UST) models are capable of generating images in a specific style, it can be challenging to adjust the generated results to meet different user needs. In this paper, we present a novel EI generation framework and achieve flexible control over the perceived difficulty of the EIs by extracting and quantifying different cognitive cues and setting the corresponding parameters to adjust the proportion of these cues rendered in the EIs. The experimental results both qualitatively and quantitatively demonstrate that our methods generates EIs with higher quality while allowing for more flexible control over the perceived difficulty. Furthermore, we prove the potential of EIs as a CAPTCHA through sufficient experiments.},
  archive      = {J_CVIU},
  author       = {Jingmeng Li and Hui Wei and Surun Yang and Lukang Fu},
  doi          = {10.1016/j.cviu.2023.103919},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103919},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Emerging image generation with flexible control of perceived difficulty},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient cross-information fusion decoder for semantic
segmentation. <em>CVIU</em>, <em>240</em>, 103918. (<a
href="https://doi.org/10.1016/j.cviu.2023.103918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For fine-scale prediction tasks such as semantic segmentation , existing segmentation models cannot support detailed segmentation due to the difficulty of assigning deep feature semantics generated by the encoder to shallow features, thus making the segmentation of details ambiguous in semantic segmentation scenarios. In addition, high-precision models often require large quantities of computational resources . To solve the above problems, we design an efficient cross-information fusion decoder (ECFD). In the ECFD, we design a cross-information fusion block (CFB), and contextual information is used to assign semantic information to the shallow features in spatial domain, thus facilitating the classification of the details of segmented objects. To reduce the computational effort of the model, we choose the same decoder structure as used by the efficient SenFormer: the feature pyramid structure. Compared with SenFormer, ECFD-Swin-Large reduces the numbers of parameters and floating-point operations by 1/3, and achieves 83.61% and 64.98% of mIoU values for the benchmark datasets Cityscapes and Pascal Context, respectively, outperforming SenFormer, especially for in detailed segmentation. In addition, 69.19% is obtained on BDD100K. The code is publicly available at https://github.com/songyang-xiaobai/ECFD-main .},
  archive      = {J_CVIU},
  author       = {Songyang Zhang and Ge Ren and Xiaoxi Zeng and Liang Zhang and Kailun Du and Gege Liu and Hong Lin},
  doi          = {10.1016/j.cviu.2023.103918},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103918},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient cross-information fusion decoder for semantic segmentation},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformer with large convolution kernel decoder network
for salient object detection in optical remote sensing images.
<em>CVIU</em>, <em>240</em>, 103917. (<a
href="https://doi.org/10.1016/j.cviu.2023.103917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite salient object detection in optical remote sensing images (ORSI-SOD) has made great strides in recent years, it is still a very challenging topic due to various scales and shapes of objects, cluttered backgrounds, and diverse imaging orientations. Most previous deep learning-based methods fails to effectively capture local and global features, resulting in ambiguous localization and semantic information and inaccurate detail and boundary prediction for ORSI-SOD. In this paper, we propose a novel Transformer with large convolutional kernel decoding network, named TLCKD-Net, which effectively models the long-range dependence that is indispensable for feature extraction of ORSI-SOD. First, we utilize Transformer backbone network to perceive global and local details of salient objects. Second, a large convolutional kernel decoding module based on self-attention mechanism is designed for different sizes of salient objects to extract feature information at different scales. Then, a large convolutional refinement and a Salient Feature Enhancement Module are used to recover and refine the saliency features to obtain high quality saliency maps. Extensive experiments on two public ORSI-SOD datasets show that our proposed method outperforms 16 state-of-the-art methods both qualitatively and quantitatively. In addition, a series of ablation studies demonstrate the effectiveness of different modules for ORSI-SOD. Our source code is publicly available at https://github.com/Dpw506/TLCKD-Net .},
  archive      = {J_CVIU},
  author       = {Pengwei Dong and Bo Wang and Runmin Cong and Hai-Han Sun and Chongyi Li},
  doi          = {10.1016/j.cviu.2023.103917},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103917},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transformer with large convolution kernel decoder network for salient object detection in optical remote sensing images},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive locally-aligned transformer for low-light video
enhancement. <em>CVIU</em>, <em>240</em>, 103916. (<a
href="https://doi.org/10.1016/j.cviu.2023.103916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light enhancement is a crucial task that aims to enhance the under-exposed input in computer vision . While state-of-the-art static single-image enhancement methods have made remarkable progress, yet, few attempts are explored the spatial-temporal sequence problem in low-light video enhancement. In this paper, we propose a simple yet highly effective method, termed as A daptive L ocally- A ligned T ransformer (ALAT) for low-light video enhancement based on visual transformers. ALAT consists of three parts: feature encoder, locally-aligned transformer block (LATB) and pyramid feature decoder. Specifically, the transformer block enables the network to model the long-range spatial and appearance dependencies in videos due to its self-attention parallel computing mechanism. However, different from some previous approaches directly using the vanilla transformer, we consider that locality is significant in low-level vision tasks since the misaligned contextual local features ( i . e ., edges, shapes) may affect the prediction quality. Therefore, the proposed LATB is designed to align the video pixel with its most relevant ones adaptively in the local region to preserve the regional content information. Furthermore, we publish a new real-world low-light video dataset, named ExpressWay , to fill the gaps in the lack of dynamic low-light video scenarios, which contains high-quality videos with moving objects in both dark- and bright-light conditions. We conduct experiments on five benchmarks under three comprehensive settings including synthesized, static and our proposed dynamic low-light video datasets. Extensive experimental results show that our ALAT can outperform the previous state-of-the-arts by a large margin of 0.20∼1.10 0.20∼1.10 dB dB . Our method can be also extended to other video enhancement applications. The project is available at https://github.com/y1wencao/LLVE-ALAT .},
  archive      = {J_CVIU},
  author       = {Yiwen Cao and Yukun Su and Jingliang Deng and Yu Zhang and Qingyao Wu},
  doi          = {10.1016/j.cviu.2023.103916},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103916},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive locally-aligned transformer for low-light video enhancement},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint learning of foreground, background and edge for
salient object detection. <em>CVIU</em>, <em>240</em>, 103915. (<a
href="https://doi.org/10.1016/j.cviu.2023.103915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although significant progress has been made in saliency detection , predicting saliency remains challenging when the scene is complex, especially when salient and non-salient regions are similar or salient objects have intricate contours. Previous advanced methods rarely explored learning in the background of images. In fact, background and foreground of an image contain complementary information. In this work, we propose to decompose the saliency detection task into three subtasks: foreground awareness, background suppression, and edge refinement. More specifically, our decoder is comprised of three branches: a foreground awareness branch, a background suppression branch, and an edge refinement branch. Each branch aims to learn specific features for predicting its corresponding map. Meanwhile, we design a regional focus loss function with controllable modulating factors to supervise the learning of each branch at the training stage. Moreover, we build an attention guided feature fusion module to adaptively fuse multi-scale features and a global information capture module to locate salient objects. Experiments on five benchmark datasets demonstrate that our approach is superior to the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Qin Wu and Pengcheng Zhu and Zhilei Chai and Guodong Guo},
  doi          = {10.1016/j.cviu.2023.103915},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103915},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint learning of foreground, background and edge for salient object detection},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DFNet-trans: An end-to-end multibranching network for depth
estimation for transparent objects. <em>CVIU</em>, <em>240</em>, 103914.
(<a href="https://doi.org/10.1016/j.cviu.2023.103914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transparent objects play a vital role in modern industries and find widespread applications across various engineering scenarios. However, capturing accurate depth maps of transparent objects remains challenging due to their reflective and refractive properties, which pose difficulties for most commercial-grade optical sensors . In this paper, we propose a novel depth estimation method called DFNet-Trans, designed to estimate depth from a noisy RGB-D image input. Initially, a multiscale feature fusion module (FFM) is incorporated into the existing depth estimation network to generate the initial depth map. Subsequently, we enhance the network by adding a confidence branch and a mask branch on the same encoder, enabling improved distortion correction and real scene restoration in the depth estimation. Based on the framework representation, missing depth can be completed. Comprehensive experiments demonstrate that the proposed approach significantly outperforms the current state-of-the-art methods on the recently popular large-scale real dataset TransCG. the proposed approach achieves a remarkable 27.7% reduction in RMSE and a notable 34.6% reduction in REL. The generalization experiment shows that the proposed approach outperforms existing methods when generalized to an unknown real dataset.},
  archive      = {J_CVIU},
  author       = {Xiangyin Meng and Jie Wen and Yang Li and Chenlong Wang and Jingzhen Zhang},
  doi          = {10.1016/j.cviu.2023.103914},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103914},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {DFNet-trans: An end-to-end multibranching network for depth estimation for transparent objects},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards adversarial robustness verification of no-reference
image- and video-quality metrics. <em>CVIU</em>, <em>240</em>, 103913.
(<a href="https://doi.org/10.1016/j.cviu.2023.103913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method of analysing the stability of modern deep image- and video-quality metrics to different adversarial attacks . The stability analysis of quality metrics is becoming important because nowadays the majority of metrics employ neural networks . Unlike traditional quality metrics based on nature scene statistics or other hand-crafter features, learning-based methods are more vulnerable to adversarial attacks. The usage of such unstable metrics in benchmarks may lead to being exploited by the developers of image and video processing algorithms to achieve higher positions in leaderboards. The majority of known adversarial attacks on images designed for computer vision tasks are not fast enough to be used within real-time video processing algorithms. We propose four fast attacks on metrics suitable for real-life scenarios. The proposed methods are based on creating perturbations that increase metrics scores and can be applied frame-by-frame to attack videos. We analyse the stability of seven widely used no-reference image- and video-quality metrics to proposed attacks. The results showed that only three metrics are stable against our real-life attacks. This research yields insights to further aid in designing stable neural-network-based no-reference quality metrics. Proposed attacks can serve as an additional verification of metrics’ reliability.},
  archive      = {J_CVIU},
  author       = {Ekaterina Shumitskaya and Anastasia Antsiferova and Dmitriy Vatolin},
  doi          = {10.1016/j.cviu.2023.103913},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103913},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Towards adversarial robustness verification of no-reference image- and video-quality metrics},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodel fore-/background alignment for seam-based
parallax-tolerant image stitching. <em>CVIU</em>, <em>240</em>, 103912.
(<a href="https://doi.org/10.1016/j.cviu.2023.103912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image stitching with large parallax is a challenging computer vision problem. Although existing seam-based approaches were proposed to achieve pleasing results, issues like object dislocation, disappearance, and duplication can still occur. In this paper, to alleviate these problems, we propose a novel seam-based parallax-tolerant image stitching method, which relies on accurately aligning background and foreground regions using multiple warping models. To estimate various spatially smooth models based on feature correspondences from depth-varying objects, we introduce an iterative algorithm that selects inliers and solves the mesh warping model by assigning weights to data. Additionally, we construct matching confidences of foreground pixels based on selecting and grouping unaligned feature pairs, thus penalizing the duplication of seam cuts. To further improve alignment, we refine the models by minimizing pixel-level errors. We then choose the best seam among multiple candidate alignment and seam finding solutions. Finally, we re-estimate the warping model by sampling and weighting points near the seam to achieve a natural-looking stitching result. Experimental results on real-world images demonstrate the effectiveness and superiority of our proposed method over other state-of-the-arts.},
  archive      = {J_CVIU},
  author       = {Zhihao Zhang and Jie He and Mouquan Shen and Jiantao Shi and Xianqiang Yang},
  doi          = {10.1016/j.cviu.2023.103912},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103912},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multimodel fore-/background alignment for seam-based parallax-tolerant image stitching},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Hierarchical compositional representations for few-shot
action recognition. <em>CVIU</em>, <em>240</em>, 103911. (<a
href="https://doi.org/10.1016/j.cviu.2023.103911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently action recognition has received more and more attention for its comprehensive and practical applications in intelligent surveillance and human–computer interaction. However, few-shot action recognition has not been well explored and remains challenging because of data scarcity. In this paper, we propose a novel hierarchical compositional representations (HCR) learning approach for few-shot action recognition. Specifically, we divide a complicated action into several sub-actions by carefully designed hierarchical clustering and further decompose the sub-actions into more fine-grained spatially attentional sub-actions (SAS-actions). Although there exist large differences between base classes and novel classes, they can share similar patterns in sub-actions or SAS-actions. Furthermore, we adopt the Earth Mover’s Distance in the transportation problem to measure the similarity between video samples in terms of sub-action representations. It computes the optimal matching flows between sub-actions as distance metric, which is favorable for comparing fine-grained patterns. Extensive experiments show our method achieves the state-of-the-art results on HMDB51, UCF101 and Kinetics datasets.},
  archive      = {J_CVIU},
  author       = {Changzhen Li and Jie Zhang and Shuzhe Wu and Xin Jin and Shiguang Shan},
  doi          = {10.1016/j.cviu.2023.103911},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103911},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hierarchical compositional representations for few-shot action recognition},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Twin-SegNet: Dynamically coupled complementary segmentation
networks for generalized medical image segmentation. <em>CVIU</em>,
<em>240</em>, 103910. (<a
href="https://doi.org/10.1016/j.cviu.2023.103910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is deemed an important task in biomedicine, often required for proper diagnosis and prognosis of many diseases. Deep learning (DL) based segmentation methods have received considerable attention in recent years due to the increasing availability of clinical datasets. Many novel ideas have been proposed over the years driving progress in the field of automatic segmentation research. Contrary to the theme of contemporary literature, we demonstrate that considering the background tissue segmentation task alongside the main foreground task can improve overall segmentation performance when considered from a general medical image segmentation perspective. We, therefore, propose a DL framework called Twin Segmentation Network (Twin-SegNet) that ties together two streams (foreground and background) through an image reconstruction task. A boxed Mean Squared Error loss is proposed to complement the dice losses from both streams. We furthermore propose a Wavelet Convolutional Block (WCB) to enhance the edge information extracting capabilities of both streams and also a Partial Channel Recalibration (PCR) block to allow mutual feature exchange between the two streams so that each stream can emphasize more on channels with more discriminative and relevant features. We present experimental results on five public datasets: BUSI, GLAS, ISIC-2018, MoNuSeg, and CVC-ClinicDB. Unlike conventional baselines that demonstrate convincing performance in some datasets and poor performance in others, Twin-SegNet is able to consistently achieve state-of-the-art results with impressive F1 scores of 88.46%, 93.11%, 91.61%, 81.78% and 94.69% on each dataset respectively, showing its great potential as a general segmentation framework.},
  archive      = {J_CVIU},
  author       = {Shahed Ahmed and Md. Kamrul Hasan},
  doi          = {10.1016/j.cviu.2023.103910},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103910},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Twin-SegNet: Dynamically coupled complementary segmentation networks for generalized medical image segmentation},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid AI for panoptic segmentation: An informed deep
learning approach with integration of prior spatial relationships
knowledge. <em>CVIU</em>, <em>240</em>, 103909. (<a
href="https://doi.org/10.1016/j.cviu.2023.103909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panoptic segmentation is a computer vision task that aims to identify and analyze all objects present in an image. While semantic segmentation focuses on labeling each pixel in an image with a category label, panoptic segmentation goes further by not only assigning semantic labels but also identifying and distinguishing individual instance of objects. This task is valuable for various applications, such as robotics, surveillance systems or autonomous vehicle navigation. In this work, we propose a new informed deep learning approach that combines the strengths of deep neural networks for panoptic segmentation with additional knowledge about spatial relationships between objects. This is particularly important as spatial relationships can provide useful cues for resolving ambiguities, distinguishing between overlapping or similar object instances, and capturing the holistic structure of the scene. We propose a novel training methodology that integrates knowledge directly into the deep neural network optimization process. Our approach includes a process for extracting and representing spatial relationships knowledge, which is incorporated into the training using a specially designed loss function. The effectiveness of the proposed method is evaluated and validated on various challenging datasets, namely CityScapes, KITTI, IDD and COCO datasets.},
  archive      = {J_CVIU},
  author       = {Fatima Ezzahra Benkirane and Nathan Crombez and Vincent Hilaire and Yassine Ruichek},
  doi          = {10.1016/j.cviu.2023.103909},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103909},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hybrid AI for panoptic segmentation: An informed deep learning approach with integration of prior spatial relationships knowledge},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse graph matching network for temporal language
localization in videos. <em>CVIU</em>, <em>240</em>, 103908. (<a
href="https://doi.org/10.1016/j.cviu.2023.103908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal language localization in videos aims to retrieve the moment that best matches the text description in the untrimmed video using the query text. Existing methods using graph convolutional networks have been effective in feature representation and cross-modal interaction, but the existing methods do not consider the sparsity constraint of the graph when constructing the graph structure, which can easily cause an increase in computational cost and introduce redundant connections that may adversely affect the accuracy of the results. Therefore, we propose a novel sparse graph matching network for temporal language localization in videos. Specifically, we use graph convolutional networks to learn video features and dynamically construct video graph with constraints on sparsity and connectivity; the complementarity between the sequential context and the syntactic structure of text is used to model the semantic features of text. For cross-modal interaction, we design a sparse graph matching method based on affinity matrix to match video and text graphs, and align cross-modal semantic features. Finally, by fusing the features of the two modalities, candidate moments are generated, and their confidence scores are calculated to locate the moment matching the query. Experimental results on the public datasets TACoS, ActivityNet Caption, and QVHighlights demonstrate the superiority of our method compared to state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Guangli Wu and Tongjie Xu and Jing Zhang},
  doi          = {10.1016/j.cviu.2023.103908},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103908},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Sparse graph matching network for temporal language localization in videos},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Indoor synthetic data generation: A systematic review.
<em>CVIU</em>, <em>240</em>, 103907. (<a
href="https://doi.org/10.1016/j.cviu.2023.103907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding require a large amount of training data to achieve generalization. Time-consuming annotation processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome this lack of data, synthetic data generation has been proposed, including multiple facets in the area of domain randomization to extend the data distribution . The objective of this review is to identify methods applied for synthetic data generation aiming to improve 6D pose estimation, object recognition, and semantic scene understanding in indoor scenarios . We further review methods used to extend the data distribution and discuss best practices to bridge the gap between synthetic and real-world data. We adhered to the guidelines of the systematic PRISMA technique. Three databases, IEEE Xplore, Springer Link, and ACM, and an additional manual search were conducted. In total, we identified 241 studies and included 34 in our systematic review. In summary, synthetic data generation has been performed using crop-out methods, graphic APIs, 3D modeling or authoring tools, or game engine-based methods. To extend the data distribution, varying scene parameters, i.e., lighting conditions or textures and the use of distracting objects in the scene are promising.},
  archive      = {J_CVIU},
  author       = {Hannah Schieber and Kubilay Can Demir and Constantin Kleinbeck and Seung Hee Yang and Daniel Roth},
  doi          = {10.1016/j.cviu.2023.103907},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103907},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Indoor synthetic data generation: A systematic review},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IGMG: Instance-guided multi-granularity for domain
generalizable person re-identification. <em>CVIU</em>, <em>240</em>,
103905. (<a href="https://doi.org/10.1016/j.cviu.2023.103905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning generalized feature embedding is crucial for various computer vision tasks , including domain generalizable person re-identification (ReID). ReID aims to develop deep learning-based feature embeddings that can effectively recognize individuals in both trained (source) domains and unseen target domains. However, many state-of-the-art ReID methods suffer from overfitting as they train and test within the same source domain. To address this issue, we investigate the potential of multi-granularity approaches in mitigating domain shift challenges in person re-identification. Specifically, we propose a novel framework called instance-guided multi-granularity (IGMG), which leverages style-free features through non-parametric Instance Normalization (IN) at multiple granularity levels. While high-level abstract concepts are often not shared across different classes, low- and mid-level features can offer more shareable information to enhance the model’s generalization capabilities. By incorporating this concept, our framework can dynamically eliminate style variations across various levels of abstraction. As a result, it enables the model to capture fine-grained details and high-level semantics, leading to enhanced robustness against changes in data distribution . To validate the effectiveness of our approach, we conduct extensive experiments on multiple benchmark ReID datasets. The results consistently demonstrate that our proposed framework exhibits strong generalization capabilities, performing consistently well on unseen target domains. The code is available at https://github.com/mdamranhossenbhuiyan/IGMG/ .},
  archive      = {J_CVIU},
  author       = {Amran Bhuiyan and Jimmy Xiangji Huang and Aijun An},
  doi          = {10.1016/j.cviu.2023.103905},
  journal      = {Computer Vision and Image Understanding},
  month        = {3},
  pages        = {103905},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {IGMG: Instance-guided multi-granularity for domain generalizable person re-identification},
  volume       = {240},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning feature contexts by transformer and CNN hybrid deep
network for weakly supervised person search. <em>CVIU</em>,
<em>239</em>, 103906. (<a
href="https://doi.org/10.1016/j.cviu.2023.103906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search is a computer vision task that aims to locate and re-identify specific pedestrians in images captured by non-overlapping cameras. However, the identity annotation in person search is labor-intensive, especially as the amount of data increases. Therefore, more and more studies consider training person search models using weakly supervised learning with only location annotations. The context information is useful to improve feature representations in the absence of pedestrian identity as supervision. Existing weakly supervised person search methods focus on logic-driven contexts while ignoring feature contexts. In this paper, we propose a hybrid deep network for weakly supervised person search. The hybrid architecture consists of a Transformer-based feature extraction network and a fully convolution-based region recognition head network. The purpose is to enable the model to learn feature contexts at different levels. In our network, hierarchical vision Transformers are used to extract features in order to obtain discriminative representations of scene images. The context-enhanced head network is designed to integrate different features for candidate pedestrians. In addition, a pedestrian proposal network is proposed to improve the quality of predicted proposals. Experiments are conducted on the CUHK-SYSU and the PRW benchmarks to evaluate the effectiveness of the proposed method.},
  archive      = {J_CVIU},
  author       = {Ning Lv and Xuezhi Xiang and Xinyao Wang and Yulong Qiao and Abdulmotaleb El Saddik},
  doi          = {10.1016/j.cviu.2023.103906},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103906},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning feature contexts by transformer and CNN hybrid deep network for weakly supervised person search},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MLGPnet: Multi-granularity neural network for 3D shape
recognition using pyramid data. <em>CVIU</em>, <em>239</em>, 103904. (<a
href="https://doi.org/10.1016/j.cviu.2023.103904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a Multi-Granularity 3D shape recognition network comprising point-granularity, line-granularity, and Pyramid-granularity networks, as well as multi-granularity convolutional layers (MLGPnet). The network takes pyramid data with high-level features generated from mesh data as input. The point-granularity, line-granularity, and pyramid-granularity networks respectively generate features at the point, line, and pyramid levels. Finally, two multi-granularity convolutional layers merge the features from these different levels to generate more efficient 3D shape global features. Compared to some classical 3D shape recognition network models, the proposed network achieves superior results on three publicly general-purpose datasets. Notably, among all mesh-based recognition networks, the proposed network demonstrates the best recognition accuracy and retrieval rate. Furthermore, the proposed network model performs better in terms of training time and model complexity, with faster training time and fewer model parameters, resulting in faster recognition speed.},
  archive      = {J_CVIU},
  author       = {Zekun Li and Hock Soon Seah and Baolong Guo and Muli Yang},
  doi          = {10.1016/j.cviu.2023.103904},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103904},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MLGPnet: Multi-granularity neural network for 3D shape recognition using pyramid data},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sketch-based 3D shape retrieval via teacher–student
learning. <em>CVIU</em>, <em>239</em>, 103903. (<a
href="https://doi.org/10.1016/j.cviu.2023.103903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main difficulties of sketch-based 3D shape retrieval is the significant cross-modal difference between 2D sketches and 3D shapes. Most previous works adopt one-stage methods to directly learn the aligned common embedding space of sketches and shapes by a shared classifier . However, the intra-class difference of the sketch is more significant than the shape, harming the feature learning of 3D shapes when the two modalities are considered under the shared classifier. This issue harms the discrimination of the learned common embedding space. This paper proposes a novel two-stage method to learn a common aligned embedding space via teacher–student learning to address the issue. Specifically, we first employ a classification network to learn the discriminative features of shapes. The learned shape features are considered a teacher to guide the feature learning of sketches. Moreover, we design a guidance loss to achieve the feature transfer with semantic alignment. The proposed method achieves an effective, aligned cross-modal embedding space. Experiments on three public benchmark datasets prove the superiority of the proposed method over state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Shuang Liang and Weidong Dai and Yiyang Cai and Chi Xie},
  doi          = {10.1016/j.cviu.2023.103903},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103903},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Sketch-based 3D shape retrieval via teacher–student learning},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Global key knowledge distillation framework. <em>CVIU</em>,
<em>239</em>, 103902. (<a
href="https://doi.org/10.1016/j.cviu.2023.103902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep neural networks is typically associated with their depth and width. As the performance of a model improves, it demands greater computational resources and memory. However, self-distillation offers a pathway to augment this performance by internally transferring knowledge. Traditional self-distillation methods lack guidance from teacher networks, thus lacking the ability to obtain correct and key information. In this paper, we propose a new self-distillation framework—Global key knowledge distillation framework (GK-KD). We implement error correction mechanisms to network logit outputs and assess the importance of feature maps through rank to obtain correct and important knowledge. Experiments demonstrate the effectiveness of this framework: on the CIFAR100 dataset, ResNet18 achieved a 4.56% accuracy improvement over the baseline, ResNet152 also achieved a 5.26% accuracy improvement, and for the VGG series, the average accuracy increased by 5.82%, outperforming previous research methods without adding additional computational costs. Further experiments show that this framework can effectively enhance the robustness and calibration of the network.},
  archive      = {J_CVIU},
  author       = {Junhuang Wang and Weiwei Zhang and Yufeng Guo and Peng Liang and Ming Ji and Chenghui Zhen and Hanmeng Wang},
  doi          = {10.1016/j.cviu.2023.103902},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103902},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Global key knowledge distillation framework},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving rare relation inferring for scene graph generation
using bipartite graph network. <em>CVIU</em>, <em>239</em>, 103901. (<a
href="https://doi.org/10.1016/j.cviu.2023.103901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) task is plagued by insufficient and long-tailed training samples . Thus external text knowledge has been introduced into SGG task, which augments the training dataset. However, rare relations or low-frequency events, such as &amp;gt; , are still hard to be discovered since they also rarely appear in external texts. Addressed these issues, this paper proposes a model-agnostic Bipartite Graph Network with Dual-Group Message Passing (DG-BGN). It extends the relation space of each object pair in training dataset based on the similarity of different object pairs, which cooperates external text knowledge and internal visual information. Specifically, the main framework of DG-BGN consists of three parts, that is, bipartite graph construction (BGC), dual-group message propagation (DG-MP), and pseudo label generation (PLG). For BGC, alignment between external texts and visual dataset is performed to mine information from multimodal resources. Taking object pair as the index of the grouping, dual bipartite graph containing intra/inter-group graph is established to represent predicate label probability distribution of object pair instances. Among them, inter-group bipartite graph is built based on the similarity between object pair instance. For DG-MP, efficient message passing is running on both kinds of bipartite graph through Graph Convolution Neural Networks to refine the graphs, in order to let each object pair instance learn from its similar instances. Finally in PLG, pseudo labels, which are used to train SGG models in fully supervised way, are obtained by aggregating these refined graphs. Systematic experiments on Visual Genome dataset and Conceptual Captions show that our method performs better on discovering rare relations.},
  archive      = {J_CVIU},
  author       = {Jiale Lu and Lianggangxu Chen and Haoyue Guan and Shaohui Lin and Chunhua Gu and Changbo Wang and Gaoqi He},
  doi          = {10.1016/j.cviu.2023.103901},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103901},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Improving rare relation inferring for scene graph generation using bipartite graph network},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A closer look at branch classifiers of multi-exit
architectures. <em>CVIU</em>, <em>239</em>, 103900. (<a
href="https://doi.org/10.1016/j.cviu.2023.103900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-exit architectures consist of a backbone and branch classifiers that offer shortened inference pathways to reduce the run-time of deep neural networks . In this paper, we analyze different branching patterns that vary in their allocation of computational complexity for the branch classifiers. Constant-complexity branching keeps all branches the same, while complexity-increasing and complexity-decreasing branching place more complex branches later or earlier in the backbone respectively. Through extensive experimentation on multiple backbones and datasets, we find that complexity-decreasing branches are more effective than constant-complexity or complexity-increasing branches, which achieve the best accuracy-cost trade-off. We investigate a cause by using knowledge consistency to probe the effect of adding branches onto a backbone. Our findings show that complexity-decreasing branching yields the least disruption to the feature abstraction hierarchy of the backbone, which explains the effectiveness of the branching patterns.},
  archive      = {J_CVIU},
  author       = {Shaohui Lin and Bo Ji and Rongrong Ji and Angela Yao},
  doi          = {10.1016/j.cviu.2023.103900},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103900},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A closer look at branch classifiers of multi-exit architectures},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic manipulation through the lens of geometric algebra.
<em>CVIU</em>, <em>239</em>, 103899. (<a
href="https://doi.org/10.1016/j.cviu.2023.103899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic image manipulation is a complex problem defined as the ability to change high-level features while keeping the final result visually similar to the original. Recent deep generative solutions show that manipulating semantic features in latent space produces compelling results. Even if we consider those advances, the question remains: Can we algebraically operate high-level visual semantic features in images with meaningful operations? In this paper, we demonstrate the feasibility of interpreting and manipulating image pseudovectors ( n − 1 n−1 -dimensional subspaces) as the union of visual features ( k k -dimensional subspaces, for 0 &lt; k &lt; n 0&amp;lt;k&amp;lt;n ) operated using Geometric Algebra (GA). Depending on how the latent space is organized, any GA operation would be applicable, enabling the solution to handle an open set of problems without retraining generative models for specific tasks. As a proof of concept , in this paper, we demonstrate how GA operations can be applied to manipulate subspaces in the latent space of faces to perform operations like putting on or taking off clothing accessories, transferring age characteristics, changing hairstyles, and performing semantic queries in sets of images.},
  archive      = {J_CVIU},
  author       = {Raphael dos S. Evangelista and Andre Luiz da S. Pereira and Rogério Ferreira de Moraes and Leandro A.F. Fernandes},
  doi          = {10.1016/j.cviu.2023.103899},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103899},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Semantic manipulation through the lens of geometric algebra},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative object tracking by domain contrast.
<em>CVIU</em>, <em>239</em>, 103891. (<a
href="https://doi.org/10.1016/j.cviu.2023.103891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-domain tracking method as a novel learning paradigm improves object tracking by sharing domain information with a common backbone whilst learning private information with domain-specific layers. In that context, each individual video sequence as a specific domain serves for a domain-specific layer. In this paper, we observe an intriguing finding that target features from different domains are highly confused with each other, thus having weak discriminative ability, for lack of domain interaction. To this end, we propose a simple yet effective domain interaction training paradigm called domain contrast to boost discriminative object features by effectively using amounts of instances from all the domains in two novel aspects: (1) a light-weight memory-saving training algorithm is proposed to solve the “out-of-the-memory” problem, which paves the way to couple with previous multi-domain trackers, and (2) a composite class-balanced loss is explored to tackle a more practical imbalanced problem, which not only involves the usual class imbalance problem but also accounts for the case of the totally mere negative instances. Experiments on multiple popular tracking benchmark datasets show that our mechanism consistently achieves the tracking performance gain of both base multi-domain tracker and its real-time variant thereof, without any other changes made on the original network.},
  archive      = {J_CVIU},
  author       = {Huayue Cai and Xiang Zhang and Long Lan and Changcheng Xiao and Chuanfu Xu and Jie Liu and Zhigang Luo},
  doi          = {10.1016/j.cviu.2023.103891},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103891},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Discriminative object tracking by domain contrast},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S3U-PVNet: Arbitrary-scale point cloud upsampling via
point-voxel network based on siamese self-supervised learning.
<em>CVIU</em>, <em>239</em>, 103890. (<a
href="https://doi.org/10.1016/j.cviu.2023.103890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, benefiting from the rapid development of deep learning , many research on point cloud upsampling task has been successfully proposed with remarkable performance. The quality of the upsampled point clouds is also vital for the quality of the reconstructed meshes in 3D surface reconstruction task. However, most existing methods either independently train a specific upsampling network for each scale factor or heavily rely on the paired training data as the supervision information. To address these limitations which are both inefficient and impractical for storage and computation in real applications, we present a Point-Voxel Network based on Siamese Self-supervised learning for arbitrary-Scale point cloud Upsampling (S 3 3 U-PVNet), which mainly includes a Down–Up pipeline and an Up–Down pipeline, to support self-supervised point cloud upsampling with arbitrary scale factors by a single network. The core of our network is a series of stacked point-voxel feature fusion (PVFF) modules, which can effectively and efficiently extract and fuse multi-granularity features from input point clouds. Each module includes two branches, a point-based branch that represents the sparse inputs in points to adaptively learn the spatial relationship among points, and a voxel-based branch that extract features in voxels to reduce the problem of inaccurate feature extraction caused by the irregularity and sparsity of point clouds. Furthermore, we also propose an end-to-end training objective for our siamese self-supervised network encapsulating reconstruction loss and similarity loss, which considers both global shape constraint and local geometric constraint . We achieve new state-of-the-art unsupervised upsampling results on various synthetic datasets and demonstrate the generalization of the proposed method on challenging real-world data.},
  archive      = {J_CVIU},
  author       = {Bing Han and Lixiang Deng and Yi Zheng and Shuang Ren},
  doi          = {10.1016/j.cviu.2023.103890},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103890},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {S3U-PVNet: Arbitrary-scale point cloud upsampling via point-voxel network based on siamese self-supervised learning},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BiPR-RL: Portrait relighting via bi-directional consistent
deep reinforcement learning. <em>CVIU</em>, <em>239</em>, 103889. (<a
href="https://doi.org/10.1016/j.cviu.2023.103889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portrait relighting via deep reinforcement learning could conduct portrait relighting by sequentially predicting local light editing strokes, simulating image editing by artists using brush strokes. It is a locally effective, scale-invariant and interpretable method, which yields state-of-the-art results in relighting wild portrait images. However, it still has problems due to the lack of supervision information on intermediate processes, which makes the reinforcement learning agent not receive sufficient feedback during the exploration process. This will cause incorrect strokes in the relighting. To further optimize the relighting effect, we take advantage of the fact that the light editing actions are invertible and the inverse actions can generate a backward sequence of states. The backward state sequence also provide useful supervision signals as they convey information about the future states given the action choices. Therefore, we propose to combine the forward and the backward state sequence predictions to improve the learning efficiency for portrait relighting. Using the bi-directional state sequences, we design corresponding bi-directional consistent rewards to guide the model to explore the actions with higher accuracy to maximize the performance of the proposed method. The proposed approach is used for portrait relighting tasks based on both SH-lighting and reference images . The results show that our method has further improved the performance by avoiding wrong strokes. Meanwhile, our method performs better than SOTA methods in producing locally effective relighting images for wild portrait images.},
  archive      = {J_CVIU},
  author       = {Yukai Song and Guangxin Xu and Xiaoyan Zhang and Zhijun Zhang},
  doi          = {10.1016/j.cviu.2023.103889},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103889},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {BiPR-RL: Portrait relighting via bi-directional consistent deep reinforcement learning},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generating novel scene compositions from single images and
videos. <em>CVIU</em>, <em>239</em>, 103888. (<a
href="https://doi.org/10.1016/j.cviu.2023.103888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a large dataset for training, generative adversarial networks (GANs) can achieve remarkable performance for the image synthesis task. However, training GANs in extremely low data regimes remains a challenge as overfitting often occurs, leading to memorization or training divergence. In this work, we introduce SIV-GAN, an unconditional generative model that can generate new scene compositions from a single training image or a single video clip. We propose a two-branch discriminator architecture with content and layout branches that are designed to judge internal content and scene layout realism separately from each other. This discriminator design enables the synthesis of visually plausible, novel compositions of a scene with varying content and layout while preserving the context of the original sample. Compared to previous single-image GANs, our model generates more diverse images of higher quality while not being restricted to a single image setting. We further introduce a new challenging task of learning from a few frames of a single video. In this training setup the training images are highly similar to each other, which makes it difficult for prior GAN models to achieve a synthesis of both high quality and diversity.},
  archive      = {J_CVIU},
  author       = {Vadim Sushko and Dan Zhang and Juergen Gall and Anna Khoreva},
  doi          = {10.1016/j.cviu.2023.103888},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103888},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Generating novel scene compositions from single images and videos},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Certifiable planar relative pose estimation with gravity
prior. <em>CVIU</em>, <em>239</em>, 103887. (<a
href="https://doi.org/10.1016/j.cviu.2023.103887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose a certifiable solver for the relative pose problem between two calibrated cameras under the assumptions that the unknown 3D points lay on an unknown plane and the axis of rotation is given, e.g . by an IMU. The problem is stated in terms of the rotation, translation and plane parameters and solved iteratively by an on-manifold optimization. Since the problem is nonconvex, we then try to certify this solution as the global optimum. For that, we leverage four different definitions for the search space that provide us with different certification capabilities. Since the formulations lack the Linear Independence Constraint Qualification and two of them have more constraints than variables, we cannot derive a closed-form certifier. Instead, we leverage the iterative algorithm proposed in our previous work Garcia-Salguero and Gonzalez-Jimenez (2023) that does not assume any condition on the problem formulation. Our evaluation on synthetic and real data shows that the smaller formulations are enough to certify most of the solutions, whereas the redundant ones certify all of them, including problem instances with highly noisy data. Code can be found in https://github.com/mergarsal .},
  archive      = {J_CVIU},
  author       = {Mercedes Garcia-Salguero and Javier Gonzalez-Jimenez},
  doi          = {10.1016/j.cviu.2023.103887},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103887},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Certifiable planar relative pose estimation with gravity prior},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-layered self-attention mechanism for weakly supervised
semantic segmentation. <em>CVIU</em>, <em>239</em>, 103886. (<a
href="https://doi.org/10.1016/j.cviu.2023.103886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly Supervised Semantic Segmentation (WSSS) provides efficient solutions for semantic image segmentation using image-level annotations. WSSS requires no pixel-level labeling that Fully Supervised Semantic Segmentation (FSSS) does, which is time-consuming and label-intensive. Most WSSS approaches have leveraged Class Activation Maps (CAM) or Self-Attention (SA) to generate pseudo pixel-level annotations to perform semantic segmentation tasks coupled with fully supervised approaches (e.g., Fully Convolutional Network). However, those approaches often provides incomplete supervision that mainly includes discriminative regions from the last convolutional layer . They may fail to capture regions of low- or intermediate-level features that may not be present in the last convolutional layer . To address the issue, we proposed a novel Multi-layered Self-Attention (Multi-SA) method that applies a self-attention module to multiple convolutional layers, and then stack feature maps from the self-attention layers to generate pseudo pixel-level annotations. We demonstrated that integrated feature maps from multiple self-attention layers produce higher coverage in semantic segmentation than using only the last convolutional layer through intensive experiments using standard benchmark datasets.},
  archive      = {J_CVIU},
  author       = {Avinash Yaganapu and Mingon Kang},
  doi          = {10.1016/j.cviu.2023.103886},
  journal      = {Computer Vision and Image Understanding},
  month        = {2},
  pages        = {103886},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-layered self-attention mechanism for weakly supervised semantic segmentation},
  volume       = {239},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing domain gap for continual domain adaptation in
object detection. <em>CVIU</em>, <em>238</em>, 103885. (<a
href="https://doi.org/10.1016/j.cviu.2023.103885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure reliable object detection in autonomous systems , the detector must be able to adapt to changes in appearance caused by environmental factors such as time of day, weather, and seasons. Continually adapting the detector to incorporate these changes is a promising solution, but it can be computationally costly. Our proposed approach is to selectively adapt the detector only when necessary, using new data that does not have the same distribution as the current training data . To this end, we investigate three popular metrics for domain gap evaluation and find that there is a correlation between the domain gap and detection accuracy. Therefore, we apply the domain gap as a criterion to decide when to adapt the detector. Our experiments show that our approach has the potential to improve the efficiency of the detector’s operation in real-world scenarios, where environmental conditions change in a cyclical manner, without sacrificing the overall performance of the detector. Our code is publicly available https://github.com/dadung/DGE-CDA .},
  archive      = {J_CVIU},
  author       = {Anh-Dzung Doan and Bach Long Nguyen and Surabhi Gupta and Ian Reid and Markus Wagner and Tat-Jun Chin},
  doi          = {10.1016/j.cviu.2023.103885},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103885},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Assessing domain gap for continual domain adaptation in object detection},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature learning based on connectivity estimation for
unbiased mammography mass classification. <em>CVIU</em>, <em>238</em>,
103884. (<a href="https://doi.org/10.1016/j.cviu.2023.103884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is the most commonly diagnosed female malignancy worldwide. Recent developments in deep convolutional neural networks have shown promising performance for breast cancer detection and classification. However, due to variations in appearance and small datasets, biased features can be learned by the networks in distinguishing malignant and benign instances. To investigate these aspects, we trained a densely connected convolutional network (DenseNet) to obtain representative features of breast tissue, selecting texture features representing different physical morphological representations as the network’s inputs. Connectivity estimation, represented by a connection matrix, is proposed for feature learning . To make the network provide an unbiased prediction, we used k k -nearest neighbors to find k k training samples whose connection matrices are closest to the test case. When evaluated on OMI-DB we achieved improved diagnostic accuracy 73 . 89 ± 2 . 89 % 73.89±2.89% compared with 71 . 35 ± 2 . 66 % 71.35±2.66% for the initial CNN model, which showed a statistically significant difference ( p = 0 . 00036 p=0.00036 ). The k k training samples can provide visual explanations which are useful in understanding the model predictions and failures of the model.},
  archive      = {J_CVIU},
  author       = {Guobin Li and Reyer Zwiggelaar},
  doi          = {10.1016/j.cviu.2023.103884},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103884},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Feature learning based on connectivity estimation for unbiased mammography mass classification},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MFMAM: Image inpainting via multi-scale feature module with
attention module. <em>CVIU</em>, <em>238</em>, 103883. (<a
href="https://doi.org/10.1016/j.cviu.2023.103883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, most image inpainting algorithms based on deep learning cause information loss when acquiring deep level features. It is not conducive to the image inpainting of texture details and often ignore the image restoration of semantic features , which generates image inpainting results with unreasonable structures. To address those problems, we have proposed improved image inpainting network using multi-scale feature module and improved attention module. Firstly, we propose the multi-scale fusion module based on dilated convolution to reduce information loss during the convolution process by fusing multi-scale features when acquiring image deep level features. Then, the attention module can strengthen the ability of image semantic inpainting and ensure that the proposed model can generate clear texture inpainting results. To ensure the consistency of image inpainting details and styles, the style loss and perceptual loss functions are introduced in the proposed network. The qualitative experimental results on CelebA-HQ, Places2 and Outdoor Scene datasets and common evaluation metrics such as PSNR , SSIM and FID. These metrics can validate the proposed method to be superior to the state-of-the-arts.},
  archive      = {J_CVIU},
  author       = {Yuantao Chen and Runlong Xia and Kai Yang and Ke Zou},
  doi          = {10.1016/j.cviu.2023.103883},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103883},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MFMAM: Image inpainting via multi-scale feature module with attention module},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedER: Federated learning through experience replay and
privacy-preserving data synthesis. <em>CVIU</em>, <em>238</em>, 103882.
(<a href="https://doi.org/10.1016/j.cviu.2023.103882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the medical field, multi-center collaborations are often sought to yield more generalizable findings by leveraging the heterogeneity of patient and clinical data . However, recent privacy regulations hinder the possibility to share data, and consequently, to come up with machine learning-based solutions that support diagnosis and prognosis. Federated learning (FL) aims at sidestepping this limitation by bringing AI-based solutions to data owners and only sharing local AI models, or parts thereof, that need then to be aggregated. However, most of the existing federated learning solutions are still at their infancy and show several shortcomings, from the lack of a reliable and effective aggregation scheme able to retain the knowledge learned locally to weak privacy preservation as real data may be reconstructed from model updates. Furthermore, the majority of these approaches, especially those dealing with medical data, relies on a centralized distributed learning strategy that poses robustness, scalability and trust issues. In this paper we present a federated learning strategy, FedER , that, exploiting experience replay and generative adversarial concepts, effectively integrates features from local nodes, providing models able to generalize across multiple datasets while maintaining privacy. FedER is tested on two tasks — tuberculosis and melanoma classification — using multiple datasets in order to simulate realistic non-i.i.d. medical data scenarios. Results show that our approach achieves performance comparable to standard (non-federated) learning and significantly outperforms state-of-the-art federated methods. Remarkably, we also observe that FedER enables any node model to be used as a global federation model. Indeed, the experience replay strategy with privacy-preserving synthetic data allows all node models to converge to reach the same optimum without the need of a single shared model. Code is available at https://github.com/perceivelab/FedER .},
  archive      = {J_CVIU},
  author       = {Matteo Pennisi and Federica Proietto Salanitri and Giovanni Bellitto and Bruno Casella and Marco Aldinucci and Simone Palazzo and Concetto Spampinato},
  doi          = {10.1016/j.cviu.2023.103882},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103882},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {FedER: Federated learning through experience replay and privacy-preserving data synthesis},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wavelet-based network for high dynamic range imaging.
<em>CVIU</em>, <em>238</em>, 103881. (<a
href="https://doi.org/10.1016/j.cviu.2023.103881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images has been suffering from ghosting artifacts caused by scene and objects motion. Existing methods, such as optical flow based and end-to-end deep learning based solutions, are error-prone either in detail restoration or ghosting artifacts removal. Comprehensive empirical evidence shows that ghosting artifacts caused by large foreground motion are mainly low-frequency signals and the details are mainly high-frequency signals. In this work, we propose a novel frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion in the frequency domain, and Discrete Wavelet Transform (DWT) is used to decompose inputs into different frequency bands. The low-frequency signals are used to avoid specific ghosting artifacts, while the high-frequency signals are used for preserving details. Using a U-Net as the backbone, we propose two novel modules: merging module and frequency-guided upsampling module. The merging module applies the attention mechanism to the low-frequency components to deal with the ghost caused by large foreground motion. The frequency-guided upsampling module reconstructs details from multiple frequency-specific components with rich details. In addition, a new RAW dataset is created for training and evaluating multi-frame HDR imaging algorithms in the RAW domain. Extensive experiments are conducted on public datasets and our RAW dataset, showing that the proposed FHDRNet achieves state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Tianhong Dai and Wei Li and Xilei Cao and Jianzhuang Liu and Xu Jia and Ales Leonardis and Youliang Yan and Shanxin Yuan},
  doi          = {10.1016/j.cviu.2023.103881},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103881},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Wavelet-based network for high dynamic range imaging},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Disentangled generation network for enlarged license plate
recognition and a unified dataset. <em>CVIU</em>, <em>238</em>, 103880.
(<a href="https://doi.org/10.1016/j.cviu.2023.103880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {License plate recognition plays a critical role in many practical applications, but license plates of large vehicles are difficult to be recognized due to the factors of low resolution, contamination, low illumination, and occlusion, to name a few. To overcome the above challenges, the transportation management department generally introduces the enlarged license plate behind the rear of a vehicle. However, enlarged license plates have high diversity as they are non-standard in position, size, and style. Furthermore, the background regions contain a variety of noisy information which greatly disturbs the recognition of license plate characters. In this work, we address the enlarged license plate recognition problem and contribute a dataset containing 9342 images, which cover most of the challenges of real scenes. However, the created data are still insufficient to train deep methods of enlarged license plate recognition, and building large-scale training data is very time-consuming and high labor cost. To handle this problem, we propose a novel data generation framework based on the Disentangled Generation Network (DGNet), which disentangles the generation of enlarged license plate data into the text generation and background generation in an end-to-end manner to effectively ensure the generation diversity and integrity, for robust enlarged license plate recognition. Extensive experiments on the created dataset are conducted, and we demonstrate the effectiveness of the proposed approach in three representative text recognition frameworks.},
  archive      = {J_CVIU},
  author       = {Chenglong Li and Xiaobin Yang and Guohao Wang and Aihua Zheng and Chang Tan and Jin Tang},
  doi          = {10.1016/j.cviu.2023.103880},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103880},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Disentangled generation network for enlarged license plate recognition and a unified dataset},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AFA-net: Adaptive feature attention network in image
deblurring and super-resolution for improving license plate recognition.
<em>CVIU</em>, <em>238</em>, 103879. (<a
href="https://doi.org/10.1016/j.cviu.2023.103879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although a number of license plate recognition (LPR) systems have become significantly advanced, they are still far from producing ideal images. They are still fiddled with low-resolution (LR) and motion blur , which are two of the most common problems in images extracted from automobile driving environment. In order to address this issue, we present a novel LPR method that processes LR and motion blurred images from dash cams. We propose a unique framework, AFA-Net (Adaptive Feature Attention Network) organized by image pre-restoration, feature composition and image reconstruction modules. The proposed AFA-Net can generate a clear restoration image for robust LPR performance with the images obtained from dash cams in an unconstrained environment. Furthermore, we explore the novel problem, Joint-IRLPRNet (Joint-Image Restoration and License Plate Recognition Network), that simultaneously address image restoration (i.e. SR and deblurring) and LPR in an end-to end trainable manner. Moreover, we introduce a dataset called LBLP (LR and blurred license plate (LP)). The dataset is composed of 2,779 LR and motion blurred cropped LP images, extracted from unconstrained dash cams. The experimental results on LBLP dataset indicate that AFA-Net achieves 15.28% improvement in recognition accuracy , 6.47% in sequence similarity, and 3.89% in character similarity, compared to the traditional LPR model with image restoration model. Moreover, Joint-IRLPRNet can be more effective results than AFA-Net.},
  archive      = {J_CVIU},
  author       = {Dogun Kim and Jin Kim and Eunil Park},
  doi          = {10.1016/j.cviu.2023.103879},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103879},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AFA-net: Adaptive feature attention network in image deblurring and super-resolution for improving license plate recognition},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transform, contrast and tell: Coherent entity-aware
multi-image captioning. <em>CVIU</em>, <em>238</em>, 103878. (<a
href="https://doi.org/10.1016/j.cviu.2023.103878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherent entity-aware multi-image captioning aims to generate coherent captions for neighboring images in a news document. There are coherence relationships among neighboring images because they often describe same entities or events. These relationships are important for entity-aware multi-image captioning, but are neglected in entity-aware single-image captioning. Most existing work focuses on single-image captioning, while multi-image captioning has not been explored before. Hence, this paper proposes a coherent entity-aware multi-image captioning model by making use of coherence relationships. The model consists of a Transformer-based caption generation model and two types of contrastive learning-based coherence mechanisms. The generation model generates the caption by paying attention to the image and the accompanying text. The caption-caption coherence mechanism aims to render entities in the caption of the image be also in captions of neighboring images. The caption-image-text coherence mechanism aims to render entities in the caption of the image be also in the accompanying text. To evaluate coherence between captions, two coherence evaluation metrics are proposed. The new dataset DM800K is constructed that has more images per document than two existing datasets GoodNews and NYT800K, and is more suitable for multi-image captioning. Experiments on three datasets show the proposed captioning model outperforms 7 baselines according to BLUE, Rouge, METEOR, and entity precision and recall scores . Experiments also show that the generated captions are more coherent than that of baselines according to caption entity scores, caption Rouge scores, the two proposed coherence evaluation metrics, and human evaluations.},
  archive      = {J_CVIU},
  author       = {Jingqiang Chen},
  doi          = {10.1016/j.cviu.2023.103878},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103878},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Transform, contrast and tell: Coherent entity-aware multi-image captioning},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial neon beam: A light-based physical attack to
DNNs. <em>CVIU</em>, <em>238</em>, 103877. (<a
href="https://doi.org/10.1016/j.cviu.2023.103877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the physical world, the interplay of light and shadow can significantly impact the performance of deep neural networks (DNNs), leading to substantial consequences, as exemplified by incidents such as the Tesla self-driving car collision caused by an unexpected light flash. Traditional methods involving stickers for physical attacks have inherent limitations, particularly in terms of stealthiness. In response, researchers have delved into light-based perturbations, including lasers and projectors, with the objective of achieving stealthy attacks . However, these efforts have often fallen short in terms of achieving robustness. In our study, we introduce a pioneering black-box light-based physical attack known as Adversarial Neon Beam (AdvNB). Our method stands out for its excellence in attack modeling, efficient attack simulation, and robust optimization , striking a harmonious balance between robustness and efficiency. We employ effectiveness, stealthiness, and robustness as the key metrics to evaluate the proposed AdvNB. Through rigorous evaluation, we attain an impressive 84.40% attack success rate in digital attacks , requiring an average of 189.70 queries. In real-world scenarios, our method excels with a 100% attack success rate indoors and a commendable 81.82% success rate outdoors. AdvNB demonstrates its stealthiness through comparisons with baseline samples, and it further underscores its robustness by consistently achieving a success rate exceeding 80% when targeting advanced DNN models . We carry out a comprehensive analysis of the proposed attack and note that the generated perturbations share similarities with objects present in the dataset or real-world settings. Additionally, we implement adversarial defense mechanisms against AdvNB. Given its superior performance compared to baseline methods as a light-based attack, we advocate for its broader acknowledgment and recommend its adoption as a reference point for future research and practical applications. Our code and data can be accessed from the following link: https://github.com/ChengYinHu/AdvNB .},
  archive      = {J_CVIU},
  author       = {Chengyin Hu and Weiwen Shi and Ling Tian and Wen Li},
  doi          = {10.1016/j.cviu.2023.103877},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103877},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial neon beam: A light-based physical attack to DNNs},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed multi-target tracking and active perception with
mobile camera networks. <em>CVIU</em>, <em>238</em>, 103876. (<a
href="https://doi.org/10.1016/j.cviu.2023.103876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart cameras are an essential component in surveillance and monitoring applications, and they have been typically deployed in networks of fixed camera locations. The addition of mobile cameras, mounted on robots, can overcome some of the limitations of static networks such as blind spots or back-lightning, allowing the system to gather the best information at each time by active positioning. This work presents a hybrid camera system, with static and mobile cameras, where all the cameras collaborate to observe people moving freely in the environment and efficiently visualize certain attributes from each person. Our solution combines a multi-camera distributed tracking system, to localize with precision all the people, with a control scheme that moves the mobile cameras to the best viewpoints for a specific classification task . The main contribution of this paper is a novel framework that exploits the synergies that result from the cooperation of the tracking and the control modules, obtaining a system closer to the real-world application and capable of high-level scene understanding. The static camera network provides global awareness of the control scheme to move the robots. In exchange, the mobile cameras onboard the robots provide enhanced information about the people on the scene. We perform a thorough analysis of the people monitoring application performance under different conditions thanks to the use of a photo-realistic simulation environment. Our experiments demonstrate the benefits of collaborative mobile cameras with respect to static or individual camera setups.},
  archive      = {J_CVIU},
  author       = {Sara Casao and Álvaro Serra-Gómez and Ana C. Murillo and Wendelin Böhmer and Javier Alonso-Mora and Eduardo Montijano},
  doi          = {10.1016/j.cviu.2023.103876},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103876},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Distributed multi-target tracking and active perception with mobile camera networks},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patch-based stochastic attention for image editing.
<em>CVIU</em>, <em>238</em>, 103866. (<a
href="https://doi.org/10.1016/j.cviu.2023.103866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention mechanisms have become of crucial importance in deep learning in recent years. These non-local operations, which are similar to traditional patch-based methods in image processing , complement local convolutions. However, computing the full attention matrix is an expensive step with heavy memory and computational loads . These limitations curb network architectures and performances, in particular for the case of high resolution images. We propose an efficient attention layer based on the stochastic algorithm PatchMatch, which is used for determining approximate nearest neighbors. We refer to our proposed layer as a “Patch-based Stochastic Attention Layer” (PSAL). Furthermore, we propose different approaches, based on patch aggregation, to ensure the differentiability of PSAL, thus allowing end-to-end training of any network containing our layer. PSAL has a small memory footprint and can therefore scale to high resolution images. It maintains this footprint without sacrificing spatial precision and globality of the nearest neighbors, which means that it can be easily inserted in any level of a deep architecture, even in shallower levels. We demonstrate the usefulness of PSAL on several image editing tasks, such as image inpainting , guided image colorization, and single-image super-resolution. Our code is available at: https://github.com/ncherel/psal .},
  archive      = {J_CVIU},
  author       = {Nicolas Cherel and Andrés Almansa and Yann Gousseau and Alasdair Newson},
  doi          = {10.1016/j.cviu.2023.103866},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103866},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Patch-based stochastic attention for image editing},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MDC-net: Multi-domain constrained kernel estimation network
for blind image super resolution. <em>CVIU</em>, <em>238</em>, 103865.
(<a href="https://doi.org/10.1016/j.cviu.2023.103865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based super resolution (SR) has made remarkable progress in improving image quality compared to traditional methods. However, most of these algorithms assume ideal and known image degradation processes , such as bicubic downsampling. As a result, the performance of these algorithms significantly decreases when the degradation kernel changes in low-resolution images. Therefore, blind SR networks that can estimate the degradation kernel for each image are more adaptable in realistic scenarios. Hence, how to estimate accurate degradation kernels efficiently plays an extremely important role. Nevertheless, previous blind SR designs only use image information to constrain the kernel estimation network and train the network during the inference process, resulting in limited performance and very slow runtime. In this paper, we are the first to impose constraints for the kernel estimation network in both the image domain and kernel domain to effectively optimize estimated degradation kernels. Furthermore, an efficient multi-stage network structure is leveraged to accelerate inference speed while producing high-quality kernels. The results of evaluation experiments on publicly available datasets and realistic scenarios show that, the SR network based on the proposed design can not only produce state-of-the-art high-resolution SR images but also achieve a runtime of 0.03 s for kernel estimation when a low-resolution image is enlarged by 4 times on one NVIDIA 2080Ti GPU platform.},
  archive      = {J_CVIU},
  author       = {Hang Wang and Zhenyu Ding and Cheng Cheng and Yuhai Li and Hongbin Sun},
  doi          = {10.1016/j.cviu.2023.103865},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103865},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MDC-net: Multi-domain constrained kernel estimation network for blind image super resolution},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMGNet: Collaborative multi-modal graph network for video
captioning. <em>CVIU</em>, <em>238</em>, 103864. (<a
href="https://doi.org/10.1016/j.cviu.2023.103864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In video captioning, it is very challenging to comprehensively describe multi-modal content information of a video, such as appearance, motion, and object. Prior arts often neglect interactions among multiple modalities and thus their video representations may not fully depict scene contents. In this paper, we propose a collaborative multi-modal graph network (CMGNet) to explore the interactions among multi-modal features in video captioning. Our CMGNet is composed of an encoder–decoder structure: a Compression-driven Intra-inter Attentive Graph (CIAG) encoder and an Adaptive Multi-modal Selection (AMS) decoder. Specifically, in our CIAG encoder, we first design a Basis Vector Compression (BVC) module to reduce the redundant nodes in graphs and thus improve the efficiency in coping with a large number of nodes. Then we propose an Intra-inter Attentive Graph (IAG) to improve the graph representation by sharing information across intra-and-inter nodes. Afterwards, we present an AMS decoder to generate video captions from the encoded video res presentations. In particular, we let the proposed AMS decoder learn to produce words by adaptively focusing on different modality information, thus leading to comprehensive and accurate captions. Extensive experiments on the large-scale benchmarks, i.e. , MSR-VTT and TGIF, demonstrate that our proposed CMGNet achieves the state-of-the-art.},
  archive      = {J_CVIU},
  author       = {Qi Rao and Xin Yu and Guang Li and Linchao Zhu},
  doi          = {10.1016/j.cviu.2023.103864},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103864},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CMGNet: Collaborative multi-modal graph network for video captioning},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new deep CNN for 3D text localization in the wild through
shadow removal. <em>CVIU</em>, <em>238</em>, 103863. (<a
href="https://doi.org/10.1016/j.cviu.2023.103863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text localization in the wild is challenging due to the presence of 2D and 3D texts, the presence of shadows, arbitrary orientated text with non-linear arrangements, varying lighting conditions as well as complex background. This paper proposes the first approach for 3D text localization in natural scene images through shadow removal and a new deep CNN model. In a first step, exploiting the observation that 3D text generates shadow information in natural scenes, the proposed model detects and removes the shadow pixels of 3D text based on the Generalized Gradient Vector Flow concept and a new clustering approach . The performance of the classification of 2D and 3D texts in the scene images is strengthened by using key features, including pixel strength, sharpness and edge potential, which are extracted to eliminate false text and shadow pixels. For text localization after removing shadow information, EfficientNet is used as an encoder (backbone) and UNet as a decoder in a novel way employing differential binarization . Experimental validation and comparative analysis with state-of-the-art approaches on both a new purpose-built dataset as well as on the benchmark datasets of ICDAR MLT 2019, ICDAR ArT 2019, CTW1500, DAST1500, Total-Text, and MSRATD500 for each of the different steps of the method, show that the proposed approach outperforms the existing methods.},
  archive      = {J_CVIU},
  author       = {Palaiahnakote Shivakumara and Ayan Banerjee and Lokesh Nandanwar and Umapada Pal and Apostolos Antonacopoulos and Tong Lu and Michael Blumenstein},
  doi          = {10.1016/j.cviu.2023.103863},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103863},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A new deep CNN for 3D text localization in the wild through shadow removal},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust visual question answering via semantic cross modal
augmentation. <em>CVIU</em>, <em>238</em>, 103862. (<a
href="https://doi.org/10.1016/j.cviu.2023.103862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in vision-language models have resulted in improved accuracy in visual question answering (VQA) tasks. However, their robustness remains limited when faced with out-of-distribution data containing unanswerable questions. In this study, we first construct a simple randomised VQA dataset, incorporating unanswerable questions from the VQA v2 dataset, to evaluate the robustness of a state-of-the-art VQA model. Our findings reveal that the model struggles to predict the “unknown” answer or provides inaccurate responses with high confidence scores for irrelevant questions. To address this issue without retraining the large backbone models, we propose Cross Modal Augmentation (CMA), a model-agnostic, test-time-only, multi-modal semantic augmentation technique. CMA generates multiple semantically-consistent but heterogeneous instances from the visual and textual inputs, which are then fed to the model, and the predictions are combined to achieve a more robust output. We demonstrate that implementing CMA enables the VQA model to provide more reliable answers in scenarios involving unanswerable questions, and show that the approach is generalisable across different categories of pre-trained vision language models .},
  archive      = {J_CVIU},
  author       = {Akib Mashrur and Wei Luo and Nayyar A. Zaidi and Antonio Robles-Kelly},
  doi          = {10.1016/j.cviu.2023.103862},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103862},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Robust visual question answering via semantic cross modal augmentation},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progressive recurrent network for shadow removal.
<em>CVIU</em>, <em>238</em>, 103861. (<a
href="https://doi.org/10.1016/j.cviu.2023.103861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image shadow removal is a significant task that is still unresolved. Most existing deep learning-based approaches attempt to remove the shadow directly, which cannot deal with the shadow well. To handle this issue, we consider removing the shadow in a coarse-to-fine fashion and propose a simple but effective Progressive Recurrent Network (PRNet). The network aims to remove the shadow progressively, enabling us to flexibly adjust the number of iterations to strike a balance between performance and time. Our network comprises two parts: shadow feature extraction and progressive shadow removal. Specifically, the first part is a shallow ResNet which constructs the representations of the input shadow image on its original size, preventing the loss of high-frequency details caused by the downsampling operation. The second part has two critical components: the re-integration module and the update module. The proposed re-integration module can fully use the outputs of the previous iteration, providing input for the update module for further shadow removal. In this way, the proposed PRNet makes the whole process more concise and only uses 29% network parameters than the best published method. Extensive experiments on the three benchmarks, ISTD, ISTD+, and SRD, demonstrate that our method can effectively remove shadows and achieve superior performance.},
  archive      = {J_CVIU},
  author       = {Yonghui Wang and Wengang Zhou and Hao Feng and Li Li and Houqiang Li},
  doi          = {10.1016/j.cviu.2023.103861},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103861},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Progressive recurrent network for shadow removal},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CT-VOS: Cutout prediction and tagging for self-supervised
video object segmentation. <em>CVIU</em>, <em>238</em>, 103860. (<a
href="https://doi.org/10.1016/j.cviu.2023.103860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel self-supervised Video Object Segmentation (VOS) approach that strives to achieve better visual correspondence across frames and object-background discriminability for accurate object segmentation. Distinct from previous self-supervised VOS methods , our approach is based on a learning loss formulation that takes into account both object and background information to ensure object-background discriminability, rather than using only object appearance. The objective function comprises cutout-based reconstruction (cutout region represents part of a frame, whose pixels are replaced with some constant values) and tag prediction loss terms. The cutout-based reconstruction term utilizes a simple cutout scheme to learn the pixel-wise correspondence between the current and previous frames in order to reconstruct the original current frame with added cutout region in it. The introduced cutout patch guides the model to focus on the reappearance of scene parts , thereby implicitly equipping the model to address occlusion-based scenarios Next, the tag prediction term encourages object-background separability by grouping tags of all pixels in the cutout region that are similar, while separating them from the tags of the rest of the reconstructed pixels. Additionally, we introduce a zoom-in scheme that addresses the problem of small object segmentation by capturing fine structural information at multiple scales. Our proposed approach, termed CT-VOS, achieves state-of-the-art results on two challenging benchmarks: DAVIS-2017 and Youtube-VOS. A detailed ablation showcases the importance of the proposed loss formulation to effectively establish correspondences, object-background discriminability, and the impact of our zoom-in scheme to accurately segment small-sized objects.},
  archive      = {J_CVIU},
  author       = {Jyoti Kini and Fahad Shahbaz Khan and Salman Khan and Mubarak Shah},
  doi          = {10.1016/j.cviu.2023.103860},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103860},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CT-VOS: Cutout prediction and tagging for self-supervised video object segmentation},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble learning-based method for maritime background
subtraction in open sea environments. <em>CVIU</em>, <em>238</em>,
103859. (<a href="https://doi.org/10.1016/j.cviu.2023.103859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime autonomous surface ships employ computer vision to detect open sea environments for various applications, including safety surveillance, collision prevention, and autonomous navigation , all of which have grown increasingly important as maritime traffic has increased and automated systems have become more prevalent. Background subtraction (BS) may be used for real-time change detection in unseen contexts to obtain foreground information of interest without prior knowledge and training. However, because marine settings are dynamic, classic BS methods are vulnerable to noise (e.g., reflections and ship wakes) on the ocean surface. As a result, developing effective background models is challenging, resulting in considerable false foreground information. This study proposes a novel maritime BS method based on ensemble learning theory that integrates heterogeneous BS methods. Maritime noise filtering is also included to increase the background model’s ability to cope with complex marine situations. Experimental comparisons conducted on the Maritime BS Benchmark dataset showed that the proposed method had the highest real-time detection accuracy. The proposed method may also be used to improve autonomous ships’ situational awareness abilities in open waters , enhancing maritime transportation security.},
  archive      = {J_CVIU},
  author       = {Yi-Tung Chan},
  doi          = {10.1016/j.cviu.2023.103859},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103859},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Ensemble learning-based method for maritime background subtraction in open sea environments},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive semantic transfer network for unsupervised 2D
image-based 3D model retrieval. <em>CVIU</em>, <em>238</em>, 103858. (<a
href="https://doi.org/10.1016/j.cviu.2023.103858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised 2D image-based 3D model retrieval has been a highlighted research topic to enable flexible retrieval from 2D photos to 3D shapes. Although what methods we have so far have been great progress in this aspect, it still exists some issues in learning discriminative features and to well align the distribution diversity from various domains because of the huge cross-domain interval. According to our paper, we propose an adaptive semantic transfer network (ASTN) to improve the discrimination of feature representations and conveniently narrow the discrepancy of different domains by utilizing the intermediate domain to conduct semantic alignment. Our ASTN composes of the adaptive feature encoding module (AFE) and the dynamic semantic alignment module (DSA). To improve the quality of feature representation, the AFE module deploys a new strategy that trains the learnable parameters on multiple convolutional layers , which can adaptively pay different attentions to these layers. The DSA module dynamically constructs an intermediate domain that aims to convert the familiar direct alignment into the sum of two alignments which are source-intermediate and target-intermediate alignments, effectively narrowing the domain gap and further realizing the semantic alignment. On two arduous datasets, MI3DOR-1 and MI3DOR-2, we design abundant experiments that have demonstrated the effectiveness of our suggested method.},
  archive      = {J_CVIU},
  author       = {Dan Song and Yuanxiang Yang and Wenhui Li and Zhuang Shao and Weizhi Nie and Xuanya Li and An-An Liu},
  doi          = {10.1016/j.cviu.2023.103858},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103858},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adaptive semantic transfer network for unsupervised 2D image-based 3D model retrieval},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). AdaNI: Adaptive noise injection to improve adversarial
robustness. <em>CVIU</em>, <em>238</em>, 103855. (<a
href="https://doi.org/10.1016/j.cviu.2023.103855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have been proven vulnerable to adversarial perturbations, which narrow their applications in safe-critical scenarios such as video surveillance and autonomous driving . To counter this threat, a very recent line of adversarial defense methods is proposed to increase the uncertainty of DNNs via injecting random noises in both the training and testing process. Note the existing defense methods usually inject noises uniformly to DNNs. We argue that the magnitude of noises is highly correlated with the response of corresponding features and the randomness on important feature spots can further weaken adversarial attacks . As such, we propose a new method, namely AdaNI , which can increase feature randomness via Ada ptive N oise I njection to improve the adversarial robustness. Compared to existing methods, our method creates non-unified random noises guided by features and then injects them into DNNs adaptively. Extensive experiments are conducted on several datasets ( e.g. , CIFAR10, CIFAR100, Mini-ImageNet) with comparisons to state-of-the-art defense methods, which corroborates the efficacy of our method against a variety of powerful white-box attacks ( e.g. , FGSM, PGD , C&amp;W, Auto Attack) and black-box attacks ( e.g. , Transferable, ZOO, Square Attack). Moreover, our method is adapted to improve the robustness of DeepFake detection to demonstrate its applicability.},
  archive      = {J_CVIU},
  author       = {Yuezun Li and Cong Zhang and Honggang Qi and Siwei Lyu},
  doi          = {10.1016/j.cviu.2023.103855},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103855},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {AdaNI: Adaptive noise injection to improve adversarial robustness},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scene adaptive mechanism for action recognition.
<em>CVIU</em>, <em>238</em>, 103854. (<a
href="https://doi.org/10.1016/j.cviu.2023.103854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene knowledge plays an important role in visual analysis. For the task of action recognition, human activities often occur in specific scenes. However, it should be emphasised that the association between actions and scenes is very complex. Simplistic attempts to improve the effectiveness of action recognition by intensifying or suppressing the scene knowledge are unwise. In this article, we tackle this problem by proposing a new action recognition framework based on the Scene Adaptive Mechanism. Specifically, with the Scene Knowledge Modulation module, we can control the feature extractors to either suppress or intensify scene knowledge. And then, through an Adaptive Fusion Layer, the role of scene information in different visual feature sequences can thus be dynamically regulated and fused. The resulting model is abbreviated as SAM-Net. Our method serves as a pluggable module , capable of integration into other backbones to further enhance their performance. We perform extensive experiments on three large datasets: Something-Something V1&amp;V2 and Kinetics-400. The quantitative and qualitative experimental results demonstrate the effectiveness of SAM-Net, with a great improvement in performance compared to the baseline methods .},
  archive      = {J_CVIU},
  author       = {Cong Wu and Xiao-Jun Wu and Tianyang Xu and Josef Kittler},
  doi          = {10.1016/j.cviu.2023.103854},
  journal      = {Computer Vision and Image Understanding},
  month        = {1},
  pages        = {103854},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Scene adaptive mechanism for action recognition},
  volume       = {238},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
