<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PARCO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="parco---21">PARCO - 21</h2>
<ul>
<li><details>
<summary>
(2024). FastPTM: Fast weights loading of pre-trained models for
parallel inference service provisioning. <em>PARCO</em>, <em>122</em>,
103114. (<a href="https://doi.org/10.1016/j.parco.2024.103114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained models (PTMs) have demonstrated great success in a variety of NLP and CV tasks and have become a significant development in the field of deep learning. However, the large memory and high computational requirements associated with PTMs can increase the cost and time of inference, limiting their service provisioning in practical applications. To improve the Quality of Service (QoS) of PTM applications by reducing waiting and response times, we propose the FastPTM framework. This general framework aims to accelerate PTM inference services in a multi-tenant environment by reducing model loading time and switching overhead on GPUs. The framework utilizes a fast weights loading method based on weights and model separation of PTMs to efficiently accelerate parallel inference services in resource-constrained environments. Furthermore, an online scheduling algorithm is designed to reduce the inference service time. The results of the experiments indicate that FastPTM can improve the throughput of inference services by an average of 4x and up to 8.2x, while reducing the number of switches by 4.7x and the number of overtimes by 15.3x.},
  archive      = {J_PARCO},
  author       = {Fenglong Cai and Dong Yuan and Zhe Yang and Yonghui Xu and Wei He and Wei Guo and Lizhen Cui},
  doi          = {10.1016/j.parco.2024.103114},
  journal      = {Parallel Computing},
  month        = {11},
  pages        = {103114},
  shortjournal = {Parallel Comput.},
  title        = {FastPTM: Fast weights loading of pre-trained models for parallel inference service provisioning},
  volume       = {122},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed consensus-based estimation of the leading
eigenvalue of a non-negative irreducible matrix. <em>PARCO</em>,
<em>122</em>, 103113. (<a
href="https://doi.org/10.1016/j.parco.2024.103113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an algorithm to solve the problem of estimating the largest eigenvalue and its corresponding eigenvector for irreducible matrices in a distributed manner. The proposed algorithm utilizes a network of computational nodes that interact with each other, forming a strongly connected digraph where each node handles one row of the matrix, without the need for centralized storage or knowledge of the entire matrix. Each node possesses a solution space, and the intersection of all these solution spaces contains the leading eigenvector of the matrix. Initially, each node selects a random vector from its solution space, and then, while interacting with its neighbors, updates the vector at each step by solving a quadratically constrained linear program (QCLP). The updates are done so that the nodes reach a consensus on the leading eigenvector of the matrix. The numerical outcomes demonstrate the effectiveness of our proposed method.},
  archive      = {J_PARCO},
  author       = {Rahim Alizadeh and Shahriar Bijani and Fatemeh Shakeri},
  doi          = {10.1016/j.parco.2024.103113},
  journal      = {Parallel Computing},
  month        = {11},
  pages        = {103113},
  shortjournal = {Parallel Comput.},
  title        = {Distributed consensus-based estimation of the leading eigenvalue of a non-negative irreducible matrix},
  volume       = {122},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel pattern compiler for automatic global
optimizations. <em>PARCO</em>, <em>122</em>, 103112. (<a
href="https://doi.org/10.1016/j.parco.2024.103112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance computing (HPC) systems enable scientific advances through simulation and data processing. The heterogeneity in HPC hardware and software increases the application complexity and reduces its maintainability and productivity. This work proposes a prototype implementation for a parallel pattern-based source-to-source compiler to address these challenges. The prototype limits the complexity of parallelism and heterogeneous architectures to parallel patterns that are optimized towards a given target architecture. By applying high-level optimizations and a mapping between parallel patterns and execution units during compile time, portability between systems is achieved. The compiler can address architectures with shared memory, distributed memory, and accelerator offloading. The approach shows speedups for seven of the nine supported Rodinia benchmarks, reaching speedups of up to twelve times. Porting LULESH to the Parallel Pattern Language (PPL) shows a compression of code size by 65% (3.4 thousand lines of code) through a more concise expression and a higher level of abstraction. The tool’s limitations include dynamic algorithms that are challenging to analyze statically and overheads during the compile time optimization. This paper is an extended version of a previous PMAM publication (Schmitz et al., 2024).},
  archive      = {J_PARCO},
  author       = {Adrian Schmitz and Semih Burak and Julian Miller and Matthias S. Müller},
  doi          = {10.1016/j.parco.2024.103112},
  journal      = {Parallel Computing},
  month        = {11},
  pages        = {103112},
  shortjournal = {Parallel Comput.},
  title        = {Parallel pattern compiler for automatic global optimizations},
  volume       = {122},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Task scheduling in cloud computing based on grey wolf
optimization with a new encoding mechanism. <em>PARCO</em>,
<em>122</em>, 103111. (<a
href="https://doi.org/10.1016/j.parco.2024.103111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task scheduling in the cloud computing still remains challenging in terms of performance. Several evolutionary-derived algorithms have been proposed to solve or alleviate this problem. However, evolutionary algorithms have good exploration ability, but the performance drops significantly in high dimensions. To address this issue, considering the characteristic of task scheduling in cloud computing (i.e. all task-VM mappings are 1-dimensional and have the same search range), we propose a task scheduling algorithm based on grey wolf optimization using a new encoding mechanism (GWOEM) in this work. Through this new encoding mechanism, greedy and evolutionary algorithms are rationally integrated in GWOEM. Besides, based on the new mechanism, the dimension of search space is reduced to 1 and the key parameter (i.e., the population size) is eliminated. We apply the proposed GWOEM to the Google Cloud Jobs dataset (GoCJ) and demonstrate better performance than the prior state of the art in terms of makespan.},
  archive      = {J_PARCO},
  author       = {Xingwang Huang and Min Xie and Dong An and Shubin Su and Zongliang Zhang},
  doi          = {10.1016/j.parco.2024.103111},
  journal      = {Parallel Computing},
  month        = {11},
  pages        = {103111},
  shortjournal = {Parallel Comput.},
  title        = {Task scheduling in cloud computing based on grey wolf optimization with a new encoding mechanism},
  volume       = {122},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated OpenMP mutation testing framework for
performance optimization. <em>PARCO</em>, <em>121</em>, 103097. (<a
href="https://doi.org/10.1016/j.parco.2024.103097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance optimization continues to be a challenge in modern HPC software. Existing performance optimization techniques, including profiling-based and auto-tuning techniques, fail to indicate program modifications at the source level thus preventing their portability across compilers. This paper describes Muppet , a new approach that identifies program modifications called mutations aimed at improving program performance. Muppet ’s mutations help developers reason about performance defects and missed opportunities to improve performance at the source code level. In contrast to compiler techniques that optimize code at intermediate representations (IR), Muppet uses the idea of source-level mutation testing to relax correctness constraints and automatically discover optimization opportunities that otherwise are not feasible using the IR. We demonstrate the Muppet ’s concept in the OpenMP programming model. Muppet generates a list of OpenMP mutations that alter the program parallelism in various ways, and is capable of running a variety of optimization algorithms such as delta debugging, Bayesian Optimization and decision tree optimization to find a subset of mutations which, when applied to the original program, cause the most speedup while maintaining program correctness. When Muppet is evaluated against a diverse set of benchmark programs and proxy applications, it is capable of finding sets of mutations that induce speedup in 75.9% of the evaluated programs.},
  archive      = {J_PARCO},
  author       = {Dolores Miao and Ignacio Laguna and Giorgis Georgakoudis and Konstantinos Parasyris and Cindy Rubio-González},
  doi          = {10.1016/j.parco.2024.103097},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103097},
  shortjournal = {Parallel Comput.},
  title        = {An automated OpenMP mutation testing framework for performance optimization},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstractions for c++ code optimizations in parallel
high-performance applications. <em>PARCO</em>, <em>121</em>, 103096. (<a
href="https://doi.org/10.1016/j.parco.2024.103096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computational problems consider memory throughput a performance bottleneck, especially in the domain of parallel computing. Software needs to be attuned to hardware features like cache architectures or concurrent memory banks to reach a decent level of performance efficiency. This can be achieved by selecting the right memory layouts for data structures or changing the order of data structure traversal. In this work, we present an abstraction for traversing a set of regular data structures (e.g., multidimensional arrays) that allows the design of traversal-agnostic algorithms. Such algorithms can easily optimize for memory performance and employ semi-automated parallelization or autotuning without altering their internal code. We also add an abstraction for autotuning that allows defining tuning parameters in one place and removes boilerplate code. The proposed solution was implemented as an extension of the Noarr library that simplifies a layout-agnostic design of regular data structures. It is implemented entirely using C ++ template meta-programming without any nonstandard dependencies, so it is fully compatible with existing compilers, including CUDA NVCC or Intel DPC++. We evaluate the performance and expressiveness of our approach on the Polybench-C benchmarks.},
  archive      = {J_PARCO},
  author       = {Jiří Klepl and Adam Šmelko and Lukáš Rozsypal and Martin Kruliš},
  doi          = {10.1016/j.parco.2024.103096},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103096},
  shortjournal = {Parallel Comput.},
  title        = {Abstractions for c++ code optimizations in parallel high-performance applications},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobilizing underutilized storage nodes via job path: A
job-aware file striping approach. <em>PARCO</em>, <em>121</em>, 103095.
(<a href="https://doi.org/10.1016/j.parco.2024.103095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users’ limited understanding of the storage system architecture prevents them from fully utilizing the parallel I/O capability of the storage system, leading to a negative impact on the overall performance of supercomputers. Therefore, exploring effective strategies for utilizing parallel I/O capabilities is imperative. In this regard, we conduct an analysis of the workload on two production supercomputers’ Object Storage Targets (OSTs) and study the potential inefficient I/O patterns for high performance computing jobs. Our research findings indicate that under the traditional stripe settings that most supercomputers use to ensure stability, the real-time load on OSTs is severely unbalanced. This imbalance results in I/O requests that fail to fully utilize the available OSTs. To tackle this issue, we propose a job-aware optimization approach, which includes static and dynamic file striping. Static file striping optimizes all user jobs, whereas dynamic file striping employs clustering of job names and job paths to extract similarities among jobs and predict partially stripe-optimizable jobs for users. Additionally, a stripe recovery mechanism is employed to mitigate the negative impact of stripe misconfigurations. This approach appropriately adjusts the file stripe layout based on the job’s I/O pattern, allowing for better mobilization of underutilized OSTs to enhance parallel I/O capabilities. Through experimental verification, the number of OSTs that jobs can use has been increased, effectively improving the parallel I/O performance of the job without significantly affecting operational stability.},
  archive      = {J_PARCO},
  author       = {Gang Xian and Wenxiang Yang and Yusong Tan and Jinghua Feng and Yuqi Li and Jian Zhang and Jie Yu},
  doi          = {10.1016/j.parco.2024.103095},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103095},
  shortjournal = {Parallel Comput.},
  title        = {Mobilizing underutilized storage nodes via job path: A job-aware file striping approach},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NxtSPR: A deadlock-free shortest path routing dedicated to
relaying for triplet-based many-core architecture. <em>PARCO</em>,
<em>121</em>, 103094. (<a
href="https://doi.org/10.1016/j.parco.2024.103094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deadlock-free routing is a significant challenge in Network-on-Chip (NoC) design as it affects the network’s latency, power consumption, and load balance, impacting the performance of multi-processor systems-on-chip. However, achieving deadlock-free routing will routinely result in expensive overhead as previous solutions either sacrifice performance or power efficiency to proactively avoid deadlocks or impose high hardware complexity to resolve deadlocks when they occur reactively. Utilizing the various characteristics of NoC to implement deadlock-free routing can be significantly more cost-effective with less impact on performance. This paper proposes a relay routing algorithm (NxtSPR) with a shortest path property and a deadlock prevention mechanism based on a synchronized Hamiltonian ring. The proposal is based on an in-depth study of the characteristics of a Triplet-Based many-core Architecture (TriBA) NoC. We establish various important topology-related theories and perform a formal verification (proof-based) for them. By utilizing the critical subgraph and apex of TriBA, NxtSPR can pre-calculate downstream nodes forwarding ports for packets by using a concise judgment strategy. This significantly reduces the computational overhead required for data transmission while optimizing the pipeline design of routers to decrease packet transmission latency and power consumption compared to other TriBA routing algorithms. We group the data transmissions according to the levels of maximum Hamiltonian edges a packet will traverse during its transmission life cycle. Independent data transmissions between groups can avoid mutual interference and resource competition, eliminating potential deadlocks. Gem5 simulation results show that, under the synthetic traffic patterns, compared to the representative (Table) and up-to-date (SPR4T) routing algorithms, NxtSPR achieves a 20.19%, 14.76%, and 5.54%, 4.66% reduction in average packet latency and per-packet power consumption, respectively. Moreover, it has an average of 18.50% and 4.34% improvement in throughput, as compared to them. PARSEC benchmark results show that NxtSPR reduces application runtime by up to a maximum of 22.30% and 12.82% compared to Table and SPR4T, and running the same applications with TriBA results in a maximum runtime reduction of 10.77% compared to 2D-Mesh.},
  archive      = {J_PARCO},
  author       = {Chunfeng Li and Karim Soliman and Fei Yin and Jin Wei and Feng Shi},
  doi          = {10.1016/j.parco.2024.103094},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103094},
  shortjournal = {Parallel Comput.},
  title        = {NxtSPR: A deadlock-free shortest path routing dedicated to relaying for triplet-based many-core architecture},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-GPU 3D k-nearest neighbors computation with
application to ICP, point cloud smoothing and normals computation.
<em>PARCO</em>, <em>121</em>, 103093. (<a
href="https://doi.org/10.1016/j.parco.2024.103093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k-Nearest Neighbors algorithm is a fundamental algorithm that finds applications in many fields like Machine Learning, Computer Graphics, Computer Vision, and others. The algorithm determines the closest points (d-dimensional) of a reference set R according to a query set of points Q under a specific metric (Euclidean, Mahalanobis, Manhattan, etc.). This work focuses on the utilization of multiple Graphical Processing Units for the acceleration of the k-Nearest Neighbors algorithm with large or very large sets of 3D points. With the proposed approach the space of the reference set is divided into a 3D grid which is used to facilitate the search for the nearest neighbors. The search in the grid is performed in a multiresolution manner starting from a high-resolution grid and ending up in a coarse one, thus accounting for point clouds that may have non-uniform sampling and/or outliers. Three important algorithms in reverse engineering are revisited and new multi-GPU versions are proposed based on the introduced KNN algorithm. More specifically, the new multi-GPU approach is applied to the Iterative Closest Point algorithm, to the point cloud smoothing, and to the point cloud normal vectors computation and orientation problem. A series of tests and experiments have been conducted and discussed in the paper showing the merits of the proposed multi-GPU approach.},
  archive      = {J_PARCO},
  author       = {Alexander Agathos and Philip Azariadis},
  doi          = {10.1016/j.parco.2024.103093},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103093},
  shortjournal = {Parallel Comput.},
  title        = {Multi-GPU 3D k-nearest neighbors computation with application to ICP, point cloud smoothing and normals computation},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). WBSP: Addressing stragglers in distributed machine learning
with worker-busy synchronous parallel. <em>PARCO</em>, <em>121</em>,
103092. (<a href="https://doi.org/10.1016/j.parco.2024.103092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter server is widely used in distributed machine learning to accelerate training. However, the increasing heterogeneity of workers’ computing capabilities leads to the issue of stragglers, making parameter synchronization challenging. To address this issue, we propose a solution called Worker-Busy Synchronous Parallel (WBSP). This approach eliminates the waiting time of fast workers during the synchronization process and decouples the gradient upload and model download of fast workers into asymmetric parts. By doing so, it allows fast workers to complete multiple steps of local training and upload more gradients to the server, improving computational resource utilization. Additionally, the global model is only updated when the slowest worker uploads the gradients, ensuring the consistency of global models that are pulled down by all workers and the convergence of the global model. Building upon WBSP, we propose an optimized version to further reduce the communication overhead. It enables parallel execution of communication and computation tasks on workers to shorten the global synchronization interval, thereby improving training speed. We conduct theoretical analyses for the proposed mechanisms. Extensive experiments verify that our mechanism can reduce the required time to achieve the target accuracy by up to 60% compared with the fastest method and increase the proportion of computation time from 55%–72% in existing methods to 91%.},
  archive      = {J_PARCO},
  author       = {Duo Yang and Bing Hu and An Liu and A-Long Jin and Kwan L. Yeung and Yang You},
  doi          = {10.1016/j.parco.2024.103092},
  journal      = {Parallel Computing},
  month        = {9},
  pages        = {103092},
  shortjournal = {Parallel Comput.},
  title        = {WBSP: Addressing stragglers in distributed machine learning with worker-busy synchronous parallel},
  volume       = {121},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extending the limit of LR-TDDFT on two different approaches:
Numerical algorithms and new sunway heterogeneous supercomputer.
<em>PARCO</em>, <em>120</em>, 103085. (<a
href="https://doi.org/10.1016/j.parco.2024.103085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-principles time-dependent density functional theory (TDDFT) is a powerful tool to accurately describe the excited-state properties of molecules and solids in condensed matter physics, computational chemistry, and materials science. However, a perceived drawback in TDDFT calculations is its ultrahigh computational cost O ( N 5 ∼ N 6 ) O(N5∼N6) and large memory usage O ( N 4 ) O(N4) especially for plane-wave basis set, confining its applications to large systems containing thousands of atoms. Here, we present a massively parallel implementation of linear-response TDDFT (LR-TDDFT) and accelerate LR-TDDFT in two different aspects: (1) numerical algorithms on the X86 supercomputer and (2) optimizations on the heterogeneous architecture of the new Sunway supercomputer. Furthermore, we carefully design the parallel data and task distribution schemes to accommodate the physical nature of different computation steps. By utilizing these two different methods, our implementation can gain an overall speedup of 10x and 80x and efficiently scales to large systems up to 4096 and 2744 atoms within dozens of seconds.},
  archive      = {J_PARCO},
  author       = {Qingcai Jiang and Zhenwei Cao and Xinhui Cui and Lingyun Wan and Xinming Qin and Huanqi Cao and Hong An and Junshi Chen and Jie Liu and Wei Hu and Jinlong Yang},
  doi          = {10.1016/j.parco.2024.103085},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103085},
  shortjournal = {Parallel Comput.},
  title        = {Extending the limit of LR-TDDFT on two different approaches: Numerical algorithms and new sunway heterogeneous supercomputer},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach for low-power heterogeneous parallel
implementation of ALC-PSO algorithm using OmpSs and CUDA.
<em>PARCO</em>, <em>120</em>, 103084. (<a
href="https://doi.org/10.1016/j.parco.2024.103084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PSO (particle swarm optimization), is an intelligent search method for finding the best solution according to population state. Various parallel implementations of this algorithm have been presented for intensive-computing applications. The ALC-PSO algorithm (PSO with an aging leader and challengers) is an improved population-based procedure that increases convergence rapidity, compared to the traditional PSO. In this paper, we propose a low-power heterogeneous parallel implementation of ALC-PSO algorithm using OmpSs and CUDA, for execution on both CPU and GPU cores. This is the first effort to heterogeneous parallel implementing ALC-PSO algorithm with combination of OmpSs and CUDA. This hybrid parallel programming approach increases the performance and efficiency of the intensive-computing applications. The proposed approach of this article is also useful and applicable for heterogeneous parallel execution of the other improved versions of PSO algorithm, on both CPUs and GPUs. The results demonstrate that the proposed approach provides higher performance, in terms of delay and power consumption, than the existence implementations of ALC-PSO algorithm.},
  archive      = {J_PARCO},
  author       = {Fahimeh Yazdanpanah and Mohammad Alaei},
  doi          = {10.1016/j.parco.2024.103084},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103084},
  shortjournal = {Parallel Comput.},
  title        = {An approach for low-power heterogeneous parallel implementation of ALC-PSO algorithm using OmpSs and CUDA},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning based modulation classification for
multipath channels. <em>PARCO</em>, <em>120</em>, 103083. (<a
href="https://doi.org/10.1016/j.parco.2024.103083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL)-based automatic modulation classification (AMC) is a primary research field for identifying modulation types. However, traditional DL-based AMC approaches rely on hand-crafted features, which can be time-consuming and may not capture all relevant information in the signal. Additionally, they are centralized solutions that are trained on large amounts of data acquired from local clients and stored on a server, leading to weak performance in terms of correct classification probability. To address these issues, a federated learning (FL)-based AMC approach is proposed, called FL-MP-CNN-AMC, which takes into account the effects of multipath channels (reflected and scattered paths) and considers the use of a modified loss function for solving the class imbalance problem caused by these channels. In addition, hyperparameter tuning and optimization of the loss function are discussed and analyzed to improve the performance of the proposed approach. The classification performance is investigated by considering the effects of interference level, delay spread, scattered and reflected paths, phase offset, and frequency offset. The simulation results show that the proposed approach provides excellent performance in terms of correct classification probability, confusion matrix, convergence and communication overhead when compared to contemporary methods.},
  archive      = {J_PARCO},
  author       = {Sanjay Bhardwaj and Da-Hye Kim and Dong-Seong Kim},
  doi          = {10.1016/j.parco.2024.103083},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103083},
  shortjournal = {Parallel Comput.},
  title        = {Federated learning based modulation classification for multipath channels},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PPS: Fair and efficient black-box scheduling for
multi-tenant GPU clusters. <em>PARCO</em>, <em>120</em>, 103082. (<a
href="https://doi.org/10.1016/j.parco.2024.103082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tenant GPU clusters are common, where users purchase GPU quota to run their neural network training jobs. However, strict quota-based scheduling often leads to cluster under-utilization, while allowing quota groups to use excess GPUs improves utilization but results in fairness problems. We propose PPS, a probabilistic prediction based scheduler, which uses job history statistics to predict future cluster status for making good scheduling decisions. Different from existing schedulers that rely on deep learning frameworks to adjust bad scheduling decisions and/or require detailed job information, PPS treats jobs as black boxes in that PPS runs a job to completion without adjustment once scheduled and requires only aggregate job statistics. The black-box feature is favorable due to its good generality, compatibility and security, and made possible by the predictability of aggregate resource utilization statistics of large clusters. Extensive experiments show that PPS achieves high cluster utilization and good fairness simultaneously.},
  archive      = {J_PARCO},
  author       = {Kaihao Ma and Zhenkun Cai and Xiao Yan and Yang Zhang and Zhi Liu and Yihui Feng and Chao Li and Wei Lin and James Cheng},
  doi          = {10.1016/j.parco.2024.103082},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103082},
  shortjournal = {Parallel Comput.},
  title        = {PPS: Fair and efficient black-box scheduling for multi-tenant GPU clusters},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing the impact of CUDA versions on GPU applications.
<em>PARCO</em>, <em>120</em>, 103081. (<a
href="https://doi.org/10.1016/j.parco.2024.103081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CUDA toolkits are widely used to develop applications running on NVIDIA GPUs. They include compilers and are frequently updated to integrate state-of-the-art compilation techniques. Hence, many HPC users believe that the latest CUDA toolkit will improve application performance; however, considering results from CPU compilers, there are cases where this is not true. In this paper, we thoroughly evaluate the impact of CUDA toolkit version on the performance, power consumption, and energy consumption of GPU applications with four GPU architectures. Our results show that though the latest CUDA toolkit obtains the best performance, power consumption, and energy consumption for many applications in most cases, but we found a few exceptions. For such applications, we conducted an in-depth analysis using the SASS to identify why older CUDA toolkit achieve performance improvement. Our analysis showed that the factors that caused them are by three phenomena: aggressive loop unrolling, inefficient instruction scheduling, and the impact of host compilers.},
  archive      = {J_PARCO},
  author       = {Kohei Yoshida and Shinobu Miwa and Hayato Yamaki and Hiroki Honda},
  doi          = {10.1016/j.parco.2024.103081},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103081},
  shortjournal = {Parallel Comput.},
  title        = {Analyzing the impact of CUDA versions on GPU applications},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel optimization and application of unstructured sparse
triangular solver on new generation of sunway architecture.
<em>PARCO</em>, <em>120</em>, 103080. (<a
href="https://doi.org/10.1016/j.parco.2024.103080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale sparse linear equation solver plays an important role in both numerical simulation and artificial intelligence, and sparse triangular equation solver is a key step in solving sparse linear equation systems. Its parallel optimization can effectively improve the efficiency of solving sparse linear equation systems. In this paper, we design and implement a parallel algorithm for solving sparse triangular equations in combination with the features of the new generation of Sunway architecture, and optimize the access and communication respectively for 949 real equations and 32 complex equations in the SuiteSparse collection. The solution efficiency of the algorithm presented in this paper outperforms the cuSparse algorithm on NVIDIA V100 GPU platforms in more than 71% of the cases, and the speedup is even better in solving larger cases (matrix size greater than 10,000): our method increases the speedup from 1.29 time of the previous version to an average speedup of 5.54 and the best speedup of 32.18 over the sequential method on the next generation of Sunway architecture when using 64 slave cores.},
  archive      = {J_PARCO},
  author       = {Jianjiang Li and Lin Li and Qingwei Wang and Wei Xue and Jiabi Liang and Jinliang Shi},
  doi          = {10.1016/j.parco.2024.103080},
  journal      = {Parallel Computing},
  month        = {6},
  pages        = {103080},
  shortjournal = {Parallel Comput.},
  title        = {Parallel optimization and application of unstructured sparse triangular solver on new generation of sunway architecture},
  volume       = {120},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial for parallel computing. <em>PARCO</em>,
<em>119</em>, 103065. (<a
href="https://doi.org/10.1016/j.parco.2024.103065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PARCO},
  author       = {Anne Benoit},
  doi          = {10.1016/j.parco.2024.103065},
  journal      = {Parallel Computing},
  month        = {2},
  pages        = {103065},
  shortjournal = {Parallel Comput.},
  title        = {Editorial for parallel computing},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating FPGA-based hardware acceleration with relational
databases. <em>PARCO</em>, <em>119</em>, 103064. (<a
href="https://doi.org/10.1016/j.parco.2024.103064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosion of data over the last decades puts significant strain on the computational capacity of the central processing unit (CPU), challenging online analytical processing (OLAP). While previous studies have shown the potential of using Field Programmable Gate Arrays (FPGAs) in database systems, integrating FPGA-based hardware acceleration with relational databases remains challenging because of the complex nature of relational database operations and the need for specialized FPGA programming skills. Additionally, there are significant challenges related to optimizing FPGA-based acceleration for specific database workloads, ensuring data consistency and reliability, and integrating FPGA-based hardware acceleration with existing database infrastructure. In this study, we proposed a novel end-to-end FPGA-based acceleration system that supports native SQL statements and storage engine. We defined a callback process to reload the database query logic and customize the scanning method for database queries. Through middleware process development, we optimized offloading efficiency on PCIe bus by scheduling data transmission and computation in a pipeline workflow. Additionally, we designed a novel five-stage FPGA microarchitecture module that achieves optimal clock frequency, further enhancing offloading efficiency. Results from systematic evaluations indicate that our solution allows a single FPGA card to perform as well as 8 CPU query processes, while reducing CPU load by 34%. Compared to using 4 CPU cores, our FPGA-based acceleration system reduces query latency by 1.7 times without increasing CPU load. Furthermore, our proposed solution achieves 2.1 times computation speedup for data filtering compared with the software baseline in a single core environment. Overall, our work presents a valuable end-to-end hardware acceleration system for OLAP databases.},
  archive      = {J_PARCO},
  author       = {Ke Liu and Haonan Tong and Zhongxiang Sun and Zhixin Ren and Guangkui Huang and Hongyin Zhu and Luyang Liu and Qunyang Lin and Chuang Zhang},
  doi          = {10.1016/j.parco.2024.103064},
  journal      = {Parallel Computing},
  month        = {2},
  pages        = {103064},
  shortjournal = {Parallel Comput.},
  title        = {Integrating FPGA-based hardware acceleration with relational databases},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast data-dependence profiling through prior static
analysis. <em>PARCO</em>, <em>119</em>, 103063. (<a
href="https://doi.org/10.1016/j.parco.2024.103063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-dependence profiling is a program-analysis technique for detecting parallelism opportunities in sequential programs. It captures data dependences that actually occur during program execution, filtering parallelism-preventing dependences that purely static methods assume only because they lack critical runtime information, such as the values of pointers and array indices. Profiling, however, suffers from high runtime overhead. In our earlier work, we accelerated data-dependence profiling by excluding polyhedral loops that can be handled statically using certain compilers and eliminating scalar variables that create statically-identifiable data dependences. In this paper, we combine the two methods and integrate them into DiscoPoP, a data-dependence profiler and parallelism discovery tool. Additionally, we detect reduction patterns statically and unify the three static analyses with the DiscoPoP framework to significantly diminish the profiling overhead and for a wider range of programs. We have evaluated our unified approaches with 49 benchmarks from three benchmark suites and two computer simulation applications. The evaluation results show that our approach reports fewer false positive and negative data dependences than the original data-dependence profiler and reduces the profiling time by at least 43%, with a median reduction of 76% across all programs. Also, we identify 40% of reduction cases statically and eliminate the associated profiling overhead for these cases.},
  archive      = {J_PARCO},
  author       = {Mohammad Norouzi and Nicolas Morew and Qamar Ilias and Lukas Rothenberger and Ali Jannesari and Felix Wolf},
  doi          = {10.1016/j.parco.2024.103063},
  journal      = {Parallel Computing},
  month        = {2},
  pages        = {103063},
  shortjournal = {Parallel Comput.},
  title        = {Fast data-dependence profiling through prior static analysis},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GPU-based hydrodynamic simulator with boid interactions.
<em>PARCO</em>, <em>119</em>, 103062. (<a
href="https://doi.org/10.1016/j.parco.2023.103062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hydrodynamic simulation system using the GPU compute shaders of DirectX for simulating virtual agent behaviors and navigation inside a smoothed particle hydrodynamical (SPH) fluid environment with real-time water mesh surface reconstruction. The current SPH literature includes interactions between SPH and heterogeneous meshes but seldom involves interactions between SPH and virtual boid agents. The contribution of the system lies in the combination of the parallel smoothed particle hydrodynamics model with the distributed boid model of virtual agents to enable agents to interact with fluids. The agents based on the boid algorithm influence the motion of SPH fluid particles, and the forces from the SPH algorithm affect the movement of the boids. To enable realistic fluid rendering and simulation in a particle-based system, it is essential to construct a mesh from the particle attributes. Our system also contributes to the surface reconstruction aspect of the pipeline, in which we performed a set of experiments with the parallel marching cubes algorithm per frame for constructing the mesh from the fluid particles in a real-time compute and memory-intensive application, producing a wide range of triangle configurations. We also demonstrate that our system is versatile enough for reinforced robotic agents instead of boid agents to interact with the fluid environment for underwater navigation and remote control engineering purposes.},
  archive      = {J_PARCO},
  author       = {Xi Liu and Gizem Kayar and Ken Perlin},
  doi          = {10.1016/j.parco.2023.103062},
  journal      = {Parallel Computing},
  month        = {2},
  pages        = {103062},
  shortjournal = {Parallel Comput.},
  title        = {A GPU-based hydrodynamic simulator with boid interactions},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Program partitioning and deadlock analysis for MPI based on
logical clocks. <em>PARCO</em>, <em>119</em>, 103061. (<a
href="https://doi.org/10.1016/j.parco.2023.103061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The message passing interface (MPI) has become a standard for programming models in the field of high performance computing . It is of great importance to ensure the reliability of MPI programs by detecting whether there exist errors in them. However, as one of the most common errors in MPI programs, deadlock is difficult to detect due to the non-determinism and the asynchronous communication supported by MPI. Existing approaches mainly focus on detecting deadlocks by traversing all possible execution paths in an MPI program. But in this way the detection efficiency is always limited since the number of execution paths increases exponentially with the number of wildcard receives and processes in the program. In order to alleviate the path explosion problem for single-path MPI programs, we propose a program partitioning approach based on logical clocks to detecting deadlocks. In the approach, the program is first divided into several preliminary partitions based on the matching detection rule. Then to obtain the dependency relationships of partitions, the Binary Lazy Clocks algorithm is raised to mark clocks for communication operations. Based on the clocks, the completion orders of communication operations in each process of the program are tracked. Further, we get the dependency relationships of the preliminary partitions by analyzing these completion orders and merge the preliminary partitions with the dependency relationships for generating the final partitions . Finally, deadlocks are detected by traversing all possible execution paths of each final partition. We have implemented our method in a tool called PDMPI and performed experimental evaluation on 14 programs. The experimental results indicate that PDMPI is more effective for detecting deadlocks in MPI programs than two most related tools ISP and SAMPI, especially in programs with numerous interleavings.},
  archive      = {J_PARCO},
  author       = {Shushan Li and Meng Wang and Hong Zhang and Yao Liu},
  doi          = {10.1016/j.parco.2023.103061},
  journal      = {Parallel Computing},
  month        = {2},
  pages        = {103061},
  shortjournal = {Parallel Comput.},
  title        = {Program partitioning and deadlock analysis for MPI based on logical clocks},
  volume       = {119},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
