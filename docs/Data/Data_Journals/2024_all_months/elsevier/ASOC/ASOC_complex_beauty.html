<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ASOC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="asoc---1277">ASOC - 1277</h2>
<ul>
<li><details>
<summary>
(2024). Optimized hybrid XGBoost-CatBoost model for enhanced
prediction of concrete strength and reliability analysis using monte
carlo simulations. <em>ASOC</em>, <em>167</em>, 112490. (<a
href="https://doi.org/10.1016/j.asoc.2024.112490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on our previous work demonstrating the exceptional potential of the Extreme Gradient Boosting model (XGBoost) for predicting the uniaxial compressive strength of concrete, this study introduces several significant advancements. First, we develop a novel optimized hybrid model that synergistically combines XGBoost, CatBoost (one of the most advanced tree-boosting models), and the Optuna algorithm to achieve unprecedented prediction accuracy. Second, we apply this hybrid model in Monte Carlo simulations to conduct a pioneering reliability analysis of concrete strength, capturing the effects of input uncertainty. Third, we propose an innovative technique for estimating tree leaf values, which fundamentally improves prediction accuracy. Our optimized hybrid model delivers outstanding performance, as evidenced by a five-fold cross-validation showing a coefficient of determination (R²) of 0.953, a root mean squared error (RMSE) of 3.603 MPa, and a mean absolute error (MAE) of 2.261 MPa—metrics that surpass the best results reported in the existing literature. Additionally, our Monte Carlo simulations reveal a substantial error range of 10–20 MPa for a ±5 % variation in input features, underscoring the critical impact of input uncertainty on prediction reliability. Furthermore, our new leaf value estimation technique significantly outperforms traditional averaging methods, offering a transformative improvement in model accuracy. These findings are crucial for broadening the scope of machine learning applications in civil engineering and other engineering disciplines.},
  archive      = {J_ASOC},
  author       = {Tuan Nguyen-Sy},
  doi          = {10.1016/j.asoc.2024.112490},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112490},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized hybrid XGBoost-CatBoost model for enhanced prediction of concrete strength and reliability analysis using monte carlo simulations},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A z-number-based three-way decision method with
classification-based state determination for the evaluation of new
energy enterprises. <em>ASOC</em>, <em>167</em>, 112489. (<a
href="https://doi.org/10.1016/j.asoc.2024.112489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New energy enterprises are important promoting factors for sustainable development of modern society. Limited budgets of governments require that new energy enterprises should be efficiently evaluated before they are founded. Existing evaluation methods ignored the hesitation of experts to some alternatives. Although the three-way decision method has been applied widely as a method to evaluate alternatives, the determination of the state set has not been deeply discussed. To solve these challenges, this paper proposes a Z-number-based three-way decision method with classification-based state determination, which can assign alternatives with hesitation to a boundary region for further consideration and compute the conditional probability with a classification-based method. First, since traditional fuzzy sets cannot ensure the reliability of decision information, an evaluation matrix based on Z-numbers is constructed. Second, a fuzzy best-worst method is applied to determine the weights of criteria. Third, the conditional probability is computed based on classification-based state sets that are obtained by a sorting method. An example regarding the evaluation and selection of new energy enterprises demonstrates the validity and stability of the proposed method. The comparison analysis shows that our proposed method can divide alternatives into different regions efficiently and is less affected by the variation of parameters.},
  archive      = {J_ASOC},
  author       = {Xiaowan Jin and Huchang Liao and Zhiying Zhang},
  doi          = {10.1016/j.asoc.2024.112489},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112489},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A Z-number-based three-way decision method with classification-based state determination for the evaluation of new energy enterprises},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep supervision network with contrastive learning for
zero-shot sketch-based image retrieval. <em>ASOC</em>, <em>167</em>,
112474. (<a href="https://doi.org/10.1016/j.asoc.2024.112474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) is an extremely challenging cross-modal retrieval task. In ZS-SBIR, hand-drawn sketches are used as queries to retrieve corresponding natural images in zero-shot scenarios. Existing methods utilize diverse loss functions to guide deep neural networks (DNNs) to align feature representations of both sketches and images. In general, these methods supervise only the last layer of DNNs and then update each layer of DNNs using back-propagate technology. However, this strategy cannot effectively optimize the intermediate layers of DNNs, potentially hindering retrieval performance. To address this issue, we propose a deep supervision network with contrastive learning (DSNCL) approach for ZS-SBIR. Specifically, we employ a novel deep supervision network training method that attaches multiple projection heads to the intermediate layers of DNNs. These projection heads map multi-level features to a normalized embedding space and are trained by contrastive learning. The proposed method instructs the intermediate layers of DNNs to learn the invariance of various data augmentation, thereby aligning the feature representations of both sketches and images. This significantly narrows its domain gap and semantic gap. Besides, we use contrastive learning to directly optimize the intermediate layers of DNNs, which effectively reduces the optimization difficulty of their intermediate layers. Furthermore, we investigate the cross-batch metric (CBM) learning mechanism, which stores samples of different batches for metric learning by constructing a semantic queue, to further improve the performance in ZS-SBIR applications. Comprehensive experimental results on the Sketchy and TU-Berlin datasets validate the superiority of our DSNCL method over existing state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Zhenqiu Shu and Guangyao Zhuo and Jun Yu and Zhengtao Yu},
  doi          = {10.1016/j.asoc.2024.112474},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep supervision network with contrastive learning for zero-shot sketch-based image retrieval},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ANFIS and metaheuristics for double orbit retrial queue with
complete and working vacations. <em>ASOC</em>, <em>167</em>, 112473. (<a
href="https://doi.org/10.1016/j.asoc.2024.112473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance analysis of a double orbit retrial queue that incorporates impatience behaviour of the customers, feedback, complete vacation and working vacation has been investigated. The server opts for the complete vacation or working vacation after serving the customers of premium orbit or ordinary orbit, respectively. The probability generating approach has been implemented to obtain the performance metrices of the concerned queueing model. To obtain the steady state probabilities of individual system state, the maximum entropy principle has been employed. The joining strategy for the cooperative and non-cooperative cases have been discussed. The numerical experiment has been conducted to validate the analytical results. Furthermore, ANFIS approach is used so as to compare the numerical results obtained using analytical approach with ANFIS results. The optimal social net benefit and optimal joining probability have been computed using metaheuristics particle swarm optimization (PSO) and grey wolf optimizer (GWO).},
  archive      = {J_ASOC},
  author       = {Palak Mehta and Madhu Jain},
  doi          = {10.1016/j.asoc.2024.112473},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ANFIS and metaheuristics for double orbit retrial queue with complete and working vacations},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust rank aggregation method for malicious disturbance
based on objective credit. <em>ASOC</em>, <em>167</em>, 112471. (<a
href="https://doi.org/10.1016/j.asoc.2024.112471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rank aggregation is a task of combining individual rankings into a consensus, which has widespread applications in many areas, ranging from social choice to information retrieval. As some users may have incentives to disrupt the aggregated ranking for enormous benefits, making rank aggregation methods robust to malicious disturbance becomes a crucial challenge. In this study, we propose a robust rank aggregation method based on objective credit. The underlying idea is that a consensus ranking is obtained by combining multiple input rankings with users’ credit, while users’ credit is reflected by the differences between their input rankings and the consensus ranking. This idea motivates a novel iterative algorithm, which iteratively updates a consensus ranking weighted by users’ credit and modifies users’ credit by measuring the differences from a consensus ranking until all credit converges. In this way, the algorithm objectively assigns different credit to users, leading to a more reliable aggregated ranking. Extensive experiments on synthetic and real data demonstrate the superior performance of our method over state-of-the-art baselines.},
  archive      = {J_ASOC},
  author       = {Dongmei Chen and Yu Xiao and Huan Zhu and Ye Deng and Jun Wu},
  doi          = {10.1016/j.asoc.2024.112471},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112471},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust rank aggregation method for malicious disturbance based on objective credit},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broad distributed game learning for intelligent
classification in rolling bearing fault diagnosis. <em>ASOC</em>,
<em>167</em>, 112470. (<a
href="https://doi.org/10.1016/j.asoc.2024.112470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new Single Layer Feedforward Network (SLFN) architecture, Broad Learning System (BLS) has been widely used in the field of fault diagnosis because of its fast-training speed and high generalization capability. However, when features in different classes of signals are similar or weak, BLS generates a large number of redundant features that may be difficult to classify accurately. In view of this, a new Broad Distributed Game Learning (BDGL) method is proposed in this paper, which maps data into the game space by constructing two non-parallel game hyperplanes to achieve game and segmentation of different similar features, thereby making the data linearly differentiable in the game space. Meanwhile, a linear distribution constraint term is designed to reduce noise fitting and weak feature learning in training data learning by limiting the complexity of model parameters, thereby making the solution of the objective function simpler and faster. By comparing the Precision, Recall, F-score, Kappa and Accuracy of BDGL and the comparison methods on the two types of rolling bearing experimental data, the results show that BDGL has a high classification accuracy. In addition, the experimental results on small and noisy samples once again demonstrate the effectiveness of BDGL, which provides an efficient solution for rolling bearing fault diagnosis.},
  archive      = {J_ASOC},
  author       = {Haoran Liu and Haiyang Pan and Jinde Zheng and Jinyu Tong and Mengling Zhu},
  doi          = {10.1016/j.asoc.2024.112470},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad distributed game learning for intelligent classification in rolling bearing fault diagnosis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Human-UAV interactive perception: Skeleton-based iterative
perspective optimization algorithm for UAV patrol tracking of
large-scale pedestrian abnormal behavior. <em>ASOC</em>, <em>167</em>,
112467. (<a href="https://doi.org/10.1016/j.asoc.2024.112467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a system framework for UAV patrolling pedestrian abnormal behavior in public places based on human key point features and a skeleton based UAV large-scale pedestrian patrol viewpoint optimal with PID-iterative learning control algorithm is designed. The system framework provides early warning of abnormal pedestrian behavior through images obtained by UAV and human key point recognition by OpenPose. Aiming at the problem of recognizability of images acquired during UAV patrol, an algorithm inspired by iterative learning control for tracking large crowds with human torso and shoulder skeleton information is proposed and a PID-iterative learning control algorithm is designed to improve the control effect under the situation that pedestrian motion has an approximate repeatability. The algorithm is able to enhance the number of photographed pedestrians and increase the sustained observation time of their behaviors. Finally, the usability and effectiveness of the algorithm are verified by experiment results. The proposed method will contribute to the research of interactive perception technology in embodied intelligence.},
  archive      = {J_ASOC},
  author       = {Ziao Wang and Tao Chen and Jian Chen},
  doi          = {10.1016/j.asoc.2024.112467},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human-UAV interactive perception: Skeleton-based iterative perspective optimization algorithm for UAV patrol tracking of large-scale pedestrian abnormal behavior},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential evolution with multi-strategies for UAV
trajectory planning and point cloud registration. <em>ASOC</em>,
<em>167</em>, 112466. (<a
href="https://doi.org/10.1016/j.asoc.2024.112466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present study introduces a novel adaptive algorithm, MELSHADE-cnEpSin, which aims to enhance the performance of LSHADE-cnEpSin, which is not only stands out as one of the most competitive versions of differential evolution but also holds the distinction of being one of the CEC winner algorithms. Compared to the original methodology, three main distinctions are presented. To begin with, we adopt an adaptive selection mechanism (ASM) of crossover rate Cr value based on the external archive to rechoose a suitable value. In the next place, a nonlinear population reduction strategy using Sigmoid function is employed to improve population distribution. Additionally, a restart strategy is implemented to mitigate the risk of algorithmic convergence towards suboptimal solutions. Furthermore, the performance of MELSHADE-cnEpSin was evaluated using standard CEC2017 and CEC2022 test suites in conjunction with nine CEC-winning algorithms (L-SHADE, EBOwithCMAR, AGSK, LSHADE-SPACMA, LSHADE-cnEpSin, ELSHADE-SPACMA, EA4eig, MadDE and APGSK-IMODE) as well as two novel algorithms (ACD-DE and MIDE). Furthermore, MELSHADE-cnEpSin was effectively employed to address the challenge of UAV trajectory planning in intricate mountainous terrain and underwent simulation with point cloud registration cases utilizing a rapid global registration dataset, thereby showcasing the potential of MELSHADE-cnEpSin in tackling real-world optimization problems.},
  archive      = {J_ASOC},
  author       = {Guozhang Zhang and Shengwei Fu and Ke Li and Haisong Huang},
  doi          = {10.1016/j.asoc.2024.112466},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution with multi-strategies for UAV trajectory planning and point cloud registration},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating ergonomic requirements of graphical user
interface: A DEMATEL method integrating linguistic pythagorean fuzzy
rough numbers. <em>ASOC</em>, <em>167</em>, 112465. (<a
href="https://doi.org/10.1016/j.asoc.2024.112465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As intelligent interaction systems evolve, ergonomic requirements (ERs) for graphical user interfaces (GUIs) have become increasingly complex and subjective. Inadequate evaluation during the GUI prototype design phase can lead to significant challenges in later development and testing. To address this issue, we propose a novel Decision-making Trial and Evaluation Laboratory (DEMATEL) approach that integrates rough set (RS) theory, linguistic Pythagorean fuzzy set (LPFS), and models relationships among decision makers (DMs). Specifically, we introduce linguistic Pythagorean fuzzy rough numbers (LPFRNs) to capture DM groups&#39; uncertain and interrelated opinions on ERs, accounting for linguistic uncertainty and inherent roughness. We detail the construction of LPFRNs, including operation rules and aggregation operators based on the Dombi t-norm (DTN) and t-conorm(DTCN), and establish ranking rules and distance measures. By incorporating a DM objective weight determination method based on similarity measures, we enhance the precision of the DEMATEL model. Applying this method, we construct a hierarchical system of 31 ERs based on six GUI design elements (DEs) and evaluate their importance. Our findings reveal that layout-related ERs are critically significant, providing valuable insights for GUI designers to prioritize ERs effectively. The novelty of this work lies in integrating LPFRN and its aggregation operator into the DEMATEL method, which can better meet the complex and subjective correlation evaluation. Potential applications include automated evaluation of GUI design quality and optimization of the design process. We validated the robustness and feasibility of our method through sensitivity and comparative analysis.},
  archive      = {J_ASOC},
  author       = {Qinghua Liu and Xiaojiao Chen},
  doi          = {10.1016/j.asoc.2024.112465},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating ergonomic requirements of graphical user interface: A DEMATEL method integrating linguistic pythagorean fuzzy rough numbers},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surrogate-assisted fully-informed particle swarm
optimization for high-dimensional expensive optimization. <em>ASOC</em>,
<em>167</em>, 112464. (<a
href="https://doi.org/10.1016/j.asoc.2024.112464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-Assisted Evolutionary Algorithms (SAEAs) have been proven to be powerful optimization tools for tackling Expensive Optimization Problems (EOPs) where a limited number of function evaluations are available. However, many SAEAs are only designed for low- or medium-dimensional EOPs. Existing SAEAs are challenging to address High-dimensional EOPs (HEOPs) owing to the curse of dimensionality and lack of powerful exploitation capacity. To tackle HEOPs efficiently, a Surrogate-Assisted Fully-informed Particle Swarm Optimization (SA-FPSO) algorithm is proposed in this paper. Firstly, a generation-based Social Learning-based PSO (SLPSO) is adopted to explore the whole decision space with the help of the global surrogate model. Secondly, the fully-informed search scheme is incorporated into the framework of SLPSO to improve its exploitation capacity in the surrogate-assisted search environment. Thirdly, a local space identification strategy is proposed to determine the search range for the local surrogate-assisted search. Seven commonly used expensive benchmark functions with dimensions ranging from 30D to 300D are used to verify the performance of SA-FPSO for HEOPs. Experiment results indicate that SA-FPSO obtains superior performance over several state-of-the-art SAEAs both in terms of convergence speed and solution accuracy.},
  archive      = {J_ASOC},
  author       = {Chongle Ren and Qiutong Xu and Zhenyu Meng and Jeng-Shyang Pan},
  doi          = {10.1016/j.asoc.2024.112464},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Surrogate-assisted fully-informed particle swarm optimization for high-dimensional expensive optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-functional group decision making with heterogeneous
cooperation for digital transformation in supply chain resilience.
<em>ASOC</em>, <em>167</em>, 112463. (<a
href="https://doi.org/10.1016/j.asoc.2024.112463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply chain resilience plays a critical role in gaining competitive advantages for companies. The resilience of supply chains can be achieved by leveraging emerging digital technologies to realize digital transformation. It is necessary to select an appropriate digitalization technology under such background. The wide-spanning of digital transformation and technology selection needs cross-functional integration of various expertise. However, in the process of making decisions by leveraging expert wisdom, differences in experts’ willingness to cooperate lead to difficulties in reaching a consensus. The existing literature fails to incorporate both non-cooperation and proactive-cooperation into the consensus reaching process. Thus, in this study, we introduce a cross-functional multi-attribute group decision making model for digitalization technology selection. To manage potential non-cooperative behaviors in the group consensus reaching process, the proposed model allows experts to have proactive cooperation, i.e., making more contributions than recommended feedback suggestions provided by the moderator. Proactive cooperation can make up for the loss caused by the non-cooperative behaviors of experts. A knowledge mining method is proposed to mine academic and practical preferences for attributes. Two consensus mechanisms are put forward for the meso decision-making process in functional teams and the macro decision-making process in the whole group, respectively. An illustrative example regarding the technology selection in shipbuilding industry is provided to verify the applicability of our model. Numerical experiments suggest that our model will improve the efficiency of consensus reaching process.},
  archive      = {J_ASOC},
  author       = {Ming Tang and Huchang Liao},
  doi          = {10.1016/j.asoc.2024.112463},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-functional group decision making with heterogeneous cooperation for digital transformation in supply chain resilience},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Address wind farm layout problems by an adaptive moth-flame
optimization algorithm. <em>ASOC</em>, <em>167</em>, 112462. (<a
href="https://doi.org/10.1016/j.asoc.2024.112462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a clean renewable energy source, wind energy has been widely used to generate electricity in wind farms. Plenty of turbines work together to form a wind farm to increase electricity output. However, this produces an inevitable wake effect, which affects the efficiency of turbines in capturing wind energy, further leading to a decrease in the power generated by wind farms. To minimize the wake effect, optimizing the turbines layout in wind farms is necessary. Therefore, an adaptive Moth-Flame Optimization algorithm with enhanced Exploration and Exploitation capabilities (MFOEE) is proposed. The refinements of the algorithm include: i) defining a selection probability to utilize diverse information of moths; ii) an enhanced exploitation strategy by pushing moths towards the best flame; iii) developing an enhanced exploration strategy, in which three moths exchange information and two inferior moths fly to the superior moth. To validate the performance of MFOEE, four scenarios are set up using grid-based simulations of actual wind farm conditions. The experimental findings demonstrate that MFOEE can offer the most optimal scheme among the four scenarios, which indicates that MFOEE proves highly effective in addressing wind farm layout optimization challenges.},
  archive      = {J_ASOC},
  author       = {Xiaobing Yu and Wen Zhang},
  doi          = {10.1016/j.asoc.2024.112462},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Address wind farm layout problems by an adaptive moth-flame optimization algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning hyper-heuristic algorithm for the
distributed flowshops scheduling problem under consideration of
emergency order insertion. <em>ASOC</em>, <em>167</em>, 112461. (<a
href="https://doi.org/10.1016/j.asoc.2024.112461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large enterprises are composed of several subproduction centers. The production plan is changed based on the procedure of the manufacturing system. The distributed flowshop scheduling problem under consideration of emergence order insertion is challenging as the assignment of the product, and the scheduling of the process are coupled with each other. A general distributed flow shop scheduling problem regarding emergency order insertion (DFSP_EOI) is addressed under rescheduling circumstances in this paper. The Q-learning hyper-heuristic algorithm with dynamic insertion rule (QLHH_DIR) is proposed to solve the DFSP_EOI. Eight low-level heuristics (LLHs) for static job assignment. A dynamic insertion rule based on the state of each production center is designed for emergency order insertion. The Q-learning mechanism at high-level space selects appropriate LLH through learning the experience from the optimization process. The computational simulation is carried out, and the results confirm that the proposed algorithm is superior to the competitors in solving the distributed flow shop rescheduling problem. The results of the 720 problem instances show that the proposed algorithm is highly efficient in rescheduling problems.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Yuebao Liu and Tianpeng Xu and Jonrinaldi},
  doi          = {10.1016/j.asoc.2024.112461},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reinforcement learning hyper-heuristic algorithm for the distributed flowshops scheduling problem under consideration of emergency order insertion},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M-net based stacked autoencoder for ransomware detection
using blockchain data. <em>ASOC</em>, <em>167</em>, 112460. (<a
href="https://doi.org/10.1016/j.asoc.2024.112460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ransomware is a kind of malevolent program software that encrypts the items on the hard disc and prevents the clients from accessing them until they are paid a ransom. Associations like monetary establishments and medical care areas (i.e., smart medical care) are mostly targeted by ransomware attacks. Ransomware assaults are crucial holes still in blockchain technology and prevent effective data communication in networks. This study aims to introduce an efficient system, named M-Net-based Stacked Autoencoder (M-Net_SA) for ransomware detection using blockchain data. Initially, the input data is taken from a dataset and then sent to the feature extraction process, which utilizes sequence-based statistical features. After that, data transformation is completed using the Yeo-Johnson transformation to transform the data into a usable format. After that, feature fusion is executed using a Deep Q-network (DQN) with Lorentzian similarity to enhance the representativeness of the target features. Finally, ransomware detection is accomplished by the proposed M-Net_SA, which is the integration of MobileNet and Deep Stacked Autoencoder (DSAE). The experimental validation of the proposed M-Net_SA is compared with other conventional techniques and the proposed model attained maximum accuracy, sensitivity, and specificity of 0.959, 0.967, and 0.95 7 respectively.},
  archive      = {J_ASOC},
  author       = {Uma Devi Gurumuni Nathan and P. Balashanmuga Vadivu and Balajee Maram and Guru Kesava Dasu Gopisetty and Smritilekha Das and Daniya T},
  doi          = {10.1016/j.asoc.2024.112460},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {M-net based stacked autoencoder for ransomware detection using blockchain data},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A p-ary choquet-based multicriteria decision-making model
for customer-oriented product design scheme selection. <em>ASOC</em>,
<em>167</em>, 112459. (<a
href="https://doi.org/10.1016/j.asoc.2024.112459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an efficient customer-oriented product design tool for converting customer requirements (CRs) into quality characteristics (QCs) of a product, quality function deployment (QFD) is applied to the selection of new product design schemes. However, the effective implementation of QFD is hampered by two fundamental challenges: (a) biases in people’s understanding of customer satisfaction and (b) the ambiguously assessed relationship between CRs and QCs because of the inherent uncertainty in human judgment. Therefore, this paper provides a p -ary Choquet-based multicriteria decision-making model to rank new product design schemes considering customer psychological and risk attitudes. The selection of a new product design scheme is divided into two stages. In the first stage, the p -ary Choquet integral converts the objective technical parameters of a new product design scheme into utility, which is the subjective feeling from the perspective of two-factor theory. Then, the integration of 2-additive measures and the Choquet integral is used to account for the redundancy and complementary effects among customer requirements. Finally, a case study of an electric vehicle manufacturing company and a comparison analysis are presented to illustrate the validity of our proposal. The p -ary Choquet-based multicriteria decision-making model realizes customer-oriented product design scheme selection by systematically analyzing customer preferences and bridging the gap between products that are emotionally recognized by customers and those that exhibit excellent performance in terms of functionality and cost.},
  archive      = {J_ASOC},
  author       = {Yu Gao and Mei Cai and Jingmei Xiao and Guang Yang},
  doi          = {10.1016/j.asoc.2024.112459},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A p-ary choquet-based multicriteria decision-making model for customer-oriented product design scheme selection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep-coupling neural network and genetic algorithm based on
sobol-PR for reactor lightweight optimization. <em>ASOC</em>,
<em>167</em>, 112458. (<a
href="https://doi.org/10.1016/j.asoc.2024.112458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep-coupling neural network and genetic algorithm method based on Sobol-PR method for reactor lightweight shielding optimization. The Sobol method is first used to analyze the sensitivities between inputs and outputs of the neural network, and then these sensitivities are used to adjust the fitness function of the genetic algorithm dynamically. Meanwhile, two indicators (precision and recall rate) are introduced to facilitate the sample evaluation and selection, where the precision quantifies the prediction ability of the neural network, and the recall rate quantifies the optimization efficiency of the genetic algorithm. The deep coupling between the neural network and the genetic algorithm based on Sobol-PR method contributes to an integrated framework of “calculation-optimization-reconstruction-evaluation,” which is applied to the lightweight shielding design of a small helium-xenon-cooled reactor. It is found that the performance of the neural network and the genetic algorithm is improved, with the precision of the neural network reaches up to 99 % and the recall rate of the genetic algorithm reaches up to 84 %. Compared with the traditional method, the new method improves the ratio of ideal solutions by up to 3.8 times and the optimization depth by up to 3.2 times.},
  archive      = {J_ASOC},
  author       = {Qingquan Pan and Songchuan Zheng and Xiaojing Liu},
  doi          = {10.1016/j.asoc.2024.112458},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep-coupling neural network and genetic algorithm based on sobol-PR for reactor lightweight optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From one-dimensional to multidimensional map neural
networks. <em>ASOC</em>, <em>167</em>, 112457. (<a
href="https://doi.org/10.1016/j.asoc.2024.112457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape, artificial neural networks transcend their traditional role of function approximation and have found diverse applications in fields such as image classification, machine translation, speech recognition, and natural language processing. In some datasets, traditional architectures exhibit low test and training accuracy, coupled with high loss and prolonged training times. This study aims to introduce innovative neural network architectures that outperform conventional models. The research presents a novel framework integrating one-dimensional and multidimensional map neural networks, consisting of three distinct architectures: 1D-Map, 2D-Map, and 3D-Map. A systematic performance comparison with traditional models is conducted following the implementation of these architectures. The evaluation spans four datasets, encompassing the domains of heat treatment of electroless Ni-P nano coatings, letter recognition, combined cycle power plants, and Seoul bike sharing demand. In the heat treatment dataset, the proposed 3D-Map architecture reached 0.055 higher average test accuracy than traditional MLP architecture. In the letter recognition dataset, the test accuracy of 3D-Map architecture was 0.0523 higher than the test accuracy of the LSTM architecture. In the combined cycle power plant dataset, the test accuracy of the 3D-Map architecture was 0.0612 more than the test accuracy of the MLP architecture. In the Seoul bike-sharing demand dataset, the test accuracy of the 2D-Map architecture was 0.0696 higher than the test accuracy of the LSTM architecture. The study&#39;s findings underscore the consistently superior performance of the proposed architectures compared to their traditional counterparts.},
  archive      = {J_ASOC},
  author       = {Sayed Yousef Monir Vaghefi and Faramarz Safi-Esfahani and Shervan Fekri-Ershad and Sayed Mahmoud Monir Vaghefi},
  doi          = {10.1016/j.asoc.2024.112457},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From one-dimensional to multidimensional map neural networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy logic constrained particle swarm optimization
algorithm for industrial design problems. <em>ASOC</em>, <em>167</em>,
112456. (<a href="https://doi.org/10.1016/j.asoc.2024.112456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the industrial design problems have non-linear constraints, high computational cost, non-convex, complicated, and large number of solution spaces. This poses a challenge for algorithms to effectively handle constraints and improve solution accuracy. To address these challenges, a fuzzy logic particle swarm optimization algorithm incorporating a correlation-based constraint handling method (FILPSO-SCA ɛ ) is proposed. In FILPSO-SCA ɛ , an adaptive ɛ constraint handling method with correlation analysis is introduced to dynamically adjust the utilization of constraints and the objective function information. The particle swarm optimization algorithm is employed as the searcher, and to augment its search capability, a set of fuzzy logic rules integrating individual feasibility is designed. These rules dynamically generate parameters in learning strategies by considering fitness and the distance between individuals. To mitigate premature convergence problems, we introduce an individual learning mechanism utilizing stagnation detection. 28 constrained optimization problems and 2 industrial design problems are utilized for comparison with 16 well-known constrained evolutionary algorithms. The proposed algorithm ranks first among the 16 comparative algorithms, with a success rate of 100% in solving industrial design problems.},
  archive      = {J_ASOC},
  author       = {Bo Sun and Peixi Peng and Guang Tan and Mingjun Pan and Luntong Li and Yonghong Tian},
  doi          = {10.1016/j.asoc.2024.112456},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy logic constrained particle swarm optimization algorithm for industrial design problems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). GA based construction of maximin latin hypercube designs
for uncertainty design of experiment with dynamic strategy management.
<em>ASOC</em>, <em>167</em>, 112454. (<a
href="https://doi.org/10.1016/j.asoc.2024.112454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flexible construction of maximin Latin Hypercube Designs (LHDs) meets the NP-hard problem known as the Maximum Diversity Problem (MDP). Traditional algorithms, such as Genetic Algorithms (GAs), face challenges like premature convergence and limited optimization performance, particularly due to the number of hyperparameters that require to be tuned and their limited ability to generalize across diverse problem domains. Thus, this paper proposed a self-adaptive method called GA with Dynamic Strategy Management for the flexibly and efficient construction of maximum LHDs. This method is based on premature convergence prediction, dynamic triggered optimization strategies, and performance control. Furthermore, nearly all critical factor, such as population initialization and selection, crossover, mutation, and local search, are involved in this framework. By comparing this method to LHD construction techniques (Simulated Annealing, Enhanced Stochastic Evolution, and Latin Hypercube Particle Swarm Optimization), as well as the adaptive GAs and state-of-the-art metaheuristics, the algorithm demonstrates superior performance due to its optimized structural self-organization.},
  archive      = {J_ASOC},
  author       = {Dong Liu and Shaoping Wang and Jian Shi and Di Liu},
  doi          = {10.1016/j.asoc.2024.112454},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GA based construction of maximin latin hypercube designs for uncertainty design of experiment with dynamic strategy management},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hesitant fuzzy linguistic preference consistency-driven
consensus model with large-scale group interaction measure for venture
capital investment selection. <em>ASOC</em>, <em>167</em>, 112453. (<a
href="https://doi.org/10.1016/j.asoc.2024.112453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, consensus-based large-scale group decision making (LSGDM) has been widely interactive with the study of social network, clustering and trust-based concepts. This study develops a novel hesitant fuzzy linguistic preference consistency-driven consensus model with interaction measure for large-scale group decision makers (DMs) in social networks. Firstly, directed social network is constructed by measuring the similarity between incomplete hesitant fuzzy linguistic preference relation (HFLPR) matrices. Community detection method is further conducted to categorize DMs into several communities. Secondly, driven by exploring the consistency of HFLPR matrices and interactive trusts between DMs, a novel optimization model is established to estimate the missing elements. Thirdly, the 2-order additive fuzzy measures of different coalitions between divided communities for capturing their fully or partially interactions are derived by a consistency-based optimization model. Accordingly, the attitudinal Choquet integral operator is employed to aggregate preferences into the collective one. Fourthly, a consensus improving mechanism is devised to achieve the unanimous agreement of DMs characterized by the bounded confidence. Personalized and specific adjustment scales obtained by investigating interval consistency of HFLPRs are provided in support of DMs’ modifications. Finally, an illustrative case on syndicated venture capital investment selection is conducted and related simulation analyses are performed to elucidate the feasibility and validity of the proposed methods. The comparisons with other approaches reveal the superiority and improvement of our proposal.},
  archive      = {J_ASOC},
  author       = {Yuanyuan Liang and Yanbing Ju and Xiao-Jun Zeng and Peiwu Dong and Mihalis Giannakis and Hengxia Gao and Tianyu Zhang},
  doi          = {10.1016/j.asoc.2024.112453},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hesitant fuzzy linguistic preference consistency-driven consensus model with large-scale group interaction measure for venture capital investment selection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint optimization of empty container repositioning and
inventory control applying dynamic programming and simulated annealing.
<em>ASOC</em>, <em>167</em>, 112452. (<a
href="https://doi.org/10.1016/j.asoc.2024.112452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the division of public and exclusive hinterlands, this paper studies the joint optimization problem of empty container repositioning and inventory control under non-stationary demand within the port cluster. This paper established a stochastic mixed integer programming model using distributed robust optimization methods, combined with quantitative and periodic inventory control strategies. After conducting a deterministic transformation, this paper designed a hybrid algorithm of dynamic programming and simulated annealing to solve the model, and compared the various costs under stationary and non-stationary scenarios. The results show that the joint optimization of empty container inventory control and repositioning can always reduce the total cost of empty container management for shipping companies. Periodic inventory control strategies are more suitable for shipping companies to apply to empty container management in non-stationary situations. Sensitivity analysis shows that there is a positive correlation between uncertain demand parameters and the total cost of shipping companies. The superiority of the empty container repositioning mode proposed in this paper under sea-land coordination has also been demonstrated by changing the accessibility parameters between terminals.},
  archive      = {J_ASOC},
  author       = {Jiaxin Cai and Ying Huang and Cuijie Diao and Zhihong Jin},
  doi          = {10.1016/j.asoc.2024.112452},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint optimization of empty container repositioning and inventory control applying dynamic programming and simulated annealing},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross-shard transaction optimization based on community
detection in sharding blockchain systems. <em>ASOC</em>, <em>167</em>,
112451. (<a href="https://doi.org/10.1016/j.asoc.2024.112451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain systems have always faced the challenge of performance bottlenecks, and sharding technology is considered a promising mainstream on-chain scalability solution to solve this problem. Due to the complexity and high cost of the cross-shard transaction processing mechanism in the sharding blockchain system, as well as the high proportion of cross-shard transactions, it becomes challenging for the sharding blockchain system to reach the ideal theoretical performance upper limit. Therefore, this paper aims to reduce the proportion of cross-shard transactions by dividing accounts with frequent transactions into the same shard, thereby improving system throughput. This paper builds a hypergraph based on historical transaction data to represent the diverse transaction relationships between accounts, and formulates the account division problem in the blockchain as a community discovery problem on the hypergraph structure. A time-aware community detection algorithm is proposed to partition accounts by considering the sustainability of transaction relationships between accounts. This also solves the problem of community detection algorithms tending to partition into larger shards. In addition, this paper builds a local Ethereum test network and implements the proposed algorithm on a real transaction dataset. Experimental results show that this algorithm can reduce the proportion of cross-shard transactions from about 95% to about 10%. Furthermore, it shows superior performance in terms of transaction throughput and latency compared with other community detection-based account partitioning algorithms.},
  archive      = {J_ASOC},
  author       = {Peng Han and Linzhao Sun and Quang-Vi Ngo and Yuanyuan Li and Guanqiu Qi and Yiyao An and Zhiqin Zhu},
  doi          = {10.1016/j.asoc.2024.112451},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-shard transaction optimization based on community detection in sharding blockchain systems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable rough neural network for lung nodule
diagnosis. <em>ASOC</em>, <em>167</em>, 112450. (<a
href="https://doi.org/10.1016/j.asoc.2024.112450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided diagnosis (CAD) systems based on deep learning have shown significant potential in lung nodule diagnosis, providing substantial assistance to medical professionals. However, the inherent lack of interpretability in deep learning models and the uncertainty of annotations limit their widespread application. We propose that uncertain annotations actually imply additional valuable information that can enhance both model performance and interpretability. To address these challenges, we have developed a novel soft computing methodology integrating rough sets with deep neural networks. Firstly, this methodology employs rough sets to process uncertain region of interest (ROI) annotations into upper and lower approximations. Secondly, a novel rough neuron is designed to predict these approximations. Thirdly, the newly proposed region-constraint strategy embeds interpretable radiological domain knowledge into the neural network. Finally, this methodology proposes interpretation curves and regional consistency metrics to quantitatively evaluate the model’s interpretability. We conducted extensive comparison experiments on LIDC-IDRI and LNDb public benchmarks. Detailed experimental results demonstrate that by maximally retaining uncertain samples, the proposed method achieves classification accuracies of 84.6% and 89.74%, and mean absolute errors of 0.4988 and 0.5208 in attribute prediction, representing improvements of 3.4% and 2.5%, respectively, over the backbone networks.},
  archive      = {J_ASOC},
  author       = {Huanan Bao and Guoyin Wang and Chen Liu and Qun Liu and Qiuyu Mei and Changhua Xu and Xin Wang},
  doi          = {10.1016/j.asoc.2024.112450},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112450},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretable rough neural network for lung nodule diagnosis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of theft in a power distribution network: A
tree-based approach. <em>ASOC</em>, <em>167</em>, 112449. (<a
href="https://doi.org/10.1016/j.asoc.2024.112449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity is an essential commodity, yet it is not freely available to everyone in the country due to widespread electricity theft. This theft results in significant financial losses for Distribution Companies (DISCOMS), hindering their ability to provide reliable services to consumers. Although there are methods to curb electricity theft, DISCOMS have struggled to control it effectively. The most successful current method is the manual investigation of nodes; however, it has its drawbacks, such as a low probability of theft detection when nodes are randomly sampled and excessive time consumption when all nodes are investigated. To address these issues, a systematic approach utilizing the capture, storage, and analysis of data is essential. This paper presents a coloring-based tree approach to identify power theft without altering the existing network infrastructure. The proposed approach aims to reduce the dataset for manual inspection, thereby increasing the probability of theft detection while substantially reducing investigation time. By discouraging blind checking of nodes and providing a basis for targeted node inspections, this method benefits both DISCOMS and general consumers. In this paper, the existing methods of theft detection have been compared with the proposed approach of tree-based theft detection. The application of the proposed approach removed certain nodes from the set of nodes to be investigated for theft detection.},
  archive      = {J_ASOC},
  author       = {Navdeep Bhatnagar and Suchi Johari and Vivek Kumar Sehgal and Amita Nandal and Adi Alhudhaif and Kemal Polat and Surjeet Singh},
  doi          = {10.1016/j.asoc.2024.112449},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112449},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identification of theft in a power distribution network: A tree-based approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Heterogeneous graph neural network with hierarchical
attention for group-aware paper recommendation in scientific social
networks. <em>ASOC</em>, <em>167</em>, 112448. (<a
href="https://doi.org/10.1016/j.asoc.2024.112448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the academic groups established in Scientific Social Networks (SSNs) have not only facilitated collaboration among researchers but also enriched the relations in SSNs, providing valuable information for paper recommendation tasks. However, existing paper recommendation methods rarely consider group information and they fail to fully leverage the group information due to the heterogeneous and complex relations between researchers, papers, and groups. In this paper, a heterogeneous graph neural network with hierarchical attention, named HHA-GPR, is proposed for group-aware paper recommendation. Firstly, a heterogeneous graph is constructed based on the interactions of researchers, papers, and groups in SSNs. Secondly, a random walk-based sampling strategy is utilized to sample highly correlated heterogeneous neighbors for researchers and papers. Thirdly, a hierarchical attention network with intra-type and inter-type attention mechanisms is designed to aggregate the sampled neighbors and comprehensively model the complex relations among the heterogeneous neighbors. More specifically, an intra-type attention mechanism is introduced to aggregate the neighbors of the same type, and an inter-type attention mechanism is employed to combine the embeddings of different types to form the ultimate node embedding. Extensive experiments are conducted on the real-world CiteULike and AMiner datasets, and the experimental results demonstrate that our proposed method outperforms other benchmark methods with an average improvement of 5.3 % in Precision, 5.6 % in Recall, and 5.1 % in Normalized Discounted Cumulative Gain (NDCG) across both datasets.},
  archive      = {J_ASOC},
  author       = {Gang Wang and Li Zhou and Junqiao Gong and Xuan Zhang},
  doi          = {10.1016/j.asoc.2024.112448},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous graph neural network with hierarchical attention for group-aware paper recommendation in scientific social networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection of advanced persistent threat: A genetic
programming approach. <em>ASOC</em>, <em>167</em>, 112447. (<a
href="https://doi.org/10.1016/j.asoc.2024.112447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced Persistent Threats (APTs) are an intimidating class of cyberattacks known for their persistence, sophistication, and targeted nature. These attacks, coordinated by highly motivated adversaries, pose a grave risk to organizations and individuals, often operating stealthily and evading detection. While existing research primarily focuses on applying Machine Learning (ML) methods to analyze network traffic data for APT detection, this article introduces a novel approach that utilizes Genetic Programming (GP). The proposed method not only detects APT attacks but also identifies their specific life cycle stages through the evolutionary capabilities of GP. Its effectiveness lies in its ability to excel in detecting intricate patterns, even within classes with a limited number of instances, a feat that is often challenging for traditional ML techniques. The method involves evolving and optimizing its models to effectively learn and adapt to complex APT behaviors. Experimentation with a publicly available dataset showcases the efficacy of the proposed method across diverse APT stages. The results demonstrate that the proposed method, GPC, achieves a 3.71% improvement in balanced accuracy compared to the best-performing model from related works. Moreover, a thorough analysis of the best-evolved GP model uncovers valuable insights about identified features and significant patterns. This research advances the APT detection paradigm by leveraging GP’s capabilities, providing a fresh and effective perspective on countering these persistent threats.},
  archive      = {J_ASOC},
  author       = {Abdullah Al Mamun and Harith Al-Sahaf and Ian Welch and Masood Mansoori and Seyit Camtepe},
  doi          = {10.1016/j.asoc.2024.112447},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Detection of advanced persistent threat: A genetic programming approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale spatio-temporal feature fusion based
non-intrusive appliance load monitoring for multiple industrial
industries. <em>ASOC</em>, <em>167</em>, 112445. (<a
href="https://doi.org/10.1016/j.asoc.2024.112445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The appliance types and power consumption patterns vary greatly across different industries. This can lead to unstable identification results of traditional appliance load monitoring methods in different industries. A non-intrusive appliance load monitoring (NIALM) method for multiple industries based on multiscale spatio-temporal feature fusion has been proposed. Firstly, the ConvNeXt Block with efficient channel attention has strong feature extraction capability. Spatial features of appliance state changes and micro-variations generated during operation can be extracted from mixed industrial load information by it. Meanwhile, the bidirectional gated recurrent neural network is used to learn the bidirectional dependencies of the load data, obtaining temporal features. Then, the multi-scale feature extraction module is used to extract temporal and spatial features from different depths of the network layers. And the extracted multi-scale temporal and spatial features are fully integrated. Finally, the proposed model is optimized using the Stochastic Weight Averaging method. During the training process, a certain number of model weights are randomly averaged, which can improve the model&#39;s generalization ability and identification accuracy. The experiment was conducted on six different industries. The evaluation indexes such as accuracy, F1 score, and Wasserstein distance are also used to verify the effectiveness and superiority of the method.},
  archive      = {J_ASOC},
  author       = {Lin Lin and Jiang Liu and Nantian Huang and Shilin Li and Yunshan Zhang},
  doi          = {10.1016/j.asoc.2024.112445},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112445},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiscale spatio-temporal feature fusion based non-intrusive appliance load monitoring for multiple industrial industries},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sparse diverse-branch large kernel convolutional neural
network for human activity recognition using wearables. <em>ASOC</em>,
<em>167</em>, 112444. (<a
href="https://doi.org/10.1016/j.asoc.2024.112444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the past decade, large convolutional kernels have long been under the shadow of small convolutional kernels since the introduction of VGG backbone network. It always remains mysterious whether one can design pure convolutional neural network (CNN) while plugging larger kernels to model long-range dependency for human activity recognition (HAR), which has been rarely explored in previous literatures. In this paper, we revive the usage of larger kernels in the context of HAR and attempt to eliminate the performance gap between large kernels and small kernels by strategically applying a large receptive field, without incurring high memory and computational footprints. Built on two recipes, i.e., Diverse-Branch and Dynamic Sparsity, we design a pure CNN architecture named SLK-Net for activity recognition, which is equipped with sparse diverse-branch larger kernels. To validate the effectiveness of our approach, we perform a series of extensive experiments on four public benchmarks including UCI-HAR, WISDM, UniMiB-SHAR and USC-HAD, which show that large kernels can benefit its ability to capture long-range dependency and consistently beat state-of-the-art small-kernel counterparts across a wide range of activity classification tasks. Real activity inference latency is measured on a mobile device, which reveals that such sparse diverse-branch kernels can lead to inference speedup than vanilla large kernels. We hope this work may further inspire relevant CNN-based studies in the HAR community.},
  archive      = {J_ASOC},
  author       = {Minghui Yao and Dongzhou Cheng and Lei Zhang and LiangDong Liu and Shuangteng Song and Hao Wu and Aiguo Song},
  doi          = {10.1016/j.asoc.2024.112444},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112444},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sparse diverse-branch large kernel convolutional neural network for human activity recognition using wearables},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network embedding on metric of relation. <em>ASOC</em>,
<em>167</em>, 112443. (<a
href="https://doi.org/10.1016/j.asoc.2024.112443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding maps the nodes of a given network into a low-dimensional space such that the semantic similarities among the nodes can be effectively inferred. Most existing approaches use inner-product of node embeddings to measure the similarity between nodes leading to the fact that they lack the capacity to capture complex relationships among nodes. Moreover, they take only structural information but not semantical information when deciding paths in the embedding. In this paper, We propose a novel method called Network Embedding on the Metric of Relation, abbreviated as NEMR, which can learn the embeddings of nodes in a relational metric space efficiently. It first models the relationships among nodes in a metric space with deep learning methods including variational inference that maps the relationship of nodes to a gaussian distribution so as to capture the uncertainties, then infers the embeddings of the nodes by considering not only the equivalence of multiple-paths in order to capture the multiple relationships among nodes that should have the same semantical distance, e.g., age, hobby and profession, but also the natural order of nodes along the path in calculating similarity distance. Experimental results on several public datasets show that the NEMR outperforms the state-of-the-art methods on relevant inference tasks including link prediction and node classification.},
  archive      = {J_ASOC},
  author       = {Luodi Xie and Hong Shen and Jiaxin Ren and Huimin Huang},
  doi          = {10.1016/j.asoc.2024.112443},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112443},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Network embedding on metric of relation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective cooperation search algorithm based on
decomposition for complex engineering optimization and reservoir
operation problems. <em>ASOC</em>, <em>167</em>, 112442. (<a
href="https://doi.org/10.1016/j.asoc.2024.112442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel multi-objective cooperation search algorithm based on decomposition (MOCSA/D) to address multi-objective competitive challenges in engineering problem. Inspired by the optimization strategy of single-objective Cooperation Search Algorithm (CSA) and the decomposition framework of MOEA/D, MOCSA/D algorithm randomly generates initial solutions in the optimization space, and then repeatedly executes four search strategies until the end of iteration: Cooperative updating strategy gathers high-quality information to update solutions with balanced distribution. Reflective adjustment strategy expands the exploration range of the population, enabling the acquisition of solutions with strong optimization capabilities. Internal competition strategy selects superior individuals with better performance for subsequent optimization. Density updating strategy improves the competitiveness of optimized individuals within the population, fostering a more diverse solution set. Three numerical experiments (including DTLZ, WFG unconstrained test problems, ZXH_CF constrained test problems and RWMOP real-world multi-objective optimization problems) are tested to further comprehensively evaluate the dominant performance of MOCSA/D. The test results in different problem scenarios show that compared with the existing excellent evolutionary algorithms, MOCSA/D can always obtain a better, stable and uniform distribution of non-dominated solutions, and has higher solving efficiency and optimization quality under different performance evaluation metrics with the increasing difficulty of solving problems. Finally, the proposed algorithm is applied to the multi-objective reservoir engineering optimization problem to verify the feasibility of the decision scheme and the comprehensive benefit optimization of MOCSA/D. Overall, MOCSA/D can simplify the problem optimization difficulty based on decomposition mechanism, and improve the global optimization of population, path diversity and individual competition through different search strategies, which provides an advantageous tool for addressing multi-objective competitive challenges.},
  archive      = {J_ASOC},
  author       = {Xin-ru Yao and Zhong-kai Feng and Li Zhang and Wen-jing Niu and Tao Yang and Yang Xiao and Hong-wu Tang},
  doi          = {10.1016/j.asoc.2024.112442},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112442},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective cooperation search algorithm based on decomposition for complex engineering optimization and reservoir operation problems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multi-module echo state network with variable skip length
for chaotic time series prediction. <em>ASOC</em>, <em>167</em>, 112441.
(<a href="https://doi.org/10.1016/j.asoc.2024.112441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo state networks (ESNs) have been extensively applied in time series prediction problems. However, the memory-nonlinearity trade-off problem severely limits the ability of ESNs to deal with chaotic time series prediction problems. In this study, a multi-module echo state network with variable skip length (MESN-VSL) is proposed to address this problem. First, the reservoir is divided into a nonlinear mapping module and multiple linear memory modules based on the idea of memory and nonlinearity separation. This idea can effectively balance the memory-nonlinearity trade-off problems. Second, a multi-module mechanism with skip length is put forward to model the characteristics of chaotic time series. The skip length and the number of linear memory modules of the MESN-VSL model are automatically determined based on the idea of phase-space reconstruction. Finally, the experimental results further demonstrate that the MESN-VSL model is superior to some existing models in chaotic time series prediction.},
  archive      = {J_ASOC},
  author       = {Qianwen Liu and Fanjun Li and Shoujing Zheng and Xingshang Li},
  doi          = {10.1016/j.asoc.2024.112441},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112441},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-module echo state network with variable skip length for chaotic time series prediction},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy-based frame transformation to mitigate the impact of
adversarial attacks in deep learning-based real-time video surveillance
systems. <em>ASOC</em>, <em>167</em>, 112440. (<a
href="https://doi.org/10.1016/j.asoc.2024.112440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) techniques have become integral to smart city projects, including video surveillance systems (VSS). These advanced technologies offer significant benefits, such as enhanced accuracy and efficiency in monitoring and managing urban environments. However, despite their advantages, these systems are not without vulnerabilities. One of the most pressing challenges is their susceptibility to adversarial attacks, which can lead to critical misclassifications during inference. To address these challenges, our research focuses on developing a more robust smart city VSS. Our research unfolds across two pivotal initiatives. In our initial exploration, we introduce a pioneering framework that extends the reach of adversarial attacks to real-time VSS. A practical manifestation involved implementing a real-time face mask surveillance system based on Multi-Task Cascaded Convolutional Networks (MTCNN) for face detection and MobileNet-v2 for face mask classification, subjecting it to the Fast Gradient Sign Method (FGSM) adversarial attack in real-time. In our subsequent endeavor, we propose a sophisticated defense mechanism deploying Fuzzy Image Transformation as a pre-processing unit (FITP). This strategic defense fortification significantly reinforces our real-time VSS against adversarial intrusions. Experimental findings highlight the effectiveness of the proposed adversarial attack framework in real-time, resulting in a marked reduction in the model&#39;s performance from a precision (P) of 93 %, recall (R) of 93 %, F1 score (F) of 93 %, and accuracy (A) of 93–22 %, 21 %, 22 %, and 22 %, respectively. However, the post-implementation efficacy of our defense mechanism is striking, enhancing the model&#39;s average performance to a noteworthy improvement, with P, R, F, and A ascending to 91 %, 90 %, 91 %, and 91 %. This research illuminates the vulnerabilities intrinsic to VSS in the face of adversarial threats, underscoring the critical need for heightened awareness and the development of robust defense mechanisms before real-world deployment.},
  archive      = {J_ASOC},
  author       = {Sheikh Burhan Ul Haque},
  doi          = {10.1016/j.asoc.2024.112440},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112440},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy-based frame transformation to mitigate the impact of adversarial attacks in deep learning-based real-time video surveillance systems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient DDoS attack detection and prevention model
using fusion heuristic enhancement of deep learning approach in FANET
sector. <em>ASOC</em>, <em>167</em>, 112438. (<a
href="https://doi.org/10.1016/j.asoc.2024.112438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Problem overview A Flying Ad-hoc Network (FANET) is a decentralized communication network formed by Unmanned Aerial Vehicles (UAVs). However, it faces significant security challenges due to its decentralized nature and mobility. One of the most critical threats is the Distributed Denial-of-Service (DDoS) attack, which aims to overcome the network by flooding it with malicious actions, disrupting communication, and causing system failures. The primary challenge lies in detecting and mitigating such attacks in real-time, given the dynamic and complex nature of FANETs, where the rapid movement of UAVs complicates monitoring and defending mechanisms. Methodology A novel detection and prevention model has been proposed to address the issue of DDoS attacks utilizing a hybrid heuristic-based machine learning approach tailored for FANET environments. The model begins by collecting necessary information using benchmark datasets to train and enhance the detection performance. The collected data is used for detecting the DDoS attack using Hybrid Deep Learning (HDL). The HDL model developed by combining Deep Temporal Convolutional Networks (DTCN) and Long Short-Term Memory (LSTM) networks, offers a powerful solution. DTCNs excel at detecting short-term, rapidly changing patterns in network traffic, which is essential for recognizing the instant effects of topology changes in FANETs. At the same time, LSTMs are designed to handle long-term dependencies and can used to the evolving patterns of UAV movement and traffic behavior over time. Here, the hyper-parameters are optimized by proposing the Hybrid of Water Strider and Cuckoo Search (HWSCS). Water Strider optimization (WSA) and Cuckoo Search Optimizer (CSO) are combined in the HWSCS algorithm. This algorithm is a versatile and powerful tool for optimizing multifaceted systems across different fields. Its combination of local and global search abilities allows it to find optimal solutions in complicated environments, making it invaluable for tasks ranging from machine learning to network security and beyond. In network security, HWSCS is effective in optimizing the parameters of intrusion detection systems, particularly in detecting complex cyber threats like DDoS attacks, ensuring more accuracy while reducing false positives. After detecting the DDoS attack, it is prevented from communication by establishing the routing process. This routing process involves rerouting the network traffic away from the affected areas and towards secure servers, effectively isolating the attack and minimizing its impact on the network. Additionally, it allows for greater flexibility and adaptability in responding to evolving threats in realtime. Here, the attack mitigation is accomplished by finding the Optimal Link State Routing (OLSR), estimated by the hybrid HWSCS algorithm. This algorithm determines the most efficient path for redirecting traffic away from the targeted servers, preventing overload, and maintaining network stability. The integration of HWSCS with OLSR not only improves network security but also proves the importance of innovative solutions in protecting malicious activities. Lastly, the model&#39;s performance is validated and measured with different metrics. Results The proposed model demonstrated superior performance compared to traditional models. It achieved a recall of 93.87, significantly higher than other approaches like 89.22 for MobileNet, 89.40 for DTCN, 88.44 for LSTM, and 88.35 for DTCN-LSTM. This improved detection accuracy, combined with the effective routing mechanism, ensures better prevention and mitigation of DDoS attacks in FANETs. The results confirm the model&#39;s ability to not only detect attacks but also minimize network disruption, providing a robust solution for maintaining secure and stable communication in FANET environments.},
  archive      = {J_ASOC},
  author       = {SP Priyadharshini and P. Balamurugan},
  doi          = {10.1016/j.asoc.2024.112438},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112438},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient DDoS attack detection and prevention model using fusion heuristic enhancement of deep learning approach in FANET sector},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated search of an optimal configuration of FETI-based
algorithms with the swarm and evolutionary algorithms. <em>ASOC</em>,
<em>167</em>, 112437. (<a
href="https://doi.org/10.1016/j.asoc.2024.112437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite Element Tearing and Interconnecting (FETI) methods are used in the engineering community to solve extremely large engineering simulations on clusters and supercomputers with thousands of computational nodes. This paper focuses on minimizing the execution time of these methods by searching for an optimal configuration with swarm and evolutionary algorithms (SEAs). It incorporates optimization of an expensive cost function (taking tens to hundreds of seconds) of discrete search space with up to hundreds of thousands of combinations constrained by its boundaries and incompatible individuals (invalid combination of FETI solver parameters). In addition, the optimization occurs in real time, i.e., during a simulation. Hence, the number of objective function evaluations must remain low (tens to lower hundreds). The paper compares the performance of 3 basic SEAs with a reduced population size (Differential Evolution, Particle Swarm Optimization, and Self-Organizing Migrating Algorithm), 4 micro SEAs (Micro Differential Evolution Ray, Improved Micro-Particle Swarm Optimization, Micro-Particle Swarm Optimization, and Micro-Genetic Algorithm), and a random search on 4 distinct time-dependent simulations of a heat transfer. The experiments show that basic SEAs with a small population (about 5 individuals) and a penalty system as protection against incompatible configurations represent the most effective solution. One can use them to find the optimal configuration of FETI-based methods in approximately 130 evaluations. It can improve the utilization of expensive hardware resources of modern computational clusters.},
  archive      = {J_ASOC},
  author       = {Tomáš Panoc and Ondřej Meca and Lukas Tomaszek and Tomáš Brzobohatý and Lubomír Říha and Ivan Zelinka and Tomáš Kozubek},
  doi          = {10.1016/j.asoc.2024.112437},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112437},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated search of an optimal configuration of FETI-based algorithms with the swarm and evolutionary algorithms},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-strategy fruit fly optimization algorithm for the
distributed permutation flowshop scheduling problem with
sequence-dependent setup times. <em>ASOC</em>, <em>167</em>, 112436. (<a
href="https://doi.org/10.1016/j.asoc.2024.112436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed manufacturing has become one of the mainstream manufacturing modes today and is widely present in industries such as aviation and electronics. However, in actual production processes, unexpected situations such as machine failures and tool changes may occur, which require time. Based on practical needs, this paper studies a distributed permutation flow shop scheduling problem with sequence-dependent setup times (DPFSP/SDST) aimed at minimizing the makespan and proposes a hybrid multi-strategy fruit fly optimization algorithm (HMFOA) to solve it. In HMFOA, three strategies are constructed to initialize the positions of some individual flies in the solution space to improve population diversity. In the smell search phase, four problem-oriented neighborhood perturbation operators are designed, and sinusoidal optimization algorithm is introduced to control the search range, which improves the global search ability of the algorithm. In the visual search phase, a position reconstruction strategy is proposed to divide individual flies into different populations based on their mass. Through the interaction of individuals from different populations, the convergence is accelerated and the algorithm efficiency is improved. In addition, a local search strategy is designed to guide the flies to more promising areas. Based on well-known examples of DPFSP in the literature, a comprehensive test set was generated for DPFSP/SDST, taking into account various combinations of jobs, machines, factories, and SDST, resulting in 270 benchmark instances used to validate the performance of HMFOA, and compared to eight other advanced algorithms. The relative percentage deviation of HMFOA is 1.00%, which is significant improvement.},
  archive      = {J_ASOC},
  author       = {Cai Zhao and Lianghong Wu},
  doi          = {10.1016/j.asoc.2024.112436},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-strategy fruit fly optimization algorithm for the distributed permutation flowshop scheduling problem with sequence-dependent setup times},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive fuzzy neighborhood decision tree. <em>ASOC</em>,
<em>167</em>, 112435. (<a
href="https://doi.org/10.1016/j.asoc.2024.112435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision tree algorithms have gained widespread acceptance in machine learning, with the central challenge lying in devising an optimal splitting strategy for node sample subspaces. In the context of continuous data, conventional approaches typically involve fuzzifying data or adopting a dichotomous scheme akin to the CART tree. Nevertheless, fuzzifying continuous features often entails information loss, whereas the dichotomous approach can generate an excessive number of classification rules, potentially leading to overfitting. To address these limitations, this study introduces an adaptive growth decision tree framework, termed the fuzzy neighborhood decision tree (FNDT). Initially, we establish a fuzzy neighborhood decision model by leveraging the concept of fuzzy inclusion degree. Furthermore, we delve into the topological structure of misclassified samples under the proposed decision model, providing a theoretical foundation for the construction of FNDT. Subsequently, we utilize conditional information entropy to sift through original features, prioritizing those that offer the maximum information gain for decision tree nodes. By leveraging the conditional decision partitions derived from the fuzzy neighborhood decision model, we achieve an adaptive splitting method for optimal features, culminating in an adaptive growth decision tree algorithm that relies solely on the inherent structure of real-valued data. Experimental evaluations reveal that, compared with advanced decision tree algorithms, FNDT exhibits a simple tree structure, stronger generalization capabilities, and superior performance in classifying continuous data.},
  archive      = {J_ASOC},
  author       = {Xinyu Cui and Changzhong Wang and Shuang An and Yuhua Qian},
  doi          = {10.1016/j.asoc.2024.112435},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive fuzzy neighborhood decision tree},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive decomposition-based evolutionary algorithm for
many-objective optimization with two-stage dual-density judgment.
<em>ASOC</em>, <em>167</em>, 112434. (<a
href="https://doi.org/10.1016/j.asoc.2024.112434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to better balance the convergence and diversity of MOEA/D for many objective optimization problems (MaOPs) with various Pareto fronts (PFs), an adaptive decomposition-based evolutionary algorithm for MaOPs with two-stage dual-density judgment is proposed. To solve the problem that weighted Tchebycheff decomposition may produce weakly Pareto optimal solutions when the solution is not unique or the uniqueness is difficult to guarantee, an augmented weighted Tchebycheff decomposition is adopted. To balance the convergence and diversity of non-dominated solutions in the external archive, different sparsity-level evaluations using vector angles or Euclidean distances are used to measure the distribution of solutions at different stages. To improve the diversity of solution sets obtained by MOEA/D for various PFs, an adaptive weight vector adjustment method based on two-stage dual-density judgment is presented. For weight vector addition, the potential search area is found according to the two-stage density judgment, and then a two-stage sparsity level judgment on the solutions of this area is performed for a second density judgment. For weight vector deletion, the degree of crowding is used to delete the weight vectors with a high crowding degree. Compared with nine advanced multi-objective optimization algorithms on DTLZ and WFG problems, the results demonstrate that the performance of the proposed algorithm is significantly better than other algorithms.},
  archive      = {J_ASOC},
  author       = {Yongjun Sun and Jiaqi Liu and Zujun Liu},
  doi          = {10.1016/j.asoc.2024.112434},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112434},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive decomposition-based evolutionary algorithm for many-objective optimization with two-stage dual-density judgment},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of ant colony optimization algorithm based on
farthest point optimization and multi-objective strategy in robot path
planning. <em>ASOC</em>, <em>167</em>, 112433. (<a
href="https://doi.org/10.1016/j.asoc.2024.112433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of high technology and the continuous progress of intelligent industry, mobile robots are gradually widely used in various fields. In the field of mobile robot research, path planning is crucial. However, the current ant colony optimization algorithm applied to mobile robot path planning still has some limitations, such as early blind search, slower convergence speed, and lower path smoothness. To overcome these problems, this paper proposes an ant colony optimization algorithm based on farthest point optimization and multi-objective strategy. The algorithm introduces new heuristic information such as the normal distribution model, triangle inequality principle, smoothness function, safety value function, etc. It adopts multi-objective comprehensive evaluation indexes to judge the quality of paths. For the high-quality and poor-quality paths, the algorithm takes additional pheromone increments and decrements in pheromone concentration to speed up the algorithm’s convergence. Besides, the farthest point optimization strategy is used to improve the quality of the paths further. Finally, to verify the algorithm’s effectiveness, the algorithm is compared with 20 existing methods for solving the robot path planning problem, and the experimental results show that the algorithm exhibits better results in terms of convergence, optimal path length, and smoothness. Specifically, the algorithm can produce the shortest path in four different environments while realizing the least number of turns with faster convergence, further proving the effectiveness of the improved algorithm in this paper.},
  archive      = {J_ASOC},
  author       = {Shuai Wu and Ani Dong and Qingxia Li and Wenhong Wei and Yuhui Zhang and Zijing Ye},
  doi          = {10.1016/j.asoc.2024.112433},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112433},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of ant colony optimization algorithm based on farthest point optimization and multi-objective strategy in robot path planning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering by detecting skeletal structure and identifying
density fluctuation. <em>ASOC</em>, <em>167</em>, 112432. (<a
href="https://doi.org/10.1016/j.asoc.2024.112432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is one of the most important techniques for unsupervised learning, it tries to divide points into different clusters without any priori knowledge of data. Therefore, several criterions for clustering algorithm are as follows: 1. Handling clusters with arbitrary shape and various density; 2. Finding cluster centers automatically; 3. Low parameter sensitivity and computational complexity. In this context, a novel algorithm namely clustering by detecting skeletal structure and identifying density fluctuation (CSSDF) was presented. In CSSDF, an efficient strategy based on density and local information of neighborhood is firstly proposed to detect the skeletal structure, which can collect the local information and identify the rough distribution of data. With the identified distribution information, a method takes expanded neighborhood and density fluctuation into consideration is proposed to further collect global information of data, which can assign all skeleton points and find cluster centers. To sum up, CSSDF can not only discover the underlying structure of data regardless of its’ distribution, but also ensure the correct assignment of all skeleton points and thus lead to a satisfying clustering performance. In addition, the computational complexity of the proposed approach is O ( nlogn ) , which makes it possible to deal with some large clustering problem.},
  archive      = {J_ASOC},
  author       = {Wenjie Guo and Wei Chen and Xinggao Liu},
  doi          = {10.1016/j.asoc.2024.112432},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112432},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering by detecting skeletal structure and identifying density fluctuation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shapelet selection for time series classification.
<em>ASOC</em>, <em>167</em>, 112431. (<a
href="https://doi.org/10.1016/j.asoc.2024.112431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, increasing attention has been given to shapelet-based methods for time series classification. However, in the majority of current methods, similar subsequences were often selected as shapelets, thereby reducing the final interpretability of these methods. Aiming to circumvent the selection of similar subsequences as the final shapelets, a novel shapelet selection method (SSM) was proposed in this paper. Firstly, shapelet candidates were generated by SSM through time series segmentation to avoid excessive generation of similar candidates from a single time series. Secondly, all shapelet candidates were evaluated simultaneously to improve evaluation efficiency. Finally, SSM introduced a position-based filter to prevent the selection of similar sequences repeatedly. The results obtained on the UCR TSC archive demonstrated the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Cun Ji and Yanxuan Wei and Xiangwei Zheng},
  doi          = {10.1016/j.asoc.2024.112431},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112431},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Shapelet selection for time series classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering based fuzzy classification with a noise cluster
in detecting fraud in insurance. <em>ASOC</em>, <em>167</em>, 112430.
(<a href="https://doi.org/10.1016/j.asoc.2024.112430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fraud detection is one of the main issues in reducing the unsystematic risks in insurance business as its costs might reach to catastrophic amounts leading to higher loadings on reserves and premiums. Due to its cause of nature in diversity, fraud detection may require a wide range of factors and variables to be considered. To make logical relations between many factors and reveal their differences, estimate odds (or probabilities), and predict the fraud risk, scoring systems become an important aid. In this paper, we introduce a clustering-based fuzzy classification with a noise cluster (CBFCN) to identify the true state of a fraud. The approach proposed in this paper is based on fuzzy k-means clustering having a noise cluster (FKMN) and is a novel method for identifying outliers by achieving robust clustering. We integrate fuzzy theory to boost the prediction ability of machine learning (ML) approaches for a proper determination of the contributing features. The two critical features of the CBFCN method which are the membership values obtained from the FKMN clustering algorithm are implemented to capture the behavior of an existing structure better and detect the noise (extremes) in the dataset. Extensive analyses are made on two real datasets exposing different characteristics in their variables to demonstrate how CBFCN performs in detecting the fraud compared to the conventional approaches. Additionally, employing fuzzy approach to improve the ML performance is elaborated through the inclusion of noise clusters. The findings indicate that the suggested CBFCN models produce promising classification results in fraud detection in insurance claims occurrences.},
  archive      = {J_ASOC},
  author       = {Oguz Koc and Furkan Baser and A. Sevtap Selcuk-Kestel},
  doi          = {10.1016/j.asoc.2024.112430},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112430},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering based fuzzy classification with a noise cluster in detecting fraud in insurance},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combined fuzzy-metaheuristic framework for bridge health
monitoring using UAV-enabled rechargeable wireless sensor networks.
<em>ASOC</em>, <em>167</em>, 112429. (<a
href="https://doi.org/10.1016/j.asoc.2024.112429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is essential to monitor the health of important infrastructure ( e.g. , bridges) to maintain their functions. Visual inspections have been conventionally dominant in this regard, although they are susceptible to human errors. Wireless sensor networks (WSNs) provide an automated, convenient, and low-cost option for developing bridge health monitoring (BHM) networks. However, the constant use of WSNs for monitoring the structural and environmental health of bridges can pose a serious challenge due to the limited lifetimes of these networks that depend on the battery lifetimes of sensor nodes. This paper proposes a combined fuzzy-metaheuristic framework to maintain the BHM stability by using rechargeable sensors. This framework benefits from metaheuristic methods and the fuzzy logic to match the sensor network configuration management to the specific conditions of each bridge, recognizing that different bridges share very few common characteristics. Every new bridge is unique; hence, it is difficult to design a BHM paradigm that fits the conditions of all bridges. The proposed framework manages the current network configuration concerning the conditions of each bridge. This framework manages the network topology formation, information relay, and recharge by using a multipurpose objective, tuning control parameters, and controlling network activities in an optimization process. Moreover, unmanned aerial vehicles (UAVs) are employed to recharge sensor nodes under the proposed framework strategies to overcome the energy limitation of sensor nodes. The proposed framework is evaluated on three bridge scenarios: Hardanger Bridge, Bergsøysund Bridge, and New Carquinez Suspension Bridge. Compared to common WSN methods, it demonstrated superior performance under various conditions, including the rate of active and inactive nodes, energy efficiency, survival rate, stability, recharge delay, average node energy, recharge requests, and total packets received. The evaluation results demonstrate that the proposed framework significantly surpasses existing methods in terms of WSN performance metrics. The results show that the proposed framework outperforms existing methods by an average of 32.8 % for the Hardanger Bridge, 53.2 % for the Bergsøysund Bridge, and 31.2 % for the New Carquinez Suspension Bridge.},
  archive      = {J_ASOC},
  author       = {Fakhrosadat Fanian and Marjan Kuchaki Rafsanjani and Mohammad Shokouhifar},
  doi          = {10.1016/j.asoc.2024.112429},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112429},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined fuzzy-metaheuristic framework for bridge health monitoring using UAV-enabled rechargeable wireless sensor networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A strengthened constrained-dominance based evolutionary
algorithm for constrained many-objective optimization. <em>ASOC</em>,
<em>167</em>, 112428. (<a
href="https://doi.org/10.1016/j.asoc.2024.112428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving constrained multi-objective optimization problems have received increasing attention. However, there are few researches based on constrained many-objective optimization problems that widely exist in real life. Given the above fact, we propose a strengthened constrained-dominance based evolutionary algorithm for constrained many-objective optimization (SCEA). The proposed SCEA includes the following main components. First, a dual-assistance mating selection is developed to select elite parents for variation, and further accelerate the generation of feasible solutions. Second, a strengthened constrained-dominance relation is proposed, which favors feasible solutions but still leaves the room for selecting infeasible solutions. This is achieved by simultaneously considering the objective optimization and constraint satisfaction. Third, the designed unconstrained aggregation (UA) indicator and crowded detector cooperate reference points to promote the convergence and diversity of population. Finally, a cooperation mechanism based on the constrained aggregation (CA) indicator and hierarchical clustering is designed to drive individuals toward different feasible regions, and further balance the objective optimization and constraint satisfaction. Extensive experimental studies are conducted on three benchmark test suites and two real-world applications to validate the performance of SCEA. The corresponding experiment results have demonstrated that SCEA is more competitive than its peer competitors.},
  archive      = {J_ASOC},
  author       = {Wei Zhang and Jianchang Liu and Junhua Liu and Yuanchao Liu and Shubin Tan},
  doi          = {10.1016/j.asoc.2024.112428},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112428},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A strengthened constrained-dominance based evolutionary algorithm for constrained many-objective optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual compression model for online continual learning.
<em>ASOC</em>, <em>167</em>, 112427. (<a
href="https://doi.org/10.1016/j.asoc.2024.112427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-Free Continual Learning (TFCL) presents a notably demanding but realistic ongoing learning concept, aiming to address catastrophic forgetting in sequential learning systems. In this paper, we tackle catastrophic forgetting by introducing an innovative dynamic expansion framework designed to adaptively enhance the model’s capacity for novel data learning while also remembering the information learnt in the past, by using a minimal-size processing architecture. Our proposed framework incorporates three key mechanisms to mitigate model’ forgetting: (1) by employing a Maximum Mean Discrepancy (MMD)-based expansion mechanism that assesses the disparity between previously acquired knowledge and that from the new training data, serving as a signal for the model’s architecture expansion; (2) a component discarding mechanism that eliminates components characterized by redundant information, thereby optimizing the model size while fostering knowledge diversity; (3) a novel training sample selection strategy that leads to the diversity of the training data for each task. We conduct a series of TFCL experiments that demonstrate the superiority of the proposed framework over all baselines while utilizing fewer components than alternative dynamic expansion models. The results on the Split Mini ImageNet dataset, after splitting the original dataset into multiple tasks, are improved by more than 2% when compared to the closest baseline.},
  archive      = {J_ASOC},
  author       = {Fei Ye and Adrian G. Bors},
  doi          = {10.1016/j.asoc.2024.112427},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continual compression model for online continual learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective optimization and multi-attribute
decision-making support for optimal operation of multi stakeholder
integrated energy systems. <em>ASOC</em>, <em>167</em>, 112426. (<a
href="https://doi.org/10.1016/j.asoc.2024.112426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To efficiently tackle the optimal operation problem of multi-stakeholder integrated energy systems (IESs), this paper develops a multi-objective optimization and multi-attribute decision-making support method. Mathematically, The optimal operation of IESs interconnected with distributed district heating and cooling units (DHCs) via the power grid and gas network, can be formulated as a multi-objective optimization problem considering both economic, reliability and environment-friendly objectives with numerous constraints of each energy stakeholder. Firstly, a multi-objective group search optimizer with probabilistic operator and chaotic local search (MPGSO) is proposed to balance global and local optimality during the random search iteration. The MPGSO utilizes a crowding probabilistic operator to select producers to explore areas with higher potential but less crowding and reduce the number of fitness function calculations. Moreover, a new parameter selection strategy based on chaotic sequences with limited computational complexity is adopted to escape the local optimal solutions. Consequently, a set of superior Pareto-optimal fronts could be obtained by the MPGSO. Subsequently, a multi-attribute decision-making support method based on the interval evidential reasoning (IER) approach is used to determine a final optimal solution from the Pareto-optimal solutions, taking multiple attributes of each stakeholder into consideration. To verify the effectiveness of the MPGSO, the DTLZ suite of benchmark problems are tested compared with the original GSOMP, NSGA-II and SPEA2. Additionally, simulation studies are conducted on a modified IEEE 30-bus system connected with distributed DHCs and a 15-node gas network to verify the proposed approach. The quality of the obtained Pareto-optimal solutions is assessed using a set of criteria, including hypervolume (HV), generational distance (GD), and Spacing index, among others. Simulation results show that the number of Pareto-optimal solutions (NPS) of MPGSO are higher by about 32.6 %-62.1%, computation time (CT) are lower by about 2.94 %-46.1 % compared with other algorithms. Besides, to further evaluate the performance of the proposed approach in addressing larger-scale issues, the study employs the modified IEEE 118-bus system of greater magnitude. The proposed MPGSO algorithm effectively handles multi-objective and non-convex optimization problems with Pareto sets in terms of better convergence and distributivity.},
  archive      = {J_ASOC},
  author       = {J.H. Zheng and L.X. Zhai and Fang Li and Dandan Wang and Yalou Li and Zhigang Li and Q.H. Wu},
  doi          = {10.1016/j.asoc.2024.112426},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization and multi-attribute decision-making support for optimal operation of multi stakeholder integrated energy systems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Radial basis function neural network based data-driven
iterative learning consensus tracking for unknown multi-agent systems.
<em>ASOC</em>, <em>167</em>, 112425. (<a
href="https://doi.org/10.1016/j.asoc.2024.112425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a novel data-driven-distributed-consensus control protocol for unknown nonlinear non-affine discrete-time multi-agent systems (MAS) with repetitive properties. The leader’s commands are directed to the followers in the topological graph. The dynamic linearization technology (DLT) is used to build the distributed iterative learning (IL) controller along the iteration axis. In the iterative process, the control gain is automatically adjusted by updating the weight matrix of the high-order radial basis function neural network (RBFNN, HORBFNN). In global control, the higher order parameter (HOP) Newton method is used to achieve global convergence and stability of the control process. All the above processes do not require the understanding of dynamical equations or physical models for each agent, and only use local communication information of multi-agent to achieve consistent tracking of MAS leaders and followers. Based on the strong connection, the convergence performance, stability and boundedness properties of the proposed control protocol in the fixed topology as well as in the iterative topology are validated by a rigorous theoretical analysis. Simulation experiments are conducted to verify the effectiveness of the control protocol.},
  archive      = {J_ASOC},
  author       = {Kechao Xu and Bo Meng and Zhen Wang},
  doi          = {10.1016/j.asoc.2024.112425},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112425},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Radial basis function neural network based data-driven iterative learning consensus tracking for unknown multi-agent systems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An altitude-aware fuzzy approach for energy efficiency in
UAV-assisted 3D wireless sensor networks. <em>ASOC</em>, <em>167</em>,
112424. (<a href="https://doi.org/10.1016/j.asoc.2024.112424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Wireless Sensor Networks (WSNs) that use multi-hop topologies, issues like energy holes and hotspots have become prominent. To address these, recent research has proposed using mobile sinks with abundant resources. These include mobile robots, drones, and notably, Unmanned Aerial Vehicles (UAVs), as solutions to alleviate these challenges. This paper introduces a novel altitude-aware fuzzy approach aimed at improving energy efficiency in UAV-supported 3D WSNs. The proposed methodology comprises two key components. Firstly, a tailored fuzzy clustering algorithm is developed to manage the spatial structure of the 3D WSN, optimizing energy consumption. Secondly, a hybrid grey wolf optimization algorithm is utilized to fine-tune the parameters of the fuzzy clustering algorithm, ensuring optimal performance. The synergistic and seamless integration of these components addresses the energy efficiency challenges inherent in UAV-assisted 3D WSNs. The significance of this approach lies in its capacity to navigate the escalating complexity and energy demands of modern sensor networks, offering a harmonious blend of theoretical innovation and practical applicability. Experimental analysis and results substantiate the superior performance of the proposed approach compared to existing solutions, as measured by the metrics commonly employed to evaluate the network lifetime of protocols in the literature.},
  archive      = {J_ASOC},
  author       = {Seyyit Alper Sert and Adnan Yazici},
  doi          = {10.1016/j.asoc.2024.112424},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112424},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An altitude-aware fuzzy approach for energy efficiency in UAV-assisted 3D wireless sensor networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal analysis and trend prediction of regional
crop disease based on electronic medical records. <em>ASOC</em>,
<em>167</em>, 112423. (<a
href="https://doi.org/10.1016/j.asoc.2024.112423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent diagnosis of individual crop diseases has matured. How to understand the evolution patterns and predict regional disease trends remains a significant challenge. Plant Electronic Medical Records (PEMRs) offer valuable spatial-temporal characteristics about crop diseases, presenting a new opportunity for predicting the occurrence of regional diseases. In this study, we used a large prescription database from Beijing (2018–2021) to reframe regional disease prediction as a time series forecasting task. Firstly, to analyze spatial-temporal evolution patterns, we use ArcGIS to extract key information and identify potential connections between different disease occurrence points. Then, we developed a novel deep learning combined model SV-CBA, which combines Seasonal and Trend decomposition (STL) with Variational Mode Decomposition (VMD) to identify trend, seasonal, and residual components, and re-decomposes the residuals. STL-VMD can capture long-term trends and periodic variations while managing nonlinear and volatile characteristics. The CNN-BiLSTM-Attention model calculates disease trends by linearly integrating predictions of each sub-series. To reduce computational complexity while maintaining predictive performance, we propose an improved simplified attention mechanism. Our model demonstrates superior performance in both comparative and ablation experiments using the PEMRs dataset, outperforming numerous other models. This study provides accurate disease trend predictions, aiding farmers and regional managers in agricultural production management.},
  archive      = {J_ASOC},
  author       = {Chang Xu and Lei Zhao and Haojie Wen and Lingxian Zhang},
  doi          = {10.1016/j.asoc.2024.112423},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112423},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spatial-temporal analysis and trend prediction of regional crop disease based on electronic medical records},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective bi-decoding networks for rail-surface defect
detection by knowledge distillation. <em>ASOC</em>, <em>167</em>,
112422. (<a href="https://doi.org/10.1016/j.asoc.2024.112422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {No-service rail-surface defect detection is a crucial method for assessing the quality of railroad tracks. However, the low-contrast and dark-tone characteristics of track-surface textures pose challenges to current defect-monitoring techniques. Real-time and on-site online inspections are important to ensure safe railway operation; however, most complex models for no-service inspections are difficult to deploy on mobile devices. To address these challenges and overcome the detection difficulties associated with complex scenes, we designed a knowledge distillation-based double decoding-layer refinement network (EBDNet-KD). The first decoding process is guided by a bimodal high-level semantic feature map obtained by extending the attention-based graph convolution to incrementally enhance the dual-stream features and obtain an image restoration prior. A divide-and-conquer decoder is then designed to distinguish features using different decoding layers. The prior is then used in the second decoding layer, which enables the bimodal features to interact fully and obtain the final prediction map. We introduce a knowledge distillation strategy that enables a lightweight, compact student network to learn a complex teacher network’s feature extraction process. This facilitates pixel-consistent learning of the knowledge within the bi-decoder layer, as well as bidirectional learning of the focused contextual response knowledge to optimize the model. The EBDNet-KD significantly reduces computational costs while guaranteeing performance with a parameter count of only 28 M. EBDNet-KD demonstrated superior performance over 15 state-of-the-art methods in experiments conducted on NEU RSDDS-AUG, an industrial RGB-depth dataset. We assessed the generalizability of EBDNet-KD by evaluating its performance on three additional public datasets, yielding competitive results. The source code and results can be found at https://github.com/Wuyue15/EBDNet .},
  archive      = {J_ASOC},
  author       = {Wujie Zhou and Yue Wu and Weiwei Qiu and Caie Xu and Fangfang Qiang},
  doi          = {10.1016/j.asoc.2024.112422},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective bi-decoding networks for rail-surface defect detection by knowledge distillation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Knowledge graph-driven mountain railway alignment
optimization integrating karst hazard assessment. <em>ASOC</em>,
<em>167</em>, 112421. (<a
href="https://doi.org/10.1016/j.asoc.2024.112421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Karst hazard is a considerable threat that should be considered in railway alignment design for mountainous regions with dense water systems. Nevertheless, alignment design principles in karst regions have not been systematically studied. Moreover, a quantitative karst hazard assessment model is currently lacking for automated alignment optimization. To solve the above problems, based on the analyses of karst inducing factors and hazard representation, the railway alignment design principles in karst regions are summarized through an event tree. A highly-coupled knowledge graph (called KaRAD-KG) modeling method is proposed. Then, a bi-objective alignment optimization model considering railway construction cost and karst hazard (mainly including hazard components of synclinal karst, anticlinal karst and karst depression) is constructed. To solve the optimization model, a knowledge-driven distance transform algorithm incorporating a karst hazard assessment method and a multicriteria tournament decision method is customized. Finally, the application in a real-world case indicates that the proposed method can generate an alignment which reduces construction cost by 3.39 % and karst hazard by 18.73 % compared to the best manually-designed alternative, which verifies the effectiveness of this method for assisting actual railway alignment design in a karst-dense mountainous region.},
  archive      = {J_ASOC},
  author       = {Hao Pu and Ting Hu and Taoran Song and Paul Schonfeld and Wei Li and Lihui Peng},
  doi          = {10.1016/j.asoc.2024.112421},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Knowledge graph-driven mountain railway alignment optimization integrating karst hazard assessment},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A shared multi-scale lightweight convolution generative
network for few-shot multivariate time series forecasting.
<em>ASOC</em>, <em>167</em>, 112420. (<a
href="https://doi.org/10.1016/j.asoc.2024.112420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is an important time series data mining technique. Among them, multivariate time series (MTS) forecasting has received extensive attention in many fields. However, many existing MTS forecasting models usually rely on a large amount of labeled data for model training, and data collection and labeling are difficult in real systems. The insufficient amount of data makes it difficult for the model to fully learn the intrinsic patterns and features of the data, which not only increases the prediction error, but also makes it hard to obtain satisfactory prediction results. To address this challenge, we propose a shared multi-scale lightweight convolution generative (SMLCG) network for few-shot multivariate time series forecasting by using samples generation strategy. The overall goal is to design a shared multi-scale feature generation prediction framework that generates data highly similar to the original sample and enriches the training sample to improve prediction accuracy. Specifically, the MTS is divided into different scales, and the multi-scale feature fusion module is utilized to capture and fuse the MTS information in different spatial dimensions to eliminate the heterogeneity among the data. Then, the key information in the multi-scale features is captured by a lightweight convolution generative network, and the feature weights are dynamically assigned to explore the change information. In addition, a spatio-temporal memory module is designed based on the parameter sharing strategy to capture the spatio-temporal dynamic relationship of sequences by learning the common knowledge in multi-scale features, thus improving the robustness and generalization ability. Through comprehensive experiments on four publicly available datasets and comparisons with other reported models, it is demonstrated that the SMLCG model can efficiently generate approximate samples in the few-shot case and provide excellent prediction results. The architecture of SMLCG serves as a valuable reference for practical solutions to address the few-shot problem in multivariate time series.},
  archive      = {J_ASOC},
  author       = {Minglan Zhang and Linfu Sun and Jing Yang and Yisheng Zou},
  doi          = {10.1016/j.asoc.2024.112420},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A shared multi-scale lightweight convolution generative network for few-shot multivariate time series forecasting},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A switching based forecasting approach for forecasting sales
data in supply chains. <em>ASOC</em>, <em>167</em>, 112419. (<a
href="https://doi.org/10.1016/j.asoc.2024.112419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting future demand has been a challenging task for supply chain practitioners, which is further exacerbated due to the recent pandemic effects. While the literature suggests a potential for improved accuracy with ML/AI approaches compared to probabilistic distribution-based traditional forecasting methods, the extent of this enhancement may vary based on the specific case. It is recognized that traditional probabilistic forecasting approaches are often considered less accurate and may lead to errors, potentially influencing the estimation of overall business costs. Meanwhile, with the advancement of artificial intelligence (AI) approaches, such as machine learning (ML) and deep learning (DL), this misestimation of cost can be reduced by forecasting demand more accurately from historical data. Consequently, this paper applies several AI-based approaches to predict demand data. Since no fixed AI approach works best for all datasets, a switching-based forecasting approach (SBFA) is proposed to exploit the merit of different advanced ML/DL approaches for different days ahead of prediction. Based on the performance of validation data, the proposed system automatically switches between different approaches to determine a more appropriate forecasting approach. A two-echelon supply chain model with different attributes is developed to validate the proposed SBFA against a few traditional forecasting approaches. The reorder points of this supply chain model are calculated based on the predictions from conventional/ML/DL forecasting approaches. Predictions from SBFA and other approaches are analysed by calculating overall supply chain cost. Based on overall supply chain costs under static and dynamic lead time settings, the effectiveness and applicability of the proposed SBFA against traditional forecasting approaches are demonstrated.},
  archive      = {J_ASOC},
  author       = {Supriyo Ahmed and Ripon K. Chakrabortty and Daryl L. Essam and Weiping Ding},
  doi          = {10.1016/j.asoc.2024.112419},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A switching based forecasting approach for forecasting sales data in supply chains},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fairness in online vehicle-cargo matching: An intuitionistic
fuzzy set theory and tripartite evolutionary game approach.
<em>ASOC</em>, <em>167</em>, 112418. (<a
href="https://doi.org/10.1016/j.asoc.2024.112418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the concept of fairness and equitable matching in an on-line vehicle-cargo matching setting, addressing the varying degrees of satisfaction experienced by shippers and carriers. Relevant indicators for shippers and carriers in the on-line matching process are categorized as attributes, expectations, and reliability, which are subsequent quantified to form satisfaction indicators. Employing the intuitionistic fuzzy set theory, we devise a transformed vehicle-cargo matching optimization model by combining the fuzzy set’s membership, non-membership, and uncertainty information. Through an adaptive interactive algorithm, the matching scheme with fairness concerns is solved using CPLEX. The effectiveness of the proposed matching mechanism in securing high levels of satisfaction is established by comparison with three benchmark methods. To further investigate the impact of considering fairness in vehicle-cargo matching, a shipper-carrier-platform tripartite evolutionary game framework is developed under the waiting response time cost (WRTC) sharing mechanism. Simulation results show that with fairness concerns in vehicle-cargo matching, all stakeholders are better off: The platform achieves positive revenue growth, and shippers and carriers receive positive subsidy. This study offers both theoretical insights and practical guidance for the long-term and stable operation of the on-line freight stowage industry.},
  archive      = {J_ASOC},
  author       = {Binzhou Yang and Ke Han and Wenrui Tu and Qian Ge},
  doi          = {10.1016/j.asoc.2024.112418},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fairness in online vehicle-cargo matching: An intuitionistic fuzzy set theory and tripartite evolutionary game approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online q-learning for stochastic linear systems with state
and control dependent noise. <em>ASOC</em>, <em>167</em>, 112417. (<a
href="https://doi.org/10.1016/j.asoc.2024.112417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For continuous-time (CT) systems that are characterized by stochastic differential equations (SDEs) with completely unknown dynamics parameters, a reinforcement learning (RL)-based optimal control framework is presented in this paper. To obtain the near-optimal control policy, an online Q-learning algorithm is proposed by learning the data sampled from the system state trajectory, while an integral reinforcement learning (IRL) approach is developed in stochastic situation so as to formulate the Q-learning iterative algorithm. In particular, an actor/critic neural network (NN) structure is applied in iteration, where a critic approximator is used for estimating the designed Q-function while an actor approximator is for estimating the optimal control policy online. To ensure the convergence of iteration, the tuning laws of two neural networks are designed, respectively, by a gradient descent scheme. Moreover, the mean-square stability of the closed-loop system is proved through rigorous analysis, and the convergence to optimal solution is guaranteed as well.},
  archive      = {J_ASOC},
  author       = {Hongxu Zhu and Wei Wang and Xiaoliang Wang and Shufan Wu and Ran Sun},
  doi          = {10.1016/j.asoc.2024.112417},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online Q-learning for stochastic linear systems with state and control dependent noise},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Segmentation of the customers based on customer value: A
three-way decision perspective. <em>ASOC</em>, <em>167</em>, 112415. (<a
href="https://doi.org/10.1016/j.asoc.2024.112415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper establishes an innovative value evaluation framework based on the criterion-oriented three-way decision (3WD) in the double hierarchy linguistic term (DHLT) environment to help the customer manager finish customer segmentation. Customer relationship management is the key to the success of enterprises in the information economy era. The segmentation of customers based on their relative criteria can identify the customers who are high-value customers for enterprises. However, multi-criteria decision-making can only display the value ranking of customers, rather than the value segmentation of customers. The employment of 3WD solves this problem. Then we classify the customers based on the 3WD method. First, the criteria are evaluated by using DHLTs, while the weights of criteria are acquired according to the maximum deviation method. Second, the conditional probabilities are estimated by the improved TOPSIS method combined with gray relation analysis, while the threshold values are calculated by the relative utilities which are constructed on the basis of the criterion information. Subsequently, the segmentation of customers is obtained according to the maximum-utility principle. Lastly, case research about the segmentation of customers based on value is used to demonstrate the practicality of our method, while some strategies about customer relationship management are given based on customer segmentation for obtaining maximum returns with minimum investment.},
  archive      = {J_ASOC},
  author       = {Xiang Li and Zeshui Xu},
  doi          = {10.1016/j.asoc.2024.112415},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112415},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Segmentation of the customers based on customer value: A three-way decision perspective},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated segmentation of retinal layers in optical
coherence tomography images using xception70 feature extraction.
<em>ASOC</em>, <em>167</em>, 112414. (<a
href="https://doi.org/10.1016/j.asoc.2024.112414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography (OCT) imaging plays a critical role in evaluating retinal layer thickness, serving as a pivotal diagnostic tool for numerous retinal conditions. However, challenges such as speckle noise, poor image contrast, and ambiguous retinal detachments, like drusen, often hinder accurate segmentation of retinal layers. To address these challenges and enhance diagnostic precision, we introduce the Retinal Segmentation Network, or &quot;Ret-Seg Net,&quot; a deep neural network-based approach. Leveraging the advanced Xception70 feature extractor, Ret-Seg Net extracts and comprehends the intricate properties of retinal layers. By integrating acquired feature maps from Xception70 into the atrous spatial pyramid-pooling module, Ret-Seg Net extracts multiscale feature information. The encoder-decoder module of Ret-Seg Net achieves automated segmentation of retinal layers in OCT images by reconstructing distinct retinal layer borders. This advanced module accurately recognizes and differentiates retinal layer boundaries, providing precise and reliable segmentation. Validation of our approach using real-time images and the Duke dataset, comprising 310 volumes with 40 B-scans each, demonstrates outstanding performance. Mean intersection over union (MIoU) and sensitivity (Se) metrics achieved remarkable values of 94.52 % and 96.25 % respectively. Furthermore, our approach offers a versatile segmentation framework applicable to various tissues and cell types in clinical settings. Automating segmentation of retinal layers enhances precision in disease identification and monitoring while significantly improving labor efficiency by reducing the need for manual segmentation.},
  archive      = {J_ASOC},
  author       = {Pavithra Mani and Neelaveni Ramachandran and Palanichamy Naveen and Prasanna Venkatesh Ramesh},
  doi          = {10.1016/j.asoc.2024.112414},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112414},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated segmentation of retinal layers in optical coherence tomography images using xception70 feature extraction},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). A dynamical teaching-learning-based optimization algorithm
for fuzzy energy-efficient parallel batch processing machines scheduling
in fabric dyeing process. <em>ASOC</em>, <em>167</em>, 112413. (<a
href="https://doi.org/10.1016/j.asoc.2024.112413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fabric dyeing is the most time-consuming and energy-intensive process in textile production with some batch processing machines (BPMs) and uncertainty. In this study, a fuzzy energy-efficient parallel BPMs scheduling problem (FEPBSP) with machine eligibility and sequence-dependent setup time (SDST) in fabric dyeing process is investigated, and a dynamical teaching-learning-based optimization algorithm (DTLBO) is proposed to simultaneously optimize the total agreement index, fuzzy makespan, and total fuzzy energy consumption. In DTLBO, multiple classes are constructed by non-dominated sorting. Dynamical class evolution is designed, which incorporates diversified search among students and adaptive self-learning of teachers. The former is implemented using various combinations of the teacher phase and the learner phase, and the latter is achieved through teacher quality and an adaptive threshold. Additionally, a reinforcement local search based on neighborhood structure dynamic selection is also applied. Extensive experiments are conducted, and the computational results demonstrated that the new strategies of DTLBO are effective, and it is highly competitive in solving the considered problem.},
  archive      = {J_ASOC},
  author       = {Jing Wang and Debiao Li and Hongtao Tang and Xixing Li and Deming Lei},
  doi          = {10.1016/j.asoc.2024.112413},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamical teaching-learning-based optimization algorithm for fuzzy energy-efficient parallel batch processing machines scheduling in fabric dyeing process},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective ensemble broad learning system based
on structural diversity. <em>ASOC</em>, <em>167</em>, 112412. (<a
href="https://doi.org/10.1016/j.asoc.2024.112412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble Broad Learning Systems (ENBLS) have received considerable attention from researchers and experienced rapid development due to its outstanding performance in handling complex data. A study has revealed that when ENBLS is trained with multiple independent storages, the issue of memory consumption is inevitable. To address this issue, this paper proposes an Ensemble Broad Learning Systems based on structural diversity (EBLSSD). This method effectively reduces the model memory consumption while maintaining superior performance. In EBLSSD, the scaling vectors are trained by constraining the scaling vectors of the current model and those of other existing models. These vectors are used to search for hidden paths of subnetworks within the network, extracting different network structures for integration. By preserving the scaling vectors during the storage process, memory pressure is reduced. EBLSSD introduces diversity from the perspective of network structure, which reduces cooperation between nodes and the risk of model overfitting, thereby enhancing model performance. The feasibility and effectiveness of the proposed algorithm is validated by experimental results on public datasets.},
  archive      = {J_ASOC},
  author       = {Fei Chu and Jianwen Wang and Yiwan Cao and Shuai Li},
  doi          = {10.1016/j.asoc.2024.112412},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112412},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient and effective ensemble broad learning system based on structural diversity},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A closed-loop dual-channel supply chain network for leather
products: An integrated simulation optimization clustering approach.
<em>ASOC</em>, <em>167</em>, 112411. (<a
href="https://doi.org/10.1016/j.asoc.2024.112411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leather industry is pivotal to the economic development of certain countries; however, it also poses significant environmental challenges. Thus, this study aims to design a closed-loop leather supply chain network that incorporates waste recycling and product regeneration processes. In light of the growing prevalence of both in-store and online shopping, the concept of a dual-channel supply chain is examined. Furthermore, as advertising through traditional and social media continues to expand, this research uniquely investigates the impact of two advertising modalities—SMS and social networks—alongside the design and layout of retail environments, considering their effects on product sales within the supply chain context. Additionally, outlet stores are integrated into the model to facilitate the sale of products that may become outdated over time, thereby enhancing the model&#39;s real-world applicability. To achieve this, a multi-product, multi-period mathematical model is developed to minimize costs and maximize customer responsiveness in both forward and reverse supply chain flows. The K-means clustering method is employed to categorize customers based on recency, frequency, and monetary (RFM) features. Through simulation, demand parameters are derived for each customer cluster across various scenarios. Moreover, the Aghezzaf approach is utilized to address scenario-based uncertainties within the model, and the proposed multi-objective model is solved using the Augmented Epsilon constraint method. A real-world case study is conducted to apply the proposed model, and critical parameters are examined through sensitivity analysis to validate its effectiveness. The findings suggest that incorporating return flows into the supply chain can significantly enhance both producer profitability and customer responsiveness regarding leather products.},
  archive      = {J_ASOC},
  author       = {Beheshteh Moghadaspoor and Mohammad Sheikhalishahi and Ali Bozorgi-Amiri},
  doi          = {10.1016/j.asoc.2024.112411},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112411},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A closed-loop dual-channel supply chain network for leather products: An integrated simulation optimization clustering approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image inpainting for periodic discrete density defects via
frequency analysis and an adaptive transformer-GAN network.
<em>ASOC</em>, <em>167</em>, 112410. (<a
href="https://doi.org/10.1016/j.asoc.2024.112410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting based on deep learning has made significant progress in addressing regular and coherent irregular defects. However, little has been studied on periodic discrete density (PDD) defects that are prevalent in microscopic images obtained by advanced instruments like transmission electron microscopes (TEM) and scanning tunneling microscopes (STM). The PDD defects usually introduce low-frequency noise in the fast Fourier transform (FFT) images, preventing the extraction of useful information particularly in the low-frequency regions. Despite its significant impact, no method has been reported to date to efficiently remove the PDD-induced noise from the FFT of high-resolution microscopic images. In this study, we introduced a novel GAN-based two-stage network (FGTNet), a novel coarse-to-fine inpainting framework, which is built upon the architecture of Generative Adversarial Networks (GAN) and transformer blocks. By integrating the information from both frequency and spatial domains, contextual structures are preserved and high-frequency details are generated in our method. We also proposed an adaptive-window transformer block (A-LeWin) to enhance the spatial feature representation and to fully use the information around the defects. To validate our approach, we constructed a specialized microscopic image dataset with 2730 training samples and 105 testing samples. For comparison, we also extended the experiments to the public Describable Texture Dataset (DTD) and coherence defects that are often discussed in the field of image inpainting. The experiment results indicate that our method performs well on six pixel-level and perceptual-level metrics, and shows the best performance and visual effect of coherent texture.},
  archive      = {J_ASOC},
  author       = {Hui Ding and Yuhan Huang and Nianzhe Chen and Jiacheng Lu and Shaochun Li},
  doi          = {10.1016/j.asoc.2024.112410},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112410},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image inpainting for periodic discrete density defects via frequency analysis and an adaptive transformer-GAN network},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust chinese clinical named entity recognition with
information bottleneck and adversarial training. <em>ASOC</em>,
<em>167</em>, 112409. (<a
href="https://doi.org/10.1016/j.asoc.2024.112409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese Clinical Named Entity Recognition (CCNER) aims to extract entities with specific medical significance from Chinese clinical texts, which is an important part of medical data mining. Some existing CCNER models may assume perfect text data and design complex models to improve their accuracy. However, due to the complexity of Chinese clinical entity semantics and the professionalism of annotation, Chinese clinical texts are prone to contain irregular misrepresentations and sparse entity labeling. That would lead to noisy or incomplete text features extracted by CCNER, seriously threatening the robustness of recognition in real-world scenarios. To address these problems, we propose the Robust Chinese Clinical Named Entity Recognition model (RCCNER). RCCNER comprises three essential components: multifaceted text representation, robust feature extraction, and robust model training. For multifaceted text representation, the model enhances consistency and collaboration between feature representations by integrating word embedding, radical embedding, and dictionary embedding to help withstand textual noise. Then, guided by the information bottleneck and the Hilbert–Schmidt independence criterion, robust feature extraction compresses the dependency between text representation and extracted features, while enhancing the dependency between extracted features and labels, which consequently provides reliable text features for robust recognition. The robust model training aspect leverages adversarial training to diminish RCCNER’s sensitivity to noise disturbances and sparse entity labeling, thereby reinforcing its robustness in entity recognition. RCCNER collaboratively enhances the noise immunity through text representation, text feature extraction and model training. Several experiments on two popular public datasets validate the effectiveness and robustness of RCCNER.},
  archive      = {J_ASOC},
  author       = {Yunfei He and Zhiqiang Zhang and Jinlong Shen and Yuling Li and Yiwen Zhang and Weiping Ding and Fei Yang},
  doi          = {10.1016/j.asoc.2024.112409},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112409},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust chinese clinical named entity recognition with information bottleneck and adversarial training},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical attention for aspect extraction using LSTM in
fine-grained sentiment analysis and evaluation. <em>ASOC</em>,
<em>167</em>, 112408. (<a
href="https://doi.org/10.1016/j.asoc.2024.112408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the current digital world, aspect-based sentiment analysis (ABSA) from a review sentence is a big challenge. In recent works, the aspect term detection from the reviews of the discourse is performed independently, even for contextually interdependent reviews. Further, the baseline techniques could not correlate valid multi-word aspect extraction from different review sentences, which further degrades the aspect extraction performance. Therefore, this paper aims to improve the performance of the aspect term detection subtask using a supervised hierarchical attention-based model. A sentence alignment step is added as preprocessing before ABSA on multi-sentence reviews. This step removes deficient knowledge, captures aspect alignment from different review sentences, and properly relates the aspect to the context domain. Next, the context information is used to extract contextual aspects using word-vector representation. The input sentences and labeled data using sequence tagging are supplied to the proposed supervised hierarchical attention network. The proposed model can capture the semantics of words for different domains, enhancing the extraction of domain-specific relevant aspect terms. The experimental results shown in the SemEval-16 reviews present the effectiveness of the proposed attention-based approach for aspect term detection. The F-score is improved by approximately 3 % as compared to the recent hybrid unsupervised approach for both the laptop and restaurant domains when coreference resolution is not performed. On the other side, F-score is improved by approximately 4 % and 3 % as compared to the recent hybrid unsupervised approach for the laptop and restaurant domains, respectively, when coreference resolution is not performed. When the results of the proposed approach are compared with those of recent supervised approaches, the F-score of the proposed model is improved by 5 % and 9 % for laptops and restaurants, respectively, when coreference resolution is not performed. Whereas the F-score of the proposed model is improved by 9 % and 12 % for laptops and restaurants, respectively, when coreference resolution is performed before the aspect extraction subtask.},
  archive      = {J_ASOC},
  author       = {Ganpat Singh Chauhan and Akash Saxena and Ravi Nahta and Yogesh Kumar Meena},
  doi          = {10.1016/j.asoc.2024.112408},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112408},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical attention for aspect extraction using LSTM in fine-grained sentiment analysis and evaluation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic generative adversarial networks with KNN-transformers
for missing traffic data completion. <em>ASOC</em>, <em>167</em>,
112406. (<a href="https://doi.org/10.1016/j.asoc.2024.112406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the face of the huge amount of intelligent transportation data, it is necessary and important to collect and statistically process it. Due to adverse weather conditions, sensor malfunctions and other reasons, the collected data inevitably contains missing data. Aiming at the phenomenon of missing traffic data, we propose an interpolation method of missing traffic data based on Cyclic Generative Adversarial Networks with hybrid KNN-Transformer method (KT-CyclicGAN). This method effectively utilizes K-Nearest Neighbor as prior knowledge to guide network training, employs transformers to extract the spatiotemporal relationships present in traffic data, and reconstructs missing data. In GANs, it uses a multi weight sharing cyclic structure to thoroughly learn the spatiotemporal sequences in the traffic data, resulting in accurate imputed data. We assess the performance of the proposed model using Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared, and the Concordance Correlation Coefficient (CCC), while simulating three missing data scenarios on the PEMSD4 dataset. The experimental results show that the algorithm proposed in this paper can deal with various missing scenarios and missing rates more effectively than the other five algorithms. Even with a high missing rate of up to 90%, the data imputed by KT-CyclicGAN can still fit the real data quite well.},
  archive      = {J_ASOC},
  author       = {Lie Luo and Zouyang Fan and Yumin Chen and Xin Liu},
  doi          = {10.1016/j.asoc.2024.112406},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112406},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cyclic generative adversarial networks with KNN-transformers for missing traffic data completion},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy preserving verifiable federated learning scheme
using blockchain and homomorphic encryption. <em>ASOC</em>,
<em>167</em>, 112405. (<a
href="https://doi.org/10.1016/j.asoc.2024.112405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel Privacy-Preserving Verifiable Federated Learning (PPVFL) scheme that integrates blockchain technology and homomorphic encryption to address critical challenges in decentralized machine learning. The proposed scheme ensures data privacy, integrity, verifiability, robust security, and efficiency in collaborative learning environments, particularly in sensitive domains such as healthcare. By leveraging blockchain’s decentralized, immutable ledger and homomorphic encryption’s capability to perform computations on encrypted data, the model maintains the confidentiality of sensitive information throughout the learning process. The inclusion of Byzantine fault tolerance and Elliptic Curve Digital Signature Algorithm (ECDSA) further enhances the system’s security against malicious attacks and data tampering, while the optimization of computational processes ensures efficient model training and communication. The novelty of this work lies in the seamless integration of blockchain and homomorphic encryption within a federated learning framework, specifically tailored for post-quantum cryptography, a combination that has not been extensively explored in prior research. This research represents a significant advancement in secure and efficient federated learning, offering a promising solution for industries that prioritize data privacy, security, and trust in collaborative machine learning. The effectiveness, security, and efficiency of the PPVFL scheme were validated using the Glaucoma dataset. The proposed method outperformed baseline federated learning algorithms, achieving a Dice coefficient of 0.918 and a Hausdorff distance of 4.05 on Severe Glaucoma (SG) cases, compared to 0.905 and 5.27, respectively, with traditional FedAvg. Moreover, the integration of blockchain and homomorphic encryption ensured that data privacy was upheld without compromising model performance, while efficient computation and communication processes minimized latency and resource consumption. This study contributes a robust, privacy-preserving, secure, efficient, and verifiable federated learning framework that addresses the pressing need for secure and scalable data management in distributed machine learning environments.},
  archive      = {J_ASOC},
  author       = {Ganesh Kumar Mahato and Aiswaryya Banerjee and Swarnendu Kumar Chakraborty and Xiao-Zhi Gao},
  doi          = {10.1016/j.asoc.2024.112405},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112405},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privacy preserving verifiable federated learning scheme using blockchain and homomorphic encryption},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Harnessing text reviews through a multi-granularity
heterogeneous graph convolutional network for facilitating
multi-attribute crowd decision-making. <em>ASOC</em>, <em>167</em>,
112404. (<a href="https://doi.org/10.1016/j.asoc.2024.112404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd decision-making (CDM) has emerged as a powerful tool to harness the wisdom of the crowd for decision-making purposes. With social media and other online platforms developing rapidly, text reviews from Internet users have become a significant medium for expressing opinions, sparking a keen interest in utilizing large-scale textual comments as a source of collective knowledge. However, there is a significant barrier to the transformation from textual information into crowd decision-making outcomes, primarily because traditional numeric decision-making models are generally not equipped to directly process and analyze unstructured textual data. This study aims at addressing this issue. We innovate a multi-granularity heterogeneous graph convolutional network (MH-GCN) as a bridge between text reviews and the CDM process, creatively introducing a probabilistic linguistic term set (PLTS) to enhance the representation of uncertain linguistic information. Simultaneously, during the information integration phase, individual experience differences and candidate advantage discrepancies are fully considered. The efficacy of the model in terms of sentiment information extraction and CDM is demonstrated through a simulation and comparison analysis based on online text reviews.},
  archive      = {J_ASOC},
  author       = {Ming Liu and Liu Liu and Zeshui Xu and Xiaoxiong Zhang},
  doi          = {10.1016/j.asoc.2024.112404},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112404},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Harnessing text reviews through a multi-granularity heterogeneous graph convolutional network for facilitating multi-attribute crowd decision-making},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A customer-driven quality function deployment approach for
intelligent product design: A case of automatic-dishwasher.
<em>ASOC</em>, <em>167</em>, 112403. (<a
href="https://doi.org/10.1016/j.asoc.2024.112403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality Function Deployment (QFD) is one of the very effective customer-driven product development tools. With automatic dishwashers entering the Chinese market, manufacturers can use QFD to develop products that meet the unique Chinese customer requirements and explore the Chinese market. To help manufacturers maximize profits by developing automatic dishwashers to China’s market situation and competitive environment, this paper proposes an improved QFD model considering competitive learning and cost analysis based on market information analysis with Product Life Cycle (PLC) theory. On the one hand, the improved QFD model determines the life cycle stage of the product by analyzing the characteristics of the market. The improvement rate of customer requirements (CRs) determined by competitive learning in different stages has different influence on the importance weights of CRs, so that the new products developed based on design requirements (DRs) converted from CRs can better adapt to the current market environment. On the other hand, the cost strategy will also change according to different PLC stages. In cost analysis, different cost criteria weights will be determined in different life cycle stages to deal with product development risks in different market environments and help enterprises achieve better sustainable development. Finally, we take Chinese HE company’s automatic-dishwasher design as an example to show the specific application of the proposed model. The proposed model identifies the important DRs and their relative importance ranking for the product development. Comparison with existing QFD models shows the ranking obtained through the model is better integrated with market information, enabling the development of products that are better suited to the market, and helping companies to achieve sustainable development.},
  archive      = {J_ASOC},
  author       = {Decui Liang and Chenghao Ou and Zeshui Xu},
  doi          = {10.1016/j.asoc.2024.112403},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112403},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A customer-driven quality function deployment approach for intelligent product design: A case of automatic-dishwasher},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neutral network with physically consistent
and residual learning for excavator precision operation control.
<em>ASOC</em>, <em>167</em>, 112402. (<a
href="https://doi.org/10.1016/j.asoc.2024.112402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data-driven methodologies can establish accurate Inverse Dynamics Model (IDM) of the excavator thus improving control precisions. However, the inherent black-box nature of these models often results in overfitting to the dataset, leading to predictions that deviate from the constraints of physical system. Consequently, this can lead to controller failures, introducing unpredictable behavior that threatens operation precision. In addition, the uncertainty of the external disturbance poses great challenge to the precision of controller. This study presents a physics-informed neural network to build accurate IDM with physical consistency. The Rigid Body Dynamics (RBD) of the excavator are coupled within a Deep Lagrangian Network (DeLaN), while a Convolutional Neural Network (CNN) and a Long Short-Term Memory Network (LSTM) are employed to assimilate the residual nonlinear characteristics, such as hydraulic flexibilities and stick–slip friction. To the uncertainty of the external disturbance, the Prescribed Performance Inverse Dynamics Controller combination with the DeLaN-CNN-LSTM model (PPIDC-DCL) is constructed for precise control by constraining the control error within a finite region. The experimental results demonstrate that the model captures the underlying structure of the dynamic and builds the IDM with high accuracy and robustness. Moreover, the PPIDC-DCL controller effectively constrains the control error and realizes precision control. The proposed method has potential applications and provides novel insights for achieving precise operation control of excavators.},
  archive      = {J_ASOC},
  author       = {Chenlong Feng and Jixin Wang and Yuying Shen and Qi Wang and Yi Xiong and Xudong Zhang and Jiuchen Fan},
  doi          = {10.1016/j.asoc.2024.112402},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Physics-informed neutral network with physically consistent and residual learning for excavator precision operation control},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSKINet: A multimodal network model integrating conceptual
semantic knowledge injection for relation extraction of chinese
corporate reports. <em>ASOC</em>, <em>167</em>, 112401. (<a
href="https://doi.org/10.1016/j.asoc.2024.112401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing the associations among entities in corporate reports accurately is crucial for market regulation and policy development. Nevertheless, confronted with massive corporate information, the traditional manual screening approach is cumbersome, struggling to match the demand. Consequently, we propose a multimodal network model incorporating conceptual semantic knowledge injection, CSKINet, for accurately extracting relations from Chinese corporate reports. The essential highlights in the design of the CSKINet model are the following: (1) Integrate the conceptual descriptions of corporations from external resources to construct the semantic knowledge repository of corporate concepts, which provides a solid semantic foundation for the model. (2) Multimodal features are extracted from the documents by various means and corporate conceptual knowledge is integrated into the model representation to enhance the representation capability of the model. (3) The multimodal self-attention mechanism that captures cross-modal associations and the biaffine classifier with Taylor polynomial loss function that optimizes training iterations further improve the learning efficiency and prediction accuracy. The results on the real corporate report dataset show that our proposed model can more accurately extract the relations from Chinese corporate reports compared to other baseline models, where the F1 score reaches 85.76%.},
  archive      = {J_ASOC},
  author       = {Shun Luo and Juan Yu and Yunjiang Xi},
  doi          = {10.1016/j.asoc.2024.112401},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CSKINet: A multimodal network model integrating conceptual semantic knowledge injection for relation extraction of chinese corporate reports},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-branch network with hypergraph feature augmentation and
adaptive logits adjustment for long-tailed visual recognition.
<em>ASOC</em>, <em>167</em>, 112400. (<a
href="https://doi.org/10.1016/j.asoc.2024.112400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The long-tailed distribution presents issues of data scarcity and significant class imbalance, which causes the model to increase the prediction tendency for the head classes and reduce performance on tail classes. The decoupling learning strategy separates the learning process into representation learning and classifier learning to enhance model performance. However, this approach does not delve into the complex semantic relationships within images and overlooks the bias of the model confidence. To address this pervasive issue, we propose a novel algorithm called Hypergraph Feature Augmentation and Adaptive Logits Adjustment for Long-tailed Visual Recognition (HALR) based on a decoupling learning framework. For the representation learning task, we extract hypergraph features from the mixed input samples to capture the global spatial contextual semantic information of the images. For the classifier learning task, we propose an adaptive logits adjustment function that automatically corrects prediction score biases, thereby yielding robust decision boundaries. Extensive experiments on widely-used benchmark datasets, including CIFAR10/100-LT, ImageNet-LT, and iNaturalist 2018, validate the efficacy of HALR. For example, the test accuracy of HALR on CIFAR10-LT with an imbalance factor of 100 is 88.25 %, which is a 17.85 % improvement over the baseline model accuracy and 0.68 % higher than the state-of-the-art model. The results demonstrate that HALR effectively optimizes representation and classifier learning tasks for long-tailed learning, mitigating the negative impact of class imbalance and data scarcity.},
  archive      = {J_ASOC},
  author       = {Jia-yi Han and Jian-wei Liu and Jing-dong Xu},
  doi          = {10.1016/j.asoc.2024.112400},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112400},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-branch network with hypergraph feature augmentation and adaptive logits adjustment for long-tailed visual recognition},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical image segmentation network based on feature
filtering with low number of parameters. <em>ASOC</em>, <em>167</em>,
112399. (<a href="https://doi.org/10.1016/j.asoc.2024.112399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the medical image segmentation method based on hybrid convolutional neural network (CNN) and Vision Transformer (ViT) has made great progress, but it still faces the challenge of unbalanced global and local modeling, and excessive parameters. In addition, ViT repeatedly uses the whole feature map to model the global information, thus generating irrelevant and weakly related information, which will weaken the performance of the model when facing small datasets and segmentation targets. Therefore, this paper proposes a feature screening network based on similarity, named Screening Feature (SF)-MixedNet. Specifically, this paper first proposes a new feature extractor, namely Correlation based Similarity Transformer (CSimFormer). On the basis of parameter pruning, it uses the Screening Feature Multi-head Self Attention (SF-MSA) to establish the remote dependency, and calculates the similarity between local elements through the Location-Sensitive Mechanism (LsM) to obtain the weight matrix. Then, the correlation between regional elements is mined by Region Matching and Selection (RMS) mechanism, and the obtained information is filtered according to the corresponding rules to reduce the side effects of redundant information. Extensive experiments on Synapse dataset, ACDC dataset and SegPC-2021 dataset show that the segmentation accuracy reaches 83.51%, 92.20% and 81.27% respectively. Especially in the Synapse dataset, our method is 6.31% higher than the baseline. The method proposed in this paper effectively improves the segmentation accuracy, provides more detailed information for medical diagnosis and promotes the development of medical artificial intelligence technology.},
  archive      = {J_ASOC},
  author       = {Zitong Ren and Zhiqing Guo and Liejun Wang and Lianghui Xu and Chao Liu},
  doi          = {10.1016/j.asoc.2024.112399},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112399},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Medical image segmentation network based on feature filtering with low number of parameters},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-elastic time series fuzzy clustering for efficient
analysis of industrial data sets. <em>ASOC</em>, <em>167</em>, 112398.
(<a href="https://doi.org/10.1016/j.asoc.2024.112398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In line with the green transition and digitalization trends, there is an increasing need for effective performance and health monitoring of production machines. Machine operation footprints resonate in quantities such as supply current and pneumatic line pressure, providing valuable insights when analyzed. Examining these low-level signals allows for monitoring and identification of changes in machine operation over time. A critical step in analyzing machine data is clustering, requiring an effective method for calculating time series cluster centroids. This paper introduces Error in Aligned series (ERAL), a novel method that generates a time series centroid that distills the fundamental shape of the underlying datasets. ERAL employs a fuzzy clustering-inspired iterative process for temporal alignment and averaging, avoiding the pathological artifacts often introduced by popular time-warping methods. Our analysis shows that existing methods can create artificial spikes and plateaus, which ERAL mitigates; it remains faithful to the original data shape. Additionally, ERAL offers improvements in computational efficiency and prototype quality. We evaluate the method against Dynamic Time Warping (DTW)-based methods across various datasets, and apply it to a time series clustering task using a real-world industrial dataset. In combination with a fuzzy clustering algorithm, ERAL generates visually convincing clusters. By leveraging fuzzy membership concepts, it achieves robust and adaptable clustering outcomes that reflect real-world data complexity and ambiguity.},
  archive      = {J_ASOC},
  author       = {Žiga Stržinar and Boštjan Pregelj and Igor Škrjanc},
  doi          = {10.1016/j.asoc.2024.112398},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112398},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-elastic time series fuzzy clustering for efficient analysis of industrial data sets},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A non-uniform multi-point incremental kriging facing with
discrete response surfaces. <em>ASOC</em>, <em>167</em>, 112397. (<a
href="https://doi.org/10.1016/j.asoc.2024.112397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional Kriging model is widely employed for continuous response surface fitting. Nonetheless, when dealing with discrete response surfaces with non-uniform sample points, Kriging exhibits limited construction efficiency and lacks accuracy. In this regard, we analyze the influence that non-uniform training data and discrete response values bring to the Kriging model both theoretically and empirically. On this basis, a non-uniform multi-point incremental Kriging facing with discrete response surfaces (PRIK) is proposed, aiming at striking a balance between model accuracy and construction efficiency. PRIK employs the multiple sample points prescreening strategy to select training data from the sample pool that embodies both uncertainty and diversity. Subsequently, ridge regression Kriging is introduced to prevent the potential accuracy reductions stemming from local densely distributed sample points for discrete response surfaces. Lastly, a multi-point incremental updating method and control criterion are introduced to reduce the construction complexity of the ridge regression Kriging. Starting from benchmarks commonly used for surrogate model testing, we developed 10 test cases with discrete response values characterized by diverse input space dimensional, ranges, and landscape features, along with five evaluation metrics. We have performed extensive experiments on various numbers of sample points and parameter values. Experiment results illustrate that the prescreening strategy and the incremental updating method can significantly reduce Kriging’s training time, calling time, and the number of calls to the original system. Ridge regression Kriging notably enhances Kriging’s accuracy. By applying PRIK to a decision boundary search problem that needs to fit a discrete response surface with non-uniform sample points, it is found that the accuracy of the model constructed by PRIK is greatly improved compared with the original Kriging.},
  archive      = {J_ASOC},
  author       = {Shiqi Wang and Hui Lu and Yuxuan Zhang},
  doi          = {10.1016/j.asoc.2024.112397},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112397},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A non-uniform multi-point incremental kriging facing with discrete response surfaces},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced trust classification in social networks using a
triple generative adversarial network-assisted capsule network enhanced
by gannet optimization. <em>ASOC</em>, <em>167</em>, 112396. (<a
href="https://doi.org/10.1016/j.asoc.2024.112396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past ten years, social networks (SN) have evolved into the primary infrastructure for people&#39;s everyday activities. Trust classification in social networks involves evaluating the trustworthiness of users or the information they share. Traditional trust classification methods often rely on explicit features, such as user ratings, reviews, and social ties. These methods utilize rule-based to assign trust scores for users or entities. However, they face challenges in capturing the exact nature of trust. In this manuscript, Advanced Trust Classification in Social Networks using a Triple Generative Adversarial Network-Assisted Capsule Network Enhanced by Gannet Optimization (Trust-TripleGAN-CapsNet-GOA) is proposed. In this manuscript, the feature vector has computed to every social network users pair after the raw data from the Sentiment140 dataset, is analysed. The membership of trust is then ranked according to five-class classifications by incorporating Tanimoto Trust Similarity coefficient. Then, Triple Generative Adversarial Network-Assisted Capsule Network (TripleGAN-CapsNet) is used to categorise the trust values of users. Finally, the weight parameters of TripleGAN-CapsNet is optimized by the Gannet Optimization Algorithm (GOA) to enhance the accuracy of the trust behaviour in SN. The proposed Trust-TripleGAN-CapsNet-GOA method attains 22.94 %, 32.36 % and 21.96 % higher accuracy, 30.27 %, 19.46 % and 12.39 %, higher precision when analyzed with the existing models, such as a method for trust mirroring assessment under social networks parameters with fuzzy system (Trust-SNP-FS), deep matrix factorization in social networks for trust-aware recommendation (Trust-DMF-SN) and towards time-aware context-aware deep trust prediction on the online social networks (TACADTrust-SN). The simulation outcomes exhibit that the proposed method attains 99 % accuracy.},
  archive      = {J_ASOC},
  author       = {R. Gnanakumari and P. Vijayalakshmi},
  doi          = {10.1016/j.asoc.2024.112396},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112396},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advanced trust classification in social networks using a triple generative adversarial network-assisted capsule network enhanced by gannet optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CoaddNet: Enhancing signal-to-noise ratio in single-shot
images using convolutional neural networks with coadded image effect.
<em>ASOC</em>, <em>167</em>, 112395. (<a
href="https://doi.org/10.1016/j.asoc.2024.112395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise in astronomical images significantly impacts observations and analyses. Traditional denoising methods, such as increasing exposure time and image stacking, are limited when dealing with single-shot images or studying rapidly changing astronomical objects. To address this, we developed a novel deep-learning denoising model, CoaddNet, designed to improve the image quality of single-shot images and enhance the detection of faint sources. To train and validate the model, we constructed a dataset containing high and low signal-to-noise ratio (SNR) images, comprising coadded and single-shot types. CoaddNet combines the efficiency of convolutional operations with the advantages of the Transformer architecture, enhancing spatial feature extraction through a multi-branch structure and reparameterization techniques. Performance evaluation shows that CoaddNet surpasses the baseline model, NAFNet, by increasing the Peak Signal-to-Noise Ratio (PSNR) by 0.03 dB and the Structural Similarity Index (SSIM) by 0.005 while also improving throughput by 35.18%. The model significantly improves the SNR of single-shot images, with an average increase of 22.8, surpassing the noise reduction achieved by stacking 70-90 images. By boosting the SNR, CoaddNet significantly enhances the detection of faint sources, enabling SExtractor to detect an additional 22.88% of faint sources. Meanwhile, CoaddNet reduced the Mean Absolute Percentage Error (MAPE) of flux measurements for detected sources by at least 27.74%.},
  archive      = {J_ASOC},
  author       = {Zhi-Ren Pan and Bo Qiu and A-Li Luo and Qi Li and Zhi-Jun Liu and Fu-Ji Ren},
  doi          = {10.1016/j.asoc.2024.112395},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112395},
  shortjournal = {Appl. Soft. Comput.},
  title        = {CoaddNet: Enhancing signal-to-noise ratio in single-shot images using convolutional neural networks with coadded image effect},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable feature alignment with attention-based data
augmentation for handling data issue in ancient documents.
<em>ASOC</em>, <em>167</em>, 112394. (<a
href="https://doi.org/10.1016/j.asoc.2024.112394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing ancient cursive handwritten characters presents unique challenges due to the diversity of writing styles and significant class imbalances, where some characters have disproportionately more samples than others. This imbalance leads to higher misclassification rates for minority classes compared to majority classes. To address these challenges, we propose a novel framework that integrates learnable channel and spatial attention modules to effectively align features between source and target domains for better representation. Our approach incorporates a learnable sequential feature alignment process that dynamically adjusts to the specific characteristics of the data, enhancing the transfer of knowledge across domains. Furthermore, we introduce an attention-based augmentation module to amplify the influence of tail classes. This module leverages class activation maps to identify and augment discriminative features, ensuring the model focuses on the most semantically rich regions, particularly for minority classes. As a result, it aligns the weight norms of minority classes with those of majority classes, effectively mitigating the limitations posed by imbalanced class distributions. This approach effectively mitigates the constraints posed by imbalanced character distributions in ancient handwritten documents. The proposed method increases the accuracy for the CCR, Hanja, Nancho, and Kuzushiji datasets.},
  archive      = {J_ASOC},
  author       = {Amin Jalali and Sangbeom Lee and Minho Lee},
  doi          = {10.1016/j.asoc.2024.112394},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learnable feature alignment with attention-based data augmentation for handling data issue in ancient documents},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting the highest and lowest stock price indices: A
combined BiLSTM-SAM-TCN deep learning model based on re-decomposition.
<em>ASOC</em>, <em>167</em>, 112393. (<a
href="https://doi.org/10.1016/j.asoc.2024.112393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of stock price indices is crucial for market participants to obtain valuable information and mitigate risks. For more accurate forecasting of stock price indices, this study proposes a deep learning combined prediction model based on re-decomposition, utilizing the daily highest price series and the daily lowest price series of the Standard &amp; Poor&#39;s 500 Index (S&amp;P 500) and the Shanghai Stock Exchange Composite Index (SSEC) as experimental data. This model involves several key steps. Initially, an improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN) is used to decompose the stock&#39;s highest price sequence or lowest price sequence into multiple subsequences. Next, the highest frequency subsequence is re-decomposed using variational mode decomposition (VMD) optimized by particle swarm optimization (PSO). Then, bidirectional long short-term memory (BiLSTM), self-attention mechanism (SAM) and temporal convolutional network (TCN) are combined to construct BiLSTM-SAM-TCN for predicting each subsequence independently. Finally, the predicted values of the subsequences are linearly integrated to get the prediction results of the highest price sequence or the lowest price sequence of the stock. The empirical analyses demonstrate that the proposed ICEEMDAN-PSO-VMD-BiLSTM-SAM-TCN model exhibits excellent forecasting performance in both developed and developing country stock markets, surpassing other models. The modified Diebold-Mariano (MDM) test also statistically confirms the superiority of the proposed model. Furthermore, the model is also verified to have good robustness and generalization ability by changing the time scale of the original stock price series and then forecasting it, as well as forecasting the stock price under extreme market conditions. Overall, this study presents a reliable method for accurately predicting the highest and lowest stock price indices with the potential for practical application in diverse stock markets.},
  archive      = {J_ASOC},
  author       = {Hao Gong and Haiyang Xing},
  doi          = {10.1016/j.asoc.2024.112393},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting the highest and lowest stock price indices: A combined BiLSTM-SAM-TCN deep learning model based on re-decomposition},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective surrogate-assisted rank method for evolutionary
neural architecture search. <em>ASOC</em>, <em>167</em>, 112392. (<a
href="https://doi.org/10.1016/j.asoc.2024.112392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary neural architecture search (ENAS) is able to automatically design high-performed architectures of deep neural networks (DNNs) for specific tasks. In recent years, surrogate models have gained significant traction because they can estimate the performance of neural architectures, avoiding excessive computational costs for training. However, most existing surrogate models primarily predict the performance of architectures directly or predict pairwise comparison relationships, which makes it challenging to obtain the rank of a group of architectures when training samples are limited. To address this problem, we propose TCMR-ENAS, an effective triple-competition model-assisted rank method for ENAS. TCMR-ENAS employs a novel triple-competition surrogate model combined with a score-based fitness evaluation method to predict group performance rank. Moreover, a progressive online learning method is proposed to enhance the predictive performance of the triple-competition surrogate model in the framework of modified genetic search. To validate the effectiveness of TCMR-ENAS, we conducted a series of experiments on NAS-Bench-101, NAS-Bench-201, NATS-Bench and NAS-Bench-301, respectively. Experimental results show that TCMR-ENAS can achieve better performance with lower computational resources. The accuracies of searched architectures achieve the best results compared with those of the state-of-the-art methods with limited training samples. In addition, the factors that may influence the effectiveness of TCMR-ENAS are explored in the ablation studies.},
  archive      = {J_ASOC},
  author       = {Yu Xue and Anjing Zhu},
  doi          = {10.1016/j.asoc.2024.112392},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective surrogate-assisted rank method for evolutionary neural architecture search},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Document-level multiple relations extraction method via
evidence guidance and relation correlation. <em>ASOC</em>, <em>167</em>,
112391. (<a href="https://doi.org/10.1016/j.asoc.2024.112391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document-level Relation Extraction (DocRE) aims to extract semantic relations between entity pairs, spanning multiple sentences, paragraphs or even the entire document. These relations can often be predicted by partial sentences within the document, the evidence sentence. However, the relation derived only from sentence information is incomplete, because it ignores the case of multiple relations between entity pairs. Therefore, how to select effective evidence sentences and how to predict multiple relations more accurately have become challenges for the existing DocRE models. In response to these challenges, we introduce Reinforcement Learning (RL) to select more effective evidence sentences, while using heuristic rules to narrow down the search space of RL. Secondly, we utilize GAT to acquire the features of co-occurrence relations, which can greatly improve multiple relations prediction performance. Moreover, the combination of the features of co-occurrence relations and the evidence sentence information enables our method to achieve both high effectiveness and precision. The experimental results show that, compared with other advanced methods, our method achieves an F 1 score of 66.56 and the E v i F 1 score of 56.69, which attains the state-of-the-art performance on public datasets.},
  archive      = {J_ASOC},
  author       = {Hao Yang and Qiming Fu and You Lu and Yunzhe Wang and Lanhui Liu and Jianping Chen},
  doi          = {10.1016/j.asoc.2024.112391},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Document-level multiple relations extraction method via evidence guidance and relation correlation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Personalized blood pressure estimation using multiview
fusion information of wearable physiological signals and transfer
learning. <em>ASOC</em>, <em>167</em>, 112390. (<a
href="https://doi.org/10.1016/j.asoc.2024.112390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous blood pressure (BP) monitoring is crucial for individual health management, yet the significant inter-individual variations among patients pose challenges to achieving precision medicine. In response to this issue, we propose a parallel cross-hybrid architecture that integrates a convolutional neural network backbone and a Mix-Transformer backbone. This model, grounded in multi-view physiological signals and personalized fine-tuning strategies, aims to estimate BP, facilitating the capture of physiological information across diverse receptive fields and enhancing network expressive capabilities. Our proposed architecture exhibits superior performance in estimating systolic blood pressure and diastolic blood pressure, with average absolute errors of 3.94 mmHg and 2.24 mmHg, respectively. These results surpass existing baseline models and align with the standards set by the British Hypertension Society, the Association for the Advancement of Medical Instrumentation, and the Institute of Electrical and Electronics Engineers for BP measurement. Additionally, this study explores a personalized model fine-tuning strategy by adjusting specific layers and incorporating individual information, presenting an optimal solution. The model&#39;s generalization ability is validated through transfer learning across databases (public and self-made). To enhance the proposed architecture&#39;s usability in wearable devices, this study employs a knowledge distillation strategy for model lightweighting, with preliminary application in our designed real-time BP estimation system. This study provides an efficient and accurate solution for personalized BP estimation, exhibiting broad potential applications.},
  archive      = {J_ASOC},
  author       = {Jian Liu and Shuaicong Hu and Yanan Wang and Wei Xiang and Qihan Hu and Cuiwei Yang},
  doi          = {10.1016/j.asoc.2024.112390},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112390},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Personalized blood pressure estimation using multiview fusion information of wearable physiological signals and transfer learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust approximation of the conditional mean for
applications of machine learning. <em>ASOC</em>, <em>167</em>, 112389.
(<a href="https://doi.org/10.1016/j.asoc.2024.112389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning approaches are increasingly used in a range of applications. They are shown to produce low conventional errors but in many real applications fail to model the underlying input–output relationships. This is because the error measures used only predict the conditional mean under some restrictive assumptions, often not met by the data we extract from applications. However, new approaches to Machine Learning, for example using Evolutionary Computation, allow a range of alternative error measures to be used. This paper explores the use of the Fit to Median Error measure in machine learning regression automation, using evolutionary computation in order to improve the approximation of the ground truth. When used alongside conventional error measures it improves the robustness of the learnt input–output relationships to the conditional median. It is compared to traditional regularisers to illustrate that the use of the Fit to Median Error produces regression neural networks which model more consistent input–output relationships. The problem considered is ship power prediction using a fuel-saving air lubrication system, which is highly stochastic in nature. The networks optimised for their Fit to Median Error are shown to approximate the ground truth more consistently, without sacrificing conventional Minkowski-r error values.},
  archive      = {J_ASOC},
  author       = {Amy Parkes and Josef Camilleri and Dominic Hudson and Adam Sobey},
  doi          = {10.1016/j.asoc.2024.112389},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112389},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust approximation of the conditional mean for applications of machine learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust control of uncertain asymmetric hysteretic nonlinear
systems with adaptive neural network disturbance observer.
<em>ASOC</em>, <em>167</em>, 112387. (<a
href="https://doi.org/10.1016/j.asoc.2024.112387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a novel command-filtered adaptive control scheme is proposed for uncertain asymmetric hysteretic nonlinear systems with unknown external disturbances, where the hysteresis nonlinearities are described by an asymmetric Bouc–Wen model. Firstly, the hysteresis model uncertainties are considered, a robust hysteresis state observer is constructed to compensate the asymmetric hysteresis nonlinearity. Secondly, both tracking errors and prediction errors are employed to develop the adaptive neural networks (NNs) for approximating unknown nonlinear functions of the system, where the acquisition of the prediction error avoids the need to differentiate the state measurements. Then, a nonlinear disturbance observer with the filtered control input and system states is constructed to estimate a total disturbance resulting from both the NN approximation errors and the external disturbances. With the help of filtering differentiators, a command-filtered backstepping control law is designed by using the adaptive NN approximators, the disturbance observers, and the hysteretic compensator. The effects of the filtering errors, the disturbance estimation errors, and the hysteresis compensation error on the closed-loop stability are rigorously analyzed. Finally, the proposed control algorithm is applied to a piezoelectric micro-displacement servo system, the real-time experimental results indicate that the relative average error and the relative maximal error of the sinusoidal trajectory tracking are 0.04% and 0.06%, respectively. Compared with the existing adaptive robust control algorithm, a significant improvement on the tracking accuracy is achieved.},
  archive      = {J_ASOC},
  author       = {Yangming Zhang and Qi Yao and Biao Luo and Ning Chen},
  doi          = {10.1016/j.asoc.2024.112387},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112387},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust control of uncertain asymmetric hysteretic nonlinear systems with adaptive neural network disturbance observer},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving policy training for autonomous driving through
randomized ensembled double q-learning with transformer encoder feature
evaluation. <em>ASOC</em>, <em>167</em>, 112386. (<a
href="https://doi.org/10.1016/j.asoc.2024.112386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the burgeoning field of autonomous driving, reinforcement learning (RL) has gained prominence for its adaptability and intelligent decision-making. However, conventional RL methods face challenges in efficiently extracting relevant features from high-dimensional inputs and maximizing the use of environment-agent interaction data. To surmount these obstacles, this paper introduces a novel RL-based approach that integrates randomized ensembled double Q-Learning (REDQ) with a Transformer encoder. The Transformer encoder’s attention mechanism is utilized to dynamically evaluate features according to their relevance in different driving scenarios. Simultaneously, the implementation of REDQ, characterized by a high update-to-data (UTD) ratio, enhances the utilization of interaction data during policy training. Especially, the incorporation of ensemble strategy and in-target minimization in REDQ significantly improves training stability, especially under high UTD conditions. Ablation studies indicate that the Transformer encoder exhibits significantly enhanced feature extraction capabilities compared to conventional network architectures, resulting in a 13.6% to 21.4% increase in success rate for the MetaDrive autonomous driving task. Additionally, when compared to standard RL methodologies, the proposed approach demonstrates a faster rate of reward acquisition and achieves a 67.5% to 69% improvement in success rate.},
  archive      = {J_ASOC},
  author       = {Jie Fan and Xudong Zhang and Yuan Zou and Yuanyuan Li and Yingqun Liu and Wenjing Sun},
  doi          = {10.1016/j.asoc.2024.112386},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112386},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving policy training for autonomous driving through randomized ensembled double Q-learning with transformer encoder feature evaluation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dissipative structure theory based emotion updating method
applied to multi-stage emergency decision making. <em>ASOC</em>,
<em>167</em>, 112385. (<a
href="https://doi.org/10.1016/j.asoc.2024.112385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions of decision makers (DMs) play an important role in decision-making and have been widely concerned by scholars, but few of them have focused on the dynamic changes in emotions caused by emergency risk situations and their impacts on emergency decision making (EDM). Therefore, a dissipative structure theory (DST) based emotion updating method of DMs is proposed and applied to multi-stage emergency decision making (MEDM). First, an analysis of the openness, nonlinear, and dynamic evolution of emergency risk systems demonstrates their suitability for application within the DST framework, based on which the research problem and its solution procedure are then described. Second, a DST-based model is developed for judging the emergency risk situation at different time, utilizing calculation of fuzzy entropy of decision information and probabilities of scenarios being controlled by alternatives. Then a probability distribution for emotion updating is defined, with accompany propositions demonstrating its capacity to accurately capture the characteristics of emotion evolving with the risk situation. Finally, the proposed emotion updating method for DMs is applied to MEDM, with a case study and the comparison with other approaches carried out to demonstrate the efficacy of the proposed method.},
  archive      = {J_ASOC},
  author       = {Huifang Nie and Zhiying Wang and Hongli Zhao and Zhipeng Chang},
  doi          = {10.1016/j.asoc.2024.112385},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112385},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dissipative structure theory based emotion updating method applied to multi-stage emergency decision making},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving kernel-based fuzzy system with nonlinear
consequences. <em>ASOC</em>, <em>167</em>, 112384. (<a
href="https://doi.org/10.1016/j.asoc.2024.112384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fuzzy system equipped with nonlinear consequences poses significant nonlinear generalization capabilities. Few works are presented on evolving fuzzy system with nonlinear consequences. This paper presents an evolving kernel-based fuzzy system (EKFS) with nonlinear consequences considering the powerful modeling ability of nonlinear consequences and self-evolving learning ability. The major novelties of our work are: (1) employing the measurement of kernel similarity and approximation error to obtain an appropriate structure with the strategies of rule recruiting and rule shrinking during the adaptive learning phase instead of structure predefining, (2) designing the rule shrinking strategy which is indeed a dimensionality reduction technology for the kernel vector to calculate an equivalent low dimensional expression without losing approximation accuracy, (3) achieving the system structural lightweight and online performance improvement via the structure evolution throughout the approximation process. Through the real prediction problems drawn from the Time Series Data Library and Typhoon tracking problem, the proposed EKFS demonstrate better prediction performance in terms of Nonlinear Auto-Regressive with eXogenous inputs model compared with the existing state-of-art methods.},
  archive      = {J_ASOC},
  author       = {Zhao-Xu Yang and Hai-Jun Rong},
  doi          = {10.1016/j.asoc.2024.112384},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112384},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving kernel-based fuzzy system with nonlinear consequences},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-branching feature interaction representation learning for
multivariate time series. <em>ASOC</em>, <em>167</em>, 112383. (<a
href="https://doi.org/10.1016/j.asoc.2024.112383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Representational learning of time series plays a crucial role in various fields. However, existing time-series models do not perform well in representation learning. These models usually focus only on the relationship between variables at the same timestamp or only consider the change of individual variables in time, while failing to effectively integrate the two, which limits their ability to capture complex time dependencies and multivariate interactions. We propose a Bi -Branching F eature I nteraction Representation Learning for Multivariate Time Series (Bi-FI) to address these issues. Specifically, we elaborated a frequency domain analysis branch to address the complex associations between variables that are difficult to visualize in the time domain. In addition, to eliminate the time lag effect, another branch employs the mechanism of variable tokenization for attention to learning intra-variable representations. Ultimately, we interactively fuse the features learned from the two branches to obtain a more comprehensive time series representation. Bi-FI performs well in three time series analysis tasks: long sequence prediction, classification, and anomaly detection. Our code and dataset will be available at https://github.com/wwy8/Bi_FI .},
  archive      = {J_ASOC},
  author       = {Wenyan Wang and Enguang Zuo and Ruiting Wang and Jie Zhong and Chen Chen and Cheng Chen and Xiaoyi Lv},
  doi          = {10.1016/j.asoc.2024.112383},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112383},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-branching feature interaction representation learning for multivariate time series},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gesture recognition framework for upper-limb prosthetics
using entropy features from electromyographic signals and a gaussian
kernel SVM classifier. <em>ASOC</em>, <em>167</em>, 112382. (<a
href="https://doi.org/10.1016/j.asoc.2024.112382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper puts forward a novel entropy features based multi-class SVM classifier framework to predict the limb movement of the transradial amputees from the surface electromyography (sEMG) signals. The major challenges with the sEMG signal are nonlinear and non-stationary characteristics and susceptibility to noise. Consequently, a robust and an effective feature extraction framework which is invariant to force level variations is central in sEMG based prosthesis control. To address the aforementioned challenges, this study leverages the potential of variational mode decomposition (VMD) technique to identify the prominent frequency modes of the sEMG signals, and performs the spectral evaluation of the decomposed sEMG modes to identify the dominant ones to extract the entropy features. Subsequently, we evaluate the efficacy of four nonlinear optimal feature selection techniques and identify the prominent entropy features to train the multi-class SVM model that can predict the gestures. Specifically, to handle the nonlinearly separable input data, this study implements a kernelization named a radial basis function (RBF), which has good generalization and noise tolerance features. The efficacy of the proposed framework is tested using the publicly available datasets that contain gestures from transradial and congenital amputees for functional gestures. Experimental results obtained for various gestures with dynamic force levels underscore that the proposed framework is highly robust against the force level variations and can achieve a classification accuracy of 99.07%.},
  archive      = {J_ASOC},
  author       = {Prabhavathy T. and Vinodh Kumar Elumalai and Balaji E.},
  doi          = {10.1016/j.asoc.2024.112382},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112382},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gesture recognition framework for upper-limb prosthetics using entropy features from electromyographic signals and a gaussian kernel SVM classifier},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Many-to-many: Domain adaptation for water quality
prediction. <em>ASOC</em>, <em>167</em>, 112381. (<a
href="https://doi.org/10.1016/j.asoc.2024.112381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting water quality is crucial for sustainable water management. To mitigate data scarcity for specific water quality targets, domain adaptation methods have been employed, adjusting a model to perform in a related domain and leveraging learned knowledge to bridge domain differences. However, these methods often fall short by overfitting certain domain-specific patterns, overlooking consistent water quality patterns in multi-water domains. Despite regional variations, these Consistent patterns show fundamental commonalities and can be observed across monitoring sites, stemming from their widespread and interconnected nature. Addressing these limitations, we introduce the Many-to-Many Domain Adaptation framework (M2M) for prediction to bridge the gap between multi-source domains and multi-target domains, aligning shared insights with the distinct profiles of individual monitoring sites while considering their geographical interconnections. M2M adeptly addresses the formidable challenge of concurrently deciphering and integrating multifaceted patterns across an array of source and target domains, while also navigating the intricate regional heterogeneity intrinsic to the water quality of different sites. The M2M includes a domain pattern fusion module for consistent pattern extraction and numerical scale maintenance from source domains, a domain pattern sharing module for sharing pattern extraction from target domains, and an M2M learning method to ensure the training of these modules. Extensive experiments conducted on 120 diverse monitoring stations demonstrate that M2M markedly enhances the accuracy of water quality predictions using various time series encoders. Code available at https://github.com/biya0105/M2M .},
  archive      = {J_ASOC},
  author       = {Shunnan Wang and Min Gao and Huan Wu and Fengji Luo and Feng jiang and Liang Tao},
  doi          = {10.1016/j.asoc.2024.112381},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112381},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-to-many: Domain adaptation for water quality prediction},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Neighborhood entropy guided by a decision attribute and its
applications in multi-source information fusion and attribute selection.
<em>ASOC</em>, <em>167</em>, 112380. (<a
href="https://doi.org/10.1016/j.asoc.2024.112380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing methods for information fusion and attribute selection typically require a computationally intensive grid search to determine the optimal parameter. This grid search can significantly impact the efficiency of these processes. To tackle this issue, this paper introduces the concept of neighborhood entropy, which incorporates both the neighborhood and decision classes, and delves into its applications in multi-source information fusion and attribute selection. First, the concept of a δ -neighborhood based on a predefined distance is introduced. Then the neighborhood entropy based on the δ -neighborhood is defined. The optimal parameters for information fusion and attribute selection are determined by minimizing the neighborhood entropy. Furthermore, an information fusion method and an attribute selection algorithm based on the neighborhood entropy, conditional information entropy (CIE), and belief functions are developed. Finally, we demonstrate the effectiveness of our approach through experiments and Wilcoxon tests on 12 datasets, including a large-scale gene dataset. The experimental results show that the optimal parameter determined by the neighborhood entropy aligns with that obtained through grid search and the neighborhood entropy reduces the computational burden of information fusion and attribute selection compared to grid search. When compared to other state-of-the-art algorithms, our approach based on the neighborhood entropy and CIE demonstrates superior classification performance and time efficiency. These findings demonstrate the efficiency and effectiveness of neighborhood entropy and pave the way for applying granular-computing-based information fusion methods to large-scale datasets.},
  archive      = {J_ASOC},
  author       = {Qinli Zhang and Pei Wang and Witold Pedrycz and Zhaowen Li},
  doi          = {10.1016/j.asoc.2024.112380},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112380},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neighborhood entropy guided by a decision attribute and its applications in multi-source information fusion and attribute selection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion and context interaction for RGB-d indoor
semantic segmentation. <em>ASOC</em>, <em>167</em>, 112379. (<a
href="https://doi.org/10.1016/j.asoc.2024.112379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth information in indoor scenes is crucial to understanding the spatial and occluding relationships of objects. Depth information contains rich geometric details and is unaffected by lighting, texture, and color, significantly enhancing traditional color images. RGB images and depth images have been widely applied in various image analysis. However, effectively leveraging the complementary information from both modalities remains a challenge. To address this issue, we propose a unified and effective feature fusion and context interaction network (FCINet). It includes three modules. Firstly, we construct a multi-modal feature fusion module (MFFM), which can achieve the aggregation of two modalities along spatial and channel dimensions. Secondly, we construct a global and local information interaction context module (GLCM) to encode rich semantic information. Finally, we construct a feature alignment and fusion module (FAFM), which integrates upsampled low-level features with high-level features to mitigate spatial information loss. Experimental results indicate that the model achieved favorable outcomes on both the NYU Depth v2 dataset and the more complex SUN RGB-D dataset.},
  archive      = {J_ASOC},
  author       = {Heng Liu and Wen Xie and Shaoxun Wang},
  doi          = {10.1016/j.asoc.2024.112379},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112379},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature fusion and context interaction for RGB-D indoor semantic segmentation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Service similarity measurement integrating bi-LSTM
contextual representation and attention mechanism for web service
discovery. <em>ASOC</em>, <em>167</em>, 112378. (<a
href="https://doi.org/10.1016/j.asoc.2024.112378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth and wide adoption of web services, service discovery is becoming commonly used to locate the optimal services that can meet the requirements of users. This paper proposes a bidirectional long short-term memory (Bi-LSTM) service discovery method that integrates contextual representation and attention mechanisms for web service discovery. To conduct the study, FastText word embedding with the N-gram feature is primarily used as the input of the Bi-LSTM model to obtain the context feature vector of the service function description. Moreover, the attention mechanism is employed to calculate the similarity of the service function description to gain the semantic feature vector of the correlation degree between services. Then, the multilayer perceptron network is used to mine the mapping relationship between semantic feature vectors and matching levels among services. Finally, for a given query, target services are retrieved by ranking candidate services according to the prediction cores through the matching method. The proposed method is examined against multiple evaluation metrics, including accuracy and error, and compared with the state-of-the-art searching approaches. The experimental results show that the proposed method is more effective and attains better relevant web services, which helps the request service find more accurate results. Furthermore, this study provides new insights for researchers into web service discovery.},
  archive      = {J_ASOC},
  author       = {Zhao Huang and Jin Li},
  doi          = {10.1016/j.asoc.2024.112378},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112378},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Service similarity measurement integrating bi-LSTM contextual representation and attention mechanism for web service discovery},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive deep learning models for efficient multivariate
anomaly detection in IoT infrastructures. <em>ASOC</em>, <em>167</em>,
112377. (<a href="https://doi.org/10.1016/j.asoc.2024.112377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adapting machine learning-based systems to dynamic environments poses significant challenges due to their diverse and rapidly changing nature. Traditional Deep Neural Network (DNN) algorithms often struggle to cope effectively with such variations. This paper presents a novel evolutionary algorithm named Double Evaluation Genetic Evolution (DEGE), specifically tailored to evolve DNNs within dynamic contexts. DEGE represents a pioneering approach in evolutionary computing, focusing on the adaptive evolution of DNN structures across generations. This adaptability plays a crucial role in enabling DNNs to seamlessly adjust to evolving environmental conditions and complexities. To evaluate the efficacy of DEGE, we apply it to the domain of anomaly detection, rigorously testing the adapted DNNs within this specific context. Furthermore, we conduct comparative analyses between DEGE and established optimization methods using standard metrics to elucidate its advantages. Our findings shed light on DEGE’s effectiveness in addressing the challenges posed by dynamic environments, indicating its potential to revolutionize DNN optimization. As a practical application, we integrate DEGE-based DNNs into an IoT anomaly detection system to assess the overall impact of DEGE on anomaly detection performance. Our experiments demonstrate the efficiency of DEGE across 10 generations, showcasing its high adaptability to the dynamism inherent in IoT infrastructures. The proposed DEGE-based anomaly detection system processes highly dynamic environments within IoT infrastructure and classifies/predicts different types of anomalies efficiently with 99% detection accuracy across multiple benchmark and live experiment datasets. Solving the problem of multiclassification in dynamic abnormality detection, the proposed DEGE-based anomaly detection system was highly adaptable to the environment across generations, reaching the optimal DNN structure that delivers the best accuracy, and precision, with minimum loss value.},
  archive      = {J_ASOC},
  author       = {Ali Jameel Hashim and M.A. Balafar and Jafar Tanha and Aryaz Baradarani},
  doi          = {10.1016/j.asoc.2024.112377},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive deep learning models for efficient multivariate anomaly detection in IoT infrastructures},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of triple orbit single unreliable server retrial
queue in the diagnostic center using fuzzy approach. <em>ASOC</em>,
<em>167</em>, 112376. (<a
href="https://doi.org/10.1016/j.asoc.2024.112376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a queueing model with a single server, triple orbit, and an unreliable server to examine the model’s performance in the healthcare industry, specifically in a diagnostic center where a single radiologist performs Magnetic Resonance Imaging (MRI) on patients. In this scenario, the MRI machine can be considered as a server, and patients waiting to use the MRI machine are referred to as customers. The proposed triple orbit retrial queueing model takes into account the arrival rate of patients, service time, triple orbit, and the probability of server breakdown. To construct the mathematical model, we frame the Chapman–Kolmogorov (C-K) steady-state equations. These equations are then solved using the probability generating function (PGF) method to derive the steady-state queue size probabilities. Subsequently, leveraging these probabilities, we formulate various performance measures, including the queue length of patients, waiting time, and long-run probabilities. The model is then transformed into a fuzzy environment by considering the input parameters of the queueing model as the trapezoidal fuzzy number. Zadeh’s extension principle, the α -cut method, and parametric nonlinear programming (P-NLP) are employed to construct membership functions for the average number of patients and waiting time. The numerical results are presented in both crisp and fuzzy environments, demonstrating that the proposed fuzzy queueing model can provide more accurate and realistic predictions of the system’s performance compared to traditional queueing models. An application example is provided based on an MRI diagnostic center to analyze the system’s performance under various scenarios. This example numerically illustrates performance indices, including the fuzzy average number of patients and the fuzzy average waiting time of patients. Furthermore, Yager’s ranking method (YRM) and the graded mean integration method (GMIM) are employed to defuzzify both the average number of patients in the diagnostic center and the average waiting time for patients.},
  archive      = {J_ASOC},
  author       = {Sudeep Singh Sanga and Khushbu S. Antala},
  doi          = {10.1016/j.asoc.2024.112376},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112376},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of triple orbit single unreliable server retrial queue in the diagnostic center using fuzzy approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A few-shot learning methodology for improving safety in
industrial scenarios through universal self-supervised visual features
and dense optical flow. <em>ASOC</em>, <em>167</em>, 112375. (<a
href="https://doi.org/10.1016/j.asoc.2024.112375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industrial safety aims to prevent and mitigate workplace accidents and property damage. One common approach to identifying and analyzing potentially risky situations involves the use of static cameras to capture images or videos of facilities and production processes. However, current state-of-the-art deep learning-based solutions require extensive labeled datasets and substantial computational power to detect these dangerous situations. To address these limitations, this paper presents DINOFSAFE , a methodology that combines dense optical flow and the DINOv2 model, a vision transformer that learns universal visual features without supervision. Our methodology demonstrates dual efficacy by both minimizing the manual labeling efforts necessary for model training and ensuring computational efficiency. Optical flow estimates the apparent motion of objects in the input video streams, while the DINOv2 model generates high-dimensional universal representations capturing their visual properties. Using these representations, we train simple linear classifiers to identify moving objects and categorize them. This information aids in identifying and preventing hazardous conditions in industrial settings, such as pedestrians crossing paths with forklifts, forklifts approaching dangerous areas, loads falling from forklifts, and similar situations. We tested our solution on real videos sourced from industrial environments, resulting in promising outcomes. Furthermore, we compiled a comprehensive dataset consisting of approximately 6 500 images, which we have made publicly available for research and development purposes.},
  archive      = {J_ASOC},
  author       = {Juan Jesús Losada del Olmo and Ángel Luis Perales Gómez and Pedro E. López-de-Teruel and Alberto Ruiz},
  doi          = {10.1016/j.asoc.2024.112375},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112375},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A few-shot learning methodology for improving safety in industrial scenarios through universal self-supervised visual features and dense optical flow},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patient centric trustworthy AI in medical analysis and
disease prediction: A comprehensive survey and taxonomy. <em>ASOC</em>,
<em>167</em>, 112374. (<a
href="https://doi.org/10.1016/j.asoc.2024.112374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) integration in healthcare is revolutionizing medical analysis and disease prediction, enhancing diagnostic accuracy and patient care. However, with the growing adoption of AI, concerns surrounding trustworthiness, ethics, and transparency persist. This survey paper explores Trustworthy AI in healthcare, with a distinct focus on a patient-centric approach. By analyzing 132 relevant papers, we present a novel taxonomy across ten dimensions, emphasizing the criticality of safety, robustness, and patient trust. We highlight factors influencing trustworthiness and investigate the ethical frameworks guiding responsible AI deployment. A key contribution is the introduction of the Trustworthy AI Scoring System (TAI-SS), a novel framework to assess AI trustworthiness in healthcare, emphasizing ethics, privacy, and reliability. Case studies, such as AI-powered cancer diagnosis, demonstrate TAI-SS’s practical application. Additionally, we discuss transparency through Explainable AI (XAI) techniques and segmentation approaches. Our analysis underscores the importance of healthcare datasets and AI algorithms while recommending seven Trustworthy AI requirements and four ethical principles. This paper serves as a roadmap for AI-driven, patient-centric healthcare, offering insights for researchers, healthcare professionals, and policymakers.},
  archive      = {J_ASOC},
  author       = {Avaneesh Singh and Krishna Kumar Sharma and Manish Kumar Bajpai and Antonio Sarasa-Cabezuelo},
  doi          = {10.1016/j.asoc.2024.112374},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112374},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Patient centric trustworthy AI in medical analysis and disease prediction: A comprehensive survey and taxonomy},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ant colony optimization attribute reduction algorithm for
hybrid data using fuzzy β covering and fuzzy mutual information.
<em>ASOC</em>, <em>167</em>, 112373. (<a
href="https://doi.org/10.1016/j.asoc.2024.112373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an effective tool for handling the uncertainty and fuzziness of data, fuzzy β covering can fit the given dataset well. Swarm intelligence algorithms are suitable for solving complex combinatorial optimization problems and then have unique advantages in attribute reduction. This paper proposes an ant colony optimization attribute reduction algorithm based on fuzzy β covering and fuzzy mutual information. Initially, a fuzzy β covering decision information system for hybrid data is built based on fuzzy β covering theory. Then, fuzzy mutual information is introduced to measure the uncertainty of this system. Subsequently, an evaluation function is constructed using fuzzy mutual information for designing a forward attribute reduction algorithm based on heuristic search strategy. Moreover, to identify potentially more optimal attribute subsets, an ant colony optimization attribute reduction algorithm based on random search strategy is designed. Finally, two proposed algorithms are experimentally compared with six existing attribute reduction algorithms. The results indicate that these two algorithms surpass the other six algorithms in terms of classification accuracy and reduction rate.},
  archive      = {J_ASOC},
  author       = {Yuan Chen and Xiaopeng Cai and Zhaowen Li},
  doi          = {10.1016/j.asoc.2024.112373},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ant colony optimization attribute reduction algorithm for hybrid data using fuzzy β covering and fuzzy mutual information},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A MIL-based framework via contrastive instance learning and
multimodal learning for long-term ECG classification. <em>ASOC</em>,
<em>167</em>, 112372. (<a
href="https://doi.org/10.1016/j.asoc.2024.112372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning-based models are widely employed for electrocardiogram (ECG) classification. However, classifying long-term ECGs, which contain vast amounts of data, is challenging. Due to the limitation of memory with respect to the original data size, preprocessing techniques such as resizing or cropping are often applied, leading to information loss. Therefore, introducing multi-instance learning (MIL) to address long-term ECG classification problems is crucial. However, a major drawback of employing MIL is the destruction of sample integrity, which consequently hinders the interaction among instances. To tackle this challenge, we proposed a multimodal MIL neural network named CIMIL, which consists of three key components: an instance interactor, a feature fusion method based on attention mechanisms, and a multimodal contrastive instance loss. First, we designed an instance interactor to improve the interaction and keep continuity among instances. Second, we proposed a novel feature fusion method based on attention mechanisms to effectively aggregate multimodal instance features for final classification, which selects key instances within each class, not only enhances the performance of our model but also reduces the number of parameters. Third, a multimodal contrastive instance loss is proposed to enhance the model’s ability to distinguish positive and negative multimodal instances. Finally, we evaluated CIMIL on both intrapatient and interpatient patterns of two commonly used ECG datasets. The experimental results show that the proposed CIMIL outperforms existing state-of-the-art methods on long-term ECG tasks.},
  archive      = {J_ASOC},
  author       = {Haozhan Han and Cheng Lian and Bingrong Xu and Zhigang Zeng and Adi Alhudhaif and Kemal Polat},
  doi          = {10.1016/j.asoc.2024.112372},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A MIL-based framework via contrastive instance learning and multimodal learning for long-term ECG classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A ranking improved teaching-learning-based optimization
algorithm for parameters identification of photovoltaic models.
<em>ASOC</em>, <em>167</em>, 112371. (<a
href="https://doi.org/10.1016/j.asoc.2024.112371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar energy is an important clean energy source, primarily applied for photovoltaic (PV) power generation. The precise identification of PV system parameters is critical for system control and simulation, posing a challenge due to the models&#39; non-linearity, implicitness, and multiple optimal properties. So, a ranking improved teaching-learning-based optimization (RITLBO) is developed in this work to solve the problem of identifying the parameters of the PV model. RITLBO is a meta-heuristic algorithm based on teaching-learning-based optimization (TLBO) that simulates classroom teacher-student interaction. In RITLBO, learners are classified into inferior and superior groups based on their fitness ranking. During the teacher phase, superior learners emulate the top three agents with the highest fitness for local search, while inferior learners engage in guided mutual learning for global search, effectively utilizing computing resources. In the learner phase, superior learners receive guided information, while inferior learners engage in broader information exchange, balancing exploration and exploitation. RITLBO and fourteen algorithms are used to identify the parameters for five different PV models to confirm that the RITLBO is effective. Statistical results and analysis demonstrate that RITLBO is accurate and reliable in identifying PV model parameters. RITLBO offers promising prospects in optimizing PV system parameters through its unique strategies.},
  archive      = {J_ASOC},
  author       = {Haoyu Wang and Xiaobing Yu},
  doi          = {10.1016/j.asoc.2024.112371},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A ranking improved teaching-learning-based optimization algorithm for parameters identification of photovoltaic models},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VT-PINN: Variable transformation improves physics-informed
neural networks for approximating partial differential equations.
<em>ASOC</em>, <em>167</em>, 112370. (<a
href="https://doi.org/10.1016/j.asoc.2024.112370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach was proposed, that is, physics-informed neural networks combined with variable transformations(VT-PINN). This work demonstrates a technique that can greatly improve the approximation of physics-informed neural networks(PINN). We construct alternative equations by introducing variable transformations into partial differential equations(PDEs), and the solutions of alternative equations have smoother geometric properties. The alternative equations and the original equations are interrelated by variable transformations. The PINN loss function of the associated alternative equations is rederived. Several numerical examples have been used to verify the effectiveness of our proposed method, including the Poisson equation (RMSE: 0.17%), the wave equation (RMSE: 0.034%, example 3), and the advection–diffusion equation(0.25%, κ : L 2 relative error), etc. Compared with the standard PINN(2.8%, 3.6%, 1.9%), the efficiency and accuracy of VT-PINN are demonstrated by numerical examples. The approximation accuracy of VT-PINN has generally increased by an order of magnitude. The numerical results reveal that the proposed method greatly improves the approximation of PINN. At the same time, VT-PINN does not add additional time cost. In addition, the relevant numerical experiments show that this method greatly improves the effect of PINN in estimating the unknown parameters of partial differential equations, that is, it has good performance for solving inverse problems.},
  archive      = {J_ASOC},
  author       = {Jiachun Zheng and Yunlei Yang},
  doi          = {10.1016/j.asoc.2024.112370},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {VT-PINN: Variable transformation improves physics-informed neural networks for approximating partial differential equations},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep anomaly detection: A linear one-class SVM approach for
high-dimensional and large-scale data. <em>ASOC</em>, <em>167</em>,
112369. (<a href="https://doi.org/10.1016/j.asoc.2024.112369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection plays a crucial role in various fields including cyber security, finance, healthcare, and industrial monitoring. Traditional methods in anomaly detection often face several challenges such as scalability, adaptability, and difficulty in handling high-dimensional data. So a novel Recurrent Extreme Learning based –Boosted Chimp (REL-BC) algorithm is proposed for anomaly detection. The REL-BC model involves a data pre-processing phase and an anomaly detection phase. The data pre-processing phase involves three stages namely one-hot-encoding, outlier disposal, and min-max normalization. In this study, a Recurrent Neural Network is utilized to seize the temporal dependencies and traffic data in the network. Also, the Extreme Learning Machine (ELM) is employed in distinguishing normal as well as anomalous patterns. Further Chimp Optimization is employed for optimizing hyperparametersto improve the efficiency of the REL-BC approach. The outcome of the experimentation revealed that it demonstrated the improvement of performance for the REL-BC method in detecting anomalies based on various measures.},
  archive      = {J_ASOC},
  author       = {K. Suresh and K. Jayasakthi Velmurugan and R. Vidhya and S. Rahini sudha and Kavitha V},
  doi          = {10.1016/j.asoc.2024.112369},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep anomaly detection: A linear one-class SVM approach for high-dimensional and large-scale data},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential neural network based adaptive average output
feedback control design for dosage determination on cancer based
immunotherapy treatment. <em>ASOC</em>, <em>167</em>, 112368. (<a
href="https://doi.org/10.1016/j.asoc.2024.112368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapy involves natural and synthetic substances to stimulate the body’s immune response. This treatment approach is practical not only for addressing immune deficiencies but also for combating malignancies. This paper describes a non-parametric approximated adaptive control process for managing cancer dynamics under immunotherapy treatment, utilizing a combination of a differential neural network (DNN) observer and nonlinear control techniques such as sliding mode and local optimal strategies. By employing the state estimation and control methods, close tracking between the estimated states provided by the neural network and the cancer model dynamics is possible. Internal model reconstruction and an observer provided by a variable structure model are essential for controlling unknown plants. Furthermore, the control design has successfully reduced tumor cells despite uncertainties and external perturbations affecting cancer dynamics. This robustness enhances the reliability of the proposed design. A virtual real-time scheme was developed to demonstrate this controller’s feasibility in real clinical scenarios. In this scheme, a simulated patient generates variables of immunotherapy dynamics as electrical signals, which are then analyzed by a real-time project.},
  archive      = {J_ASOC},
  author       = {N. Aguilar-Blas and I. Chairez and A. Cabrera},
  doi          = {10.1016/j.asoc.2024.112368},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential neural network based adaptive average output feedback control design for dosage determination on cancer based immunotherapy treatment},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An auxiliary correction model implemented by the correction
property of intermediate layer against adversarial examples.
<em>ASOC</em>, <em>167</em>, 112367. (<a
href="https://doi.org/10.1016/j.asoc.2024.112367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In powerful adversarial attacks against deep neural networks (DNN), the generated adversarial example will mislead the DNN-implemented classifier by destroying the features of the last layer. To enhance the robustness of the classifier, in our paper, a F eature A nalysis and C onditional M atching prediction distribution (FACM) model is proposed to utilize the features of intermediate layers to correct the misclassification. Specifically, we first prove that the intermediate layers of the classifier still retain effective features for the original category when the classifier is subjected to adversarial attacks, which is defined as the Correction Property in our paper. According to this, we propose the FACM model consisting of F eature A nalysis (FA) correction module, C onditional M atching P rediction D istribution (CMPD) correction module and decision module. Specifically, the FA correction module is comprised of fully connected layers, which takes the features of the intermediate layers as the input to correct the misclassification of the classifier. The CMPD correction module is based on a conditional autoencoder, which not only uses the features of intermediate layers as the condition to accelerate convergence but also mitigates the negative effect of adversarial examples, trained with the Kullback–Leibler loss to match prediction distribution. Through the empirically verified Diversity Property among the individual correction modules, the decision module is proposed to integrate the proposed correction modules to enhance the DNN-implemented classifier’s robustness by reducing the dimensionality of adversarial subspace. That is, the input perturbed in certain directions (i.e., dimensions) that lead to misclassifications for the classifier can be correctly classified by the proposed correction modules. The extended experiments demonstrate our FACM model outperforms the existing methods against adversarial attacks, especially optimization-based white-box attacks and query-based black-box attacks.},
  archive      = {J_ASOC},
  author       = {Xiangyuan Yang and Jie Lin and Hanlin Zhang and Xinyu Yang and Peng Zhao},
  doi          = {10.1016/j.asoc.2024.112367},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An auxiliary correction model implemented by the correction property of intermediate layer against adversarial examples},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential evolution optimization based ensemble framework
for accurate cervical cancer diagnosis. <em>ASOC</em>, <em>167</em>,
112366. (<a href="https://doi.org/10.1016/j.asoc.2024.112366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer remains a significant global health concern, demanding accurate and efficient diagnostic approaches for early detection and treatment. However, there are situations where a single deep learning model might not be sufficient to adequately capture the pertinent information required for precise disease prediction from intricate datasets. In this paper, we introduce an innovative ensemble learning framework that leverages the Differential Evolution (DE) algorithm to enhance the detection of cervical cancer. Our innovative approach combines the collective strength of a diverse set of base models using feature fusion techniques to achieve precise and robust classification. We have developed three base models, denoted as Fusion 1, Fusion 2, and Fusion 3, where each fusion model incorporates two pre-trained models using a feature fusion technique. To capture sequential dependencies and improve feature attention, we have integrated ConvLSTM layers and Squeeze-and-Excitation (SE) blocks into these base models. Subsequently, we&#39;ve amalgamated the predictions of each base model using a weighted aggregation scheme, with the weights optimized using the DE algorithm. Our ensemble framework introduces a novel method for dynamic weight adjustment, utilizing advanced fusion techniques to improve cervical cancer detection accuracy. This approach ensures robust diagnostic performance across cancer cells datasets, enhancing patient outcomes and community health. Our evaluation covers two datasets, SIPaKMeD and Mendeley LBC, demonstrating the remarkable performance and robustness of our ensemble model. It consistently outperforms individual base models and even surpasses state-of-the-art methods, achieving an impressive accuracy rate of 98.96 % on the Mendeley LBC dataset and 95.68 % on the SIPaKMeD dataset. The incorporation of ConvLSTM layers, SE blocks, and DE optimization significantly contributes to the model&#39;s exceptional accuracy, its ability to generalize effectively, and its adaptability to diverse scenarios. Additionally, we employ Grad-CAM to visualize how the base models focus on crucial regions for accurate predictions, providing insights into the model&#39;s decision-making process. Our ensemble model exhibits outstanding Area under the Curve (AUC) values in Receiver Operating Characteristic (ROC) analysis, further confirming its effectiveness. This research represents a significant advancement in cervical cancer detection, underscoring the potential of ensemble learning techniques optimized through DE for enhancing diagnostic accuracy and advancing patient care.},
  archive      = {J_ASOC},
  author       = {Omair Bilal and Sohaib Asif and Ming Zhao and Yangfan Li and Fengxiao Tang and Yusen Zhu},
  doi          = {10.1016/j.asoc.2024.112366},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolution optimization based ensemble framework for accurate cervical cancer diagnosis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grasp with push policy for multi-finger dexterity hand based
on deep reinforcement learning. <em>ASOC</em>, <em>167</em>, 112365. (<a
href="https://doi.org/10.1016/j.asoc.2024.112365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When interacting with the external environment, dexterous hands face various challenges, including interference in cluttered environments and difficulty in accurately locating target objects. We observe that the human hand is often at ease in the face of complex unstructured environments: pushing objects that can rearrange clutter, locating target objects, and creating space for fingers. In addition, the gripping action complements the push by achieving the precise movement of unrelated objects. Taking inspiration from this, to enable manipulators to perform accurate grasping tasks in complex environments, we propose a new data-driven grasping approach for robotic dexterous hands: the push-fit grasping approach. This method combines human grasping with model-free deep reinforcement learning to realize the robot&#39;s collaborative grasping ability. An end-to-end conditional converter grasping network is first trained, which takes the visual point cloud input to the action output. We use DQN algorithms for strategy training, further enhanced by fine-tuning methods for real-world learning. The experimental results show that the push strategy we learned significantly improves the grasping performance, and the success rate of cooperative grasping is increased by 8 %. It is worth noting that this fine-tuning method significantly reduces the actual training time, with the real world training taking up only one-fifth of the time in the simulated world. Even in demanding scenarios characterized by confusion and complexity, it facilitates rapid learning. This study provides new insights for effectively addressing complex challenges involving multi-fingered dexterous hands and human-computer interaction. Our code is available in the github repository.},
  archive      = {J_ASOC},
  author       = {Baojiang Li and Shengjie Qiu and Jibo Bai and Haiyan Wang and Bin Wang and Zhekai Zhang and Liang Li and Xichao Wang},
  doi          = {10.1016/j.asoc.2024.112365},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grasp with push policy for multi-finger dexterity hand based on deep reinforcement learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Motor delay image recognition based on deep learning and
human skeleton model. <em>ASOC</em>, <em>167</em>, 112364. (<a
href="https://doi.org/10.1016/j.asoc.2024.112364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children with developmental delays often exhibit gross motor delay during their early childhood. Identifying such delay typically requires standardized assessments, but these assessments are susceptible to subjective interpretation. To enhance objectivity and reliability in these assessments, this study proposes a novel method utilizing an AI model for identifying gross motor delay in children. The proposed method divides the assessment of gross motor skills into two components: static balance and dynamic balance. First, the Yolo target detection model and the Mediapipe human posture detection model are employed for preprocessing to determine skeletal joints and body positioning information. Subsequently, dimensionality reduction techniques are applied to extract key features. A sliding window method is then used to extract action features identifying static balance features of the human body (e.g., standing on one foot, standing on the left foot) and dynamic balance features (e.g., lifting the heels while walking, jumping on a mat). The machine learning classifier and LSTM-CNN are then used to categorize the static and dynamic balance feature data. Experimental results demonstrate that the proposed method achieved accuracy scores of 80 % and 87 % for static and dynamic balancing skills, respectively.},
  archive      = {J_ASOC},
  author       = {Yi-Fang Tu and Ling-Yi Lin and Meng-Hsiun Tsai and Yi-Shan Sung and Yi-Shan Liu and Mu-Yen Chen},
  doi          = {10.1016/j.asoc.2024.112364},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Motor delay image recognition based on deep learning and human skeleton model},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fusion model for multiple partial-order quotient spaces
based on three-way decision. <em>ASOC</em>, <em>167</em>, 112363. (<a
href="https://doi.org/10.1016/j.asoc.2024.112363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With a quotient space, a complex problem can be transformed into an appropriate granularity for processing. However, within a quotient space, there are no partial order relations between information granules. In practical decision-making, it may be necessary to deal with quotient spaces that have partial order relations between information granules. In this paper, first, the concept of a partial-order quotient space is proposed and some of its properties are studied. Second, a fusion model for multiple partial-order quotient spaces based on three-way decision is proposed, and some related theorems are proved. Finally, a case study of teacher evaluation, an experiment on real data, and several simulation experiments are presented to demonstrate the practicality of the proposed model in this paper.},
  archive      = {J_ASOC},
  author       = {Longjun Yin and Qinghua Zhang and Chengying Wu and Nanfang Luo and Fan Zhao and Ying Yang},
  doi          = {10.1016/j.asoc.2024.112363},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fusion model for multiple partial-order quotient spaces based on three-way decision},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Synthesizing complexity: Trends, challenges, and future
directions in fuzzy-based multicriteria decision-making (FMCDM) methods.
<em>ASOC</em>, <em>167</em>, 112362. (<a
href="https://doi.org/10.1016/j.asoc.2024.112362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the significance of fuzzy-based multicriteria decision-making (FMCDM) methods in addressing intricate decision-making (DM) scenarios. In response to the increasing demand for effective DM processes amidst uncertainty, this study investigates trends, challenges, and future directions in FMCDM from 2021--2023. Traditional methods often struggle with the complexity and uncertainty of real-world issues. Conversely, FMCDM methods, which incorporate fuzzy logic (FL), provide an innovative solution by addressing ambiguity and imprecision, thus improving DM clarity and effectiveness. This paper emphasizes the practical applications of FMCDM across various fields, highlighting its ability to enhance decision quality amid complexity and uncertainty. FMCDM methods are categorized into standalone fuzzy methods (SFMs) and nonstandalone fuzzy methods (NSFMs). SFMs independently offer robust solutions, whereas NSFMs combine FL with other techniques to address intricate decision problems. Despite their increasing prominence, challenges such as computational complexity and the need for greater scalability remain. Integrating advanced technologies such as artificial intelligence, machine learning, blockchain, and big data analytics holds promise for further improving FMCDM performance. By offering insights into implementation and addressing challenges, this review provides decision-makers (DMRs) with the necessary tools to confidently and clearly navigate modern complexities. In the future, this study suggests future directions, including the incorporation of evolving expert opinions and comprehensive comparative analyses. These findings contribute to the continuous improvement of FMCDM methodologies, promoting informed DM in a rapidly changing world.},
  archive      = {J_ASOC},
  author       = {Farshad Nezhad Shahmohammad and Yashar Pourrahimian and Naeimeh Akbari-Gharalari},
  doi          = {10.1016/j.asoc.2024.112362},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Synthesizing complexity: Trends, challenges, and future directions in fuzzy-based multicriteria decision-making (FMCDM) methods},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adapting the population size in CMA-ES using nearest-better
clustering method for multimodal optimization. <em>ASOC</em>,
<em>167</em>, 112361. (<a
href="https://doi.org/10.1016/j.asoc.2024.112361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The covariance matrix adaptation evolution strategy (CMA-ES) is one of the advanced algorithms for solving continuous single-objective optimization. In this algorithm, the population size is an important parameter which represents the number of candidate points generated in each iteration. A larger population size allows for better exploration of the search space and is typically required for handling multimodal functions, while a smaller population size facilitates quicker convergence. Adapting the population size in this algorithm raises a question about how to increase/decrease/keep it in an efficient way to solve specific problems. In this paper, we propose a novel approach for this purpose based on the information of niches observed during the evolution process. Firstly, the nearest-better clustering (NBC) technique is employed to detect the number of niches in each iteration, then the information will be used to adapt the population size accordingly. Additionally, a quasi-Newton method, which serves as a local search, can be incorporated into the algorithm in order to speed it up. We evaluate the effectiveness of our method through numerical simulations on multimodal test functions, the black-box optimization benchmarking (BBOB) noiseless testbed, and the CEC 2014 and CEC 2017 benchmark testing suites.},
  archive      = {J_ASOC},
  author       = {Duc Manh Nguyen},
  doi          = {10.1016/j.asoc.2024.112361},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adapting the population size in CMA-ES using nearest-better clustering method for multimodal optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards sustainable resource allocation in agriculture: A
systematic review on cropping pattern optimization approaches.
<em>ASOC</em>, <em>167</em>, 112360. (<a
href="https://doi.org/10.1016/j.asoc.2024.112360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic growth and population increase have exacerbated water scarcity and environmental issues by elevating demand, pollution, and climate change. To ensure food security and sustainability, it is essential to optimize the use of water and land resources in agriculture Numerous studies have explored this problem, focusing on cropping pattern optimization. This paper systematically reviews 162 articles on cropping pattern optimization (1996–2024), using statistical analysis to identify keywords and filter papers most relevant to agricultural resource allocation and cropping pattern optimization. Our review examines modeling characteristics, decision types, and approaches related to mathematical programming and the scope of research. Finally, a gap analysis is conducted to identify areas for further exploration. Our analysis revealed that the majority of studies were carried out in Asia. In the last decade, food security has significantly influenced optimal cropping patterns, with a trend toward the simultaneous allocation of land and water resources. Furthermore, multi-objective models are emerging in response to the need for sustainable solutions, while stochastic approaches predominate in modeling uncertainty. However, there is a need for more innovative methods to capture the inherent uncertainty of environmental parameters, particularly in the context of climate change where historical data is not reliable. The review highlights the need for integrating cropping pattern optimization with supply chain design and considering broader aspects such as international trade, seasonal uncertainty, and resource allocation. It also emphasizes the importance of incorporating diverse agricultural practices, managing agricultural waste, and aligning with farmers&#39; preferences. By addressing these issues, the findings align with several Sustainable Development Goals, including zero hunger, clean water and sanitation, and responsible consumption and production, promoting sustainable agricultural practices, efficient resource use, and enhanced food security. Future research should focus on integrating interdisciplinary insights, leveraging advanced technologies, and fostering stakeholder partnerships to develop effective, sustainable agricultural systems and a climate-resilient system under increasing environmental uncertainties.},
  archive      = {J_ASOC},
  author       = {Nima Taheri and Mir Saman Pishvaee and Hamed Jahani and Donya Zakeri},
  doi          = {10.1016/j.asoc.2024.112360},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112360},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards sustainable resource allocation in agriculture: A systematic review on cropping pattern optimization approaches},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning techniques via ensemble approaches in stock
exchange index prediction: Systematic review and bibliometric analysis.
<em>ASOC</em>, <em>167</em>, 112359. (<a
href="https://doi.org/10.1016/j.asoc.2024.112359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of time series indices in stock markets has become an area of growing interest, providing investors with an advanced insight into a country’s economic prospects in the short, medium, and long term. Much of this type of prediction has been conducted using machine learning techniques, aiming to develop effective and efficient models that can provide greater accuracy in forecasting. In this context, however, single-predict model approaches have shown limitations, such as the inability to handle diverse information efficiently, the non-linearity, and chaos in financial data. In the face of these limitations, ensemble approaches emerge as a strategy to mitigate these challenges. Although these ensemble approaches can be promising, there is no structured understanding related to their application in activities to predict time series indices in stock markets. Within this context, this study aims to investigate to what extent the literature has proposed and evaluated solutions that contribute to characterizing the state of the art in predicting the Stock Market Index through ensemble machine learning approaches. Fifty-three articles resulting from a systematic review, focused on scientific production between 2017 and the first half of 2023, were analyzed. The study performs a bibliometric analysis and categorization of the results, detailing and highlighting the main themes in the field. The bibliometric analysis identifies key authors, countries, conferences, and approaches to predict the Stock Market Index. The qualitative analysis reveals the benefits arising from the combination of algorithms in mitigating accuracy problems in index prediction, as well as systematizing information to characterize the state of the art in the field of study. Our results show that combined algorithms are an effective strategy to increase accuracy in predicting the Stock Market Index, which can benefit investors and financial institutions. This paper presents the future research scope by highlighting gaps in the existing literature for developing techniques to handle concept drifts.},
  archive      = {J_ASOC},
  author       = {João Victor Ribeiro Ferro and Roberio Jose Rogerio Dos Santos and Evandro de Barros Costa and José Rubens da Silva Brito},
  doi          = {10.1016/j.asoc.2024.112359},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112359},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning techniques via ensemble approaches in stock exchange index prediction: Systematic review and bibliometric analysis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Dual-stream fusion network with multi-head self-attention
for multi-modal fake news detection. <em>ASOC</em>, <em>167</em>,
112358. (<a href="https://doi.org/10.1016/j.asoc.2024.112358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of social media platforms like Weibo and WeChat, alongside the emergence of deepfake technologies, tackling fake information has become a major challenge for society and government institutions. To address this, developing efficient and intelligent methods for fake news detection is crucial. This paper introduces a dual-stream fusion network model (DSF-MHSA) based on deep learning, designed to detect fake news across web pages, images, and text. The model tackles issues such as cross-lingual discrepancies, data imbalance, and multimodal information fusion by integrating deep learning models like ERNIE-M, AlexNet, and ShuffleNet, along with three multi-head self-attention mechanisms. It processes textual and image data separately to capture long-range dependencies and global information, enhancing understanding and recognition. A unified multi-head self-attention mechanism then merges these insights to strengthen cross-modal correlation detection. The model is tested on datasets from Twitter, Weibo, and IKCEST-2023, which includes real online news from various social media sources. Results show that the DSF-MHSA model achieves over 90 % accuracy, surpassing traditional models in news detection tasks. This research offers significant practical value for the identification and understanding of news content.},
  archive      = {J_ASOC},
  author       = {Yimei Yang and Jinping Liu and Yujun Yang and Lihui Cen},
  doi          = {10.1016/j.asoc.2024.112358},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112358},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-stream fusion network with multi-head self-attention for multi-modal fake news detection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A vine-copulas based multi-sensor fusion structural damage
monitoring method and its application in dam engineering. <em>ASOC</em>,
<em>167</em>, 112356. (<a
href="https://doi.org/10.1016/j.asoc.2024.112356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a civil engineer that is susceptible to the mechanism of slow and continuous deterioration, the dam must detect damage according to the static data of the arranged sensors, which is an important issue in structural health monitoring. Due to the characteristics of huge solid structures, damage with a slight degree and small range usually does not cause significant changes in the monitoring data of a single sensor. Therefore, it is difficult to effectively detect local damage by traditional monitoring methods that establish operating warning values for a single sensor. Currently, there have been some approaches to fuse sensor information that consider correlations between multiple sensors as well as the overall operational behavior of the dam. However, their objectives are focused on improving the accuracy of response variable prediction models without addressing the local damage identification aspect. In this study, we apply the Vine-Copula model for the first time in a multi-sensor information fusion dam damage detection method based on static monitoring data. In this way, it improves the sensitivity of local damage identification for huge solid structures. A prediction model is fitted with the measuring data of each sensor to obtain the residual series. Then the joint probability distribution of the multidimensional residuals is acquired by constructing the Vine-Copula model. Finally, the joint cumulative distribution function value is used as an alarm limit to determine whether the structure has an abnormal damage condition. The effectiveness and accuracy of the proposed method were demonstrated in both a numerical arch dam simulating local damage and an actual rockfill dam engineering with cracks at the crest.},
  archive      = {J_ASOC},
  author       = {Yu Lu and Zhenyu Wu},
  doi          = {10.1016/j.asoc.2024.112356},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112356},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A vine-copulas based multi-sensor fusion structural damage monitoring method and its application in dam engineering},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Credit risk assessment method driven by asymmetric loss
function. <em>ASOC</em>, <em>167</em>, 112355. (<a
href="https://doi.org/10.1016/j.asoc.2024.112355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk assessment is significantly hindered by the problem of class imbalance, and cost-sensitive methods represent an effective strategy to address this issue. However, most algorithms tend to approach the imbalance from a class perspective, overlooking the finer details at the sample level. Moreover, such methods are susceptible to interference from noise and outliers. In response to these challenges, this paper proposes an asymmetric cost-sensitive support vector machine (QTSVM) that combines the quadratic type squared error loss function (QTSELF) with support vector machine (SVM). It not only leverages the asymmetry of the loss function by applying varying penalties based on classification errors but also employs different processing measures for samples from different classes. Additionally, it enhances model robustness by imposing a tiny penalty on noise or outliers. The adaptive moment estimation (Adam) algorithm is employed to optimize QTSVM. Extensive experiments and statistical tests profoundly demonstrate the superior performance of QTSVM.},
  archive      = {J_ASOC},
  author       = {Xiaoxi Zhao and Yingjie Tian},
  doi          = {10.1016/j.asoc.2024.112355},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112355},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Credit risk assessment method driven by asymmetric loss function},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Wavelet-based spatiotemporal sparse quaternion dictionary
learning for reconstruction of multi-channel vibration data.
<em>ASOC</em>, <em>167</em>, 112354. (<a
href="https://doi.org/10.1016/j.asoc.2024.112354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate reconstruction of multi-source data information plays an essential role in remote intelligent operation, and prognostic and health maintenance (RIO-PHM) of industrial systems. However, traditional signal reconstruction methods such as compressed sensing fail to capture the spatio-temporal characteristics and coupling nature of the multi-source or multi-channel data. To alleviate this bottleneck, in this paper, a new wavelet-based spatiotemporal sparse quaternion dictionary learning (WSTS-QDL) algorithm is formulated for the reconstruction of multi-channel vibration data for the first time. Specifically, the wavelet-based spatiotemporal sparse low-rank matrix (SLRM) algorithm is elaborated and the sparse coefficient matrix of the multi-channel signals can be estimated using the split Bregman iteration (SBI) technique. Then, quaternion-based dictionary learning is introduced for multi-channel data reconstruction according to the updated sparse coefficient matrix during quaternion dictionary learning. Eventually, two equipmental scenarios including run-to-failure data of rolling bearing in bearing test bench and vibration signal of the failured bearing in corn thresher, are utilized for verification. The experimental results demonstrate that the highest recovery accuracy performance with scoring function and the lowest errors are achieved by the proposed method in contrast with the state-of-the-art benchmarks such as orthogonal matching pursuit (OMP) method and spatiotemporal sparse Bayesian learning (SSBL), and the waveform of phase space trajectory and points cloud distribution of the Poincare section in the original signal are similar/same to that obtained by the proposed method.},
  archive      = {J_ASOC},
  author       = {Qing Li},
  doi          = {10.1016/j.asoc.2024.112354},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112354},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wavelet-based spatiotemporal sparse quaternion dictionary learning for reconstruction of multi-channel vibration data},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ensemble-based semi-supervised learning approach for
non-stationary imbalanced data streams with label scarcity.
<em>ASOC</em>, <em>167</em>, 112353. (<a
href="https://doi.org/10.1016/j.asoc.2024.112353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the challenges of learning from multi-class imbalanced data streams, particularly in scenarios with scarce labeled data and concept drift, remains an open problem in the field of data stream mining. Despite the prevalence of such data scenarios in real-world applications, existing approaches have yet to provide effective solutions. In this paper, we propose a novel chunk-based semi-supervised framework, GMCSSEL, which leverages an ensemble of base classifiers trained on micro-cluster centers with labels inferred through a graph-fusion and label propagation process. Our approach incorporates chunk-based incremental label propagation by integrating both current and previously inferred label information into the propagation equation, with regularization parameters applied to control their influence. Furthermore, our method includes a novel concept drift detection mechanism specifically designed for imbalanced data with label scarcity. The imbalanced data problem is addressed through a combination of graph fusion, label matrix normalization, and SMOTE techniques. Experimental results on synthetic data streams with varying class ratios and concept drifts, as well as real multi-class streams, demonstrate the superior classification performance of our approach compared to the IOE semi-supervised algorithm. Our method achieves an average increase in evaluation metrics of 12.5 % across multiple data streams, with improvements ranging from 8 % to 18 % in G-mean, AUC, Kappa, and F1-score metrics. Statistical analysis confirms that these improvements are significant, highlighting the robustness of our approach in handling non-stationary imbalanced data streams with label scarcity.},
  archive      = {J_ASOC},
  author       = {Yousef Abdi and Mohammad Asadpour and Mohammad-Reza Feizi-Derakhshi},
  doi          = {10.1016/j.asoc.2024.112353},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112353},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble-based semi-supervised learning approach for non-stationary imbalanced data streams with label scarcity},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A state-of-the-art review of long short-term memory models
with applications in hydrology and water resources. <em>ASOC</em>,
<em>167</em>, 112352. (<a
href="https://doi.org/10.1016/j.asoc.2024.112352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Short-Term Memory (LSTM) has recently emerged as a crucial tool for scientific research in hydrology and water resources. Despite its widespread use, a comprehensive analysis of LSTM&#39;s application in this field remains lacking in the literature. Here, we conducted a detailed exploration using VOSviewer and Bibiometrix literature visualization software to analyze aspects such as authors, keywords, and journal citations, providing insights into the research landscape. We examined the evolution of LSTM models, including their advanced variants and comparisons with other models like Gated Recurrent Units (GRUs) and hybrid LSTM-based approaches. Additionally, we reviewed recent research findings that demonstrate LSTM&#39;s applications in hydrology and water resources. Furthermore, we addressed challenges and proposed future research directions for LSTM modeling in this domain. These include integrating LSTM with complementary techniques for enhanced performance, optimizing hyperparameters to improve accuracy and efficiency, and enhancing model interpretability for deeper insights. This paper provides a comprehensive review of LSTM models applications in hydrology and water resources, offering insights into current research trends and guiding future research endeavors.},
  archive      = {J_ASOC},
  author       = {Zhong-kai Feng and Jing-shuai Zhang and Wen-jing Niu},
  doi          = {10.1016/j.asoc.2024.112352},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112352},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A state-of-the-art review of long short-term memory models with applications in hydrology and water resources},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards explainable evaluation: Explaining predicted
performance using local performance regions. <em>ASOC</em>,
<em>167</em>, 112351. (<a
href="https://doi.org/10.1016/j.asoc.2024.112351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying which instances in a learning problem are difficult to be predicted by a model is important to avoid critical errors at deployment time as well as to plan how to learn an improved model (e.g., by training data cleaning or augmentation). Previous works have been mainly devoted on measuring instance hardness or developing meta-learners (e.g., assessors) to predict a base model performance based on the instances’ features while neglecting interpretability. In this paper, we propose a method to explain the performance of learned models in a problem based on the induction of meta-rules. Each meta-rule identifies a local region of instances, called Local Performance Region (LPR), where the base model has a predictable performance. The meta-rules are induced using a reduced number of attributes, in such a way that each LPR can be more easily inspected (e.g., by an attribute plot). The proposed method combines assessors, data augmentation and rule induction procedures. Initially, given a dataset of interest and a base model, we build an assessor model, which will be able to predict the base model’s performance for new instances. The assessor is trained based on the test results obtained when the base model is evaluated, thus generalizing the observed errors across instances in the dataset. In our work, we built an assessor in a case study to predict the probability of incorrect classifications of a Random Forest (RF) base model, achieving a mean absolute error of 0.05 in a hold out experiment. Once learned, the assessor is used to predict the model’s errors for new instances in an augmented dataset, covering a variety of features. Finally, meta-rules are learned to approximate the assessor’s predictions in local regions of instances. Experiments show the usefulness of the proposal by finding 18 local regions of bad RF performance, demonstrating a special case of LPRs, called Local Hard Regions (LHRs). By explaining the (in)correctness of model predictions, LPRs constitute a novel application in explainable AI, but focusing on explaining model performance, which can be adopted to different ML contexts.},
  archive      = {J_ASOC},
  author       = {Patricia Drapal and Ricardo B.C. Prudêncio and Telmo M. Silva Filho},
  doi          = {10.1016/j.asoc.2024.112351},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112351},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards explainable evaluation: Explaining predicted performance using local performance regions},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multidimensional dynamic attention for multivariate time
series forecasting. <em>ASOC</em>, <em>167</em>, 112350. (<a
href="https://doi.org/10.1016/j.asoc.2024.112350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention-based models have been very effective in identifying important lagged variables for multivariate time series (MTS) forecasting applications. However, current attention-based models only provide static weights and do not consider the dynamic nature of predictions for multistep predictions of heterogeneous MTS. To address these limitations, this paper proposes a novel multidimensional dynamic attention (MDA) model for computing lagged variable importance. It incorporates a dynamic representation learner unit and considers multiple attention calculations to account for prediction dynamics, temporal information, and variable relations. Extensive experiments with both synthetic and real-world data demonstrate the effectiveness of the MDA model. It outperforms existing methods in the literature for sequence-to-sequence prediction of heterogeneous MTS in most cases and accurately identifies important features. MDA demonstrates enhancements up to 33% with real-world datasets. These findings demonstrate that the MDA model is a promising approach to MTS forecasting. The proposed attention mechanism can be utilised for other tasks related to MTS analysis beyond just forecasting, potentially enhancing the performance and interpretability of various MTS applications. The code is publicly available on GitHub, promoting widespread adoption and further research in this field https://github.com/sarah-almaghrabi/MDA-Multidimensional-Dynamic-Attention .},
  archive      = {J_ASOC},
  author       = {Sarah Almaghrabi and Mashud Rana and Margaret Hamilton and Mohammad Saiedur Rahaman},
  doi          = {10.1016/j.asoc.2024.112350},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112350},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multidimensional dynamic attention for multivariate time series forecasting},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A k-means triangular synthesis large margin classifier with
unified pinball loss for imbalanced data. <em>ASOC</em>, <em>167</em>,
112349. (<a href="https://doi.org/10.1016/j.asoc.2024.112349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Margin Distribution Machine (LDM) improves the Support Vector Machine (SVM) by integrating the marginal distribution of samples into the objective function, which exhibits excellent classification and generalization performance. However, most of the existing LDM-based models exhibit inherent flaws: (1) The assumption of a category uniform distribution, resulting in an inability to effectively address the issue of class imbalance. (2) The inheritance of the hinge loss in SVM, leading to inferior anti-noise performance. Given the shortcomings above, this paper constructs a novel K -means Triangular Synthesis (KTS) method for imbalanced data classification and introduces Unified Pinball (UP) loss to ensure robust performance, demonstrated as a novel KTS large margin classifier with UP Loss (KTS-UPLMC) model. Specifically, the KTS method utilizes the triangular synthesis and generates samples based on the cluster density, effectively mitigating the problems of introducing noise samples and strip-shaped sample distribution in the traditional synthetic minority oversampling technique, further alleviating the LDM’s classification line offset. In addition, the UP Loss considers quantile distance, improving the anti-noise performance of the model. Comparative experiments on artificial datasets and benchmark datasets validate the effectiveness and noise resistance of the proposed KTS-UPLMC in imbalanced data classification problems. Furthermore, the stability of the KTS-UPLMC is substantiated by the parameter sensitivity analysis experiment. In conclusion, the KTS method effectively addresses category imbalance, while the integration of the UP Loss enhances noise resistance.},
  archive      = {J_ASOC},
  author       = {Danlin Shao and Yixi Dai and Junjie Li and Shenglin Li and Rui Chen},
  doi          = {10.1016/j.asoc.2024.112349},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112349},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A K-means triangular synthesis large margin classifier with unified pinball loss for imbalanced data},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Coulomb’s law-inspired parameter-free outlier detection
algorithm. <em>ASOC</em>, <em>167</em>, 112348. (<a
href="https://doi.org/10.1016/j.asoc.2024.112348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection plays a crucial role in the field of data mining and machine learning. Despite the diverse range of outlier detection algorithms, most still face two main challenges: (i) the difficulty of choosing neighborhood parameter and top- n , existing algorithms can only solve one of them; (ii) the hardship of simultaneously detecting global outliers, local outliers, and outlier clusters. To solve these shortcomings, we propose a novel outlier factor, called L ocal C oulomb O utlier F actor (LCOF), inspired by Coulomb’s law in physics. It can measure the outlier degree of data objects without parameters due to the use of Coulomb force and natural neighbors. LCOF first applies natural neighbors to adaptively derive the neighborhood parameter k . It further employs the interquartile range method to identify the top- n data points with the largest LCOF values as outliers. Finally, it can identify global outliers, local outliers and outlier clusters simultaneously completely without parameter k and top- n . Extensive experiments demonstrate that LCOF achieves the highest average F1 and Acc scores, with a maximum value of 0.99, outperforming most existing algorithms.},
  archive      = {J_ASOC},
  author       = {Rui Pu and Jia Xu and Lijun Yang and Tianshuo Li and Juntao Yang and Jingyi Li and Dongming Tang},
  doi          = {10.1016/j.asoc.2024.112348},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112348},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Coulomb’s law-inspired parameter-free outlier detection algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconstructing representations using diffusion models for
multimodal sentiment analysis through reading comprehension.
<em>ASOC</em>, <em>167</em>, 112346. (<a
href="https://doi.org/10.1016/j.asoc.2024.112346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary challenge in multimodal sentiment analysis (MSA), which utilizes textual, audio, and visual information to analyze speakers&#39; emotions, lies in constructing representation vectors that incorporate both unimodal semantic and multimodal interaction information. While existing research has extensively focused on multimodal fusion strategies, there remains insufficient exploration of the intrinsic potential within concurrently enhancing unimodal and multimodal representations. To address this gap, we introduce two additional steps to traditional three-step MSA: text modality enhancement through the machine reading comprehension (MRC) framework and multimodal representation reconstruction via proposing the diverse diffusion denoising autoencoder (D3AE) module. The MRC queries are integrated to locate sentiment-related prior knowledge, thereby deepening textual semantic understanding from a pretrained language model. Meanwhile, D3AE employs a single-step denoising strategy along with diffusion models across multiple time intervals, enabling efficient reconstruction and enhancement of multimodal representations. Extensive experiments conducted on two benchmark datasets, CMU-MOSI and CMU-MOSEI, validate that our model, MRC-D3AE, achieves state-of-the-art performance. The superiority of our model over existing baselines is primarily attributed to integrating MRC for enhancing text modality and D3AE for reconstructing multimodal representations.},
  archive      = {J_ASOC},
  author       = {Hua Zhang and Yongjian Yan and Zijing Cai and Peiqian Zhan and Bi Chen and Bo Jiang and Bo Xie},
  doi          = {10.1016/j.asoc.2024.112346},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112346},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reconstructing representations using diffusion models for multimodal sentiment analysis through reading comprehension},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Tri-training algorithm based nuclear power systems
semi-supervised fault diagnosis under multiple restricted data
conditions. <em>ASOC</em>, <em>167</em>, 112345. (<a
href="https://doi.org/10.1016/j.asoc.2024.112345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An improved tri-training semi-supervised classification method was proposed as a solution to the restricted data conditions of supervised classification models relying on labeled data, semi-supervised models not considering imbalanced proportion of faults data and the presence of faults data with similar feature or varying severity in existing data-driven nuclear power system fault diagnosis studies. Based on the structure of three identical sub-classifiers in the original tri-training algorithm, data update paths have been added, the updated data selection has been strengthened, and the initial information and permissions of sub-models have been differentiated. The novel method only requires the dataset to meet the Smoothness Assumption, and through multiple strict data update paths, it improves the utilization of unlabeled training data while reducing Pseudo-labeling process errors. The difference in permissions and initial information between sub-classifiers is utilized to improve the training rigor for fault types with smaller sample sizes, in order to alleviate the problem of imbalanced data. Based on a marine nuclear power system secondary circuit model and a widely recognized nuclear power plant dataset, multiple nuclear power system common restricted fault diagnosis datasets faced in ships and power plants were obtained to verify the novel method&#39;s advantages and generalization. The conclusion is that under the same conditions, compared with the original tri-training method and other semi-supervised learning methods, the Accuracy, AUC, Precision Rate and Recall Rate of the novel method have improved by about 21 %, 10 %, 20 %, and 21 %, respectively, and can reduce about 25 % of misjudgments between similar feature faults.},
  archive      = {J_ASOC},
  author       = {Haotong Wang and Yanjun Li and Site Li and Guolong Li and Shengdi Sun and Baozhi Sun and Yuanwei Cao and Jianxin Shi},
  doi          = {10.1016/j.asoc.2024.112345},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112345},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tri-training algorithm based nuclear power systems semi-supervised fault diagnosis under multiple restricted data conditions},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). A dual-sampling based evolutionary algorithm for
large-scale multi-objective optimization. <em>ASOC</em>, <em>167</em>,
112344. (<a href="https://doi.org/10.1016/j.asoc.2024.112344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast search space in large-scale multi-objective optimization represents a significant challenge for evolutionary algorithms to converge towards the Pareto Front. As an effective search strategy, direction-guided sampling technique could improve the search efficiency by exploring along the approximated directions to approach the Pareto set. However, the approximated directions may fail to interact with the true Pareto set and result in inefficient search. To address this issue, a dual-sampling method is proposed in this paper. In addition to the samples along the directions approximated by direction-guided sampling, fuzzy Gaussian sampling is applied to adjust the search direction and generate more accurate and evenly distributed solutions. Moreover, a convergence-and-diversity-based mating selection is introduced to balance the exploration and exploitation. The experiments on 72 test benchmarks with bi- and tri-objectives and 500–5000 decision variables show the superiority of the proposed algorithm compare with the state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Weiwei Zhang and Sanxing Wang and Guoqing Li and Weizheng Zhang and Xiao Wang},
  doi          = {10.1016/j.asoc.2024.112344},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-sampling based evolutionary algorithm for large-scale multi-objective optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Green AI in the finance industry: Exploring the impact of
feature engineering on the accuracy and computational time of machine
learning models. <em>ASOC</em>, <em>167</em>, 112343. (<a
href="https://doi.org/10.1016/j.asoc.2024.112343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As research and practice in Artificial Intelligence (AI) applications rapidly expand, the support for AI deployment is also increasing. While the abundance of data allows for sophisticated feature engineering techniques that can enhance accuracy, it is crucial to highlight both the computational costs and the efficiency with which these models operate. This paper compares the processing time and accuracy of individual and hybrid Machine Learning (ML) models in predicting customer loyalty within financial contexts. Frameworks that incorporate feature engineering and green AI principles are used separately in both individual and hybrid approaches. The individual models are the commonly used regressor-based algorithms applied to business problems. The hybrid models first use k-Means to cluster customers, followed by the application of individual regressor-based models (e.g., decision trees, gradient boosting, and LightGBM). The present results show that using fewer features results in only a marginally lower accuracy compared to models with more features (a difference of ≈ 0 . 01 in MAE when comparing the use of 18 versus 85 features). Additionally, this article clearly demonstrate the trade-off between higher accuracy and longer computational time in hybrid ML models versus lower accuracy and shorter computational time in individual models when predicting customer loyalty. Hybrid models exhibit a lower MSE ( ≈ 0 . 88 ) compared to individual models ( ≈ 0 . 91 ). These findings provide managers with insights on selecting the most appropriate model based on their organization’s specific needs.},
  archive      = {J_ASOC},
  author       = {Marcos R. Machado and Amin Asadi and Renato William R. de Souza and Wallace C. Ugulino},
  doi          = {10.1016/j.asoc.2024.112343},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112343},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Green AI in the finance industry: Exploring the impact of feature engineering on the accuracy and computational time of machine learning models},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Cross-modal scenario generation for stock price forecasting
using wasserstein GAN and GCN. <em>ASOC</em>, <em>167</em>, 112342. (<a
href="https://doi.org/10.1016/j.asoc.2024.112342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The forecasting of stock price has always been a difficult problem, as the various inputs like company performance, technical innovation, political factors are intricate and often assessed with uncertainty. While most of the existing studies belong to deterministic forecast that focused on either the trend or exact value of future stock price, the forecasted results contain little uncertain information, thus bringing certain risks to investors. In this study, an effective scenario generation method based on wasserstein generative adversarial network (WGAN) is proposed for stock price forecasting, which can characterize the temporally and spatially correlations among different stocks, so as to help investors make proper investment strategies in complicated market environments. To the best of our knowledge, the scenario generation of stock price has been rarely studied by existing publications. Specifically, considering the inherent correlation among stocks, graph convolutional network (GCN) is used to capture the features of related stocks, whereafter the features of target stocks and related stocks are combined. In terms of feature fusion, this paper further uses a pre-trained model to obtain the key information of news headlines as text features, and fully integrates them with historical data through the attention mechanism, thus improving the overall performance of scenario generation. In summary, the proposed model can generate promising scenarios for future stock prices over time based on cross-modal information like historical data and news headlines. Numerical experiments show that our method outperforms other baseline methods in terms of root-mean-square error and average absolute percentage error (the average of several scenarios). For each part of the method, stock correlation is the most helpful for the results, followed by cross-modal information, with scenario generation contributing less to the results. Besides, experiments on a real-life portfolio selection problem also demonstrate that our method brings the highest returns, which proves its effectiveness in practical application.},
  archive      = {J_ASOC},
  author       = {Zixu Wang and Bo Wang and You Li and Shu Liu and Huaxiong Li and Junzo Watada},
  doi          = {10.1016/j.asoc.2024.112342},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112342},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-modal scenario generation for stock price forecasting using wasserstein GAN and GCN},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fisher regularized discriminative broad learning system for
visual classification. <em>ASOC</em>, <em>167</em>, 112341. (<a
href="https://doi.org/10.1016/j.asoc.2024.112341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Broad Learning System (BLS) is an innovative learning paradigm with significant success in image classification. However, the 0–1 labeling matrix employed in BLS struggles to align with the true data distribution, limiting the flexibility of the regression objective. The ℓ 2 -norm-based Broad Learning System (L2DBLS) introduces the ɛ -dragging technique to enhance labeling diversity, but the randomness inherent in ɛ -dragging weakens the label correlation within the same category. This paper proposes the Fisher Regularized Discriminative Broad Learning System (FRBLS) to tackle these issues and aims to achieve the following objectives: Firstly, the generated label matrix offers sufficient flexibility to maintain intra-class compactness and inter-class separability. Secondly, the constraints of Fisher Regularization on the same class of features ensure better alignment between samples and labels. Finally, the overall solution process is optimized using an ADAM-based alternating multiplier method, which ensures closed-form solutions at each iteration. Experimental results demonstrate that FRBLS achieves up to 98% accuracy on various face and object datasets, offering superior time efficiency and a 1% improvement in classification performance compared to recent state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Xianghua Li and Jinlong Wei and Junwei Jin and Tao Xu and Dengxiu Yu},
  doi          = {10.1016/j.asoc.2024.112341},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112341},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fisher regularized discriminative broad learning system for visual classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive group sample with central momentum contrast loss
for unsupervised individual identification of cows in changeable
conditions. <em>ASOC</em>, <em>167</em>, 112340. (<a
href="https://doi.org/10.1016/j.asoc.2024.112340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual identification is the basis for accurate breeding and management of dairy cows. Most of the studies on individual identification of dairy cows focused on closed data sets. Meanwhile, the training process requires lots of manual labeling data as support. Aiming at solving the above shortcomings, an unsupervised cow individual identification model named AC-Cowid (Adaptive Group Sample-Central Momentum Contrastive loss-Cowid) was proposed. The innovation of the model lied in the novel sampler and loss function. Adaptive Group Sampler made up for the sampling imbalance problem in the current samplers. The sampler would self-update with model optimization. The central constraint term in Central Momentum Contrastive loss made the updated features more inclined to the cluster center, effectively solved the discomfort of Contrastive loss to cow image data. Feature similarity matching was adopted to meet the challenge of data conversion from a closed set to an open and changeable set. The unsupervised training reduced the workload of manual data labeling. The CMC-1 and mAP index of the final model were 99.50 % and 77.90 %, respectively. Ablation and hyper-parameter experiments proved the effectiveness of the proposed method. The comparison experiment tested eight unsupervised methods and five supervised algorithms in the state-of-the-art. The mAP and CMC-1 of the proposed method was the best among the comparison algorithms. Above all, the proposed method was 4.60 % and 5.38 % higher on mAP and CMC-1 compared with the best algorithm in supervised learning. This study could provide technical support for individual identification of cows in large-scale cattle farms.},
  archive      = {J_ASOC},
  author       = {Yunfei Wang and Xingshi Xu and Shujin Zhang and Yuchen Wen and Liuru Pu and Yongjie Zhao and Huaibo Song},
  doi          = {10.1016/j.asoc.2024.112340},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112340},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive group sample with central momentum contrast loss for unsupervised individual identification of cows in changeable conditions},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instance-wise multi-view visual fusion for zero-shot
learning. <em>ASOC</em>, <em>167</em>, 112339. (<a
href="https://doi.org/10.1016/j.asoc.2024.112339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) has become increasing popular in computer vision due to its ability to recognize categories unobserved in the training data. So far, most existing ZSL approaches adopt visual representations that are either derived from pretrained networks or learned using an end-to-end architecture. However, a single group of visual representations can hardly capture all features hidden in the images, yielding incomplete visual information. In numerous real-life scenarios, multi-view visual representations are often accessible which describe the instances more comprehensively and are potential for better learning performance. In this paper, we introduce an instance-wise multi-view visual fusion (IMVF) for zero-shot learning (ZSL) model. In accordance with the consensus principle, a multi-view visual-semantic mapping is created by minimizing the disparities of seen-class semantic projections on different views. Meanwhile, a straightforward linear constraint is performed on each seen-class instance to adhere to the complementary principle so that the cross-view information exchange is well motivated. In order to mitigate the domain shift problem, the predicted unseen-class semantic projections are further refined through a multi-view manifold alignment under the consensus principle. Our proposed IMVFZSL is compared with the State-of-the-Art ZSL methods on AwA2, CUB and SUN datasets. Exciting experimental results validate the effectiveness of the IMVF mechanism. To the best of our understanding, this is an initial attempt to fuse multi-view visual representations in ZSL, which will stimulate valuable contemplation in this field.},
  archive      = {J_ASOC},
  author       = {Long Tang and Jingtao Zhao and Yingjie Tian and Changhua Yao and Panos M. Pardalos},
  doi          = {10.1016/j.asoc.2024.112339},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112339},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Instance-wise multi-view visual fusion for zero-shot learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Few-shot intent detection with mutual information and
contrastive learning. <em>ASOC</em>, <em>167</em>, 112338. (<a
href="https://doi.org/10.1016/j.asoc.2024.112338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot intent detection is a challenging task. Most existing methods only focus on acquisition of generalization knowledge in known classes, or on the adaptation situation of meta-learning tasks. However, these methods lack both the learning of beneficial information for intent detection and the learning of discriminative representation of intent keywords. Solving these problems can effectively improve the model’s ability to recognize text intent and thus effectively execute user commands. Therefore, in this paper, we propose a few-shot intent detection framework based on Mutual Information and Contrastive Learning (MICL). It improves the robustness of the model learning of intent keywords through mutual information maximization, thus improving the accuracy of intent detection. Additionally, maximization of mutual information enhances the intent representation consistency on two text views, 1 which helps to promote the efficiency of sample contrastive learning. Specifically, we use a neural network to approximately estimate the joint probability distribution and marginal probability distribution of two text views and maximize the mutual information through back-propagation. In order to enable the model to focus on intent keywords in text, we introduce a sample-level contrastive learning. It helps the model to learn discriminative representations of intent keywords by comparing the similarity of text from two text views. In addition, to prevent overfitting of the model on known classes, we introduce a class-level contrastive learning. We regard it as regularization of known classes, aiming to improve the model generalization performance on unknown classes. Our method achieves state-of-the-art performance on four public datasets. 2},
  archive      = {J_ASOC},
  author       = {Shun Yang and YaJun Du and JiaMing Huang and XianYong Li and ShangYi Du and Jia Liu and YanLi Li},
  doi          = {10.1016/j.asoc.2024.112338},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112338},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Few-shot intent detection with mutual information and contrastive learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AttentivECGRU: GRU based autoencoder with attention
mechanism and automated fuzzy thresholding for ECG arrhythmia detection.
<em>ASOC</em>, <em>167</em>, 112337. (<a
href="https://doi.org/10.1016/j.asoc.2024.112337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiograms can reveal irregular cardiac cycles, i.e., arrhythmia and detecting arrhythmia from its morphology is challenging. This article proposes a novel approach for arrhythmia detection using Gated Recurrent Unit based autoencoder with an attention mechanism named AttentivECGRU , followed by fuzzy based threshold selection procedure. The proposed AttentivECGRU autoencoder has three segments namely AttentivECGRU - Encoder , AttentivECGRU - Latent-Attention Space Representation and AttentivECGRU - Decoder which are responsible respectively for generating latent space representation from the input signals, imposing relative score based on the importance of the features and reconstruct the input signals. The proposed method only requires the normal signals in the training phase; however, it can predict both normal and abnormal test signals effectively. Fuzzy based automated threshold selection procedure is also proposed to handle the overlapping nature of the reconstruction loss probability density function of normal and arrhythmia test signals. Results on ECG5000 dataset show that the proposed AttentivECGRU outperformed eleven other state-of-the-art methods, achieving the highest accuracy of 99.14% with Precision 0.9931, Recall 0.9682, and F1-score 0.9804. Confidence Interval tests confirm the statistical superiority of the proposed method over other compared techniques yielding the smallest error rate and error bound. Following the similar trend, the proposed method has produced very high predictive performance on TwoLeadECG dataset as well achieving 98.99% accuracy.},
  archive      = {J_ASOC},
  author       = {Moumita Roy and Anindya Halder and Sukanta Majumder and Utpal Biswas},
  doi          = {10.1016/j.asoc.2024.112337},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112337},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AttentivECGRU: GRU based autoencoder with attention mechanism and automated fuzzy thresholding for ECG arrhythmia detection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Electric vehicle energy consumption prediction for unknown
route types using deep neural networks by combining static and dynamic
data. <em>ASOC</em>, <em>167</em>, 112336. (<a
href="https://doi.org/10.1016/j.asoc.2024.112336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate energy consumption prediction of electric vehicles (EVs) is crucial for drivers considering long trips. All the data should be provided beforehand to determine the energy consumption at the beginning of the trip. Although dynamic vehicle data (vehicle speed, state-of-charge, acceleration, etc.) cannot be known before the trip, factors related to the specified route (route type, elevation, average speed, weather, driving time, etc.) can be used to predict the consumed energy. These factors can be categorized as static and dynamic features, and thus, the question of how to effectively use static and dynamic data arises. This paper investigates the problem of predicting the energy consumption of an EV for a predetermined trip using a deep neural network (DNN) model that effectively uses static features along with dynamic segment features. Furthermore, we address the problem where the route types are unknown in advance. To include more information in the prediction model, we clustered the speed profiles using shape-based clustering with dynamic time warping (DTW) to predict the route type and used the cluster labels as static inputs. Real driving data collected from various drivers of a specific EV were used to train the DNN. The proposed DNN model was compared with the average energy consumption (AEC) model and five machine learning models. The results show that labels obtained from shape-based clustering improved the prediction more than feature-based cluster labels. The prediction errors were minimized with the proposed DNN model, where static features are introduced to the first and second layers twice.},
  archive      = {J_ASOC},
  author       = {Hilal Yılmaz and Betul Yagmahan},
  doi          = {10.1016/j.asoc.2024.112336},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112336},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Electric vehicle energy consumption prediction for unknown route types using deep neural networks by combining static and dynamic data},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving large-scale instances of the urban transit routing
problem with a parallel artificial bee colony-hill climbing optimization
algorithm. <em>ASOC</em>, <em>167</em>, 112335. (<a
href="https://doi.org/10.1016/j.asoc.2024.112335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm Intelligence simulates the collective behavior of decentralized and self-organized swarms. One of the main relevant methods is the Artificial Bee Colony (ABC) algorithm which simulates the foraging behavior of bee swarms in a colony to produce efficient solutions to various problems. The Urban Transit Routing Problem (UTRP) involves finding an efficient set of routes in a transit network to satisfy travel demand subject to operational and budget constraints. It is a complex, NP-Hard problem, in which otherwise correct solutions can be rejected because of impracticability. In this study, a hybrid algorithm consisting of a parallel ABC and Hill Climbing was used to find quality solutions to the UTRP. Thorough comparative results on Mandl’s well-known instance and Mumford’s large-scale instances demonstrate that the proposed algorithm outperforms existing techniques, achieving high levels of direct trip coverage in small computational times. Remarkably, when applied to the most extensive benchmark comprising over 6 million trips and 60 bus routes, the proposed algorithm demonstrates an impressive 11 % enhancement in direct coverage over the previously best-reported results, allowing the design of real-world bus networks in under 3 hours.},
  archive      = {J_ASOC},
  author       = {Alexandros Zervas and Christina Iliopoulou and Ioannis Tassopoulos and Grigorios Beligiannis},
  doi          = {10.1016/j.asoc.2024.112335},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112335},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving large-scale instances of the urban transit routing problem with a parallel artificial bee colony-hill climbing optimization algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). AnFiS-MoH: Systematic exploration of hybrid ANFIS
frameworks via metaheuristic optimization hybridization with
evolutionary and swarm-based algorithms. <em>ASOC</em>, <em>167</em>,
112334. (<a href="https://doi.org/10.1016/j.asoc.2024.112334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive neuro-fuzzy inference system (ANFIS) has shown promising performance in modeling nonlinear problems, leveraging the strengths of both neural networks and fuzzy inference systems. However, as the problem scale increases, the growing number of tunable parameters in ANFIS can make it challenging to optimize via traditional gradient-based methods alone. This study introduces AnFiS-MoH , a novel framework that synergistically integrates ANFIS with metaheuristic optimization algorithms to address these challenges. By leveraging the global search capabilities of metaheuristics such as ant colony optimization (ACO), particle swarm optimization (PSO), genetic algorithm (GA), and simulated annealing (SA), ANFIS-MOH enhances the parameter tuning process of ANFIS models. We evaluate ANFIS-MOH on benchmark datasets including Boston Housing and Wine Quality, demonstrating significant improvements in prediction accuracy and generalization compared to traditional ANFIS and neural network approaches. The proposed framework achieves up to 20% reduction in Mean Squared Error and 15% increase in R 2 scores, particularly excelling in handling high-dimensional, noisy data. This work contributes to the field of hybrid intelligent systems by introducing effective ways to combine the strengths of ANFIS with powerful metaheuristic optimization algorithms. The findings suggest that such hybrid approaches can be effective in tackling challenging nonlinear modeling problems. Our code is available at https://github.com/AmbitYuki/Metaheuristic-Adaptive-ANFIS .},
  archive      = {J_ASOC},
  author       = {Haoyu Wang and Bin Chen and Hangling Sun and Anji Li and Chenyu Zhou},
  doi          = {10.1016/j.asoc.2024.112334},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112334},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AnFiS-MoH: Systematic exploration of hybrid ANFIS frameworks via metaheuristic optimization hybridization with evolutionary and swarm-based algorithms},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Long term 5G base station traffic prediction method based on
spatial-temporal correlations. <em>ASOC</em>, <em>167</em>, 112333. (<a
href="https://doi.org/10.1016/j.asoc.2024.112333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of 5G network management, accurately predicting traffic volumes at base stations remains a critical yet challenging endeavor, primarily due to the complexities inherent in the spatial and temporal data interactions. Current methods often fall short in effectively harnessing long-term trends and spatial interconnections among base stations. To bridge these gaps, this paper introduces the GCformer model, a novel approach that capitalizes on both spatial relationships and temporal patterns for multi-base station traffic prediction. Spatially, the proposed model employs graph convolutional networks to integrate diverse spatial information and construct insightful adjacency matrices that includes Euclidean distances and non-Euclidean distances (area types of base station locations and similarities in traffic flow among various stations), thereby enhancing the predictability of traffic dynamics. Temporally, the application of the Transformer&#39;s attention mechanism enables better capture of long-term relational dependencies in the temporal domain of 5 G base station traffic data. Additionally, a time-variant optimization module is designed to establish diurnal cycle data for each base station&#39;s traffic, replacing the traditional positional encoding with a more nuanced model that improves the learning of historical data correlations. Empirical results from exhaustive case studies confirm the superiority of the GCformer model in forecasting traffic volumes. The GCformer exhibits a 4.01 % improvement in mean squared error and a 3.37 % enhancement in mean absolute error compared to the best-performing baseline model, showcasing its potential to significantly improve operational strategies in 5G networks.},
  archive      = {J_ASOC},
  author       = {Yimeng Shang and Wei Deng and Jianhua Liu and Jian Ma and Yitong Shang and Jingwei Dai},
  doi          = {10.1016/j.asoc.2024.112333},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112333},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Long term 5G base station traffic prediction method based on spatial-temporal correlations},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective genetic algorithm for multi-view feature
selection. <em>ASOC</em>, <em>167</em>, 112332. (<a
href="https://doi.org/10.1016/j.asoc.2024.112332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of features within a view and between views under a unified framework. The MMFS-GA framework demonstrates superior performance and interpretability for feature selection on multi-view datasets in both binary and multiclass classification tasks. The results of our evaluations on nine benchmark datasets, including synthetic and real data, show improvement over the best baseline methods. This work provides a promising solution for multi-view feature selection and opens up new possibilities for further research in multi-view datasets.},
  archive      = {J_ASOC},
  author       = {Vandad Imani and Carlos Sevilla-Salcedo and Elaheh Moradi and Vittorio Fortino and Jussi Tohka and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.asoc.2024.112332},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112332},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective genetic algorithm for multi-view feature selection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel highly efficient alternating direction method of
multipliers for large-scale trimmed concave SVM. <em>ASOC</em>,
<em>167</em>, 112331. (<a
href="https://doi.org/10.1016/j.asoc.2024.112331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) is widely utilized for classification in diverse fields thanks to its superior performance. However, when tackling large-scale SVM problems, it encounters high computational complexity. To address this issue, we develop a novel trimmed concave loss SVM model called L t r i -SVM, which achieves sparsity and robustness simultaneously. The new optimality theory for the nonconvex and nonsmooth L t r i -SVM is developed by our new constructed proximal stationary point. Based on which, a new fast alternating direction method of multipliers (ADMM) with working set and low computational complexity is established for addressing L t r i -SVM. Based on our new established optimality theory, our proposed algorithm will divides the training dataset into two distinct categories: the non-working and the working sets. In every training iteration, the algorithm modifies the parameters associated with the working set, while ensuring that the parameters belonging to the non-working set remain unchanged. This algorithm enables the updating of parameters on smaller datasets, which in turn decreases computational complexity and improves the efficiency of runtime. Further, we prove that our algorithm is global convergence. Numerical experiments have confirmed the excellent performance of our algorithm in terms of accuracy in classification, number of support vector and computational speed, surpassing nine other leading solvers. For instance, when solving the real dataset with over 1 0 7 samples, our algorithm is able to complete the task in just 18.92 s, which is much faster than other solvers that take at least 650.4 s.},
  archive      = {J_ASOC},
  author       = {Huajun Wang and Wenqian Li},
  doi          = {10.1016/j.asoc.2024.112331},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel highly efficient alternating direction method of multipliers for large-scale trimmed concave SVM},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective scheduling for an energy-efficient flexible
job shop problem with peak power constraint. <em>ASOC</em>,
<em>167</em>, 112330. (<a
href="https://doi.org/10.1016/j.asoc.2024.112330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controlling electricity costs, which are closely related to the peak power reached, while simultaneously reducing completion time and energy consumption, is crucial for achieving sustainability in the manufacturing industry. However, most existing research on energy-efficient flexible job shop scheduling problems primarily focuses on optimizing completion time and energy consumption, often neglecting the critical aspect of controlling electricity costs. With decreasing resources and increasing energy prices, it is necessary to control electricity costs by limiting the peak power reached during production. To address this gap, this paper investigates an energy-efficient flexible job shop scheduling problem with peak power constraint considering setup and transportation time (EEFJSSP-PPST). We establish a mathematical model for EEFJSSP-PPST and propose an improved non-dominated sorting genetic algorithm II (INSGA-II) incorporating several key improvements. We first introduce three scheduling rules in decoding to optimize objectives and employ heuristic rules to generate a high-quality initial population. Then, we propose an innovative cluster crossover method to accelerate convergence and design an adaptive-based local optimization strategy to further enhance performance. Finally, our experiments explore the impacts of different degrees of the peak power constraint on objectives and evaluate the effectiveness of the three scheduling rules. Additionally, the performance of INSGA-II is tested using 135 benchmark instances. The results indicate that INSGA-II outperformed its variations without improvements, achieving the best hypervolume (HV) indicator in 80% of the instances and the best inverted generational distance (IGD) indicator in 68.89% of the instances. Compared to other state-of-the-art algorithms, INSGA-II achieves the best HV and IGD indicators in 82.2% and 81.5% of the instances, respectively, and demonstrates better convergence and a superior Pareto front. Therefore, we conclude that the improvements in INSGA-II are effective and enhance the algorithm’s performance, making it outstanding in solving EEFJSSP-PPST.},
  archive      = {J_ASOC},
  author       = {Jianhua Wang and Chuanyu Wu and Yongtao Peng},
  doi          = {10.1016/j.asoc.2024.112330},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective scheduling for an energy-efficient flexible job shop problem with peak power constraint},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of deep learning for automatic detection of
table tennis balls from an intelligent serving machine. <em>ASOC</em>,
<em>167</em>, 112329. (<a
href="https://doi.org/10.1016/j.asoc.2024.112329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Intelligent Table Tennis Serving Machine (TTSM) has revolutionized table tennis training by simulating the variability and precision of human opponents. However, current systems lack the capability to track and analyze ball landing points, limiting comprehensive performance evaluation. To address this, we propose a novel approach utilizing YOLOv8 as the benchmark model for detecting table tennis balls launched by a TTSM, tailored to meet the demands of intelligent training systems. Our key innovation involves integrating a Res2Net architecture enhanced with a Non-local Attention Module (NLAM) and a Dilated Atrous Spatial Pyramid Pooling (DASPP) unit into the neck network. This design improves the network’s ability to integrate multi-scale local features, thereby enhancing global information processing. The DASPP module, with adaptive dilation convolution, ensures thorough context comprehension. Additionally, we incorporate an Omni-dimensional Dynamic Convolution (ODConv) module in the detection head, employing parallel strategies to learn diverse and complementary attention cues, further enhancing detection accuracy and robustness. To validate our method, we conducted evaluations using the publicly available OpenTTGames dataset and a self-constructed dataset comprising over 10,000 table tennis ball images captured in various environments. Experimental results indicate that our method enhances mAP0.5, Precision, and Recall by 2.3%, 5.1%, and 0.9% on the OpenTTGames dataset, and by 3%, 2%, and 1.6% on the self-built dataset, respectively. Finally, we describe the optimization process of the intelligent TTSM system, which leverages keyframe analysis of ball landing points to provide feedback, thereby enabling continuous performance improvement in subsequent phases.},
  archive      = {J_ASOC},
  author       = {Tao Ning and Meng Fu and Yuzhe Wang and Xiaodong Duan and Mohammad Zoynul Abedin},
  doi          = {10.1016/j.asoc.2024.112329},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of deep learning for automatic detection of table tennis balls from an intelligent serving machine},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dissolved oxygen levels prediction method based on
single-hidden layer feedforward neural network using neighborhood
information metric. <em>ASOC</em>, <em>167</em>, 112328. (<a
href="https://doi.org/10.1016/j.asoc.2024.112328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluctuations in dissolved oxygen levels directly impact prawn growth and development, serving as a crucial indicator for monitoring water quality in shrimp ponds. Analyzing and predicting dissolved oxygen content changes due to the intricate interplay of multiple feature parameters is challenging. Current prediction models for dissolved oxygen must adequately consider the interactions among feature parameters during data processing to enhance the accuracy of dissolved oxygen predictions. We propose a technique using neighborhood information entropy to measure the uncertainty of correlation, redundancy, and interaction between factors and dissolved oxygen. A prediction model (FSNRS-SFNN) has been developed, based on a single-hidden layer feedforward neural network, utilizing neighborhood information metrics to forecast dissolved oxygen levels in shrimp ponds and optimize shrimp culture practices. Compared with multiple linear and ridge regression models, the FSNRS-SFNN model exhibits the smallest mean squared error and mean absolute error and the highest coefficient of determination ( R 2 ). The predictive impact of dissolved oxygen on shrimp pond management is significant. Therefore, the FSNRS-SFNN model holds excellent potential for widespread application in predicting dissolved oxygen levels in shrimp ponds.},
  archive      = {J_ASOC},
  author       = {Yongming Luo and Jingjing Hu and Gangqiang Zhang and Pengfei Zhang and Ying Xie and Zhaomin Kuang and Xingji Zeng and Shushi Li},
  doi          = {10.1016/j.asoc.2024.112328},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dissolved oxygen levels prediction method based on single-hidden layer feedforward neural network using neighborhood information metric},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Handling imbalanced data in intrusion detection using time
weighted adaboost support vector machine classifier and crossover
boosted dwarf mongoose optimization algorithm. <em>ASOC</em>,
<em>167</em>, 112327. (<a
href="https://doi.org/10.1016/j.asoc.2024.112327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity threats pose a serious challenge in the present day and age, and Intrusion Detection Systems (IDS) have emerged as an effective solution to counter these threats. In this paper, a novel IDS is proposed that captures data from the NSL-KDD dataset and are preprocessed. The Kernel Principal Component Analysis (KPCA) model extracts features presented in the data, and the Crossover Boosted Dwarf Mongoose Optimization (CDMO) algorithm selects the relevant features for classification. The CDMO algorithm offers the advantages of improving exploitation, providing optimal solutions, and balancing global exploitation and local search capabilities. The selected features are classified into five classes using the Time Weighted Adaboost Support Vector Machine (TWASVM) classifier. The TWASVM classifier effectively handles imbalanced data and delivers high-performance results. Experiments conducted on MATLAB R2019a and the proposed model achieved an higher accuracy of 98.6 % and less time complexity of 13 seconds. Comparative analysis demonstrated that the proposed IDS outperforms other state-of-the-art methods. The advantages of CDMO algorithm include improved exploitation, optimal solutions, and a balanced crossover strategy for global exploitation and local search capabilities. The advantages of the TWASVM classifier include the ability to handle imbalanced data and deliver high-performance results. Overall, the proposed IDS offer a novel solution to the challenges of intrusion detection in a rapidly evolving cybersecurity landscape.},
  archive      = {J_ASOC},
  author       = {Hemalatha Chandrasekaran and Kanipriya Murugesan and Suja Cherukullapurath Mana and Bhagavathi Kannu Uma Anu Barathi and Sumathy Ramaswamy},
  doi          = {10.1016/j.asoc.2024.112327},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Handling imbalanced data in intrusion detection using time weighted adaboost support vector machine classifier and crossover boosted dwarf mongoose optimization algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion for a robust traffic accident assistance
forecasting model with deep learning. <em>ASOC</em>, <em>167</em>,
112326. (<a href="https://doi.org/10.1016/j.asoc.2024.112326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millions of people are involved in traffic accidents each year. According to the World Health Organization (WHO), approximately 1.19 million people experience fatal outcomes, while 20 to 50 million suffer non-fatal consequences that may affect them for the rest of their lives. Multiple studies suggest that one of the main causes of this high number of affected individuals could be mitigated through the rapid intervention of medical assistance services. This paper thus proposes a general GTAAF model based on deep learning that predicts the severity of traffic accidents in real-time based on the surrounding characteristics, determining whether an accident requires medical assistance, using tabular data as input. This model can be utilized by emergency services to prioritize the allocation of medical resources in any region of the world. The main difference from existing state-of-the-art methods is that these are focused on specific localities and so cannot be applied universally, because each existing method is applied in a specific region of the world due to the individual characteristics of these data. To demonstrate this generalization capability, the GTAAF model was compared with six other state-of-the-art models applied to 8 populations in 3 different countries, showing a significant quality advantage over the other models (up to a 13.8% improvement in the F1-Score metric in classifying accidents that require assistance in the best case, and up to a 6.5% improvement in accidents that do not require assistance).},
  archive      = {J_ASOC},
  author       = {Luis Pérez-Sala and Manuel Curado and Jose F. Vicent},
  doi          = {10.1016/j.asoc.2024.112326},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature fusion for a robust traffic accident assistance forecasting model with deep learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remaining useful life prediction of aero-engine via temporal
convolutional network with gated convolution and channel selection unit.
<em>ASOC</em>, <em>167</em>, 112325. (<a
href="https://doi.org/10.1016/j.asoc.2024.112325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a hot spot in the prognostics and health management (PHM), predicting the remaining useful life (RUL) is significant to ensure the availability and reliability of industrial systems. Data-driven methods, such as the traditional temporal convolutional networks (TCNs), have been widely used for RUL prediction. However, traditional TCNs lack effective means to sufficiently extract both the spatial and temporal features of multi-sensory data. Furthermore, the prediction ability of traditional TCNs is constrained by their inflexible network structure. Here, an integrated model called the improved temporal convolutional network with gated convolution and channel selection unit (GCSU-ITCN) is proposed as a solution to the above problems. First, “the loss boundary to mapping ability” (LM) method is introduced to select sensors with superior mapping ability to the RUL. It contributes to the extraction of features from multi-sensory data. Then, the gated convolution and channel selection unit (GCSU) is designed to extract both local and global features of multi-sensory data, encompassing the spatial and temporal dimensions. Last, the improved temporal convolutional network (ITCN) is developed to capture long temporal correlations within the extracted features. The ITCN has a flexible structure that enables it to learn deep temporal features more effectively than traditional TCNs. A series of comparative experiments is conducted on the C-MAPSS dataset to evaluate the prediction capability of the GCSU-ITCN. Furthermore, ablation experiments are also conducted to assess the contribution of each component within the model.},
  archive      = {J_ASOC},
  author       = {Fanfan Gan and Yujie Qin and Baizhan Xia and Dong Mi and Lizhang Zhang},
  doi          = {10.1016/j.asoc.2024.112325},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Remaining useful life prediction of aero-engine via temporal convolutional network with gated convolution and channel selection unit},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Novel imbalanced multi-class fault diagnosis method using
transfer learning and oversampling strategies-based multi-layer support
vector machines (ML-SVMs). <em>ASOC</em>, <em>167</em>, 112324. (<a
href="https://doi.org/10.1016/j.asoc.2024.112324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For health monitoring and fault diagnosis of critical mechanical system components, historical data related to equipment failures are often limited and exhibit varying imbalanced multi-class characteristics (e.g., with noisy and time-series data). Moreover, fault diagnosis frameworks based on traditional resampling algorithms (e.g., SMOTE) mostly heavily rely on manual feature extraction, making them difficult to adapt to diverse working conditions or objects. To address these challenges, we propose a novel end-to-end imbalanced multi-class fault diagnosis architecture using transfer learning and oversampling strategies-based multi-layer support vector machines (ML-SVMs). ML-SVMs utilize a VGG-based deep migration feature extraction method to extract features from original time-domain vibration signals, employing natural source domain weights to reduce dependence on human experience and sample size. Then, ML-SVMs introduce ISCOTE (i.e., the first and second layers of ML-SVMs), an improved version of the sample-characteristic over-sampling technique (SCOTE). ISCOTE generates more effective and reasonable samples for each fault class through a scaling factor and iterative optimization mechanism, whether in noisy feature spaces with fuzzy boundaries or in clear boundary feature spaces. Finally, in the third layer of ML-SVMs, multi-class SVMs (e.g., LS-SVMs and standard SVMs) are utilized to train balanced feature samples and derive classification models with strong generalization ability. The effectiveness of ML-SVMs is demonstrated through 16 fault diagnosis instances using CWRU and IMS bearing data, PHM 2010 and TTWD tool wear data. Results indicate that ML-SVMs outperform 8 well-known oversampling-based algorithms in fault diagnosis recognition rates and algorithm robustness. It has offered a feasible architecture for multi-class imbalanced fault scenarios with limited data and multiple adverse features.},
  archive      = {J_ASOC},
  author       = {Jianan Wei and Hualin Chen and Yage Yuan and Haisong Huang and Long Wen and Weidong Jiao},
  doi          = {10.1016/j.asoc.2024.112324},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Novel imbalanced multi-class fault diagnosis method using transfer learning and oversampling strategies-based multi-layer support vector machines (ML-SVMs)},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-phase adaptive large neighborhood search algorithm for
the electric location routing problem with range anxiety. <em>ASOC</em>,
<em>167</em>, 112323. (<a
href="https://doi.org/10.1016/j.asoc.2024.112323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to environmental pollution and global warming, electric vehicles (EVs) have been widely applied in supply chain, and lots of researches have been focused on the electric location routing problem (ELRP) to optimize the location of battery-swapping stations (BSSs). However, few studies have simultaneously considered the EV’s collaborative behavior and range anxiety in ELRP. Therefore, this study focuses on formulating an ELRP with battery-swapping stations considering both collaborative behavior and range anxiety. The model takes into account the collaboration between the supply and demand sides to share supply chain resources effectively. The detour probability function is employed to handle the uncertainty caused by drivers’ range anxiety during the trip, which can be evaluated through a proposed range anxiety function. In addition, a new two-dimensional matrix-based solution representation is proposed to explore ELRP optimal solutions intuitively and efficiently. To solve the proposed model effectively, a two-phase adaptive large neighborhood search (TALNS) algorithm that integrates the adaptive large neighborhood search (ALNS) algorithm and the extended binary particle swarm optimization (EBPSO) algorithm is proposed. In the EBPSO algorithm, a new position update mechanism and local search strategy are used to strengthen the local search ability. In the ALNS algorithm, multiple destroy and repair operators are developed for the proposed model. Further, to validate the effectiveness and performance of the proposed algorithm, comparison experiments with the Gurobi optimizer and other baseline heuristic algorithms are conducted.},
  archive      = {J_ASOC},
  author       = {Shuai Zhang and Jieman Xia and Qinjie Chen and Wenyu Zhang},
  doi          = {10.1016/j.asoc.2024.112323},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-phase adaptive large neighborhood search algorithm for the electric location routing problem with range anxiety},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A guided twin delayed deep deterministic reinforcement
learning for vaccine allocation in human contact networks.
<em>ASOC</em>, <em>167</em>, 112322. (<a
href="https://doi.org/10.1016/j.asoc.2024.112322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript introduces an innovative approach to optimizing the distribution of a limited vaccine resource within a population modeled as a contact network, aiming to mitigate the spread of infectious diseases. The study develops a novel methodology that combines reinforcement learning and graph neural networks. To understand the dynamics of disease propagation, the study constructs an analytical model that outlines conditions for disease eradication or endemic states. This model supports a series of simulation experiments across various scenarios, demonstrating the proposed method’s superiority over random and centrality-based approaches in reducing the average number of infections per individual during an outbreak. The adaptability of the proposed method is further emphasized by its robust performance across networks of diverse sizes and configurations, highlighting its real-world applicability. The findings of this study have significant implications for public health policy and resource allocation, offering a promising framework for managing infectious disease outbreaks in complex and dynamic environments.},
  archive      = {J_ASOC},
  author       = {Ehsan Ardjmand and Alireza Fallahtafti and Ehsan Yazdani and Anwar Mahmoodi and William A. Young II},
  doi          = {10.1016/j.asoc.2024.112322},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A guided twin delayed deep deterministic reinforcement learning for vaccine allocation in human contact networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient prediction uncertainty quantification in dam
behavior monitoring with attention-based sequence-to-sequence learning.
<em>ASOC</em>, <em>167</em>, 112321. (<a
href="https://doi.org/10.1016/j.asoc.2024.112321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Displacement is an intuitive monitoring indicator of dam structural behavior. Conventional deterministic modeling methods frequently disregard the inherent uncertainty associated with monitoring data. This study aims to quantify such uncertainties within dam displacement predictions by augmenting a sequence-to-sequence structure with the novel recurrent unit, termed as tiny gated unit (TGU), attention mechanism, as well as quantile regression, resulting in the Att-S2STQ model. Specifically, TGU adopts only one gate to identify nonlinearity in data sequences, while Bahdanau attention mechanism dynamically assigns weights to different parts of the input sequence. The inclusion of quantile regression allows Att-S2STQ to produce prediction intervals (PIs), from which the probability density functions (PDFs) and cumulative distribution functions (CDFs) are further derived via kernel density estimation. PIs, PDFs, and CDFs jointly serve to reveal the prediction uncertainty. The effectiveness of the model is illustrated through comparative experiments using real-world dam monitoring datasets. Results indicate that the proposed model outperforms traditional methods in point, interval, and probability predictions, while also having a simpler structure and faster training. The superiority of both accuracy and efficiency makes it a valuable tool for dam management, aiding in data-driven decision-making and enhancing operational safety.},
  archive      = {J_ASOC},
  author       = {Minghao Li and Qiubing Ren and Mingchao Li and Yun Chen and Xiaocui Ji and Hao Liu},
  doi          = {10.1016/j.asoc.2024.112321},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient prediction uncertainty quantification in dam behavior monitoring with attention-based sequence-to-sequence learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing driver attention and road safety through
EEG-informed deep reinforcement learning and soft computing.
<em>ASOC</em>, <em>167</em>, 112320. (<a
href="https://doi.org/10.1016/j.asoc.2024.112320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a transformative edge computing-based approach for enhancing driver attention and road safety using EEG-driven deep reinforcement learning (DRL). As driver inattention remains a significant factor in accidents, real-time cognitive state monitoring enabled by in-vehicle edge devices offers new promise. Our method leverages EEG data collected from drivers using headsets, analyzing signals related to visual attention. Edge computing resources in the vehicle extract features and classify attention levels using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) models trained to approximate optimal driving decisions. A novel reward structure combining driving performance and attention guides the models. Our edge computing-powered framework reacts within critical time latencies to maximize attention through interventions adapting to the driving environment. Results demonstrate the effectiveness of this approach, with PPO agent on edge devices achieving high average rewards up to 489,752.4 and 99.3% reward as accuracy in classifying attention states, thereby significantly outperforming traditional methods. This underscores edge computing’s potential to enable real-time integration of neuroscience and AI, advancing road safety. The edge resources deliver time-critical analysis and adaptation, while connectivity to the fog and cloud allows optimizing and learning at scale across populations. This research pioneers a new epoch for road safety powered by edge intelligence.},
  archive      = {J_ASOC},
  author       = {Muhammad Yousaf and Muhammad Farhan and Yousaf Saeed and Muhammad Jamshaid Iqbal and Farhan Ullah and Gautam Srivastava},
  doi          = {10.1016/j.asoc.2024.112320},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing driver attention and road safety through EEG-informed deep reinforcement learning and soft computing},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive fuzzy coordinated control design for wind turbine
using gray wolf optimization algorithm. <em>ASOC</em>, <em>167</em>,
112319. (<a href="https://doi.org/10.1016/j.asoc.2024.112319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the randomness and intermittency of wind speed, the actual output power curve of a wind turbine (WT) deviates greatly from the theoretical power curve, thereby reducing the power generation capacity of the WT. An adaptive fuzzy coordinated control (AFCC) of WT is presented in this study to improve the power generation of WT. Firstly, a multi-objective optimization model (MOOM) for WT output power, generator speed and pitch angle is established, and its optimal solution set is used as the input eigenvector of a novel effective wind speed soft sensor (NEWSSS) model, which is modeled with kernel extreme learning machine (KELM). Secondly, a novel improved gray wolf optimization (NIGWO) algorithm is presented by improving the convergence factor and adaptive weights, which is used to solve MOOM and optimize the parameters of KELM. A variable pitch control (VPC) is designed by estimating the effective wind speed. Finally, an adaptive fuzzy control (AFC) is presented for WT. Based on the AFC and VPC, an AFCC for pitch angle and generator torque is designed for WT. The high measuring precision of NEWSSS and the good robustness and dynamic performance of AFCC are demonstrated by the simulation results.},
  archive      = {J_ASOC},
  author       = {Bangjun Lei and Shumin Fei},
  doi          = {10.1016/j.asoc.2024.112319},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive fuzzy coordinated control design for wind turbine using gray wolf optimization algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Granular computing-based time series anomaly pattern
detection with semantic interpretation. <em>ASOC</em>, <em>167</em>,
112318. (<a href="https://doi.org/10.1016/j.asoc.2024.112318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series analysis may suffer from the “curse of dimensionality” due to its high-dimensionality characteristics. In terms of this issue, information granulation offers an effective vehicle to process time series at a higher level of abstraction. To take this advantage, this study conducts an interpretable time series anomaly pattern detection under the framework of granular computing, where each time series is granulated into a series of semantics according to its patterns and the anomaly ones are identified based on the obtained semantics. First, the time series is partitioned into a predefined number of segments and trend-based information granules are formed for the data points in each time interval. Guided by the principle of justifiable granularity, the granular results maintaining the main features of the original time series realize informative feature extraction and meaningful dimensionality reduction. Then, to realize semantic anomaly pattern detection, the Axiomatic Fuzzy Set (AFS) theory is generalized to construct and compute with semantic time series, and an AFS-based anomaly score is proposed to discover anomaly patterns. In the experiments, the proposed method is conducted on both UCR data sets and real-world time series, where the detected anomaly patterns are equipped with well-defined semantics.},
  archive      = {J_ASOC},
  author       = {Hongyue Guo and Yashuang Mu and Lidong Wang and Xiaodong Liu and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2024.112318},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Granular computing-based time series anomaly pattern detection with semantic interpretation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fermatean trapezoidal fuzziness average aggregation scheme
for selection of infant clothing by group decision-making.
<em>ASOC</em>, <em>167</em>, 112317. (<a
href="https://doi.org/10.1016/j.asoc.2024.112317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The textile industry has gradually increased its use of various chemicals and the quantity of residues produced. Newborn baby apparel should be made of safe materials since it often comes into touch with them. Infant cloth selection is determined via group decision-making in Fermatean trapezoidal fuzzy number (FTFN). This study provides a new score and accuracy function and weighted, ordered, and hybrid averaging aggregation operators on FTFN. The properties of the proposed averaging aggregation operators on FTFN are presented. An algorithm for the multi-attribute group decision-making (MAGDM) approach will be proposed by applying the proposed averaging aggregation operators. The case studies concentrate on choosing newborn clothing with reduced chemical content within the textile sector. The presentation of the sensitivity analysis of the criterion weights is conducted to guarantee the durability and stability of the framework that has been implemented. Furthermore, a comprehensive analysis compares the novel framework with previous models, highlighting its superior performance.},
  archive      = {J_ASOC},
  author       = {Vijayakumar R. and G.S. Mahapatra and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2024.112317},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fermatean trapezoidal fuzziness average aggregation scheme for selection of infant clothing by group decision-making},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A generalized lorenz system-based initialization method for
deep neural networks. <em>ASOC</em>, <em>167</em>, 112316. (<a
href="https://doi.org/10.1016/j.asoc.2024.112316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) are a powerful tool for solving complex problems. The effectiveness of DNNs largely depends on the initialization technique used. This research develops a new initialization method for DNNs that uses chaotic sequences from the generalized Lorenz system to improve their performance. The proposed method, termed Generalized Lorenz Initialization (GLI), has been compared with two established initialization methods (Kaiming and Xavier) across four different DNN architectures: Informer, Neural Basis Expansion Analysis for Interpretable Time Series, Long Short-Term Memory, and NeuRewriter. The performance of these methods has been evaluated on seven time series forecasting datasets and one combinatorial optimization dataset. Results show that the GLI method improved forecasting accuracy by up to 86.47% compared to the Kaiming method and 88.86% compared to the Xavier method across all time series datasets. For the combinatorial optimization task, the GLI method reduced computational time by up to 9.24% with the better solution quality. These indicate the superiority of the GLI method over the two representative initialization methods for different DNN architectures across different problem domains.},
  archive      = {J_ASOC},
  author       = {Bowen Jia and Zhaoxia Guo and Tao Huang and Feng Guo and Huyu Wu},
  doi          = {10.1016/j.asoc.2024.112316},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generalized lorenz system-based initialization method for deep neural networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage deep image restoration network with application to
single image shadow removal. <em>ASOC</em>, <em>167</em>, 112315. (<a
href="https://doi.org/10.1016/j.asoc.2024.112315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a two-stage deep learning-based image restoration network and its application to remove shadow information from a single image, named by ESCNet (Encoder-decoder based Shadow removal with Colorization Network). Most existed single image-based shadow removal methods may suffer from that the shadow contains multiple regions of different colors or rich image details. To tackle with the problems, our key idea is to first remove shadow(s) from an image followed by repainting the shadow-removed region(s) in this image. To accomplish this, we present a deep two-stage network, cascading a shadow removal network (SRN) and a colorization network (CN). The presented encoder-decoder-based SRN with fusion of global and local feature information is used to remove the shadow(s) in the grayscale domain of the input image while recovering the image details for the shadow-removed region(s). Then the proposed CN aims at repainting the removed shadow region(s) via re-colorization. The proposed deep model has been well trained and well evaluated on the two well-known public datasets, i.e., ISTD (Image Shadow Triplets Dataset) and SRD (Shadow Removal Dataset). Experimental results have shown that the proposed method outperforms the compared state-of-the-art (SOTA) shadow removal approaches quantitatively and qualitatively.},
  archive      = {J_ASOC},
  author       = {Chia-Hung Yeh and Zhi-Xiang Zhan and Li-Wei Kang},
  doi          = {10.1016/j.asoc.2024.112315},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage deep image restoration network with application to single image shadow removal},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework of integrated differential evolution variants
based on adaptive relay mode for global optimization. <em>ASOC</em>,
<em>167</em>, 112314. (<a
href="https://doi.org/10.1016/j.asoc.2024.112314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) is highly competitive in single-objective real parameter optimization. However, there are still some problems with differential evolutionary variants in optimizing complex multimode functions, such as poor solution accuracy and slow convergence. To address these problems, an optimization framework of integrated DE variants based on adaptive relay mode ( f DE-ARM) is proposed. In this framework, different DE variants are integrated through two adaptive relay mechanisms to give full play to the advantages of the algorithms, thereby improving the performance of the whole algorithm. For the first adaptive relay mode, when the currently executed algorithm is judged to have converged, the population is updated by Gaussian random walk, and then the optimization is continued through the relay algorithm. For the second adaptive relay mode, the relay condition is determined by the average fitness improvement rate. If the condition is met, the relay algorithm takes over the optimization. At the same time, if the diversity is lower than the threshold, to improve the exploration, the roulette wheel method is used to select some individuals to perform the Gaussian random walk. In addition, a feedback mechanism is introduced in the relay process to avoid false switching. To verify the performance of the proposed algorithm, extensive simulations are performed on CEC2005, CEC2014, CEC2017, and CEC2021 benchmark functions. In addition, three engineering problems are used to test the performance. Compared with some newly proposed optimization algorithms, f DE-ARM is statistically superior to the comparison algorithms in terms of solution accuracy, convergence speed, and stability.},
  archive      = {J_ASOC},
  author       = {Yongjun Sun and Tingting Sun and Zujun Liu},
  doi          = {10.1016/j.asoc.2024.112314},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A framework of integrated differential evolution variants based on adaptive relay mode for global optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple fairness criteria in decision tree learning.
<em>ASOC</em>, <em>167</em>, 112313. (<a
href="https://doi.org/10.1016/j.asoc.2024.112313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of algorithmic decision-making systems based on machine learning models has led to a need for fair (unbiased) and explainable classification outcomes. In particular, machine learning algorithms can encode biases, which might result in discriminatory decisions for certain groups such as gender, race, or age. Although a number of works on decision tree learning have been proposed to decrease the chance of discrimination, they usually focus on the use of a single fairness metric. In general, creating a model based on a single fairness metric is not a sufficient way to mitigate discrimination since bias can originate from various sources—e.g., the data itself or the optimization process. In this paper, we propose a novel decision tree learning process that utilizes multiple fairness metrics to address both group and individual discrimination. This is achieved by extending the attribute selection procedure to consider not only information gain but also gain in fairness. Computational experiments on fourteen different datasets with various sensitive features demonstrate that the proposed Fair-C4.5 models improve fairness without a loss in predictive accuracy when compared to the well-known C4.5 and the fairness-aware FFTree algorithms.},
  archive      = {J_ASOC},
  author       = {Meryem Bagriacik and Fernando E.B. Otero},
  doi          = {10.1016/j.asoc.2024.112313},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple fairness criteria in decision tree learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preference learning based on adaptive graph neural network
for multi-criteria decision support. <em>ASOC</em>, <em>167</em>,
112312. (<a href="https://doi.org/10.1016/j.asoc.2024.112312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent decision-making assists decision-makers (DMs) in making choices through data analysis, model prediction, and automated processes. Central to this field are two key concepts: multi-criteria decision making (MCDM) and preference learning (PL). While MCDM and PL both aim to develop decision models that rank alternatives based on observed or revealed preferences, they diverge in focus. MCDM concentrates on the DMs&#39; perspectives, whereas PL emphasizes model-driven approaches. This divergence presents significant challenges in integrating these methodologies, particularly in ensuring the integrated method remains scalable and interpretable amidst the complexity of decision scenarios. To bridge this gap, our study introduces the use of graph structures to frame decision problems and proposes a novel PL method employing graph neural network (GNN) for multi-criteria decision support. This method is anchored in the Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) technique and combines an adaptive GNN model with a weight determination model. The GNN model updates embeddings from the alternative&#39;s criteria and category features, utilizing an attention mechanism to adaptively assess their importance. Concurrently, the weight determination model contains a weight neural network module to set objective criterion weights and a game theory-based module for calculating combined criterion weights. The method not only inherits the interpretability and intuitive appeal of decision models but also leverages the computational efficiency and high accuracy of machine learning. In experiments conducted on benchmark datasets, our method exhibits significant performance improvements, especially in ranking-related evaluation metrics, outperforming the best baseline by 5.78 %.},
  archive      = {J_ASOC},
  author       = {Zhenhua Meng and Rongheng Lin and Budan Wu},
  doi          = {10.1016/j.asoc.2024.112312},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112312},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preference learning based on adaptive graph neural network for multi-criteria decision support},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new framework for ultra-short-term electricity load
forecasting model using IVMD–SGMD two–layer decomposition and
INGO–BiLSTM–TPA–TCN. <em>ASOC</em>, <em>167</em>, 112311. (<a
href="https://doi.org/10.1016/j.asoc.2024.112311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of electricity loads is crucial for the development of electricity scheduling and supply services. With the increase in distributed electricity energy sources and the complexity of electricity systems, the strong volatility of load changes brings more challenges to the reliability of load forecasting. Therefore, we propose a novel ultra-short-term electricity load forecasting model using improved two-layer decomposition and an improved deep learning model with temporal pattern attention based on improved northern goshawk optimization (INGO). First, the load data were decomposed thoroughly using the two-layer decomposition model of improved variational mode decomposition (IVMD) with parameter optimization by INGO and symplectic geometry mode decomposition (SGMD) to improve the interpretability of the subsequences. Subsequently, INGO is used to optimize the parameters in bidirectional long short-term memory (BiLSTM). Temporal pattern attention (TPA) is added to BiLSTM, which extracts complex relationships from the hidden neurons of BiLSTM and selects relevant information from different time scales. After predicting and reconstructing the subsequences, the temporal convolutional network (TCN) prediction model is used to perform error correction to improve the final prediction accuracy. Because many government reports and policy information are summarized and published quarterly, to provide information support, we divide the electricity load datasets of two countries by quarters before forecasting. By performing multiple sets of experiments on the two datasets, it is demonstrated that the proposed model has high precision and robustness, and the obtained ultra-short-term electricity load forecasting results can accurately fit the load fluctuation trend.},
  archive      = {J_ASOC},
  author       = {Xiwen Cui and Xiaodan Zhang and Dongxiao Niu},
  doi          = {10.1016/j.asoc.2024.112311},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new framework for ultra-short-term electricity load forecasting model using IVMD–SGMD two–layer decomposition and INGO–BiLSTM–TPA–TCN},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interval neural network-based caputo fractional-order
extreme learning machine applied to classification. <em>ASOC</em>,
<em>167</em>, 112310. (<a
href="https://doi.org/10.1016/j.asoc.2024.112310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, interval-valued data is processed using an interval neural network (INN) based on a fractional-order extreme learning machine (FOELM). The traditional gradient-based algorithms of INN learn slowly. Extreme learning machine (ELM) achieves optimal weights for the output layer in a single calculation, which is faster than iterative training algorithms. However, due to the nonlinear constraints of INN, ELM cannot compute the output layer weights through the Moore–Penrose generalized inverse. Therefore, FOELM is proposed to train the INN in this paper, which ensures faster learning and significant performance. Specifically, in a single hidden layer INN, the hidden layer weights are randomly generated and fixed, and the output layer weights are trained by the fractional-order gradient descent method (FGDM). The experimental results indicate that FOELM exhibits faster convergence and superior classification performance compared to alternative algorithms.},
  archive      = {J_ASOC},
  author       = {Yuanquan Liu and Qiang Shao and Yan Liu and Dakun Yang},
  doi          = {10.1016/j.asoc.2024.112310},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interval neural network-based caputo fractional-order extreme learning machine applied to classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The physical information LSTM surrogate model for
establishing a digital twin model of reciprocating air compressors.
<em>ASOC</em>, <em>167</em>, 112309. (<a
href="https://doi.org/10.1016/j.asoc.2024.112309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reciprocating air compressors play a crucial role in industrial production processes. However, due to the complex structure and long operating time of reciprocating air compressors, real-time monitoring to grasp the operating status of reciprocating air compressors has become particularly important. Digital twin is a technology that can reflect the behavior of physical entities in real time, accurately predicting and evaluating the operation status of reciprocating air compressors. However, the establishment of a digital twin model for reciprocating air compressors requires a significant amount of computational resources, which can result in the inability to meet the requirements of real-time performance evaluation. To overcome this limitation, this paper proposes a method for constructing a digital twin model of reciprocating air compressors based on a surrogate model. The surrogate model is constructed based on a long short-term memory neural network with physical information(PILSTM). This model can accurately describe the changes in cylinder pressure by combining physical information. According to the characteristics of cylinder pressure changes, regularization formulas are added to ensure the smoothness of the predicted pressure. The experimental results show that the digital twin model based on the surrogate model has high prediction accuracy and real-time performance. Therefore, this model provides a new method for monitoring the operating status of reciprocating air compressors.},
  archive      = {J_ASOC},
  author       = {Yingkang Lu and Yanfei Li and Gaocai Fu and Yu Jiang and Yuzhe Huang and Jiaxing Zhu and Buyun Sheng},
  doi          = {10.1016/j.asoc.2024.112309},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The physical information LSTM surrogate model for establishing a digital twin model of reciprocating air compressors},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized underwater light attenuation prior-based depth
estimation and adaptive feature fusion CNN for underwater image and
video enhancement. <em>ASOC</em>, <em>167</em>, 112308. (<a
href="https://doi.org/10.1016/j.asoc.2024.112308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater images and videos play an essential communication tool in exploring ocean resources and understanding underwater scene perception. The underwater imaging environment and the complex lighting conditions degrade the quality of the images and make them low contrast, color deviation, and blurring. These degraded images lack effective target recognition information, significantly impacting the performance of underwater application systems. To solve the above issues, a new module for underwater video as well as image enhancement is proposed. The input video is given to the frame extraction phase for converting the video into the frame. Thereafter, the blur detection and determination are done using Laplacian&#39;s variance technique for determining blurred images. The depth estimation is done using Underwater Light Attenuation Prior (ULAP), wherein coefficients are determined using the proposed Competitive Multi-Verse bird swarm Optimization (CMVBSO). The CMVBSO is combined by integrating a competitive multiverse optimizer (CMVO) and a Bird Swarm Algorithm (BSA). The blurred value, input video frame and depth estimated output are given to adaptive feature fusion CNN in which training is done utilizing Manta-Ray Foraging Lion Optimization (MRLLO). The pixel enhancement is carried out using a Type II Fuzzy system and Cuckoo Search optimization algorithm filter (T2FCS) filter. The proposed CMVBSO_ULAP provided better performance with a Peak signal to noise ratio (PSNR) of 35.037 dB, Mean square error (MSE) of 3.684, Structural similarity index (SSIM) of 0.911, underwater image quality measure (UIQM) of 5.266 and Underwater Color Image Quality Evaluation (UCIQE) of 0.818 respectively.},
  archive      = {J_ASOC},
  author       = {Pooja Honnutagi and Laitha YS and V.D. Mytri},
  doi          = {10.1016/j.asoc.2024.112308},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112308},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized underwater light attenuation prior-based depth estimation and adaptive feature fusion CNN for underwater image and video enhancement},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing gene selection for alzheimer’s disease
classification: A bayesian approach to filter and embedded techniques.
<em>ASOC</em>, <em>167</em>, 112307. (<a
href="https://doi.org/10.1016/j.asoc.2024.112307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) classification, which is crucial for identifying AD-associated genes, relies heavily on effective feature selection (FS) to tackle the curse of dimensionality. Traditional methods like filter, wrapper, and embedded techniques have their drawbacks, including ignoring feature independence, sensitivity to classifier choices, and high computational costs. Hybrid approaches combining these methods seek to harness their collective strengths but face challenges, particularly in selecting the optimal number of features from each method. This selection is typically manual or requires time-intensive k-fold cross-validation (KFCV), significantly increasing computational demands and complicating the process with the need for extensive parameter optimization across families, thereby escalating the complexity and resource requirements of model development. To overcome these challenges, this work proposes a framework for optimal FS and classification in AD using a combination of filter and embedded techniques, enhanced with hyperparameter tuning. Firstly, gene expression data (GED) from the AD Neuroimaging Initiative (ADNI) is preprocessed. Then, Chi-square filter selection is applied to decrease correlated features. Next, Logistic Regression with ElasticNet penalty (LREN) is employed to further refine the feature set. Finally, Bayesian Optimization (BO) is introduced to automatically determine the optimal number of features ( k for Chi-square and max_features for LREN), iteratively evaluating different combinations to find the set that maximizes model accuracy. The selection process is used primarily in the function of BO to tune automatically the ( k and max_features while minimizing the number of features and maximizing the accuracy of the Support Vector Machine (SVM). The SVM was used as a classifier to overcome the problem of embedded selection sensitivity to the model of selection. The tuned features are then used to select relevant features from the ADNI dataset and fitted to different models. We evaluated the performance of five classifiers—logistic regression (LR), SVM, Ridge Classifier (RC), stochastic gradient descent classifier (SGD), and Gaussian Naïve Bayes(GNB) across various metrics. Among these, SVM achieved 100% performance in all metrics. This approach significantly reduced the FS time and the number of initial features to 0.6% and 0.02%, respectively. Notably, it identified 6 out of 20 selected features as directly AD-related. Comparative analysis reveals that the proposed method outperforms existing approaches on the ADNI dataset and other datasets. Statistical tests were conducted to assess the significance of the results compared to other methods, confirming a significant improvement and underscoring the effectiveness of the proposed framework in optimal FS and biological relevance confirmation.},
  archive      = {J_ASOC},
  author       = {Bouchra Guelib and Rayene Bounab and Salah Eddine Aliouane and Haithem Hermessi and Nawres Khlifa and Karim Zarour},
  doi          = {10.1016/j.asoc.2024.112307},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112307},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing gene selection for alzheimer’s disease classification: A bayesian approach to filter and embedded techniques},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective cat swarm optimization algorithm based on
two-archive mechanism for UAV 3-d path planning problem. <em>ASOC</em>,
<em>167</em>, 112306. (<a
href="https://doi.org/10.1016/j.asoc.2024.112306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving a balance between convergence and diversity is crucial in addressing multi-objective optimization problems (MOPs). In this paper, a multi-objective cat swarm optimization based on a new two-archive mechanism (MOCSO_TA) is proposed for the above challenge. In this approach, solutions that can promote convergence are stored by the convergence archive (CA). While solutions that can enhance diversity in the population are saved in the diversity archive (DA). Path planning is a critical process for unmanned aerial vehicles (UAVs), involving the identification of a route that is short and secure. Multi-objective algorithms have become a crucial approach for addressing UAV path planning problem, thus motivating the use of the proposed MOCSO_TA in path planning problem. The proposed algorithm is compared with two sets of representative multi-objective algorithms on the DTLZ, WFG, and ZDT benchmark problems. The experimental results demonstrate the outstanding performance of MOCSO_TA. The effectiveness of the MOCSO_TA is demonstrated by designing two terrains and comparing it with various multi-objective algorithms. The results confirmed the superiority of MOCSO_TA.},
  archive      = {J_ASOC},
  author       = {Sen-Yuan Pang and Qing-Wei Chai and Ning Liu and Wei-Min Zheng},
  doi          = {10.1016/j.asoc.2024.112306},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective cat swarm optimization algorithm based on two-archive mechanism for UAV 3-D path planning problem},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified grey wolf optimization-based dendritic neural
model for stock index return prediction. <em>ASOC</em>, <em>167</em>,
112305. (<a href="https://doi.org/10.1016/j.asoc.2024.112305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of stock index returns has always been a hot topic, which is an important basis for asset allocation and investment decisions of business managers and most investors. In order to improve the accuracy of the prediction, this paper proposes the use of a model based on the combination of the Improved Grey Wolf Optimization (IGWO) and Dendritic Neural Model (DNM). DNM has a more transparent structure, as well as a complex nonlinear processing capability and a unique architecture of automatic pruning, which can deal with the complexity of financial markets. For the traditional grey wolf optimization algorithm, this study designs four strategies to improve it. (i) Develop a nonlinear control parameter for dynamic equilibrium in global and local search algorithms. (ii) Chaos theory is introduced to plan the weights of wolves (iii) A local search scheme applicable to the head wolf is developed. (iv) The method of Levy leap is utilized to help the algorithm get rid of the local optimum. A total of 10 other algorithms are compared and validated based on data from 9 world representative stock index markets. It is pointed out that using IGWO in conjunction with DNM can achieve higher accuracy prediction.},
  archive      = {J_ASOC},
  author       = {Ruizhe Wang},
  doi          = {10.1016/j.asoc.2024.112305},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified grey wolf optimization-based dendritic neural model for stock index return prediction},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical system in conflict scenarios constructed based
on cluster analysis-inspired method for attribute significance
determination. <em>ASOC</em>, <em>167</em>, 112304. (<a
href="https://doi.org/10.1016/j.asoc.2024.112304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study considers the Pawlak model of a conflict situation in which for a set of conflict issues, agents give one of three decisions — they can be favorable, against or neutral toward the issue. The main purpose of the paper is to propose a method for determining the importance of issues in creating coherent coalitions. Coherent coalitions are coalitions whose members agree with each other while agents belonging to different coalitions significantly differs in their views. The importance of attributes can be used during negotiations to persuade agents belonging to a coalition to make certain concessions to improve the coalition cohesion. Such cohesive coalitions provide solid support for the future and are less likely to collapse. In addition, the paper proposes a measure for assessing coalition cohesion as well as a method for determining centroids of coalitions. A method of constructing a hierarchical system based on obtaining consensus on the most relevant attributes is also presented. The paper also includes two applicable scenarios of the proposed solutions.},
  archive      = {J_ASOC},
  author       = {Małgorzata Przybyła-Kasperek and Rafał Deja and Alicja Wakulicz-Deja},
  doi          = {10.1016/j.asoc.2024.112304},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112304},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical system in conflict scenarios constructed based on cluster analysis-inspired method for attribute significance determination},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonlinear subspace clustering by functional link neural
networks. <em>ASOC</em>, <em>167</em>, 112303. (<a
href="https://doi.org/10.1016/j.asoc.2024.112303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear subspace clustering based on a feed-forward neural network has been demonstrated to provide better clustering accuracy than some advanced subspace clustering algorithms. While this approach demonstrates impressive outcomes, it involves a balance between effectiveness and computational cost. In this study, we employ a functional link neural network to transform data samples into a nonlinear domain. Subsequently, we acquire a self-representation matrix through a learning mechanism that builds upon the mapped samples. As the functional link neural network is a single-layer neural network, our proposed method achieves high computational efficiency while ensuring desirable clustering performance. By incorporating the local similarity regularization to enhance the grouping effect, our proposed method further improves the quality of the clustering results. We name our method as Functional Link Neural Network Subspace Clustering (FLNNSC). Furthermore, we propose a convex combination subspace clustering scheme that combines a linear subspace clustering method with the functional link neural network subspace clustering approach. This combination method is named as Convex Combination Subspace Clustering (CCSC), which allows for a dynamic balance between linear and nonlinear representations. Extensive experiments conducted on four widely used datasets, including Extended Yale B, USPS, COIL20, and ORL, demonstrate that both FLNNSC and CCSC outperform several state-of-art subspace clustering methods in terms of clustering accuracy. Our affinity graph experiments reveal that FLNNSC exhibits clear block diagonal structures. We provide recommendations for hyperparameters in FLNNSC by performing a parameter sensitivity analysis, and empirically verify the convergence of FLNNSC. Additionally, we show that FLNNSC has a lower computational cost compared to two high-performing methods.},
  archive      = {J_ASOC},
  author       = {Long Shi and Lei Cao and Zhongpu Chen and Yu Zhao and Badong Chen},
  doi          = {10.1016/j.asoc.2024.112303},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nonlinear subspace clustering by functional link neural networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning-based few-shot relation extraction with
open-book datastore. <em>ASOC</em>, <em>167</em>, 112302. (<a
href="https://doi.org/10.1016/j.asoc.2024.112302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot relation extraction (FSRE) aims to identify relations with the aid of few instances. In real-world application scenarios, users often encounter new entities or relations, and there is insufficient labeled data to train the model, making FSRE particularly important. However, limited by the quality of instances in real-world settings, FSRE struggles to discern the subtle semantic differences among instances and corresponding relations. This leads to the critical problem of effectively learning these semantic differences of similar instances and similar relations effectively. In this paper, we propose a novel F ew- S hot C ontrastive L earning (FSCL) framework to alleviate the semantics challenges of similar instances and relations. To learn the semantic information in low-resource environments, FSCL jointly utilizes contrastive learning with an open-book datastore, while updating the instance representations in the open-book datastore asynchronously. To further enhance the robustness and reduce the impact of semantic bias caused by contrastive learning, we explore six data augmentation strategies on three public datasets to capture more meaningful semantic distinctions among instances as well as their relations. By conducting data augmentation experiments, we find that dropout noise is an effective strategy for data augmentation that can significantly highlight semantic differences between similar instances when learning similar relational representations. This helps the model obtain more potent and meaningful representations. Experiments on five public datasets demonstrate the effectiveness of the proposed method. Our codes are available on https://github.com/JohnnyGWY/FSCL .},
  archive      = {J_ASOC},
  author       = {Wanyuan Gong and Qifeng Zhou},
  doi          = {10.1016/j.asoc.2024.112302},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning-based few-shot relation extraction with open-book datastore},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated EEG-based language detection using directed
quantum pattern technique. <em>ASOC</em>, <em>167</em>, 112301. (<a
href="https://doi.org/10.1016/j.asoc.2024.112301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) signals contain complex useful information about brain activities. These EEG signals are noisy, highly varying and nonstationary in nature. Hence, extracting meaningful information from these signals is challenging. The existing machine learning systems struggle to capture the minute changes from the signals and yield high performance. This study introduces a novel quantum-inspired feature extraction technique called Directed Quantum Pattern (DQP), designed to address these challenges by using a lattice structure to capture directional binary features. These directions (paths) are computed using a maximum function providing a dynamic and adaptive feature representation. This paper presents a novel DQP-LangNet developed using DQP for automated classification of two- languages using EEG signals. We have proposed a hybrid approach, combining DQP, statistical features, and multi-level discrete wavelet transform (MDWT) to extract salient features similar to the deep learning approach. The EEG dataset consisting of 14 channels, produces 7 feature vectors per channel, yielding 98 feature vectors. Neighborhood component analysis and Chi-square (Chi2) feature selection approaches generated 196 feature vectors. In addition to the innovative feature extraction a new classification structure called “t” is proposed k-nearest neighbor (tkNN) and support vector machine (tSVM) classifiers are employed. Using the proposed tkNN and tSVM classifiers, 392 (=196×2) classifier-based outcomes are obtained. To further improve classification performance, we applied the iterative majority voting (IMV) technique to automatically select the best result. Our DQP-based model achieved a classification accuracy of 95.68 %using EEG language dataset with leave-one-subject-out (LOSO) cross-validation strategy. Also, an explainable feature engineering (XFE) structure of DQP-LangNet is employed to obtain channel-specific explainable results. Our proposed DQP-LangNet model can be employed for other applications in neuroscience.},
  archive      = {J_ASOC},
  author       = {Sengul Dogan and Turker Tuncer and Prabal Datta Barua and U.R. Acharya},
  doi          = {10.1016/j.asoc.2024.112301},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated EEG-based language detection using directed quantum pattern technique},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale and multi-receptive field-based feature fusion
for robust segmentation of plant disease and fruit using agricultural
images. <em>ASOC</em>, <em>167</em>, 112300. (<a
href="https://doi.org/10.1016/j.asoc.2024.112300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate and fast assessment of plant diseases and fruits is important for sustainable and productive agriculture. However, typically manual methods are adopted for the assessment of plant diseases and fruit, which is a time-consuming, resource-intensive, and error-prone approach. A few artificial intelligence (AI)-based methods have been introduced to automate this process, but they show limitations in attaining high performance in challenging imaging conditions such as occlusions, poor lighting, noise, indistinctive boundaries, and excessive morphological variations. Moreover, current approaches also lack in providing computationally efficient solutions. To overcome these issues, two novel architectures are developed to segment plant diseases and fruits with higher segmentation performance and less computational requirements. The efficient feature fusion segmentation network (EFFS-Net) is the base network, and a multi-scale dilated feature fusion segmentation network (MDFS-Net) is the final network of this study. EFFS-Net uses an identity skip path-based feature fusion mechanism with an efficient grouped convolutional depth (EGCD) to provide satisfactory segmentation performance with high computational efficiency. MDFS-Net uses a multi-scale feature fusion mechanism and fuses low-level information in the EGCD and other sections of the network to learn detailed input features for delivering promising performance even in challenging imaging conditions. MDFS-Net also applies multiple receptive fields to the low-level information in the effective receptive field processing (ERFP) block for fusion near the pixel classification stage for further performance enhancement. Both networks are evaluated using a Brazilian Arabica coffee leaf (BRACOL) image dataset, an Australian Center for Field Robotics orchard fruit (apple) dataset, and a necrotized cassava root cross-section image dataset. The proposed method provides a promising segmentation performance, achieving dice similarity coefficients of 88.81 %, 95.01 %, and 86.04 % with performance improvement of 3.5 %, 2.6 %, and 1.07 % on the three datasets, respectively, with approximately ten times less number of required trainable parameters compared with the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Adnan Haider and Muhammad Arsalan and Jin Seong Hong and Haseeb Sultan and Nadeem Ullah and Kang Ryoung Park},
  doi          = {10.1016/j.asoc.2024.112300},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale and multi-receptive field-based feature fusion for robust segmentation of plant disease and fruit using agricultural images},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble clustering via dual self-enhancement by alternating
denoising and topological consistency propagation. <em>ASOC</em>,
<em>167</em>, 112299. (<a
href="https://doi.org/10.1016/j.asoc.2024.112299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering aims at learning a more robust consensus partition among a set of weak base clustering results. However, due to the low accuracy of the base clustering results, there exist some links that reflect direct correlations between samples are incorrect. Current ensemble clustering methods primarily reduce these incorrect links by retaining larger element values in the Co-association (CA) matrix, but they ignore the indirect correlations, i.e., the topological similarity, among the samples. To cope with this issue, this paper introduces an Ensemble Clustering framework based on Dual self-Enhancing CA matrices, known as ECDE. In ECDE, a Kullback–Leibler divergence weighting scheme is initially applied to enhance the representability of the CA matrix. Then, a dual self-enhancing approach, incorporating denoising and topological consistency propagation based on the CA matrix, is alternately applied to improve the quality of the CA matrix. Notably, both denoising self-enhancement and topological consistency propagation are integrated into a unified objective function. To optimize this objective function, the Alternating Direction Method of Multipliers (ADMM) is employed. Experimental results on multiple datasets demonstrate the effectiveness of ECDE in terms of clustering performance compared to eight state-of-the-art ensemble clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Jiaxuan Xu and Taiyong Li and Jiang Wu and Duzhong Zhang},
  doi          = {10.1016/j.asoc.2024.112299},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble clustering via dual self-enhancement by alternating denoising and topological consistency propagation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unravelling sleep patterns: Supervised contrastive learning
with self-attention for sleep stage classification. <em>ASOC</em>,
<em>167</em>, 112298. (<a
href="https://doi.org/10.1016/j.asoc.2024.112298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sleep data scoring is a crucial and primary step for diagnosing sleep disorders to know the sleep stages from the PSG signals. This study uses supervised contrastive learning with a self-attention mechanism to classify sleep stages. We propose a deep learning framework for automatic sleep stage classification, which involves two training phases: (1) the feature representation learning phase, in which the feature representation network (encoder) learns to extract features from the electroencephalogram (EEG) signals, and (2) the classification network training phase, where a pre-trained encoder (trained during phase I) along with the classifier head is fine-tuned for the classification task. The PSG data shows a non-uniform distribution of sleep stages, with wake (W) (around 30% of total samples) and N2 stages (around 58% and 37% of total samples in Physionet EDF-Sleep 2013 and 2018 datasets, respectively) being more prevalent, leading to an imbalanced dataset. The imbalanced data issue is addressed using a weighted softmax cross-entropy loss function that assigns higher weights to minority sleep stages. Additionally, an oversampling technique (the synthetic minority oversampling technique (SMOTE) (Chawla et al., 2002) [1] ) is applied to generate synthetic samples for minority classes. The proposed model is evaluated on the Physionet EDF-Sleep 2013 and 2018 datasets using Fpz-Cz and Pz-Oz EEG channels. It achieved an overall accuracy of 94.1%, a macro F1 score of 92.64, and a Cohen’s Kappa coefficient of 0.92. Ablation studies demonstrated the importance of triplet loss-based pre-training and oversampling for enhancing performance. The proposed model requires minimal pre-processing, eliminating the need for extensive signal processing expertise, and thus is well-suited for clinicians diagnosing sleep disorders.},
  archive      = {J_ASOC},
  author       = {Chandra Bhushan Kumar and Arnab Kumar Mondal and Manvir Bhatia and Bijaya Ketan Panigrahi and Tapan Kumar Gandhi},
  doi          = {10.1016/j.asoc.2024.112298},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112298},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unravelling sleep patterns: Supervised contrastive learning with self-attention for sleep stage classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A staged fuzzy evolutionary algorithm for constrained
large-scale multiobjective optimization. <em>ASOC</em>, <em>167</em>,
112297. (<a href="https://doi.org/10.1016/j.asoc.2024.112297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multiobjective optimization problems (CMOPs) are prevalent in practical applications, where constraints play a significant role. Building on techniques from constrained single-objective optimization, classical methods such as the constrained dominance principle have been extended to CMOPs. However, these methods struggle with CMOPs characterized by complex infeasible regions. Furthermore, as the number of decision variables increases, the search efficiency of algorithms deteriorates dramatically. To solve those issues, we propose a staged fuzzy evolutionary algorithm (i.e., SFEA) for constrained large-scale problems. To balance exploration and exploitation, a fuzzy stage adjustment strategy based on the sigmoid function is proposed. Furthermore, this article develops an improved fuzzy operator to perform fuzzy operations on various vectors (e.g., solutions or constraint violations). Computational experiments were conducted on CMOP test suites with up to 500 decision variables and a series of real-world applications. The experimental results demonstrate that, compared to existing peer algorithms, our algorithm exhibits superior or competitive performance.},
  archive      = {J_ASOC},
  author       = {Jinlong Zhou and Yinggui Zhang and Fan Yu and Xu Yang and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2024.112297},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A staged fuzzy evolutionary algorithm for constrained large-scale multiobjective optimization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel lightweight deep learning framework with knowledge
distillation for efficient diabetic foot ulcer detection. <em>ASOC</em>,
<em>167</em>, 112296. (<a
href="https://doi.org/10.1016/j.asoc.2024.112296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Foot Ulcers (DFUs) are a critical healthcare issue requiring early detection. Although deep learning models show promise in DFU diagnosis, their large parameter sizes and high computational demands limit their practical use. This work addresses these challenges by introducing DFU-LWNet, a lightweight, knowledge-distilled model that maintains or even surpasses the performance of existing models while drastically reducing computing costs. The DFU-LWNet model synergistically integrates the MBConv Block, leveraging its capabilities in feature expansion, depthwise convolution for parameter efficiency, and adaptive feature recalibration through the Squeeze-and-Excitation (SE) mechanism. The methodology commences with the fine-tuning of seven pre-trained models, ultimately selecting InceptionV3 as the optimal teacher model based on experimental results. Subsequently, knowledge distillation (KD) is applied, leveraging the insights from this robust teacher model to enhance the performance of the student model, DFU-LWNet. The primary objective is to reduce the student model&#39;s parameter size and computational complexity while maintaining diagnostic accuracy. Through comprehensive evaluations, DFU-LWNet achieves an impressive classification accuracy of 96.23 % on a publicly available dataset comprising 1055 foot images. Notably, this achievement is accomplished with a remarkably low parameter size of only 0.49 million, consuming 2MB of disk space. This reduction in parameter size not only improves inference speed but also significantly reduces computational costs compared to pre-trained models. Moreover, the study employs Grad-CAM visualizations to demonstrate the model&#39;s enhanced saliency and interpretability in predictions post-KD, thereby offering valuable insights to clinicians. Additionally, the efficacy of DFU-LWNet extends to thermal imagery domains, as validated by experiments on a diverse dataset of thermal images of diabetic foot cases. The proposed approach achieves remarkable accuracy of 93.13 % on a separate thermal image dataset, demonstrating cross-domain success and reinforcing the model&#39;s practical utility in real-world scenarios. Compared to existing state-of-the-art methods, this work demonstrates promising practical viability. DFU-LWNet’s minimal parameter count and low prediction times make it well-suited for deployment in resource-limited healthcare settings.},
  archive      = {J_ASOC},
  author       = {Kamran Amjad and Sohaib Asif and Zafran Waheed and Ying Guo},
  doi          = {10.1016/j.asoc.2024.112296},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel lightweight deep learning framework with knowledge distillation for efficient diabetic foot ulcer detection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fitness-guided particle swarm optimization with adaptive
newton-raphson for photovoltaic model parameter estimation.
<em>ASOC</em>, <em>167</em>, 112295. (<a
href="https://doi.org/10.1016/j.asoc.2024.112295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a new approach for parameter optimization in the four-diode photovoltaic (PV) model, employing a Dynamic Fitness-Guided Particle Swarm Optimization (DFGPSO) algorithm and Enhanced Newton-Raphson (ENR) method. The new DFGPSO algorithm is specifically designed to address the intrinsic challenges in PV modelling, such as local optima entrapment and slow convergence rates that typically hinder traditional optimization methods. By integrating a dynamically evolving fitness function derived from advanced swarm intelligence, the proposed approach significantly enhances global search capabilities. This new fitness function adapts continuously to the search landscape, facilitating rapid convergence towards optimal solutions and effectively navigating the complex, non-linear, and multi-modal parameter space of the PV model. Moreover, the robustness of the DFGPSO algorithm is substantially improved through the strategic incorporation of the ENR method. This integration not only provides accurate initial guesses for the particle positions, thus expediting the convergence process, but also minimizes computational burden, making the method more efficient. Comprehensive simulation studies across various case scenarios demonstrate that the proposed method markedly outperforms existing state-of-the-art optimization algorithms. It delivers faster convergence, enhanced accuracy, and robust performance under diverse environmental conditions, establishing a reliable and precise tool for optimizing PV system performance. This advancement promises significant improvements in energy yield and system reliability for the PV industry.},
  archive      = {J_ASOC},
  author       = {Manoharan Premkumar and Sowmya Ravichandran and Tengku Juhana Tengku Hashim and Tan Ching Sin and Rabeh Abbassi},
  doi          = {10.1016/j.asoc.2024.112295},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fitness-guided particle swarm optimization with adaptive newton-raphson for photovoltaic model parameter estimation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight visual mamba network for image recognition
under resource-limited environments. <em>ASOC</em>, <em>167</em>,
112294. (<a href="https://doi.org/10.1016/j.asoc.2024.112294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vision Transformers (ViTs) models show great potential in recognition due to their excellent self-attention mechanisms. However, they often need more computational complexity, making achieving high performance in resource-constrained environments challenging. This paper proposes a lightweight visual model (called LightViM) to tackle these challenges, specifically devised for recognition challenges under resource-constrained environments. Considering the imperative of reducing the model’s computational complexity and resource consumption , we propose lightweight local–global feature fusion modules based on Mamba (LGF-Mamba) aimed at integrating spatially detailed local information with globally contextualized features while maintaining lower linear time complexity. In LGF-Mamba, Mamba sub-blocks are first designed to extract global information; then, for the rapid extraction of local features, LocalE module is designed and integrated into LGF-Mamba, exploring more comprehensive feature representation. This mechanism efficiently directs the network to integrate features from different spatial scales, thereby improving the recognition performance in resource-constrained environments. Experimental results indicate that, in comparison to other leading-edge lightweight methods, the proposed procedure simultaneously achieves superior recognition performance and minimal resource consumption.},
  archive      = {J_ASOC},
  author       = {Yuchen Liu and Hu Liang and Shengrong Zhao},
  doi          = {10.1016/j.asoc.2024.112294},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight visual mamba network for image recognition under resource-limited environments},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A least-squares framework for developing interval type-2
fuzzy semantics. <em>ASOC</em>, <em>167</em>, 112293. (<a
href="https://doi.org/10.1016/j.asoc.2024.112293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The developing of IT2 fuzzy semantics is critical for computing with words (CWW), but exiting approaches lack flexibility and fail to adapt user’s diversified demands. The study proposes a least-squares framework for designing CWW encoders that construct interval type-2 fuzzy sets (IT2 FSs) to represent the semantic meanings of linguistic words. In the least-squares framework, an CWW encoder is characterized by two elements: an intra-uncertain semantic mapping and an inter-uncertain semantic family. The intra-uncertain semantic mapping transforms data intervals into type-1 fuzzy sets (T1 FSs), then an optimal IT2 FS is derived from the inter-uncertain semantic family using the least-squares method. Furthermore, several intra-uncertain semantic mappings are introduced, and a compatibility measure is defined to facilitate model selection. The least-squares framework benefits from the flexible selection of intra-uncertain semantic mappings and least-squares optimization-based construction of IT2 FSs. In experiments, the least-squares framework is applied to handle real-world online survey data and the large-scale online review data set of a Chinese life service review site, Dianping.com . Compared to the enhanced interval approach and the Hao-Mendel approach, the least-squares framework shows its favorable efficiency in experiments and statistical tests, and adapts to user-defined intra- and inter-uncertain semantic families.},
  archive      = {J_ASOC},
  author       = {Hao Li and Xianchao Dai and Ligang Zhou and Qun Wu and Muhammet Deveci and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2024.112293},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112293},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A least-squares framework for developing interval type-2 fuzzy semantics},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gaussian interval type-2 fuzzy characterization method
based on heterogeneous big data and its application in forest ecological
assessment. <em>ASOC</em>, <em>167</em>, 112292. (<a
href="https://doi.org/10.1016/j.asoc.2024.112292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The under-forest economy is a new agricultural and forestry production mode. It can improve the utilization efficiency of forest land and make full use of forest resources, forest space, and forest ecological environment through different forms of industries such as understory planting and breeding. The under-forest economy has important ecological and economic benefits, significant to rural revitalization and industrial structure adjustment. It has been widely discussed by people in recent years. The forest ecological security level is a core factor to be considered in selecting sites for the under-forest economy. In practice, some forest ecological security evaluation indexes usually show heterogeneous forms. To comprehensively evaluate the level of forest ecological security, this study constructs a new multi-indicator fuzzy evaluation method based on heterogeneous big data. Firstly, a new linguistic scale function is defined to realize the consistent conversion of multi-granularity linguistic information. A Gaussian interval type-2 fuzzy aggregation formula for heterogeneous large-scale data is proposed. This formula is used to describe the differences in evaluation information between individuals caused by different knowledge backgrounds of evaluators. Then, the Gaussian interval type-2 fuzzy cross-entropy is defined, and it is verified to satisfy the excellent properties. This cross-entropy is applied to determine the objective weights of evaluation indexes. Considering the risk-averse psychology of experts and combining the prospect theory, the MULTIMOORA method in the Gaussian interval type-2 fuzzy environment is proposed. Finally, the applicability of the constructed Gaussian interval type-2 fuzzy comprehensive evaluation method (GIT2FCE) is verified through a case study conducted on the site selection of the under-forest economic industry.},
  archive      = {J_ASOC},
  author       = {Junzhe Zhang and Jian Lin and Ying Lin and Zeshui Xu},
  doi          = {10.1016/j.asoc.2024.112292},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gaussian interval type-2 fuzzy characterization method based on heterogeneous big data and its application in forest ecological assessment},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel integrated soft sensing model for online cement
clinker quality monitoring based on fuzzy fine-grained classification.
<em>ASOC</em>, <em>167</em>, 112291. (<a
href="https://doi.org/10.1016/j.asoc.2024.112291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The content of free calcium oxide (f-CaO) in cement clinker is an important index for cement quality. For the problems of multiple working conditions and unbalanced distribution of sample labels in cement clinker production, a soft sensing method of cement clinker f-CaO content based on fuzzy fine-grained classification (FF) is proposed. First, a divide-and-conquer strategy is used to divide the samples into high, medium, and low subsets according to cement clinker f-CaO and extract the fine-grained features under diverse types of multiple production conditions. Second, fuzzy classification based on the membership function is used in the FF model to solve the uncertainty of the sample categories. To ensure the rationality of the classification, the fuzzy membership rule is combined with a convolutional neural network to implement the fuzzy classification method. Finally, different feature extraction methods are proposed to be selected according to the data size of various categories of samples. After experimental validation, the evaluation metrics of RMSE decreased by 4.5 % and R 2 increased by 17.8 % for the direct classification model compared to the single model. The RMSE of the fuzzy classification model over the direct classification model was again reduced by 2.34 % and R 2 was again improved by 7.24 %, showing the effectiveness of the proposed soft measurement model.},
  archive      = {J_ASOC},
  author       = {Yantao Zhao and Yuxuan Han and Ce Zhang and Bochuan Ding and Xiaochen Hao},
  doi          = {10.1016/j.asoc.2024.112291},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112291},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel integrated soft sensing model for online cement clinker quality monitoring based on fuzzy fine-grained classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reinforcement learning-based multiobjective heuristic
algorithm for multiple-truck routing problems with heterogeneous drones.
<em>ASOC</em>, <em>167</em>, 112290. (<a
href="https://doi.org/10.1016/j.asoc.2024.112290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a result of new technological developments, drone usage has become common in last-mile logistics activities. Owing to the limited flight range of drones, drones may service customers by using trucks as hubs. Although the truck routing problem with drones (TRP-D) has been studied frequently, studies that have addressed this problem in a multiobjective manner are rare. In this study, multiobjective TRP-D is discussed. In this problem, multiple trucks are considered. Each truck is equipped with multiple heterogeneous drones. A multiobjective mixed-integer linear programming (MILP) model is proposed. Since the problem is NP-hard, a reinforcement learning (RL)-based multiobjective heuristic algorithm is proposed to obtain solutions in large-scale cases. A three-phase heuristic feasible-solution-generation algorithm is proposed. The variable neighbourhood descent (VND) algorithm with RL is proposed as a local search algorithm for the multiobjective optimization problem. The results of the proposed heuristic algorithm are compared with the Pareto optimal solutions for small-sized test problems via the augmented ε-constraint (AUGMECON) method and the CPLEX solver. The proposed algorithm is also compared with state-of-the-art multiobjective heuristic algorithms for large-scale problems. According to the results, the proposed algorithm is an efficient multiobjective heuristic algorithm for the addressed problem.},
  archive      = {J_ASOC},
  author       = {Gulcin Bektur},
  doi          = {10.1016/j.asoc.2024.112290},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112290},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reinforcement learning-based multiobjective heuristic algorithm for multiple-truck routing problems with heterogeneous drones},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced CRADIS decision model for optimizing radioactive
waste reduction through transmutations based on disc spherical fuzzy
information. <em>ASOC</em>, <em>167</em>, 112289. (<a
href="https://doi.org/10.1016/j.asoc.2024.112289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radioactive waste reduction through transmutation techniques is significant for mitigating environmental risks and ensuring long-term safety. It offers a sustainable approach to minimizing the volume of radioactive waste, contributing to both environmental preservation and enhanced nuclear energy sustainability. This paper introduces an extension of aggregation operations to hybrid operators in Disc Spherical Fuzzy Sets (D-SFSs), including D-SFS algebraic hybrid weighted averaging and D-SFS hybrid geometric aggregation operators. These novel operators are tailored for D-SFSs and are supported by rigorous proofs, enhancing their theoretical robustness. Additionally, a Maximum Deviation Method (MDM) is developed in conjunction with a proposed distance measure to assess attribute weights in Multiple Attribute Decision-Making (MADM) problems with Disc Spherical Fuzzy (D-SF) information. The study proposes a new MADM method in the D-SF environment, leveraging the D-SF distance measure and the Compromise Ranking of Alternatives from Distance to Ideal Solution (CRADIS) method. Notably, the paper focuses on extending D-SFs to the CRADIS method, particularly when weight information is entirely unknown. A comprehensive case study on optimizing radioactive waste volume reduction through transmutation technologies is provided. A comparative analysis with existing decision-making methods validates the improved MADM method’s validity and practicality.},
  archive      = {J_ASOC},
  author       = {Shahzaib Ashraf and Wania Iqbal and Muhammad Shazib Hameed and Vladimir Simic and Nebojsa Bacanin},
  doi          = {10.1016/j.asoc.2024.112289},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112289},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced CRADIS decision model for optimizing radioactive waste reduction through transmutations based on disc spherical fuzzy information},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimum adjustment cost consensus model with bounded risk
preference attitudes: A perspective based on fractional stochastic
dominance. <em>ASOC</em>, <em>167</em>, 112288. (<a
href="https://doi.org/10.1016/j.asoc.2024.112288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linguistic multi-attribute group decision making with linguistic distribution assessment (LMAGDM-LDA) problems have been widely investigated. However, in the existing studies, the risk attitudes of decision maker are seldom considered. Inspired by this, in this paper, we employ the idea of fractional stochastic dominance to develop a consensus reaching framework for LMAGDM-LDA with bounded risk preference attitudes. Then, we design an interactive learning algorithm for obtaining the bounded risk preference threshold. Next, we analyze the deception behaviors and average behaviors of experts, and develop the minimum adjustment cost consensus model with bounded risk preference attitude (MACC-BRP). And we justify the connection between the MACC-BRP and the minimum adjustment cost consensus model with other risk attitudes. Furthermore, we provide a distance-based method to obtain the rankings of alternatives. Furthermore, a numerical analysis is provided to show the computation process of the proposed method, and a comparison analysis and a simulation analysis are further conducted to show the advantages and features of the proposal. The simulation results show the effects of the risk preference threshold, the intrinsic individual weight and the average threshold. The comparison results show that the proposed minimum adjustment cost consensus models have clear advantages.},
  archive      = {J_ASOC},
  author       = {Haiming Liang and Hengjie Zhang and Sihai Zhao and Hui Hu},
  doi          = {10.1016/j.asoc.2024.112288},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112288},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimum adjustment cost consensus model with bounded risk preference attitudes: A perspective based on fractional stochastic dominance},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A cognitive load assessment method for fighter cockpit
human-machine interface based on integrated multi-criteria decision
making. <em>ASOC</em>, <em>167</em>, 112287. (<a
href="https://doi.org/10.1016/j.asoc.2024.112287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary interface for communication between pilots and aircraft systems is the fighter cockpit Human-Machine Interface (HMI). Since reduced cognitive load ensures that pilots will operate the aircraft safely and effectively, it is critical to evaluate pilot cognitive load during HMI. A cognitive load assessment method, based on multi-criteria decision-making, is proposed to accurately quantify the relationship between the HMI and the cognitive load in the fighter’s cockpit. Firstly, based on the integrated Multi-Criteria Decision-Making (MCDM) method, the Step-wise Weight Assessment Ratio Analysis (SWARA) and MEthod based on the Removal Effects of Criteria (MEREC) methods are used to assign subjective and objective weights, respectively. Moreover, the Combined Compromise Solution (CoCoSo) method is applied to rank the scenarios to establish a cognitive load assessment model for the cockpit HMI of the fighter jet. Secondly, an evaluation standard system of fighter cockpit HMI is proposed, drawn upon multiple sets of eye-movement criteria and subjective assessment criteria. Moreover, cognitive load experiments of fighter cockpit HMI are conducted using eye-tracking technology to get the objective physiological cognitive data as well as the subjective assessment data of the subjects. Consequently, the parameter data sets of the eye-movement criteria and the subjective criteria for the evaluation of cognitive load are obtained. The proposed method is applied to analyze the cognitive load assessment of a fighter jet cockpit HMI layout. This application aims to verify the effectiveness of the assessment method in evaluating the cognitive load of the HMI layout. Through sensitivity and comparison analyses, the model was further verified to have excellent robustness and applicability for cognitive load assessment. The advantages of this method can be seen through the comparison, which is that it has higher discriminability when assessing the degree of cognitive load. At the same time, it has higher flexibility in dealing with complex and ambiguous cognitive load assessment information.},
  archive      = {J_ASOC},
  author       = {Huining Pei and Ziyu Wang and Jingru Cao and Yunfeng Chen and Zhonghang Bai},
  doi          = {10.1016/j.asoc.2024.112287},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112287},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A cognitive load assessment method for fighter cockpit human-machine interface based on integrated multi-criteria decision making},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A three-way decision with prospect-regret theory under
pythagorean fuzzy environments. <em>ASOC</em>, <em>167</em>, 112286. (<a
href="https://doi.org/10.1016/j.asoc.2024.112286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As science and technology continue to advance at a rapid pace, people are facing increasingly complex data, and research on uncertain information technology has become a hot field. Compared to intuitionistic fuzzy sets, Pythagorean fuzzy sets are more tolerant and flexible in representing uncertain information. Therefore, this article mainly focuses on the study of three-way decisions based on Pythagorean fuzzy β β covering. First, considering the disadvantages of other methods in comparing the magnitudes of Pythagorean fuzzy numbers, we propose the concept of the ideal positive degree in a Pythagorean fuzzy environment and determine a method for calculating conditional probabilities based on this newly defined ideal positive degree. Second, regret theory and prospect theory are combined to obtain a relative utility function. Then, based on the aforementioned conditional probabilities and relative utility function, a three-way multi-attribute decision-making method based on the Pythagorean fuzzy environment is established. Finally, the three-way multi-attribute decision-making method is utilized to solve the problem of selecting the optimal choice for rural revitalization industrial projects. Through experimental comparison and analysis, the effectiveness and reliability of this method are verified.},
  archive      = {J_ASOC},
  author       = {Haidong Zhang and Yin Liu and Deji Selang and Yanping He},
  doi          = {10.1016/j.asoc.2024.112286},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112286},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way decision with prospect-regret theory under pythagorean fuzzy environments},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-optimization-guided neural network (MOGNN) applied to
chemical processes. <em>ASOC</em>, <em>167</em>, 112285. (<a
href="https://doi.org/10.1016/j.asoc.2024.112285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a Model-Optimization-guided Neural Network (MOGNN) is proposed to optimize chemical processes. The model is trained with pre-selected process data, resulting from optimization of engineering models described by systems of algebraic equations. MOGNN aims to predict the optimal operating points for a range of input variables. The models were simulated in the Unisim-Design® process simulator for training data generation and optimized in Python using the Particle Swarm Optimization algorithm. The resulting models were applied to optimize two chemical engineering cases. The results showed that the computational cost of optimization corresponded to 0.83 % and 2.12 %, respectively, about the simulation cost for a given input dataset. Also, both processes revealed good alignment between the predicted profiles and the simulator data, while their respective objective function profiles yielded an average improvement of 9.61 % and 18.83 % for the two examples.},
  archive      = {J_ASOC},
  author       = {Felipe Macedo Freitas Siqueira and Lizandro de Sousa Santos},
  doi          = {10.1016/j.asoc.2024.112285},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112285},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model-optimization-guided neural network (MOGNN) applied to chemical processes},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noise-resistant fuzzy multineighbourhood rough set-based
feature selection with label enhancement and its application for
multilabel classification. <em>ASOC</em>, <em>167</em>, 112284. (<a
href="https://doi.org/10.1016/j.asoc.2024.112284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noise pollution in process of feature selection greatly reduces the classification efficacy of multilabel data, and the estimation of label descriptions is insufficient because of the lack of label enhancement for label distribution learning. To overcome the limitations, this work presents a noise-resistant fuzzy multineighbourhood rough set-based feature selection methodology with label enhancement and its application for multilabel classification. First, under the fuzzy T -equivalence relation, to construct new adaptive fuzzy neighbourhood class, the multineighbourhood radius set on feature space is integrated with fuzzy neighbourhood radius via sample interval on label space; then, the adaptive fuzzy multineighbourhood granules are obtained. Second, by integrating the parameterized fuzzy decision of labels and adaptive fuzzy multineighbourhood granules, noise-resistant lower and upper approximations via fuzzy multineighbourhood are constructed. A noise-resistant multilabel fuzzy multineighbourhood rough set model and its noise-resistant approximate accuracy can be proposed. Third, the description degree of label is computed to design label proportion after label enhancement. Certain fuzzy multineighbourhood entropy measures are developed to obtain label enhancement-based mutual information. When the algebraic and information perspectives are fused, label enhancement-based mutual information with fuzzy multineighbourhood via approximate accuracy is presented to evaluate the final association relationship between the features and label sets. Finally, a multilabel feature selection strategy via label enhancement will be constructed to obtain the best feature subset. The experimental results applied to 14 multilabel datasets indicate that this algorithm is significant.},
  archive      = {J_ASOC},
  author       = {Lin Sun and Wenjuan Du and Jiucheng Xu and Baofang Chang},
  doi          = {10.1016/j.asoc.2024.112284},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Noise-resistant fuzzy multineighbourhood rough set-based feature selection with label enhancement and its application for multilabel classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Semi-supervised prediction method for time series based on
monte carlo and time fusion feature attention. <em>ASOC</em>,
<em>167</em>, 112283. (<a
href="https://doi.org/10.1016/j.asoc.2024.112283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and reliable forecasting of time sequences is a challenging task in cyber–physical systems (CPS). Traditional time series prediction models struggle to provide accurate predictions for diverse types of time series data, especially with missing data due to variations in scale, types, and the complexity of real-world production environments. In this paper, we introduce a hybrid model named Temporal Feature Fusion Attention-based Monte Carlo Semi-supervised Long Short Term Memory (LSTM) network to address this issue. The model encodes the current state and historical information using a current time feature state vector. It then calculates the hidden feature vectors for the time series at different time points (past, present, and future), as well as for the current Monte Carlo filtering sequences. This approach leverages the correlation of time series features, transfers and fuses crucial historical features within the sequence with the optimized sequence feature information from the Monte Carlo algorithm. Our experiments confirm that with 10% of labeled data missing, our proposed method significantly improves the evaluation metric Mean Absolute Percentage Error (MAPE) by 17.827% compared to the baseline LSTM model. Moreover, our method surpasses other state-of-the-art methods across four distinct time series datasets, achieving the best prediction results.},
  archive      = {J_ASOC},
  author       = {Yang Yang and Jing Zhang and Lulu Wang},
  doi          = {10.1016/j.asoc.2024.112283},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised prediction method for time series based on monte carlo and time fusion feature attention},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimized multi-layer ensemble model for airborne
networks intrusion detection. <em>ASOC</em>, <em>167</em>, 112282. (<a
href="https://doi.org/10.1016/j.asoc.2024.112282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In view of the characteristics of airborne networks, such as the traffic data of 1553B data bus and public networks, numerous redundant and irrelevant data, and fewer but more types of attack behaviors, a highly representative balanced data subset is generated by combining K-means with synthetic minority oversampling technique (SMOTE). A feature selection (FS) method of fast correlation-based filter combined with information gain (IG-FCBF) is proposed to filter the key characteristics with 90 % cumulative importance. Aiming at the problem that it is not easy for a single classifier to accurately classify various types of attacks, as well as the difficulty in obtaining the best prediction results by adjusting default and manual parameters, a multi-layer ensemble model based on Bayesian optimization tree-structured Parzen estimator (BO-TPE) is proposed for airborne networks intrusion detection, combining the advantages of supervised learning, stacking ensemble method and hyperparameter optimization (HPO). The experimental results show the superiority and effectiveness of the model, as well as its universality in Integrated Avionics Systems (IAS) and onboard public networks, which provides a new approach for airborne networks intrusion identification and protection.},
  archive      = {J_ASOC},
  author       = {Huang Li and Hongjuan Ge and Yiqin Sang and Cong Gao},
  doi          = {10.1016/j.asoc.2024.112282},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimized multi-layer ensemble model for airborne networks intrusion detection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Unveiling the power of haar frequency domain: Advancing
small target motion detection in dim light. <em>ASOC</em>, <em>167</em>,
112281. (<a href="https://doi.org/10.1016/j.asoc.2024.112281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual small target motion detection finds successful applications in varied scenarios. However, dim-light conditions, such as the tunnel scenes and nighttime environments, present significant challenges to existing detection methods which mainly operate within the spatiotemporal domain. This is because the transmission of small target motion information suffers from the inevitable interference of image noise caused by dim light in the spatiotemporal domain, resulting in the detriment of extracting essential spatiotemporal features of the target motion. Given the significant obstacles posed by dim-light imaging to small target motion detection within the spatiotemporal domain, the exploration of an alternative observation domain for small target motion, alongside the development of a corresponding detection method, emerges as a viable solution. To address this, in this paper, we discovered the remarkable potential of the Haar frequency domain in characterizing the small target motion in dim light. To investigate the advantages of integrating Haar frequency processing in small target motion detection, we introduce a Haar-windowed summation mechanism into an existing bio-inspired small target motion detection model. The proposed mechanism integrates visual information in spatiotemporal windows regulated by frequency parameters of Haar wavelets and effectively discriminates the small target motion from the disturbance of random noise caused by dim light. Theoretical analysis and numerical experiments confirm the superior performance of integrating the Haar frequency processing. This study provides a new vision of small target motion detection through the lens of the frequency domain and extends the limits of existing bio-inspired models for practical applications in dim light.},
  archive      = {J_ASOC},
  author       = {Hao Chen and Xuelong Sun and Cheng Hu and Hongxin Wang and Jigen Peng},
  doi          = {10.1016/j.asoc.2024.112281},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unveiling the power of haar frequency domain: Advancing small target motion detection in dim light},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal sentiment analysis using deep learning and fuzzy
logic: A comprehensive survey. <em>ASOC</em>, <em>167</em>, 112279. (<a
href="https://doi.org/10.1016/j.asoc.2024.112279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) is the process of identifying sentiment polarities that users may simultaneously display in text, audio, and video data. Sentiment analysis methods based on a single data type are becoming increasingly unsuitable. Therefore, MSA has emerged, developed, and become increasingly popular today. MSA plays an important role in many practical applications, particularly in decision-making, recommendation systems, and fake news detection systems. Therefore, new proposals and performance improvements in multimodal sentiment-analysis methods are of great interest to scientists. Many multimodal sentiment-analysis methods have been proposed and developed, including those based on deep learning (DL) models, which have achieved significant prospects. This study comprehensively surveys the aspects related to MSA methods based on deep learning (MSA-based on DL) techniques. MSA-based on DL is the process of identifying sentiment polarities using DL, such as CNN, RNN, LSTM, GAN, and DBN, to create hidden layers for the automation of time-consuming stages, such as feature selection, feature extraction, parameter optimization, feature vector processing, and prediction generation on various data types simultaneously. In this paper, we propose a new taxonomy for MSA methods based on DL, and evaluate and compare method groups using commonly used data types. This article presents the advantages, disadvantages, and challenges that must be addressed in the future. Unlike previous survey works, this study proposes a particular classification by adding a group of multimodal sentiment-analysis methods based on a combination of DL techniques and fuzzy logic. The results of this study are expected to provide essential guidance to beginners, practitioners, and researchers regarding building and improving the performance of multimodal sentiment-analysis methods.},
  archive      = {J_ASOC},
  author       = {Hoang Nam Do and Huyen Trang Phan and Ngoc Thanh Nguyen},
  doi          = {10.1016/j.asoc.2024.112279},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal sentiment analysis using deep learning and fuzzy logic: A comprehensive survey},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristic-driven space partitioning and ensemble
learning for imbalanced classification. <em>ASOC</em>, <em>167</em>,
112278. (<a href="https://doi.org/10.1016/j.asoc.2024.112278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced classification is a common issue in Machine Learning, particularly when misclassifying minor instances leads to significant costs. In literature, various strategies have been employed to address this problem. These include data-level, algorithm-level, cost-sensitive, and hybrid-level algorithms designed to tackle imbalanced problems. This paper aims to introduce a novel method that simultaneously enhances the ability of classification models to identify patterns more effectively and addresses imbalanced problems while minimizing alterations to the original data distribution. Our proposed framework combines ensemble learning, space partitioning, and the Synthetic Minority Oversampling Technique (SMOTE). This method decomposes the space into balanced sub-spaces and then trains an ensemble classifier based on these sub-spaces using a bagging approach. In the initial step, we develop a Space Partitioning by Metaheuristic algorithm (SPMH) to divide the space into multiple balanced subspaces. In the subsequent step, we present Imbalanced Classification by SPMH (ICSPMH) as a solution to imbalanced class problems. ICSPMH uses SPMH multiple times to divide the space into different sub-spaces, creating various sub-spaces each time. It then trains different classifiers for each portion of the space, creating an ensemble classifier through a bagging technique. To assess the performance of our proposed framework, we selected 44 well-known datasets for comparison with some state-of-the-art approaches. The results demonstrate that ICSPMH outperforms other competent methods and can potentially reduce the oversampling rate to zero. Additionally, an experiment indicated that the choice of metaheuristic algorithm in SPMH does not significantly impact the final performance. The paper also includes a correlation analysis between oversampling rate and final performance, revealing that the framework effectively eliminates imbalanced data problems with minimal changes to the original dataset. In summary, because ICSPMH applies fewer changes in data distribution and sets up local classifiers that improve classification performance, it looks like a promising method for classifying imbalanced datasets.},
  archive      = {J_ASOC},
  author       = {Saeed Kamro and Majid Rafiee and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2024.112278},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic-driven space partitioning and ensemble learning for imbalanced classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). An adaptive genetic algorithm with neighborhood search for
integrated O2O takeaway order assignment and delivery optimization by
e-bikes with varied compartments. <em>ASOC</em>, <em>167</em>, 112277.
(<a href="https://doi.org/10.1016/j.asoc.2024.112277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the dining experiences, online-to-offline (O2O) takeaway services with warm-keeping or refrigerated requirements are quickly expanding and becoming popular. However, single-compartment e-bikes are commonly used in takeaway platforms, which can only meet one kind of requirements and may result in an inflexible delivery. Within this context, a new type of e-bikes, namely e-bikes with mixed compartments, is introduced. Thus, warm-keeping and refrigerated orders can be delivered by one bike, namely on one route. This paper considers an integrated order assignment and delivery problem by e-bikes with warm, refrigerated, and mixed compartments (AD-EBM). To solve the problem, we develop a novel integer programming formulation to minimize the total cost by determining order assignments and finding optimal routes, and then some properties of the solutions are provided from the view of mathematics. An algorithm is designed by combining the self-adaptive genetic algorithm with the neighborhood search method (SGA-NS). Numerical experiments are conducted based on simulated different-scale takeaway instances. The experimental results highlight the excellent performance of the SGA-NS and the results are quite encouraging compared with Gurobi solver, SGA, and NS. The results of the model comparison demonstrate that the AD-EBM offers 12.38% total cost savings on average, compared to using only single-compartment e-bikes. A sensitivity analysis is performed to explore the effects of the mixed compartment costs, the customer acceptable delay time, the penalty costs for delays, and the e-bike capacity for the platform’s daily operations. Some management insights are provided to facilitate the O2O takeaway delivery.},
  archive      = {J_ASOC},
  author       = {Yanfang Ma and Lining Yang and Zongmin Li and Benjamin Lev},
  doi          = {10.1016/j.asoc.2024.112277},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive genetic algorithm with neighborhood search for integrated O2O takeaway order assignment and delivery optimization by e-bikes with varied compartments},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An energy-saving distributed flexible job shop scheduling
with machine breakdowns. <em>ASOC</em>, <em>167</em>, 112276. (<a
href="https://doi.org/10.1016/j.asoc.2024.112276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the change of consumption pattern, distributed flexible production is now becoming a major manufacturing mode. The machine breakdown seriously affects the production in workshops. How to efficiently adjust the scheduling scheme after machine breakdowns in distributed flexible job shop is an important concern for enterprises. Meanwhile, energy consumption is attracting more and more attention as environmental issues become increasingly serious. Thus, the energy-saving scheduling on distributed flexible job shop considering machine breakdowns is studied, and a mixed-integer programming model is established to optimize makespan, total energy consumption and machine load difference, simultaneously. Based on the problem characteristics, an improved memetic algorithm is designed. To improve the quality of initial solutions, an initialization strategy incorporating multiple rules is designed. An active decoding method with an adjustment strategy is adopted to fully utilize machine idleness. To expand the search range, the idea of dual-population collaborative optimization is used. Multi-level intelligent mutation operation strategies are introduced to further enhance the quality of solutions. In addition, an adaptive parameter rule is designed to adjust to appropriate search capabilities at different evolutionary stages. To verify the effectiveness of the proposed strategies and algorithm, the memetic algorithm, the artificial bee colony algorithm, the differential evolution algorithm and the estimation of distribution algorithm are chosen as comparison algorithms. The same iteration number and same running time are used as the termination criteria of all algorithms, and 60 instances of different scales are solved. The experimental results demonstrate that the effectiveness of the strategies and algorithm designed in this paper. Notably, our algorithm consistently achieved good results even when the same running time is used as the termination criterion.},
  archive      = {J_ASOC},
  author       = {Hongliang Zhang and Chaoqun Qin and Gongjie Xu and Yi Chen and Zhenhua Gao},
  doi          = {10.1016/j.asoc.2024.112276},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An energy-saving distributed flexible job shop scheduling with machine breakdowns},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). The continuous memory: A neural network with ordinary
differential equations for continuous-time series analysis.
<em>ASOC</em>, <em>167</em>, 112275. (<a
href="https://doi.org/10.1016/j.asoc.2024.112275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous-time series analysis has garnered significant research interest due to its extensive applications; however, it remains a challenging endeavor. In recent years, deep learning methods have achieved remarkable success in tasks such as time series classification and forecasting. Nevertheless, the inherent continuity of time series data has not been fully addressed, presenting a persistent obstacle to performance. Recent studies have highlighted a natural similarity between differential equations and continuous-time series, as both inherently encapsulate the concept of continuity. However, several critical issues with differential equations hinder their direct applicability to time series analysis. These issues include the elusive nature of analytical solutions, the indirect observability of the effects of differential equation parameters, and the dependence of numerical solutions on initial conditions. In this context, we propose the integration of differential equations into neural networks to serve as the continuous memory of the model. This integration imparts a continuous nature to the model, resulting in the development of an efficient deep learning architecture known as Long Short Deep Memory (LSDM). Furthermore, we analyze the characteristics of differential equations when employed as the memory component in neural networks, which leads to the proposal of a novel pre-training approach that incorporates an innate memory mechanism into these networks. Additionally, we utilize LSDM to construct a stacked architecture for processing very long time series data. The proposed model can naturally accommodate arbitrary time gaps between observations, thereby enhancing its effectiveness and suitability for continuous time series analysis tasks. Extensive experiments conducted on various real-world datasets demonstrate that the proposed model outperforms existing methods, providing a new solution to the challenges associated with continuous time series analysis.},
  archive      = {J_ASOC},
  author       = {Bo Li and Haoyu Chen and Zhiyong An and Yuan Yu and Ying Jia and Long Chen and Mingyan Sun},
  doi          = {10.1016/j.asoc.2024.112275},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The continuous memory: A neural network with ordinary differential equations for continuous-time series analysis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A personality-guided preference aggregator for ephemeral
group recommendation. <em>ASOC</em>, <em>167</em>, 112274. (<a
href="https://doi.org/10.1016/j.asoc.2024.112274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ephemeral group recommendation (EGR) aims to suggest items for a group of users who come together for the first time. Existing work typically consider individual preferences as the sole factor in aggregating group preferences. However, they neglect to take into account the importance of the individual inherent factors, such as personality, and thus fail to accurately simulate the group decision-making process. Additionally, these methods often struggle due to insufficient interactive records. To tackle these issues, a Pe rsonality- G uided Preference A ggregator (PEGA) is proposed, which guides the preference aggregation of group members based on their personalities, rather than relying solely on their preferences. Specifically, implicit personalities are first extracted from user reviews. Hyper-rectangles are then used to aggregate individual personalities to obtain the “Group Personality”, which allows for the learning of personality distributions within the group. Subsequently, a personality attention mechanism is employed to aggregate group preferences, and a preference-based fine-tuning module is used to balance the weights of personality and preferences. The role of personality in this approach is twofold: (1) To estimate the importance of individual users in a group and provide explainability; (2) To alleviate the data sparsity issue encountered in ephemeral groups. Experimental results demonstrate that, on four real-world datasets, the PEGA model significantly outperforms related baseline models in terms of classification accuracy and interpretability. Moreover, empirical evidence supports the idea that personality plays a pivotal role in enhancing the performance of EGR tasks.},
  archive      = {J_ASOC},
  author       = {Guangze Ye and Wen Wu and Liye Shi and Wenxin Hu and Xi Chen and Liang He},
  doi          = {10.1016/j.asoc.2024.112274},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A personality-guided preference aggregator for ephemeral group recommendation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable deep learning-based dynamic prediction of
surface settlement considering temporal characteristics during deep
excavation. <em>ASOC</em>, <em>167</em>, 112273. (<a
href="https://doi.org/10.1016/j.asoc.2024.112273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of surface settlement (SS) induced by deep foundation pit (DFP) excavation is challenging considering the complicated soil distributions and uncertain construction situations. Therefore, a Recurrent Gated Unit (GRU) neural network model incorporated with Variation Mode Decomposition (VMD) was developed for SS prediction. By data denoising with VMD and hyperparameter tuning using Bayesian Optimization (BO), the proposed GRU model achieved dynamic and accurate prediction of SS caused by DFP excavation. Shapley Additive exPlanations (SHAP) analysis was then performed to enhance the interpretability of the GRU model. The presented GRU model was validated with a case study of Wuhan Metro Line 12 Shiqiao Station. The results indicate that: (1) The GRU models with VMD data denoising technique achieved accurate prediction with the average RMSE of 0.0244, MAE of 0.0189, MAPE of 0.0017, R 2 of 0.9894, and VAF of 99.1206. (2) The GRU model outperformed the other five state-of-the-art models in predictive performance with robustness improvement of 13.5 %, 12.0 %, 50.0 %, 29.2 %, and 27.3 %, separately, compared to the other five models. (3) The historical values of the SS make the most significant contributions to the outcomes of the GRU models. In short, this study enhances the accurate and dynamic prediction of SS caused by DFP excavation, contributing to the safe execution of DFP projects.},
  archive      = {J_ASOC},
  author       = {Xuefeng An and Hanbin Luo and Fei Zheng and Yuyong Jiao and Jianfeng Qi and You Zhang},
  doi          = {10.1016/j.asoc.2024.112273},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable deep learning-based dynamic prediction of surface settlement considering temporal characteristics during deep excavation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decomposition-based multi-objective evolutionary algorithm
using infinitesimal method. <em>ASOC</em>, <em>167</em>, 112272. (<a
href="https://doi.org/10.1016/j.asoc.2024.112272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Objective Evolutionary Algorithm based on decomposition (MOEA/D) has been extensively employed to address a diverse array of real-world challenges and has shown excellent performance. However, the initial collection of aggregate weight vectors proves unsuitable for multi-objective optimization problems (MOPs) featuring intricate Pareto front (PF) structures, and the solving performance will be greatly affected when MOEA/D solves these irregular MOPs. In light of these challenges, a refined MOEA/D algorithm utilizing infinitesimal method is proposed. This algorithm incorporates the notion of global decomposition stemming from infinitesimal method to streamline the feature information of PF, thereby facilitating the adjustment of the weight vector towards optimal distribution. Consequently, enhancements in resource allocation efficiency and algorithmic performance are achieved. In the empirical investigation, the algorithm’s performance is tested on 28 benchmarks from ZDT,DTLZ and WFG test suits.Wilcoxon’s rank-sum test and Fredman’s test were carried out on performance metrics, which proved that the proposed MOEA/D-DKS was superior to other comparison algorithms.},
  archive      = {J_ASOC},
  author       = {Jing Wang and Shunce Mei and Changxin Liu and Hu Peng and Zhijian Wu},
  doi          = {10.1016/j.asoc.2024.112272},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decomposition-based multi-objective evolutionary algorithm using infinitesimal method},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonlinear randomly reuse-based mutated whale optimization
algorithm and its application for solving engineering problems.
<em>ASOC</em>, <em>167</em>, 112271. (<a
href="https://doi.org/10.1016/j.asoc.2024.112271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The whale optimization algorithm (WOA) is a meta-heuristic optimization algorithm inspired by the social behavior of whale hunting. However, the WOA has the defects of easily falling into local optimum and slow convergence speed. Therefore, to overcome the shortcomings of the original WOA, a novel variant of WOA called nonlinear randomly reuse-based mutated whale optimization algorithm (NRRMWOA) is proposed in this study. In detail, the proposed NRRMWOA includes three novel strategies as compared with original WOA. Firstly, a nonlinear adaptive parameter strategy is introduced to achieve nonlinear adjustment of search pattern along with the iteration time. The second improvement is the random reuse strategy, which can fully utilize the current optimal whale to improve the solution accuracy. The thirdly proposed late disturbance mutation strategy has the function of increasing population diversity in the later stage of iteration. The three proposed strategies complement each other and significantly improve the convergence efficiency and global exploration capability of WOA. To verify the superiority of the proposed NRRMWOA, a series of typical benchmark functions are utilized to conduct comprehensive numerical experiments. Compared with three sets of comparison algorithms (five variants of WOA, five advanced algorithms, and five well-known algorithms), the ARVs (average ranking values) obtained by NRRMWOA are 1.4667, 1.9167, and 2.1333, respectively. The rankings are all the best in the corresponding group. The detailed experimental and statistics results indicate that the presented NRRMWOA has higher convergence speed and better solution quality compared with the three sets of existing algorithms. In addition, 15 real-world optimization problems are employed to further test the performance of NRRMWOA for solving engineering optimization problems. For the results of 15 real-world optimization problems, the proposed NRRMWOA achieves the best results including 8 best “Mean”, 10 best “Min.”, and 7 best “Max.”. As a conclusion, the proposed NRRMWOA is a promising and excellent algorithm.},
  archive      = {J_ASOC},
  author       = {Lei Wu and Dengpan Xu and Qiang Guo and Erqi Chen and Wensheng Xiao},
  doi          = {10.1016/j.asoc.2024.112271},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A nonlinear randomly reuse-based mutated whale optimization algorithm and its application for solving engineering problems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized twin spatio-temporal convolutional neural network
with squeezenet transfer learning model for cancer miRNA biomarker
classification. <em>ASOC</em>, <em>167</em>, 112270. (<a
href="https://doi.org/10.1016/j.asoc.2024.112270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer MicroRNA (miRNA) biomarkers are small RNA molecules identified in cancer cells that indicate the presence or progression of cancer. miRNA is detected through circulating (liquid biopsy) and tissue-based approaches. Handling the data involves sequencing, followed by bioinformatic analysis to interpret the miRNA profiles. The valuable knowledge is extracted from the cancer genomic datasets using data mining and machine learning techniques, although the accuracy of certain classes has been unsatisfied. To address this issue, an optimized twin spatio-temporal convolutional neural network with SqueezeNet transfer learning model for cancer miRNA biomarker classification (TSTCNN-SNet-AGT-CBC) is proposed. The input imageries are gathered from Cancer Genome Atlas dataset. The noise is reduced and image quality is improved in preprocessing by applying Reliable asynchronous sampled-data filtering (RASDF). Then, features are extracted and the cancer miRNA is effectively classified as a diagnosis, therapy, and prognostic normal region using twin spatio-temporal convolutional neural network with SqueezeNet. The batch normalization layer of the twin Spatio-Temporal Convolutional Neural Network (TSTCNN) is eliminated and added with Squeeze Net Layer (SNet). Then, the Artificial Gorilla Troops Optimization (AGTO) is used for tuning the TSTCNN-SNet optimum weight parameter. The performance of proposed technique is examined under performance metrics like precision, sensitivity, F-scores and accuracy. The proposed TSTCNN-SNet-AGT-CBC method attains 24.52 %, 20.97 % and 16.36 % higher accuracy and 23.68 %, 18.77 %, and 41.36 % greater precision when compared with existing techniques.},
  archive      = {J_ASOC},
  author       = {Sanakaranarayanan R and Senthilkumar M},
  doi          = {10.1016/j.asoc.2024.112270},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized twin spatio-temporal convolutional neural network with squeezenet transfer learning model for cancer miRNA biomarker classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fermat-weber location particle swarm optimization for
cooperative path planning of unmanned aerial vehicles. <em>ASOC</em>,
<em>167</em>, 112269. (<a
href="https://doi.org/10.1016/j.asoc.2024.112269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an effective algorithm, called the Fermat-Weber location particle swarm optimization (FWL-PSO), developed for cooperative path planning of Unmanned Aerial Vehicles (UAVs). Initially, FWL-PSO is constructed by harnessing the Fermat-Weber optimality to identify potential solutions. Within the framework of FWL-PSO, a collection of high-performing particles is established, determined by their respective fitness scores. Following this, the Fermat-Weber location of these elite particles is calculated to supersede the traditional global best, thereby augmenting the learning strategy of the standard PSO. As a result, this method enables the evolution of information while encouraging search diversity. Subsequently, FWL-PSO is employed for handling the interactions of multiple UAVs. In this context, the path planning for a group of UAVs is formulated as a Nash game that incorporates all cooperative interdependencies and safety conditions. The algorithm is then integrated to solve the optimization problem for achieving the Nash equilibrium. To assess its efficacy, extensive simulations and experiments are conducted across a variety of path-planning scenarios. Comparative analyses between FWL-PSO and existing PSO variants underscore the enhanced efficiency and reliability of our proposed approach.},
  archive      = {J_ASOC},
  author       = {Lanh Van Nguyen and Ngai Ming Kwok and Quang Phuc Ha},
  doi          = {10.1016/j.asoc.2024.112269},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fermat-weber location particle swarm optimization for cooperative path planning of unmanned aerial vehicles},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust variable-order fractional PID-LP fuzzy controller for
automatic voltage regulator systems. <em>ASOC</em>, <em>167</em>,
112268. (<a href="https://doi.org/10.1016/j.asoc.2024.112268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise and robust Automatic Voltage Regulator (AVR) control is essential for maintaining a stable voltage output from a synchronous generator under varying operating conditions. This study aims to improve transient response and robustness characteristics of AVR systems by developing a novel fuzzy-based variable-order fractional PID-LP control scheme. To achieve this goal, in the first step, a novel fractional-order PID-LP controller tuned with a hybrid Particle Swarm Optimization algorithm is designed. This controller outperforms current fractional- and integer-order PID controllers in terms of performance and transient response. To further improve robustness against changing conditions in power system dynamics, in the second step, a fuzzy-based variable-order fractional PID-LP controller is developed to adaptively adjust the controller’s parameters based on error and its variations. The results demonstrate that the proposed technique surpasses recently proposed methods in terms of performance indexes, peak overshoot, rise time, and settling time. An extensive analysis is also conducted to compare the new approach with other methods in improving stability, robustness, trajectory tracking, rejecting disturbances, and handling nonlinearities and sensor delays. The results confirm the applicability and superiority of the controller in maintaining stability under fluctuating power system dynamics with up to ± ± 50% variations.},
  archive      = {J_ASOC},
  author       = {Mohsen Ahmadnia and Ahmad Hajipour and Hamidreza Tavakoli},
  doi          = {10.1016/j.asoc.2024.112268},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust variable-order fractional PID-LP fuzzy controller for automatic voltage regulator systems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An in-depth analysis of ensemble multi-criteria decision
making: A comprehensive guide to terminology, design, applications,
evaluations, and future prospects. <em>ASOC</em>, <em>167</em>, 112267.
(<a href="https://doi.org/10.1016/j.asoc.2024.112267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble multi-criteria decision-making (ensemble MCDM) aggregates results from various MCDM techniques into a cohesive ranking, aiming to mitigate individual technique weaknesses and provide a robust approach to MCDM challenges. However, literature lacks in-depth analysis and a comprehensive guide defining ensemble MCDM. Therefore, this study aims to clarify ensemble MCDM terminology, conduct a systematic literature review, and outline a roadmap for future research in this domain. A systematic review protocol with clear inclusion and exclusion criteria was implemented, selecting relevant articles from IEEE, ScienceDirect, Web of Science, and Google Scholar. Out of 730 collected articles, only 10 met the criteria for ensemble MCDM techniques. Four main data extraction techniques were employed: defining ensemble MCDM terminology, mapping the literature, identifying ensemble decision-making techniques, and exploring ensemble levels within the MCDM framework. From the review, three primary ensemble MCDM techniques emerged: statistical means, voting technique, and Half-Quadratic programming. These techniques were not only described theoretically but also implemented and demonstrated through a numerical example, supported by provided code. Furthermore, the study mathematically describes several other ensemble techniques for potential future applications. The implications of this research establish a foundational understanding of ensemble MCDM and propose a roadmap for future investigations. By addressing individual MCDM technique weaknesses and offering a reliable aggregation method, this research serves as a fundamental reference for future studies. The accompanying codes, tutorials, and mathematical illustrations aim to advance knowledge and facilitate practical applications in the field of ensemble MCDM},
  archive      = {J_ASOC},
  author       = {Bilal Bahaa Zaidan and Hassan Abdulsattar Ibrahim and Nahia Mourad and Aws Alaa Zaidan and Hossein Pilehkouhic and Sarah Qahtan and Muhammet Deveci and Dursun Delen},
  doi          = {10.1016/j.asoc.2024.112267},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112267},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An in-depth analysis of ensemble multi-criteria decision making: A comprehensive guide to terminology, design, applications, evaluations, and future prospects},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic multi-objective service composition based on
improved social learning optimization algorithm. <em>ASOC</em>,
<em>167</em>, 112266. (<a
href="https://doi.org/10.1016/j.asoc.2024.112266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technique of multi-objective service composition (MOSC) aims to create powerful and large grained services through aggregating some simple services while optimizing some conflicting objectives, which has become a hot topic in fields of service computing, cloud computing, cloud manufacturing, and so on. Due to the dynamic nature of the services and users’ objectives, the problem of MOSC is actually an dynamic multi-objective service composition problem (DMOSC). Recently, although some wonderful research has been conducted on multi-objective service composition, few studies have paid attention to the issue of DMOSC. To fill this gap, we propose the problem of DMOSC for the first time and then develop a method for DMOSC based on the improved social learning optimization algorithm (named DMOSCO). First, we construct the mathematical model for the problem of DMOSC considering the dynamics of services and users’ objectives. Then, we devise the changes perception strategy and changes response strategy to deal with the changes of candidate services and users’ objectives during the service composition process, which is designed to strike a balance between convergence and diversity within improved SLO. Finally, to solve the problem of DMOSC, we improve the SLO algorithm by designing a new observation learning operator and incorporating the changes detection strategy and response strategy. Extensive experimental results have proved the performance superiority of our proposed method.},
  archive      = {J_ASOC},
  author       = {Yan Hai and Xin Xu and Zhizhong Liu},
  doi          = {10.1016/j.asoc.2024.112266},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112266},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic multi-objective service composition based on improved social learning optimization algorithm},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of the sugeno integral in fuzzy rule-based
classification. <em>ASOC</em>, <em>167</em>, 112265. (<a
href="https://doi.org/10.1016/j.asoc.2024.112265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Rule-Based Classification System (FRBCS) is a well-known technique to deal with classification problems. Recent studies have considered the usage of the Choquet integral and its generalizations (e.g.: C T CT -integral, C F CF -Integral and C C CC -integral) to enhance the performance of such systems. Such fuzzy integrals were applied to the Fuzzy Reasoning Method (FRM) to aggregate the fired fuzzy rules when classifying new data. However, the Sugeno integral, another well-known aggregation operator, obtained good results in other applications, such as brain–computer interfaces. These facts led to the present study, in which we consider the Sugeno integral in classification problems. That is, the Sugeno integral is applied in the FRM of a widely used FRBCS, and its performance is analyzed over 33 different datasets from the literature, also considering different fuzzy measures. To show the efficiency of this new approach, the results obtained are also compared with previous studies that involved the application of different aggregation functions. Finally, we perform a statistical analysis of the application.},
  archive      = {J_ASOC},
  author       = {Jonata Wieczynski and Giancarlo Lucca and Eduardo Borges and Asier Urio-Larrea and Carlos López Molina and Humberto Bustince and Graçaliz Dimuro},
  doi          = {10.1016/j.asoc.2024.112265},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112265},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of the sugeno integral in fuzzy rule-based classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sigmoidal learning rate optimizer for deep neural network
training using a two-phase adaptation approach. <em>ASOC</em>,
<em>167</em>, 112264. (<a
href="https://doi.org/10.1016/j.asoc.2024.112264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The optimization of machine learning models is an open research question since an optimal procedure to change the learning rate throughout training is still unknown. Manually defining a learning rate schedule involves troublesome, time-consuming try-and-error procedures to determine hyperparameters, such as learning rate decay epochs and learning rate decay rates, in order to navigate the intricate landscape of loss functions. Although adaptive learning rate optimizers automatize this process, recent studies suggest they may produce overfitting and reduce performance compared to fine-tuned learning rate schedules. Considering that the new machine learning loss function approaches present landscapes with much more saddle points than local minima, we proposed the Training Aware Sigmoidal Optimizer (TASO). This automated two-phase learning rate adaptation mechanism significantly reduces the need for manual hyperparameter tuning. The first phase uses a high learning rate to quickly traverse the numerous saddle points in the error surface, while the second phase uses a low learning rate to gradually approach the center of the local minimum previously found. We compared the proposed approach with commonly used adaptive learning rate schedules such as Adam, RMSProp, and Adagrad. The validation experiments were performed in image and text datasets and showed that TASO outperformed all competing methods in optimal (i.e., performing hyperparameter validation) and suboptimal (i.e., using default hyperparameters) scenarios. In our benchmark tests, TASO demonstrated promising performance, achieving an 8.32% increase in accuracy and a significant 46.62% decrease in training loss on average across various datasets and models. This remarkable performance positions TASO ahead of well-established adaptive optimizers, suggesting higher effectiveness and consistency in performance.},
  archive      = {J_ASOC},
  author       = {David Macêdo and Cleber Zanchettin and Teresa Ludermir},
  doi          = {10.1016/j.asoc.2024.112264},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112264},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sigmoidal learning rate optimizer for deep neural network training using a two-phase adaptation approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Suppressed possibilistic fuzzy c-means clustering based on
shadow sets for noisy data with imbalanced sizes. <em>ASOC</em>,
<em>167</em>, 112263. (<a
href="https://doi.org/10.1016/j.asoc.2024.112263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Possibilistic fuzzy c-means clustering (PFCM) algorithm generates fewer overlapping clustering centers than the possibilistic c-means clustering (PCM) algorithm and possesses better noise immunity than fuzzy c-means clustering (FCM) algorithm. However, with the increasing noise intensity and number of clusters, PFCM still faces the problem of getting partially overlapping clustering centers or mislocated centers in noise regions. Moreover, the sample-size imbalance increasingly intensifies the difficulty of positioning centers of small clusters. To solve the above problems, a suppressed possibilistic fuzzy c-means clustering algorithm based on shadow sets (S-SPFCM) is proposed by introducing the shadow set theory and the &quot;suppressed competitive learning&quot; strategy. Firstly, KL divergence is introduced in the objective function of PFCM to increase the anti-noise robustness of fuzzy memberships against long-range noise and outliers. Secondly, to reduce the number of overlapping centers caused by possibilistic memberships, the shadow set theory is introduced to divide each class adaptively into three regions (core, shadow, and exclusion regions) by an uncertainty balance method. The suppressed competitive learning method is extended by modifying the memberships of points within the three regions, thus artificially guiding the iterative track of clustering centers. Meanwhile, to further reduce the influence of imbalanced sizes, a scheme to reset mislocated centers in the core and shadow regions is also designed. In addition, to improve the segmentation performance of S-SPFCM for noisy images, a suppressed possibilistic fuzzy c-means clustering algorithm based on shadow sets and local information (SL-SPFCM) is also proposed. The SL-SPFCM first improves the Euclidean distance by a distance filtering scheme. Then SL-SPFCM introduces the local median membership of each pixel into the KL divergence of the objective function. Finally, experiments on synthetic datasets and color images which are characteristic of imbalanced sizes and noise injection demonstrate the proposed S-SPFCM and SL-SPFCM algorithms achieve smaller center deviations and higher clustering accuracy compared with several state-of-the-art clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Haiyan Yu and Honglei Li and Xiaoyu Xu and Qian Gao and Rong Lan},
  doi          = {10.1016/j.asoc.2024.112263},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112263},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Suppressed possibilistic fuzzy c-means clustering based on shadow sets for noisy data with imbalanced sizes},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature radiance fields (FeRF): A multi-level feature fusion
method with deep neural network for image synthesis. <em>ASOC</em>,
<em>167</em>, 112262. (<a
href="https://doi.org/10.1016/j.asoc.2024.112262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Field (NeRF) has brought revolutionary changes to the field of image synthesis with its unique ability to generate highly realistic multi-view consistent images from a neural scene representation. However, current NeRF-based methods still largely depend on multiple, precisely posed images, especially for complex or dynamic scenes, limiting their versatility. Furthermore, some recent strategies attempt to integrate simple feature extraction networks with volume rendering techniques to reduce multi-view dependence but create blurry outputs, highlighting the need for more sophisticated feature handling to unlock NeRF&#39;s full potential. In this paper, we propose an image synthesis method named FeRF, distinguished by its capacity to perform comprehensive feature extraction on individual unposed images and facilitate feature fusion at any stage. Additionally, we present an &quot;elaborated-feature generation network&quot; (EGN) composed of four modules, which is configured with two advanced feature extraction modules aimed at precisely refining and processing subtle, complex visual features from a single image. Given that the core objective of FeRF is the precise capture and processing of intricate features from the input images, we innovatively incorporated precisely designed attention mechanisms into the network architecture to optimize and highlight the importance of key feature attributes, thereby effectively enhancing their contribution to subsequent volume rendering processes. Extensive experimentation validates the outstanding qualitative and quantitative performance of our proposed network structure. In comparison to current image feature-based generalized image synthesis methods, it achieves superior reconstruction quality and level of detail.},
  archive      = {J_ASOC},
  author       = {Jubo Chen and Xiaosheng Yu and Chengdong Wu and Xiaolei Tian and Ke Xu},
  doi          = {10.1016/j.asoc.2024.112262},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112262},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature radiance fields (FeRF): A multi-level feature fusion method with deep neural network for image synthesis},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A preordonance-based decision tree method and its parallel
implementation in the framework of map-reduce. <em>ASOC</em>,
<em>167</em>, 112261. (<a
href="https://doi.org/10.1016/j.asoc.2024.112261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In supervised classification, decision trees are one of the most popular learning algorithms that are employed in many practical applications because of their simplicity, adaptability, and other perks. The development of effective and efficient decision trees remains a major focus in machine learning. Therefore, the scientific literature provides various node splitting measurements that can be utilized to produce different decision trees, including Information Gain, Gain Ratio, Average Gain, and Gini Index. This research paper presents a new node splitting metric that is based on preordonance theory. The primary benefit of the new split criterion is its ability to deal with categorical or numerical attributes directly without discretization. Consequently, the Preordonance-based decision tree” (P-Tree) approach, a powerful technique that generates decision trees using the suggested node splitting measure, is developed. Both multiclass classification problems and imbalanced data sets can be handled by the P-Tree decision tree strategy. Moreover, the over-partitioning problem is addressed by the P-Tree methodology, which introduces a threshold ϵ ϵ as a stopping condition. If the percentage of instances in a node falls below the predetermined threshold, the expansion of the tree will be halted. The performance of the P-Tree procedure is evaluated on fourteen benchmark data sets with different sizes and contrasted with that of five already existing decision tree methods using a variety of evaluation metrics. The results of the experiments demonstrate that the P-Tree model performs admirably across all of the tested data sets and that it is comparable to the other five decision tree algorithms overall. On the other hand, an ensemble technique called “ensemble P-Tree” offers a reliable remedy to mitigate the instability that is frequently associated with tree-based algorithms. This ensemble method leverages the strengths of the P-Tree approach to enhance predictive performance through collective decision-making. The ensemble P-Tree strategy is comprehensively evaluated by comparing its performance to that of two top-performing ensemble decision tree methodologies. The experimental findings highlight its exceptional performance and competitiveness against other decision tree procedures. Despite the excellent performance of the P-Tree approach, there are still some obstacles that prevent it from handling larger data sets, such as memory restrictions, time complexity, or data complexity. However, parallel computing is effective in resolving this kind of problem. Hence, the MR-P-Tree decision tree technique, a parallel implementation of the P-Tree strategy in the Map-Reduce framework, is further designed. The three parallel procedures MR-SA-S, MR-SP-S, and MR-S-DS for choosing the optimal splitting attributes, choosing the optimal splitting points, and dividing the training data set in parallel, respectively, are the primary basis of the MR-P-Tree methodology. Furthermore, several experimental studies are carried out on ten additional data sets to illustrate the viability of the MR-P-Tree technique and its strong parallel performance.},
  archive      = {J_ASOC},
  author       = {Hasna Chamlal and Fadwa Aaboub and Tayeb Ouaderhman},
  doi          = {10.1016/j.asoc.2024.112261},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A preordonance-based decision tree method and its parallel implementation in the framework of map-reduce},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GANs-generated synthetic datasets for face alignment
algorithms in complex environments. <em>ASOC</em>, <em>167</em>, 112260.
(<a href="https://doi.org/10.1016/j.asoc.2024.112260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face alignment has matured over the past several decades, but privacy violations or data abuse have also triggered global controversy. Moreover, existing face algorithms are still challenging in complex environments. For the question: ”Can synthetic datasets introduce novel variations in real-world data?”. We proposed a new research direction concerning key point detection tasks utilizing synthetic datasets, aiming to reduce the model’s reliance on real-world datasets. Considering the differences between synthetic and real-world data, our work proposed two different transfer ways based on GANs: (1) S → R S→R model converts the synthetic face images generated by the Face middleware 3D model (FaceGen) into more realistic face images for training face alignment. (2) R → S R→S model converts the real-world face images into a synthetic style image for testing face alignment. Extensive experiments explored the synthetic data complementarity and availability.},
  archive      = {J_ASOC},
  author       = {Haoqi Gao and Xing Yang and Yihua Hu and Bingwen Wang and Haoli Xu and Zhenyu Liang and Hua Mu and Yangyang Wang and Yangxiaocao Chen},
  doi          = {10.1016/j.asoc.2024.112260},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {GANs-generated synthetic datasets for face alignment algorithms in complex environments},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aspect based sentiment analysis of consumer reviews using
unsupervised attention neural framework. <em>ASOC</em>, <em>167</em>,
112259. (<a href="https://doi.org/10.1016/j.asoc.2024.112259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consumer reviews posted on e-commerce platforms offer an assessment of their opinion and sentiments about various aspects of a product or service. The aspects may belong to a specific category at subsystem and component level. The irrealis often associated with an opinion term, if not handled carefully, may distort the sentiment scores. This paper introduces a clustering and negative-sampling backed feed-forward Unsupervised Attention Deep Neural Framework for Opinion-Aspect-Category-Irrealis-Sentiment (UAN-OACIS) Quintuple extraction . This framework splits the five subtasks into two stages. In the first stage , aspects and opinions are extracted using word embedding-based multi-layer Attention Deep Neural Network (DNN) models; while the second stage projects corresponding categories , irrealis , and sentiment scores by creating a hash table and a couple of algorithms. We also propose a POS-Tag-based algorithm to make an irrealis dictionary to suggest sentiment modification percentage scores; and another algorithm to modify existing sentiment-lexicon based on domain-specific words. We compare UAN-OACIS with four unsupervised and two semi-supervised approaches at subtask level, and two supervised approaches at quadruples level excluding irrealis, and have got comparable results. Since no method is available for overall comparison at quintuple level, we additionally study the impact of irrealis on sentiment scores and observe substantial improvement compared to state-of-the-art lexicons methods.},
  archive      = {J_ASOC},
  author       = {Atanu Dey and Mamata Jenamani},
  doi          = {10.1016/j.asoc.2024.112259},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112259},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aspect based sentiment analysis of consumer reviews using unsupervised attention neural framework},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable contrastive-based dilated convolutional
network with transformer for pediatric pneumonia detection.
<em>ASOC</em>, <em>167</em>, 112258. (<a
href="https://doi.org/10.1016/j.asoc.2024.112258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pediatric pneumonia remains a significant global threat, surpassing all other communicable diseases in child mortality and necessitating rapid diagnosis. UNICEF identifies it as a primary cause of death in children under five. However, traditional image processing methods are time-consuming and struggle to capture distinct features due to low-intensity radiation in chest radiographs. Furthermore, skewed training data complicates the extraction of reliable and relevant features. To this end, we propose a novel E X plainable C ontrastive-based Dilated C onvolutional Net work with Transformer (XCCNet) for pediatric pneumonia detection. XCCNet harnesses the spatial power of dilated convolutions and the global insights of contrastive-based transformers for effective feature refinement. A robust chest X-ray processing module tackles low-intensity radiographs, while adversarial-based data augmentation mitigates data distribution challenges. Furthermore, we actively integrate an explainability approach through feature visualization, directly aligning it with the attention region that pinpoints the presence of pneumonia or normality in radiographs. We comprehensively evaluate the efficacy of XCCNet on four publicly available datasets: Kermany, VinDR-PCXR, NIH-Pediatric, and Trivedi, achieved accuracies of 99.76%, 91.56%, 92.87%, and 97.19%, respectively. Quantitative and qualitative assessments demonstrate the effectiveness of XCCNet capabilities. XCCNet’s superior performance enhances the early diagnosis of pneumonia, potentially saving many children’s lives.},
  archive      = {J_ASOC},
  author       = {Chandravardhan Singh Raghaw and Parth Shirish Bhore and Mohammad Zia Ur Rehman and Nagendra Kumar},
  doi          = {10.1016/j.asoc.2024.112258},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112258},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable contrastive-based dilated convolutional network with transformer for pediatric pneumonia detection},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Increasing the explainability and trustiness of wang–mendel
fuzzy system for classification problems. <em>ASOC</em>, <em>167</em>,
112257. (<a href="https://doi.org/10.1016/j.asoc.2024.112257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy systems are the perfect systems for knowledge representation — the fuzzy rules on which they are based can be interpreted and explained during the presentation of the system’s results. In this paper, a new method for fuzzy system optimization is proposed in which the fuzzy rules are built with the Wang–Mendel approach. The proposed method allows taking into account a new additional criterion during optimization — trustiness, which is aimed at obtaining better trust in the created knowledge base. In addition, a new criterion of explainability and explainable interface are proposed, which are aimed at presenting fuzzy rules in an explainable form. The proposed approach was tested in many variants and compared to methods from the literature. The results of the simulations showed that not only the accuracy of the classification was improved, but also the resulting rules had less significant conflicts, and the explanation of the classification was more transparent. Such significant results confirm the effectiveness of the proposed method-it not only allowed for the simultaneous improvement of the three mentioned elements without any trade-offs compared to known methods but also provided further development potential in this direction.},
  archive      = {J_ASOC},
  author       = {Krystian Łapa},
  doi          = {10.1016/j.asoc.2024.112257},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112257},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Increasing the explainability and trustiness of Wang–Mendel fuzzy system for classification problems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive neighbors graph learning for large-scale data
clustering using vector quantization and self-regularization.
<em>ASOC</em>, <em>167</em>, 112256. (<a
href="https://doi.org/10.1016/j.asoc.2024.112256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In traditional adaptive neighbors graph learning (ANGL)-based clustering, the time complexity is more than O ( n 2 ) O(n2) , where n n is the number of data points, which is not scalable for large-scale data problems in real applications. Subsequently, ANGL adds a balance regularization to its objective function to avoid the sparse over-fitting problem in the learned similarity graph matrix. Still, the regularization may leads to many weak connections between data points in different clusters. To address these problems, we propose a new fast clustering method, namely, A daptive N eighbors G raph L earning for L arge-Scale D ata C lustering using Vector Quantization and Self-Regularization (ANGL-LDC), to perform vector quantization (VQ) on original data and feed the obtained VQ data as the input in the n × n n×n similarity graph matrix learning. Hence, the n × n n×n similarity graph matrix learning problem is simplified to weighted m × m m×m ( m ≪ n ) (m≪n) graph learning problem, where m m is the number of distinct points and weight is the duplicate times of distinct points in VQ data. Consequently, the time complexity of ANGL-LDC is much lower than that of ANGL. At the same time, we propose a new ANGL objective function with a graph connection self-regularization mechanism, where the ANGL-LDC objective function will get an infinity value if the value of one graph connection is equal to 1. Therefore, ANGL-LDC naturally avoids obtaining the sparse over-fitting problem since we need to minimize the value of ANGL-LDC’s objective function. Experimental results on synthetic and real-world datasets demonstrate the scalability and effectiveness of ANGL-LDC.},
  archive      = {J_ASOC},
  author       = {Yongda Cai and Joshua Zhexue Huang and Alladoumbaye Ngueilbaye and Xudong Sun},
  doi          = {10.1016/j.asoc.2024.112256},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112256},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive neighbors graph learning for large-scale data clustering using vector quantization and self-regularization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual channel‐spatial self‐attention transformer and CNN
synergy network for 3D medical image segmentation. <em>ASOC</em>,
<em>167</em>, 112255. (<a
href="https://doi.org/10.1016/j.asoc.2024.112255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even though the Vision Transformer leverages the self-attention mechanism to capture long-range dependencies, showing significant potential in medical image segmentation, the limited annotations in the image dataset make it difficult for the Transformer model to extract different global features, resulting in attention collapse and generating similar or identical attention maps. Previous studies have attempted to solve the problem by integrating convolutional neural layers into Transformer-based architectures. However, improper integration may lead to the inability of the model to effectively capture local and global information in both spatial and channel dimensions. To address the above issue, we propose a hybrid architecture using the Dual Channel-Spatial Self-Attention Transformer and CNN Synergy Network (DTC-SUNETR) for medical image segmentation. Specifically, we redesigned the self-attention mechanism. A novel Channel-Spatial Self-Attention (CSSA) block is introduced to integrate the enhanced channel and spatial self-attention mechanism to capture the global relationship and local structure among image features. This helps the model to more comprehensively understand the inter-dependencies between different channels and capture the relationships between different pixels, thus enhancing the feature representation of the corresponding dimensions. Simultaneously, it also improves the overall computational efficiency of the network. Extensive experiments on four different medical image segmentation datasets, including Synapse, ACDC, Brain Tumor, and Lung Tumor, demonstrate the superiority of the proposed DTC-SUNETR over state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Fan Yang and Bo Wang},
  doi          = {10.1016/j.asoc.2024.112255},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112255},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual Channel‐Spatial Self‐Attention transformer and CNN synergy network for 3D medical image segmentation},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SS-ALDL: Consistency-based semi-supervised label
distribution learning for acne severity classification. <em>ASOC</em>,
<em>167</em>, 112254. (<a
href="https://doi.org/10.1016/j.asoc.2024.112254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acne vulgaris is a common skin disease among adolescents. Accurate classification of acne severity is critical to patient treatment. Most existing acne severity classification models ignore the number of acne lesions and the similarity between samples. Moreover, training a supervised model requires collecting a large amount of labeled data, which is labor-intensive and time-consuming. To solve the above problems, this study presents a consistency-based semi-supervised label distribution learning (SS-ALDL) framework, which is the first acne-specific semi-supervised framework for acne severity classification. It generates three distributions based on acne grading criteria, including acne severity, lesion counts, and grading transformed by counts. These three distributions are integrated by multi-task learning loss and optimized in supervised training for joint acne image grading and counting. Furthermore, a feature similarity consistency learning method is proposed for semi-supervised training. By maintaining the batch-level feature similarity matrix between different samples under different perturbations, the proposed method can effectively explore extra semantic information from the unlabeled data. The performance of the proposed model is evaluated on the ACNE04 dataset, the RetinaMNIST dataset, and a private dataset. It achieves the best classification accuracy and the lowest mean absolute error. These experimental results show that the proposed method outperforms other state-of-the-art semi-supervised methods and can significantly reduce the manual assessment workload.},
  archive      = {J_ASOC},
  author       = {Wenjie Liu and Lei Zhang and Jianwei Zhang and Jiaqi Li and Junyou Wang and Xian Jiang},
  doi          = {10.1016/j.asoc.2024.112254},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112254},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SS-ALDL: Consistency-based semi-supervised label distribution learning for acne severity classification},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse large-scale high-order fuzzy cognitive maps guided by
spearman correlation coefficient. <em>ASOC</em>, <em>167</em>, 112253.
(<a href="https://doi.org/10.1016/j.asoc.2024.112253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series prediction is one of the most important applications of Fuzzy Cognitive Maps (FCMs). In general, the state of FCMs in forecasting depends only on the state of the previous moment, but in fact it is also affected by the past state. Hence Higher-Order Fuzzy Cognitive Maps (HFCMs) are proposed based on FCMs considering historical information and have been widely used for time series forecasting. However, using HFCMs to deal with sparse and large-scale multivariate time series are still a challenge, while large-scale data makes it difficult to determine the causal relationship between nodes because of the increased number of nodes, so it is necessary to explore the relationship between nodes to guide large-scale HFCMs learning. Therefore, a sparse large-scale HFCMs learning algorithm guided by Spearman correlation coefficient, called SG-HFCM, is proposed in the paper. The SG-HFCM model is specified as follows: First, the solving of HFCMs model is transform into a regression model and an adaptive loss function is utilized to enhance the robustness of the model. Second, l 1 l1 -norm is used to improve the sparsity of the weight matrix. Third, in order to more accurately characterize the correlation relationship between variables, the Spearman correlation coefficients is added as a regular term to guide the learning of weight matrices. When calculating the Spearman correlation coefficient, through splitting domain interval method, we can better understand the characteristics of the data, and get better correlation in different small intervals, and more accurately characterize the relationship between the variables in order to guide the weight matrix. In addition, the Alternating Direction Multiplication Method and quadratic programming method are used to solve the algorithms to get better solutions for the SG-HFCM, where the quadratic programming can well ensure that the range of the weights and obtaining the optimal solution. Finally, by comparing with five algorithms, the SG-HFCM model showed an average improvement of 11.93% in prediction accuracy for GRNs, indicating that our proposed model has good predictive performance.},
  archive      = {J_ASOC},
  author       = {Xuli Li and Yingcang Ma and Qimin Zhou and Xiaohong Zhang},
  doi          = {10.1016/j.asoc.2024.112253},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparse large-scale high-order fuzzy cognitive maps guided by spearman correlation coefficient},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Competitive swarm optimizer with dynamic multi-competitions
and convergence accelerator for large-scale optimization problems.
<em>ASOC</em>, <em>167</em>, 112252. (<a
href="https://doi.org/10.1016/j.asoc.2024.112252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale optimizations (LSOPs) with high dimensional decision variables have become one of the most challenging problems in engineering optimization. High dimensional information causes serious interference to the algorithm optimization performance. The optimization performance of the algorithms will be seriously degraded. Competitive swarm optimizer (CSO) is a robust algorithm to tackle LSOPs. However, CSO randomly selects two particles to compare, then generates the winner and the loser. Although this search mechanism can enhance the diversity of the swarm, a single comparison is difficult to guarantee the quality of winners and losers. Therefore, there exists a risk of producing unqualified solutions. In order to enhance the quality of solution, a novel CSO with dynamic multi-competitions and convergence accelerator, namely DMCACSO, is designed in this paper. In the DMCACSO, a dynamic multi-competitions based evolutionary information is designed to pick out the losers more efficiently and improve the quality of winners. In addition, a convergence accelerator with hybrid evolutionary strategy is developed to speed up the particle search when the algorithm is a state of stagnation. The experiment results in solving large-scale benchmark functions from CEC2010 and CEC2013 indicate that the DMCACSO has competitive optimization performance by comparing with some state-of-the-art algorithms. Finally, the DMCACSO is effective in terms of quality in solving an actual feature selection problem.},
  archive      = {J_ASOC},
  author       = {Chen Huang and Daqing Wu and Xiangbing Zhou and Yingjie Song and Huiling Chen and Wu Deng},
  doi          = {10.1016/j.asoc.2024.112252},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Competitive swarm optimizer with dynamic multi-competitions and convergence accelerator for large-scale optimization problems},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VTNet: A multi-domain information fusion model for long-term
multi-variate time series forecasting with application in irrigation
water level. <em>ASOC</em>, <em>167</em>, 112251. (<a
href="https://doi.org/10.1016/j.asoc.2024.112251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is intricately tied to production and life, garnering widespread attention over an extended period. Enhancing the performance of long-term multivariate time series forecasting (MTSF) poses a highly challenging task, as it requires mining complicated and obscure temporal patterns in many aspects. For this reason, this paper proposes a long-term forecasting model based on multi-domain fusion (VTNet) to adaptively capture and refine multi-scale intra- and inter-variate dependencies. In contrast to previous techniques, we devise a dual-stream learning architecture. Firstly, the fast Fourier Transform (FFT) is adopted to extract frequency domain information. The original sequences are then transformed into 2D visual features in the temporal-frequency domain, and a 2D-TBlock is designed for multi-scale dynamic learning. Secondly, a combination of convolution and recurrent networks continues to explore the local temporal features and preserve the global trend. Finally, multi-modal circulant fusion is applied to achieve a more comprehensive and enriched feature fusion representation, further promoting overall performance. Extensive experiments are conducted on 9 public benchmark datasets and the real-world irrigation water level to showcase VTNet’s promoted performance and generalization. Moreover, VTNet yields 46.93% and 25.36% relative improvements for water level forecasting, revealing its potential application value in water-saving planning and extreme event early warning.},
  archive      = {J_ASOC},
  author       = {Rui Dai and Zheng Wang and Wanliang Wang and Jing Jie and Jiacheng Chen and Qianlin Ye},
  doi          = {10.1016/j.asoc.2024.112251},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112251},
  shortjournal = {Appl. Soft. Comput.},
  title        = {VTNet: A multi-domain information fusion model for long-term multi-variate time series forecasting with application in irrigation water level},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ELSNC: A semi-supervised community detection method with
integration of embedding-enhanced links and node content in attributed
networks. <em>ASOC</em>, <em>167</em>, 112250. (<a
href="https://doi.org/10.1016/j.asoc.2024.112250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex network analysis, detecting communities is becoming increasingly important. However, it is difficult to fuse multiple types of information to enhance the community-detection performance in real-world applications. Besides the nodes and the edges, a network also contains the structure of communities, its networking topological structure, and the network embeddings. Note that existing works on community detection have limited usage of all these information types in combination. In this work, we designed a novel unified model called embedding-enhanced link-based semi-supervised community detection with node content (ELSNC). ELSNC integrates the structure of the topology, the priori information, the network embeddings, and the node content. First, we employ two non-negative matrix factorization (NMF)–based stochastic models to characterize the node-community membership and the content-community membership (by performing similarity detection between a topic model and the NMF). Second, we introduce the nodes’ and networking embeddings’ topological similarity into the model as topological information. To model the topological similarity, we introduce a strong constraint ( i . e ., the priori information) and apply matrix completion to identify the community membership with the network embeddings’ representation ability. Finally, we present a semi-supervised community-detection method based on NMF that combines the network topology, content information, and the network embeddings. Our work’s innovation can be captured in two points: 1) As a type of semi-supervised community detection method, we extend the theory of semi-supervised methods on attributed networks and propose a unified model that integrates multiple information types. 2) The community membership obtained by the unified model simultaneously contains different information, including the topological, content, priori, and embedding information, which can more robustly be explored in the community structure in real-world scenarios. Furthermore, we performed a comprehensive evaluation of our proposed approach compared with state-of-the-art methods on both synthetic and real-world networks. The results show that our proposed method significantly outperformed the baseline methods.},
  archive      = {J_ASOC},
  author       = {Jinxin Cao and Xiaoyang Zou and Weizhong Xu and Weiping Ding and Hengrong Ju and Lu Liu and Fuxiang Chen and Di Jin},
  doi          = {10.1016/j.asoc.2024.112250},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112250},
  shortjournal = {Appl. Soft. Comput.},
  title        = {ELSNC: A semi-supervised community detection method with integration of embedding-enhanced links and node content in attributed networks},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on aspect base sentiment analysis methods and
challenges. <em>ASOC</em>, <em>167</em>, 112249. (<a
href="https://doi.org/10.1016/j.asoc.2024.112249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis has spread to virtually every industry, including service, finance, e-commerce, consumer goods, telecommunications, health care, political campaigns, social events, and elections. Aspect-based sentiment analysis (ABSA) enables the automatic extraction of sentiment information from textual information or phrases that are deeply embedded. To address ABSA in various situations, numerous tasks for assessing various sentiment components and associated relationships. Our proposed ABSA workflow divides into broad categories: ABSA task as a single and compound, deep learning approach for ABSA. Specifically, we present a novel taxonomy for ABSA that organizes existing studies based on the axes of sentimental components of relevance, with a focus on modern improvements in complex ABSA tasks. In contrast to earlier ABSA studies that focused on a single sentiment element, numerous ABSA tasks involving numerous components have been examined in recent years in order to capture more comprehensive aspect-level sentiment polarity. Nonetheless, a comprehensive examination of the various ABSA objectives and their corresponding results is still lacking, a gap that this survey aims to address. In addition, trends in ABSA research are noted, and how the ABSA area can be progressed in the future prospective.},
  archive      = {J_ASOC},
  author       = {Mayur Wankhade and Chaitanya Kulkarni and Annavarapu Chandra Sekhara Rao},
  doi          = {10.1016/j.asoc.2024.112249},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112249},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on aspect base sentiment analysis methods and challenges},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction interval soft sensor for dissolved oxygen content
estimation in an electric arc furnace. <em>ASOC</em>, <em>167</em>,
112246. (<a href="https://doi.org/10.1016/j.asoc.2024.112246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a novel soft sensor modeling approach using Takagi–Sugeno (TS) fuzzy models and Prediction Intervals (PIs) is presented to quantify uncertainties in Electric Arc Furnace (EAF) steel production processes, namely to estimate the dissolved oxygen content in the steel bath. In real EAF operation, dissolved oxygen content is measured only a few times in the refining stage; therefore, the approach addresses the challenge of predicting unobserved output under conditions of irregular and scarce output measurements, using two distinct methods: Instant TS (I-TS) and Input Integration TS (II-TS). In the I-TS method, the model is computed for each individual indirect measurement, while the II-TS approach integrates these indirect measurements. The inclusion of PIs in TS models allows the derivation of the narrowest band containing a prescribed percentage of data, despite the presence of heteroscedastic noise. These PIs provide valuable insight into potential variability and allow decision-makers to evaluate worst-case scenarios. When evaluated against real EAF data, these methods were shown to effectively overcome the obstacles posed by scarce output measurements. Despite its simplicity, the I-TS model performed better in terms of interpretability and robustness to the operational reality of the EAF process. The II-TS model, on the other hand, showed excellent performance on all metrics but exhibited theoretical inconsistencies when deviating from typical operations. In addition, the proposed method successfully estimates carbon content in the steel bath using the established dissolved oxygen/carbon equilibrium, eliminating the need for direct carbon measurements. This shows the potential of the proposed methods to increase productivity and efficiency in the EAF steel industry.},
  archive      = {J_ASOC},
  author       = {Aljaž Blažič and Igor Škrjanc and Vito Logar},
  doi          = {10.1016/j.asoc.2024.112246},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112246},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction interval soft sensor for dissolved oxygen content estimation in an electric arc furnace},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LesionMix data enhancement and entropy minimization for
semi-supervised lesion segmentation of lung cancer. <em>ASOC</em>,
<em>167</em>, 112244. (<a
href="https://doi.org/10.1016/j.asoc.2024.112244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the location and contour of the lesion is a crucial prerequisite for medical diagnosis, subsequent personalized treatment plan and prognostic prediction of lung cancer. Semi-supervised learning and data augmentation methods facilitate deep learning to be used in many fields of medical imaging. In this paper, we introduce a novel data enhancement technique called LesionMix. This method involves extracting and reusing lesions from a limited amount of labeled CT data, thereby enhancing the efficiency of utilizing those labeled data. Meanwhile, we propose a two-stage semi-supervised training strategy called Entropy Minimization LesionMix (EMLM). In the first stage, features containing lesion contour information are rapidly learned through LesionMix data augmentation. Entropy minimization strategy optimizes the model parameters to alleviate overfitting as much as possible in the second stage and improves prediction confidence. Our proposed method is validated on the public dataset LIDC-IDRI and the in-house dataset GDPHLUAD. Extensive experiments demonstrate that our method achieves promising performance and outperforms seven state-of-the-art semi-supervised models; Moreover, ablation experiments validate the effectiveness of various aspects of our approach.},
  archive      = {J_ASOC},
  author       = {Xipeng Pan and Mingwei Chen and Huan Lin and Xinjun Bian and Siyang Feng and Jiale Chen and Lin Wang and Xin Chen and Zaiyi Liu and Rushi Lan},
  doi          = {10.1016/j.asoc.2024.112244},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112244},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LesionMix data enhancement and entropy minimization for semi-supervised lesion segmentation of lung cancer},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep contextual reinforcement learning algorithm for
scalable power scheduling. <em>ASOC</em>, <em>167</em>, 112243. (<a
href="https://doi.org/10.1016/j.asoc.2024.112243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a deep contextual reinforcement learning (DCRL) optimization algorithm to tackle the NP-hard power scheduling problem. The algorithm is based on a multi-agent simulation environment that decomposes the power scheduling problem into sequential Markov decision processes (MDPs). The simulation environment inherently corrects incorrect decisions and adjusts supply capacities so that the MDPs adhere to the optimization constraints. A deep reinforcement learning (DRL) model is trained on these MDPs to provide optimal solutions. Demonstrating its applicability and effectiveness, the proposed method is evaluated on various test systems with different unit numbers, constraints, and production cost functions. The experimental results show that the proposed method has better performance relative to alternative methods, such as binary alternative moth-flame optimization (BAMFO), binary particle swarm optimization (BPSO), teaching learning-based optimization (TLBO), new binary particle swarm optimization (NBPSO), binary learning particle swarm optimization (BLPSO), quasi-oppositional teaching learning-based algorithm (QOTLBO), binary-real-coded genetic algorithm (BRCGA), hybrid genetic-imperialist competitive algorithm (HGICA), three-stage priority list (TSPL), and hybridized evolutionary algorithm and sequential quadratic programming (HEPSQP). The novelties of the proposed method lie in its adaptability to longer planning horizons and linear dimensionality complexity, rendering it suitable for large-scale power systems.},
  archive      = {J_ASOC},
  author       = {Awol Seid Ebrie and Chunhyun Paik and Yongjoo Chung and Young Jin Kim},
  doi          = {10.1016/j.asoc.2024.112243},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112243},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep contextual reinforcement learning algorithm for scalable power scheduling},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A giza pyramids construction metaheuristic approach based on
upper bound calculation for solving the network reliability problem.
<em>ASOC</em>, <em>167</em>, 112241. (<a
href="https://doi.org/10.1016/j.asoc.2024.112241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network reliability optimization is an optimization problem that focuses on finding an optimal solution for a reliable network design. In network reliability optimization, the goal is to maximize network reliability so that the overall cost of the network is reduced at the same time. The discussion of the reliability of various systems in the field of industry and engineering is of great importance, that&#39;s why reliability optimization has received a lot of attention in recent decades. Since this problem is included in the category of NP-Hard problems, the use of soft computing methods will be highly effective in solving it. In this paper, an approach based on the Giza Pyramids Construction (GPC) metaheuristic algorithm is proposed to solve the network reliability problem. For this purpose, two independent single objective functions with constraints are defined, and then the reliability of the network is calculated through optimistic estimation using the upper bound method. In order to compare the performance, 12 types of diverse and complex network models have been generated and the proposed algorithm has been compared with 10 popular and state-of-the-art algorithms. Statistical analysis has been used to find significant differences in the performance of algorithms. Also, a real model of the university network has been generated, investigated, and solved as a case study. The results of experiments, statistical analysis, and observations show that the proposed algorithm has a better performance than other metaheuristic algorithms and the proposed approach in solving reliability is an effective and low-cost approach.},
  archive      = {J_ASOC},
  author       = {Sasan Harifi and Amirmasoud Razavi and Melika Heydari Rad and Alireza Moradi},
  doi          = {10.1016/j.asoc.2024.112241},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112241},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A giza pyramids construction metaheuristic approach based on upper bound calculation for solving the network reliability problem},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposition based cross-parallel multiobjective genetic
programming for symbolic regression. <em>ASOC</em>, <em>167</em>,
112239. (<a href="https://doi.org/10.1016/j.asoc.2024.112239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic Programming (GP) based Symbolic Regression (SR) algorithms suffer from the ineluctable effects over model bloat, blind search and diversity loss when determining explicit symbolic models to best depict the concealed laws in historical data, which often make them time-consuming and unstable. Most efforts often dealt with one of these effects, such that the algorithms still suffer from other effects. To deal with these effects, we propose a cross-parallel SR algorithm framework based on problem decomposition and multiobjective GP in this paper. The decomposition method is proposed to distill simple subproblems named global and local regression, which can be fast solved to produce various high quality models. In the proposed framework, by expressing the SR problem as the multiobjective optimization model, a number of subproblems are automatically distilled and solved in parallel to reduce model bloat and accelerate the algorithm. Traditional regression methods are employed to produce high quality models to seed the evolving populations for each subtask to maintain population diversity and improve search efficiency. Elite models obtained by each subtasks will be collected and randomly sent to other subtasks to improve the model generalization. Ablation and comparison experiments are conducted to evaluate the performance of the proposed algorithms. The ablation results show that the developed algorithm framework plays a positive role in reducing above ineluctable effects, and can fast determine concise symbolic models for the benchmarks. Comparisons by SRBench demonstrate the effectiveness of the developed algorithm on wide range problems.},
  archive      = {J_ASOC},
  author       = {Lei Fan and Zhaobing Su and Xiyang Liu and Yuping Wang},
  doi          = {10.1016/j.asoc.2024.112239},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112239},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decomposition based cross-parallel multiobjective genetic programming for symbolic regression},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An experimental study on the application of reinforcement
learning in injection molding in the spirit of industry 4.0.
<em>ASOC</em>, <em>167</em>, 112236. (<a
href="https://doi.org/10.1016/j.asoc.2024.112236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of reinforcement learning in the injection molding process is a little-researched area in the era of Industry 4.0. The use of a smart decision-making algorithm is necessary for such a complex production method. Therefore, our research aims to extend the knowledge of the practical use of reinforcement learning in injection molding. In our study, we examined the effect of the parameters of the Actor-Critic algorithm to give a broader picture of the learning process. In addition, we show how to use simulation data, as prior knowledge, to set up the injection molding process for the production of an unknown part.},
  archive      = {J_ASOC},
  author       = {Richárd Dominik Párizs and Dániel Török},
  doi          = {10.1016/j.asoc.2024.112236},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112236},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An experimental study on the application of reinforcement learning in injection molding in the spirit of industry 4.0},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated position control of tunnel boring machine during
excavation using deep reinforcement learning. <em>ASOC</em>,
<em>167</em>, 112234. (<a
href="https://doi.org/10.1016/j.asoc.2024.112234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Position control of tunnel boring machines (TBM) is critical to ensure precise construction and meets design specifications. However, the high dependence on human intervention largely affects the efficiency and reliability of the operations due to slow response, inconsistent judgment and high risk of errors. Targeting to automate the position control of the TBM to enable it to the planned route in a more efficient and reliable manner, this research proposes a deep reinforcement learning (DRL) method considering spatial-temporal dynamics to perform automated real-time TBM operations. The DRL algorithm is embedded with a time-series deep learning model to simulate the working environment with dynamic time-space variations. The whole process is coupled with the designed reward and evaluation metrics for model enhancement. To showcase the viability and applicability of the suggested approach, a project from Singapore is utilized as an illustrative example. The outcomes indicate that the suggested method exhibits robust capabilities in forecasting and controlling TBM positions, with an R 2 of 0.86 and 0.92 in forecasting the horizontal and vertical deviations of the TBM tail three steps ahead and an overall improvement of 50.7 %. The proposed method is capable of providing an optimal strategy to intelligently forecast and control the TBM deviation. The novelty of this method comes from the feasible integration of time series prediction and DRL to simulate the spatial-temporal dynamics along with TBM excavation and obtain a position control model for optimal tunneling strategy, contributing to the effective facilitation of TBM tunneling to improve efficiency and reliability.},
  archive      = {J_ASOC},
  author       = {Penghui Lin and Ankang Ji and Yunxiang Zhou and Limao Zhang and Robert L.K. Tiong},
  doi          = {10.1016/j.asoc.2024.112234},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated position control of tunnel boring machine during excavation using deep reinforcement learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An implicit aspect-based sentiment analysis method using
supervised contrastive learning and knowledge embedding. <em>ASOC</em>,
<em>167</em>, 112233. (<a
href="https://doi.org/10.1016/j.asoc.2024.112233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aspect-based sentiment analysis aims to analyze and understand people’s opinions from different aspects. Some comments do not contain explicit opinion words but still convey a clear human-perceived emotional orientation, which is known as implicit sentiment. Most previous research relies on contextual information from a text for implicit aspect-based sentiment analysis. However, little work has integrated external knowledge with contextual information. This paper proposes an implicit aspect-based sentiment analysis model combining supervised contrastive learning with knowledge-enhanced fine-tuning on BERT (BERT-SCL+KEFT). In the pre-training phase, the model utilizes supervised contrastive learning (SCL) on large-scale sentiment-annotated corpora to acquire sentiment knowledge. In the fine-tuning phase, the model uses a knowledge-enhanced fine-tuning (KEFT) method to capture explicit and implicit aspect-based sentiments. Specifically, the model utilizes knowledge embedding to embed external general knowledge information into textual entities by using knowledge graphs, enriching textual information. Finally, the model combines external knowledge and contextual features to predict the implicit sentiment in a text. The experimental results demonstrate that the proposed BERT-SCL+KEFT model outperforms other baselines on the general implicit sentiment analysis and implicit aspect-based sentiment analysis tasks. In addition, ablation experimental results show that the proposed BERT-SCL+KEFT model without the knowledge embedding module or supervised contrastive learning module significantly decreases performance, indicating the importance of these modules. All experiments validate that the proposed BERT-SCL+KEFT model effectively achieves implicit aspect-based sentiment classification.},
  archive      = {J_ASOC},
  author       = {Junsen Fu and Xianyong Li and Yihong Zhu and Yajun Du and Yongquan Fan and Xiaoliang Chen and Dong Huang and Shumin Wang},
  doi          = {10.1016/j.asoc.2024.112233},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An implicit aspect-based sentiment analysis method using supervised contrastive learning and knowledge embedding},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A continuous concrete vibration method for robots based on
machine vision with integrated spatial features. <em>ASOC</em>,
<em>167</em>, 112231. (<a
href="https://doi.org/10.1016/j.asoc.2024.112231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional manual concrete vibration work faces numerous limitations, necessitating efficient automated method to assist in this task. This study proposes a vision-based continuous concrete vibration method for vibrating robots. By enhancing the YOLOv8n model with attention mechanisms, our proposed method demonstrates a high AP of 93.31 % in identifying reinforcing grids, and an FPS of 25.6 on embedded systems. For the first time in concrete vibration tasks, this study utilizes spatial positional information to cluster coordinate data, transforming confidence-sorted data into spatially ordered sequences. Vibrating robot case test shows that the proposed method enhances the vibration speed by 22.18 % and improves the vibration success rate by 11.67 % compared to traditional strategies. Additionally, the on-site experiment conducted at four construction sites demonstrated the robustness of the proposed method. These findings advance automation in concrete vibration work, offering significant implications for the fields of robotics and construction engineering.},
  archive      = {J_ASOC},
  author       = {Tan Li and Hong Wang and Jiasheng Tan and Lingjie Kong and Daqi Jiang and Dongxu Pan and Chi Zhang},
  doi          = {10.1016/j.asoc.2024.112231},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A continuous concrete vibration method for robots based on machine vision with integrated spatial features},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal relation transformer for robust visual tracking
with dual-memory learning. <em>ASOC</em>, <em>167</em>, 112229. (<a
href="https://doi.org/10.1016/j.asoc.2024.112229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transformer trackers mostly associate multiple reference images with the search area to adapt to the changing appearance of the target. However, they ignore the learned cross-relations between the target and surrounding, leading to difficulties in building coherent contextual models for specific target instances. This paper presents a Temporal Relation Transformer Tracker (TRTT) for robust visual tracking, providing a concise approach to modeling temporal relations by dual target memory learning. Specifically, a temporal relation transformer network generates paired memories based on static and dynamic templates, which are reinforced interactively. The memory contains implicit relation hints that capture the relations between the tracked object and its immediate surroundings. More importantly, to ensure consistency of target instance identities between frames, the relation hints from previous frames are transferred to the current frame for merging temporal contextual attention. Our method also incorporates mechanisms for reusing favorable cross-relations and instance-specific features, thereby overcoming background interference in complex spatio-temporal interactions through a sequential constraint. Furthermore, we design a memory token sparsification method that leverages the key points of the target to eliminate interferences and optimize attention calculations. Extensive experiments demonstrate that our method surpasses advanced trackers on 8 challenging benchmarks while maintaining real-time running speed.},
  archive      = {J_ASOC},
  author       = {Guohao Nie and Xingmei Wang and Zining Yan and Xiaoyuan Xu and Bo Liu},
  doi          = {10.1016/j.asoc.2024.112229},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal relation transformer for robust visual tracking with dual-memory learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy self-tuning bees algorithm for designing optimal
product lines. <em>ASOC</em>, <em>167</em>, 112228. (<a
href="https://doi.org/10.1016/j.asoc.2024.112228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Product Line Design (PLD) problem is an NP-hard combinatorial optimization problem in marketing that aims at determining an optimal product line through which a firm can optimize a desired objective, like its profits or market share. Since the PLD problem has been proved to have high complexity in real-life applications, high-quality solutions have been detected by researchers who develop various optimization methods and test their performance. The Bees Algorithm (BA) is a successful swarm intelligent optimization algorithm which is based on the behavior of bees. The aim of this research is to develop and assess BA in the optimal PLD problem. In this effort, a set of fuzzy rules has been developed to autonomously compute parameters for each individual solution throughout the optimization process. The performance of two BA variants is compared with those of popular previous approaches, using both real and simulated data of customer preferences. The findings reveal that BA constitutes an enhanced alternative approach for designing optimal product lines.},
  archive      = {J_ASOC},
  author       = {Konstantinos Zervoudakis and Stelios Tsafarakis},
  doi          = {10.1016/j.asoc.2024.112228},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy self-tuning bees algorithm for designing optimal product lines},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Large-scale multiple criteria group decision-making with
information emendation based on unsupervised opinion evolutions.
<em>ASOC</em>, <em>167</em>, 112227. (<a
href="https://doi.org/10.1016/j.asoc.2024.112227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multiple criteria group decision-making (MCGDM) is prevalent in diverse decision-making scenarios, involving numerous decision makers (DMs), the set of alternatives and criteria, and continuous temporal cycles. Opinions from DMs dynamically evolve through iterative interaction, leading to dynamic opinion evolutions. However, traditional MCGDM methodology usually establish the opinion formation on a static time point throughout information aggregation, which will lead to information distortion. This study develops a novel large-scale MCGDM method with information emendation based on an unsupervised opinion dynamics (UOD) model, combining with the intuitionistic fuzzy set (IFS) and the technique for order preference by similarity to an ideal solution (TOPSIS). The IFS is utilized to quantify opinions since it can effectively achieve a tradeoff between information retention and convenience of evaluation. Simultaneously, in the proposed UOD model, the weight updating mechanism is further considered to improve the interaction adequacy, and the unsupervised mechanism for interaction threshold helps to decrease the influences of subjectivity from DMs. Moreover, numerical simulations validate the UOD model’s feasibility. Finally, a school site selection problem is carried out to elaborate the effectiveness of the proposed method. This study will provide a methodological reference for solving large-scale MCGDM problems, facilitating rapid convergence of opinions within large-scale groups, and enrich the research on opinion dynamics in the field of decision-making.},
  archive      = {J_ASOC},
  author       = {Yupeng Li and Jie Huan and Meng Liu and Na Zhang and Jin Cao and Liujun Chen},
  doi          = {10.1016/j.asoc.2024.112227},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112227},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Large-scale multiple criteria group decision-making with information emendation based on unsupervised opinion evolutions},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A surrogate-assisted expensive constrained multi-objective
global optimization algorithm and application. <em>ASOC</em>,
<em>167</em>, 112226. (<a
href="https://doi.org/10.1016/j.asoc.2024.112226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expensive multi-objective optimization problems (MOPs) have seen the successful applications of surrogate-assisted evolutionary algorithms (SAEAs). Nevertheless, the majority of SAEAs are developed for costly unconstrained optimization, and costly constrained MOPs (CMOPs) have received less attention. Therefore, this article proposes a surrogate-assisted global optimization algorithm (named CTEA) for solving CMOPs within a very limited number of fitness evaluations. The proposed algorithm combines two selection frameworks, a bi-level selection framework, and an adaptive sampling framework, to enhance optimization performance. Leveraging on a constraint-improving strategy and a Pareto-based three-indicator criterion (convergence, constraint, and diversity indicators) at the different levels, the proposed bi-level selection framework can select more promising solutions. Moreover, an adaptive sampling framework is developed to prioritize objective and constraint functions and select the candidate solutions for real function evaluations according to the priority. Experimental results demonstrate that the proposed CTEA exhibits superior performance when compared with five state-of-the-art algorithms, achieving the best results in 61.9 % out of the 64 test instances. Finally, CTEA is applied to the multidisciplinary design optimization of blended-wing-body underwater gliders, and an impressive solution set is obtained.},
  archive      = {J_ASOC},
  author       = {Wenxin Wang and Huachao Dong and Xinjing Wang and Peng Wang and Jiangtao Shen and Guanghui Liu},
  doi          = {10.1016/j.asoc.2024.112226},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112226},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A surrogate-assisted expensive constrained multi-objective global optimization algorithm and application},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable AI-driven machine learning for heart disease
detection using ECG signal. <em>ASOC</em>, <em>167</em>, 112225. (<a
href="https://doi.org/10.1016/j.asoc.2024.112225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of countries globally find that cardiovascular diseases have the greatest death rate. Early detection of heart disease is necessary to prevent deaths from it. Big data for medical diagnostics has made it easier to construct sophisticated machine learning (ML) and deep learning (DL)-based models for the early automatic identification of heart disease. Using three common ECG datasets, i.e. , the Physionet Challenge 2016, the PASCAL Challenge Competition, and the MIT-BIH - random forest (RF) and extreme gradient boosting (XGBoost), two tree explainer classifiers are proposed for the detection of heart disease in this study. During pre-processing variety of techniques like bandpass filtering, denoising, butterworth filtering and zero-phase filtering of the signal are carried out. After the signals are restored to the time domain form using IDWT, DWT and EWT are utilized to extract features. SHapley Additive exPlanations (SHAP) is used for the feature importance which identifies important features that have more impact on the prediction output of the models. It is demonstrated from the simulation results that EWT with XGB performs well in the three datasets considered. RF explainer algorithm with EWT feature performs best and results in an AUC of 95.36 % for the PASCAL Challenge Competition dataset. However, with AUC values of 97.44 % and 98.25 % for the Physionet Challenge 2016 and MIT-BIH datasets, respectively, the XGB explainer algorithm with EWT features performs better than (DWT+XGB), (DWT+RF), and (EWT+RF). When all three datasets are compared to the current models, overall the suggested model performs better and has a higher AUC.},
  archive      = {J_ASOC},
  author       = {Babita Majhi and Aarti Kashyap},
  doi          = {10.1016/j.asoc.2024.112225},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112225},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable AI-driven machine learning for heart disease detection using ECG signal},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sound event detection support system for smart home based
on “two-to-one” teacher–student learning. <em>ASOC</em>, <em>167</em>,
112224. (<a href="https://doi.org/10.1016/j.asoc.2024.112224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound event detection (SED) is a core technology in smart home projects that rely on detected sound events to trigger specific actions. SED systems face two major challenges: high labeling costs and complex acoustic environments. To reduce labeling costs, some semi-supervised systems extract both global and local features for classification. However, these methods treat global and local features equally, not accounting for their varying importance when recognizing different types of sound events. Furthermore, to address complex acoustic environments, some studies use multitask learning frameworks to introduce SED-related tasks as auxiliaries to improve detection performance. However, these methods fail to align tasks within the framework, leading to conflicting outputs that may limit system performance. To address these issues, in this paper we propose a “two-to-one” teacher-student learning based semi-supervised SED system. This system employs a gating mechanism to selectively enhance global and local features, improving adaptability to different types of sound events, and incorporates a cross-task alignment module to interact SED with related tasks, reducing the risk of performance degradation caused by conflicting outputs. Experimental results on two datasets demonstrate that our system achieves the best performance in all metrics, with EB-F1 scores of 48.1 % and 64.7 %, representing improvements of 15.3 % and 10.6 % over the baseline ConformerSED system, respectively. Our work offers an effective SED solution for smart home projects by providing a semi-supervised SED system that performs well while reducing labeling costs.},
  archive      = {J_ASOC},
  author       = {Rongyan Wang and Yan Leng and Jian Zhuang and Chengli Sun},
  doi          = {10.1016/j.asoc.2024.112224},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112224},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sound event detection support system for smart home based on “two-to-one” teacher–student learning},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A tree seed algorithm with multi-strategy for parameter
estimation of solar photovoltaic models. <em>ASOC</em>, <em>167</em>,
112220. (<a href="https://doi.org/10.1016/j.asoc.2024.112220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree seed algorithm, which is one of the metaheuristics algorithms recently proposed for the solution of continuous optimization problems, has an effective algorithmic structure inspired by the relation between trees and seeds. At the same time, the use of two different solution generation mechanisms by depending on the control parameter in TSA aims to balance the exploration and exploitation capabilities of the algorithm. However, when the structure of the algorithm is examined in detail, it is seen that there are some disadvantages such as loss of population diversity and getting stuck in local minimums. To overcome these disadvantages in the basic algorithm, three different approaches (self-adaptive weighting mechanism, chaotic elite learning approach and experience-based learning method) were proposed to TSA under the name of multi-strategies in this study. The algorithm improved with these approaches is named as the multi-strategy-based tree seed algorithm (MS-TSA). MS-TSA was first tested on CEC2017 functions. Then MS-TSA was applied to the problems in the CEC2020 competition and compared with the results of the best performing algorithms in this competition. As a result of the comparisons, MS-TSA was found to be a competitive method on solving benchmark functions. Then, parameter estimation of single diode, double diode and photovoltaic module models using the input data of various solar panels was carried out by the MS-TSA. The results obtained with MS-TSA were compared with both the results of the basic TSA and the results of well-known algorithms in the literature. The results obtained are 9.8642E-04, 9.8356E-04, 2.4251E-03, 1.7534E-03 respectively. As a result of the comparative analysis, the lowest RMSE value was obtained by MS-TSA. In addition, comprehensive performance analyzes of the algorithms were made with the convergence curve, boxplots, current ( I ) - voltage ( V ) and power ( P ) - voltage ( V ) characteristic curves obtained according to the experimental results. As a result of the experiments and analyses, MS-TSA was found to be a more successful method than the compared algorithms in parameter estimation of PV models.},
  archive      = {J_ASOC},
  author       = {Ayşe Beşkirli and İdiris Dağ and Mustafa Servet Kiran},
  doi          = {10.1016/j.asoc.2024.112220},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112220},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A tree seed algorithm with multi-strategy for parameter estimation of solar photovoltaic models},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A linear programming aggregation method based on
generalized zhenyuan integral in q-ROFN environment and the application
of talent recruitment in universities. <em>ASOC</em>, <em>167</em>,
112214. (<a href="https://doi.org/10.1016/j.asoc.2024.112214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reasonable ranking of binary pairs that characterize fuzzy information in many fuzzy decision problems is very important. To overcome some defects of the existing score functions for the q-rung orthopair fuzzy numbers (q-ROFNs), a novel score function and ranking criterion are proposed by the q-compression transformation and hesitation factor. The main motivation is to introduce the generalized Zhenyuan (GZ)-integral into the q-ROFN environment, and cleverly transform the aggregation operations into a linear programming problem through the arithmetic operations of q-ROFNs. The main contribution is to solve the aggregation problem of q-rung orthopair fuzzy generalized Zhenyuan integral ordered weighted average (q-ROFGZIOWA) operator through the optimization technique of linear programming, and a new decision making method is established by using the q-ROFGZIOWA operator and ranking criterion. The main innovation is to map all q-ROFNs to the unit triangle in the first quadrant (converted into intuitionistic fuzzy numbers, IFNs) according to the q-compression transformation in geometric significance, and the novel score function and its ranking criterion are proposed by combining hesitation factor, and then the aggregation operation based on generalized Z-integral is converted to an optimization problem in linear programming. Finally, the superiority of the proposed method are verified by comparing the aggregation results of two integral operators through an example, and apply the proposed method to the optimal decision-making of talent recruitment in universities. The proposed method can not only correct some flaws in the ranking of existing q-ROFNs, but also overcomes some defects of existing Choquet integral average (geometric) operators in a q-ROFN environment. These results are of great significance for further research on the widespread application of q-ROFNs.},
  archive      = {J_ASOC},
  author       = {Deli Zhang and Guijun Wang},
  doi          = {10.1016/j.asoc.2024.112214},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A linear programming aggregation method based on generalized zhenyuan integral in q-ROFN environment and the application of talent recruitment in universities},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-strategy three-way decision approach for tri-state
risk loss under q-rung orthopair fuzzy environment. <em>ASOC</em>,
<em>167</em>, 112197. (<a
href="https://doi.org/10.1016/j.asoc.2024.112197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the decision-making challenge arising from the uncertainty of human cognition, three-way decision (3WD) and q-rung orthopair fuzzy sets (q-ROFSs) are integrated in this paper to propose a multi-strategy three-way decision approach (MS3WDA) for tri-state risk loss (TSRL) under q-rung orthopair fuzzy environment. Based on the ternary thinking of human cognition, the risk loss with hesitation state is considered and constructed under q-rung orthopair fuzzy environment. The TSRL with hesitation state is further constructed by combining the q-rung orthopair fuzzy (q-ROF) information. The conditional probability adopted by the original object classes is improved and extended by the three components of q-ROFSs. Next, the TSRL with q-ROF information and three components of q-ROFSs are integrated with decision-theoretic rough sets (DTRSs) to establish a novel 3WD model. Some relevant properties are also analyzed and discussed for the developed 3WD model. Then, its multi-strategy decision method is proposed based on the multi-strategy perspective. The related strategies with five different levels are designed by considering three different risk appetite perspectives and four different aspects of q-ROF information. The relevant threshold theorems are also given and proved to further provide the theoretical support for our MS3WDA. According to the five different strategies, we further derive the corresponding decision rules of MS3WDA. The key steps and specific algorithm are summarized for MS3WDA. Finally, a case study is provided to demonstrate the practicability and feasibility of MS3WDA. Meanwhile, the rationality, robustness and superiority of MS3WDA are further validated by the sensitivity analysis and comparative analysis.},
  archive      = {J_ASOC},
  author       = {Ping Wu and Yihua Zhong and Chuan Chen and Yanlin Wang and Chao Min},
  doi          = {10.1016/j.asoc.2024.112197},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112197},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-strategy three-way decision approach for tri-state risk loss under q-rung orthopair fuzzy environment},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing robustness and time efficiency of random vector
functional link with optimized affine parameters in activation functions
and orthogonalization. <em>ASOC</em>, <em>167</em>, 112184. (<a
href="https://doi.org/10.1016/j.asoc.2024.112184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random Vector Functional Link (RVFL) is a widely used learning technique due to its less computational complexity, fast learning speed, and ease of implementation. However, generalization ability of RVFL is not good because its randomly generated parameters in the hidden layer makes input data distribution vulnerable to saturated regime of an activation function. For making data distribution evenly distributed within the hidden layer, this paper proposes a robust and efficient affine-transformation-based RVFL approach, termed as RT-ATRVFL, with optimized parameters. The proposed RT-ATRVFL, introduces sparse and shallow network in RVFL, that uses a subset of hidden layer structure, and obtains optimized and robust affine parameters for an activation function. This way it not only avoids saturated regime, but also learns the non-linearity more effectively and efficiently. This paper also investigates the different variants of the proposed approaches viz., RT-ATRVFL-ORTHO, RT-ATRVFL-CLOG, and RT-ATRVFL-CLOG-ORTHO using orthogonalization and cloglogm activation functions. For a thorough investigation of the proposed approaches, we conduct extensive experiments considering 28 benchmark classification datasets for a set of [60,2000] Monte Carlo runs. We show that our proposed approaches are more generalized and reliable, and outperform the affine-transformation-based extreme learning machine (ATELM) and its variants in terms of accuracy and computational time. Also, analysis of the results through well-accepted metrics such as Levene’s test, inter quartile range, mean absolute deviation and standard deviation confirms that proposed RT-ATRVFL and its other introduced variants are more robust than their respective counterparts. Results of two well-known statistical significance tests: Wilcoxon test and Friedman ranking, and the time complexity analysis also establishes the superiority of the proposed RT-ATRVFL approach and its other variants over their respective counterparts.},
  archive      = {J_ASOC},
  author       = {Shubham Srivastav and Sandeep Kumar and Pranab K. Muhuri},
  doi          = {10.1016/j.asoc.2024.112184},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112184},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing robustness and time efficiency of random vector functional link with optimized affine parameters in activation functions and orthogonalization},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling barriers to the adoption of metaverse in the
construction industry: An application of fuzzy-DEMATEL approach.
<em>ASOC</em>, <em>167</em>, 112180. (<a
href="https://doi.org/10.1016/j.asoc.2024.112180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idea of the metaverse has been rather popular in many different sectors since it provides immersive virtual reality where users may connect and participate. Leveraging metaverse technologies—such as virtual reality and augmented reality—for project visualization, collaborative design, and training simulations—in the building industry is attracting increasing attention. Notwithstanding its possible advantages, there is a clear knowledge vacuum on the obstacles preventing the general acceptance of metaverse technology in building. The study intends to close this gap by means of a literature review and analysis of the identified barriers using the fuzzy-DEMATEL technique, therefore separating the causal links among them. There were 26 obstacles found in the literature review and expert comments, arranged technically, organizationally, environmentally, socially, and economically. The most important obstacles shown by results are Security Concerns, Resistance to Change, Lack of Expertise, Siloed Departments, and Training and Education Needs. The results of this research provide building companies and legislators with important new perspectives and direction in developing plans to remove obstacles and encourage the use of metaverse technology. Moreover, the findings of the study provide a road map for industry players in tackling the important issue of restricted data capacities, thereby enabling a better and more successful integration of metaverse technology into building methods.},
  archive      = {J_ASOC},
  author       = {Muhammad Irfan and Abishek Rauniyar and Jin Hu and Atul Kumar Singh and Sathvik Sharath Chandra},
  doi          = {10.1016/j.asoc.2024.112180},
  journal      = {Applied Soft Computing},
  month        = {12},
  pages        = {112180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling barriers to the adoption of metaverse in the construction industry: An application of fuzzy-DEMATEL approach},
  volume       = {167},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristics-based multistage iterative optimization
framework for decomposed high-speed rail systems planning problem.
<em>ASOC</em>, <em>166</em>, 112357. (<a
href="https://doi.org/10.1016/j.asoc.2024.112357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-speed rail (HSR) systems planning and decision-making is a large-scale, multistage, intricate problem, where each stage represents a complex sub-problem. The solution approach to the multistage problem needs information exchange across the stages. A non-optimal planning and decision at any stage could yield a sub-optimal HSR system. This study proposes a novel multistage iterative HSR station location and alignment optimization framework (HSLAOF) to solve the main problem by decomposing it into the identification of (a) HSR potential regions and corridor, (b) station location, (c) shortest sequence/corridor of stations, and (d) collision-free alignment (horizontal and vertical) sub-problems. It integrates these sub-problems, which are solved separately using customized artificial intelligence- and motion planning-based metaheuristics such as particle swarm optimization for station location, ant colony optimization for shortest sequence of stations, path planner method for horizontal alignment and exploration, and exploitation-based ant colony optimization for vertical profile. In HSLAOF, the information from the upper and lower stages of the optimization process are exchanged and combined for a holistic optimized HSR system. Application of HSLAOF in the Mumbai-Ahmedabad HSR project yielded a solution that has 14.24 % lower total system cost, no environmental impact, 5.82 % better socio-economic impact, 21.14 % lesser noise and vibration impacted population, and 2.79 % higher station location utility than the one proposed by the experts. For predefined station location, the HSLAOF yielded alignment with 13.90 % lower total system cost and 24.00 % less noise and vibration impacted population than the one developed by experts. It required about 150 min to obtain a solution for each iteration.},
  archive      = {J_ASOC},
  author       = {Sandeepan Roy and Avijit Maji},
  doi          = {10.1016/j.asoc.2024.112357},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112357},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristics-based multistage iterative optimization framework for decomposed high-speed rail systems planning problem},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term power load forecasting based on Seq2Seq model
integrating bayesian optimization, temporal convolutional network and
attention. <em>ASOC</em>, <em>166</em>, 112248. (<a
href="https://doi.org/10.1016/j.asoc.2024.112248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power load forecasting is of great significance to the electricity management. However, extant research is insufficient in comprehensively combining data processing and further optimization of existing prediction models. Therefore, this paper propose an improved power load prediction methods from two aspects: data processing and optimization of Sequence to Sequence (Seq2Seq) model. Firstly, in the data processing, Extreme Gradient Boosting (XGBoost) is adopted to eliminate the redundant features for feature extraction. Meanwhile, Successive Variational Mode Decomposition (SVMD) is employed to solve the unsteadiness and nonlinearities present in electricity data during the decomposition process. Secondly, the Seq2Seq model is selected and improved with a variety of machine learning methods. Specifically, input data features are extracted using Convolutional Neural Networks (CNN), enhancing the decoder&#39;s focus on vital information with the Attention mechanism (AM). Temporal Convolutional Network (TCN) serves as both the encoder and decoder of Seq2Seq, with further optimization of the model parameters through the Bayesian Optimization (BO) algorithm. Finally, the cases of two real power market datasets in Switzerland and Singapore illustrate the efficiency and superiority of proposed hybrid forecasting method. Through a comprehensive comparison and analysis with the other six models and four commonly used evaluation metrics, it is evident that the proposed method excels in performance, attaining the highest level of prediction accuracy, with the highest accuracy rate of 95.83 %. Consequently, it exhibits significant practical utility in the realm of power load forecasting.},
  archive      = {J_ASOC},
  author       = {Yeming Dai and Weijie Yu},
  doi          = {10.1016/j.asoc.2024.112248},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112248},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term power load forecasting based on Seq2Seq model integrating bayesian optimization, temporal convolutional network and attention},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-efficient multi-objective distributed assembly
permutation flowshop scheduling by q-learning based meta-heuristics.
<em>ASOC</em>, <em>166</em>, 112247. (<a
href="https://doi.org/10.1016/j.asoc.2024.112247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses energy-efficient multi-objective distributed assembly permutation flowshop scheduling problems with minimisation of maximum completion time, mean of earliness and tardiness, and total carbon emission simultaneously. A mathematical model is introduced to describe the concerned problems. Five meta-heuristics are employed and improved, including the artificial bee colony, genetic algorithms, particle swarm optimization, iterated greedy algorithms, and Jaya algorithms. To improve the quality of solutions, five critical path-based neighborhood structures are designed. Q-learning, a value-based reinforcement learning algorithm that learns an optimal strategy by repeatedly interacting with the environment, is embedded into meta-heuristics. The Q-learning guides algorithms intelligently select appropriate neighborhood structures in the iterative process. Then, two machine speed adjustment strategies are developed to further optimize the obtained solutions. Finally, extensive experimental results show that the Jaya algorithm with Q-learning has the best performance for solving the considered problems.},
  archive      = {J_ASOC},
  author       = {Hui Yu and Kaizhou Gao and Zhiwu Li and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2024.112247},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112247},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy-efficient multi-objective distributed assembly permutation flowshop scheduling by Q-learning based meta-heuristics},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tumor classification algorithm via parallel collaborative
optimization of single- and multi-objective consistency on PET/CT.
<em>ASOC</em>, <em>166</em>, 112245. (<a
href="https://doi.org/10.1016/j.asoc.2024.112245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malignant tumors still have a high incidence and mortality rate worldwide. Pathological examination remains the clinical gold standard for tumor diagnosis. However, some patients cannot undergo pathological examination due to advanced age and special lesion location. Therefore, making full use of PET/CT to assist doctors in tumor classification has important clinical significance. Since category labels are calibrated according to pathological images, it is difficult to obtain effective pathological category features directly using PET-CT image modeling. In response to this problem, this paper proposes a novel tumor classification algorithm. This method fully utilizes multi-gray-level 3D gray-level co-occurrence matrix and the proposed rough and fine constraint network under the constraint loss of rough and fine labels. Based on single- and multi-objective consistency, a parallel collaborative optimization method is proposed, including category consistency loss and feature specificity loss. To reduce the interference of redundant features, an improved Boruta feature selection method using multiple classifiers and multiple steps is proposed. The final result is obtained through a conditional weighted voting function. The proposed method shows excellent performance in both the submodels and the fusion model. We validated the proposed tumor classification method on three datasets and achieved good performance with the accuracy of 0.80–0.85 and F1-score of 0.78–0.88. The results indicate that the proposed method has good performance and generalization ability.},
  archive      = {J_ASOC},
  author       = {Yang Zhou and Huiyan Jiang and Qiu Luan and Yaming Li and Xuena Li and Yan Pei},
  doi          = {10.1016/j.asoc.2024.112245},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112245},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tumor classification algorithm via parallel collaborative optimization of single- and multi-objective consistency on PET/CT},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study on improving drug–drug interactions prediction using
convolutional neural networks. <em>ASOC</em>, <em>166</em>, 112242. (<a
href="https://doi.org/10.1016/j.asoc.2024.112242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate studies on drug–drug interactions (DDIs) can evade possible adverse side effects due to the ingestion of multiple drugs. This paper proposes a novel framework called Similarity Network Fusion and Hybrid Convolutional Neural Network (SNF–HCNN) to predict the DDIs better. The proposed framework leverages data from DrugBank, PubChem, and SIDER. Seven critical drug features are extracted: Target , Transporter , Enzymes , Chemical substructure , Carrier , Offside , and Side effects . The Jaccard Similarity measure evaluates the similarity of drug features to construct a comprehensive similarity matrix that effectively captures potential drug relationships and patterns. The similarity selection process identifies the most relevant features, reduces redundancy, and enhances identifying potential drug interactions. Integrating Similarity Network Fusion (SNF) with the selected similarity matrix ensures a comprehensive representation of drug features and leads to superior accuracy compared to conventional methods. Our experimental results demonstrate the effectiveness of the proposed hybrid convolutional neural network (HCNN) architectures, such as CNN+LR (CNN+Logistic Regression), CNN+RF (CNN+Random Forest), and CNN+SVM (CNN+Support Vector Machine), showing impressive accuracies of 95.19%, 94.45%, and 93.65%, respectively. Moreover, CNN+LR outperforms other approaches regarding precision, sensitivity, F1-score, and AUC score, which implicate better outcomes for ensuring medication safety aspects in clinical settings in the future.},
  archive      = {J_ASOC},
  author       = {Deepa Kumari and Dhruv Agrawal and Arjita Nema and Nikhil Raj and Subhrakanta Panda and Jabez Christopher and Jitendra Kumar Singh and Sachidananda Behera},
  doi          = {10.1016/j.asoc.2024.112242},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112242},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A study on improving drug–drug interactions prediction using convolutional neural networks},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ultra-high-definition multi-exposure image fusion method
based on multi-scale feature extraction. <em>ASOC</em>, <em>166</em>,
112240. (<a href="https://doi.org/10.1016/j.asoc.2024.112240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple exposure image fusion is a technique used to obtain high dynamic range images. Due to its low cost and high efficiency, it has received a lot of attention from researchers in recent years. Currently, most deep learning-based multiple exposure image fusion methods extract features from different exposure images using a single feature extraction method. Some methods simply rely on two different modules to directly extract features. However, this approach inevitably leads to the loss of some feature information during the feature extraction process, thus further affecting the performance of the model. To minimize the loss of feature information as much as possible, we propose an ultra-high-definition (UHD) multiple exposure image fusion method based on multi-scale feature extraction. The method adopts a U-shaped structure to construct the overall network model, which can fully exploit the feature information at different levels. Additionally, we construct a novel hybrid stacking paradigm to combine convolutional neural networks and Transformer modules. This combined module can extract both local texture features and global color features simultaneously. To more efficiently fuse and extract features, we also design a cross-layer feature fusion module, which can adaptively learn the correlation between features at different layers. Numerous quantitative and qualitative results demonstrate that our proposed method performs well in UHD multiple exposure image fusion.},
  archive      = {J_ASOC},
  author       = {Xiuyi Jia and Qiaowanni Lin and Weiping Ding},
  doi          = {10.1016/j.asoc.2024.112240},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112240},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ultra-high-definition multi-exposure image fusion method based on multi-scale feature extraction},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FMCF: A fusing multiple code features approach based on
transformer for solidity smart contracts source code summarization.
<em>ASOC</em>, <em>166</em>, 112238. (<a
href="https://doi.org/10.1016/j.asoc.2024.112238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A smart contract is a software program executed on a blockchain, designed to facilitate functionalities such as contract execution, asset administration, and identity validation within a secure and decentralized ecosystem. Summarizing the code of Solidity smart contracts aids developers in promptly grasping essential functionalities, thereby enhancing the security posture of Ethereum-based projects. Existing smart contract code summarization works mainly use traditional information retrieval and single code features, resulting in suboptimal performance. In this study, we propose a fusing multiple code features (FMCF) approach based on Transformer for Solidity summarization. First, FMCF created contract integrity modeling and state immutability modeling in the data preprocessing stage to process and filter data that meets security conditions. At the same time, FMCF retains the self-attention mechanism to construct the Graph Attention Network (GAT) encoder and CodeBERT encoder, which respectively extract multiple feature vectors of the code to ensure the integrity of the source code information. Furthermore, the FMCF uses a weighted summation method to input these two types of feature vectors into the feature fusion module for fusion and inputs the fused feature vectors into the Transformer decoder to obtain the final smart contract code summarization. The experimental results show that FMCF outperforms the standard baseline methods by 12.45% in the BLEU score and maximally preserves the semantic information and syntax structures of the source code. The results demonstrate that the FMCF can provide a good direction for future research on smart contract code summarization, thereby helping developers enhance the security of development projects.},
  archive      = {J_ASOC},
  author       = {Gang Lei and Donghua Zhang and Jianmao Xiao and Guodong Fan and Yuanlong Cao and Zhiyong Feng},
  doi          = {10.1016/j.asoc.2024.112238},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112238},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FMCF: A fusing multiple code features approach based on transformer for solidity smart contracts source code summarization},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient face anti-spoofing via head-aware transformer
based knowledge distillation with 5 MB model parameters. <em>ASOC</em>,
<em>166</em>, 112237. (<a
href="https://doi.org/10.1016/j.asoc.2024.112237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although face recognition technology has been applied in many scenarios, it still suffers from many types of presentation attacks, so face anti-spoofing (FAS) becomes a hot topic in computer vision. Recently, vision transformer is recognized as the mainstream architecture for FAS, which always relies on auxiliary information, sophisticated tricks and huge model parameters. Considering that face based identity authentication usually takes place on mobile-like devices, therefore how to design an effective and lightweight model is of great significance. Inspired by the powerful global modeling ability of self-attention and the model compression ability of knowledge distillation, a simple yet effective knowledge distillation approach is proposed for FAS under transformer framework. Our primary idea is to leverage the rich knowledge of a teacher network pre-trained on large-scale face data to guide the learning of a lightweight student network. The main contributions of our method are threefold: (1) Feature- and logits-level distillation are combined to transfer the rich knowledge of teacher to student. (2) A head-aware strategy is proposed to deal with the dimension mismatching issue of middle encoder layers between teacher and student networks, in which a novel attention head correlation matrix is introduced. (3) Our method can bridge the performance gap between teacher and student, and the resulting student network is extremely lightweight with only 5 MB parameters. Extensive experiments are conducted on three public face-spoofing datasets, CASIA-FASD, Replay-Attack and OULU-NPU, the results demonstrate that our method can obtain performance on par with or superior to most FAS methods and outperform many knowledge distillation methods. Meanwhile, the distilled student network achieves excellent performance with 17 × × fewer parameters and 9 × × faster inference time compared to the teacher network. The code will be publicly available at https://github.com/Maricle-zhangjun/HaTFAS .},
  archive      = {J_ASOC},
  author       = {Jun Zhang and Yunfei Zhang and Feixue Shao and Xuetao Ma and Shu Feng and Yongfei Wu and Daoxiang Zhou},
  doi          = {10.1016/j.asoc.2024.112237},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112237},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient face anti-spoofing via head-aware transformer based knowledge distillation with 5 MB model parameters},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on hand gesture recognition based on surface
electromyography: Fundamentals, methods, applications, challenges and
future trends. <em>ASOC</em>, <em>166</em>, 112235. (<a
href="https://doi.org/10.1016/j.asoc.2024.112235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gestures are crucial for developing prosthetic and rehabilitation devices, enabling intuitive human–computer interaction (HCI) and improving accessibility for individuals with impairments. Recently, gesture recognition systems based on surface electromyography (sEMG) have been widely employed in various fields, demonstrating remarkable advantages and developments. In this paper, we present a comprehensive survey on sEMG-based hand gesture recognition. We provide an overview of the basic knowledge and background of sEMG signals and the acquisition equipment used. We delve into the applied feature extraction methods and classification models, focusing on recent advances in deep learning techniques. We also identify the datasets of sEMG signals used for hand gesture recognition. Moreover, we highlight recent applications of sEMG-based gesture recognition methods, including HCI, sign language recognition, rehabilitation, prosthesis control, and exoskeletons for augmentation. Additionally, we outline the latest innovative progress in this field, such as the influence of force, user identity detection, and migration effects. We also discuss the current limitations and challenges. Finally, we summarize the main findings and discuss future directions to enhance sEMG-based hand gesture recognition.},
  archive      = {J_ASOC},
  author       = {Sike Ni and Mohammed A.A. Al-qaness and Ammar Hawbani and Dalal Al-Alimi and Mohamed Abd Elaziz and Ahmed A. Ewees},
  doi          = {10.1016/j.asoc.2024.112235},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on hand gesture recognition based on surface electromyography: Fundamentals, methods, applications, challenges and future trends},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative coevolution for non-separable large-scale
black-box optimization: Convergence analyses and distributed
accelerations. <em>ASOC</em>, <em>166</em>, 112232. (<a
href="https://doi.org/10.1016/j.asoc.2024.112232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the ubiquity of non-separable optimization problems in real worlds, in this paper we analyze and extend the large-scale version of the well-known cooperative coevolution (CC), a divide-and-conquer black-box optimization framework, on non-separable functions. First, we reveal empirical reasons of when decomposition-based methods are preferred or not in practice on some non-separable large-scale problems, which have not been clearly pointed out in many previous CC papers. Then, we formalize CC to a continuous-game model via simplification, but without losing its essential property. Different from previous evolutionary game theory for CC, our new model provides a much simpler but useful viewpoint to analyze its convergence, since only the pure Nash equilibrium concept is needed and more general fitness landscapes can be explicitly considered. Based on convergence analyses, we propose a hierarchical decomposition strategy for better generalization, as for any decomposition, there is a risk of getting trapped into a suboptimal Nash equilibrium. Finally, we use powerful distributed computing to accelerate it under the recent multi-level learning framework, which combines the fine-tuning ability from decomposition with the invariance property of CMA-ES. Experiments on a set of high-dimensional test functions validate both its search performance and scalability (w.r.t. CPU cores) on a clustering computing platform with 400 CPU cores.},
  archive      = {J_ASOC},
  author       = {Qiqi Duan and Chang Shao and Guochen Zhou and Haobin Yang and Qi Zhao and Yuhui Shi},
  doi          = {10.1016/j.asoc.2024.112232},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative coevolution for non-separable large-scale black-box optimization: Convergence analyses and distributed accelerations},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive shuffled frog-leaping algorithm for flexible
flow shop scheduling problem with batch processing machines.
<em>ASOC</em>, <em>166</em>, 112230. (<a
href="https://doi.org/10.1016/j.asoc.2024.112230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch Processing Machines (BPM) and transportation are seldom studied simultaneously in Flexible Flow Shop. In this study, Flexible Flow Shop Scheduling Problem (FFSP) with BPM at the last stage and transportation is considered and an adaptive shuffled frog-leaping algorithm (ASFLA) is proposed to minimize makespan. To produce high-quality solutions, a heuristic is employed to produce initial solution, two groups are formed by using all memeplexes, then an adaptive memeplex search is implemented, in which the number of searches is dynamically determined by the quality of the memeplex, an adaptive group search is also conducted by exchanging memeplexes or supporting of the worse memeplex. A novel population shuffling and the worst memeplex elimination are proposed. A number of computational experiments are executed to test the new strategies and performances of ASFLA. Computational results demonstrate that new strategies are effective and ASFLA is a very competitive algorithm for FFSP with BPM and transportation.},
  archive      = {J_ASOC},
  author       = {Deming Lei and Chenyu He},
  doi          = {10.1016/j.asoc.2024.112230},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive shuffled frog-leaping algorithm for flexible flow shop scheduling problem with batch processing machines},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Challenges and opportunities of generative models on tabular
data. <em>ASOC</em>, <em>166</em>, 112223. (<a
href="https://doi.org/10.1016/j.asoc.2024.112223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tabular data, organized like tables with rows and columns, is widely used. Existing models for tabular data synthesis often face limitations related to data size or complexity. In contrast, deep generative models, a part of deep learning, demonstrate proficiency in handling large and complex data sets. While these models have shown remarkable success in generating image and audio data, their application in tabular data synthesis is relatively new, lacking a comprehensive comparison with existing methods. To fill this gap, this study aims to systematically evaluate and compare the performance of deep generative models with these existing methods for tabular data synthesis, while also investigating the efficacy of post-processing techniques. We aim to identify strengths and limitations and provide insights for future research and practical applications. Our study showed that the Synthetic Minority Oversampling Technique (SMOTE) and its variants outperform deep generative models, especially for small datasets. However, we observed that an ensemble of deep generative models and post-generation processing performs better on large datasets than SMOTE alone. The results of our study indicate that deep generative models hold promise as a valuable tool for generating tabular data. Nonetheless, further research is warranted to enhance the performance of deep generative models and gain a comprehensive understanding of their limitations.},
  archive      = {J_ASOC},
  author       = {Alex X. Wang and Stefanka S. Chukova and Colin R. Simpson and Binh P. Nguyen},
  doi          = {10.1016/j.asoc.2024.112223},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112223},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Challenges and opportunities of generative models on tabular data},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interval forecasting of baltic dry index within a secondary
decomposition-ensemble methodology. <em>ASOC</em>, <em>166</em>, 112222.
(<a href="https://doi.org/10.1016/j.asoc.2024.112222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Baltic Dry Index (BDI) is one of the leading indexes that is the most commonly used to reflect the prosperity of the shipping industry. The index’s volatility indicates the operational risks that shipping-related enterprises and service institutions may face. In order to more accurately estimate the volatility, this study proposes a secondary decomposition-ensemble model that can be used to predict interval-valued time series (ITS) of the BDI. Four main steps are involved, namely ITS construction and primary decomposition, secondary decomposition, component ITS forecasting, and ensemble. To be specific, bivariate empirical mode decomposition (BEMD) is employed for the primary decomposition, and multivariate variational mode decomposition (MVMD) is used for the secondary decomposition. Using daily BDI data, an empirical analysis is conducted to verify the proposed model. The investigation shows that, compared to other models, the proposed method has better forecasting performance and stronger robustness in ITS forecasting of the BDI. The results indicate that using the proposed model is a promising method for the volatility estimation of complex ITS data.},
  archive      = {J_ASOC},
  author       = {Gang Xie and Shuihan Liu and Hongyu Dong and XianKai Huang},
  doi          = {10.1016/j.asoc.2024.112222},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112222},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval forecasting of baltic dry index within a secondary decomposition-ensemble methodology},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive operator selection cuckoo search for parameter
extraction of photovoltaic models. <em>ASOC</em>, <em>166</em>, 112221.
(<a href="https://doi.org/10.1016/j.asoc.2024.112221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate, reliable, and efficient extraction of photovoltaic (PV) model parameters is an essential step towards PV system simulation, control, and optimization. Nevertheless, this problem is still facing great challenges because of its intrinsic nonlinear, multivariate, and multimodal properties. In this paper, a new variant of cuckoo search (CS), adaptive operator selection CS (AOSCS), is advanced for the PV model parameter extraction problems. AOSCS includes two major improvements: (1) an adaptive operator selection mechanism is developed to automatically assign the workloads of exploration and exploitation operators, and (2) the exploration and exploitation operators used in the original CS are modified to promote the exploration capability and reduce the blindness of search, respectively. The performance of AOSCS is firstly validated on CEC 2017 test suite and then it is utilized to solve the parameter extraction problems of five PV models. Moreover, further experiments on two commercial PV modules under distinct irradiance and temperature levels are also conducted to evaluate the practicality of the proposed algorithm. It is manifested that the results yielded by AOSCS are very competitive relative to other parameter extraction approaches. Accordingly, the proposed AOSCS is able to be served as an up-and-coming candidate algorithm for PV model parameter extraction problems.},
  archive      = {J_ASOC},
  author       = {Qiangda Yang and Yubo Wang and Jie Zhang and Hongbo Gao},
  doi          = {10.1016/j.asoc.2024.112221},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112221},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive operator selection cuckoo search for parameter extraction of photovoltaic models},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term inbound passenger flow prediction at high-speed
railway stations considering the departure passenger arrival pattern.
<em>ASOC</em>, <em>166</em>, 112219. (<a
href="https://doi.org/10.1016/j.asoc.2024.112219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of short-term inbound passenger flow at high-speed railway stations is of great significance for the refined operation of stations, the formulation of emergency plans, and the provision of intelligent services. The arrival of passengers traveling on the same train at the same station shows a similar pattern, which is called the departure passenger arrival pattern (DPAP). The short-term inbound passenger flow at the station is composed of the short-term inbound passenger flow of all waiting trains within the same period. Inspired by this, this paper develops an ensemble prediction model based on the time series decomposition modeling strategy to introduce the DPAP to the short-term inbound passenger flow prediction at stations. Firstly, we propose a new framework for studying the DPAP to calculate the fitted station short-term inbound passenger flow, which is only affected by the DPAP. During this process, we find that 7 minutes is the optimal time granularity. Secondly, based on the singular spectrum analysis, we prove that the DPAP is the determining factor affecting the station short-term inbound passenger flow. Finally, we propose an ensemble prediction model that considers the DPAP to achieve short-term inbound passenger flow prediction at stations. The model consists of two parts: the deterministic and stochastic components prediction, where the former is predicted by the fitted station short-term inbound passenger flow, and the latter is achieved by the combination of historical stochastic components and weather type with the help of the Seq2Seq model based on time attention mechanism. Using real inbound passenger flow data, we compare the proposed model with 13 benchmark models and the results show that under different training and prediction steps, our model achieves optimal prediction performance, whether in all-day period and the busiest period of the station. Through further ablation experiments, it has been proven that the introduction of the DPAP effectively improves the prediction accuracy. Our model can provide scientific support for the intelligent operation of stations and the refined management of passenger flow.},
  archive      = {J_ASOC},
  author       = {Yifan Niu and Bin Shuai and Rui Zhang and Huiyan Fa and Wencheng Huang},
  doi          = {10.1016/j.asoc.2024.112219},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112219},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term inbound passenger flow prediction at high-speed railway stations considering the departure passenger arrival pattern},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated probabilistic bipolar fermatean hesitant fuzzy
transportation configuration for sustainable management of floral waste
with risk assessment. <em>ASOC</em>, <em>166</em>, 112215. (<a
href="https://doi.org/10.1016/j.asoc.2024.112215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {M.K. Sharma and Sadhna Chaudhary and Apu Kumar Saha},
  doi          = {10.1016/j.asoc.2024.112215},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated probabilistic bipolar fermatean hesitant fuzzy transportation configuration for sustainable management of floral waste with risk assessment},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularization method for reduced biquaternion neural
network. <em>ASOC</em>, <em>166</em>, 112206. (<a
href="https://doi.org/10.1016/j.asoc.2024.112206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reduced biquaternion neural network (RQNN) has achieved significant success in machine learning. However, as the reduced biquaternion algebra system contains infinite zero divisors, the RQNN can be easily trapped in a local minimum and overfitting. In this paper, we propose a new regularization scheme for the RQNN to address these issues. Firstly, we propose a new operation in the reduced biquaternion domain named the reduced biquaternion complex modulus (RQCM), which can extract the scale transformation of reduced biquaternions and decrease the unreasonable network constraints caused by constrained phases. Secondly, we mathematically analyse the properties of the reduced biquaternions and obtain the geometric meaning of the RQCM. Finally, we propose an improved weight decay method using the RQCM which can better project the reduced biquaternion in terms of the scale and phase. In addition, our proposed method can effectively solve the non-differentiability of reduced biquaternion matrix and overfitting problem in the process of network parameter updating. The experimental results demonstrate that the proposed method is effective in color image classification and denoising taskas, and outperforms the state of the arts.},
  archive      = {J_ASOC},
  author       = {Shan Gai and Xiang Huang},
  doi          = {10.1016/j.asoc.2024.112206},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112206},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regularization method for reduced biquaternion neural network},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shapley visual transformers for image-to-text generation.
<em>ASOC</em>, <em>166</em>, 112205. (<a
href="https://doi.org/10.1016/j.asoc.2024.112205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape of the web, text-to-image generation stands out as a crucial information service. Recently, deep learning has emerged as the cutting-edge methodology for advancing text-to-image generation systems. However, these models are typically constructed using domain knowledge specific to the application at hand and a very particular data distribution. Consequently, data scientists must be well-versed in the relevant subject. In this research work, we target a new foundation for text-to-image generation systems by introducing a consensus method that facilitates self-adaptation and flexibility to handle different learning tasks and diverse data distributions. This paper presents I2T-SP (Image-to-Text Generation for Shapley Pruning) as a consensus method for general-purpose intelligence without the assistance of a domain expert. The trained model is developed using a general deep-learning approach that investigates the contribution of each model in the training process. Multiple deep learning models are trained for each set of historical data, and the Shapley Value is determined to compute the contribution of each subset of models in the training. Subsequently, the models are pruned according to their contribution to the learning process. We present the evaluation of the generality of I2T-SP using different datasets with varying shapes and complexities. The results reveal the effectiveness of I2T-SP compared to baseline image-to-text generation solutions. This research marks a significant step towards establishing a more adaptable and broadly applicable foundation for image-to-text generation systems.},
  archive      = {J_ASOC},
  author       = {Asma Belhadi and Youcef Djenouri and Ahmed Nabil Belbachir and Tomasz Michalak and Gautam Srivastava},
  doi          = {10.1016/j.asoc.2024.112205},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112205},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Shapley visual transformers for image-to-text generation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cross branch co-attention network multimodal models based on
raman and FTIR spectroscopy for diagnosis of multiple selected cancers.
<em>ASOC</em>, <em>166</em>, 112204. (<a
href="https://doi.org/10.1016/j.asoc.2024.112204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of artificial intelligence (AI) in the medical field has brought unprecedented opportunities and challenges for early diagnosis and precision treatment of cancer. As complex multi-omics data in the medical field tends to be multimodal, a single type of data cannot provide enough information to support accurate diagnosis. Vibrational spectroscopy consists of Raman spectroscopy and FTIR spectroscopy, both of which can reflect the structural information of molecules and are used to detect the vibration and rotational energy levels of material molecules. However, the application of multimodal tasks in fusing vibrational spectroscopy is not comprehensive. In response to the above problems, this paper focuses on interactive multimodal fusion strategies to process and mine vibrational spectral information. A Cross Branch Co-Attention Network (CBCAN) is proposed to solve the problem of insufficient spectral fusion, and a spectral branch network and a collaborative attention network are constructed for collaborative information fusion. Finally, feature-level fusion is combined to achieve better sequential decision-making effects. Extensive experiments were conducted on cancer datasets and thyroid dysfunction binary classification datasets, with the corresponding sample numbers of 192 and 379, respectively. The research results show that compared with traditional deep learning algorithms and the latest related multimodal medical fusion methods, the proposed CBCAN classification model achieved 96.88 % accuracy, 93.61 % precision, 91.52 % sensitivity, 98.03 % specificity, 91.73 % F1 score and 99.75 % AUC value, respectively, with the best classification effect, providing a new method for rapid and non-invasive identification of multiple selected cancers, which has important reference value for the early diagnosis of cancer patients and helps to assist clinical diagnosis.},
  archive      = {J_ASOC},
  author       = {Xuguang Zhou and Chen Chen and Enguang Zuo and Cheng Chen and Xiaoyi Lv},
  doi          = {10.1016/j.asoc.2024.112204},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112204},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross branch co-attention network multimodal models based on raman and FTIR spectroscopy for diagnosis of multiple selected cancers},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on load frequency control using reinforcement
learning-based data-driven controller. <em>ASOC</em>, <em>166</em>,
112203. (<a href="https://doi.org/10.1016/j.asoc.2024.112203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load frequency control (LFC) is a significant control problem in the operation of interconnected power systems. It keeps the change in system frequency within specific limits by maintaining the balance between power generation and load demand. In modern interconnected power systems, various control strategies, including conventional control techniques and other data-driven approaches, have been adopted to improve the effectiveness of LFC. The control technique based on reinforcement learning (RL) is one of the contemporary data-driven control strategies for LFC. Recently, the attention of researchers has surged towards RL-based control strategies for LFC. Several survey literature has been published in the field of LFC concerning the various control strategies for the effective operation of the power system. However, these surveys have not considered a complete systematic review of RL-driven LFC. An exhaustive review is essential to demonstrate the current status and identify future advancements in this field. This paper presents a comprehensive review of LFC based on the RL-driven control strategy. This study begins by presenting a mathematical and conceptual understanding of reinforcement learning. Finally, a broad classification of RL algorithms and the algorithm-wise literature survey on LFC are provided extensively. This comprehensive and insightful literature survey may serve as a valuable resource for the researchers, addressing the gaps between recent advances, implementation difficulties, and future developments in LFC using the RL-driven control strategy.},
  archive      = {J_ASOC},
  author       = {Rasananda Muduli and Debashisha Jena and Tukaram Moger},
  doi          = {10.1016/j.asoc.2024.112203},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112203},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on load frequency control using reinforcement learning-based data-driven controller},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cervical cytology screening using the fused deep learning
architecture with attention mechanisms. <em>ASOC</em>, <em>166</em>,
112202. (<a href="https://doi.org/10.1016/j.asoc.2024.112202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer remains a significant global health concern. Given the disparity between limited medical resources and the requisite professional personnel, the coverage of cervical screening is inadequate, particularly in underdeveloped areas. Computer-assisted liquid-based cytology diagnostic systems offer favorable solutions. Detection of small nuclei within a complex liquid-based environment poses a challenge, exacerbated by the restricted availability of manual annotations. In this study, we propose FuseDLAM, a comprehensive computer-aided diagnostic system, which employs enhanced YOLOv8 with transformers for rapid localization of individual squamous epithelial cells. We leverage artificial intelligence-generated content techniques for data augmentation, effectively reducing the need for costly manual annotations. By integrating multiple deep convolutional neural network models with self-attention mechanisms, the system extracts crucial features from cell nuclei. These features are then fused through a fully connected layer to facilitate robust cell classification. FuseDLAM achieves an F1-score of 99.3 % % on the public SIPaKMeD dataset, demonstrating comparability with state-of-the-art approaches. It also proves its practical applicability in real-world clinical scenarios, achieving an F1-score of 91.2 % in identifying abnormal cervical squamous cells. Additionally, ablation experiments in both datasets validate the model&#39;s effectiveness. This underscores its potential for widespread application in medical imaging tasks.},
  archive      = {J_ASOC},
  author       = {Yuqi Jin and Jinghang Ma and Yong Lian and Fang Wang and Tunhua Wu and Huan Hu and Zhen Feng},
  doi          = {10.1016/j.asoc.2024.112202},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112202},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cervical cytology screening using the fused deep learning architecture with attention mechanisms},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving generative adversarial network inversion via
fine-tuning GAN encoders. <em>ASOC</em>, <em>166</em>, 112201. (<a
href="https://doi.org/10.1016/j.asoc.2024.112201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks (GANs) can synthesize high-quality (HQ) images, and GAN inversion is a technique that discovers how to invert given images back to latent space. While existing methods perform on StyleGAN inversion, they have limited performance and are not generalized to different GANs. To address these issues, we proposed a self-supervised method to pre-train and fine-tune GAN encoders. First, we designed an adaptive block to fit different encoder architectures for inverting diverse GANs. Then we pre-train GAN encoders using synthesized images and emphasize local regions through cropping images. Finally, we fine-tune the pre-trained GAN encoder for inverting real images. Compared with state-of-the-art methods, our method achieved better results that reconstructed high-quality images on mainstream GANs. Our code and pre-trained models are available at: https://github.com/disanda/Deep-GAN-Encoders .},
  archive      = {J_ASOC},
  author       = {Cheng Yu and Wenmin Wang and Roberto Bugiolacchi},
  doi          = {10.1016/j.asoc.2024.112201},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112201},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving generative adversarial network inversion via fine-tuning GAN encoders},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensor-type agnostic heat detection in dairy cows using
multi-autoencoders with shared latent space. <em>ASOC</em>,
<em>166</em>, 112200. (<a
href="https://doi.org/10.1016/j.asoc.2024.112200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring heat events in dairy cows is crucial for determining the heat on time, and the heat events have usually been estimated using machine learning on cow behavioral data collected from wireless activity sensors recently. However, ensuring robust performance of heat detection is difficult because of the difference in data domains (e.g., sensor types) and insufficient heat-labeled data. Therefore, this study proposes a multi-autoencoder-based heat detection in dairy cows that can represent the common representation of cow behavior across the different sensors. The proposed method can train a sensor-type agnostic heat detector using entire labeled data from the two different sensor types by aligning the latent spaces for two sensors. In addition, our approach can train the model by combining anomaly detection and weakly supervised classification to improve the performance of heat detection that can reduce the dependency on label accuracy. The results showed that the proposed approach improved cow heat detection performance by approximately 46 % than independently trained autoencoders, and the average F1-score increased by up to 0.70. The proposed method also outperformed other supervised and unsupervised learning models in heat detection using our dataset. From the results, our model effectively estimates cow behaviors by integrating sensor modalities, thereby enhancing data capabilities in low-resource settings. This study can be key for addressing the detection discrepancy in time series data based on the location of the mounted sensor, and offers the advantage of practical applications to various activity sensors currently used on farms.},
  archive      = {J_ASOC},
  author       = {Dae-Hyun Lee and Mingyung Lee and Wang-Hee Lee and Seongwon Seo},
  doi          = {10.1016/j.asoc.2024.112200},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112200},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sensor-type agnostic heat detection in dairy cows using multi-autoencoders with shared latent space},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing digital transformation using fuzzy cognitive
mapping supported by artificial intelligence techniques. <em>ASOC</em>,
<em>166</em>, 112199. (<a
href="https://doi.org/10.1016/j.asoc.2024.112199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital transformation process aims to align people, machines, and technologies in interactive systems, prompting businesses to adapt their models for competitiveness. Understanding and implementing digital transformation becomes crucial for organizations to integrate physical and virtual systems throughout their operations. The study presents a comprehensive model emphasizing intelligent technology, organization, and manufacturing as essential criteria. Integrating fuzzy cognitive maps and fuzzy DEMATEL methods, the study captures complex relationships and employs a dynamic analysis involving three firms in the defense industry. Results showcase the companies&#39; diverse digital transformation levels, assessed through scenarios and predictive analyses. Additionally, a sensitivity analysis underscores the impact of adjustments to the top three criteria, aiding businesses in deciding their transformational starting points. This integrated method, underpinned by fuzzy cognitive maps and artificial intelligence, emerges as a decision support system and a roadmap for enterprises navigating digital transformation.},
  archive      = {J_ASOC},
  author       = {Enes Furkan Erkan and Özer Uygun and Halil İbrahim Demir},
  doi          = {10.1016/j.asoc.2024.112199},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112199},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessing digital transformation using fuzzy cognitive mapping supported by artificial intelligence techniques},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fermatean fuzzy SWARA-TOPSIS methodology based on SCOR
model for autonomous vehicle parking lot selection. <em>ASOC</em>,
<em>166</em>, 112198. (<a
href="https://doi.org/10.1016/j.asoc.2024.112198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Population growth in crowded cities and the resulting increase in vehicle use have led to the problem of insufficient parking. When public parking lots and urban growth are not in coordination, vehicles park on the street and close the crosswalks. In the coming years, this problem will become more complicated with the addition of autonomous vehicles (AVs) to urban traffic. This study addresses the research question of how to effectively select AV parking lots in urban areas experiencing population growth and increased vehicle usage. For this aim, a hybrid Multi-Criteria Decision Making (MCDM) methodology, combining SWARA (Step-wise Weight Assessment Ratio Analysis) and TOPSIS (Technique for Order Preference by Similarity) approaches in a Fermatean Fuzzy (FF) environment is proposed. The decision hierarchy based on the SCOR model has been developed to determine and construct the evaluation criteria. Then, a case study analysis has been applied to selected districts in Istanbul, which is Turkiye&#39;s most populous and developing city. Operating expenses, safety and security, and land costs are determined as the most important factors. As a result of the detailed fuzzy analysis, which districts should primarily be chosen for AV parking lots in Istanbul is determined and finally, the robustness and validity of the results obtained by the sensitivity analysis being questioned. The study contributes by providing insights into AV parking lot selection, demonstrating the efficacy of the proposed methodology, and highlighting the importance of addressing this issue in urban planning.},
  archive      = {J_ASOC},
  author       = {Ertugrul Ayyildiz and Melike Erdogan},
  doi          = {10.1016/j.asoc.2024.112198},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112198},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fermatean fuzzy SWARA-TOPSIS methodology based on SCOR model for autonomous vehicle parking lot selection},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-sided matching theory-based second-hand house
transaction evaluation and recommendation by the modified PLC-DEMATEL
method. <em>ASOC</em>, <em>166</em>, 112196. (<a
href="https://doi.org/10.1016/j.asoc.2024.112196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance the two-sided matching efficiency in a multi-source heterogeneous environment, this paper takes the randomness and unstructured features of the online comments into consideration, and proposes a new matching mechanism by introducing the complex information representation tool. Firstly, the concept of the probabilistic linguistic normal cloud (PLNC) model is introduced to preserve information cohesion and characteristic distribution. Next, the basic operation laws and corresponding operators are given. Then, an innovative maximum boundary concept skipping the indirect approach is presented to update the similarity degree and distance measures, also the correlation coefficient. Furthermore, for the multi-indicator systems with interactions, the peer experts are invited to evaluate the relationship between indicators, a modified algorithm based on the Decision-Making and Trial Evaluation Laboratory (DEMATEL) method is applied to obtain the subjective weights of indicators. After that, a whole matching process and a correlation coefficient cluster method-based recommendation algorithm are presented. A case study is provided to illustrate the method, wherein a new indicator system is constructed by analyzing the correlation of multiple indicators based on online linguistic evaluations. The random forest model is combined to obtain the objective weights and balance its reliability. Finally, sensitivity analysis and comparative analysis are employed to validate the effectiveness and applicability.},
  archive      = {J_ASOC},
  author       = {Bo Li and Wenwen Zhu and Zeshui Xu and Chonghui Zhang},
  doi          = {10.1016/j.asoc.2024.112196},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112196},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-sided matching theory-based second-hand house transaction evaluation and recommendation by the modified PLC-DEMATEL method},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oriented to a multi-learning mode: Establishing
trend-fuzzy-granule-based LSTM neural networks for time series
forecasting. <em>ASOC</em>, <em>166</em>, 112195. (<a
href="https://doi.org/10.1016/j.asoc.2024.112195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the construction of information granule based neural networks for time series multi-step forecasting, the existing works tend to focus on consecutive-learning mode while rarely explore multi-learning mode. In fact, only under the multi-learning mode can diversified associations among data collected over time granules be well learned. Also, the existing works exhibit limited time interpretability. Here the problem centers around how to endow information granule based neural networks with a multi-learning mode to learn diversified associations simultaneously, and well-articulated trend semantics. To solve these problems, the first originality of this paper stems from a scale equalization method for multilinear-trend fuzzy information granules to track complex trend changes of data in a more accurate and explainable manner from both global and local views. Furthermore, an adaptive rather than empirical or traversal method, which is trend-driven in nature, is tailored for mining diversified associations. The resulting model can give forecasts in the form of granules as well as numerical values, being interpretable and accurate in the sense that: (a) its inputs and output are granules which come with well-defined trend semantics under a customary time concept, and (b) a clump of data is considered in a concise granule whilst roles of diversified associations are ware of during forecasting, making the model less prone to cumulative errors. Appealing experimental results corroborate the effectiveness of the proposed model.},
  archive      = {J_ASOC},
  author       = {Yuqing Tang and Fusheng Yu and Witold Pedrycz and Fang Li and Chenxi Ouyang},
  doi          = {10.1016/j.asoc.2024.112195},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112195},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Oriented to a multi-learning mode: Establishing trend-fuzzy-granule-based LSTM neural networks for time series forecasting},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic heterogeneous graph contrastive networks for
knowledge tracing. <em>ASOC</em>, <em>166</em>, 112194. (<a
href="https://doi.org/10.1016/j.asoc.2024.112194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing (KT) is a crucial task in online education that traces students’ evolving cognition changes over time. However, it is a challenging task due to the heterogeneity of knowledge and incomplete cognition evolution sequences. This paper proposes KT-Deeper, a long-term K nowledge T racing framework based on D ynamic r e inforced h e terogeneous gra p h contrastiv e netwo r ks, to predict students’ cognitive states on specific skills. Particularly, KT-Deeper initially employs temporal heterogeneous graphs to model the interconnections between different types of knowledge entities ( e.g. , students, exercises, and skills). Subsequently, KT-Deeper formalizes knowledge tracing as a dynamic link prediction task on the temporal heterogeneous graph sequence and proposes a reinforced graph generation approach to refine the incomplete graph sequence for supporting long-term knowledge tracing. KT-Deeper further presents a self-supervised heterogeneous graph embedding method to extract the structural features of knowledge evolution. Finally, KT-Deeper leverages recurrent neural networks to learn the temporal features of students’ cognitive evolution and predict whether a student will master a specific skill. Experimental results confirm that KT-Deeper exhibits superior performance compared to existing cutting-edge techniques, showcasing its promising accuracy and robustness in incomplete long-term knowledge tracing tasks.},
  archive      = {J_ASOC},
  author       = {Yehong Han and Hailiang Tang and Wenxiao Zhang and Lin Du and Jun Zhao and Minglai Shao},
  doi          = {10.1016/j.asoc.2024.112194},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112194},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic heterogeneous graph contrastive networks for knowledge tracing},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy anomaly scores for isolation forest. <em>ASOC</em>,
<em>166</em>, 112193. (<a
href="https://doi.org/10.1016/j.asoc.2024.112193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of anomalies in data presents a significant challenge in various applications. The Isolation Forest (IF) has gained attention due to its notable performance features, including high accuracy, efficiency, simplicity, and rapid computation. However, prior research has primarily concentrated on the constructing isolation trees (iTrees), overlooking the considerable influence of anomaly scoring methods on anomaly detection performance. This study introduces an innovative anomaly scoring method that integrates fuzzy concepts to enhance detection performance. Fuzzy concepts adeptly manage ambiguity and uncertainty, making them more readily applicable in anomaly scoring than in iTree training. Unlike conventional methods that assign a sample to a single child node in the decision path, the proposed fuzzy anomaly scoring method allows samples to be assigned to all child nodes with varying membership degrees based on the target sample. Consequently, this method aggregates the path lengths of all external nodes in a weighted manner, minimizing the impact of irrelevant splits on anomaly scores. Considering that even IF algorithms using informative splits select splits based on data distribution rather than label information, introducing fuzziness to the splits themselves can effectively mitigate performance degradation caused by irrelevant splits. Extensive experiments on 25 benchmark datasets demonstrated that the proposed anomaly scoring method significantly improved both the performance and stability of anomaly detection with the base IF algorithm, outperforming other IF algorithms and fuzzy rough set-based anomaly detection methods.},
  archive      = {J_ASOC},
  author       = {Kyoungok Kim},
  doi          = {10.1016/j.asoc.2024.112193},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy anomaly scores for isolation forest},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binary ant colony optimization algorithm in learning random
satisfiability logic for discrete hopfield neural network.
<em>ASOC</em>, <em>166</em>, 112192. (<a
href="https://doi.org/10.1016/j.asoc.2024.112192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduced a novel ant colony optimization algorithm that implements the population selection strategy of the Estimation of Distribution Algorithm and a new pheromone updating formula. It aimed to optimize the performance of G-type random high-order satisfiability logic structures embedded in Discrete Hopfield Neural Networks, thereby enhancing the efficiency of the Hopfield Neural Network learning algorithm. Through comparative analysis with other metaheuristic algorithms, our model demonstrated superior performance in terms of global convergence, time complexity, and algorithm complexity. Additionally, we evaluated the learning phase, retrieval phase, and similarity analysis using various ratios of literals and clauses. It was shown that our proposed model exhibits stronger search ability compared to other metaheuristic algorithms and Exhaustive Search. Our model enhanced the efficiency of the learning phase, resulting in the number of global solutions accounting for 100 %, and significantly improved the global solution diversity. These advancements contributed to the efficiency of the model in convergence, rendering it applicable to a wide range of nonlinear classification and prediction problems.},
  archive      = {J_ASOC},
  author       = {Yuan Gao and Mohd Shareduwan Mohd Kasihmuddin and Ju Chen and Chengfeng Zheng and Nurul Atiqah Romli and Mohd. Asyraf Mansor and Nur Ezlin Zamri},
  doi          = {10.1016/j.asoc.2024.112192},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binary ant colony optimization algorithm in learning random satisfiability logic for discrete hopfield neural network},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term air quality prediction using point and interval
deep learning systems coupled with multi-factor decomposition and
data-driven tree compression. <em>ASOC</em>, <em>166</em>, 112191. (<a
href="https://doi.org/10.1016/j.asoc.2024.112191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clean air, as a symbol of high-quality air quality, is the most basic requirement for people to maintain health. Moreover, in keeping humans fit, accurate short-term air quality prediction is vital. The decomposition algorithm can better capture the local features and temporal changes of the data. However, it increases the computation time, resource consumption, and complexity of the model. On the other hand, existing forecasting systems overlook instability and uncertainty. To solve the above problems, a deterministic and uncertainty AOA-DBGRU-MDN deep learning systems is proposed, which combines arithmetic optimization algorithm (AOA), double-layer bi-directional GRUs (DBGRU), and mixture density network (MDN). The above systems consider meteorological factors and air pollutants comprehensively. It involves feature selection using maximum information coefficient (MIC), decomposition using complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN) algorithm, classification, and compression of decomposed components using entropy-Huffman tree compression. Firstly, the information measurement process reduces the number of components significantly. Following the incorporation of multi-factor data, the optimal DBGRU model is then obtained using AOA. Finally, the training errors are fitted using MDN to obtain interval prediction results. The experiments demonstrate that (1) Using the CEEMDAN algorithm can improve the prediction accuracy; (2) Classifying and reconstructing the data based on entropy-Huffman tree compression can not only decrease the model&#39;s training volume and improve training efficiency but also boost the model&#39;s prediction accuracy; (3) The AOA-DBGRU-MDN system performs probabilistic prediction to obtain an effective and intuitive prediction interval to improve the point prediction of air quality prediction.},
  archive      = {J_ASOC},
  author       = {Jinxing Che and Kun Hu and Wenxin Xia and Yifan Xu and Yuerong Li},
  doi          = {10.1016/j.asoc.2024.112191},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term air quality prediction using point and interval deep learning systems coupled with multi-factor decomposition and data-driven tree compression},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing container port traffic simulation: A data-driven
machine learning approach in sparse data environments. <em>ASOC</em>,
<em>166</em>, 112190. (<a
href="https://doi.org/10.1016/j.asoc.2024.112190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient truck dispatching strategies are paramount in container terminal operations. The quality of these strategies heavily relies on accurate and expedient simulations, which provide a crucial platform for training and evaluating dispatching algorithms. In this study, we introduce data-driven machine learning methods to enhance container port truck dispatching simulation accuracy. These methods effectively surrogate the intersections within the simulation, thereby increasing the accuracy of simulated outcomes without imposing significant computational overhead in sparse data environments. We incorporate three data-driven learning methods: genetic programming (GP), reinforcement learning (RL), and a GP and RL hybrid heuristic (GPRL-H) approach. The GPRL-H method proved the most efficacious through a detailed comparative study, striking an effective balance between simulation accuracy and computational efficiency. It reduced the error rate of simulation from approximately 35% to about 7%, while also halving the simulation time compared to the RL-based method. Our proposed method also does not rely on precise Global Positioning System (GPS) data to simulate truck operations within a port accurately. Demonstrating robustness and adaptability, this approach holds promise for extending beyond port operations to improve the simulation accuracy of vehicle operations in various scenarios characterized by sparse data.},
  archive      = {J_ASOC},
  author       = {Xinan Chen and Rong Qu and Jing Dong and Haibo Dong and Ruibin Bai},
  doi          = {10.1016/j.asoc.2024.112190},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112190},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing container port traffic simulation: A data-driven machine learning approach in sparse data environments},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Health state assessment model for complex systems: Trade-off
accuracy and robustness in belief rule base. <em>ASOC</em>,
<em>166</em>, 112189. (<a
href="https://doi.org/10.1016/j.asoc.2024.112189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex system, health state assessment can determine the state of the system and identify potential system problems. However, due to the numerous uncertainties and variations present in complex systems, it is difficult to effectively construct assessment models. Belief rule base (BRB) can use data-driven and knowledge-driven methods to effectively address uncertain information, and is widely used for modeling health state assessments of complex systems. The primary modeling and optimization goals of BRB is currently at accuracy, ignoring the impact of robustness on complex systems, and the reliability of the model is reduced. Therefore, this article introduces a novel method to balance the accuracy and robustness of BRB models. This method enhances the performance of the BRB model in assessing complex system health and provides valuable guidance for engineering applications. Firstly, the guidelines for BRB modeling are systematically summarized to address the trade-off between accuracy and robustness. This provides essential guidance for constructing BRB models during the model-building process. Secondly, four feasible domain criteria are proposed to enhance the reliability of the BRB during the model optimization process. A modified multi-objective optimization algorithm is proposed based on the feasible domain criteria. Finally, in the case studies of aerospace relay and lithium-ion battery health assessments, the MSE of the proposed model for aerospace relay health assessment is 0.0015 with a Lipschitz constant of 6.73, while for lithium-ion battery health assessment, the MSE is 0.0013 with a Lipschitz constant of 24.17. The experimental results demonstrate that the proposed model has an advantage in terms of the trade-offs between both robustness and accuracy.},
  archive      = {J_ASOC},
  author       = {Mingyuan Liu and Wei He and You Cao and Shaohua Li and Hailong Zhu and Ning Ma},
  doi          = {10.1016/j.asoc.2024.112189},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112189},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Health state assessment model for complex systems: Trade-off accuracy and robustness in belief rule base},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From ensemble learning to deep ensemble learning: A case
study on multi-indicator prediction of pavement performance.
<em>ASOC</em>, <em>166</em>, 112188. (<a
href="https://doi.org/10.1016/j.asoc.2024.112188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Big data analytics approaches were combined with emerging machine learning techniques, which can provide more sophisticated insights into information-intensive activity. Compared to traditional shallow-architecture machine learning algorithms, Deep learning could excavate more potential information from the raw features. However, its powerful representational capacity relies on the support of enormous samples. The ensemble trees system performs superior on small sample problems due to better generalization capacity. To merge both benefits of deep learning and ensemble tree system, this paper developed a deep ensemble algorithm applied to multiple indicators prediction of pavement performance, including International Roughness Index and pavement 3-layer modulus. The deep ensemble algorithm is developed by merging a deep neural network (DNN) with the decision manifold property of the decision trees (TabNet) into a cascade ensemble system, combined with a sliding window algorithm to extract dependency information from raw data. During the training stage, the Bayesian Optimization Algorithm (BOA) is used to search for the optimal combination of sub-decision makers in the cascade ensemble. And equipped with GPU, it can speed up by 2.6–4.0 times. In the case study of pavement engineering, with sufficient training samples, it can achieve an average accuracy of 98.74 %, higher than DNN (97.49 %) and XGBoost (96.12 %) in predicting pavement indicators. With insufficient training samples, it can achieve an accuracy improvement of 12 % than XGBoost (75 %) and 24.5 % than DNN (62.5 %).},
  archive      = {J_ASOC},
  author       = {Yi Wu},
  doi          = {10.1016/j.asoc.2024.112188},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112188},
  shortjournal = {Appl. Soft. Comput.},
  title        = {From ensemble learning to deep ensemble learning: A case study on multi-indicator prediction of pavement performance},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Point and interval forecasting approach for short-term
urban subway passenger flow based on residual term decomposition and
fuzzy information granulation. <em>ASOC</em>, <em>166</em>, 112187. (<a
href="https://doi.org/10.1016/j.asoc.2024.112187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting information of short-term subway passenger flow is an important scientific reference for daily operations and urban management. The rapid time-varying nature of subway passenger flow caused by various factors that affect travel behavior poses a huge challenge to accurate forecasting. The complexity and uncertainty of data mainly focus on residual terms that deeply reflect the fluctuations. Reasonably mining the residual terms has become the key to achieving accurate forecasting. Therefore, this paper proposes a sophisticated decomposition strategy to extract and analyze the useful information hidden in the residual terms. Firstly, the potential trend, seasonal and residual terms from the original data are extracted by seasonal-trend decomposition procedure based on loess. Secondly, the residual term is decomposed into a series of subcomponents with different frequency features by symplectic geometric mode decomposition. Thirdly, these subcomponents are classified into three clusters based on fuzzy C-means clustering (FCM), and then the corresponding forecasting models are matched to the three obtained clusters and trend and seasonal terms. Finally, based on a decomposition-ensemble framework and information granulation for high-frequency components, we have established point and interval forecasting approach, respectively. Three experiments on real data sets in Beijing, Guangzhou and Shenzhen are conducted to verify the performance of the proposed approach, and the experimental results show that our approach is superior to all benchmark models and contributes to improving operational management and service quality.},
  archive      = {J_ASOC},
  author       = {Duo Chen and Hongtao Li and Shaolong Sun and Juncheng Bai and Zhipeng Huang},
  doi          = {10.1016/j.asoc.2024.112187},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112187},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Point and interval forecasting approach for short-term urban subway passenger flow based on residual term decomposition and fuzzy information granulation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical heterogeneous ant colony optimization based
oversampling algorithm using feature similarity for classification of
imbalanced data. <em>ASOC</em>, <em>166</em>, 112186. (<a
href="https://doi.org/10.1016/j.asoc.2024.112186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification is one of the challenging problems in machine learning. Oversampling is a promising technique that generates synthetic minority instances to balance the dataset. Inappropriate minority instances generated may deteriorate the performance of the classifier. Majority of the oversampling algorithms create new minority instances by choosing nearest neighbors for random interpolation. However, these methods do not provide new information to the dataset and therefore standard classifiers do not show good performance on such datasets. Therefore, it is necessary to generate diverse minority class instances to increase the performance of the classifier. Since, every feature of each minority class instance contribute valuable information, generating synthetic instances from the features of all minority instances would produce diverse minority instances, thereby increasing the performance of the classifier. This paper proposes a Hierarchical Heterogeneous Ant Colony Optimization based oversampling algorithm using Feature Similarity (HHACO-FSOTe) for generation of synthetic minority instances. Instead of choosing few neighbors for interpolation, the proposal considers all minority instances for generation of synthetic instances. HHACO-FSOTe generates new feature values by computing the minimum absolute difference between the features of a given minority instance and the corresponding features of the remaining minority instances. The features in the dataset are distributed among the ant agents enabling parallelism, thereby reducing the time taken for oversampling. HHACO-FSOTe do not require parameter tuning or training. The proposal is evaluated on 41 low dimensional, 11 high dimensional and 8 noisy datasets. Experiments reveal that HHACO-FSOTe is competent with the state-of-art oversampling techniques. Results were validated using non-parametric statistical tests.},
  archive      = {J_ASOC},
  author       = {Sreeja N.K and Sreelaja N.K},
  doi          = {10.1016/j.asoc.2024.112186},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112186},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hierarchical heterogeneous ant colony optimization based oversampling algorithm using feature similarity for classification of imbalanced data},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new representation learning based maximum power operation
towards improved energy management integration with DG controllers for
photovoltaic generators using online deep exponentially expanded RVFLN
algorithm. <em>ASOC</em>, <em>166</em>, 112185. (<a
href="https://doi.org/10.1016/j.asoc.2024.112185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To incorporate Local Energy Management System (LEMS) based Tertiary Controller (TC) operation with multiple Photovoltaic based Distributed Generations (PV-DGs) level Primary Controller/ PC: Independent DG Controllers (IDGC) more adequately, a new representation learning (Symmetrical Input - Output weight Encoded Data Compression/ S IO DC, with Exponential Expansion based Random Vector Functional Link Network towards Softmax layer’s Control Reference Estimation/ Ex RVFLN- So CRE) based MPPT controller is proposed in this paper in terms of Secondary Controllers ( SC s). A new Hybrid LEMS ( Hy LEMS) is presented towards the proposed Deep Neural Network based SC s (i.e. DNN- SC s) in a distributed ( SC dist ), as well as centralized ( SC cent ) manner, to ease the computational , and communication requirements. To investigate that multiple PV-DGs based duty cycle ( k th instant Control References/CRs estimation) controllers are considered here with auxiliary Battery Energy Storage Systems (BESS), and AC utility, integrated to Common DC feeder in terms of DC-DC converter dynamics. To avoid Control Reference Estimation Error (CREE) due to initial randomization, optimization subroutines are incorporated for S IO DC by generalized semi-supervised learning, and for Ex RVFLN- So CRE by Lagrange multiplier weighted, rms based cost function. The proposed control performance is verified in MATLAB Simulink® based average modeling, and validated through dSPACE DS1104 based RTI with multi-PV (emulators) test-bench as well.},
  archive      = {J_ASOC},
  author       = {Anshuman Satpathy and Snehamoy Dhar and P.K. Dash and Ranjeeta Bisoi and Niranjan Nayak},
  doi          = {10.1016/j.asoc.2024.112185},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112185},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new representation learning based maximum power operation towards improved energy management integration with DG controllers for photovoltaic generators using online deep exponentially expanded RVFLN algorithm},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The adaptive consensus reaching process with the ER-PSO
interaction mechanism and its application in public charging stations
evaluation. <em>ASOC</em>, <em>166</em>, 112183. (<a
href="https://doi.org/10.1016/j.asoc.2024.112183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public charging stations (PCSs) have become integral public infrastructure in China, responding to the considerable and rapidly growing demand. Evaluating the service quality of PCSs by a diverse user base has become essential for the effective management of PCSs. In the group evaluation of PCSs&#39; service quality, which encompasses broad public interests, consensus reaching process (CRP) is commonly utilized to manage group conflicts and achieve evaluation consensus. The intricate interactions among decision makers (DMs) significantly influence the evolution and resilience of collective viewpoints. Against this backdrop, this research aims to develop an adaptive CRP to enhance the discriminatory power of group consensus and improve consensus robustness in the presence of non-cooperative behavior. With the above objectives, this study proposes an adaptive CRP supported by a novel interaction mechanism, consisting of two stages: (1) In the interaction network generation stage, based on the Erdős-Rényi (ER) random network model, innovative interaction signals are generated to guide the generation of an ER-based interaction network, enhancing the dissemination of dominant viewpoints, and constraining the influence of minority opinions. (2) In the decision opinion interaction stage, based on the Particle Swarm Optimization (PSO) algorithm, utilizing novel dual individual fitness functions, the PSO algorithm is employed to formulate the expected updated belief, driving the internal evolution and convergence of opinions. The proposed adaptive CRP is implemented in the group evaluation of PCSs&#39; service quality in Nanjing. Comparative analysis reveals that the proposed CRP offers significant advantages in consensus efficiency, the discriminatory power of group consensus, and consensus robustness in the presence of non-cooperative behavior compared to existing CRPs. Differing from previous CRP research primarily focuses on minimizing adjustment scales, this study emphasizes the adaptive evolution and convergence of opinions, providing a more suitable description of group opinion dynamics. Additionally, from a methodological perspective, this research utilizes the ER network model and the PSO algorithm to support the adaptive CRP at both structural and preference levels, enhancing consensus discriminatory power and robustness.},
  archive      = {J_ASOC},
  author       = {Xiwen Tao and Wenqi Jiang and Weijian Jin and Jiali Wang},
  doi          = {10.1016/j.asoc.2024.112183},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The adaptive consensus reaching process with the ER-PSO interaction mechanism and its application in public charging stations evaluation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Branching evolution for unknown objective optimization in
biclustering. <em>ASOC</em>, <em>166</em>, 112182. (<a
href="https://doi.org/10.1016/j.asoc.2024.112182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclusters hold significant importance in microarray analysis. Given the EA algorithm’s efficacy in tackling nonlinear problems, it has become a prevalent choice for evolutionary biclustering in microarray analysis. However, in conventional approaches, the objective of bicluster volume remains elusive, as it heavily relies on the yet-to-be-discovered real bicluster. This discrepancy introduces a novel research problem termed “unknown objective optimization” in our study. To address this issue, our paper introduces an innovative branching evolution strategy within a multi-objective framework. This strategy aims to resolve the challenge of unknown objectives. Throughout the biclustering search process, we meticulously observe the evolution of optimal bicluster individuals. Stability in both mean squared residue (MSR) and volume suggests a high likelihood of reaching an optimal solution, whether local or global. If a global optimal solution is attained at the end of the final evolution, our initial assumption is validated; otherwise, it necessitates an update. The proposed branching strategy is subsequently implemented to bifurcate the original evolution into two branches. One continues the original evolution with an unknown objective of bicluster volume, while the other pursues a new evolution with an estimated objective of bicluster volume. Our algorithm’s performance is assessed through comparisons with various traditional and evolutionary biclustering algorithms. The experimental results affirm its enhanced efficacy on both synthetic datasets and real gene datasets.},
  archive      = {J_ASOC},
  author       = {Qinghua Huang and Hao Xu and Haoning Li},
  doi          = {10.1016/j.asoc.2024.112182},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Branching evolution for unknown objective optimization in biclustering},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rotated object detection strategy for remote sensing
images using misaligned cross-fusion structures. <em>ASOC</em>,
<em>166</em>, 112181. (<a
href="https://doi.org/10.1016/j.asoc.2024.112181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing image object detection is a challenging task with a focus on detection accuracy. In the object detection task of remote sensing images, scenes often contain objects with varying scales and dense arrays, leading to issues such as low accuracy, missed detections, and false alarms. To address these challenges, this paper introduces MCSC-Net, a rotated object detector, designed to enhance detection performance for objects of different scales in densely arranged scenarios. The proposed detector features a novel neck network named MCFN-V3, which incorporated a cross-fusion structure. MCFN-V3 facilitates simultaneous feature extraction in upper and lower layers with feedback to the middle layer. Through the cross-fusion of feature layers, it achieves the integration of multi-scale feature information, thereby enhancing the network&#39;s detection capabilities for objects at multiple scales. Additionally, the paper presents the STC module to address the problem of poor feature correlation in high-resolution image processing by CNN networks. This module improves feature expressiveness and range, strengthening inter-feature relationships within layers. Furthermore, in the target positioning stage, the paper transforms the angle regression problem into a classification task, making the network more suitable for detecting rotated objects in dense scenes. To assess the effectiveness of the algorithm, experiments were conducted on two publicly available remote sensing datasets, DOTA and UCAS-AOD. Our method achieved a multiclass average precision (mAP) of 77.4 % on DOTA and an outstanding performance of 97.0 % on UCAS-AOD. These experimental results validate the effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Haocong Li and Hui Ma},
  doi          = {10.1016/j.asoc.2024.112181},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rotated object detection strategy for remote sensing images using misaligned cross-fusion structures},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing image inpainting efficiency: An exploration of
pixel and channel split operations. <em>ASOC</em>, <em>166</em>, 112179.
(<a href="https://doi.org/10.1016/j.asoc.2024.112179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based image inpainting techniques have achieved unprecedented results using encoder–decoder structures to recover complex missing areas of an image. Recent inpainting models use additional information or networks (e.g., landmarks, edges, styles, and filters) to realize improved restoration performance, but at the cost of increased computational resources. To improve the relationship between inpainting performance and the number of model parameters, researchers have investigated efficient structural approaches such as recurrent and residual connection structures. However, these methods are difficult to apply in the general encoder–decoder structure. In this study, we explored the downsampling and upsampling operations associated with an encoder–decoder structure. We propose two novel split operations: the pixel-split operation (PSO) and channel-split operation (CSO). The proposed PSO transfers image features from high to low resolution with two dilation rate effects and a similar number of parameters as existing downsampling operations. Conversely, the proposed CSO increases the image resolution using only one-fourth the number of parameters of existing upsampling operations. The restoration performance and efficiency of the proposed model were evaluated in terms of five metrics on public datasets, e.g., the Places2 and CelebA datasets, to validate our proposed operations’ contribution to inpainting performance. We achieved state-of-the-art performance and reduced the size of the parameters by 20%. An ablation study was conducted to confirm the effect of each operation on the CelebA-HQ dataset. Results indicated that these split operations exhibit an advanced relationship between inpainting performance and optimization of the model parameters. The corresponding codes are available online ( https://github.com/MrCAIcode/Split_operation_for_inpainting ).},
  archive      = {J_ASOC},
  author       = {Youngjun Choo and Adrian Matias Chung Baek and Namhun Kim},
  doi          = {10.1016/j.asoc.2024.112179},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112179},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing image inpainting efficiency: An exploration of pixel and channel split operations},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal segmentation of non-linear and non-stationary time
series based on fractal dimension and poincare section and its
application in solving EEG-signal. <em>ASOC</em>, <em>166</em>, 112178.
(<a href="https://doi.org/10.1016/j.asoc.2024.112178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-linearity and non-stationarity characteristics in physiological and biological time series limit the reliability of conventional analysis. The main challenge in pattern recognition problems is determining the Optimal-Window-Duration (OWD) for segmenting these time series. Our study presents an innovative algorithm, based on Individual-Optimal-Segmentation (IOS), which integrates chaos theory using fractal dimension, and Poincare section into the estimation of the OWD for segmenting time series with non-linear and non-stationary characteristics. In the logistic map case study, the IOS algorithm proved to be effective in detecting cycles in a range of numerical examples, from period-2 to chaotic behavior. So that its performance was superior to previous approaches, especially when it comes to detecting chaotic behaviors. Furthermore, the combination of the IOS algorithm with the Welch signal processing method shows promise in identifying abnormal Power-Spectral-Density (PSD) curves related to Beta and Gamma rhythms in depression diagnosis scenarios. The generated depression diagnosis system attained an accuracy (%) of 99.07±0.35 (with 2-channel) using the Monte Carlo simulation method. These findings suggest a potentially valuable tool for addressing segmentation challenges in time series analysis, particularly in contexts involving complex, dynamic data. While, it is suggested that future research investigate the computational efficiency and robustness of the IOS algorithm when handling large-scale or real-time data and increase its application in various fields in data science and signal processing. Furthermore, expanding the application range of the IOS algorithm for other pattern recognition problems with physiological or biological data can strengthen the generalizability and practical application of the algorithm.},
  archive      = {J_ASOC},
  author       = {Zolfagharzadeh-Kermani Mahdi and Rashidi Saeid and Asaseh Maryam},
  doi          = {10.1016/j.asoc.2024.112178},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112178},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal segmentation of non-linear and non-stationary time series based on fractal dimension and poincare section and its application in solving EEG-signal},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal quantum circuit generation for pixel segmentation in
multiband images. <em>ASOC</em>, <em>166</em>, 112175. (<a
href="https://doi.org/10.1016/j.asoc.2024.112175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach is proposed for multiband image processing via quantum models in real situations. Quantum circuits are automatically generated ad-hoc for each use case via multiobjective genetic algorithms. Using this universal method, image processing tasks such as segmentation can be carried out by considering the properties that constitute each pixel. The generated circuits present a low level of correlation between qubits, and thus can be considered quantum-inspired machine learning models. The effectiveness of this methodology has been validated by applying it to different segmentation use cases. Comparisons are made between optimized classical kernel methods and the generated quantum-inspired models to understand their behaviors. The results show that quantum models for multiband image processing achieve accuracies similar to those of classical methods.},
  archive      = {J_ASOC},
  author       = {Sergio Altares-López and Juan José García-Ripoll and Angela Ribeiro},
  doi          = {10.1016/j.asoc.2024.112175},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal quantum circuit generation for pixel segmentation in multiband images},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer-AE: A novel autoencoder-based impact detection
model for structural digital twin. <em>ASOC</em>, <em>166</em>, 112174.
(<a href="https://doi.org/10.1016/j.asoc.2024.112174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting the location and intensity of impacts is crucial for ensuring structural safety. Currently, AI-based structural impact detection methods are widely used for their excellent detection accuracy. However, their generalization capability is limited by the scenarios present in the training data. Many complex and dangerous impact scenarios are difficult to conduct real-world experiments on to collect sufficient samples. To capture all impact scenarios and fully leverage the advantages of AI-based detection technologies, advanced methods involve combining real-world structural monitoring data with corresponding numerical models to construct digital twins. These methods continuously refine the created numerical models with limited real-world data and provide diverse impact scenarios through numerical model simulations. However, there are inevitable differences between digital models and physical models that are challenging to correct through mechanical means. This discrepancy in data distribution between the two models significantly hinders the application of digital twin technology in impact/event identification tasks. To address this challenge, this study proposes a novel model based on autoencoders, named Transfer-AE. Transfer-AE encodes the common features of digital twins in the latent space to bridge the uncertainty gap at a macro scale between numerical models and physical models and synchronously fits the magnitude and location of the impact load in the decoder. This enables consistent detection results for the same impact event, whether the sample comes from the numerical model or the physical model. Transfer-AE includes two operating modes: Mode 1 has a fixed computational complexity with stable inference speed, but the training cost and difficulty increase with data distribution. Mode 2&#39;s computational complexity increases with data distribution, but it has a fixed training cost and speed. In both cases involving the geodesic dome structure simulating a deep space habitat and the IASC-ASCE benchmark structure, Transfer-AE demonstrated the best performance in impact localization and quantification tasks compared to mainstream domain-adaptive transfer models.},
  archive      = {J_ASOC},
  author       = {Chengjia Han and Zixin Wang and Yuguang Fu and Shirley Dyke and Adnan Shahriar},
  doi          = {10.1016/j.asoc.2024.112174},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transfer-AE: A novel autoencoder-based impact detection model for structural digital twin},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parametric picture fuzzy cross-entropy measures based on
d-choquet integral for building material recognition. <em>ASOC</em>,
<em>166</em>, 112167. (<a
href="https://doi.org/10.1016/j.asoc.2024.112167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper introduces a novel picture fuzzy cross-entropy measure that utilizes Lin’s divergence as a basis for comparing two picture fuzzy sets. To improve the flexibility and applicability of this new cross-entropy measure, a family of parametric cross-entropy measures is defined. By adjusting the parameters in applications, the influence of the degree of positive membership, negative membership, and neutral membership on the picture fuzzy cross-entropy can be observed, allowing for a more tailored analysis. Additionally, we investigate the relationship between power weighted means and the d d -Choquet integral, which serves as an extension of the ordinary Choquet integral. Using this knowledge, picture fuzzy cross-entropy measures based on the d d -Choquet integral are introduced to yield more sensitive results, particularly when interactions between criteria exist in specific problem domains. This consideration of criterion interactions is often absent in existing cross-entropy measures. Moreover, an algorithm is presented for solving pattern recognition problems and applied to a building material recognition problem sourced from existing literature. Then, we proposed another algorithm and use it to investigate a novel material classification problem. Through these applications, the effectiveness of the proposed cross-entropy measures in pattern recognition is demonstrated. The paper conducts a comparative analysis between existing methods and the proposed approaches, followed by a sensitivity analysis. This analysis involves manipulating the parameters derived from both the parametric cross-entropy measures and the d d -Choquet integrals to assess their respective impacts and sensitivities. Finally, the results regarding the classification problem are examined with performance metrics such as accuracy, precision, recall, and F1 score.},
  archive      = {J_ASOC},
  author       = {Mahmut Can Bozyı̇ğı̇t and Murat Olgun and Mehmet Ünver and Di̇lek Söylemez},
  doi          = {10.1016/j.asoc.2024.112167},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parametric picture fuzzy cross-entropy measures based on d-choquet integral for building material recognition},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fine-grained RDF graph model for fuzzy spatiotemporal
data. <em>ASOC</em>, <em>166</em>, 112166. (<a
href="https://doi.org/10.1016/j.asoc.2024.112166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncertainty and spatiotemporal dynamics of information necessitate the urgent modeling of fuzzy spatiotemporal knowledge across various applications, with the Resource Description Framework (RDF) serving as a widely recognized data representation model. Existing models suffer from incomplete semantic representation and poor robustness in modeling fuzzy spatiotemporal data, e.g., lack of fuzziness in spatiotemporal semantics; lack of altitude in spatial semantics. Meanwhile, the algebraic framework regarding the model has not been investigated. Thus, in this paper, we first propose a new fine-grained fuzzy spatiotemporal RDF model. This model can represent fine-grained uncertain spatiotemporal semantics that may be associated with any element of a spatiotemporal RDF. We further define its graph algebraic operations. Note that we demonstrate the use of the algebraic operations for fuzzy spatiotemporal RDF querying. Finally, we establish a set of transformation rules for SPARQL query syntax to algebraic operations in fuzzy spatiotemporal RDF. In addition, we used experiments to evaluate the validity and rationality of our model.},
  archive      = {J_ASOC},
  author       = {Hao Ji and Li Yan and Zongmin Ma},
  doi          = {10.1016/j.asoc.2024.112166},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fine-grained RDF graph model for fuzzy spatiotemporal data},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-fidelity deep neural network with monte carlo dropout
technique for uncertainty-aware risk recognition of backward erosion
piping in dikes. <em>ASOC</em>, <em>166</em>, 112165. (<a
href="https://doi.org/10.1016/j.asoc.2024.112165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backward erosion piping (BEP) is an increasingly critical failure mechanism in dike systems, often triggered by floods resulting from extreme rainfall events, which are exacerbated by the ongoing shifts in climate patterns. This study embarks on a nuanced examination of BEP, a phenomenon that substantially undermines flood control and emergency management endeavors. Addressing the complexities of BEP incidents and the consequential variable impact on dike infrastructure, we have incorporated Monte Carlo dropout techniques within a multi-fidelity deep learning framework to enhance predictive accuracy and provide an uncertainty-aware assessment of BEP risk. Drawing on 16 sets of physical model experiments emulating dike structures from the lower Yangtze River basin in China, we conducted a detailed study of the BEP evolution mechanism under various hydrological scenarios intensified by climate change. These experiments allowed us to observe the initiation and progression of BEP in different conditions. The development of a structural equation model quantifies the effects of several critical factors—including those exacerbated by climatic variability—on BEP dynamics. Seven key factors were identified as influential to BEP risk levels, integrating distinct BEP traits, hydrological attributes, and dike engineering conditions. A Monte Carlo dropout-enhanced multi-fidelity deep neural network (MFDNN) was crafted, synthesizing low-fidelity experimental data with high-fidelity field case studies, to construct an advanced model for BEP risk level identification. Compared against four sophisticated machine learning models, our MFDNN demonstrated superior performance, effectively blending experimental insights with real-world occurrences. The proposed model emerges as a scientifically robust and pragmatic tool for delineating BEP risk levels in dike systems, providing vital guidance for categorizing BEP incidents and forging targeted, climate-informed emergency response strategies.},
  archive      = {J_ASOC},
  author       = {Hongchen Liu and Huaizhi Su and Jiaquan Yang and Haijiang Li},
  doi          = {10.1016/j.asoc.2024.112165},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-fidelity deep neural network with monte carlo dropout technique for uncertainty-aware risk recognition of backward erosion piping in dikes},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning-based low-latency task
offloading for mobile-edge computing networks. <em>ASOC</em>,
<em>166</em>, 112164. (<a
href="https://doi.org/10.1016/j.asoc.2024.112164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Mobile Edge Computing (MEC) is an emerging solution to facilitate users’ computing. However, there are still some drawbacks, such as slow learning speed and high task latency. To overcome these difficulties, a deep reinforcement learning (DRL)-based task offloading (DRTO) framework is proposed, taking into account the time-varying nature of wireless channels and the unpredictability of task arrivals in multi-user MEC systems. Many deep neural networks (DNNs) are also used as scalable solutions to quickly determine the best offloading strategy by combining the perceptual capabilities of deep learning with the decision making capabilities of reinforcement learning (RL). Additionally, the concept of a minimum upper bound on the target Q-value is utilized to address the value overestimation problem that occurs during offload policy updates. This approach aims to minimize the latency of offload decisions by considering different channel environments, variable latency constraints among many users, and limited computational resources. Simulation results show that the algorithm reduces computation time and latency while achieving near-optimal performance compared to traditional Deep Deterministic Policy Gradient (DDPG) and Q-learning optimization methods. In addition, DRTO achieves more than 99.6% computational speedup compared to current almost ideal benchmarking techniques. Thus, the DRTO computational offload approach offers great potential for reducing overall system latency and improving the user experience.},
  archive      = {J_ASOC},
  author       = {Wentao Yang and Zhibin Liu and Xiaowu Liu and Yuefeng Ma},
  doi          = {10.1016/j.asoc.2024.112164},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning-based low-latency task offloading for mobile-edge computing networks},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TAP with ease: A generic recommendation system for
trigger-action programming based on multi-modal representation learning.
<em>ASOC</em>, <em>166</em>, 112163. (<a
href="https://doi.org/10.1016/j.asoc.2024.112163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating popularity of smart devices has given rise to an increasing trend wherein users leverage customized trigger-action programming (TAP) rules within the Internet of Things (IoT) to automate various aspects of their lives. This article addresses the challenge of effectively combining functions provided by many smart devices and online services by introducing a novel multi-modal representation learning model called TAP-TAG. This model integrates both textual and graph structures inherent in TAP rules, offering a holistic method to rule recommendation. TAP-TAG comprises two branches: the Knowledge Graph Embedding model, which projects triplets extracted from the TAP dataset into embeddings, and convolution neural networks that extract semantic features from the textual content of TAP rules. Extensive experiments are conducted on real-world TAP datasets to evaluate our model’s ability to recommend relevant rules based on user preferences. The experimental results show that TAP-TAG can outperform the state-of-the-art method by 5% in P r e c i s i o n @ 5 Precision@5 , indicating that TAP-TAG is highly effective in providing accurate and diverse recommendations for TAP rules.},
  archive      = {J_ASOC},
  author       = {Gang Wu and Ming Wang and Feng Wang},
  doi          = {10.1016/j.asoc.2024.112163},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TAP with ease: A generic recommendation system for trigger-action programming based on multi-modal representation learning},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning with transformer initialization and
clustering prior for text representation. <em>ASOC</em>, <em>166</em>,
112162. (<a href="https://doi.org/10.1016/j.asoc.2024.112162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring labeled data for learning sentence embeddings in Natural Language Processing poses challenges due to limited availability and high costs. In order to tackle this issue, we introduce a novel method called C ontrastive L earning with T ransformer Initialization and C lustering Prior for Text Representation (CLTC). Our method utilizes Pre-Layernorm Transformers without warm-up, stabilizing the training process while also increasing the final performance. We employ Contrastive Learning (CL) with dropout-based augmentation to enhance sentence embeddings. Additionally, we integrate prior knowledge into the contrastive learning framework within an efficient clustering strategy. When evaluated on the SentEval task, our approach showcases a competitive performance when compared to state-of-the-art approaches in the contrastive learning domain. Our method offers stability, improved embeddings, and the utilization of prior knowledge for enhanced unsupervised representation learning in Natural Language Processing applications.},
  archive      = {J_ASOC},
  author       = {Chenjing Liu and Xiangru Chen and Peng Hu and Jie Lin and Junfeng Wang and Xue Geng},
  doi          = {10.1016/j.asoc.2024.112162},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning with transformer initialization and clustering prior for text representation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced diversity indicator-based many-objective
evolutionary algorithm with shape-conforming convergence metric.
<em>ASOC</em>, <em>166</em>, 112161. (<a
href="https://doi.org/10.1016/j.asoc.2024.112161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of objectives increases, many-objective optimization problems (MaOPs) become increasingly complex. Traditional indicator-based many-objective evolutionary algorithms can often ensure the convergence of the population but tend to struggle with maintaining its diversity. In this paper, an enhanced diversity indicator-based many-objective evolutionary algorithm with shape-conforming convergence metric, namely, MaOEA-DISC, is presented to relieve this weakness. In MaOEA-DISC, firstly, we focus on the inter-individual spacing relationships within the population based on I ϵ + Iϵ+ Indicator, proposing a novel enhanced diversity I ϵ + Iϵ+ Indicator to ensure the enhancement of population diversity while converging. Secondly, we propose a new metric of individual convergence, which calculates the convergence of individuals based on the shape of Pareto front, reducing the errors caused by the shape of the Pareto front when measuring individual convergence, thereby assessing individual convergence more accurately. Finally, to further improve the convergence speed of the population, different mating strategies are employed for mating in the parental generation. MaOEA-DISC is compared with other algorithms on various benchmark MaOPs ranging from 3 to 10 objectives, as well as a real-world MaOPs. Experimental results demonstrate that when dealing with MaOPs, MaOEA-DISC not only achieves excellent population convergence and diversity but also effectively maintains a balance between them, showing promising practical value.},
  archive      = {J_ASOC},
  author       = {Jiale Cao and Lei Yang and Kangshun Li and Yuanye Zhang and Jinglin Tian and Dongya Wang},
  doi          = {10.1016/j.asoc.2024.112161},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced diversity indicator-based many-objective evolutionary algorithm with shape-conforming convergence metric},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiobjective sparse unmixing based hyperspectral change
detection. <em>ASOC</em>, <em>166</em>, 112160. (<a
href="https://doi.org/10.1016/j.asoc.2024.112160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral unmixing can provide the composition of ground objects, while change detection can identify the changes of the same region over time. Therefore, unmixing based hyperspectral change detection can investigate changes in a subpixel-level. It can provide not only whether the change happens or not but also how to change. This paper will gradually establish an unconstrained sparse unmixing based change detection model and design a multiobjective optimization sparse unmixing approach (termed MoSU-CD) to solve it for more subpixel-level change details. Compared with the existing sparse unmixing based change detection approaches, MoSU-CD directly unmixes the difference map of multi-temporal images rather than multi-temporal images themselves respectively. Then it explores the change details of one substance in each pixel based on the unmixing results, i.e., endmembers and abundances. In addition, in order to improve the change detection accuracy, an ensemble decision strategy is designed for the Pareto optimal solutions obtained by multiobjective optimization method. Based on this strategy, the subpixel-level changes can be aggregated to generate an aggressive pixel-level change map. The experimental results on synthetic and real data sets demonstrate that the proposed MoSU-CD not only outperforms the benchmark and state-of-art approaches in terms of pixel-level change detection but also provides additional subpixel-level change information.},
  archive      = {J_ASOC},
  author       = {Xiangming Jiang and Tianqi Gao and Maoguo Gong and Fenlong Jiang and Mingyang Zhang and Jieyi Liu},
  doi          = {10.1016/j.asoc.2024.112160},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiobjective sparse unmixing based hyperspectral change detection},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hound-inspired pre-hybridized genetic approach for router
placement in wireless mesh networks. <em>ASOC</em>, <em>166</em>,
112159. (<a href="https://doi.org/10.1016/j.asoc.2024.112159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last years, wireless mesh networks (WMNs) have gained more and more popularity in many research and industrial applications thanks to their easy implementation, maintenance, and great reliability at a low cost. Nevertheless, for a large number of nodes, the performance of such networks is heavily influenced by the positioning of routers and gateways over the area to be covered. In this paper, we tackle the router placement problem, which is known to be NP-hard, and its approximate solution through a meta-heuristic approach. The proposed solution empowers the benefits offered by a genetic algorithm pre-hybridized with a local search approach inspired by the behavior of hound dogs. The basic idea is to exploit the dogs’ capabilities in moving throughout the solution space to effectively explore it by placing themselves in areas that are more favorable for achieving a high-quality approximate solution in a reasonable time. Experimental results on several benchmarking instances and comparisons with the most effective state-of-the-art algorithms have demonstrated the potential of the proposed approach. This is evidenced by very high connectivity and coverage, a low number of generations, and a small GA population required for convergence. This results in low computational effort and significant time savings, which are of paramount importance in IoT and edge scenarios. We remark that, although offering potential, at the current state, our proposal is not able to adapt to areas with obstacles or irregular shapes.},
  archive      = {J_ASOC},
  author       = {Gianni D’Angelo and Francesco Palmieri},
  doi          = {10.1016/j.asoc.2024.112159},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hound-inspired pre-hybridized genetic approach for router placement in wireless mesh networks},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating named entity recognition: A comparative analysis
of mono- and multilingual transformer models on a novel brazilian
corporate earnings call transcripts dataset. <em>ASOC</em>,
<em>166</em>, 112158. (<a
href="https://doi.org/10.1016/j.asoc.2024.112158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2018, when the Transformer architecture was introduced, Natural Language Processing has gained significant momentum with pre-trained Transformer-based models that can be fine-tuned for various tasks. Most models are pre-trained on large English corpora, making them less applicable to other languages, such as Brazilian Portuguese. In our research, we identified two models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to evaluate their performance on a financial Named Entity Recognition (NER) task and determine the computational requirements for fine-tuning and inference. To this end, we developed the Brazilian Financial NER (BraFiNER) dataset, comprising sentences from Brazilian banks’ earnings calls transcripts annotated using a weakly supervised approach. Additionally, we introduced a novel approach that reframes the token classification task as a text generation problem. After fine-tuning the models, we evaluated them using performance and error metrics. Our findings reveal that BERT-based models consistently outperform T5-based models. While the multilingual models exhibit comparable macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In terms of error metrics, BERTimbau outperforms the other models. We also observed that PTT5 and mT5 generated sentences with changes in monetary and percentage values, highlighting the importance of accuracy and consistency in the financial domain. Our findings provide insights into the differing performance of BERT- and T5-based models for the NER task.},
  archive      = {J_ASOC},
  author       = {Ramon Abilio and Guilherme Palermo Coelho and Ana Estela Antunes da Silva},
  doi          = {10.1016/j.asoc.2024.112158},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating named entity recognition: A comparative analysis of mono- and multilingual transformer models on a novel brazilian corporate earnings call transcripts dataset},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-guided population co-evolutionary algorithm based on
multiple similarity decomposition for large-scale flexible job shop
scheduling problem. <em>ASOC</em>, <em>166</em>, 112157. (<a
href="https://doi.org/10.1016/j.asoc.2024.112157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As manufacturing shifts towards large-scale production, the size of the workshop increases, and its search space exponentially expands. It is difficult for existing algorithms to obtain an ideal scheduling solution in an acceptable time. For the large-scale flexible job shop scheduling problem (LSFJSP), a multi-guided population co-evolutionary algorithm based on multiple similarity decomposition (MPCSD) is designed. Faced with the problem of high dimensionality and complex solution space, a multiple similarity decomposition strategy is proposed. It proceeds to group based on similarity information at the dimension and population level. To obtain convergence-preferred and diversity-preferred dimension groupings, a training-set solution selection method is proposed. Inspired by the idea of divide-and-conquer, a multi-guided co-evolutionary strategy is proposed. It improves the exploration efficiency of the algorithm in the search space. To test effectiveness on more complex LSFJSP, a set of large-scale test problems including LS1-12 are designed. On LS1-12, MPCSD is compared with seven other algorithms to demonstrate its superiority. MPCSD performed best on 11, 7, and 10 of the 12 test problems on Inverted Generational Distance (IGD), Hypervolume (HV) and Schott’s Spacing Metric (SP), respectively. Meanwhile, the Relative Deviation (RD) results showed that MPCSD obtained the best fitness performance.},
  archive      = {J_ASOC},
  author       = {Cong Wang and Lixin Wei and Hao Sun and Ziyu Hu},
  doi          = {10.1016/j.asoc.2024.112157},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-guided population co-evolutionary algorithm based on multiple similarity decomposition for large-scale flexible job shop scheduling problem},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A particle dynamical system algorithm to find the sparse
linear complementary solutions. <em>ASOC</em>, <em>166</em>, 112156. (<a
href="https://doi.org/10.1016/j.asoc.2024.112156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Linear Complementarity Problem (LCP) offers a comprehensive modeling framework for addressing a wide range of optimization problems. In many real-world applications, finding an LCP solution with a sparse structure is often necessary. To address this problem, we introduce an innovative global optimization framework named the Particle Dynamical System Algorithm (PDSA), which consists of two components. The first component is a dynamical system (DS) inspired by the Absolute Value Equation (AVE), proven to have equilibria corresponding to LCP solutions, with additional relaxing regulators that enhance coverage rate and stability. The second component is an Adaptive Oscillated Particle Swarm Optimization (AOPSO) designed to globally enhance sparsity in LCP solutions, addressing the complexities posed by non-convex and non-smooth regulation models. Within this framework, the DS achieves optimality, while the AOPSO promotes solution sparsity. We compared our proposed DS with relaxing regulators to two classic efficient DSs, fully validating the effectiveness of our approach and underscoring the significant role of the introduced relaxing regulators in improving the convergence rate. Our newly developed variant of PSO, AOPSO, was compared with three classic and state-of-the-art variants on fourteen benchmark functions, demonstrating its competitive performance. Finally, we performed experiments on seven test examples and an application in portfolio selection, showing that the proposed PDSA algorithm surpasses other competitors in finding sparse LCP solutions.},
  archive      = {J_ASOC},
  author       = {Feiran Wang and Jiawei Chen and Haiwu Huang and Shilong Xu},
  doi          = {10.1016/j.asoc.2024.112156},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle dynamical system algorithm to find the sparse linear complementary solutions},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic switched crowding-based multi-objective particle
swarm optimization algorithm for solving multi-objective AC-DC optimal
power flow problem. <em>ASOC</em>, <em>166</em>, 112155. (<a
href="https://doi.org/10.1016/j.asoc.2024.112155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the multi-objective AC-DC optimal power flow (MO/AC-DC OPF) problem in the presence of renewable energy sources (RESs), flexible AC transmission system (FACTS) devices and multi-terminal direct current (MTDC) systems is introduced for the first time. Conflicting objective functions and the high complexity of the objective and constraint spaces are the main challenges in finding optimal solutions for MO/AC-DC OPF. To overcome these challenges, twelve different versions of the dynamic switched crowding-based multi-objective particle swarm optimization (DSC-MOPSO) algorithm are introduced in this paper. Studies on multimodal optimization problems have shown that all DSC-MOPSO versions have better performance metrics than the MOPSO algorithm. Using the developed DSC-MOPSO and its strong competitors, the Pareto-optimal solution sets of the MO/AC-DC OPF problem are investigated. In these investigations, the performances of the algorithms are tested for the minimization of dual and triple objectives such as fuel cost, voltage level deviation, emission and power loss in a modified IEEE 30-bus power grid. According to the simulation results, the proposed DSC-MOPSO achieved an improvement in fuel cost between 0.02 % and 5.05 % and a reduction in active power loss between 0.44 % and 30.74 % compared to its competitors. The Hypervolume (HV) performance metric was used to evaluate the Pareto-front coverage performance of DSC-MOPSO and other optimizers. The results from nine case studies of the MO/AC-DC OPF were statistically analyzed by the Friedman test according to the 1/HV metric. According to the Friedman test results, the rankings of DSC-MOPSO and MOMA are 1.984 and 3.079, respectively, ranking first and second among all competitors. Finally, in this study, feasible solutions for MO/AC-DC OPF problem are identified for the first time and the stability of competitive algorithms in finding these solutions is analyzed for the first time. The success rates and search times of DSC-MOPSO and MOMA algorithms in finding feasible solutions for MO/AC-DC OPF are 91.01 % (30.641 s) and 82.01 % (46.038 s), respectively.},
  archive      = {J_ASOC},
  author       = {Hüseyin Bakır and Hamdi Tolga Kahraman and Samet Yılmaz and Serhat Duman and Ugur Guvenc},
  doi          = {10.1016/j.asoc.2024.112155},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic switched crowding-based multi-objective particle swarm optimization algorithm for solving multi-objective AC-DC optimal power flow problem},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quaternion-based 2D-DOST and stacked principal component
analysis network for multimodal face recognition. <em>ASOC</em>,
<em>166</em>, 112154. (<a
href="https://doi.org/10.1016/j.asoc.2024.112154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition is widely utilized and has become a ubiquitous part of daily lives. However, original face images are often sensitive and unauthorized access could imperil personal information. To protect privacy and security of multimodal face data, we propose an original approach that integrates differential privacy and a lightweight convolutional neural network with unsupervised predefined quaternion-type filters. Firstly, different modalities of face images are banded together by means of full quaternion matrix representation for simultaneously processing. Then the proposed quaternion two-dimensional discrete orthogonal Stockwell transform is exploited for implementing difference privacy, which is disturbed by virtue of Laplace and exponential mechanism. Afterwards, we devise a stacked quaternion two-dimensional principal component analysis network with three-stages for learning representative features. The proposed network model can enhance the discriminative characteristics through inserting nonlinearity, together with union of feature maps in different levels to achieve information complementation. Experiments conducted on four multimodal face datasets unveil that the proposed method achieves superior recognition performance in comparison with other state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Zhuhong Shao and Leding Li and Bicao Li and Yuanyuan Shang and Gouenou Coatrieux and Huazhong Shu and Changmiao Wang},
  doi          = {10.1016/j.asoc.2024.112154},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quaternion-based 2D-DOST and stacked principal component analysis network for multimodal face recognition},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A grid self-adaptive exploration-based algorithm for
multimodal multiobjective optimization. <em>ASOC</em>, <em>166</em>,
112153. (<a href="https://doi.org/10.1016/j.asoc.2024.112153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multimodal multiobjective optimization, the key is to find as many equivalent Pareto optimal solutions as possible through broad exploration in the decision space. The grid search strategy can achieve quick convergence by guiding the evolution with historical information in each grid while enabling broad exploration. However, inaccurate utilization of information in grids may lead to losing numerous potential solutions, especially on imbalanced problems. In order to resolve this issue, a grid self-adaptive exploration-based algorithm (GSEA) is proposed in this paper. In GSEA, the historical information in the grid is accurately utilized through grid-based self-adaptive exploration and niche clearing methods, which retain a large number of solutions with potential and effectively handle multimodal multiobjective optimization problems (MMOPs). Experimental results show that the proposed algorithm outperforms seven other state-of-the-art multimodal multiobjective evolutionary algorithms (MMEAs) on two types of MMOPs, and the approach can effectively deal with the MMOPs with middle-scale decision variables as memory allows.},
  archive      = {J_ASOC},
  author       = {Juan Zou and Xinjie Yang and Qi Deng and Yuan Liu and Yizhang Xia and Zeping Wu},
  doi          = {10.1016/j.asoc.2024.112153},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A grid self-adaptive exploration-based algorithm for multimodal multiobjective optimization},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive detection framework based on artificial immune
for IoT intrusion detection system. <em>ASOC</em>, <em>166</em>, 112152.
(<a href="https://doi.org/10.1016/j.asoc.2024.112152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the continual evolution of new network attack methodologies, defenders face the imperative of constantly upgrading security defenses. Current security technologies, albeit effective against known threats, often fall short in handling the intricacies of diverse and novel attacks. Artificial immunity-based network anomaly detection offers a promising avenue by dynamically adapting to evolving threats. However, prevailing algorithms in this domain suffer from low detection rates, limited adaptability, and extended detector generation times. This study aims to tackle these challenges by introducing a high-efficiency network anomaly detection framework, emphasizing both high-dimensional feature selection and adaptive detector generation. Our approach begins with an enhanced dual-module hybrid high-dimensional feature selection method, leveraging evolutionary principles. Furthermore, we introduce a self-sample clustering algorithm based on fuzzy clustering during the tolerance stage, enhancing detector tolerance efficiency. Additionally, an adaptive detector generation scheme is devised. It divides the non-boundary sub-population based on non-self differences and evolution, while employing the red fox optimization algorithm in the boundary region. This adaptive approach dynamically adjusts detector positions and radii to derive optimal detectors. Through comprehensive validation using well-established IoT and network anomaly datasets, our proposed artificial immunity-based IoT intrusion detection framework exhibits superior performance. It achieves higher classification accuracy and lower error rates compared to current state-of-the-art machine learning and artificial immunity algorithms.},
  archive      = {J_ASOC},
  author       = {Ming Ma and Geying Yang and Junjiang He and Wenbo Fang},
  doi          = {10.1016/j.asoc.2024.112152},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive detection framework based on artificial immune for IoT intrusion detection system},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acoustic feature-based emotion recognition and curing using
ensemble learning and CNN. <em>ASOC</em>, <em>166</em>, 112151. (<a
href="https://doi.org/10.1016/j.asoc.2024.112151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition and understanding plays a crucial role in various domains, including healthcare, human-computer interaction, and mental well-being. In this context, this paper proposes a methodology for recognizing and curing emotions using acoustic features and machine learning algorithms. The approach involves extracting acoustic features from the signals using diverse signal processing techniques. These features are then utilized as inputs for machine learning and deep learning algorithms, including the Random Forest classifier, XG Boost classifier, Convolutional Neural Network (CNN), and an ensemble algorithm. The ensemble algorithm combines Random Forest and XG Boost as base classifiers, with the Naïve Bayes algorithm serving as the meta classifier. We also propose a novel model that generates personalized curing strategies for individuals based on emotion recognition, so they can keep their emotional state positive. With the help of an ensemble learning model the proposed model achieved an emotion recognition accuracy of 92 % by combining three publicly available datasets containing emotional speech recordings. In the neutral and positive emotion classifications, the Receiver Operating Characteristic curve (ROC) had a 98 % accuracy rate while negative emotion classifications had a 91 % true positive rate. The effectiveness of the proposed curing methodology model has also been demonstrated by conducting experiments on a group of individuals and comparing the results with a state-of-the-art Generative Pre-Trained Transformer-3 (GPT-3) and ChatGPT, it was inferred that 89.35 % of the test group preferred the responses of the proposed curing model, over the GPT models The results of our experiments show that our proposed methodology can significantly boost the emotional state of an individual, thereby highlighting its potential for use in clinical settings.},
  archive      = {J_ASOC},
  author       = {Raghav V. Anand and Abdul Quadir Md and G. Sakthivel and T V Padmavathy and Senthilkumar Mohan and Robertas Damaševičius},
  doi          = {10.1016/j.asoc.2024.112151},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acoustic feature-based emotion recognition and curing using ensemble learning and CNN},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). A reference learning network for fault diagnosis of
rotating machinery under strong noise. <em>ASOC</em>, <em>166</em>,
112150. (<a href="https://doi.org/10.1016/j.asoc.2024.112150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The strong noise often masks the fault characteristics of equipment, which reduces the accuracy of fault diagnosis and even leads to the inability of intelligent fault diagnosis algorithms to be applied in industrial environments. This has always been a challenge in the field of mechanical fault diagnosis. As known that equipment failure results from the continuous degradation of the equipment’s state, with the failure state evolving from the healthy state. Considering that both healthy signals and fault signals contain similar noise, this paper proposes a Reference Learning Network (RLNet) model. The model aims to enhance the distinguishing features between healthy and faulty samples through reference units, thereby eliminating the influence of noise on feature distribution. Firstly, the impact of variable speed on the model’s robustness is mitigated using the computed order tracking method. Then, the difference features between healthy samples and a class of fault samples are extracted through the binary classification reference learning unit (RLU). Next, the extracted differential features are used to train the state classifier. Finally, membership weights are employed to effectively combine the feature recognition results, reducing the influence of fault features from mismatched RLUs. The robustness and superiority of the proposed method were verified by comparing it with five other intelligent fault diagnosis methods on the gear and bearing datasets. RLNet is of great significance for the engineering application of intelligent fault diagnosis methods in industrial noise environments.},
  archive      = {J_ASOC},
  author       = {Yinjun Wang and Zhigang Zhang and Xiaoxi Ding and Yanbin Du and Jian Li and Peng Chen},
  doi          = {10.1016/j.asoc.2024.112150},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reference learning network for fault diagnosis of rotating machinery under strong noise},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A compact meta-learned neuro-fuzzy technique for
noise-robust nonlinear control. <em>ASOC</em>, <em>166</em>, 112149. (<a
href="https://doi.org/10.1016/j.asoc.2024.112149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuro-fuzzy systems show promise for adaptive control but can become complex due to the need to learn many parameters. This paper presents a resilient nonlinear controller that combines a simplified neuro-fuzzy system (Simp_NFS) and simplified neural network (Simp_NN) with only two meta-learnable parameters. This architecture enables fast and stable adaptation in uncertain nonlinear discrete-time systems. Simp_NFS utilizes interpretable hyperplane-based rules without antecedent parameters, simplifying the learning process to consequent weights. Simp_NN reduces complexity by replacing hidden-output weights with their mean. The hybrid auto-adaptive controller (HAC) combines the advantages of Simp_NFS and Simp_NN, significantly reducing the number of adaptive parameters compared to standard neuro-fuzzy methods for real-time control with limited resources. Simp_NFS provides structural adaptivity to handle uncertainties, while Simp_NN ensures reliable disturbance attenuation. The stability of HAC is proven using Lyapunov analysis. Extensive testing on challenging single-input single-output (SISO) and multi-input multi-output systems (MIMO) demonstrates that HAC improves performance by up to 82.55% compared to existing techniques. Key innovations include an ultra-compact meta-learned architecture, transparent online evolution of hyperplane clusters, and enhanced modeling capability for nonlinear uncertain systems. This interpretable neuro-fuzzy approach could enhance autonomy and safety by maintaining model transparency. The implementation of HAC is publicly available on GitHub at https://github.com/m-ferdaus/HAC .},
  archive      = {J_ASOC},
  author       = {Md Meftahul Ferdaus and Ahmad Jobran Al-Mahasneh and Sreenatha G. Anavatti and J. Senthilnath},
  doi          = {10.1016/j.asoc.2024.112149},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A compact meta-learned neuro-fuzzy technique for noise-robust nonlinear control},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible assembly job shop scheduling problem considering
reconfigurable machine: A cooperative co-evolutionary matheuristic
algorithm. <em>ASOC</em>, <em>166</em>, 112148. (<a
href="https://doi.org/10.1016/j.asoc.2024.112148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With popularity of mass customization, enterprises urgently need to improve reconfigurability to handle rapidly changing market demands. Reconfigurable machines have been widely applied in the flexible assembly job shop. These reconfigurable machines can be equipped with various and limited auxiliary modules (AMs) to provide multiple alternative functions to manufacture numerous products. Therefore, this study addresses flexible assembly job shop scheduling problem considering reconfigurable machines with limited AMs to minimize makespan. First, a mixed-integer linear programming (MILP) model is formulated to define the problem. Then, a cooperative co-evolutionary matheuristic algorithm (CCMA) is presented to efficiently tackle large-scale FAJSP-RM problem. In this algorithm, a MILP-based evolution method, which relies on a decomposed MILP (D-MILP) with known sequence decisions, is proposed to explore the optimal machine and AM assignments. Meanwhile, a collaborative initialization and dual collaborative strategy are proposed to improve the performance of the proposed CCMA. Comprehensive experiments are conducted on 720 instances, extended from the well-known Fdata and BRdata benchmarks, to evaluate the proposed MILP model and CCMA algorithm. The results illustrate that the MILP model can produce optimal solutions for small-scale instances. The MILP-based evolution method improves the performance of the CCMA algorithm by 12.63, and the CCMA outperforms other well-known algorithms from numerical, statistical, differential and stable analysis.},
  archive      = {J_ASOC},
  author       = {Yifan Hu and Liping Zhang and Zikai Zhang and Zixiang Li and Qiuhua Tang},
  doi          = {10.1016/j.asoc.2024.112148},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Flexible assembly job shop scheduling problem considering reconfigurable machine: A cooperative co-evolutionary matheuristic algorithm},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing allergy source mapping: A comprehensive
multidisciplinary framework integrating machine learning, graph theory
and game theory. <em>ASOC</em>, <em>166</em>, 112147. (<a
href="https://doi.org/10.1016/j.asoc.2024.112147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Allergic reactions can range from mild discomfort to life-threatening situations. To manage the healthcare difficulty, an efficient allergens mapping is required by mapping the allergies to reduce the severity risk reactions. The allergies are mapping with specific food items according to the daily usage. It enables the clear communication, targeted avoidance, and public awareness, which is necessary to scrutinize the prevention process. Since, the allergies are highly individualized and can exhibit significant variability among individuals that are capturing the complexity in the process of overcoming the predictive challenges. The complex relationships and data challenges require advanced approaches like ML and graph theory. For this purpose, we propose a new multidisciplinary framework that integrates the Machine Learning (ML), Graph Theory and Game Theory to predict the allergies associated with relevant foods using a modest dataset. This framework has two newly built graph models such as a conventional approach and a refined approach, to pave the way for better results. Here, the ML techniques are employed to perform the classification process on tabular data that are observed the remarkable improvements and transforming the data into a graph. Moreover, the Darwinian decision-making framework is adopted theoretically in evolutionary game theory to formulate effective formulas for assessing the spread of allergies among allergens and predict the allergies dynamically. The proposed framework has been evaluated by conducting experiments by using a modest dataset by considering the evaluation metrics such as accuracy, macro-precision, macro-recall, and macro F1-score.},
  archive      = {J_ASOC},
  author       = {Isshaan Singh and Khushi Agarwal and Sannasi Ganapathy},
  doi          = {10.1016/j.asoc.2024.112147},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing allergy source mapping: A comprehensive multidisciplinary framework integrating machine learning, graph theory and game theory},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bio-inspired distributed load frequency control in islanded
microgrids: A multi-agent deep reinforcement learning approach.
<em>ASOC</em>, <em>166</em>, 112146. (<a
href="https://doi.org/10.1016/j.asoc.2024.112146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To prevent errors in load frequency control (LFC) decision-making in an islanded microgrid and reduce the waste of frequency regulation resources, a fully distributed “starfish” load frequency control method is proposed. For this method, a novel deep reinforcement learning algorithm called the distributed decomposed multirole multiagent deep deterministic policy gradient (DDMR-MADDPG) algorithm is introduced. This algorithm treats each unit as an agent, enabling centralized training of all the agents. Through the coordination and control of multiple agents, a global optimal strategy can be obtained. During online application, the algorithm employs a distributed implementation strategy, enabling each unit to make decisions autonomously. This decision-making strategy is akin to the distributed neural network of a starfish, and it can fundamentally improve the efficiency and correctness of decision-making. In addition, it employs multiple techniques, including value function decomposition, multirole learning, imitation learning, and curriculum learning approaches. The performance of the proposed method compared with 24 existing methods is demonstrated on a simulated LFC model of the Zhuzhou Island microgrid in China.},
  archive      = {J_ASOC},
  author       = {Jiawen Li and Tao Zhou},
  doi          = {10.1016/j.asoc.2024.112146},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bio-inspired distributed load frequency control in islanded microgrids: A multi-agent deep reinforcement learning approach},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective group learning algorithm with a
multi-objective real-world engineering problem. <em>ASOC</em>,
<em>166</em>, 112145. (<a
href="https://doi.org/10.1016/j.asoc.2024.112145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concern of multi-objective optimization is to deal with optimization problems with more than one objective that should be optimized at the same time. In this paper, a new multi-objective optimization algorithm called multi-objective group learning algorithm is examined. The algorithm is based on the effect of group leaders on the group members and the effect that the group members have on each other. The algorithm is evaluated against the five Zitzler, Deb and Thiele (ZDT) benchmark functions, two standard functions of the DTLZ benchmarks, and a real-world engineering problem. To quantitatively examine the ability of the algorithm four metrics are utilized. Additionally, to statistically confirm the results, standard deviation and the average metrics are used. Moreover, the Wilcoxon rank sum test is used to examine the importance of the results statistically. Finally, the results of the algorithm are evaluated against multi-objective ant lion optimizer, multi-objective evolutionary algorithm based on decomposition, Pareto envelop-based selection algorithm-II, and multi-object moth swarm algorithm. The results of the benchmark functions showed that the proposed work produced better results in most of the metrics for three out of five ZDT benchmarks, and for the rest of the benchmarks it produced better results in at least one of the metrics compared to the participated algorithms. Moreover, the result of the real world engineering problem proved the ability of the proposed algorithm in producing acceptable and better Pareto front in almost all the metrics. For the proposed algorithm, the result of the generational distance for the speed reducer design problem is 0.8002, whereas it is 7.9375710, and 61.98134 for the multi-objective evolutionary algorithm based on decomposition, multi-objective ant lion optimizer, respectively. Additionally, the produced result for the spacing metric by the proposed algorithm was smaller than the result of the other algorithms for the same metric by at least 100 points. The result of the aforementioned metric proves the good distribution of the non-dominated solutions by the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Chnoor M. Rahman and Hardi M. Mohammed and Zrar Khalid Abdul},
  doi          = {10.1016/j.asoc.2024.112145},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective group learning algorithm with a multi-objective real-world engineering problem},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated production and outbound distribution scheduling
problem with multiple facilities/vehicles and perishable items.
<em>ASOC</em>, <em>166</em>, 112144. (<a
href="https://doi.org/10.1016/j.asoc.2024.112144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated production and distribution operations are crucial and necessary for companies to outperform their competitors. In this paper, we study an integrated problem includes heterogeneous facilities and capacitated vehicles. Our goal is to produce and distribute products in batches that meet customer demands at the lowest possible cost. We first give the formal definition of the problem, then propose a Mixed Integer Programming (MIP) formulation and strengthen with several polynomial size valid inequalities. Furthermore, we develop Memetic Algorithm (MA) and finally present 2592 test instances to illustrate the efficiency of both MA and MIP formulation. The optimal or feasible solutions for 1283 of 2592 test instances are obtained by the proposed MIP formulation. MA finds optimal or near optimal solutions for all these instances. When comparing the best solutions obtained by MA with the solutions of the MIP formulation, MA produces a superior result. Besides that, the time to obtain for optimal solutions are approximately 9 min and for all test instances approximately 82 min using MIP formulation. The time needed to solve all test instances with MA is only 3 sec. These findings clearly demonstrate the speed and accuracy of MA in producing results. These results also demonstrate that, it is possible to achieve solutions that are either equal or very close to those found by the MIP formulation, in an extremely short computation time with MA.},
  archive      = {J_ASOC},
  author       = {Gözde Can Atasagun and İsmail Karaoğlan},
  doi          = {10.1016/j.asoc.2024.112144},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated production and outbound distribution scheduling problem with multiple facilities/vehicles and perishable items},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Generalized face forgery detection with self-supervised
face geometry information analysis network. <em>ASOC</em>, <em>166</em>,
112143. (<a href="https://doi.org/10.1016/j.asoc.2024.112143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of high-quality deepfake facial videos has raised concerns about the security of facial images. Existing face forgery detectors mainly tend to locate a specific forgery region of the human face for detection, which achieves satisfactory performance with known forgery patterns presented in the training set. However, with the continuous advancements in face forgery technology, this approach becomes less reliable with new forgery patterns that emerge. Towards this end, we proposed a novel S elf-supervised F ace G eometry Information A nalysis N etwork (SF-GAN) method for generalized face forgery detection. SF-GAN effectively leverages the relationships among informative regions based on information theory. Drawing from information theory, regions with high uncertainty tend to contain more valuable information. Our methodology integrates a self-supervised learning mechanism, enabling the precise identification of multiple informative regions. Furthermore, we leverage facial geometry by establishing both explicit and latent geometric relationships through the use of Graph Convolutional Networks (GCNs). Within our framework, facial landmarks and informative regions are depicted as nodes in the GCNs. By analyzing the geometric relationships between the graph of facial landmarks and the graph of informative regions, we are able to identify valid anomalous regions, thereby minimizing uncertainty. Our proposed model gains a comprehensive understanding of common information in face forgery images. Extensive experiments on eight large-scale benchmark datasets: FaceForensics++ (FF++), WildDeepfake (WDF), Celeb-DF v2 (CDF), DeepFake Detection Challenge (DFDC), DFDC preview (DFDC-P), Deepfake Detection (DFD), DeeperForensics-1.0 (DF-1.0) and ForgeryNIR, show that the proposed method is comparable to state-of-the-arts and exhibits better generalizability. Specifically, our SF-GAN, when trained on high-quality FF++ data, achieves an impressive AUC of 76.43% on the CDF dataset.},
  archive      = {J_ASOC},
  author       = {Rui Zhang and Hongxia Wang and Hanqing Liu and Yang Zhou and Qiang Zeng},
  doi          = {10.1016/j.asoc.2024.112143},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generalized face forgery detection with self-supervised face geometry information analysis network},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Design of a optimal robust adaptive neural network-based
fractional-order PID controller for h-bridge single-phase inverter.
<em>ASOC</em>, <em>166</em>, 112142. (<a
href="https://doi.org/10.1016/j.asoc.2024.112142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Neural Network- based Fractional-order PID Control (NN-FO-PID) approach is designed for H-bridge inverter. This inverter has an LC filter to decrease the level of Total Harmonic Distortion (THD) that can affect the efficiency of the system. In addition, the reduction of THD and stability insurance of the filter are challenging performances. To fulfill this function, a fractional order proportional integrated derivative (FO-PID) controller is developed for the inverter. A few merits of the Fractional-Order notion as a useful technique include reduced sensitivity to noise and parametric fluctuation; however, for a wider range of disturbances, such as noise, this approach shows an unsuitable practical application based on its fixed gain values. Moreover, having parameters uncertainties including parametric variations, load uncertainty, supply voltage variation uplifts this challenging condition severely, and the parameters need to be adjusted once more for more dependable operations. As a result, the control parameters must be optimized once more to provide ideal operations. Here, an adaptive mechanism is proposed based on neural network structure to optimize the gains of the FO-PID controller for better performances. In real applications, this approach has some benefits, since it uses the Black-box technique, which does not necessitate a precise mathematical model of the system, resulting in a reduced computational burden, simple implementation, and reduced dependence on the model’s states. Even under extremely difficult circumstances, the artificial neural network structure effectively optimizes the FO-PID gains in real-time. This benefit can reduce the amount of THD-level for the inverter, properly. Additionally, to have a proper response in the first step condition, and trying to reduce the level of dangerous conditions, based on benefits of the ant lion optimization method, it is considered to select the initial values of the FO-PID controller gains. Here to verify the superiority of the proposed controller, PID and FO-PID controllers are offered to drive a comparison with this method, which are optimized using the PSO optimization algorithm. The analysis of simulation data shows that the suggested control strategy is appropriate for maintaining stability as well as adequately compensating for disturbances and uncertainties in the inverter. Furthermore, with the help of this controller, THD level decreased lower than 2 percent.},
  archive      = {J_ASOC},
  author       = {Rasoul Kashfi and Saeed Balochian and Mohammad Alishahi},
  doi          = {10.1016/j.asoc.2024.112142},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design of a optimal robust adaptive neural network-based fractional-order PID controller for H-bridge single-phase inverter},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An alternating direction multiplier method with variable
neighborhood search for electric vehicle routing problem with time
windows and battery swapping stations. <em>ASOC</em>, <em>166</em>,
112141. (<a href="https://doi.org/10.1016/j.asoc.2024.112141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a real-world electric vehicle routing problem (EVRP). Specifically, it is an EVRP with time windows and battery swapping stations (EVRP_TWBSS). The EVRP_TWBSS considers the routing of electric vehicles (EVs), the determination of each electric vehicle’s battery level, and the selection of battery swapping stations. The criterion of EVRP_TWBSS is to minimize the operating costs. To simplify the structure of model, a time-discrete and multi-commodity flow model based on extended state-space-time network (TMFM_ESSTN) is established. Meanwhile, an alternating direction multiplier method with variable neighborhood search (ADMM_VNS) is presented to address the TMFM_ESSTN. In ADMM_VNS, the augmented lagrangian relaxation (ALR) model constructed from the TMFM_ESSTN is decomposed and linearized to a series of least cost vehicle routing subproblems through the linear augmented lagrangian relaxation (LALR) decomposed technique. Then, each subproblem is iteratively solved by using the dynamic programming and two special designed VNS strategies in ADMM_VNS iterative framework. The solution’s quality can be controlled to a certain extent through monitoring the gap between the lower and upper bounds obtained after each iteration. Test results on instances with different scales and a real-world instance based on partial road network in Kunming City demonstrate that ADMM_VNS can achieve smaller gaps and better solutions than several state-of-the-art algorithms. In which, ADMM_VNS can reduce the optimal gap by up to 2.27 % compared to the other state-of-the-art algorithms in small-scale instances. The gap of ADMM_VNS is calculated based on the lower bound and the upper bound in the large-scale instances and the real-world instance are 10.36 % and 1.57 %, respectively.},
  archive      = {J_ASOC},
  author       = {Bin Qian and Fei-Long Feng and Nai-Kang Yu and Rong Hu and Yu-Wang Chen},
  doi          = {10.1016/j.asoc.2024.112141},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An alternating direction multiplier method with variable neighborhood search for electric vehicle routing problem with time windows and battery swapping stations},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Elevating recommender systems: Cutting-edge transfer
learning and embedding solutions. <em>ASOC</em>, <em>166</em>, 112140.
(<a href="https://doi.org/10.1016/j.asoc.2024.112140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s information age and connected economy, Recommender Systems (RS) plays a vital role in managing information overload and delivering personalized suggestions to users. This paper introduces a multistage model that leverages multimodal data embedding and deep transfer learning to accurately capture user preferences and item characteristics, resulting in highly tailored recommendations. A key innovation in this model is the incorporation of an image dataset in the second phase, which addresses cold-start problems for new items by providing additional visual context. Our approach excels in overcoming challenges related to data sparsity and cold-start issues, thereby providing users with realistic and relevant product recommendations. To validate the effectiveness of the proposed model, we conducted extensive evaluations using three diverse datasets: data from Brazilian e-commerce platforms, the MovieLens 1M dataset, and the Amazon Product Review dataset. These evaluations involved comprehensive comparisons with standard RS methods to assess performance improvements. The results indicate that our proposed model significantly outperforms traditional RS techniques in terms of accuracy and reliability. Our model provides more accurate and meaningful recommendations by effectively addressing issues such as cold-start and data scarcity. Specifically, the model achieved Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) scores of 0.5883 and 0.4012, respectively, which demonstrate its superior performance metrics across all datasets tested.},
  archive      = {J_ASOC},
  author       = {Aamir Fareed and Saima Hassan and Samir Brahim Belhaouari and Zahid Halim},
  doi          = {10.1016/j.asoc.2024.112140},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Elevating recommender systems: Cutting-edge transfer learning and embedding solutions},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable weakly supervised model for multi-disease
detection and localization from thoracic x-rays. <em>ASOC</em>,
<em>166</em>, 112139. (<a
href="https://doi.org/10.1016/j.asoc.2024.112139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thoracic diseases are a major source of mortality, often requiring diagnosis from plain chest X-rays. However, differentiating between complex conditions based on subtle radiographic patterns poses challenges even for experts. Recently, deep learning methods have shown promise in automating thoracic disease detection from chest radiographs. Many existing approaches focus on the diseased organs in the radiographs by utilizing spatial regions that contribute significantly to the model’s prediction. Expert radiologists, on the other hand, first identify the prominent region before determining whether those regions are abnormal or not. Therefore, incorporating localization information through deep learning models could result in significant improvements in automatic disease classification. Motivated by this, we have proposed a generalized weakly supervised Confidence-Aware Probabilistic Class Activation Map (CAPCAM) classification model that localizes anomalies for thoracic disease. The CAPCAM used CX-Ultranet as the backbone with the combination of Confidence Aware Network (CAN) and Anomaly Detection Network (ADN) without having any localization labeling. This learning from the backbone helps the model to utilize all components of the feature extracted and, therefore eliminating the need to train them individually reducing the time taken. We have experimentally shown that the proposed CAPCAM method sets a new state-of-the-art benchmark by achieving accuracy in terms of Intersection of bounding box (IoBB) in the range of 85 % - 94 %, and Dice scores in the range of 88 %-90 % for all thirteen diseases on two publicly available large-scale CXR datasets–NIH, Stanford and CheXpert. Testing across different noise levels and different levels of blurred level assessed real-world viability. We have also added a layer of explainability to show how the image is processed. This study demonstrates deep learning’s potential to augment radiologists’ decision-making by providing fast, accurate automated aids for thoracic disease diagnosis. The proposed CAPCAM model could be readily translatable to improve clinical workflows.},
  archive      = {J_ASOC},
  author       = {Anwesh Kabiraj and Tanushree Meena and Kalyan Tadepalli and Sudipta Roy},
  doi          = {10.1016/j.asoc.2024.112139},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable weakly supervised model for multi-disease detection and localization from thoracic X-rays},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic literature review on machine learning and deep
learning-based covid-19 detection frameworks using x-ray images.
<em>ASOC</em>, <em>166</em>, 112137. (<a
href="https://doi.org/10.1016/j.asoc.2024.112137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronavirus is an endangered disease to kills more than millions of people, but it has also put tremendous pressure on the whole medical system. The initial stage of identification of COVID-19 is necessary to isolate the patients with positive cases in order to stop the disease from spreading. The amalgamation of imaging techniques and deep learning algorithms takes less time and leads to more accurate outcomes for COVID-19 detection. Deep learning techniques have been employed by scientists to identify coronavirus infection in lung images during the COVID-19 worldwide epidemic. In this review, a review of the Covid-19 detection framework based on machine learning and deep learning techniques using X-ray images is done. First, the review of existing Covid-19 detection models is done. For this purpose, a detailed literature survey is carried out on Covid-19 detection papers from 2019 to 2023. Following the literature survey, the pre-processing procedures, the segmentation process, and the classification techniques used for Covid-19 detection using deep learning, machine learning, and optimization algorithms are reviewed and categorized. After that, the dataset and the implementation tool which are utilized for Covid-19 detection works are analyzed and grouped. Finally, the performance metrics validation such as accuracy, recall, F1-score, NPV, precision, sensitivity, and specificity is carried out. The research gaps in the existing Covid-19 detection techniques are provided further as references to aid in future works.},
  archive      = {J_ASOC},
  author       = {S. Maheswari and S. Suresh and S. Ahamed Ali},
  doi          = {10.1016/j.asoc.2024.112137},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112137},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic literature review on machine learning and deep learning-based covid-19 detection frameworks using X-ray images},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel social spider algorithm based on population
mining. <em>ASOC</em>, <em>166</em>, 112136. (<a
href="https://doi.org/10.1016/j.asoc.2024.112136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social spider algorithm is a meta-heuristic algorithm inspired by the foraging behavior of biological social spiders, and is initially used to solve optimization problems with continuous functions. There exist many challenges when dealing with discrete combinatorial optimization problems. Therefore, a novel parallel social spider algorithm based on population mining is proposed. Its starting point is to mine dynamic heuristic information from the population of social spiders that cause vibrations to guide the spider’s next walk. It proposes the concept of vibration candidate sets and enhances the behavior of artificial spiders. Furthermore, it is implemented in parallel on the graphics processing unit. According to our investigation, it is the first parallel social spider algorithm. The classical traveling salesman problem is solved as an example. By solving 30 small-scale and medium-scale problems and comparing with other algorithms, it shows that its solution quality is significantly better. By solving 17 large-scale problems, compared with the parallel ant colony system and parallel iterative hill climbing algorithm on the GPU, it is found that both the solution quality and performance are also significantly better. Codes are available at https://github.com/BuptCIAGroup/P-SSA .},
  archive      = {J_ASOC},
  author       = {Zhi-bin Huang and Yi-Ming Chen and Tian-Liang Huang},
  doi          = {10.1016/j.asoc.2024.112136},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel social spider algorithm based on population mining},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new aeronautical relay health state assessment method
based on generic belief rule base with attribute reliability.
<em>ASOC</em>, <em>166</em>, 112135. (<a
href="https://doi.org/10.1016/j.asoc.2024.112135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classical rule-based modeling approach, belief rule base (BRB) expert system can integrate expert knowledge and possesses good interpretability. BRB with attribute reliability (BRB-r), built upon BRB, provides an effective way to deal with the problems of model reliability and environmental disturbances. Moreover, robustness is an important measure of perturbation resistance, and a robust BRB-r can remain reliable and stable in various environments. Therefore, to improve the model&#39;s ability to resist perturbations and enhance the model&#39;s adaptability, a new generic BRB with attribute reliability (G-BRB-r) is developed. Specifically, the robustness of BRB-r is analyzed in this paper to explore the change of BRB-r robustness under different perturbations. In addition, combining the effects of different factors on robustness, the construction criteria and constraints of robust BRB-r are given to guide modeling. Then, considering the effects of attribute reliability and robustness on modeling performance, a new generic BRB with attribute reliability is developed. Finally, the effectiveness and adaptability of the proposed method are demonstrated through a case study for health state assessment of the aerospace relay.},
  archive      = {J_ASOC},
  author       = {Xiuxian Yin and Sulong Li and Wei He and Guohui Zhou and Hongyu Li and Hailong Zhu},
  doi          = {10.1016/j.asoc.2024.112135},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112135},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new aeronautical relay health state assessment method based on generic belief rule base with attribute reliability},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting transcription factor binding sites by a
multi-modal representation learning method based on cross-attention
network. <em>ASOC</em>, <em>166</em>, 112134. (<a
href="https://doi.org/10.1016/j.asoc.2024.112134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of transcription factor binding sites (TFBS) plays a crucial role in studying cellular functions and understanding transcriptional regulatory processes. With the development of chromatin immunoprecipitation sequencing (ChIP-seq) technology, an increasing number of computer-aided TFBS prediction models have emerged. However, how to integrate multi-modal information of DNA and obtain efficient features to improve prediction accuracy remains a major challenge. Here, we propose MultiTF, a multi-modal representation learning method based on a cross-attention network for predicting transcription factor binding sites. Among TFBS prediction methods, we are the first to use graph neural networks and cross-attention networks for representation learning. MultiTF uses dna2vec to extract global contextual features of DNA sequences, DNAshapeR to extract shape features, and the CDPfold model and graph attention network for learning and representation of DNA structural features. Finally, with the help of our cross-attention module, we successfully combine sequence, structural, and shape features to achieve interactive fusion. When comparing MultiTF to other state-of-the-art methods using 165 ENCODE ChIP-seq datasets, we find that MultiTF exhibits average ACC, ROC-AUC, and PR-AUC values of 0.911, 0.978, and 0.982, respectively. The results show that MultiTF achieves unprecedented prediction accuracy compared to previous TFBS prediction models. In addition, our visual analysis of structural features provides interpretability for the prediction results.},
  archive      = {J_ASOC},
  author       = {Yuxiao Wei and Qi Zhang and Liwei Liu},
  doi          = {10.1016/j.asoc.2024.112134},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112134},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting transcription factor binding sites by a multi-modal representation learning method based on cross-attention network},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wearing myopia glasses on GANs: Mitigating bias for
pre-trained generative adversarial networks via online prior
perturbation. <em>ASOC</em>, <em>166</em>, 112133. (<a
href="https://doi.org/10.1016/j.asoc.2024.112133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained Generative Adversarial Networks (GANs) can provide rich information and make various downstream tasks beneficial. However, the training process of GANs potentially learns pre-existing biases in the historical data, which are not only inherited, but also amplified to some extent when using the generated data for prediction. Most available methods for bias mitigation utilize pre-processing or in-processing strategies to retrain GANs, which needs to access the raw data and also increases the time and computational cost required to train a fully expressive GAN. Therefore, we propose a novel post-processing mechanism to achieve fairness outputs of GANs through online prior perturbation. Unlike traditional output-based offline processing, our method mitigates biases from model inputs by designing a prior perturbation network, called “prior perturber”, to form a combined network with the pre-trained GAN. Specifically, we introduce a bias prediction network, called “bias predictor”, for online adversarial training with the prior perturber, which can decouple the generated representations from the bias features and improve the fairness of the downstream prediction results. In addition, the training process is independent of specific target labels, and the generated fair representations have good transferability. Experimental evaluations on five real datasets validate the effectiveness of the proposed method and the best utility is obtained on the same fairness level as compared to other output-based baseline methods. The result on fairness-utility trade-offs significantly improves the fairness of original samples by 56.40%, while the utility is reduced by only 2.60%.},
  archive      = {J_ASOC},
  author       = {Qiuling Chen and Ayong Ye and Guohua Ye and Chuan Huang},
  doi          = {10.1016/j.asoc.2024.112133},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112133},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wearing myopia glasses on GANs: Mitigating bias for pre-trained generative adversarial networks via online prior perturbation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation and benchmarking of research-based microgrid
systems using FWZIC-VIKOR approach for sustainable energy management.
<em>ASOC</em>, <em>166</em>, 112132. (<a
href="https://doi.org/10.1016/j.asoc.2024.112132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microgrid (MG) is one of the technologies considered in the direction of providing green and sustainable energy resources for local communities. Ensuring the best performance of these MG technologies requires extensive research to provide the most efficient system. Research-based microgrids (RB-MGs) play a vital role in the development of green energy platforms, as microgrid applications vary according to different scenarios and locations. Selecting the best research-based microgrid ensures providing local communities and stakeholders with well-tested and examined MG systems. Assessing research-based microgrid systems (RB-MG) for sustainable green applications poses a challenging multi-attribute decision-making (MADM) problem. These complexities encompass the consideration of several evaluation criteria, the relative importance of these criteria, variations in data, and the inherent trade-offs and conflicts between these factors. Crisp and definite values to evaluate the research-based microgrids could not be found despite a comprehensive investigation. In this regard, appraisals and opinions of experts and professionals in providing sustainable energy with vast knowledge and experience in assessment, selection, installation, and operation were addressed as data. A novel decision-making model was developed to evaluate and select the most proper RB-MG system by processing these data. This study proposes an integrated MADM modelling approach using Fuzzy Weighted with Zero Inconsistency (FWZIC) method in conjunction with the Vlse-kriterijumska Optimizcija I Kaompromisno Resenje (VIKOR) method. The underlying process starts with constructing a decision matrix (evaluation criteria intersectioned with RB-MGs). Then, evaluation criteria are weighted using FWZIC, and the RB-MGs are ranked for each category using VIKOR. The results derived from FWZIC weights provide valuable insights. Key criteria such as &#39;installed power (KW)&#39; and &#39;storage capacity (C3)&#39; show notable values of 0.159 and 0.151, respectively, underscoring their importance in identifying optimal RB-MGs. These weights and alternatives were used to rank the highest RB-MG, which is LIER-CIRCE of the average PV power group, and Ormazabal from the &#39;Highest PV Power&#39; group. Ormazabal obtained the lowest Qi (0.426). For the average PV power group, alternative number 7 (Atenea Centre) ranked as the best alternative with the lowest Qi among other RB-MGs in the same group (0.158107). Comparative assessments with various MCDM methods reveal strong correlations with TOPSIS and MABAC, but negative correlations with MAIRA. Additionally, there is a statistical difference in grouping by PV installed power (KW) or MPC.},
  archive      = {J_ASOC},
  author       = {Mohammed Talal and Michael Loong Peng Tan and Dragan Pamucar and Dursun Delen and Witold Pedrycz and Vladimir Simic},
  doi          = {10.1016/j.asoc.2024.112132},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112132},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation and benchmarking of research-based microgrid systems using FWZIC-VIKOR approach for sustainable energy management},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven planning in socially responsible textile units
amidst uncertainty. <em>ASOC</em>, <em>166</em>, 112131. (<a
href="https://doi.org/10.1016/j.asoc.2024.112131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social responsibility is a key factor for organizations to achieve sustainable success in the modern competitive market. This study proposes a hybrid VIKOR method to evaluate textile suppliers based on their social performance under uncertain and multi-objective conditions. The method can handle fuzzy, stochastic, and interval data simultaneously. The social criteria for the evaluation are derived from the literature review, the SA8000 standards, and the United Nations’ recommendations. Some of the criteria are also aligned with the World Bank’s Social Responsibility Diamond Model and the United Nations’ Sustainable Development Goals. Moreover, this study presents a fuzzy mathematical model for fabric purchasing that incorporates social criteria and the quality level into the optimization process. A goal programming method is developed based on the mathematical properties of the multi-objective model. A numerical study is conducted in the textile industry to demonstrate the efficiency and effectiveness of the proposed approaches. A comprehensive sensitivity analysis has been performed to investigate the behavior of the presented mathematical model under different conditions, and the results have been discussed concerning the insights for managers and stakeholders in the textile industry. The proposed model demonstrates that: 1) Customer demand and fabric orders have a direct relationship with increasing sales. 2) The fabric unit price has a direct impact on the quality value and requires cost control policies or pricing negotiations with suppliers. 3) Improving supplier and customer relations and formulating pricing consistent with social value are among the most important issues for the success of the textile and clothing industry. The best-fitting line successfully explains the variability of social performance and customer demand with an accuracy of 99.35 %.},
  archive      = {J_ASOC},
  author       = {R. Ghasemy Yaghin and Masoomeh Toorani},
  doi          = {10.1016/j.asoc.2024.112131},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112131},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven planning in socially responsible textile units amidst uncertainty},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Modeling consistency and consensus in social network group
decision making: The role of limited dual tolerance and compromise
behaviors. <em>ASOC</em>, <em>166</em>, 112130. (<a
href="https://doi.org/10.1016/j.asoc.2024.112130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In interactive social network group decision making (SNGDM), decision-makers frequently consider both their own preferences and the preferences of trusted members when adjusting preferences. This dual consideration results in the formation of limited dual tolerance and compromise behaviors, as reflected in the cost and willingness of decision-makers to adjust their preferences. This paper investigates the role of limited dual tolerance and compromise behaviors in the consistency and consensus management in SNGDM with additive preference relations. First, this paper introduces the concepts of limited dual tolerance and compromise behaviors. Then, this paper presents an optimization-based model, tailored to manage consistency and consensus in the interactive SNGDM. This model takes into account the two behaviors mentioned, with the goal of assisting decision-makers in adjusting their preferences to achieve consistency and consensus while minimizing associated costs. Given that limited dual compromise behavior may impede decision-makers from achieving consistency and consensus, this paper designs a model to help decision-makers adjust their dual compromise values. Further, this paper develops an interactive consistency and consensus management process with limited dual tolerance and compromise behaviors. Finally, an illustrative example involving sustainable supplier selection is applied, along with the simulation analysis and a discussion, to validate the effectiveness of the proposed framework.},
  archive      = {J_ASOC},
  author       = {Hengjie Zhang and Jingye Wang and Wenfeng Zhu},
  doi          = {10.1016/j.asoc.2024.112130},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112130},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling consistency and consensus in social network group decision making: The role of limited dual tolerance and compromise behaviors},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A constrained multi-objective evolutionary algorithm based
on fitness landscape indicator. <em>ASOC</em>, <em>166</em>, 112128. (<a
href="https://doi.org/10.1016/j.asoc.2024.112128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constrained multi-objective optimization problems (CMOPs) with constraints in both the decision and objective space are shown to be great challenges to be solved. Considering the different requirements of different problems on resource allocation of exploration and exploitation, this paper proposes a new constrained multi-objective evolutionary algorithm based on a novel fitness landscape indicator. The indicator regards the fitness landscape and evolutionary generation among the population to determine the selection of the offspring generation mechanism. The proposed algorithm uses the new indicator to select different differential evolutions during the evolutionary process to balance exploration and exploitation. Numerical experiments on three test suites and three practical examples compared with six existing algorithms show the proposed algorithm can effectively deal with different types of CMOPs, especially in CMOPs with constraints in both the decision and objective spaces.},
  archive      = {J_ASOC},
  author       = {Jingjing Fang and Hai-Lin Liu and Fangqing Gu},
  doi          = {10.1016/j.asoc.2024.112128},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112128},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A constrained multi-objective evolutionary algorithm based on fitness landscape indicator},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint lightly perturbation and synthesizing optimization
attack framework for point cloud perturbation. <em>ASOC</em>,
<em>166</em>, 112125. (<a
href="https://doi.org/10.1016/j.asoc.2024.112125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent characteristic of deep learning models, prior researchers have focused on studying adversarial strategies aimed at uncovering their vulnerabilities. However, these strategies have been primarily been confined to the realm of image processing. When considering 3D vision structures, such as point clouds and meshes, the robustness of learning models also diminishes significantly in the presence of discernible outliers, and these strategies often fail to generalize across different DNN architectures. In this paper, a simple adversarial perturbation and optimization attack framework, Tangent Plane Perturbation and Synthesizing Optimization Attack (TPSOA) is presented. TPSOA provides a point cloud projection direction for point clouds, subtly inducing imperceptible perturbations on their tangent planes. Furthermore, acknowledging the geometry form of adversarial generation and its potential for 3D printing in physical scenarios, we reconstruct the adversarial mesh alongside the adversarial point cloud. A synthesized loss, incorporating both the Sobolev loss and the Chamfer loss, is utilized to optimize the distances between the adversarial generation and ground truth. Through a combination of visualization and quantitative analysis experiments, we validate the imperceptible nature of TPSOA, the necessity of the loss terms, and the magnitude of perturbations. These experiments that TPSOA enables targeted point cloud attacks that are imperceptible and of minor magnitude. In real-world adversarial attack and defense scenarios, as evidenced by both white-box and black-box experiments, the results show that TPSOA surpasses other state-of-the-art adversarial attack models, effectively manipulating a greater number of victim models while maintaining high efficiency, all without compromising its original structural integrity.},
  archive      = {J_ASOC},
  author       = {Ruihan Hu and Zhiri Tang and Rui Yang},
  doi          = {10.1016/j.asoc.2024.112125},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112125},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A joint lightly perturbation and synthesizing optimization attack framework for point cloud perturbation},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiobjective optimizer with a k-means cluster algorithm
for a distributed flexible flowshop rescheduling problem. <em>ASOC</em>,
<em>166</em>, 112124. (<a
href="https://doi.org/10.1016/j.asoc.2024.112124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A distributed flexible flowshop problem (DFFSP) has been extensively studied over recent years. It is assumed that all jobs to be processed are exactly known in advance, and machines are able to work continuously. In practice, however, new jobs often arrive suddenly. Machines sometimes break down unexpectedly. These lower the performance of the scheduling generated, even make it infeasible. To address this problem, this paper considers a rescheduling DFFSP (DFFRP) with new job arrivals and machine breakdowns. The objective is to minimize makespan and the robustness metrics at the same time. Firstly, we propose a multi-objective mixed-integer linear programming model and a non-dominated sorting genetic algorithm-II based on K-means clustering algorithm (KNSGA-II). Secondly, the problem-specific knowledge is explored and a speed-up strategy is designed to save the algorithmic computation. Thirdly, an initialization strategy based on K-means clustering algorithm is developed to generate high-quality initial solutions. And a novel crossover and mutation operator is employed to accelerate the convergence of the algorithm. Finally, by comparing with a number of advanced multi-objective algorithms in the literature in comprehensive experiments, the proposed algorithm has been demonstrated to be much more effective for solving the DFFRP under consideration.},
  archive      = {J_ASOC},
  author       = {Xin-Rui Tao and Quan-Ke Pan and Hong-Yan Sang and Miao Rong},
  doi          = {10.1016/j.asoc.2024.112124},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112124},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multiobjective optimizer with a K-means cluster algorithm for a distributed flexible flowshop rescheduling problem},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Piecewise combinatorial particle swarm optimization.
<em>ASOC</em>, <em>166</em>, 112060. (<a
href="https://doi.org/10.1016/j.asoc.2024.112060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The particle swarm optimization (PSO) is easy to fall into local optima in the search process, and usually the optimum stops updating at the later stage of the iteration, especially for complex problems. In this paper, a novel piecewise combinatorial particle swarm optimization is proposed, where a new piecewise combinatorial strategy and a new exploitation space search strategy are designed. The piecewise combinatorial strategy increases population diversity through copying, crossover, and mutation operations. The exploitation space search strategy improves particles search speed by setting the exploitation particle. This algorithm is tested and compared with 11 popular PSO variants on 30 benchmark functions. The results show that the performance of this algorithm is better than many existing PSO variants, especially for complex optimization problems. In addition, the piecewise combinatorial strategy provides a new idea for integrating excellent algorithms with different focuses. In the end, the algorithm is applied to a traveling salesman problem, and it is found that the algorithm exhibits excellent performance.},
  archive      = {J_ASOC},
  author       = {Hongbo Hu and Yi Yang},
  doi          = {10.1016/j.asoc.2024.112060},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112060},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Piecewise combinatorial particle swarm optimization},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A genetic algorithm method for improving suboptimal sensor
arrangements in coverage and connectivity problems. <em>ASOC</em>,
<em>166</em>, 112047. (<a
href="https://doi.org/10.1016/j.asoc.2024.112047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the solution of a Coverage and Connectivity Problem (CCP). The proposed method performs the placement of sensors forming a connected Wireless Sensor Network (WSN) to fully cover a Field of Interest (FoI). Full coverage is attained despite the presence of opaque obstacles that impair both sensing and communication. To this end, we leverage set operations involving polygons to tessellate the free space. Moreover, we propose the concept of C R CR -visibility to assess the total area coverage of a polygon by a sensor. The deployment of the minimal number of sensors to completely cover the FoI is cast into the form of an Integer Linear Program (ILP). Two different formulations of connectivity constraints are appended to the ILP. The first one is necessary and sufficient for connectivity, whereas the second alternative is only sufficient. While the number of inequalities in the former grows combinatorially with the number of sensors, the growth is linear in the latter, rendering it more computationally appealing for real-sized FoI. Lastly, we formulate an unconstrained version of the CCP, which is solved by a Genetic Algorithm (GA) with integer variables. We present a small simulation scenario for initial illustration of the proposed method, and a larger, real-life scenario based on the map of an actual urban setting. The results obtained with the real-life scenario show that: (i) high-quality solutions can be obtained in short computation times by imposing the sufficient constraints for connectivity; (ii) the large number of inequalities associated to the necessary and sufficient constraints render the numerical solution impractical; (iii) by using random initialization, the GA solution of the unconstrained problem requires more sensors than the ILP solution with the sufficient connectivity constraint; (iv) including that ILP solution in the initial population of the GA enables it to find a sensor placement that requires fewer sensors.},
  archive      = {J_ASOC},
  author       = {Pedro A.Q. de Assis and Leonardo R. Rodrigues and Roberto K.H. Galvão and Rubens J.M. Afonso},
  doi          = {10.1016/j.asoc.2024.112047},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112047},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic algorithm method for improving suboptimal sensor arrangements in coverage and connectivity problems},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated regressive learning: Adaptive weight updates
through statistical information of clients. <em>ASOC</em>, <em>166</em>,
112043. (<a href="https://doi.org/10.1016/j.asoc.2024.112043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is a method for training models in a distributed environment where each client utilizes its local dataset to train a model and shares it with a server to create a global model. This approach offers various advantages, such as eliminating the need for local clients to directly transfer their data to the server. However, it also has limitations, including the potential for limited global model performance due to imbalanced local datasets. In this study, we propose a new model that evaluates the quality of each client’s local dataset based on its probability distribution and quantity. Based on this evaluation, the contribution of locally trained models to the global model is adjusted accordingly. Unlike previous studies, our proposed method addresses the issue of low-quality local models degrading the performance of the global model. This allows for the improvement of the global model’s performance without the need for complex processes, such as adding specially designed loss functions to local models or employing data augmentation techniques to enhance the quality of the dataset. Furthermore, the method of calculating the contribution of local models proposed in this study allows for the quantitative assessment of the performance of clients participating in federated learning based on their data collection capabilities, making it an industry-friendly approach. We conducted experiments using various benchmark datasets, simulating different scenarios that could occur in the real world. In most scenarios, except a few, our method showed superior performance on all evaluation metrics compared to existing studies reviewed in the paper.},
  archive      = {J_ASOC},
  author       = {Dong Seok Kim and Shabir Ahmad and Taeg Keun Whangbo},
  doi          = {10.1016/j.asoc.2024.112043},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated regressive learning: Adaptive weight updates through statistical information of clients},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning enabled hotel customer classification
towards imbalanced data. <em>ASOC</em>, <em>166</em>, 112028. (<a
href="https://doi.org/10.1016/j.asoc.2024.112028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hotel customer classification is the basis of customer profiling, which can significantly benefit a hotel by providing more appropriate services for targeted customers. However, imbalanced data distribution from individual hotels cannot support a reliable classification result, and sharing personal information among hotels is not allowed. In this paper, we propose to achieve a privacy-preserved hotel customer classification model via federated learning. A significant challenge is that hotels with different star ratings or distributed in different city regions usually serve specific customer groups, resulting in imbalanced data that degrade classification accuracy. We introduce an attention mechanism and design a client selection strategy to balance global and local performance upon imbalanced data. Due to privacy issues, we evaluate our solution’s communication cost and accuracy on public imbalanced datasets and demonstrate the real-world customer classification results. Extensive experiments show that our solution performs better than the COTA method.},
  archive      = {J_ASOC},
  author       = {Tao Liu and Shouqiang Chen and Meng Wu and Miao Yu},
  doi          = {10.1016/j.asoc.2024.112028},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated learning enabled hotel customer classification towards imbalanced data},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on the spacecraft ground equivalence test
assessment problem: A comprehensive assessment method combining
interval-type evaluation and prospect-two-dimensional cloud.
<em>ASOC</em>, <em>166</em>, 111882. (<a
href="https://doi.org/10.1016/j.asoc.2024.111882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Wenzhe Ding and Xiang Bai and Qingwei Wang and Huisheng Yao and Jian Liu and Hong Yang},
  doi          = {10.1016/j.asoc.2024.111882},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {111882},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research on the spacecraft ground equivalence test assessment problem: A comprehensive assessment method combining interval-type evaluation and prospect-two-dimensional cloud},
  volume       = {166},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning trustworthy model from noisy labels based on rough
set for surface defect detection. <em>ASOC</em>, <em>165</em>, 112138.
(<a href="https://doi.org/10.1016/j.asoc.2024.112138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In surface defect detection, some regions remain ambiguous and cannot be distinctly classified as abnormal or normal. This challenge is exacerbated by subjective factors, including workers’ emotional fluctuations and judgment variability, resulting in noisy labels that lead to false positives and missed detections. Current methods depend on additional labels, such as clean and multi-labels, which are both time-consuming and labor-intensive. To address this, we utilize Rough Set theory and Bayesian neural networks to learn a trustworthy model from noisy labels for Surface Defect Detection. Our approach features a novel pixel-level representation of suspicious areas using lower and upper approximations, and a novel loss function that emphasizes both precision and recall. The Pluggable Spatially Bayesian Module (PSBM) we developed enhances probabilistic segmentation, effectively capturing uncertainty without requiring extra labels or architectural modifications. Additionally, we have devised a ‘defect discrimination confidence’ metric to better quantify uncertainty and assist in product quality grading. Without the need for extra labeling, our method significantly outperforms state-of-the-art techniques across three types of datasets and enhances seven types of classic networks as a pluggable module, without compromising real-time computing performance. For further details and implementation, our code is accessible at https://github.com/ntongzhi/RoughSet-BNNs .},
  archive      = {J_ASOC},
  author       = {Tongzhi Niu and Zhenrong Wang and Weifeng Li and Kai Li and Yuwei Li and Guiyin Xu and Bin Li},
  doi          = {10.1016/j.asoc.2024.112138},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112138},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning trustworthy model from noisy labels based on rough set for surface defect detection},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimized task scheduling approach with fault tolerant load
balancing using multi-objective cat swarm optimization for multi-cloud
environment. <em>ASOC</em>, <em>165</em>, 112129. (<a
href="https://doi.org/10.1016/j.asoc.2024.112129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-cloud environment enables an organization to access services from more than one cloud service providers the use of multiple cloud computing and it can be treated as single heterogeneous environment. It enables autonomy to run the tasks on private or public cloud based on business or technical requirements. In a multi-cloud platform, load balancing is an essential task to serve the requests from multiple users with different resources effectively. It helps to improve utilization of the cloud resources, throughput, reduce makespan and avoid overload at resources. Load balancing also facilitates the redirection of traffic to resources running in another cloud when a failure occurs in a cloud. Hence, it is more vital to have optimized load balancing methods in multi-cloud infrastructure in order to improve the system performance. This paper presents an optimized fault tolerant load balancing method using multi-objective cat swarm optimization algorithm called MCSOFLB and the results are then compared against other powerful optimization algorithms. The experimental results evidently show that the proposed algorithm ranks first on the whole. The MCSOFLB method produces an average improvement of 31 % makespan, 6 % resource utilization, 12 % cost, 6 % success rate and 32 % average throughput over other benchmark algorithms.},
  archive      = {J_ASOC},
  author       = {P. Suresh and P. Keerthika and R. Manjula Devi and G.K. Kamalam and K. Logeswaran and Kishor Kumar Sadasivuni and K. Devendran},
  doi          = {10.1016/j.asoc.2024.112129},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112129},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimized task scheduling approach with fault tolerant load balancing using multi-objective cat swarm optimization for multi-cloud environment},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential three-way decision with automatic threshold
learning for credit risk prediction. <em>ASOC</em>, <em>165</em>,
112127. (<a href="https://doi.org/10.1016/j.asoc.2024.112127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms treat credit risk prediction as a binary classification problem. However, two-way decisions with a single threshold force to make either a default or non-default decision may be inappropriate. To reduce the risk of decision errors, this study introduces three-way decisions and proposes a sequential three-way decision model with automatic threshold learning to evaluate credit risk. Initially, the model uses the loan amount and interest to determine the decision loss of the three-way decision, assigning distinct decision thresholds to different samples. Subsequently, the model employs decision cost and information gain to formulate an objective for threshold optimisation. Finally, the model continuously optimises the classification process by using the outcomes of certain decisions as supplementary information. In addition, to validate our model, we conduct comparative experiments with various methods on a real credit dataset from a Chinese bank. The results indicate that the model not only enhances classification performance across several metrics but also assists financial institutions in reducing decision error costs.},
  archive      = {J_ASOC},
  author       = {Yusheng Li and Feng Gao and Mengyi Sha and Xueyan Shao},
  doi          = {10.1016/j.asoc.2024.112127},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112127},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sequential three-way decision with automatic threshold learning for credit risk prediction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-granularity spatial temporal graph convolution network
with consecutive attention for human motion prediction. <em>ASOC</em>,
<em>165</em>, 112126. (<a
href="https://doi.org/10.1016/j.asoc.2024.112126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion prediction is attracting increasing attention for its numerous potential applications in fields including autonomous driving, video surveillance and virtual reality. However, accurate motion prediction is challenging due to the complex spatial dependencies, dynamic temporal correlations and high dimension of human pose sequences. Existing graph-based methods rarely consider positional and channel information of the feature map, resulting in lower prediction accuracy. Therefore, we propose a novel multi-granularity spatial temporal graph convolution network with consecutive attention (MSTCA) for human motion prediction. Firstly, a multi-granularity spatial convolution network is introduced to capture spatial joint features through multiple kernel sizes. Then, consecutive attention module is proposed to capture both positional and channel information of the feature map. Next, MSTCA uses multi-granularity temporal convolutional network to extract temporal correlations with multiple receptive fields and predict future poses. Finally, a decoder composed of a 2D convolution layer and several PRelu layers integrates the output of the whole model. Experimental results on the GTA-IM and PROX datasets demonstrate that our method significantly improves the accuracy of human motion prediction in comparison to the existing approaches.},
  archive      = {J_ASOC},
  author       = {Jinli Ma and Yumei Zhang and Hanghang Zhou and Honghong Yang and Xiaojun Wu},
  doi          = {10.1016/j.asoc.2024.112126},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112126},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-granularity spatial temporal graph convolution network with consecutive attention for human motion prediction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization of roofs with solar panels using rao
algorithms. <em>ASOC</em>, <em>165</em>, 112123. (<a
href="https://doi.org/10.1016/j.asoc.2024.112123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steel space truss roof (SSTR) systems are widely used in structures with large spans such as stadiums, sports halls, and shopping malls, as well as in industrial buildings such as factories, workshops, and production facilities. Solar panels are integrated into SSTR systems to enable these structures to generate energy and to increase the use of sustainable energy sources. With the installation of the solar panels, an additional load was placed on the roofs. However, this raises questions regarding whether existing roofs can withstand this additional weight without compromising their design. This study can be an important reference source for future truss roof designs by increasing sustainable energy use and functionally optimizing the SSTR. Another contribution of this study is that it directly contributes to real-world applications by ensuring that SSTR designs are more efficient and economical for engineering projects. For these purposes, this study presents the size optimization of the SSTR with and without solar panels using the Rao-1 and Rao-2 algorithms, which are metaheuristic algorithms known as the Rao algorithms. Thus, information can be provided regarding whether existing roofs can safely carry solar panels. To optimize the SSTR system, a computer code was created in MATLAB, which works effectively with Rao algorithms and SAP2000-s Open Applicable Programming Interface (OAPI) features and allows repetitive analysis. For size optimization of the SSTR, which consists of 1728 elements, the roof system was divided into three and six groups. Changes in the weight of the SSTR system in the different groups were investigated. The optimum design of both the three-group and six-group SSTR systems with and without solar panels was performed. Based on the results obtained from this study, it was concluded that the Rao-1 algorithm achieved more robust and stable results than the Rao-2 algorithm in both three and six-group SSTR. The SSTR system divided into six groups achieved the minimum weight compared with the SSTR system, which was divided into three groups. In this case, increasing the number of groups provided better results.},
  archive      = {J_ASOC},
  author       = {Büşra Yakak and Barbaros Atmaca and Nur Sena Kınalı and Tayfun Dede and Maksym Grzywinski and Ravipudi Venkata Rao},
  doi          = {10.1016/j.asoc.2024.112123},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112123},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization of roofs with solar panels using rao algorithms},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A greedy randomized adaptive search procedure (GRASP) for
minimum 2-fold connected dominating set problem. <em>ASOC</em>,
<em>165</em>, 112122. (<a
href="https://doi.org/10.1016/j.asoc.2024.112122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The minimum 2-fold connected dominating set problem (M(2-fold)CDSP) is an important variant of the traditional minimum connected dominating set problem (MCDSP), with crucial applications in fields like wireless networks. Therefore, the main research purpose of this work is to design efficient algorithms for this problem in order to provide theoretical support for relevant downstream task applications. In this work, a 0-1 integer linear programming (ILP) model based on tree data structure is given firstly. Then, a greedy randomized adaptive search procedure (GRASP) framework is proposed, which consists of two procedures: construction and improvement. Based on this scheme, a two-stage initial solution construction procedure with the fusion of greedy and randomized strategies is designed, and two iterative local search procedures to improve the initial solution are proposed, resulting in our so-called GRASP and GRASP_MAR. First, in GRASP, a new vertex scoring function named Mscore , which integrates a variety of dominant cases, is designed to provide the basis of vertex movement. Second, in GRASP_MAR, two novel strategies to avoid the cyclic problem and improve performance are developed. The first strategy is random configuration checking (RCC), which is designed by improving the traditional configuration checking strategy. The main idea of RCC strategy is that using a simple parameter to control the tabu probability. The second strategy is multi-criteria search (MCS) strategy, which simultaneously considering connectivity and dominance in the movement of vertices. Experimental results show that our GRASPs, especially the GRASP_MAR, outperform all competitors, including the famous ILP solver CPLEX and three other state-of-the-art heuristic algorithms. Our proposed algorithms can efficiently and firmly deliver high-quality solutions for the M(2-fold)CDSP.},
  archive      = {J_ASOC},
  author       = {Xiaolin Nie and Quanli Zhang and Yixin Qiao and Zijun Qi and Lilin Zhang and Dangdang Niu and Hongming Zhang},
  doi          = {10.1016/j.asoc.2024.112122},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112122},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A greedy randomized adaptive search procedure (GRASP) for minimum 2-fold connected dominating set problem},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-objective and multi-objective mixed-variable grey
wolf optimizer for joint feature selection and classifier parameter
tuning. <em>ASOC</em>, <em>165</em>, 112121. (<a
href="https://doi.org/10.1016/j.asoc.2024.112121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection plays an essential role in data preprocessing, which can extract valuable information from extensive data, thereby enhancing the performance of machine learning classification. However, existing feature selection methods primarily focus on selecting feature subsets without considering the impact of classifier parameters on the optimal subset. Different from these works, this paper considers jointly optimizing the feature subset and classifier parameters to minimize the number of features and achieve a low classification error rate. Since feature selection is an optimization problem with binary solution space while classifier parameters involve both continuous and discrete variables, our formulated problem becomes a complex multi-objective mixed-variable problem. To address this challenge, we consider a single-objective optimization method and a multi-objective optimization approach. Specifically, in the single-objective optimization method, we adopt the linear weight method to convert our multiple objectives into a fitness function and then propose a mixed-variable grey wolf optimizer (MGWO) to optimize the function. The proposed MGWO introduces Chaos-Faure initialization, Log convergence factor adjustment, and optimal solution adaptive update operators to enhance its adaptability and balance the global and local search of the algorithm. Subsequently, an improved multi-objective grey wolf optimizer (IMOGWO) is introduced to directly address the problem. The proposed IMOGWO introduces improved initialization, local search, and binary variable mutation operators to balance its exploration and exploitation abilities, making it more suitable for our mixed-variable problem. Extensive simulation results show that our MGWO and IMOGWO outperform recent and classic baselines. Moreover, we also find that jointly optimizing classifier parameters can significantly improve classification accuracy.},
  archive      = {J_ASOC},
  author       = {Hongjuan Li and Hui Kang and Jiahui Li and Yanyun Pang and Geng Sun and Shuang Liang},
  doi          = {10.1016/j.asoc.2024.112121},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112121},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Single-objective and multi-objective mixed-variable grey wolf optimizer for joint feature selection and classifier parameter tuning},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adjusted SpikeProp algorithm for recurrent spiking neural
networks with LIF neurons. <em>ASOC</em>, <em>165</em>, 112120. (<a
href="https://doi.org/10.1016/j.asoc.2024.112120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A problem related to the development of a supervised learning method for recurrent spiking neural networks is addressed in the paper. The widely used Leaky-Integrate-and-Fire model has been adopted as a spike neuron model. The proposed method is based on a known SpikeProp algorithm. In detail, the developed method enables gradient descent learning of recurrent or multi-layer feedforward spiking neural networks. The research included an extended verification study for the classical XOR classification problem. In addition, the developed learning method has been used to provide a spiking neural black-box model of fast processes occurring in a pressurised water nuclear reactor. The obtained simulation results demonstrate satisfactory effectiveness of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Krzysztof Laddach and Rafał Łangowski},
  doi          = {10.1016/j.asoc.2024.112120},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112120},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adjusted SpikeProp algorithm for recurrent spiking neural networks with LIF neurons},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Object detection with a dynamic interactive network based
on relational graph routing. <em>ASOC</em>, <em>165</em>, 112119. (<a
href="https://doi.org/10.1016/j.asoc.2024.112119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combinatorial relational reasoning in neural networks used for object detection is usually static; therefore, it cannot selectively fuse visual information and semantic relations, which limits their performance. To address this problem, we propose a relational graph routing network (RGRN) that enables the dynamic interaction of visual and semantic features. The network consists of a dynamic graph network, dual path-sharing module, and relational routing interaction module. First, we used a data-driven technique to obtain the semantic information between tags from the dataset. Rich semantic information was obtained by calculating the similarity between tags. Second, the two types of semantic information were fused using a dynamic graph network to capture high-level semantic information. The visual and semantic features are then filtered and encoded through the dual path-sharing module to obtain enhanced visual and semantic features. Finally, three units were used to dynamically fuse visual and semantic information in the relational routing interaction module, which densely links the three units and routers to construct a routing space that can autonomously decide on the optimal fusion path through model learning. A series of experiments was conducted on the MS COCO dataset. RGRN achieved 54.7% box AP on object detection, which was 2.8% box AP higher than that of the Cascade Mask R-CNN. The experimental results show that the routing space enables better interaction between visual and semantic information. Therefore, our method can achieve better performance than many state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Xiwei Yang and Zhixin Li and Wenlan Kuang and Canlong Zhang and Huifang Ma},
  doi          = {10.1016/j.asoc.2024.112119},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Object detection with a dynamic interactive network based on relational graph routing},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lightweight intrusion detection model based on CNN and
knowledge distillation. <em>ASOC</em>, <em>165</em>, 112118. (<a
href="https://doi.org/10.1016/j.asoc.2024.112118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of network attacks is a primary focus in the domain of intrusion detection. Models face significant challenges in recognizing intrusion behaviors, particularly when dealing with high-dimensional and sparse datasets. Traditional machine learning methods often struggle with these dimensionality issues. In contrast, deep learning, a crucial technology in intrusion detection, excels at managing high-dimensional data. However, traditional image coding methods do not adequately address data sparsification and often overlook the spatial continuity among features. The Fourier transform is a promising solution for data sparsity issues, as it effectively mitigates the impact by converting data into a different domain. Inspired by the Fourier transform, this paper proposes a lightweight intrusion detection model called TFTKD, based on Convolutional Neural Networks (CNN) and knowledge distillation. The model applies a two-dimensional Fourier transform to convert grayscale images from the time domain to the frequency domain. This transformation enhances the similarity between neighboring pixels, effectively addressing data sparsification. During the training phase, a teacher network, comprising an 8-layer CNN, is pre-trained. In the distillation phase, a one-layer CNN serves as the student network, employing Self-adaptive Temperature Knowledge Distillation to enhance the student&#39;s generalization capabilities. This approach results in a compact student network model with a constrained parameter count, demonstrating superior learning efficiency and accuracy compared to state-of-the-art methods. Experimental validation was conducted using four publicly accessible intrusion detection datasets, demonstrating the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Long-Hui Wang and Qi Dai and Tony Du and Li-fang Chen},
  doi          = {10.1016/j.asoc.2024.112118},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lightweight intrusion detection model based on CNN and knowledge distillation},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boundary points guided 3D object detection for point clouds.
<em>ASOC</em>, <em>165</em>, 112117. (<a
href="https://doi.org/10.1016/j.asoc.2024.112117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection in LiDAR point clouds is crucial for computer vision tasks such as autonomous driving. The two-stage approach with point cloud completion achieves remarkable performance by generating semantic surface points for foreground objects or learning bird’s eye view shape heat map labels. However, these methods require additional completion datasets, leading to substantial computation and memory demands. In this context, we propose a Boundary Points Guided 3D (BPG3D) object detection method that complements point cloud boundary information without the need for additional data. Specifically, we generate Region of Interest (RoI) boundary points to aggregate the neighbor voxel information at the RoI boundary during the refinement stage to complement the missing boundary information. Meanwhile, we design a Dual Feature Selection (DFS) module to adaptively fuse RoI grid point features and RoI boundary point features for bounding box refinement with negligible computational cost. Additionally, inspired by tensor decomposition theory, we use low-rank tensors to reconstruct high-rank tensors in the point cloud feature encoder to enhance contextual semantic information. The proposed method achieves 65.81% mAP on KITTI Test Set, obtaining a good trade-off between accuracy and efficiency.},
  archive      = {J_ASOC},
  author       = {Qingsong Tang and Mingzhi Yang and Ziyi Wang and Wenhao Dong and Yang Liu},
  doi          = {10.1016/j.asoc.2024.112117},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112117},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Boundary points guided 3D object detection for point clouds},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Separating the predictable part of returns with
CNN-GRU-attention from inputs to predict stock returns. <em>ASOC</em>,
<em>165</em>, 112116. (<a
href="https://doi.org/10.1016/j.asoc.2024.112116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The noise and high randomness of the stock market are primary obstacles to profitability. These factors cause stock returns to consist of short-term predictable and stock-specific residual parts. Therefore, it is beneficial to separate and model the two items for forecasting returns in the randomness environment. On this basis, we propose a novel Temporal Return Separation and Restore Forecasting (TRSRF) method. TRSRF first extracts price- and relation-based features from historical stock inputs and uses them to decompose and restore predictable and residual elements of historical returns , respectively. Then it forecasts these components based on historical values. We conducted experiments on the Chinese small and mid-cap, blue-chip, and large-cap indexes over ten years to compare with seven proposed baselines. On the three Chinese indexes our method separately exceeds 6%, 16%, and 22% absolute annualized returns than our baseline. The ablation study also proves the necessity of return decomposition and inter-stock relation modeling. We find that modeling inter-stock relations on small-cap stocks works better than on large-cap stocks. Furthermore, we prove our method might be an implicit ensemble model of the predictable and residual parts, which is why it works stability under various market conditions.},
  archive      = {J_ASOC},
  author       = {Jiahao Yang and Ming Zhang and Ran Fang and Wenkai Zhang and Jun Zhou},
  doi          = {10.1016/j.asoc.2024.112116},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112116},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Separating the predictable part of returns with CNN-GRU-attention from inputs to predict stock returns},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PILL: Plug into LLM with adapter expert and attention gate.
<em>ASOC</em>, <em>165</em>, 112115. (<a
href="https://doi.org/10.1016/j.asoc.2024.112115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the remarkable capabilities of powerful Large Language Models (LLMs) in effectively following instructions, significant progress has recently been made in the development of Vision Language Models (VLMs), expanding the capabilities of LLMs in multi-modal learning and enabling them to be fine-tuned in a parameter-efficient manner. Promising as the existing pre-trained vision-language models might be, there are two intractable issues including modal entanglement and insufficient capability of LLMs in handling visual information. To tackle these two issues, we introduce an architecture called PILL which P lugs I nto LL M with adapter expert and attention gate. To maintain the capability of the original LLMs and the more progressive injection of visual modality, one Modality-Attention-Gating (MAG) module is introduced, enabling adaptive control of the contribution of modality tokens to the overall representation. Specifically, one Mixture-of-Modality-Adapter-Expert (MoMAE) module is proposed to handle different modalities with the dedicated adapters. In addition, further improvement is made to the adapter to enhance its learning and expressive capabilities. We adopt a two-stage training paradigm to optimize different modules of our model. Experimental results demonstrate that our approach exhibits competitive performance compared to other mainstream methods for modality fusion with much lower resources. We provide free access to the code and models. 2},
  archive      = {J_ASOC},
  author       = {Yuyu Yin and Fangyuan Zhang and Zhengyuan Wu and Qibo Qiu and Tingting Liang and Xin Zhang},
  doi          = {10.1016/j.asoc.2024.112115},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PILL: Plug into LLM with adapter expert and attention gate},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Soft computing-driven infrared and visible image fusion
network for security application service. <em>ASOC</em>, <em>165</em>,
112114. (<a href="https://doi.org/10.1016/j.asoc.2024.112114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-level visual tasks (HLVT)-specific infrared and visible image fusion networks aim to guide the fusion network to focus on specific feature regions. The resulting fused images enhance the performance of HLVT in adverse conditions. However, cross-layer visual task fusion may diminish the fusion network’s adaptability to diverse data. This deficiency results in a lack of robustness and security in the fusion network. Consequently, the fusion network is rendered insufficient to fulfill the requirements of HLVT security applications, including object detection and security systems. To address these challenges, we propose a soft computing-driven infrared and visible image fusion network based on layer separation and dual-stream adversarial learning, referred to as LsDSGFuse. Firstly, we employ a layer segmentation network based on a high-resolution network to create salient object segmentation masks for the original image. This helps mitigate the impact of task-specific guidance on the fusion network learning process, preventing overfitting to HLVT. Secondly, we use the original image after information separation as the input of the generator. The separated network inputs clearly define foreground objects and background environments, aiding the generator in better fusing foreground and background. Lastly, we adopt a no-loading training strategy to balance adversarial learning, prevent discriminator overfitting, and reduce resource waste. Experimental results demonstrate the outstanding performance of our proposed image fusion method in terms of fusion quality, surpassing several representative approaches. Furthermore, experiments in HLVT indicate that our method facilitates HLVT in better responding to potential threats and unknown variables. This improvement significantly enhances HLVT’s security performance.},
  archive      = {J_ASOC},
  author       = {Le Sun and Yuhang Li and Ghulam Muhammad},
  doi          = {10.1016/j.asoc.2024.112114},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft computing-driven infrared and visible image fusion network for security application service},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interaction-enhanced co-evolutionary algorithm for
electric vehicle routing optimization. <em>ASOC</em>, <em>165</em>,
112113. (<a href="https://doi.org/10.1016/j.asoc.2024.112113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the electric vehicle routing problem (EVRP) has emerged as a significant challenge in the transportation field. Unlike traditional vehicle routing problems, EVRP requires optimizing not only the customer route but also the charging scheme. This implies that routes must be planned while considering the availability of charging stations, and charging decisions must be made based on the specific route structure. Although the dual-population-based co-evolutionary algorithm (DPCA) has been proposed to address this coupling relationship, there is still room for further performance improvement. Therefore, this paper proposes an interaction-enhanced co-evolutionary algorithm (IECA) that facilitates a more effective collaborative search between routing and charging optimizations. For routing optimization, we propose an improved ant colony optimization method with a novel pheromone update approach to generate diverse route solutions. Specifically, we utilize information from the charging population to guide the pheromone update process. In contrast to DPCA, which solely relies on the best solution, we establish interaction among multiple solutions from both the routing and charging populations, which can better contribute the information interaction between two populations. Furthermore, we devise a novel population update strategy that enhances the selection of promising solutions. Experimental results validate the effectiveness of the proposed algorithm by demonstrating its ability to avoid local optima. Moreover, it achieves a reduction of approximately 4% in the average route distance across two public test suites. The source code is available at https://github.com/ShouLiang-Z/IECA .},
  archive      = {J_ASOC},
  author       = {Shouliang Zhu and Chao Wang},
  doi          = {10.1016/j.asoc.2024.112113},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interaction-enhanced co-evolutionary algorithm for electric vehicle routing optimization},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Solving low-carbon last mile delivery problem using
discrete marine predators algorithm. <em>ASOC</em>, <em>165</em>,
112112. (<a href="https://doi.org/10.1016/j.asoc.2024.112112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing concentration of greenhouse gases, carbon peaking and carbon neutrality have gradually become a global issue. As the last link of e-commerce logistics, the last mile delivery should not only focus on customer satisfaction and enterprise cost, but also reduce carbon emission and promote green development. In order to solve the last mile delivery problem, this paper builds the last mile delivery model for two common delivery vehicles, considering the impact of transportation cost, carbon emission and customer satisfaction on the last mile delivery path planning. The last mile delivery is an NP-hard problem, a novel Discrete Marine Predators Algorithm(DMPA) is proposed to solve the problem, which combines neighborhood search strategy and RS hybrid operator. Six different test examples are generated based on Solomon for simulation experiments, and the experimental results verify that the proposed algorithm is obviously superior to the marine predators algorithm, simulated annealing algorithm and genetic algorithm. DMPA obtains significantly higher customer satisfaction than other algorithms. The customer satisfaction of C101 and C201 is as high as 0.99, which is close to the optimal value of 1. DMPA can achieve lower transportation costs and less carbon emissions compared with other algorithms. DMPA reduces the transportation cost by at least 61.12$ and the carbon emission by at least 1033.64 kg for C101 by minivan delivery. DMPA reduces the transportation cost by at least 28.89$ and the carbon emission by at least 210.9 kg for C101 by electric tricycle delivery. It can effectively reduce carbon emission, lower transportation cost, improve service level, and promote the integration of the dual carbon target with the internal requirements of national development.},
  archive      = {J_ASOC},
  author       = {Xiaomei Yu and Yu Li and Jingsen Liu and Huan Zhou},
  doi          = {10.1016/j.asoc.2024.112112},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112112},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving low-carbon last mile delivery problem using discrete marine predators algorithm},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised spectral feature selection with neighborhood
rough set. <em>ASOC</em>, <em>165</em>, 112111. (<a
href="https://doi.org/10.1016/j.asoc.2024.112111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral feature selection, an excellent dimensionality reduction method, is extensively used in knowledge mining and protein sequence analysis. However, the graph representation derived from data with potential noises significantly impacts feature selection performance. Similarly, its performance deteriorates when label information is ignored, as the selected features have poor discrimination ability. To overcome these drawbacks, this article optimizes neighborhood structure and further constructs the conditional entropy based on neighborhood purity to generate a precise graph representation incorporating label information to enhance the compactness of intra-class samples and increase the dispersion between inter-class samples. Then this article proposes a novel spectral feature selection method. Specifically, the method dynamically learns the precise graph representation from low-dimension space, which can effectively preserve the local structure of data, and the global structure is preserved by introducing a constraint term that maintains the graph representation before and after dimension reduction. Additionally, ℓ 2 , 1 ℓ2,1 -norm is introduced to ensure row sparsity and select the valuable features. The alternative optimization algorithm is employed to solve the optimization problem and its convergence is theoretically analyzed. Finally, to assess the performance of the proposed method, a series of comprehensive experiments are performed on several real-world datasets.},
  archive      = {J_ASOC},
  author       = {Qiong Liu and Mingjie Cai and Qingguo Li},
  doi          = {10.1016/j.asoc.2024.112111},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112111},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Supervised spectral feature selection with neighborhood rough set},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A double-layer q-learning driven memetic algorithm for
integrated scheduling of procurement, production and maintenance with
distributed resources. <em>ASOC</em>, <em>165</em>, 112110. (<a
href="https://doi.org/10.1016/j.asoc.2024.112110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing research on integrated production scheduling typically focuses on activities related to both production and post-production (e.g., operation and maintenance), with limited consideration for simultaneously integrating pre-production activities (e.g., raw material procurement). However, achieving the equilibrium between the manufacturer and the demander for intelligent manufacturing systems requires the optimal scheduling solution that integrates both pre-production and post-production activities. Inspired by this, we investigate a novel integrated scheduling problem that concurrently considers raw material procurement, production scheduling and equipment maintenance (abbreviated as SIPPM). A mixed-integer linear programming model is developed to simultaneously minimize the total costs for the manufacturer and the demander. Furthermore, a double-layer Q-learning driven memetic algorithm (DQMA) is proposed to solve the SIPPM. In DQMA, a well-tailored three-layer hybrid encoding method is presented for chromosome representation. The global search of DQMA employs three crossover and three mutation operators. Moreover, a knowledge-based local search operator with six methods, guided by an effective double-layer Q-learning structure, is devised to enhance local exploitation capabilities. The superiority of DQMA is verified through comparison with three popular multi-objective optimization algorithms on 108 newly established benchmark instances. The proposed integrated scheduling mode is proven to be more effective than two separated scheduling modes without considering raw material procurement.},
  archive      = {J_ASOC},
  author       = {Jingxing Zhang and Qianwang Deng and Qiang Luo and Zhen Wang and Huining Zhuang and Yutao Huang},
  doi          = {10.1016/j.asoc.2024.112110},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112110},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A double-layer Q-learning driven memetic algorithm for integrated scheduling of procurement, production and maintenance with distributed resources},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving continuous quadratic programming for the discrete
balanced graph partitioning problem using simulated annealing algorithm
hybridized local search and multi-start algorithms. <em>ASOC</em>,
<em>165</em>, 112109. (<a
href="https://doi.org/10.1016/j.asoc.2024.112109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a discrete balanced graph partitioning problem (DBGPP), a simple undirected weighted graph in that both vertices and edges are weighted is given. The task is dividing vertices into two disjoint subsets such that the sum of the weights of cut edges is minimized and the sum of the weights of vertices in each subset must equal or be as close as possible to each other. Here in order to solve the DBGPP, we first transform the problem into continuous quadratic programming and then show that the new problem has a binary solution, which is the optimal solution for the DBGPP. We also give necessary and sufficient conditions of continuous quadratic programming to identify stationary and local optimal solutions. In addition, we propose a local search to find a binary solution of the continuous quadratic programming. An approximated solution of the DBGPP is obtained using a hybrid simulated annealing algorithm. In our proposed algorithm, the structure of the objective function in a continues problem is considered as the evaluator function. Due to the Dolan–Moré performance profiles and the non-parametric statistical one-sided Wilcoxon signed rank test, we demonstrate the efficiency of our proposed approach in comparison with other available methods.},
  archive      = {J_ASOC},
  author       = {Ahmad Abouyee Mehrizi and Reza Ghanbari and Sedigheh Sadeghi and Khatere Ghorbani-Moghadam},
  doi          = {10.1016/j.asoc.2024.112109},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112109},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving continuous quadratic programming for the discrete balanced graph partitioning problem using simulated annealing algorithm hybridized local search and multi-start algorithms},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A complex-valued encoding golden jackal optimization for
multilevel thresholding image segmentation. <em>ASOC</em>, <em>165</em>,
112108. (<a href="https://doi.org/10.1016/j.asoc.2024.112108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilevel thresholding image segmentation is not only an indispensable component of image computation and machine vision but also a fundamental element of image analysis and feature extraction, which has received extensive attention from many domestic and international academics in recent years. However, the threshold level rises in direct proportion to the computational complexity. Therefore, this paper presents a complex-valued encoding golden jackal optimization (CGJO) established on Kapur’s entropy to accomplish this issue, and the intention is to split the available image into a multitude of conspicuous salient portions that illustrate concepts and procedures of the pertinent object. The golden jackal optimization (GJO) employs the cooperated foraging of the jackals to imitate prey searching, enclosing and pouncing to generate the optimum solution. The complex-valued encoding employs the diploid’s notion to alter the real and imaginary of the search agent, which amplifies the information multiplicity and reinforces the inherent parallelism to motivate productive search and inhibit voracious convergence. The functionality and viability of the CGJO are confirmed by comparison with BWOA, DOA, COA, LCA, OOA, SABO, GJO and CRWOA. The experimental results reveal that the CGJO explores and exploits to acquire a superior convergence speed, greater numerical precision and stronger segmentation quality. Additionally, the CGJO offers excellent practicability and stability to address image segmentation successfully.},
  archive      = {J_ASOC},
  author       = {Jinzhong Zhang and Tan Zhang and Duansong Wang and Gang Zhang and Min Kong and Zebin Li and Rui Chen and Yubao Xu},
  doi          = {10.1016/j.asoc.2024.112108},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112108},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A complex-valued encoding golden jackal optimization for multilevel thresholding image segmentation},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep generative model for selecting representative periods
in renewable energy-integrated power systems. <em>ASOC</em>,
<em>165</em>, 112107. (<a
href="https://doi.org/10.1016/j.asoc.2024.112107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extensive integration of renewable energies into power systems has led to a challenging and computationally demanding scenario for the system planning. This is due to the increased number of time series involved and the greater complexity of time series data integrated into power systems. In this paper, a new deep learning-based time aggregation method is proposed for selecting representative periods for renewable energy-integrated power system datasets where multiple variable energy resources are present. Relying on the capabilities of deep generative models, especially Generative Adversarial Network (GAN), the proposed method selects a representative period, including one or more representative days, obtained from multiple time series with spatio-temporal correlations among them. The proposed approach contains the Long Short-Term Memory (LSTM) Network in both generator and discriminator parts. Also, it includes a loss term, specifically designed for clustering tasks, in the minimax objective of the vanilla GAN loss function to enhance the clustering performance in the latent space. Furthermore, the learning rate of the proposed model, as the most important parameter of the learning algorithm, is adaptively fine-tuned during the training process, based on the training error, to enhance its learning performance. Two real-world test cases with various datasets and time series are used for data-based and model-based evaluations. Results obtained on these two real-world test cases confirm the superiority of the proposed model with respect to the state-of-the-art conventional and deep learning-based models, obtaining the performance improvement in the range of [38.55 %, 46.14 %] in terms of Mean Absolute Percentage Error (MAPE), in the range of [45.75 %, 70.16 %] in terms of Root Mean Squared Error (RMSE), and in the range of [65.81 %, 74.58 %] in terms of Mean Absolute Error (MAE). The proposed model can significantly enhance the tractability and scalability of the planning problem of renewable energy-integrated power systems, facilitating renewable energy integration into power systems which is a key issue for net zero transitioning of communities.},
  archive      = {J_ASOC},
  author       = {Razieh Rastgoo and Nima Amjady and Hamidreza Zareipour},
  doi          = {10.1016/j.asoc.2024.112107},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112107},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep generative model for selecting representative periods in renewable energy-integrated power systems},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interactive reference-point-based method for
incorporating user preferences in multi-objective structural
optimization problems. <em>ASOC</em>, <em>165</em>, 112106. (<a
href="https://doi.org/10.1016/j.asoc.2024.112106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although multi-objective evolutionary algorithms (MOEAs) are often used to solve multi-objective structural optimization problems (MOSOPs), the incorporation of decision-maker (DM) preferences into MOEAs to search for regions of interest from the Pareto front has received limited attention in these problems. Recently, three MOEAs have shown promising results in solving MOSOPs incorporating DM’s preferences based on a priori reference points. However, providing this information can be challenging as the DM may not know an initial reference point for the MOSOP. Therefore, this study analyzes three interactive multi-objective optimization (IMO) algorithms, called I-NSGA-II, I-GDE3, and I-GDE3+APM, in which the reference points are progressively incorporated during the search process. These IMOs are applied to benchmark MOSOPs and evaluated using well-known performance indicators. The results demonstrate that I-NSGA-II outperforms I-GDE3 and I-GDE3+APM, which have similar overall performances. Additionally, the IMOs considered here outperformed their respective a priori approaches in MOSOPs where the reference points are located at the central sector of the Pareto front. Also, we identified the performance indicators in which the approaches considered in this work performed better than their corresponding a priori ones. Furthermore, the research findings suggest that these IMOs can generate good results without defining a priori reference points, making them a promising approach for solving MOSOPs and enabling DMs to make better and more reliable decisions.},
  archive      = {J_ASOC},
  author       = {Dênis E.C. Vargas and Afonso C.C. Lemonge and Helio J.C. Barbosa and Heder S. Bernardino},
  doi          = {10.1016/j.asoc.2024.112106},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112106},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interactive reference-point-based method for incorporating user preferences in multi-objective structural optimization problems},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The influence of externality in the graph model for conflict
resolution under fuzzy preferences. <em>ASOC</em>, <em>165</em>, 112105.
(<a href="https://doi.org/10.1016/j.asoc.2024.112105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Model for Conflict Resolution (GMCR) is a flexible and comprehensive methodology for systematically investigating strategic conflicts. However, most existing studies solely focus on internal conflicts among decision makers (DMs), often disregarding the influence of external stakeholders (ESs) on DMs’ preferences. Consequently, this leads to insufficient stability in equilibrium outcomes. The objective of this study is to address the conflict issue faced by DMs with subjective uncertain preferences who are influenced by externality. Building upon the framework of GMCR, this study proposes the graph model for conflict resolution with fuzzy preference based on externality (FEGMCR) which introduces fuzzy numbers to measure both uncertain preferences of DMs and uncertain expectations of ESs. Furthermore, four forms of stability are extended-- Fuzzy Nash Stability based on externality (FER), Fuzzy General Metarationality based on externality (FEGMR), Fuzzy Symmetric Metarationality based on externality (FESMR), and Fuzzy Sequential Stability based on externality (FESEQ). The optimal equilibrium is achieved by dynamically adjusting the weight of the impact of externalities. Finally, the method is applied to the case of plastic ban in Hainan. It is found that the expectations of ESs do change the preferences of DMs and affect the final equilibrium, which provides a sustainable solution for the issue.},
  archive      = {J_ASOC},
  author       = {Xuemei Li and Junwen Xu and Yufeng Zhao and Benshuo Yang},
  doi          = {10.1016/j.asoc.2024.112105},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The influence of externality in the graph model for conflict resolution under fuzzy preferences},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outlier detection based on multisource information fusion in
incomplete mixed data. <em>ASOC</em>, <em>165</em>, 112104. (<a
href="https://doi.org/10.1016/j.asoc.2024.112104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisource incomplete mixed data fusion (MsIMDF) plays a crucial role in outlier detection by utilizing complementary, informative, interpretative, and less noisy single-source data to identify unexpected errors or behaviors. However, existing multisource data fusion approaches only consider homogeneous data and single uncertain information,overlooking the mixed heterogeneous data and diverse uncertainty information. This limitation can negatively impact the performance of outlier detection. To address this issue, we propose MsIMDF-USF, a novel two-stage model that fully leverages the rich multisource knowledge and uncertainty information in incomplete mixed data. During the information fusion stage, the MsIMDF model combines multisource data into new single-source data using the minimum uncertainty strategy based on rough and fuzzy information. Subsequently, in the outlier detection stage, we reconstruct a neighborhood information network under a united-similar-fuzzy (USF) relationship using the new fused data. This reconstruction aims to strengthen the connections between similar objects while weakening relationships among dissimilar ones by considering single-attribute and multi-attribute information. Outlier scores are obtained based on the stationary distribution of the reconstructed networks using a Markov random walk. Experimental results on 16 real datasets demonstrate that the MsIMDF-USF model effectively extracts higher-quality data, exhibiting high applicability and robustness in outlier detection tasks.},
  archive      = {J_ASOC},
  author       = {Ran Li and Hongchang Chen and Shuxin Liu and Kai Wang and Shuo Liu and Zhe Su},
  doi          = {10.1016/j.asoc.2024.112104},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Outlier detection based on multisource information fusion in incomplete mixed data},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Enhancing mobility management in 5G networks using deep
residual LSTM model. <em>ASOC</em>, <em>165</em>, 112103. (<a
href="https://doi.org/10.1016/j.asoc.2024.112103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility management is an essential component of 5G networks to provide mobile users with seamless connectivity and efficient cell transition. However, increasing user mobility, device density, and the diversity of service requirements all pose significant challenges to achieving optimal mobility management. This article describes a novel method for improving mobility management in 5G networks that employs a deep residual Long Short-Term Memory model. Deep learning and LSTM, a type of recurrent neural network, are used in the proposed model to identify temporal dependencies and patterns in user mobility data. The model learns to predict future user locations and mobility patterns by training on historical mobility data, allowing for proactive resource allocation and handover decisions. We incorporate residual connections into the LSTM architecture, inspired by the residual learning framework, to address the inability of traditional LSTM models to capture complex temporal dynamics. This allows the model to effectively incorporate long-term dependencies and improves prediction accuracy. Furthermore, we incorporate the mLSTM model into the mobility management framework of 5G networks. The model continuously obtains real-time user location updates and predicts future user positions, allowing for proactive handover decisions. The network can optimize resource allocation, reduce handover latency, and improve user experience by leveraging anticipated mobility patterns. We test the proposed method by simulating it extensively with real-world mobility traces. The results show that the mLSTM model accurately predicts user mobility and outperforms conventional methods in transition performance. The model is not affected by changing network conditions, user mobility patterns, or service specifications.},
  archive      = {J_ASOC},
  author       = {Abdullah Baz and Jaganathan Logeshwaran and Yuvaraj Natarajan and Shobhit K. Patel},
  doi          = {10.1016/j.asoc.2024.112103},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing mobility management in 5G networks using deep residual LSTM model},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage heuristic algorithm with pseudo node-based model
for electric vehicle routing problem. <em>ASOC</em>, <em>165</em>,
112102. (<a href="https://doi.org/10.1016/j.asoc.2024.112102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric vehicle routing problem (EVRP) is a special vehicle routing problem, which owns several specific characteristics of EV technology including the limited charging stations and the limited cruising range of EVs. This paper proposes a two-stage heuristic algorithm for EVRP with a kind of pseudo node, termed EVRPPN-TSH, which not only contributes to the model innovation but also to the algorithm design. In model innovation, the pseudo node is introduced into the EVRP model, forming EVRPPN, to reduce the search space and time complexity for obtaining service orders of customers from O ( n ) O(n) to O ( 1 ) O(1) . Moreover, different from other research works which only use total distance as the fitness function, an improved fitness function is proposed, which takes all the distance, the electricity and capacity constraints compliance into consideration. In algorithm design, we first adopt the two-stage idea, which divides EVRP into the capacitated vehicle routing problem (CVRP) and a fixed route vehicle charging problem (FRVCP). Then we design a two-stage heuristic algorithm, termed as TSH. Specifically, for CVRP, a route division heuristic algorithm and a depot move heuristic algorithm are designed to divide routes. Further, an adjustment strategy with three operators is designed for adjusting the customer order. For FRVCP, an existing heuristic algorithm is adopted and enhanced by a variable extent shaking algorithm to avoid local trapped. Experimental results show that EVRPPN-TSH generally outperforms other state-of-the-art algorithms. Moreover, EVRPPN-TSH can obtain the best-known solution in all the 7 test cases, and update the best-known solution in test case E101 from 834.84 to 834.22.},
  archive      = {J_ASOC},
  author       = {Xiaoyun Xia and Helin Zhuang and Zijia Wang and Zefeng Chen},
  doi          = {10.1016/j.asoc.2024.112102},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage heuristic algorithm with pseudo node-based model for electric vehicle routing problem},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning with hard negative samples for chest
x-ray multi-label classification. <em>ASOC</em>, <em>165</em>, 112101.
(<a href="https://doi.org/10.1016/j.asoc.2024.112101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning has gained significant popularity and achieved remarkable success in learning meaningful representations in various domains. This study addresses the significant problem of dependency on labeled data in chest radiography (CXR) images, which are crucial for diagnosing respiratory diseases such as pneumonia but are both time-consuming and expensive to annotate. Despite extensive research, existing studies on CXR largely depend on labeled data. To overcome this challenge, we propose a framework named SURE (similarity, uncertainty, and representativeness) for multi-label classification with hard negatives in contrastive learning. The proposed framework incorporates these aspects when handling hard negatives and effectively combines contrastive learning and downstream tasks for robust representation learning and multi-label classification. Experimental validation using three distinct CXR datasets demonstrates that our approach significantly reduces the dependency on labeled data while achieving notable performance improvements over existing methods, highlighting its potential effectiveness and efficiency in the CXR domain.},
  archive      = {J_ASOC},
  author       = {Goeun Chae and Jiyoon Lee and Seoung Bum Kim},
  doi          = {10.1016/j.asoc.2024.112101},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning with hard negative samples for chest X-ray multi-label classification},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost-sensitive stacked long short-term memory with an
evolutionary framework for minority class detection. <em>ASOC</em>,
<em>165</em>, 112098. (<a
href="https://doi.org/10.1016/j.asoc.2024.112098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {‘Minority attack detection’ is a matter of great concern while designing a secure network and safeguarding it against cyber criminals who attempt to breach its defenses, intrude into its systems, and gain unauthorized access to sensitive information. An adequate intrusion detection system is demanded to counter such threats involving class imbalance and high dimensionality in network traffic. In this context, an artificial intelligence-based multiclass framework MCGAL is presented, which leverages the evolutionary technique of genetic algorithm to achieve the finest feature selection and stacked long short-term memory (LSTM) for traffic classification. The proposed approach utilizes cost-sensitive LSTM by assigning greater significance to minority or rarely occurring classes, thereby addressing the challenge of misclassification in the presence of imbalanced data in a real web attack subset of the CSE-CIC-IDS2018 dataset. A comprehensive performance analysis is performed to evaluate our proposed scheme concerning various class metrics using weighted and macro averages, which reveals a classification accuracy of 100 %. An achieved weighted average of 1.00 is then compared with baseline classifiers like decision trees, AdaBoost, and support vector machines utilizing both linear and radial kernels. The results of MCGAL are also assessed with recently reported techniques on the same dataset demonstrating remarkable improvements in all metrics such as precision, recall, and F-score of minority class detection. A pair-wise comparison and Kruskal-Wallis test at a 95 % significance level further illustrate the statistical significance of the proposed scheme for minority class detection.},
  archive      = {J_ASOC},
  author       = {Asima Akber Abbasi and Aneela Zameer and Earum Mushtaq and Muhammad Asif Zahoor Raja},
  doi          = {10.1016/j.asoc.2024.112098},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cost-sensitive stacked long short-term memory with an evolutionary framework for minority class detection},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nakagami-fuzzy imaging for grading brain tumors by analyzing
fractal complexity. <em>ASOC</em>, <em>165</em>, 112097. (<a
href="https://doi.org/10.1016/j.asoc.2024.112097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gliomas are the brain tumors in glial cells, which are categorized into four numerical grades, I-II-III-IV, to quantize the aggressiveness and severity of the tumors; while divided into two major groups, high-grade (HG) and low-grade (LG), in general. Among many differences between these groups, one of the most distinct and characteristic features could be seen in the shape of the tumor boundaries by magnetic resonance imaging (MRI). Due to aggressive nature of the HG tumors in proliferation phase, the boundaries of HG tumors become more shape-wise complex compared to the LG tumors, which could be differentiated by analyzing the fractal complexity of the cell membranes. However, the complexity cannot be either manually calculated or estimated by eye inspection without a reference point with one single image or sometimes even with an image set. Therefore, we present an automated glioma grading framework to provide an insight on the grades with a novel contouring and fractal dimension analysis system. The primary component of the proposed system is an automated Nakagami imaging module with a specialized fuzzy c-means algorithm to contour the boundaries of the whole tumors. The contoured images, afterwards, are analyzed by the Minkowski–Bouligand and Hausdorff methods for two panning options to generate the fractal dimensions and to estimate the fractal complexities for classifying the gliomas The results are greatly encouraging that the overall classification accuracy is computed as 88.31 % using the basic support vector machines (SVM) classifier; while as 91.96 % with the arbitrary thresholding appended. The outcomes of this paper with implementable mathematical infrastructure would be very useful and beneficial as an expert system in intelligent and automatic glioma grading, for researchers and medical experts.},
  archive      = {J_ASOC},
  author       = {Orcan Alpar},
  doi          = {10.1016/j.asoc.2024.112097},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112097},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nakagami-fuzzy imaging for grading brain tumors by analyzing fractal complexity},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Why consider quantum instead classical pattern recognition
techniques? <em>ASOC</em>, <em>165</em>, 112096. (<a
href="https://doi.org/10.1016/j.asoc.2024.112096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article delves into the evolving landscape of pattern recognition, transitioning from classical methodologies to quantum-based techniques. It underscores how quantum algorithms offer a new paradigm with the potential to overcome the limitations of classical techniques. Unlike conventional methods, which, while effective, often struggle with complex and high-dimensional datasets, quantum algorithms are poised to surpass these limitations. This study explores the applications, benefits, drawbacks, and open issues surrounding quantum pattern recognition methods and provides a comprehensive overview of the current state of quantum technology and outlines potential future directions, highlighting the intersection of quantum computing and pattern recognition for breakthroughs.},
  archive      = {J_ASOC},
  author       = {Artur Gomes Barreto and Felipe Fernandes Fanchini and João Paulo Papa and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.asoc.2024.112096},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Why consider quantum instead classical pattern recognition techniques?},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A branched convolutional neural network for RGB-d image
classification of ceramic pieces. <em>ASOC</em>, <em>165</em>, 112088.
(<a href="https://doi.org/10.1016/j.asoc.2024.112088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From smart sensors on assembly lines to robots performing complex tasks, the fourth industrial revolution is rapidly transforming manufacturing. The growing prominence of 3D cameras in the industry has led the computer vision community to explore innovative ways of integrating depth and color data to achieve higher precision, essential for ensuring product quality in manufacturing. In this study, we introduce an innovative branched convolutional neural network designed to produce high-speed classification of multimodal images, such as RGB-Depth (RGB-D) images. The fundamental concept underlying the branched approach is the specialization of each branch as a dedicated feature extractor for a single modality, followed by their merge (intermediate fusion) to enable effective classification. Feeding our model is our novel multimodal dataset, named CeramicNet, composed of 8 classes that include RGB, depth, and RGB-D variations to enable extensive experimentation and evaluation of the models which, to the best of our knowledge, has not been previously introduced in the computer vision community. We conducted a series of experiments on the CeramicNet dataset. These experiments aimed at fine-tuning the model, assessing the influence of various depth technologies, exploring individual modalities, examining their collective impact, and performing comprehensive data analysis. Comparing our solution against seven widely used models, we achieved remarkable results, securing the top position with a precision of 99.89, with a lead of over 1% against the nearest competitor. What is more, the proposed solution yields an inference time of 127.6 ms — being nearly three times faster than the second-best performer.},
  archive      = {J_ASOC},
  author       = {Daniel Carreira and Nuno Rodrigues and Rolando Miragaia and Paulo Costa and José Ribeiro and Fábio Gaspar and António Pereira},
  doi          = {10.1016/j.asoc.2024.112088},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A branched convolutional neural network for RGB-D image classification of ceramic pieces},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective regularized spatial features representation
learning for motor imagery EEG based on alternating cascaded model.
<em>ASOC</em>, <em>165</em>, 112087. (<a
href="https://doi.org/10.1016/j.asoc.2024.112087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature representation plays a pivotal role in the decoding of motor imagery electroencephalograph (MI-EEG) signals. Conventional spatial representations are often hindered by the operational time-frequency and noise interferences of MI-EEG. In response to this challenge, this paper proposed a novel method to S elective extract the R egularized S patial features and represent by the A lternating C ascaded M odel (SRS-ACM) for MI-EEG decoding. Initially, regularized common spatial patterns are derived from MI-EEG samples, capturing the relevance and redundancy between spatial features and labels. Subsequently, a global optimization framework is devised to select discriminative and comprehensive dimensions with the spatial features. Finally, an alternating cascade model, integrating sparse and collaborative representations, is developed to capture the intrinsic patterns with the selected spatial features, thereby facilitating MI-EEG decoding. Experimental evaluations were conducted across three publicly available MI-EEG datasets: BCI-III dataset 4a, BCI-IV dataset 1, and BCI-IV dataset 2a. The SRS-ACM model demonstrated notable performance improvements of 2.60 %, 6.15 %, and 1.14 % on these datasets, respectively. Ablation studies underscored the necessity and significance of the components within the SRS-ACM method, while parameter sensitivity analyses validated the robustness of the SRS-ACM approach in the development of practical MI-based brain-computer interfaces.},
  archive      = {J_ASOC},
  author       = {Tian-jian Luo},
  doi          = {10.1016/j.asoc.2024.112087},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selective regularized spatial features representation learning for motor imagery EEG based on alternating cascaded model},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-series prediction of organomineral fertilizer moisture
using machine learning. <em>ASOC</em>, <em>165</em>, 112086. (<a
href="https://doi.org/10.1016/j.asoc.2024.112086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to model and forecast the drying process of three new types of commercial organomineral fertilizers: gold sulfur, 25.5.5, and 5×10 at elevated temperatures. They absorb and release moisture, depending on the conditions. Accurate prediction of drying behaviour is essential. Drying was carried out at temperatures of 70, 75, and 80 °C through natural convection. The data are unimodal time series of the moisture rate ( MR ). Supervised machine/deep learning techniques such as nonlinear autoregressive (NAR) network, adaptive neural fuzzy inference systems (ANFIS), long short-term memory (LSTM) network, gated recurrent unit (GRU), and hybrid of convolutional neural network (CNN) and recurrent neural network (RNN) are used in addition to well-known regression-based formulas. The models can predict 30 minutes ahead. An error analysis was performed for performance comparison using the metrics, root mean square error ( RMSE ), and coefficient of determinant, R 2 R2 . Two types of validation were performed by monitoring error convergence and using prediction curves. The most effective forecasting was obtained at an air temperature of 80 °C for all materials with machine learning. While RMSE =0.0030 having R 2 = 0.98843 R2=0.98843 by the LSTM network for gold sulfur (80 °C), ANFIS was the best for 25.5.5 and 5×10 with RMSE =0.0056, R 2 = 0.75585 R2=0.75585 and RMSE =0.0060, R 2 = 0.81938 R2=0.81938 , respectively, in the MR prediction. GRU was remarkable for both its speed of 13.77 sec and RMSE =0.009. CNN-RNN has a more complex structure but lower performance. The results demonstrate that machine learning techniques are better than regressions. Among them, ANFIS provides the most reliable results. Regressions, including exponential terms, were good at providing a general curve shape but not peaks and drops. In addition, regressions are not good at forecasting.},
  archive      = {J_ASOC},
  author       = {Cem Korkmaz and İlyas Kacar},
  doi          = {10.1016/j.asoc.2024.112086},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time-series prediction of organomineral fertilizer moisture using machine learning},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Piranha predation optimization algorithm (PPOA) for global
optimization and engineering design problems. <em>ASOC</em>,
<em>165</em>, 112085. (<a
href="https://doi.org/10.1016/j.asoc.2024.112085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new nature-inspired optimization algorithm, Piranha predation optimization algorithm (PPOA), is proposed based on the unique foraging and predation behaviors of piranhas. Briefly, PPOA consists of three optimization operations, i.e., narrowing down to tear prey, swimming in a straight line, and swimming in a spiral. In this paper, various mathematical models for simulating the behavioral operators are presented in detail to solve different optimization challenges effectively. In this paper, the performance of PPOA is rigorously tested on 23 benchmark optimization functions, CEC2017 competition test set, CEC2020 real-world engineering optimization problems and four engineering design applications to show the applicability of the algorithm in different applications. Comparison experiments with other good and advanced competitive algorithms are conducted to reveal the advantages and performance of PPOA by using performance metrics such as Wilcoxon rank sum test and Friedman mean rank. The comparative results of this paper demonstrate the effectiveness of the proposed algorithmic strategy and its potential in applying it to solving optimization real-world engineering optimization problems.},
  archive      = {J_ASOC},
  author       = {Chunliang Zhang and Huang Li and Shangbin Long and Xia Yue and Haibin Ouyang and Zeyu Chen and Steven Li},
  doi          = {10.1016/j.asoc.2024.112085},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112085},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Piranha predation optimization algorithm (PPOA) for global optimization and engineering design problems},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A wind speed point-interval fuzzy forecasting system based
on data decomposition and multiobjective optimizer. <em>ASOC</em>,
<em>165</em>, 112084. (<a
href="https://doi.org/10.1016/j.asoc.2024.112084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the depletion of traditional energy resources and escalating environmental challenges, the importance of wind power in the energy sector has intensified. However, the inherent stochasticity of wind makes short-term forecasting a complex and essential task for grid stability and efficiency. This study proposes a wind speed fuzzy prediction system that integrates enhanced variational mode decomposition for data preprocessing, an optimal predictor selection strategy for selecting optimal submodels, and a modified multiobjective optimization algorithm for optimizing multiple forecasting objectives. The system employs fuzzy theory to construct fuzzification, aggregation, and defuzzification functions, leveraging the strengths of benchmark predictors to generate point and interval predictions. In addition, comparative experiments are conducted on three sampling intervals of data from five wind turbine sites in China. The proposed system achieved a mean absolute percentage error of 3.89% and a prediction interval coverage probability of 94.44% at site 1, which significantly outperformed the existing contrast models.},
  archive      = {J_ASOC},
  author       = {Yurui Xia and Jianzhou Wang and Ziyuan Zhang and Danxiang Wei and Zhining Cao and Zhiwu Li},
  doi          = {10.1016/j.asoc.2024.112084},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112084},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A wind speed point-interval fuzzy forecasting system based on data decomposition and multiobjective optimizer},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge AI-driven neural network predictions for replica sync
optimization. <em>ASOC</em>, <em>165</em>, 112083. (<a
href="https://doi.org/10.1016/j.asoc.2024.112083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed storage systems deploy data centers across multiple geographical locations to share peak loads and store massive amounts of data. By integrating edge AI technology, these systems elevate edge servers to intelligent edge nodes, deeply analyze user request patterns and data center loads, and intelligently allocate requests, thereby reducing service latency and enhancing overall service quality. However, cross-geographic distribution brings challenges in maintaining data consistency and controlling cross-node communication costs for edge AI. Additionally, a single consistency level struggles to meet diverse requirements, affecting system performance and flexibility. We explore these challenges based on a novel key–value storage system, FlexSync. Firstly, we propose a Writeless-Consistency strategy, combining machine learning to predict replica synchronization requirements, reducing redundant data transmission, and improving synchronization efficiency. Building upon this, we optimize traditional consistency algorithms and leverage Conflict-free Replicated Data Types (CRDTs) to construct a flexible multi-consistency level model, better suited for diverse application scenarios. We deployed a FlexSync cluster on the Alibaba Cloud platform and conducted comprehensive performance evaluations. Experimental results demonstrate that compared to Raft, FlexSync’s strong consistency algorithm improves performance by 1.8X, while compared to Vector Clock-based algorithms, causal consistency algorithm performance enhances by 2.5X. These results validate FlexSync’s efficiency and stability in geographically distributed environments.},
  archive      = {J_ASOC},
  author       = {Zichen Xu and Yucong Dong and Junsheng Lou and Yangyang Wang and Yan Fu},
  doi          = {10.1016/j.asoc.2024.112083},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112083},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Edge AI-driven neural network predictions for replica sync optimization},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning based improved cross-project software
defect prediction using new structural features in object oriented
software. <em>ASOC</em>, <em>165</em>, 112082. (<a
href="https://doi.org/10.1016/j.asoc.2024.112082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-project software defect prediction (CPSDP) uses data from other projects to train the defect prediction model and is often used for new projects for which previous versions of the software are not available. Most of the latest CPSDP models are based on machine learning (ML), and ML algorithms use code quality metrics of the project as input. Existing ML-based CPSDP models use trivial count-based code quality metrics of the code&#39;s object-oriented features, which can hardly capture the structural characteristics of the code and are insufficient to develop a good CPSDP model. So, this research article proposes an improved ML-based CPSDP model that uses new structural code metrics as input features. Proposed source code metrics capture the structural characteristics of the code, such as complexity of the class, coupling, lack of cohesion, abstraction, inheritance coupling, and polymorphism, in a better way than existing source code metrics. Datasets of selected projects are prepared based on the proposed structural code metrics, and a new t-test-based procedure for project selection to prepare training datasets for ML algorithms is proposed. Finally, the most appropriate ML algorithm, extreme gradient boost, is selected for our proposed CPSDP model based on three performance metrics: AUC, accuracy, and balance between AUC and accuracy (BBAA). The balance between AUC and accuracy (BBAA) is proposed in this article. The proposed CPSDP model is compared with six existing models, and results show a significant improvement in the performance.},
  archive      = {J_ASOC},
  author       = {Manpreet Singh and Jitender Kumar Chhabra},
  doi          = {10.1016/j.asoc.2024.112082},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112082},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning based improved cross-project software defect prediction using new structural features in object oriented software},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic uncertainty-aware ensemble model: Application to
lung cancer segmentation in digital pathology. <em>ASOC</em>,
<em>165</em>, 112081. (<a
href="https://doi.org/10.1016/j.asoc.2024.112081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble models have emerged as a powerful technique for improving robustness in medical image segmentation. However, traditional ensembles suffer from limitations such as under-confidence and over-reliance on poor performing models. In this work, we introduce an Adaptive Uncertainty-based Ensemble (AUE) model for tumor segmentation in histopathological slides. Our approach leverages uncertainty estimates from Monte Carlo dropout during testing to dynamically select the optimal pair of models for each whole slide image. The AUE model combines predictions from the two most reliable models (K-Net, ResNeSt, Segformer, Twins), identified through uncertainty quantification, to enhance segmentation performance. We validate the AUE model on the ACDC@LungHP challenge dataset, systematically comparing it against state-of-the-art approaches. Results demonstrate that our uncertainty-guided ensemble achieves a mean Dice score of 0.8653 and outperforms traditional ensemble techniques and top-ranked methods from the challenge by over 3 %. Our adaptive ensemble approach provides accurate and reliable lung tumor delineation in histopathology images by managing model uncertainty.},
  archive      = {J_ASOC},
  author       = {Massimo Salvi and Alessandro Mogetta and U. Raghavendra and Anjan Gudigar and U. Rajendra Acharya and Filippo Molinari},
  doi          = {10.1016/j.asoc.2024.112081},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112081},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic uncertainty-aware ensemble model: Application to lung cancer segmentation in digital pathology},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian imprecise classification method that weights
instances using the error costs. <em>ASOC</em>, <em>165</em>, 112080.
(<a href="https://doi.org/10.1016/j.asoc.2024.112080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical applications, Bayesian classification methods have been successfully employed. The Naïve Bayes algorithm (NB) is a quick, successful, and well-known Bayesian classification method. The Naïve Credal Classifier (NCC) is a version of NB that outputs imprecise predictions (sets of class values). NCC was also adapted for considering classification error costs. Such an adaptation is the only Bayesian method for Imprecise Classification proposed so far that considers misclassification costs. This paper presents a Bayesian algorithm for Imprecise Classification that weights the instances using the misclassification costs in such a way that the importance of an instance increases as the error cost of its class value is higher. We highlight that our proposal may provide more informative and intuitive outcomes than the existing cost-sensitive NCC. We experimentally show that our new proposed method improves the existing cost-sensitive NCC. Moreover, we highlight that our imprecise classifier has a processing time equivalent to the original NB algorithm for precise classification, which has been successfully applied to very large and real datasets. This is a crucial point in favor of our proposal because of the huge amount of data in many application areas nowadays.},
  archive      = {J_ASOC},
  author       = {Serafín Moral-García and Tahani Coolen-Maturi and Frank P.A. Coolen and Joaquín Abellán},
  doi          = {10.1016/j.asoc.2024.112080},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112080},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bayesian imprecise classification method that weights instances using the error costs},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Modeling 5G shared base station planning problem using an
evolutionary bi-level optimization algorithm. <em>ASOC</em>,
<em>165</em>, 112079. (<a
href="https://doi.org/10.1016/j.asoc.2024.112079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the cost of 5G network construction surges, Base Station (BS) sharing is becoming more and more popular among operators nowadays. A typical scenario of 5G shared BS planning is presented in this paper, in which different operators share the BSs constructed by the same tower company to reduce the investments. As a result, the 5G shared BS planning problem is modeled as a Bi-Level Optimization Problem (BLOP), with the tower company as the upper level decision-maker and the operators as the lower level decision-makers. The suggested bi-level 5G shared BS planning model considers constraints of various kinds involved in the real network planning scenario, therefore making it more realistic and applicable. Considering the impact of constraints, a Transfer Learning based Evolutionary Algorithm for shared BS Planning (TLEA-BSP) is introduced to solve the proposed bi-level 5G shared BS planning model. In TLEA-BSP, infeasible and feasible information can be transferred and utilized among the optimization processes of multiple lower level problems, such that the upper level and lower level optimization efficiency can be greatly improved. Experimental simulations are conducted on ten generated instances with different scales, and the effectiveness of the proposed model is confirmed by comparing the results of the bi-level model with those of two single-level 5G BS planning models. Comparing the proposed algorithm with the well-established BL-CMA-ES without using the transfer learning strategy for the bi-level 5G shared BS planning model also reveals the essential role of the transfer learning strategy.},
  archive      = {J_ASOC},
  author       = {Lei Chen and Kuntao Li and Hai-Lin Liu},
  doi          = {10.1016/j.asoc.2024.112079},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112079},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling 5G shared base station planning problem using an evolutionary bi-level optimization algorithm},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A genetic programming approach for hardware-oriented hash
functions for network security applications. <em>ASOC</em>,
<em>165</em>, 112078. (<a
href="https://doi.org/10.1016/j.asoc.2024.112078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-cryptographic (NC) hash functions are generally designed for speed and efficiency, which enables their use in many network security applications that require efficient lookup and counting, such as Bloom Filters and Count-Min (CM) Sketch structures. The performance of these structures heavily relies on underlying hash functions. Therefore, any advancement in the hash function design significantly impacts the overall performance of these structures. This paper presents a novel family of 32-bit NC hash functions (NCGPH-32) evolved using Genetic Programming (GP) and their corresponding implementation on Field Programmable Gate Arrays (FPGAs). This family of NC hash functions generates smaller hash values concatenated to produce larger hash outputs. Inspired by related work on 96-bit NC hash functions with GP, this work optimizes the performance of 32-bit NC hash functions on FPGA while achieving high scores on specific avalanche metrics (avalanche dependence, avalanche weight, and entropy) when considering concatenated 96-bit outputs. This optimization is of utmost importance to address the escalating demand for Terabit Ethernet networks, specifically in processing real-time network flow IDs (identification and monitoring) at line rate. The throughput, latency, operating frequency, and resource utilization are evaluated on an FPGA and compared against 17 state-of-the-art NC hash functions. The results show that the proposed 96-bit concatenated hash function surpasses prior GP-based and other state-of-the-art NC hash functions by at least 36% in operating frequency, 30% in throughput and reduces latency by 27%. The demonstrated improvements in the hash design not only cater to the present demands of Terabit networks but also meet the expected near-future demands. Additionally, we integrate these hash functions into the Standard Bloom Filter (SBF) architecture and demonstrate comparable false positive rates (FPR) to state-of-the-art NC hash functions, affirming their effectiveness and applicability. We have also conducted several statistical tests on hash outputs of NCGPH-32 to demonstrate the high random nature and uniform distribution.},
  archive      = {J_ASOC},
  author       = {Mujtaba Hassan and Arish Sateesan and Jo Vliegen and Stjepan Picek and Nele Mentens},
  doi          = {10.1016/j.asoc.2024.112078},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112078},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic programming approach for hardware-oriented hash functions for network security applications},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Market intelligence applications leveraging a
product-specific sentence-RoBERTa model. <em>ASOC</em>, <em>165</em>,
112077. (<a href="https://doi.org/10.1016/j.asoc.2024.112077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Market intelligence, which collects and analyzes market trends and competitive landscape, is crucial for business success in the market. In particular, defining market scope is important because the market analysis outcomes vary depending on where the market boundaries are. However, this has been challenging due to the difficulty in scoping a number of similar products into the target market appropriately. To address this challenge, this study devised a novel method for acquiring reliable market intelligence utilizing the Sentence-RoBERTa (SRoBERTa) model, a pre-trained language model (PLM) specialized in computing sentence similarity. The SRoBERTa model was fine-tuned with 26,248,771 data on product relations and built a product-specific SRoBERTa model (named Pro-SRoBERTa) for effectively identifying the products that fall within the market scope. It outperformed the untrained model by up to 82.8 % on the hierarchical product categorization task. Finally, the Pro-SRoBERTa model was applied to determine the scope of the market followed by obtaining diverse market intelligence. The analysis module was successfully implemented in the market information system using a large-scale, real-world dataset.},
  archive      = {J_ASOC},
  author       = {Ye Lim Jung},
  doi          = {10.1016/j.asoc.2024.112077},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112077},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Market intelligence applications leveraging a product-specific sentence-RoBERTa model},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Automatic localization of image semantic patches for crop
disease recognition. <em>ASOC</em>, <em>165</em>, 112076. (<a
href="https://doi.org/10.1016/j.asoc.2024.112076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop disease recognition plays a crucial role in agricultural production. However, disease images are large in scale and have a lot of redundant information, which reduces the effectiveness of deep neural networks in extracting diseases. To address the above issues and considering that not all image regions are relevant to disease recognition, this study proposes an efficient crop disease recognition method with dynamic reduction of image redundancy. The method is a two-stage process. In the first stage, we employ the lightweight CA-AnchorNet, which incorporates coordinate attention, to swiftly generate a feature map of the affected crop areas. Subsequently, class activation maps (CAMs) are utilized to identify the disease feature regions, highlighting areas that exhibit class discriminability. These regions are then mapped to a higher resolution from the original lower-resolution image, and the target patch is extracted. In the second stage, these local semantic patches, characterized by reduced spatial redundancy, are fed into the lightweight PatchNet for accurate recognition. PatchNet incorporates the Inception-C module and the ACON-C activation function. These features enhance the model&#39;s ability to express multi-scale and non-linear characteristics of crop disease features. This method does not require manually annotated region boxes and achieves an identification accuracy of 99.86 % on a 12-class crop disease dataset with complex environments. The parameter count is only 0.98 M. This method has the characteristics of accurate localization and low parameter count, and can be used for effective and high-precision recognition of crop diseases in complex environments.},
  archive      = {J_ASOC},
  author       = {Haidong Li and Hansu Zhang and Jinling Zhao and Linsheng Huang and Chao Ruan and Yingying Dong and Wenjiang Huang and Dong Liang},
  doi          = {10.1016/j.asoc.2024.112076},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112076},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic localization of image semantic patches for crop disease recognition},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broad collaborative filtering with adjusted cosine
similarity by fusing matrix completion. <em>ASOC</em>, <em>165</em>,
112075. (<a href="https://doi.org/10.1016/j.asoc.2024.112075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative filtering (CF) algorithms provide personalized recommendations based on user preferences and they are widely applied in various domains including social media and video platforms. Recently, the broad learning system (BLS) has been incorporated into CF. The combination of BLS and CF produces more accurate recommendation results due to the fact it can efficiently learn the non-linear relationship between items and users. However, the BLS-based CF adopts cosine similarity to seek several nearest neighbors when constructing features. This feature construction process generally leads to the difference of user rating scales and thus affects the calculation accuracy of the nearest neighbors. Furthermore, like many other CF algorithms, the BLS-based version cannot effectively handle with the data sparsity problem. The above two issues frequently result in poor recommendation performance for the BLS-based CF. To address these problems, a broad collaborative filtering with adjusted cosine similarity (ACOS), named MC-ABCF, is proposed by fusing matrix completion (MC). The matrix completion technique is first used to complete the rating matrix in order to relieve the phenomenon of data sparsity. Subsequently, the adjusted cosine similarity is utilized to find the nearest neighbors of a given user or item. As a result, the problem of rating scales difference between different users is avoided to some extent. The BLS is finally employed to establish complex nonlinear relationships between users and items. Extensive experiments on three benchmark datasets demonstrate that the proposed MC-ABCF model can mitigate the difficulties of data sparsity and user rating scales difference to a certain extent. Taking RMSE as an example, the MC-ABCF algorithm achieved an average improvement of 8.48 % compared to the BCF model across three datasets. In addition, ablation studies have shown the application of MC or ACOS is beneficial for the recommendation performance.},
  archive      = {J_ASOC},
  author       = {Pan He and Jiarong Shi and Wenhua Ma and Xiuyun Zheng},
  doi          = {10.1016/j.asoc.2024.112075},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112075},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad collaborative filtering with adjusted cosine similarity by fusing matrix completion},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Downstream lingering attention transformer network
(DsLATNet) for land use land cover classification: A bicolor deep
learning framework. <em>ASOC</em>, <em>165</em>, 112074. (<a
href="https://doi.org/10.1016/j.asoc.2024.112074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Land Use and Land Cover (LULC) classification is the process of locating and classifying regions of the Earth&#39;s surface (land cover) based on their physical attributes and human utilization (land use purpose). It is essential for mapping and monitoring changes in ecosystems, facilitating effective resource management and informed decision-making in land-use planning. Convolutional Neural Network (CNN’s) is incapable of capturing long-range dependencies. The effectiveness of transformers relies on extensive training datasets, yet many satellite datasets have comparatively limited sample sizes. Moreover, due to the numerous factors influencing visual prominence, it is challenging to obtain abundant features from a single-color space. To overcome these limitations, we introduce an innovative bicolor architecture Downstream Lingering Attention Transformer Network (DsLATNet). DsLATNet characterizes remote sensing images on two color spaces viz RGB and HSV by processing the information using Residual Network −50(ResNet-50) backbone network.Extracted features of backbone network are further refined to acquire detailed aware features through Downstream Lingering Feature Pyramid Network (DsLFPN).This is further integrated with a DuoTransformer that utilizes attention mechanisms to obtain long-range dependencies. Eventually, the features are aggregated using residual connections that are dilated in-depth multiple times to produce class labels. The efficacy of the algorithm is evaluated through standard classification accuracy metrics. Experimental results on both the Wuhan Dense Labeling Dataset (WHDLD) and the Gaofen Image Dataset (GID) exhibit that the suggested technique exceeds cutting-edge classification methods. WHDLD achieved an Overall Accuracy (OA) of 86.34 %, Average Accuracy (AA) of 75.62 %, and Kappa coefficient (K) of 80.75, while the GID exhibited superior performance with an OA of 86.43 %, AA of 88.13 %, and K of 81.25.},
  archive      = {J_ASOC},
  author       = {V. Anitha and D. Manimegalai and S. Kalaiselvi},
  doi          = {10.1016/j.asoc.2024.112074},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112074},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Downstream lingering attention transformer network (DsLATNet) for land use land cover classification: A bicolor deep learning framework},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-embedding fusion network for attributed graph
clustering. <em>ASOC</em>, <em>165</em>, 112073. (<a
href="https://doi.org/10.1016/j.asoc.2024.112073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attributed graph clustering, which aims to learn embedding representation and divides nodes into different groups, has attracted increasing attention in recent years. Existing investigations have demonstrated that graph attention network (GAT) exploiting graph structure and node attributes for clustering can yield remarkable performance. However, existing GAT-based algorithms usually use adjacency matrix or feature matrix directly, neglecting the processing of noise within the feature matrix. Furthermore, some methods fail to effectively fuse different levels of embedding information for the specific clustering task. To address these deficiencies, we propose a multi-embedding fusion network for attributed graph clustering (MEFGC for short) in this paper. Specifically, in our model, a novel Laplacian filter is first designed to alleviate high-frequency noise. Secondly, we design a multi-embedding fusion module, which includes an improved auto-encoder and graph attention network, to obtain superior node embedding representation. Finally, a reliable target distribution generation method is designed, utilizing a joint supervision strategy combining self-supervision and mutual supervision to optimize the node embedding. Extensive experiments on four benchmark datasets demonstrate that the proposed MEFGC achieves state-of-the-art results in clustering tasks.},
  archive      = {J_ASOC},
  author       = {Hongtao Liu and Xianbin Lu and Kefei Cheng and Xueyan Liu},
  doi          = {10.1016/j.asoc.2024.112073},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-embedding fusion network for attributed graph clustering},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). State of health estimation for lithium-ion batteries based
on incremental capacity analysis and transformer modeling.
<em>ASOC</em>, <em>165</em>, 112072. (<a
href="https://doi.org/10.1016/j.asoc.2024.112072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important performance indicator of battery management systems, lithium-ion battery state of health (SOH) information is crucial to ensure battery safety and extend battery lifetime. Aiming at the problems of feature extraction difficulty, low accuracy of long-term prediction, and poor parallel computing capability of general data-driven methods, this paper proposes a SOH estimation method for lithium-ion batteries based on incremental capacity analysis (ICA) and Transformer. First, the original incremental capacity (IC) curve of the battery is extracted based on the ICA method, and the original IC curve is processed using the dual filtering method of moving average smoothing filter plus Gaussian smoothing filter, which in turn extracts the peak features of the curve. Then, the Transformer network model based on the multi-head attention mechanism is built. Finally, the extracted peak features of the IC curve are used as model inputs, and the Transformer model is utilized to realize the SOH estimation of lithium-ion batteries. In this paper, experiments based on different input features, prediction starting points, and ambient temperatures are conducted using experimental data of lithium-ion batteries from three sources and analyzed in comparison with commonly used machine learning methods. The experimental results show that the SOH estimation method proposed in this paper has higher long-term prediction accuracy and better temperature adaptability than commonly used machine learning methods.},
  archive      = {J_ASOC},
  author       = {Zhaofan Xu and Zewang Chen and Lin Yang and Songyuan Zhang},
  doi          = {10.1016/j.asoc.2024.112072},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112072},
  shortjournal = {Appl. Soft. Comput.},
  title        = {State of health estimation for lithium-ion batteries based on incremental capacity analysis and transformer modeling},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion prediction strategy-based dynamic multi-objective
sparrow search algorithm. <em>ASOC</em>, <em>165</em>, 112071. (<a
href="https://doi.org/10.1016/j.asoc.2024.112071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solving dynamic multi-objective optimization problems with time-varying Pareto front (PF) or Pareto set (PS) is a challenging task. Such problems require algorithms to react to environmental changes and efficiently track optimal solutions. For this purpose, a dynamic multi-objective sparrow search algorithm (SSA) with fusion prediction strategy, based on difference model and kernel extreme learning machine (DMOSSA-FPS), is proposed. Given the diversity of change characteristics, a single prediction model is insufficient. Therefore, based on the historical information of the population, a difference model and a kernel extreme learning machine are integrated for PS prediction. The former is used to predict the solutions of some individuals under approximate linear changes and the latter is employed for nonlinear predictions. In a new environment, the combined predictions increase the diversity of the initial population. Additionally, a new static optimizer is proposed, which combines decomposition- and dominance-based approaches to constitute a new individual screening mechanism. Then the optimization mode of SSA is introduced to enhance both algorithmic diversity and convergence rate. The experimental results on the DF test suite demonstrate that, compared with several other advanced algorithms, DMOSSA-FPS exhibits stronger convergence and robustness.},
  archive      = {J_ASOC},
  author       = {Rui Wu and Haisong Huang and Jianan Wei and Hefan Huang and Shixin Wang and Yunwei Zhu and Zhenggong Han and Qiang Gu},
  doi          = {10.1016/j.asoc.2024.112071},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion prediction strategy-based dynamic multi-objective sparrow search algorithm},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistency-guided semi-supervised outlier detection in
heterogeneous data using fuzzy rough sets. <em>ASOC</em>, <em>165</em>,
112070. (<a href="https://doi.org/10.1016/j.asoc.2024.112070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection aims to find objects that behave differently from the majority of the data. Semi-supervised detection methods can utilize the supervision of partial labels, thus reducing false positive rates. However, most of the current semi-supervised methods focus on numerical data and neglect the heterogeneity of data information. In this paper, we propose a consistency-guided outlier detection algorithm (COD) for heterogeneous data with the fuzzy rough set theory in a semi-supervised manner. First, a few labeled outliers are leveraged to construct label-informed fuzzy similarity relations. Next, the consistency of the fuzzy decision system is introduced to evaluate attributes’ contributions to knowledge classification. Subsequently, we define the outlier factor based on the fuzzy similarity class and predict outliers by integrating the classification consistency and the outlier factor. The proposed algorithm is extensively evaluated on 20 freshly proposed datasets. Experimental results demonstrate that COD is better than or comparable with the leading outlier detectors.},
  archive      = {J_ASOC},
  author       = {Baiyang Chen and Zhong Yuan and Dezhong Peng and Xiaoliang Chen and Hongmei Chen},
  doi          = {10.1016/j.asoc.2024.112070},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consistency-guided semi-supervised outlier detection in heterogeneous data using fuzzy rough sets},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving graph convolutional network with transformer for CT
segmentation. <em>ASOC</em>, <em>165</em>, 112069. (<a
href="https://doi.org/10.1016/j.asoc.2024.112069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and robust organ and tumour segmentation from CT scans are critical for precision diagnosis and prognosis of cancer and the development of personalised treatment planning. However, the automatic segmentation of tumours and organs they invade is challenging because of significant variations, abnormalities, and unclear boundaries. While graph convolutional networks can propagate knowledge and correlations in a flexible feature space, they suffer from information saturation during deep learning, limiting their effectiveness. To overcome this issue, we propose a hybrid graph convolution transformer (HCGT) model that consists of a channel transformer (CTrans) and a convolutional graph transformer (convG-Trans). CTrans operates along the feature channel dimension to learn contextual relationships across different feature channels. The convG-Trans learns enriched relationships among distinct elements within the image by concurrently and interactively aggregating knowledge propagation from graph convolution and cross-node similarities from the transformer. Finally, a category-level attention is designed to understand the significance of the two representations from the CTrans and convG-Trans, which help adjust the fusion process before generating the segmentation output. We evaluate the HCGT on kidney and kidney tumour, and lung and non-small cell lung cancer datasets. Our evaluations include comparisons with three-dimensional (3D) medical image segmentation benchmarks and graph- and transformer-based segmentation models. The results demonstrate improved performance in abdominal and thorax organ and tumour segmentation tasks. Additionally, ablation studies show that the major technical innovations are effective and consistent when using different 3D medical image segmentation backbones.},
  archive      = {J_ASOC},
  author       = {Hui Cui and Qiangguo Jin and Xixi Wu and Linlin Wang and Tiangang Zhang and Toshiya Nakaguchi and Ping Xuan and David Dagan Feng},
  doi          = {10.1016/j.asoc.2024.112069},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving graph convolutional network with transformer for CT segmentation},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image forgery detection by combining visual transformer with
variational autoencoder network. <em>ASOC</em>, <em>165</em>, 112068.
(<a href="https://doi.org/10.1016/j.asoc.2024.112068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the applications and artificial intelligences used for image manipulation have become quite successful. In this case, the manipulation of personal data can lead to problems of insurmountable magnitude. Such problems not only put personal data at risk, but also lead us to unethical practices, with potentially irreversible negative consequences. For this reason, the reliability of image or video data is highly questionable. To solve this challenging problem, we introduce a Visual Transformer based Visual Transformer with Variational Autoencoder Network (ViT-VAE Net) model. The model includes Visual Transformer, one of the state-of-the-art architectures. In addition to this architecture, a Variational Auto Encoder structure is also included. is much more effective than models developed with the classical Convolutional Neural Network (CNN). Unlike models developed with CNN, it can perform operations on images of any size without being bound by a standard image resolution. In addition, thanks to the self-attention mechanism in the Visual Transformer architecture, manipulations on the image are caught more easily than CNN. The ViT-VAE Net model was trained with a large dataset and tested with 4 different datasets. With a success rate of 67 % on the training dataset, the model provided promising results. Very high rates were also obtained with the test datasets.},
  archive      = {J_ASOC},
  author       = {Ilker Galip Atak and Ali Yasar},
  doi          = {10.1016/j.asoc.2024.112068},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image forgery detection by combining visual transformer with variational autoencoder network},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive metamodeling simulation optimization: Insights,
challenges, and perspectives. <em>ASOC</em>, <em>165</em>, 112067. (<a
href="https://doi.org/10.1016/j.asoc.2024.112067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pillar of Industry 4.0, Simulation Optimization is a powerful tool used across several fields, enabling system evaluation under varying conditions, facilitating performance analysis, and more efficient decision-making. On the other hand, the simulations might be time-consuming, particularly when considering complex model optimization. In this sense, metamodeling has emerged as a promising technique for simulation optimization. Metamodeling aims to establish and estimate a relationship between the inputs and outputs of a simulation model, creating a simplified model used to evaluate potential solutions during the optimization process. Metamodeling approaches can be classified as metamodeling with a fixed experimental design and adaptive metamodeling. This paper presents a systematic literature review of adaptive metamodeling in simulation optimization problems. The primary contributions of this paper are the systematic collection, examination, and discussion of knowledge disseminated in this field. We aim to support upcoming research endeavors and enhance the existing literature concerning adaptive metamodeling techniques. Our scope encompassed scientific journal papers cataloged in Scopus, Web of Science, IEEE Xplore, ACM Library, Taylor &amp; Francis, and Science Direct databases. The research questions intend to aid researchers and practitioners in summarizing prevalent contexts and methods addressed in adaptive metamodeling studies within simulation optimization problems. As a result, we discuss the main metamodels algorithms, optimization and simulation methods, acquisition functions, and sampling techniques. This paper also provides guidelines for adaptive metamodeling studies by highlighting the fundamental, suggested, and optional steps to be followed in new adaptive metamodeling applications. In the conclusion, we addressed the identified gaps, opportunities, and prospective directions about the theme.},
  archive      = {J_ASOC},
  author       = {João Victor Soares do Amaral and José Arnaldo Barra Montevechi and Rafael de Carvalho Miranda and Carlos Henrique dos Santos},
  doi          = {10.1016/j.asoc.2024.112067},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive metamodeling simulation optimization: Insights, challenges, and perspectives},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast residual attention network for fine-grained
unsupervised anomaly detection and localization. <em>ASOC</em>,
<em>165</em>, 112066. (<a
href="https://doi.org/10.1016/j.asoc.2024.112066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection has gained tremendous momentum in medical applications, with Generative Adversarial Networks (GANs) playing a pivotal role in deep anomaly detection. However, GAN-based methods may not always be effective in accurately detecting anomalies especially at the pixel-level, where finer features are necessary for accurate localization. In this paper, we propose F-UNetGAN, a novel GAN-based fast residual attention network for fine-grained anomaly detection and localization in a fully unsupervised manner. Firstly, a novel U-Net-based discriminator architecture is introduced that enables the model to learn finer details of the input image by extracting low-level features, thereby enhancing its ability to output both global and local information. We define four variants of this new U-Net discriminator. Additionally, we incorporate an encoder network to the GAN model to facilitate fast mapping from images to the latent space. Moreover, we propose new cost functions to consider the new discriminator architecture, ensuring fine-grained anomaly localization. Specifically, we introduce a per-pixel consistency regularization technique using Mixup, which enhances pixel-level details by leveraging feedback from the U-Net discriminator. Furthermore, we integrate attention modules to capture spatial and channel-specific features, improving the identification of important regions and the extraction of more intricate features. We evaluate our method on a COVID-19 dataset and validate its generalization ability on four benchmark synthetic and medical datasets. Experimental results demonstrate that the proposed method achieves more accurate anomaly localization compared to other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Najeh Nafti and Olfa Besbes and Asma Ben Abdallah and Antoine Vacavant and Mohamed Hedi Bedoui},
  doi          = {10.1016/j.asoc.2024.112066},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fast residual attention network for fine-grained unsupervised anomaly detection and localization},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-unit stacked architecture: An urban scene segmentation
network based on UNet and ShuffleNetv2. <em>ASOC</em>, <em>165</em>,
112065. (<a href="https://doi.org/10.1016/j.asoc.2024.112065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic high-accuracy semantic segmentation models typically come with a large number of parameters, making them unsuitable for deployment on driverless platforms with limited computational power. To strike a balance between accuracy and limited computational budget, and enable the use of the classic segmentation model UNet in unmanned driving scenarios, this paper proposes a multi-unit stacked architecture (MSA), namely, MSA-Net, based on UNet and ShuffleNetv2. First, MSA-Net replaces the convolution blocks in the UNet encoder and decoder with stacked basic ShuffleNetv2 units, which greatly reduces computational cost while maintaining high segmentation accuracy. Second, MSA-Net designs enhanced skip connections using pointwise convolution and convolutional block attention (CBAM) to aid the decoder in selecting more relevant and valuable information. Third, MSA-Net proposes multi-scale internal connections to extend the receptive fields of encoder and decoder with little increase in model parameters. The comprehensive experiments show MSA-Net achieves an optimal balance on the Cityscapes dataset between accuracy and model complexity, with strong generalization on the enhanced PASCAL VOC 2012 dataset. MSA-Net achieves a mean intersection over union (mIoU) of 73.6% and an inference speed of 31.0 frames per second (FPS) on the Cityscapes test dataset. We also propose two other MSA-Net models of different sizes, providing more options for resource-constrained inference.},
  archive      = {J_ASOC},
  author       = {Dian Liu and Jianchao Du and Chuhan Li and Chenglong Yu and Mingjin Zhang},
  doi          = {10.1016/j.asoc.2024.112065},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-unit stacked architecture: An urban scene segmentation network based on UNet and ShuffleNetv2},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-organizing interval type-2 function-link fuzzy neural
network control for uncertain manipulators under saturation: A
predefined-time sliding-mode approach. <em>ASOC</em>, <em>165</em>,
112064. (<a href="https://doi.org/10.1016/j.asoc.2024.112064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic manipulators have extensive applications in academic and industrial fields such as grabbing, welding, and machining. Nowadays, better tracking performance and faster transient response have been the fundamental demands in the field of manipulator control. However, robotic manipulators suffer from uncertainties due to various factors in real-world environments. Therefore, achieving satisfactory tracking performance for the manipulator in the presence of uncertainties is a pressing problem. Moreover, most existing control schemes struggle to determine the convergence time of the whole system and rely on specifically designed parameters via experience and prior knowledge, which leads to poor transferability and generalization. To address the abovementioned issues, a predefined-time controller based on a self-organizing interval type-2 function-link fuzzy neural network (IT2FLFNN) is proposed in this paper. Different from most existing fuzzy neural network (FNN) based control schemes, the proposed network starts from an empty rule base and it is constructed by a self-organizing mechanism during the online control process, which provides greater flexibility in terms of network structure and input partition. Also, a function-link network (FLN) is integrated into IT2FLFNN to enhance its convergence speed and rejection of uncertainties. Furthermore, to ensure the predefined-time convergence, a predefined-time sliding-mode control part is merged into the control framework and online learning laws for IT2FLFNN are established via the Lyapunov stability analysis. Finally, a comparative simulation and an experiment demonstrate the superiority of the proposed IT2FLFNN control scheme.},
  archive      = {J_ASOC},
  author       = {Shiyu Tian and Tao Zhao},
  doi          = {10.1016/j.asoc.2024.112064},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112064},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-organizing interval type-2 function-link fuzzy neural network control for uncertain manipulators under saturation: A predefined-time sliding-mode approach},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal data novelty detection with adversarial
autoencoders. <em>ASOC</em>, <em>165</em>, 112063. (<a
href="https://doi.org/10.1016/j.asoc.2024.112063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty detection is usually defined as the identification of new or abnormal objects (outliers) from the normal ones (inliers), which has wide potential applications including instrument fault, credit card theft warning, and disease diagnosis in the real world. Most of the existing researches focus on the novelty detection for single-modal data, which may fail to provide an accurate and reliable decision because the single-modal data sometimes cannot fully reflect the actual condition of the problem. However, research on the novelty detection from multiple sources is limited. This study developed an end-to-end deep learning architecture of novelty detection for multi-modal data based on adversarial autoencoders (AAE), which requires no input data of the novelty class during the training process. The proposed model consists of three deep networks, which are trained to compete with each other, collaborate to understand the underlying concept in the normal class, and thus classify the testing samples. Among the three networks, two of them work as the novelty detectors, and the other one called the generator will support the two novelty detectors by enhancing the inliers and distorting the outliers. In addition, a pseudo novelty mechanism is developed to expand the groups of adversarial training with the aim to greatly improve the precision and robustness of the whole model. Then the proposed model is employed for the novelty detection based on the multi-modal data that is composed of images, audios, videos, and texts, etc. The experimental results on the PKU FG-XMedia dataset and the MSR-VTT dataset reveal that our proposed method learns the normal class effectively and is superior to the baseline and state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Zeqiu Chen and Kaiyi Zhao and Ruizhi Sun},
  doi          = {10.1016/j.asoc.2024.112063},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112063},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal data novelty detection with adversarial autoencoders},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-head attention neural network with non-linear
correlation approach for time series causal discovery. <em>ASOC</em>,
<em>165</em>, 112062. (<a
href="https://doi.org/10.1016/j.asoc.2024.112062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a causal discovery model for time series analysis, which can find several causal lags of one variable over another, presenting the results by means of a temporal causal graph. The proposed model is based on multi-head attention neural networks and includes convolutional networks, allowing high prediction performance and the ability to estimate confounders. The developed framework presents an innovative methodology for validating potential causes: through a second neural network model, a direct and inverse model is trained with the potential causal lag between the causal variable under study and the target variable. In addition, another potential causal discovery method based on a non-linear correlation test is implemented. The main results show that the proposed model outperforms other causal algorithms concerning the F1 Score and achieves a considerable performance in finding temporal lags. In addition, the network fit (evaluated through the mean squared error) is significantly improved when only the causal variables predicted by the proposed algorithm (with their corresponding causal lags) are used for model training, relative to the incorporation of the entire time series dataset for the prediction of the target variable.},
  archive      = {J_ASOC},
  author       = {Nicolás Irribarra and Kevin Michell and Cristhian Bermeo and Werner Kristjanpoller},
  doi          = {10.1016/j.asoc.2024.112062},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112062},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-head attention neural network with non-linear correlation approach for time series causal discovery},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight self-supervised learning segmentation model
for variable and complex high-resolution remote sensing images.
<em>ASOC</em>, <em>165</em>, 112061. (<a
href="https://doi.org/10.1016/j.asoc.2024.112061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity and variability of high-resolution remote sensing data, such as high intra-class variability and inter-class similarity, pose significant challenges to model segmentation. To address the problem, this paper constructs a lightweight self-supervised learning model for multi-label segmentation in the form of phased learning of multi-scale features. The model adopts axial depthwise separable convolutions to reduce computational complexity and enhance feature representation, utilizes dilated rates to capture large-scale and multi-scale contextual information for long-distance feature extraction, and incorporates convolution kernels of varying sizes to acquire both local and global feature information for the improved ability of learning feature representation. The experimental results show that our model achieves competitive performance and has smaller weight parameters, memory usage, and lower computational complexity compared with existing classical models that rely on large-scale weight parameters. Additionally, our ablation study delves into the encountered design issues to elucidate the rationality of our approach. The source code is avaiable: https://github.com/zhangjy2008327/remote-sensing-images .},
  archive      = {J_ASOC},
  author       = {Ji Yong Zhang and De Guang Li and Lin Li Wu and Xin Yao Shi and Bo Wang},
  doi          = {10.1016/j.asoc.2024.112061},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112061},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight self-supervised learning segmentation model for variable and complex high-resolution remote sensing images},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A knowledge-guided regional division based evolutionary
algorithm for multi-modal multi-objective optimization. <em>ASOC</em>,
<em>165</em>, 112059. (<a
href="https://doi.org/10.1016/j.asoc.2024.112059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characteristic of multi-modal multi-objective optimization problems (MMOPs) is that multiple equivalent Pareto solution sets (PSs) in the decision space correspond to the same Pareto front (PF) in the objective space. The difficulty in solving the MMOPs lies in how to maintain the distribution in space. Many multi-modal multi-objective evolutionary algorithms (MMEAs) take convergence as the primary selection criterion, which makes it difficult for the algorithm to find all PSs in the decision space. In view of this situation, this paper proposes a partitioned knowledge-guided MMEA with multi-stage. The algorithm makes stage changes according to the proportion of evaluation consumed by the algorithm during the evolution, and adjusts the environment selection strategy as the stage changes. At the beginning of evolution, region division is carried out to prevent the solutions on each PS from interfering with each other and evolving independently. When the evaluation consumption reaches a certain proportion, it enters the middle stage. The information of the obtained solutions are used to guide the evolutionary direction of the population, and the deleted promising solutions are reclaimed. In the later stage, the steady state updating is performed to improve the distribution of population. The experimental results on four multi-modal multi-objective test suites with different features show that the proposed algorithm is more competitive than other seven excellent algorithms.},
  archive      = {J_ASOC},
  author       = {Xuanyan Lei and Yizhang Xia and Qi Deng and Juan Zou},
  doi          = {10.1016/j.asoc.2024.112059},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112059},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A knowledge-guided regional division based evolutionary algorithm for multi-modal multi-objective optimization},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Android malware detection through centrality analysis of
applications network. <em>ASOC</em>, <em>165</em>, 112058. (<a
href="https://doi.org/10.1016/j.asoc.2024.112058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Android OS is a widely-used platform for mobile devices. However, with the increasing number of Android applications and ongoing advancements in application development, there is a need for flexible and scalable malware detection methods that can address the challenges posed by big data. Recently, researchers have developed methods based on complex network analysis that aim to reduce the complexity and enhance the scalability of malware detection. These methods have shown high accuracy in identifying Android malware. Our proposed method involves generating two weighted graphs that depict the relationships between applications in benign and malware states, respectively, by extracting the functions of each application. Network-analysis-based features are then extracted from the graphs and combined with static application features to distinguish malware applications from benign ones. Our approach demonstrated an increase in accuracy, achieving 99% and 98% accuracy on the DataMD and IntDroid datasets, respectively. We further demonstrate our proposed method’s superiority against state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Aso Mafakheri and Sadegh Sulaimany},
  doi          = {10.1016/j.asoc.2024.112058},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112058},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Android malware detection through centrality analysis of applications network},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overall aerodynamic performance of the airfoils with
different amplitudes via a fuzzy decision making based taguchi
methodology. <em>ASOC</em>, <em>165</em>, 112057. (<a
href="https://doi.org/10.1016/j.asoc.2024.112057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the overall aerodynamic performance of the airfoils with variable amplitude geometric structure for lift-type vertical axis wind turbine has been analyzed and optimized using Fuzzy Analytic Hierarchy Process (AHP)-Fuzzy COmbinative Distance-based Assessment (CODAS) based on Taguchi Method. In the design of the experiment, leading edge (LE) tubercle airfoil models, Reynolds number, and angle of attack are utilized as the control factors. Drag, lift, and lift to drag ratio performances by comparing the airfoils according to the baseline model are calculated and the combination of these performances is considered as the overall aerodynamic performance response. The experiments are carried out according to the orthogonal matrix L18. Since the importance of each response is different on lift-type vertical axis wind turbines, the importance weights of these responses were obtained by Fuzzy AHP according to the evaluation of the decision maker. Multiple responses are converted into a single response (the overall aerodynamic performance) via Fuzzy CODAS which enables to model uncertainty in order to overcome the physical effects that may arise in experiments. Thereby, a multi objective Taguchi methodology was applied for the overall aerodynamic performance with a novel Fuzzy logic based multi criteria decision analysis approach. As a result, Model-4, having tubercle design parameters of amplitude of 0.025c and 0.0125c and wavelength of 0.5c and 0.125c, was found to have the best performance with the novel Taguchi approach, which saves time and cost. In addition, it has been determined that the airfoils, Reynolds number and angle of attack are significant and what level of contribution to the overall aerodynamic performance. At this point, the most important control factor has emerged as the angle of attack.},
  archive      = {J_ASOC},
  author       = {Mehmet Seyhan and Huseyin Avni Es and Mustafa Sarioglu},
  doi          = {10.1016/j.asoc.2024.112057},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112057},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Overall aerodynamic performance of the airfoils with different amplitudes via a fuzzy decision making based taguchi methodology},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing cardiac diagnostics: Exceptional accuracy in
abnormal ECG signal classification with cascading deep learning and
explainability analysis. <em>ASOC</em>, <em>165</em>, 112056. (<a
href="https://doi.org/10.1016/j.asoc.2024.112056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Arrhythmias, cardiac rhythm disorders, demand precise diagnosis for effective treatment planning, emphasizing the crucial role of electrocardiogram (ECG) signal interpretation. While deep learning excels in ECG classification, its interpretability remains a challenge. Addressing this issue, our research introduces an innovative approach employing image-based ECG representations and cascading deep neural networks (CDNNs) to enhance arrhythmia detection precision. Initiating with the transformation of 12-lead ECG time-series data, particularly focusing on Lead II, into image-based representations using relative positioning matrices (RPM), this novel conversion captures spatial and temporal information concurrently, enriching the depth of our data analysis. Subsequently, CDNNs play a pivotal role in feature extraction, designed to automatically extract impactful features from image-based ECG representations, providing profound insights into cardiac electrical activity. Extracted features serve as inputs for the subsequent deep neural network for classification. Experimental results on the Shaoxing–Chapman ECG database, encompassing over 10,000 recordings with an 80/10/10 split, 80/20 split, 60/40 split, and 10-fold cross-validation, showcase a remarkable 100% accuracy in distinguishing between seven and four different arrhythmia classes. Incorporating SHapley Additive exPlanations (SHAP) values, our methodology offers comprehensive insights into the model’s decision-making, ensuring interpretability. Additionally, Gradient-weighted Class Activation Mapping (Grad-CAM) aids feature visualization, elucidating key features driving predictions. By amalgamating image-based representation, CDNNs, and interpretability techniques, our research not only attains exceptional classification performance but also boosts model transparency and trustworthiness. These findings hold promise for advancing arrhythmia diagnosis and enhancing interpretability in deep learning-based medical applications.},
  archive      = {J_ASOC},
  author       = {Wei Zeng and Liangmin Shan and Chengzhi Yuan and Shaoyi Du},
  doi          = {10.1016/j.asoc.2024.112056},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112056},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advancing cardiac diagnostics: Exceptional accuracy in abnormal ECG signal classification with cascading deep learning and explainability analysis},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Weighted error-output recurrent xavier echo state network
for concept drift handling in water level prediction. <em>ASOC</em>,
<em>165</em>, 112055. (<a
href="https://doi.org/10.1016/j.asoc.2024.112055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water level holds utmost significance in maritime domains. Precise water level predictions furnish indispensable insights for safe maritime navigation, guiding ships and vessels through passages, harbors, and waterways. This paper introduces a novel approach: the Weighted Error-Output Recurrent Xavier Echo State Network with Adaptive Forgetting Factor (WER-XESN-AFF). One of the contributions of this study is the introduction of the Xavier weights selection method, which replaces random weight selection from the Echo State Network (ESN). This method not only enhances forecasting performance but also reduces uncertainty in predictions. Additionally, two modified concept drift detectors, the Early Drift Detection Method and the Adaptive Forgetting Factor, are employed to address concept drift challenges. Another notable contribution is the introduction of a novel weighted error-output recurrent multi-step algorithm. This algorithm successfully overcomes the error accumulation problem by using past forecast errors to update current output weights. This study performs extensive experiments to evaluate the effectiveness of our approach in multi-step prediction in synthetic and real datasets. It compares the performance between the conventional randomization-based models and the ESN with the new weights selection approach and also tests the ability of concept drift detectors and the weighted error-output multi-step algorithm. Empirical findings and statistical analyses demonstrate that our proposed methods achieve expected effects, and the proposed model has better prediction ability than baselines. A significant improvement rate of 75.39% in Mean Squared Error is evident within the Jiujiang water level dataset when contrasting the performance of WER-XESN-AFF against the baseline model R-ESN across the 1–5 period.},
  archive      = {J_ASOC},
  author       = {Zongying Liu and Wenru Zhang and Mingyang Pan and Chu Kiong Loo and Kitsuchart Pasupa},
  doi          = {10.1016/j.asoc.2024.112055},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112055},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted error-output recurrent xavier echo state network for concept drift handling in water level prediction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A three-level nested portfolio optimization model with
position allocation. <em>ASOC</em>, <em>165</em>, 112054. (<a
href="https://doi.org/10.1016/j.asoc.2024.112054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing portfolio optimization models cannot well capture the real position allocation requirement, leading to limited impact in practice. To overcome this challenge, we propose a three-level nested portfolio optimization model with position allocation. Within this model, the inner and middle levels collaboratively determine the optimal portfolio, while the outer level focuses on optimizing the holding proportion of each stock in the optimal portfolio. Compared with existing models with portfolio weights, the proposed model imposes the position allocation constraint that precisely characterizes the limitations on holding each stock. This constraint is crucial for investors to obey securities trading regulations involving position limitations and to mitigate the potential impact of market risks. To address the nonlinear and nonconvex nature of the novel model, we develop an intelligent optimization algorithm by effectively hybridizing the support vector regression and the enhanced grey wolf optimizer. We comprehensively evaluate its performance using eight metrics, including accumulative return, annual return, Sharpe ratio, maximum drawdown, absolute and relative win ratios, predictive precision and accuracy. The experimental results indicate that (i) the proposed model can achieve more excess returns than those stock selection models not considering position allocation, especially for the large-cap stocks; (ii) compared with other state-of-the-art meta-heuristics, the enhanced grey wolf optimizer can yield better portfolio in conjunction with the support vector regression; (iii) in the context of the Chinese A-share stock market, specific financial indicators such as return on equity, inventory turnover rate, net income growth rate, and debt-to-equity ratio should be given greater consideration compared to other financial metrics.},
  archive      = {J_ASOC},
  author       = {Jie Ma and Kexin Yang and Kaiping Luo and Ping Li and Ankang He},
  doi          = {10.1016/j.asoc.2024.112054},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112054},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-level nested portfolio optimization model with position allocation},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining machine learning techniques and genetic algorithm
for predicting run times of high performance computing jobs.
<em>ASOC</em>, <em>165</em>, 112053. (<a
href="https://doi.org/10.1016/j.asoc.2024.112053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a novel approach combining Machine Learning (ML) techniques and Genetic Algorithms (GA) for predicting High-Performance Computing (HPC) job run times. The objective is to create a prediction method universally applicable to any HPC system, irrespective of workload characteristics, application specific parameters, user behavior, or hardware architecture. Since user-supplied run time estimates are often inaccurate, we aim to categorize job runtimes into several classes, allowing users to select appropriate classes for their jobs. A Genetic Algorithm is developed to optimally define these runtime classes, determining both the number of classes and the time intervals represented by them. Four Machine Learning algorithms (K Nearest Neighbours, Support Vector Regression, Extreme Gradient Boosting and Deep Neural Networks) are implemented for run time prediction. A unique set of features extracted from historical job data serves as input to the Machine Learning models. The generalized nature of our method is demonstrated by validating its performance on data from six clusters with distinct configurations, applications and runtime distributions. Our results illustrate the superior performance of Machine Learning models incorporating GA-defined runtime classes for all datasets. Across all six datasets, our method achieves R2 scores exceeding 0.8, and accuracy greater than 0.7.},
  archive      = {J_ASOC},
  author       = {Suja Ramachandran and M.L. Jayalal and M. Vasudevan and Sourish Das and R. Jehadeesan},
  doi          = {10.1016/j.asoc.2024.112053},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112053},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining machine learning techniques and genetic algorithm for predicting run times of high performance computing jobs},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). 2D spectrogram analysis using vision transformer to detect
mispronounced arabic utterances for children. <em>ASOC</em>,
<em>165</em>, 112052. (<a
href="https://doi.org/10.1016/j.asoc.2024.112052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pronunciation feedback is essential for teaching languages to children, urging the need to create computer-assisted pronunciation training (CAPT) systems to automate this process. Most of the current CAPT systems for Arabic Language examined a wide age range, including children, with a dominant adults’ portion. However, these systems did not consider many challenges facing children’s voice, in terms of the acoustic difference between children and adults. Moreover, due to the lack of publicly Arabic datasets, Arabic-related systems were evaluated on datasets consisting of the sound of Arabic letters, with minor efforts to contain words or sentences. In this paper, we propose the Arabic Utterance Mispronunciation Detector for Children (AUMD-Child) system that detects the mispronunciation of children for Arabic words at real-time. The proposed system is based on the fusion of Vision Transformer (ViT) and transfer learning-based model (AlexNet). A novel public dataset is constructed, in which 16 words have been collected from children aged between 7 and 12 years old to evaluate our proposed system. The experimental results, compared to the state-of-the-art models in literature, showed that the accuracy of our proposed AUMD-Child system exceeded the handcrafted features-based models by an average of 7 %, the CNN features-based models using AlexNet and SVM by 5 %, transfer learning-based model using AlexNet by 4 %, and transformers-based model using ViT by 2 %, achieving an average accuracy of 91.81 % for detecting the mispronunciation patterns in Arabic words but in an average processing time of 33 ms, exceeding the other state-of-the-art models with 13 %, 0.03 %, 0.06 %, and 23 % respectively.},
  archive      = {J_ASOC},
  author       = {Mona A. Sadik and Ahmed S. ElSayed and Sherin M. Moussa and Z.T. Fayed},
  doi          = {10.1016/j.asoc.2024.112052},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112052},
  shortjournal = {Appl. Soft. Comput.},
  title        = {2D spectrogram analysis using vision transformer to detect mispronounced arabic utterances for children},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust online active learning with cluster-based local drift
detection for unbalanced imperfect data. <em>ASOC</em>, <em>165</em>,
112051. (<a href="https://doi.org/10.1016/j.asoc.2024.112051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of data-driven technologies, a massive amount of actual data emerges from industrial systems, forming data stream. Their data distribution may change over time and outliers may be generated as unbalanced imperfect data due to time-varying working condition, aging equipment, etc. Previous methods struggle with the dual challenges of concept drift and unbalance, however, fail to efficiently distinguishing outliers from a drift under the limited labeling budget, causing the performance degradation. To address the issue, robust online active learning with cluster-based local drift detection is proposed to classify unbalanced imperfect data stream with the above characteristics. The cluster-based local drift detection is first designed to capture a new concept and recognize the corresponding drifted regions. Following that, an improved active learning mechanism is presented to distinguish outliers from a drift, and select most valuable instances for labeling and updating ensemble classifier. Experimental results for eight synthetic and four real-world data streams show that the proposed method outperforms seven comparative methods on classification accuracy and robustness.},
  archive      = {J_ASOC},
  author       = {Yinan Guo and Zhiji Zheng and Jiayang Pu and Botao Jiao and Dunwei Gong and Shengxiang Yang},
  doi          = {10.1016/j.asoc.2024.112051},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112051},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust online active learning with cluster-based local drift detection for unbalanced imperfect data},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging GANs data augmentation for imbalanced medical
image classification. <em>ASOC</em>, <em>165</em>, 112050. (<a
href="https://doi.org/10.1016/j.asoc.2024.112050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms have been widely applied and researched in the field of medical image classification. Most of the current algorithms are designed based on the assumption of data balance. However, in practical applications, medical data often face the problem of imbalanced data. The traditional data augmentation approaches can balance the original data, but they are unable to generate more effective features. Generative Adversarial Networks (GANs) can effectively perform sample augmentation. However, GAN often encounter issues such as intra-class mode collapse and generating noisy samples when faced with imbalanced data. This study proposes a GAN model called IBGAN that focuses on generating intra-class sparse samples and class boundary samples. Specifically, IBGAN addresses the issue of imbalanced data through two stages. In the first stage, the Isolation Forest (iForest) algorithm and boundary sample detection algorithm are designed to identify sparse region samples and boundary samples within the intra-class data. This enables the GAN model to effectively focus on generating such samples during training. The second stage involves the refinement process of the samples. We propose a sample evaluation method based on Support Vector Data Description (SVDD) to filter out noisy data in the generated samples, ensuring the quality of the generated data. We conducted extensive and in-depth experiments on five real-world datasets. The experimental results demonstrate that IBGAN can generate high-quality and diverse augmented samples, which contribute to the improvement of classifier performance. We compared our proposed method with seven baseline methods, including traditional approaches and classic GAN models. The experimental results, including the visualization of the generated data and the evaluation of classification performance, consistently show that our proposed method achieves more competitive results.},
  archive      = {J_ASOC},
  author       = {Hongwei Ding and Nana Huang and Xiaohui Cui},
  doi          = {10.1016/j.asoc.2024.112050},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112050},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Leveraging GANs data augmentation for imbalanced medical image classification},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adopting artificial intelligence algorithms for remote fetal
heart rate monitoring and classification using wearable fetal
phonocardiography. <em>ASOC</em>, <em>165</em>, 112049. (<a
href="https://doi.org/10.1016/j.asoc.2024.112049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fetal phonocardiography (FPCG) is a non-invasive Fetal Heart Rate (FHR) monitoring technique that can detect vibrations and murmurs in heart sounds. However, acquiring fetal heart sounds from a wearable FPCG device is challenging due to noise and artefacts. This research contributes a resilient solution to overcome the conventional issues by adopting Artificial Intelligence (AI) with FPCG for automated FHR monitoring in an end-to-end manner, named (AI-FHR). Four sequential methodologies were used to ensure reliable and accurate FHR monitoring. The proposed method removes low-frequency noises and high-frequency noises by using Chebyshev II high-pass filters and Enhanced Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (ECEEMDAN) in combination with Phase Shifted Maximal Overlap Discrete Wavelet Transform (PS-MODWT) filters, respectively. The denoised signals are segmented to reduce complexity, and the segmentation is performed using multi-agent deep Q-learning (MA-DQL). The segmented signal is provided to reduce the redundancies in cardiac cycles using the Artificial Hummingbird Optimization (AHBO) algorithm. The segmented and non-redundant signals are converted into 3D spectrograms using a machine learning algorithm called variational auto-encoder-general adversarial networks (VAE-GAN). The feature extraction and classification are carried out by adopting a hybrid of the bidirectional gated recurrent unit (BiGRU) and the multi-boosted capsule network (MBCapsNet). The proposed method was implemented and simulated using MATLAB R2020a and validated by adopting effective validation metrics. The results demonstrate that the proposed method performed better than the current method with accuracy (81.34%), sensitivity (72%), F1-score (83%), Energy (0.808 J), and complexity index (13.34). Like other optimization methods, AHO needs precise parameter adjustment in order to function well. Its performance may be greatly impacted by the selection of parameters, including population size, exploration rate, and learning rate.},
  archive      = {J_ASOC},
  author       = {Radha Abburi and Indranil Hatai and Rene Jaros and Radek Martinek and Thirunavukkarasu Arun Babu and Sharmila Arun Babu and Sibendu Samanta},
  doi          = {10.1016/j.asoc.2024.112049},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112049},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adopting artificial intelligence algorithms for remote fetal heart rate monitoring and classification using wearable fetal phonocardiography},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised adversarial and cycle consistent feature
extraction network for intelligent fault diagnosis. <em>ASOC</em>,
<em>165</em>, 112048. (<a
href="https://doi.org/10.1016/j.asoc.2024.112048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine failures often arise from cumulative aging and anomalies, with early-warning indicators hidden in sensor-collected data. When high-quality labeled data are scarce for determining machine conditions, such failures can lead to performance decline and unforeseen incidents. To address these challenges, an unsupervised adversarial and cycle-consistent feature extraction network is presented, named CycleFeature. Specifically engineered for mechanical fault diagnosis, CycleFeature excels in scenarios characterized by limited labeled data and progressive machine aging. It integrates four crucial components: a sample feature extractor, a noise feature extractor, a generator, and a discriminator. This architecture employs multi-layer perceptions and convolutional neural networks. The training protocol for CycleFeature comprises two discrete phases: pre-training and integrated training. In the pre-training phase, the network collaboration ensures that the extracted features maintain cycle consistency and accurately represent the sample data. In the integrated training phase, the noise feature extractor captures features from noise and sample data. Through adversarial learning involving the sample feature extractor and the discriminator, the model fine-tunes the generated noise features to closely mirror the sample features. Tests are conducted based on the early health and pre-retirement high-risk data from 20 normally retired and 4 accident-retired vehicles. CycleFeature shows excellent prediction accuracy in the early warnings of these cases. It garnered higher frequency and value scores on the pre-retirement high-risk data from 24 vehicles compared to early health data, robustly confirming its accuracy and effectiveness.},
  archive      = {J_ASOC},
  author       = {Wang Yi-Die and Chao Pei-Pei and Zhang Rui-Yuan and Hong Tang and Wei Yu-Cheng and Dai Hong-Liang},
  doi          = {10.1016/j.asoc.2024.112048},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112048},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised adversarial and cycle consistent feature extraction network for intelligent fault diagnosis},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chaotic rao3 based adaptive neuro-fuzzy inference system to
solve global infrastructure project selection problem. <em>ASOC</em>,
<em>165</em>, 112046. (<a
href="https://doi.org/10.1016/j.asoc.2024.112046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global infrastructure project (GIP) selection for any organization is considered to be a complex decision-making process as it involves multiple conflicting criteria and uncertainties. To resolve the issues concerned with GIP selection, in this paper, we propose an improved chaotic Rao3 (CRao3 ) based adaptive neuro-fuzzy inference system (ANFIS), which assists organizations in terms of expected economic benefits and return on investments. Our proposal is mainly two-fold. Firstly, we propose chaotic Rao3 (CRao3 ) optimization by integrating chaotic theory with Rao3 optimization technique to enhance the exploration and exploitation capabilities of Rao3 . Next, the proposed CRao3 is integrated with ANFIS to optimize the premise and consequent parameters to reduce the training error for providing the predicted values of the projects. Then, we rank the projects based on the uncertainty bands and a ranking measure to select the best global infrastructure projects. For the demonstration purpose, this study has considered two GIP related datasets and evaluated the performance of the proposed CRao3based ANFIS model (CRao3-ANFIS) by various error measures such as root mean squared error (RMSE), mean absolute percentage error (MAPE), R 2 R2 correlation coefficient and pearson’s correlation coefficient. Finally, the results of the proposed model are compared with the existing models, where the proposed CRao3-ANFIS model outperforms the existing ones in terms of accuracy and efficiency with a low error rate and better correlation. Therefore, the proposed model is found to be suitable for GIP selection process.},
  archive      = {J_ASOC},
  author       = {G Punnam Chander and Sujit Das},
  doi          = {10.1016/j.asoc.2024.112046},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112046},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Chaotic rao3 based adaptive neuro-fuzzy inference system to solve global infrastructure project selection problem},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-tuning multi-layer optimization algorithm (STML): An
innovative parameter-less approach. <em>ASOC</em>, <em>165</em>, 112045.
(<a href="https://doi.org/10.1016/j.asoc.2024.112045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational intelligence (CI)-based methods offer a practical approach to overcoming the significant challenges posed by analytical and enumeration optimization methods when dealing with complex real-world problems. However, a notable drawback of these algorithms is the need for time-consuming and computationally demanding fine-tuning procedures to achieve optimal performance. This paper proposes a novel parameterless auto-tuning meta-heuristic architecture called the self-tuning multi-layer (STML). The fundamental concept behind this architecture involves a multi-layer structure where the inner layer optimizes the main problem. In contrast, the outer layer utilizes information obtained during the search to fine-tune the performance of the inner layer. This feature eliminates manual fine-tuning, as it can autonomously handle this task. A series of mathematical and benchmark problems were employed to demonstrate the computational prowess of the STML. The results indicate its superiority over other meta-heuristic algorithms. Additionally, the STML showcases robustness, as evidenced by the numerical proximity of results obtained from different independent runs on these benchmark problems.},
  archive      = {J_ASOC},
  author       = {Babak Zolghadr-Asli and Milad Latifi and Ramiz Beig Zali and Mohammad Reza Nikoo and Raziyeh Farmani and Rouzbeh Nazari and Amir H. Gandomi},
  doi          = {10.1016/j.asoc.2024.112045},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112045},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-tuning multi-layer optimization algorithm (STML): An innovative parameter-less approach},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A second-order projection neurodynamic approach with
exponential convergence for sparse signal reconstruction. <em>ASOC</em>,
<em>165</em>, 112044. (<a
href="https://doi.org/10.1016/j.asoc.2024.112044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a class of second-order neurodynamic approaches with convergence rates of O ( 1 t ) O(1t) or O ( 1 t 2 ) O(1t2) has been developed to address the sparse signal reconstruction problem. In this paper, we propose a second-order projection neurodynamic approach (SOPNA) with exponential convergence to reconstruct a sparse signal by solving a modified inverted Gaussian function (MIGF) minimization problem. The existence, uniqueness, and feasibility of the solution to SOPNA are detailedly investigated, and the exponential convergence rate of O ( exp ( − μ t ) ) , μ &gt; 0 O(exp(−μt)),μ&amp;gt;0 is proved. This implies that our proposed SOPNA can achieve a significantly superior convergence performance than several existing second-order neurodynamic approaches. Numerical experiments also confirm its effectiveness and superiority. Finally, the applications of the proposed SOPNA in real signal and real image reconstructions validate its practical feasibility.},
  archive      = {J_ASOC},
  author       = {Chunhao Han and Jiao Xu and Bing Zheng},
  doi          = {10.1016/j.asoc.2024.112044},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A second-order projection neurodynamic approach with exponential convergence for sparse signal reconstruction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical learning multi-objective firefly algorithm for
high-dimensional feature selection. <em>ASOC</em>, <em>165</em>, 112042.
(<a href="https://doi.org/10.1016/j.asoc.2024.112042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial data preprocessing technique extensively employed in machine learning and image processing. However, feature selection encounters significant challenges when addressing high-dimensional data due to the huge and discrete decision space. This paper proposes a hierarchical learning multi-objective firefly algorithm (HMOFA) for solving the feature selection task in high-dimensional data. The main contributions are as follows: 1) Features are clustered based on the evaluation of multiple metrics, which are used to initialize the population and improve the quality of the initial population; 2) A hierarchy-guided learning model is proposed, where individuals move toward superior solutions while moving away from inferior solutions, avoiding the oscillation phenomenon that occurs under the full attraction model, and reducing the likelihood of the population being trapped in a local optimum; 3) Use duplicate solution modification mechanism to reduce the number of duplicate individuals in the population. The proposed method is compared with 8 competitive feature selection methods using 15 datasets, and the results demonstrate that HMOFA can achieve higher classification accuracy while selecting fewer features.},
  archive      = {J_ASOC},
  author       = {Jia Zhao and Siyu Lv and Renbin Xiao and Huan Ma and Jeng-Shyang Pan},
  doi          = {10.1016/j.asoc.2024.112042},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical learning multi-objective firefly algorithm for high-dimensional feature selection},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Redundancy allocation problem in repairable k-out-of-n
systems with cold, warm, and hot standby: A genetic algorithm for
availability optimization. <em>ASOC</em>, <em>165</em>, 112041. (<a
href="https://doi.org/10.1016/j.asoc.2024.112041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a single-objective redundancy allocation problem (RAP) is considered for a series system with k -out-of- n subsystems. In each subsystem, the components are binary, homogeneous, and repairable. Component failure and repair processes are independent and exponentially distributed. Three standby modes of systems are considered: cold, warm, and hot. Assuming a constant number of components in the entire system, component allocation aims to maximize system availability. A continuous-time Markov chain (CTMC) model is developed to calculate system availability. A genetic algorithm (GA) is developed to optimize the system configuration. A new way of encoding chromosomes as a sequence of binary numbers that represents the priority of the subsystem in component allocation is proposed. This method achieves feasible solutions through crossover and mutation for the entire population in each generation. First, using a hybrid of GA and CTMC, research is conducted on the solution of RAP in a series system with 10 k -out-of- n subsystems. Optimal system configurations are determined for a total number of components equal to 80, 90, 100, 110, 120, and 130. Then, a sensitivity analysis is performed to determine the impact of the model parameters and the standby mode on system availability. Finally, an analysis of performance and computation time is performed depending on the parameters of the genetic algorithm. The results demonstrate the high efficiency of the GA-CTMC algorithm in solving RAP in complex technical systems. Less than 0.01 % of feasible solutions are estimated to find an optimal solution. Furthermore, GA-CTMC is distinguished by a short computation time compared to other algorithms in the literature.},
  archive      = {J_ASOC},
  author       = {Mateusz Oszczypała and Jarosław Ziółkowski and Jerzy Małachowski},
  doi          = {10.1016/j.asoc.2024.112041},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Redundancy allocation problem in repairable k-out-of-n systems with cold, warm, and hot standby: A genetic algorithm for availability optimization},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Multitask differential evolution with adaptive dual
knowledge transfer. <em>ASOC</em>, <em>165</em>, 112040. (<a
href="https://doi.org/10.1016/j.asoc.2024.112040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of multitasking optimization (MTO) is to handle multiple tasks simultaneously. In MTO, effective knowledge transfer (KT) techniques significantly influence the performance of multitasking evolutionary algorithms (MTEAs). These techniques vary in their impact, and by assigning suitable techniques to individuals, algorithms can leverage them to enhance overall performance. With this purpose, we propose MTDE-ADKT, a novel MTEA integrating adaptive dual knowledge transfer and improved differential evolution. The MTDE-ADKT introduces several key innovations: Firstly, a novel domain adaptation (DA)-based KT technique rooted in transfer learning is proposed. Secondly, the DA-based KT technique is integrated with the traditional unified search space-based KT technique. This integration dynamically adjusts the probability allocation for each KT technique, tailoring it to suit the specific needs of each task. Thirdly, a new mutation strategy for offspring generation is presented, facilitating genetic material exchange among different tasks. The experimental results show that MTDE-ADKT outperforms 18 state-of-the-art algorithms on two MTO benchmark suites, a many-task optimization benchmark suite, and two real-world applications.},
  archive      = {J_ASOC},
  author       = {Tingyu Zhang and Wenyin Gong and Yanchi Li},
  doi          = {10.1016/j.asoc.2024.112040},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multitask differential evolution with adaptive dual knowledge transfer},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient image super-resolution based on transformer with
bidirectional interaction. <em>ASOC</em>, <em>165</em>, 112039. (<a
href="https://doi.org/10.1016/j.asoc.2024.112039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In single-image super-resolution (SISR) tasks, many methods benefit from the local and global contexts of the image. Despite that, no methods use the bidirectional interaction between these two contexts. So, we were inspired by the fully adaptive Transformer for high-level vision. We propose a fully adaptive Transformer super-resolution (FATSRN) for SISR. The model uses local and global information and their bidirectional interaction in a context-aware manner. The model is based on fully adaptive self-attention (FASA) as the main block, which uses self-modulated convolutions to extract local representation adaptively. Also, the FASA uses self-attention in down-sampled space to extract global representation. In addition, this FASA uses a bidirectional adaptation process between local and global representation to model the interaction. Moreover, a fine-grained downsampling strategy is used to improve the down-sampled self-attention mechanism. Based on the FASA, we built a fully adaptive self-attention block (FASAB) as the main block of our model. Then, the fully adaptive self-attention group (FASAG) is used as the backbone for our FATSRN. Extensive experiments are done to show the efficiency of the model against the state-of-the-art methods. For example, our model improved the PSNR from 27.69 to 27.73 compared to the SwinIR-light for the B100 dataset at the scale of × × 4. In addition, our model achieved 0.04 dB better PSNR compared to the state-of-the-art STSN model for the Set5 dataset at the scale of × × 2 with 64% and 48% fewer parameters and Mult-adds.},
  archive      = {J_ASOC},
  author       = {Garas Gendy and Guanghui He and Nabil Sabor},
  doi          = {10.1016/j.asoc.2024.112039},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient image super-resolution based on transformer with bidirectional interaction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective multi-population simplified swarm
optimization for container loading optimization with practical
constraints. <em>ASOC</em>, <em>165</em>, 112038. (<a
href="https://doi.org/10.1016/j.asoc.2024.112038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container loading problem (CLP) holds significant importance in logistics and supply chain management due to its direct influence on transportation costs and times, thereby impacting the competitive advantages of enterprises across industries. Although there are existing studies for CLP, there remains a gap in addressing the optimization of multiple objectives while satisfying practical constraints. Moreover, container loading methods must also meet performance requirements such as high computational speed, explainability, and implementation capability. Although meta-heuristic-based algorithms have shown effective computational capabilities and performance, such algorithms often being trapped in the local optima especially when searching in the vast solution space of the CLP, particularly as the quantity and diversity of cargo sizes and shapes increase. Motivated by realistic needs, this study aims to develop a UNISON-based framework that integrates the merging spaces algorithm, a novel simple but effective hybrid simplified swarm optimization genetic algorithm (SSO-GA), and a multi-populations co-evolution strategy to determine the loading sequence of parcels to maximize space utilization and weight balance while satisfying to practical constraints. Specifically, the merging spaces algorithm merges fragmented small spaces resulting from loaded parcels into larger spaces, thereby facilitating the accommodation of additional parcels. The hybrid SSO-GA splits encoded solutions and updates segment using novel strategies resulting in the rearrangement of a group of parcels and their corresponding layouts for better space utilization. Furthermore, the multi-populations co-evolution strategy enhances the diversity of the search space and stabilizes solution quality by simultaneously exploiting the best solutions and exploring alternative solutions during the solution update process. An empirical study was conducted by using a public benchmark dataset which demonstrated the practical effectiveness of the proposed framework by significantly improving average space utilization while ensuring center balance and satisfying practical constraints. Furthermore, this study can also serve as a digital support system capable of assisting decision-makers in optimizing container loading operations, thereby improving productivity and saving valuable time.},
  archive      = {J_ASOC},
  author       = {Linh-Hoang Truong and Chen-Fu Chien},
  doi          = {10.1016/j.asoc.2024.112038},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective multi-population simplified swarm optimization for container loading optimization with practical constraints},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Position-based focal loss for diverse and relevant response
generation. <em>ASOC</em>, <em>165</em>, 112037. (<a
href="https://doi.org/10.1016/j.asoc.2024.112037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response generation models trained with cross entropy loss suffer from over-general responses due to their preference for high-frequent tokens. Focal loss and anti-focal loss are candidates to solve this problem, but they have their own limitation that they exaggerate only one of relevancy or diversity of responses. Therefore, this paper proposes two novel losses of positional focal loss and adaptive positional focal loss which emphasize relevancy or diversity flexibly according to the position of a target token. The positional focal loss introduces a position function as a weight to the token position, but it tends to underestimate the relevancy for low confident predictions. To tackle this problem, the adaptive positional focal loss balances relevancy and diversity by limiting the effect of over-confident predictions.},
  archive      = {J_ASOC},
  author       = {So-Eon Kim and Seong-Bae Park},
  doi          = {10.1016/j.asoc.2024.112037},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112037},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Position-based focal loss for diverse and relevant response generation},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spiral-refraction mutation prairie dog algorithm:
Optimization framework for engineering design of interconnected
multimachine power system. <em>ASOC</em>, <em>165</em>, 112036. (<a
href="https://doi.org/10.1016/j.asoc.2024.112036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many real-world engineering problems, characterized by high-dimensionality, nonlinearity, nonconvexity, and multi-modality, demand advanced optimization methods. Traditional algorithms may struggle with these challenges. Prairie dog optimization (PDO) was proposed for function optimization but faces limitations in balancing exploration and exploitation due to two reasons. The first one is related to diversity features which may not be efficient, and the second one is related to having no leading mechanism for direct search feature towards the promising regions. With that in mind, this study introduces an enhanced PDO variant, improved PDO (IPDO), addressing PDO&#39;s drawbacks through two key strategies: refraction-based learning (RBL) and spiral search learning (SSL). RBL enhances exploration by generating refraction solutions, while SSL conducts deep local searches around the best solution, strengthening exploitation. IPDO&#39;s performance is evaluated on benchmarks, IEEE CEC2017/CEC2020 test suites, and real-world constraint engineering optimization problems. Comparative analyses, including statistical measures, convergence analysis, and boxplot analysis, demonstrate IPDO&#39;s superiority. Besides, the IPDO is applied to estimate more promising parameters of power system stabilizer utilized in an interconnected multimachine power system using Western System Coordinating Council three-machine, nine-bus power system. Results illustrate that the IPDO harvests the better estimation among the other methods, making it to be an efficient and powerful tool for dealing with the efficient operation of an interconnected multimachine power system which is a challenging real-world engineering problem.},
  archive      = {J_ASOC},
  author       = {Rizk M. Rizk-Allah and Václav Snášel and Davut Izci and Serdar Ekinci},
  doi          = {10.1016/j.asoc.2024.112036},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112036},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spiral-refraction mutation prairie dog algorithm: Optimization framework for engineering design of interconnected multimachine power system},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Covering of fuzzy graphs and its application in emergency
aircraft landing using particle swarm optimization method.
<em>ASOC</em>, <em>165</em>, 112035. (<a
href="https://doi.org/10.1016/j.asoc.2024.112035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In graph theory, a set consisting of vertices of a graph that are incident to at least one of the edges is called a vertex covering set for that fuzzy graph. Facility location problems are represented as fuzzy graphs, and a model is designed with multi-objective optimization programming problems. These problems are solved using the Particle Swarm Optimization approach combined with the covering concept of fuzzy graphs. An algorithm is designed for finding fuzzy vertex covering set of fuzzy graphs. The definitions of covering speed, covering time, and coverage impact for a fuzzy vertex cover are introduced and used to develop the model. This model uses a fuzzy graph with vertices as demand and facility nodes. In case of a sudden change in the total demand of the system, there is a change in the fuzzy covering radius or capacity of facility nodes. The problem is to cover up the fuzzy network by placing facilities with maximizing demand and optimizing unknown fuzzy parameters. These studies solve a real-life problem: emergency aircraft landing with minimum time and nearest landing place. Also, the method minimizes the loss of aircraft and passengers. The proposed methodology is a new approach to solving such complex problems.},
  archive      = {J_ASOC},
  author       = {Anushree Bhattacharya and Madhumangal Pal},
  doi          = {10.1016/j.asoc.2024.112035},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112035},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Covering of fuzzy graphs and its application in emergency aircraft landing using particle swarm optimization method},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced domain transfer deep fuzzy echo state network for
rotating machinery fault diagnosis based on current signal.
<em>ASOC</em>, <em>165</em>, 112033. (<a
href="https://doi.org/10.1016/j.asoc.2024.112033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although vibration-based fault diagnosis methods have achieved remarkable results in rotating machinery, they are still limited by various factors, such as environment noise and additional sensor installation. Meanwhile, due to inevitable distribution differences in practical engineering, deep transfer learning models are commonly used in fault diagnostic fields. Although deep architectures can extract representative domain-invariant features, they typically result in substantial computational burdens and training time. Therefore, to address the above problems, an enhanced domain transfer deep fuzzy echo state network (EDFESN) is proposed for rotating machinery fault diagnosis based on current signals. Firstly, current signals derived from the driving motor are directly collected without additional sensors. Secondly, an enhanced domain mapping method is raised to solve the problem of data distribution discrepancies between cross domain. Furthermore, a deep fuzzy echo state network (DFESN) integrated with fuzzy clustering is constructed, which utilizes layer-wise fuzzy-tuning learning paradigm instead of backpropagation step to reduce training time. Two datasets from gearbox and bearing both verify the effectiveness of the proposed method. Compared with other popular transfer methods, EDFESN not only achieves highest accuracy, but also has fastest training speed.},
  archive      = {J_ASOC},
  author       = {Fei Jiang and Weiqi Lin and Shaohui Zhang and Zhaoqian Wu and Jie Han and Weihua Li},
  doi          = {10.1016/j.asoc.2024.112033},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced domain transfer deep fuzzy echo state network for rotating machinery fault diagnosis based on current signal},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of circle search algorithm for solar PV maximum
power point tracking under complex partial shading conditions.
<em>ASOC</em>, <em>165</em>, 112030. (<a
href="https://doi.org/10.1016/j.asoc.2024.112030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solar photovoltaic (SPV) energy exhibits a potential role in the world for generating electricity and avoiding the block out in the power system. However, the hardest task in the PV system is to track the global maxima power (GPP) instead of local peak power (LPP) under partial shading conditions (PSC). To track the GPP, a new circle search algorithm (CSA) is implemented in this paper and is executed in MATLAB/SIMULINK environment. This method is examined at various irradiation conditions and compared their performance with other studied methods in terms of tracking time, settling time and efficiency. The simulation outcome of the CSA method efficiencies 99.96 % in case 1, 99.74 % in case 2, and 99.94 % in case 3. For checking the robustness and stability of the system statistical analysis was done on the proposed methods. It is found that the suggested method exhibits higher performance than the other methods.},
  archive      = {J_ASOC},
  author       = {Dokala Janandra Krishna Kishore and Mohd Rusllim Mohamed and Kumarasamy Sudhakar and Kurukuri Peddakapu},
  doi          = {10.1016/j.asoc.2024.112030},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of circle search algorithm for solar PV maximum power point tracking under complex partial shading conditions},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental-based YoloV3 model with hyper-parameter
optimization for product image classification in e-commerce sector.
<em>ASOC</em>, <em>165</em>, 112029. (<a
href="https://doi.org/10.1016/j.asoc.2024.112029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, the E-commerce industry has grown tremendously for selling products to consumers. Here, the consumer can easily purchase the products from their residing seats and gets the products at the door step. Also, the image quality is suggested and also the image classification is intruded to meet the consumer prerequisite. Most of the image classification models are developed by machine learning approaches to improve the reliability, scalability, and accuracy level. Due to the heterogeneous nature of images, it restricts the features and lengthy dimensions for datasets which it degrades the performance whereas the classification of products becomes a cumbersome situation. In order to resolve the problem raised by existing methods, a novel product image classification model is proposed for the fashion E-commerce sector. The prime aspect of the proposed model is to build an effective E-commerce sector for a layman, where every individual can purchase the products appropriately. Initially, the dataset is constructed with diverse product images that are obtained from different E-commerce fields with various classes. Consequently, a novel Incremental-based Improved YoloV3 with Hyper-parameter Optimization (II-Yolov3-HO) model is introduced to enhance the classification performance. In the proposed model, some of the hyper parameters like convolution and pooling layers are replaced with the Visual Geometry Group16 (VGG16) model, the remaining convolutional layers are enhanced by the atrous convolution operation, and pooling layers are modified with Atrous Spatial Pyramid Pooling (ASPP). In entire architecture, the weight factor is optimized using Predator-based Squirrel Search Algorithm (PDP-SSA) algorithm. Thus, the new E-commerce sector becomes free to categorize the products, and if it deals with a new product, it is declared as unknown data, which is then trained again in the proposed model. Hence, the concept of classification score matching with the threshold value is accomplished to deal the untrained product and the training weight is updated optimally using the proposed PDP-SSA. Finally, the enhanced model is compared over the state-of-the-art classification, provides the expected result for significantly classifying the multiple classes of products. The developed PDP-SSA-II-YoloV3-HO model is also able to perform recommendations in E-commerce sites even when the input is given as images other than text for searching the items. These uploaded images are analyzed in the developed II-Yolov3-HO model for providing the relevant items to the customer based on their features. The developed PDP-SSA-II-YoloV3-HO model achieves 99.46 % regarding accuracy. Throughout the validation, the developed model outperforms enhanced and reliable performance than the existing approaches.},
  archive      = {J_ASOC},
  author       = {Munmi Dutta and Amrita Ganguly},
  doi          = {10.1016/j.asoc.2024.112029},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incremental-based YoloV3 model with hyper-parameter optimization for product image classification in E-commerce sector},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimum time search using ant colony optimization for
multiple fixed-wing UAVs in dynamic environments. <em>ASOC</em>,
<em>165</em>, 112025. (<a
href="https://doi.org/10.1016/j.asoc.2024.112025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of multiple fixed-wing unmanned aerial vehicles in search and rescue missions after natural disasters has become of great interest as they can search large areas and find survivors as quickly as possible. This paper discusses a minimum time cooperative search scheme that utilizes ant colony optimization and new heuristic functions to tackle various constraints in a dynamic environment. The study makes novel use of Dubins curves in the heuristic functions to consider the kinematic limitations of fixed-wing UAVs when planning tangent continuity paths. Furthermore, a novel probabilistic approach is introduced to model the uncertainties induced by dynamic obstacles and determine optimal search paths that are safe and practical in a grid search environment. The performance of the proposed search algorithm is tested through two-dimensional and three-dimensional simulations, statistical analysis, and comparison with other well-known optimization algorithms. To randomize the simulated cooperative search, different search scenarios with static and dynamic obstacles are run several times.},
  archive      = {J_ASOC},
  author       = {Ali Motamedi and Mahdi Mortazavi and Mehdi Sabzehparvar and Frantisek Duchon},
  doi          = {10.1016/j.asoc.2024.112025},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112025},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimum time search using ant colony optimization for multiple fixed-wing UAVs in dynamic environments},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new prediction-based evolutionary dynamic multiobjective
optimization algorithm aided by pareto optimal solution estimation
strategy. <em>ASOC</em>, <em>165</em>, 112022. (<a
href="https://doi.org/10.1016/j.asoc.2024.112022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multiobjective optimization problems (DMOPs) typically involve multiple conflicting time-varying objectives that require optimization algorithms to quickly track the changing Pareto-optimal front (POF). To this end, several methods have been developed to predict new locations of moving Pareto-optimal solution set (POS) so that populations can be re-initialized around the predicted locations. In this paper, a dynamic multi-objective optimization algorithm based on a multi-directional difference model (MOEA/D-MDDM) is proposed. The multi-directional difference model predicts the initial population through the estimated populations developed by a designed POS estimation strategy. An adaptive crossover-rate approach is incorporated into the optimization process to cope with different POS structures. To investigate the performance of the proposed approach, MOEA/D-MDDM has been compared with six state-of-the-art dynamic multiobjective optimization evolutionary algorithms (DMOEAs) on 19 benchmark problems. The experimental results demonstrate that the proposed algorithm can effectively deal with DMOPs whose POS has a single-modality characteristic and continuous manifolds.},
  archive      = {J_ASOC},
  author       = {Kai Gao and Lihong Xu},
  doi          = {10.1016/j.asoc.2024.112022},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112022},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new prediction-based evolutionary dynamic multiobjective optimization algorithm aided by pareto optimal solution estimation strategy},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic multi-hoist scheduling with fuzzy processing times in
flexible manufacturing lines. <em>ASOC</em>, <em>165</em>, 112014. (<a
href="https://doi.org/10.1016/j.asoc.2024.112014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of scheduling multiple hoists (robots), which arises on real automated electroplating lines. Several hoists operate on a shared track and should not collide with each other. The processing time for each operation is preset in interval form. The problem simultaneously takes into account two criteria: maximum processing quality and maximum line productivity. To do this, we estimate the dependence of the duration of operations on the level of their quality and, in accordance with these estimates, we represent the processing times in the form of fuzzy numbers with triangular membership functions. The novelty of this study is two-fold. First, we propose a new mathematical model based on the use of fuzzy numbers and fuzzy operations on them, which is designed to deal with complex galvanic lines served by multiple hoists. The second new feature is a new combinatorial method developed for scheduling multiple hoists. For this, we modify and refine the so-called “method of prohibited intervals”, which has been actively used in scheduling theory in recent decades to solve problems, but until now was known only for the case of a single hoist. Thus, the second novelty of this work is that it is actually the first attempt to use the powerful technique of fuzzy set theory to solve a practical problem of scheduling multiple hoists. Computational experiments on real and randomly generated instances show that the new algorithm can successfully solve real problems that arise in practice.},
  archive      = {J_ASOC},
  author       = {Alexander Ptuskin and Eugene Levner and Vladimir Kats},
  doi          = {10.1016/j.asoc.2024.112014},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cyclic multi-hoist scheduling with fuzzy processing times in flexible manufacturing lines},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Numerical algorithms for generating an almost even
approximation of the pareto front in nonlinear multi-objective
optimization problems. <em>ASOC</em>, <em>165</em>, 112001. (<a
href="https://doi.org/10.1016/j.asoc.2024.112001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multiobjective optimization problem (MOP) returns a set of non-dominated points, the so-called Pareto front. Since this set is usually infinite, it is impossible to generate it completely in practice. Therefore, a discrete approximation of the Pareto front is created. One of the most important features of this approximation is a uniform distribution of points on the full Pareto front in order to present a wide variety of solutions to the decision maker who chooses a final solution. While a few algorithms consider this property, two algorithms based on the Pascoletti-Serafini (PS) scalarization approach are proposed. In addition, six well-known test problems with convex and non-convex Pareto fronts are considered to show the effectiveness of the proposed algorithms. Their results are compared with some algorithms including Normal Constraint (NC), Benson type, Non-Dominated Sorting Genetic Algorithm-II (NSGA-II), S-Metric Selection Evolutionary Multiobjective Algorithm (SMS-EMOA), Differential Evolution (DE) with Binomial Crossover and MOEA/D-DE. The computational results on CPU time and reasonable distribution of points obtained on the Pareto front show that the presented algorithms perform better than other algorithms on these criteria. In addition, although the proposed algorithms compete closely with some algorithms in terms of CPU time, they have more non-dominated solutions and more appropriate distribution than they do in most problems.},
  archive      = {J_ASOC},
  author       = {Azam Dolatnezhadsomarin and Esmaile Khorram and Majid Yousefikhoshbakht},
  doi          = {10.1016/j.asoc.2024.112001},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {112001},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Numerical algorithms for generating an almost even approximation of the pareto front in nonlinear multi-objective optimization problems},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding the third dimension in the metaverse: A
comprehensive method for reconstructing 2D NFT portraits into 3D models.
<em>ASOC</em>, <em>165</em>, 111964. (<a
href="https://doi.org/10.1016/j.asoc.2024.111964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Metaverse, 3D modeling techniques and autoencoders offer a novel approach for handling 2D portraits of Non-Fungible Tokens (NFTs). These techniques have significant applications in the metaverse, a virtual, shared, and persistently online space that combines the real world, virtual reality, and augmented reality. Within the metaverse, NFTs can represent virtual items and assets, and 3D modeling techniques can be used to create three-dimensional models of these virtual items and assets. In this paper, we propose a novel method of inferring 3D structure and texture from 2D Non-Fungible Token (NFT) portraits using image-decoupled autoencoders. By implementing 3D facial modeling, depth values are associated with each pixel in the canonical view, thereby modeling 3D faces with fine textures and accurate structures from 2D NFT portraits. The input image is decomposed into four elements: depth map, albedo image, light direction, and viewpoint, all of which are used in the 3D reconstruction process. Asymmetry in NFT portraits is also addressed, and a symmetry confidence map is used to record the symmetry prediction probability for each pixel. In the experimental section, datasets including human faces and anime faces are used to better adapt to the diverse styles of NFT images. The Adam optimizer is used for training, and a set of new evaluation metrics, including cosine similarity, PSNR, SSIM, and LPIPS, are used to assess the quality of texture reconstruction. The proposed method achieves state-of-the-art performance in 3D facial reconstruction and performs exceptionally well in 3D facial reconstruction of anime faces compared to other methods.},
  archive      = {J_ASOC},
  author       = {Erqiang Deng and Li You and Fazlullah Khan and Guosong Zhu and Zhen Qin and Saru Kumari and Hu Xiong and Ryan Alturki},
  doi          = {10.1016/j.asoc.2024.111964},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {111964},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decoding the third dimension in the metaverse: A comprehensive method for reconstructing 2D NFT portraits into 3D models},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperparameter optimization of two-branch neural networks in
multi-target prediction. <em>ASOC</em>, <em>165</em>, 111957. (<a
href="https://doi.org/10.1016/j.asoc.2024.111957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a result of the ever increasing complexity of configuring and fine-tuning machine learning models, the field of automated machine learning (AutoML) has emerged over the past decade. However, software implementations like Auto-WEKA and Auto-sklearn typically focus on classical machine learning (ML) tasks such as classification and regression. Our work can be seen as the first attempt at offering a single AutoML framework for most problem settings that fall under the umbrella of multi-target prediction, which includes popular ML settings such as multi-label classification, multivariate regression, multi-task learning, dyadic prediction, matrix completion, and zero-shot learning. Automated problem selection and model configuration are achieved by extending DeepMTP, a general deep learning framework for MTP problem settings, with popular hyperparameter optimization (HPO) methods. Our extensive benchmarking across different datasets and MTP problem settings identifies cases where specific HPO methods outperform others.},
  archive      = {J_ASOC},
  author       = {Dimitrios Iliadis and Marcel Wever and Bernard De Baets and Willem Waegeman},
  doi          = {10.1016/j.asoc.2024.111957},
  journal      = {Applied Soft Computing},
  month        = {11},
  pages        = {111957},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperparameter optimization of two-branch neural networks in multi-target prediction},
  volume       = {165},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GAI-based multi-scale convolution and attention mechanism
model for music emotion recognition and recommendation from
physiological data. <em>ASOC</em>, <em>164</em>, 112034. (<a
href="https://doi.org/10.1016/j.asoc.2024.112034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the subjectivity of emotions and the limited number of emotion categories, existing deep learning models require assistance to achieve objective, accurate, and flexible personalized music emotion recommendations. This paper introduces a deep learning approach that combines Generative Artificial Intelligence (GAI) and explicitly leverages physiological indicators to enhance the model&#39;s intelligence, versatility, and automation. Physiological indicators such as Heart Rate Variability (HRV) and Galvanic Skin Response (GSR) can be measured using sensors placed on the body&#39;s surface, providing more precise information about human emotional changes. This research employs a three-dimensional emotion model, including the tension-arousal axis, energy-arousal axis, and valence axis, to explain the correlation and accuracy between music data and emotions. Based on this, a music emotion classifier is designed, incorporating GAI algorithms to recommend music by matching users&#39; physiological and emotional types with the emotional features of music. The classifier uses Mel-Frequency Cepstral Coefficients (MFCC) to transform audio into Mel-spectrogram as input features. The music emotion selection module adopts a GAI framework of Variational Autoencoder (VAE) and integrates multi-scale parallel convolution and attention mechanism modules. Experimental results demonstrate that this approach is competitive compared to existing deep learning architectures on PMEmo, RAVDESS, and Soundtrack datasets. Furthermore, due to GAI&#39;s efficient classification capability, this model is suitable for resource-constrained mobile devices and other smart devices. The results of this study can be applied to emotion-based music recommendation systems, contributing to emotional interventions and improving the performance of exercise and music therapy.},
  archive      = {J_ASOC},
  author       = {Xiao Han and Fuyang Chen and Junrong Ban},
  doi          = {10.1016/j.asoc.2024.112034},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112034},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GAI-based multi-scale convolution and attention mechanism model for music emotion recognition and recommendation from physiological data},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse distance weight-assisted particle swarm optimized
indoor localization. <em>ASOC</em>, <em>164</em>, 112032. (<a
href="https://doi.org/10.1016/j.asoc.2024.112032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor localization using wireless fidelity (WiFi) fingerprint has attracted considerable attention due to its extensive deployment and low cost. Most indoor fingerprint localization methods could be computationally inefficient and behave with unsatisfactory positioning accuracy. Therefore, an inverse distance weight (IDW) assisted particle swarm optimized (PSO) indoor localization (IDWPSOInLoc) algorithm is proposed. It is an optimized positioning process in the online stage rather than an offline optimization approach. The main idea is to find the optimal particle with the most similar generated fingerprint to the real-time testing one in the online stage. K nearest neighbors and boundary constraints are combined to initialize particles’ positions. The generated fingerprint of each particle is interpolated based on the IDW algorithm and nearby reference points within a specified range. A new fitness function, the sum of standard deviations between the generated and real-time testing fingerprints, is proposed to find the optimal particle. The position of the optimal particle with the minimal fitness value is taken as the estimated coordinate through iterations updating the particles’ velocities, positions, and generated fingerprints. IDWPSOInLoc achieves a mean positioning accuracy of 2.477 meters in the local environment experimental test and that of about 6 meters using selected floor in the UJIIndoorLoc dataset. Compared with state-of-the-art algorithms, positioning error is decreased by at least 11.9%. In addition, IDWPSOInLoc has the characteristic of low computational complexity. Experimental results show that the proposed IDWPSOInLoc outperforms the considered WiFi indoor positioning algorithms.},
  archive      = {J_ASOC},
  author       = {Jingxue Bi and Jianhui Wang and Hongji Cao and Guobiao Yao and Yunjia Wang and Zengke Li and Meng Sun and Hongchao Yang and Jie Zhen and Guoqiang Zheng},
  doi          = {10.1016/j.asoc.2024.112032},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112032},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inverse distance weight-assisted particle swarm optimized indoor localization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault analysis in clustered microgrids utilizing SVM-CNN and
differential protection. <em>ASOC</em>, <em>164</em>, 112031. (<a
href="https://doi.org/10.1016/j.asoc.2024.112031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of distributed generation, microgrids, and renewable energy sources has significantly enhanced the resilience of modern electrical grids. However, this transition presents challenges in control, stability, safety, and protection due to low fault currents from renewables. This paper addresses these challenges by proposing novel methodologies to enhance fault detection, classification, and localization in microgrids. The literature review highlights a shift towards intelligent learning methods in microgrid protection systems, improving fault response times and identifying electrical faults, including high impedance faults. Nonetheless, existing methods often neglect high impedance fault detection and the integration of differential protection in clustered microgrids. To fill these gaps, this study presents a methodology combining support vector machines and convolutional neural networks for fault detection in microgrids, integrating differential protection for high impedance fault detection. The paper also proposes approaches to optimize protection in clustered microgrid systems. The effectiveness of the methodology is validated using Opal-RT through comparative analyses of signal decomposition techniques, performance and accuracy of support vector machines and convolutional neural networks, K-Fold validation, and sensitivity analysis. Results demonstrate robustness and high performance, achieving up to 100 % accuracy in fault detection and classification.},
  archive      = {J_ASOC},
  author       = {Paul Arévalo and Antonio Cano and Darío Benavides and Francisco Jurado},
  doi          = {10.1016/j.asoc.2024.112031},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault analysis in clustered microgrids utilizing SVM-CNN and differential protection},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new approximate belief mutual information derived from the
developed fuzzy mutual information considering belief negation.
<em>ASOC</em>, <em>164</em>, 112027. (<a
href="https://doi.org/10.1016/j.asoc.2024.112027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Dempster combination rule relies on fundamental assumption that evidences are mutually independent. However, this idealized prerequisite is frequently challenging to fulfill in real-world scenarios. How to measure belief mutual information to investigate dependence among evidence sources is a critical and significant issue. Inspired by fuzzy mutual information based on D–S evidence theory, a method of approximate belief mutual information (BMI) is developed to partially represent the dependence among the sources of evidence. In the BMI model, an interval correlation method considering the basic probability assignment (BPA) and its negation are proposed. Several examples are used to illustrate the availability of the BMI, which shows the developed BMI has better fault tolerance. Finally, the proposed method obtains higher recognition accuracy in pattern recognition tasks. The proposed method can maintain stable and reliable recognition results even when the data is disturbed, verifying the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Xuan Zhao and Yaxian Tang and Bingyi Kang},
  doi          = {10.1016/j.asoc.2024.112027},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112027},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A new approximate belief mutual information derived from the developed fuzzy mutual information considering belief negation},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning modelling of structural response for
different seismic signal characteristics: A parametric analysis.
<em>ASOC</em>, <em>164</em>, 112026. (<a
href="https://doi.org/10.1016/j.asoc.2024.112026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present study investigates the best seismic parameters for modeling the dynamic response of various non-linear structural systems by comparing different Machine Learning (ML) algorithms. A total of 400 synthetic excitations were generated and analyzed against 23 seismic parameters. These signals were used in a step-by-step numerical analysis to calculate the dynamic responses of 1000 single-degree-of-freedom (SDOF) systems with varying mechanical properties. The data obtained from these responses were processed using 20 ML algorithms, including linear regression, tree, support vector machine (SVM), boosted and bagged trees, and artificial neural network (ANN). Each ML algorithm used a single seismic parameter as input to determine the most predictive parameters for modeling structural responses, defining the high predictive seismic parameters (HPSP) set. To validate the obtained results, the most effective model predictions have been compared with the results of the parametric step-by-step analyses performed for a new group of natural ground motions. The findings demonstrate that with a properly calibrated training phase, considering the specific site hazard and selecting seismic parameters from the HPSP set, the ML model can accurately estimate seismic responses whit a significantly reduced computational effort. This study underscores the potential of integrating ML techniques into the performance-based seismic design approach.},
  archive      = {J_ASOC},
  author       = {M. De Iuliis and E. Miceli and P. Castaldo},
  doi          = {10.1016/j.asoc.2024.112026},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine learning modelling of structural response for different seismic signal characteristics: A parametric analysis},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-supervised framework for computer-aided arrhythmia
diagnosis. <em>ASOC</em>, <em>164</em>, 112024. (<a
href="https://doi.org/10.1016/j.asoc.2024.112024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases, including all types of arrhythmias, are the leading cause of death. Deep learning (DL)-based electrocardiography (ECG) diagnosis systems have attracted considerable attention in recent years. However, training DL-based models requires much high-quality labeled data, and labeling ECG records is time-consuming and expensive. In this paper, a high-precision deep network obtained via low-cost annotation, that is, a self-supervised residual convolutional neural network (SSRCNN), was designed. First, a convolutional neural network and residual blocks were designed to construct a deep feature extractor to automatically obtain the feature expression of ECG signals. Next, we developed time- and lead-dimension-based data augmentation methods and designed a pretraining framework based on unlabeled datasets such that the feature extractor could update the weights in the unlabeled samples. Furthermore, through interactions with clinicians, we used a few labeled data to fine-tune the pretrained model and feature classifier to automatically extract deep features and perform effective classification. Finally, we used different open-source datasets to validate the superiority of the SSRCNN. With the support of unlabeled datasets, SSRCNN can effectively reduce clinicians’ workload. Compared with existing methods, the SSRCNN achieves a better diagnostic performance in terms of average accuracy (99.0 %) and average F1-macro (86.83 %) using approximately 25 % of labeled data. Therefore, the SSRCNN has a potential for practical applications in clinical settings.},
  archive      = {J_ASOC},
  author       = {Yanrui Jin and Zhiyuan Li and Yuanyuan Tian and Xiaoyang Wei and Chengliang Liu},
  doi          = {10.1016/j.asoc.2024.112024},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112024},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-supervised framework for computer-aided arrhythmia diagnosis},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fire detection and risk assessment via support vector
regression with flattening-samples based augmented regularization.
<em>ASOC</em>, <em>164</em>, 112023. (<a
href="https://doi.org/10.1016/j.asoc.2024.112023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Hybrid Support Vector Regression (SVR) with Flattening-Samples Based Augmented Regularization (Hybrid FSR-SVR) architecture for multi-sensor fire detection and forest fire risk assessment. The Hybrid FSR-SVR is a lightweight architecture built upon the novel Flattening-Samples Based Augmented Regularization (FSR) approach and temporal trends of environmental variables. The FSR approach augments l 2 l2 norm based smoothing term into an l 1 l1 - l 2 l2 combination, facilitating the integration of l 1 l1 regularization into the SVR method, thereby enhancing generalization with minimal computational load. We evaluate the performance of Hybrid FSR-SVR using two distinct datasets covering indoor and forest fires, benchmarking against 15 machine learning models, including state-of-the-art techniques, such as Recurrent Trend Predictive Neural Network (rTPNN), Long-Short Term Memory (LSTM), Multi-Layer Perceptron (MLP), Gated Recurrent Unit (GRU), and Gradient Boosting. Our findings demonstrate that Hybrid FSR-SVR effectively assesses the risk of forest fire, enabling early preventive measures. Notably, it achieves a remarkable accuracy of 0.95 for forest fire detection and ranks third with 0.88 accuracy for indoor fire detection. Importantly, it exhibits computation times significantly lower – by 1 to 2 orders of magnitude – than the majority of compared techniques. The superior generalization ability of Hybrid FSR-SVR, facilitated by flattening-samples based augmented regularization, allows for high detection performance even with smaller training sets.},
  archive      = {J_ASOC},
  author       = {Mert Nakıp and Nur Keleşoğlu and Cüneyt Güzeliş},
  doi          = {10.1016/j.asoc.2024.112023},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112023},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fire detection and risk assessment via support vector regression with flattening-samples based augmented regularization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive study among distance measures on supervised
optimum-path forest classification. <em>ASOC</em>, <em>164</em>, 112021.
(<a href="https://doi.org/10.1016/j.asoc.2024.112021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised pattern classification relies on a labeled training set to learn decision boundaries that separate samples from different classes. Such samples can be either weakly- or reliably-labeled; in the first case, one can employ techniques specifically designed to cope with uncertainty during labeling, and in the other scenario, it relies on numerous alternatives, including metric learning. Pattern classifiers usually adopt the Euclidean distance to compare samples and assess their proximity, but this implies the feature space is embedded in a plane. However, samples are embedded in curved spaces for some applications, although not straightforward to prove. In this manuscript, we assessed the performance of the Optimum-Path Forest (OPF) classifier under different distance functions, which are used to weigh arcs among samples, for a graph encoding the feature space. This work compared 47 distance measures applied to the OPF classifier considering 22 datasets, plus Decision Trees, Logistic Regression, and Support Vector Machines. The experiments highlighted that OPF is user-friendly when handling distance measures and can obtain better accuracies in some situations than its standard (Euclidean) counterpart and the classifiers mentioned above. On the other hand, time-consuming distance calculations may affect OPF’s efficiency during inference.},
  archive      = {J_ASOC},
  author       = {Gustavo H. de Rosa and Mateus Roder and Leandro A. Passos and João Paulo Papa},
  doi          = {10.1016/j.asoc.2024.112021},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112021},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive study among distance measures on supervised optimum-path forest classification},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-resolution assessment of heart rate variability
signals during yogic and normal breathing using machine learning
modules. <em>ASOC</em>, <em>164</em>, 112020. (<a
href="https://doi.org/10.1016/j.asoc.2024.112020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, yoga has gained global prominence due to its wide-ranging health benefits. Heart rate variability (HRV) analysis has become a widely embraced tool to assess the impact of yoga and meditation on individuals. The effects of yoga can vary significantly among individuals based on their prior expertise, skills, and the interplay of external and internal dynamics. This variability complicates the task of extracting distinctive characteristics from raw data to categorize yogic and pre-yogic states. This paper introduces a new approach that enhances the screening of yogic states through multi-resolution analysis of HRV data combined with a machine learning module. The empirical Fourier decomposition (EFD) technique is harnessed to decompose input data into distinct multi-resolution modes. Various statistical characteristics are then extracted from these modes, allowing for a comprehensive analysis of the immediate effects of yoga breathing on HRV signals. To discern between yogic and pre-yogic HRV signals, a selection of significant features is ranked using a chi-square score and subsequently inputted into machine-learning modules. Notably, employing normalized features with the decision medium tree classifier yields an area under the curve (AUC) value of 0.99, while using direct features with an ensemble bagged tree classifier results in an AUC value of 0.82. Furthermore, this research underscores the importance of assessing outcomes from various metrics to compute the underlying dynamics of HRV data during breathing practices. This approach facilitates precise monitoring, personalized feedback, and intervention in yoga machine interfacing systems, ultimately enriching the overall experience and well-being of yoga practitioners.},
  archive      = {J_ASOC},
  author       = {Kapil Gupta and G.R. Sinha and Raghavendra Bhat and Apar Avinash Saoji and N.K. Manjunath},
  doi          = {10.1016/j.asoc.2024.112020},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112020},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-resolution assessment of heart rate variability signals during yogic and normal breathing using machine learning modules},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revolutionizing optimization: An innovative nutcracker
optimizer for single and multi-objective problems. <em>ASOC</em>,
<em>164</em>, 112019. (<a
href="https://doi.org/10.1016/j.asoc.2024.112019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nutcracker Optimization Algorithm (NOA) is a recently proposed meta-heuristic algorithm inspired by foraging and storing behavior of nutcracker birds. NOA demonstrates strong performance across various test sets and optimization problems. However, it faces challenges in effectively balancing exploration and exploitation, particularly in high-dimensional and complex applications. In this paper, an improved variant of NOA based on Bernoulli map strategy and seasonal behavior strategy, called INOA, is proposed. Firstly, the Bernoulli map strategy enhances the quality of the initial population during the initialization process. Secondly, the seasonal behavior strategy is employed to balance the exploration and exploitation of NOA, enabling it to effectively handle high-dimensional problems by improving convergence and exploration capabilities. Additionally, this paper extends INOA to a multi-objective version called MONOA, enabling the algorithm to solve multi-objective problems. The proposed algorithm, INOA, undergoes evaluation using 30 classical benchmark problems, CEC-2014, CEC-2017, CEC-2019 test suites, and two real-world engineering design problems. INOA’s performance is compared with three categories of optimization methods: (1) recently-developed algorithms, i.e., NOA, BWO, DBO, RIME, MGO, HBA, and SO, (2) highly-cited algorithms, i.e., SMA, MPA, GWO, and (3) high-performing optimizers and winners of CEC competition, i.e., CJADE, L-SHADE-RSP, L-SHADE, and EBOwithCMAR. The proposed algorithm, MONOA, undergoes evaluation using well-known ZDT and DTLZ suites, as well as six constrained and engineering design problems. MONOA’s performance is compared with some state-of-the-art approaches such as MOPSO, NSSO, MOGOA, MOSMA, and MOMGA. Five performance indicators are employed for comparison purposes. Experimental results and comparisons affirm the efficacy of INOA in solving complex and higher-dimensional optimization problems. Similarly, the findings underscore the effectiveness of MONOA in solving diverse multi-objective problems with distinct characteristics.},
  archive      = {J_ASOC},
  author       = {Mohammed Jameel and Mohamed Abouhawwash},
  doi          = {10.1016/j.asoc.2024.112019},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112019},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Revolutionizing optimization: An innovative nutcracker optimizer for single and multi-objective problems},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RoBERTa, ResNeXt and BiLSTM with self-attention: The
ultimate trio for customer sentiment analysis. <em>ASOC</em>,
<em>164</em>, 112018. (<a
href="https://doi.org/10.1016/j.asoc.2024.112018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis of customer feedback is a pivotal component of Natural Language Processing (NLP), enabling businesses to gauge consumer emotions towards their products and services. The inherent variability of language introduces substantial challenges in the analysis of unstructured customer data. Sentiment analysis techniques generally fall into two categories: traditional methods, which involve the extraction of hand-crafted features and the application of machine learning algorithms, and end-to-end deep learning approaches that process raw data, transforming them layer by layer into high-level textual features. Each method presents its own balance between simplicity and speed versus analytical power and flexibility, and they all grapple with the need for extensive labeled data and computational resources. To address these challenges, this paper presents an innovative hybrid model that amalgamates RoBERTa, ResNeXt, BiLSTM, and self-attention mechanisms. This integration capitalizes on the collective strengths of these models to surmount the limitations inherent in each. The proposed model proves to be a powerful and efficient tool for sentiment analysis, demonstrating proficiency across various data types and tasks. We undertake a comprehensive evaluation of the proposed model’s accuracy and training efficiency using four benchmark datasets. Our investigation also explores the impact of different similarity measures and convolutional neural network architectures on the model’s performance. The results affirm that our model not only achieves high accuracy but also significantly reduces training time compared to RoBERTa’s fine-tuning. Furthermore, the model exhibits exceptional domain adaptability, particularly when fine-tuned on the IMDb dataset following initial training on the Yelp Review Full dataset. The practicality of the proposed model is underscored by its reduced computational complexity and its adeptness at navigating semantic and syntactic nuances.},
  archive      = {J_ASOC},
  author       = {Amir Jabbary Lak and Reza Boostani and Farhan A. Alenizi and Amin Salih Mohammed and Seyed Mostafa Fakhrahmad},
  doi          = {10.1016/j.asoc.2024.112018},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112018},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RoBERTa, ResNeXt and BiLSTM with self-attention: The ultimate trio for customer sentiment analysis},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Evaluate the competence of fuzzy preference using
statistical conflict in the frame of DS evidence theory. <em>ASOC</em>,
<em>164</em>, 112017. (<a
href="https://doi.org/10.1016/j.asoc.2024.112017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy preference based DS evidence theory effectively solves the combined explosion ascribing to the increasing of the cardinality of the frame of discernment in the application. However, limited work has focused on evaluating the performance of the binary classifier (i.e. dynamic improvement of the according fuzzy preference element). The conflict coefficient can well reflect the relationship between various pieces of evidence and effectively evaluate the ability of each classifier. By statistically analyzing prior conflict information, the conflict interval range is determined to more effectively correct the fuzzy preference matrix. Therefore, we proposed a new method to evaluate the competence of fuzzy preference using statistical conflict in the frame of DS evidence theory. First, a binary transformation of the multiclassification problem is performed to generate the BPA and the matrix was generated by orthogonal fusion of multiple BPAs. Second, calculate the conflict coefficients and make statistics. The statistical conflict coefficients are combined and the conflict interval is solved to modify the construction of the fuzzy preference matrix. Third, based on the measured data in the feature space, the matrix is constructed by majority voting using the k-nearest neighbors of correctly classified objects. Finally, the two matrices are fused by determining the relationship between the conflict coefficient and the threshold value of each binary pair. The non-dominance degree is calculated based on the modified fuzzy preference matrix to make the final decision. Analysis and results of application examples and several datasets in practical application are used to demonstrate effectiveness and flexibility of the proposed method.},
  archive      = {J_ASOC},
  author       = {Huimin Zhang and Yuhang Chang and Yunjia Zhang and Bingyi Kang},
  doi          = {10.1016/j.asoc.2024.112017},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluate the competence of fuzzy preference using statistical conflict in the frame of DS evidence theory},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harris hawk optimization driven adaptive image encryption
integrating hilbert vibrational decomposition and chaos. <em>ASOC</em>,
<em>164</em>, 112016. (<a
href="https://doi.org/10.1016/j.asoc.2024.112016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image encryption is a technique used to protect the confidentiality of digital images. Existing encryption algorithms use chaotic maps for generating random sequences. These sequences purely depend on the fixed initial parameters, which are not adaptive to different input images, thereby limiting their ability to produce the best-possible encrypted image. This paper introduces a new Harris Hawk optimization (HHO) based adaptive image encryption algorithm that integrates Hilbert vibrational decomposition (HVD), and multiple chaotic maps. The new fitness function is specifically designed to optimize the security and robustness of the encrypted image, considering statistical and differential attack parameters (NPCR, UACI, Entropy, and correlation coefficient values). HHO is used to optimize the initial parameters of chaotic maps (Henon map, Duffing map, Lorenz equation) subject to variations in the input image. In the confusion stage, the input image is decomposed into four components of same dimension but decreasing energy using HVD. Subsequently, the block based pixel scrambling is performed on alternate components using the Henon map and Duffing map. In the diffusion stage, the image from the confusion stage undergoes a first level diffusion by bitwise XOR operation with a random image generated from the Lorenz equation. Further, second level of diffusion is achieved with a complex operation generated image from the input image, enhancing its adaptability. Finally, Arnold Cat map having number of iterations determined by the input image is used for final pixel permutation to produces the encrypted image. The optimized initial parameters and input image dependent confusion and diffusion operations enable the encryption system to adapt to changes in the input image, thereby enhancing its resistance against known plain text and known cipher text attacks. Moreover, the integration of HVD with multiple chaotic maps makes the scheme more complex to decrypt with plain text attacks. The performance of proposed algorithm is evaluated on five standard test images in terms of security and quality measures. Experimental results exhibit statistical and differential attack parameters close to ideal values (with Max. Entropy≈7.9996, Min. CC≈ 10 −5 , Max. NPCR≈99.64, Max. UACI≈33.62). The use of multiple chaotic maps at different stages makes the key space larger. Robustness analysis demonstrates the algorithm&#39;s capability in withstanding noise and image enhancement attacks. The comparison with state of the arts metaheuristic based and classical encryption algorithms highlights the superior performance of proposed HHO driven algorithm. The significant improvement in security parameters and overall fitness compared to fixed initial parameters is achieved by optimizing the encryption process.},
  archive      = {J_ASOC},
  author       = {Vinay Kumar Sharma and Janki Ballabh Sharma},
  doi          = {10.1016/j.asoc.2024.112016},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Harris hawk optimization driven adaptive image encryption integrating hilbert vibrational decomposition and chaos},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing vehicle detection in intelligent transportation
systems via autonomous UAV platform and YOLOv8 integration.
<em>ASOC</em>, <em>164</em>, 112015. (<a
href="https://doi.org/10.1016/j.asoc.2024.112015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study highlights the evolving landscape of object detection methodologies, emphasizing the superiority of deep learning-based approaches over traditional methods. Particularly in intelligent transportation systems-related applications requiring robust image processing techniques, such as vehicle identification, localization, tracking, and counting within traffic scenarios, deep learning has gained substantial traction. The YOLO algorithm, in its various iterations, has emerged as a popular choice for such tasks, with YOLOv5 garnering significant attention. However, a more recent iteration, YOLOv8, was introduced in early 2023, ushering in a new phase of exploration and potential innovation in the field of object detection. Consequently, due to its recent emergence, the number of studies on YOLOv8 is extremely limited, and an application in the field of Intelligent Transportation Systems (ITS) has not yet found its place in the existing literature. In light of this gap, this study makes a noteworthy contribution by delving into vehicle detection using the YOLOv8 algorithm. Specifically, the focus is on targeting aerial images acquired through a modified autonomous UAV, representing a unique avenue for the application of this cutting-edge algorithm in a practical context. The dataset employed for training and testing the algorithm was curated from a diverse collection of traffic images captured during UAV missions. In a strategic effort to enhance the variability of vehicle images, the study systematically manipulated flight patterns, altitudes, orientations, and camera angles through a custom-designed and programmed drone. This deliberate approach aimed to bolster the algorithm&#39;s adaptability across a wide spectrum of scenarios, ultimately enhancing its generalization capabilities. To evaluate the performance of the algorithm, a comprehensive comparative analysis was conducted, focusing on the YOLOv8n and YOLOv8x submodels within the YOLOv8 series. These submodels were subjected to rigorous testing across diverse lighting and environmental conditions using the dataset. Through tests, it was observed that YOLOv8n achieved an average precision of 0.83 and a recall of 0.79, whereas YOLOv8x attained an average precision of 0.96 and a recall of 0.89. Furthermore, YOLOv8x also outperformed YOLOv8n in terms of F1 score and mAP, achieving values of 0.87 and 0.83 respectively, compared to YOLOv8n&#39;s 0.81 and 0.79. These outcomes of the evaluation illuminated the relative strengths and weaknesses of YOLOv8n and YOLOv8x, leading to the conclusion that YOLOv8n is well-suited for real-time ITS applications, while YOLOv8x exhibits superior detection capabilities.},
  archive      = {J_ASOC},
  author       = {Murat Bakirci},
  doi          = {10.1016/j.asoc.2024.112015},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing vehicle detection in intelligent transportation systems via autonomous UAV platform and YOLOv8 integration},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion of transformer attention and CNN features for skin
cancer detection. <em>ASOC</em>, <em>164</em>, 112013. (<a
href="https://doi.org/10.1016/j.asoc.2024.112013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma is a skin cancer that spreads quickly and has serious risks. Early diagnosis is essential, but since the symptoms of skin lesions in the early stages are vague and similar, they can be difficult for specialists to detect. Therefore, machine learning-based alternative diagnostic methods can be developed in addition to existing ones. This study proposes a new deep learning model, a modified lightweight vision transformer (ViT) architecture, and a hybrid framework developed with an integrated deep learning model and an Ensemble Learning (EL) model for the early-stage diagnosis of skin lesions. The proposed deep learning model was developed based on convolution layers and transformers. The model is called multi-head attention block depthwise separable convolution network (MABSCNET). The proposed hybrid framework was developed by combining modern deep learning and EL models pre-trained with the ImageNet dataset along with the MABSCNET model. In the experimental process, the effectiveness of the proposed methods was evaluated on the ISIC 2020 dataset. Additionally, additional experiments were conducted on ISIC 2018 and a Kaggle dataset to analyze the proposed hybrid framework&#39;s classification performance. Image enhancement techniques were used in the datasets. In the ISIC 2020 dataset, the MABSCNET model reached 78.63 % accuracy, the ViT model obtained 76.50 %, and the hybrid framework reached 92.74 % accuracy. Moreover, the proposed hybrid framework achieved 100 % on the ISIC 2018 dataset and 94.24 % on the Kaggle dataset.},
  archive      = {J_ASOC},
  author       = {Hatice Catal Reis and Veysel Turk},
  doi          = {10.1016/j.asoc.2024.112013},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112013},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion of transformer attention and CNN features for skin cancer detection},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diverse joint nonnegative matrix tri-factorization for
attributed graph clustering. <em>ASOC</em>, <em>164</em>, 112012. (<a
href="https://doi.org/10.1016/j.asoc.2024.112012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster analysis of attributed graphs is a demanding and challenging task in the analysis of network-structured data. It involves learning node representation by leveraging both node attributes and the topological structure of the graph, aiming to accomplish effective clustering. Typically, existing methods fuse the topological and non-topological information by learning a consensus representation, often resulting in redundancy and overlooking their inherent distinctions. To address this issue, this paper proposes the Diverse Joint Nonnegative Matrix Tri-Factorization (Div-JNMTF), an embedding based model to detect communities in attributed graphs. The novel JNMTF model attempts to extract two distinct node representations from topological and non-topological data. Simultaneously, a diversity regularization technique utilizing the Hilbert–Schmidt Independence Criterion (HSIC) is employed. Its objective is to reduce redundant information in the node representations while encouraging the distinct contributions of both types of information. In addition, two graph regularization terms are introduced to preserve the local structures in the topological and attribute representation spaces. The Div-JNMTF model is optimized by developing an iterative optimization approach. By conducting thorough experiments on four synthetic and eight real-world attributed graph datasets, it has been demonstrated that the proposed model excels in accurately detecting attributed communities and surpasses the performance of existing methods.},
  archive      = {J_ASOC},
  author       = {Arina Mohammadi and Seyed Amjad Seyedi and Fardin Akhlaghian Tab and Rojiar Pir Mohammadiani},
  doi          = {10.1016/j.asoc.2024.112012},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diverse joint nonnegative matrix tri-factorization for attributed graph clustering},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis of social media comments based on
multimodal attention fusion network. <em>ASOC</em>, <em>164</em>,
112011. (<a href="https://doi.org/10.1016/j.asoc.2024.112011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media comments are no longer in a single textual modality, but heterogeneous data in multiple modalities, such as vision, sound, and text, which is why multimodal sentiment analysis strategies has been introduced. However, among the multimodal sentiment analysis domains, a majority of the current multimodal sentiment analysis models employ the Transformer architecture due to its great impact and benefits, thereby leading to an augmentation in resource overhead. In this paper, a multimodal attention fusion (MAF) network model is proposed for sentiment analysis of multimodal data. MAF is mainly composed of the cross attention and residual unit. The Cross Attention Unit is designed to select one core modality out of three modes, while the remaining modes serve as base modalities. The core modality is then combined with the base modality information to facilitate significant interaction between the two modalities, resulting in three sets of two-by-two attention computations. Moreover, a residual unit is employed to integrate the overall information into the attentional information. This approach not only enables modality-to-modality interaction, but also supplements the overall information. In the end, experiments are conducted on two publicly available multimodal sentiment analysis datasets from Carnegie Mellon University(CMU), CMU-MOSEI (abbreviated as MOSEI) and CMU-MOSI (abbreviated as MOSI), to validate that the method achieves high performance while removing the complex structure, and is comparable to the State-Of-The-Art(SOTA) model with high-performance A100 and V100 Graphics Processing Units(GPU) in an ordinary hardware environment.},
  archive      = {J_ASOC},
  author       = {Ziyu Liu and Tao Yang and Wen Chen and Jiangchuan Chen and Qinru Li and Jun Zhang},
  doi          = {10.1016/j.asoc.2024.112011},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112011},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment analysis of social media comments based on multimodal attention fusion network},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way decision-based takagi–sugeno–kang fuzzy classifier
for partially labeled data. <em>ASOC</em>, <em>164</em>, 112010. (<a
href="https://doi.org/10.1016/j.asoc.2024.112010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The salient features of Takagi–Sugeno–Kang (TSK) fuzzy classifiers are their superior nonlinear fitting capability and better interpretability, rendering them widely applicable in the domains of data mining and machine learning. However, using TSK fuzzy classifiers for partially labeled data has received little attention. Based on the three-way decision (TWD) theory, this study proposed a TSK fuzzy model to learn from partially labeled data. First, an adaptive sample weight strategy is introduced for the fuzzy partition of the antecedent part and the cross-entropy loss of the consequent part, respectively. Subsequently, a prototype-based firing strength loss mechanism is proposed to constrain the optimization of the model in the fuzzy representation space. It employs a TWD strategy based on the entropy criterion to classify samples as useful, useless, and uncertain, where the proposed model enhances the performance by iteratively utilizing a certain number of useful pseudo-labeled samples. Finally, the validity of the model is theoretically analyzed based on the principle of noise learning. The experimental results on UCI datasets demonstrate that the proposed model exhibits superior performance compared with classical semi-supervised methods, even attaining a performance comparable to that of the model trained on all data with ground-truth labels.},
  archive      = {J_ASOC},
  author       = {Linchao Pan and Can Gao and Jie Zhou and Gong Chen and Xiaodong Yue},
  doi          = {10.1016/j.asoc.2024.112010},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112010},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decision-based Takagi–Sugeno–Kang fuzzy classifier for partially labeled data},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable ResNet50 learning model based on copula entropy
for cotton plant disease prediction. <em>ASOC</em>, <em>164</em>,
112009. (<a href="https://doi.org/10.1016/j.asoc.2024.112009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel Deep Learning (DL) framework for cotton plant disease prediction based on Explainable Artificial Intelligence (XAI) and Copula entropy based-Grey Wolf Optimization (GWO) algorithm. The suggested framework uses a cotton plant image as input and pre-processes it to produce an image with enhanced contrast. Features are extracted from the leaf images with the ResNet50 CNN model. A crucial pre-processing model known as feature selection helps to improve the effectiveness of image classification by deleting extraneous or irrelevant features. Therefore, the Gray Wolf optimization (GWO) algorithm which is a global search method with potential use in feature selection is employed in this paper. The proposed framework introduces Copula entropy (CE) as an indicator of association to create the GWO’s initial population and enhance the GWO feature engineering process. For the GWO initialization procedure, CE has been used to choose the most significant features which substantially improved the quality of the GWO starting population and as a result improved the performance of the proposed CE-based GWO algorithm by 78.57 % faster than the traditional GWO as stated by the time complexity analysis. In addition, Feature importance explanation is determined using XAI layer. The final classification is achieved using Random Forests (RF) classifier which is an ensemble learning approach. According to the experimental results, the suggested model has a classification accuracy of 99 % and a mean squared error of 0.0383. Furthermore, the proposed model has been compared to state-of-the-art algorithms and the results showed that the proposed model has the superiority performance. The proposed model can therefore be used to track a variety of cotton areas to enable faster analysis and response, resulting in higher productivity.},
  archive      = {J_ASOC},
  author       = {Heba Askr and Mohamed El-dosuky and Ashraf Darwish and Aboul Ella Hassanien},
  doi          = {10.1016/j.asoc.2024.112009},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112009},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable ResNet50 learning model based on copula entropy for cotton plant disease prediction},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of CNN for multiple phase corrosion
identification and region detection. <em>ASOC</em>, <em>164</em>,
112008. (<a href="https://doi.org/10.1016/j.asoc.2024.112008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corrosion is a significant issue that contributes negatively to the degradation of materials most especially metals. To ensure proper maintenance, enhance reliability and prevent breakdown, it is very essential to not only effectively detect corrosion but to also understand its locations and distributions on the materials. A Multiple phase Convolutional Neural Network (CNN) model is created for this purpose. The Multiple phase CNN model consists of custom designed deep learning algorithms at various stages. This created the opportunity to make use of binary classification, multi-label classification and patch distribution algorithm to detect and identify corrosion regions on metallic materials. Six (6) different labels of corrosion were modelled to represent different levels of degradation using 600 anonymized images. The images were used in the various stages of the framework for training the respective models. Results at the binary level shows 94.87 % of corrosion detection. The multiclass stage of the Multiple phase CNN records the highest accuracy of 92.1 %. The patch distribution stage recorded a highest accuracy of 96.5 % and 94.6 % for the Average Image and Average Pixel ROCAUC (Region of Concentration Area Under Cover). It also shows a region segment average accuracy detection of 91.5 % (image level) and 89.2 %(pixel level) for 9 distinct regions. The research provides a comprehensive and detailed reliability and maintenance information for Aerospace, Transport and Manufacturing Materials experts and non-experts. The framework shows a robust approach to detecting corrosion which is essential for critical and safety applications as well as preventing economic loss due to corrosion. This can also be extended to other domains beyond the corrosion case study.},
  archive      = {J_ASOC},
  author       = {Oluseyi Ayodeji Oyedeji and Samir Khan and John Ahmet Erkoyuncu},
  doi          = {10.1016/j.asoc.2024.112008},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112008},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of CNN for multiple phase corrosion identification and region detection},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Short-term wind power forecasting based on multi-scale
receptive field-mixer and conditional mixture copula. <em>ASOC</em>,
<em>164</em>, 112007. (<a
href="https://doi.org/10.1016/j.asoc.2024.112007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating renewable wind power (WP) into the grid exacerbates variability and challenges reliability. Establishing an effective forecasting system is crucial for risk avoidance, but achieving effective, interpretable, and accurate predictions remains an obstacle due to the high volatility and uncertainty of WP. This study proposes an innovative and trustworthy wind power forecasting system combined with point and interval predictions. Specifically, the adaptive multi-scale convolutional receptive field (AMSCF) combined with gated recurrent unit (GRU) for WP point prediction is proposed to capture the spatiotemporal dependencies of WP generation. This AMSCF method uses different size receptive fields, fuses them using channel dimension concatenation, operates pooling layer and adaptive fusion through field attention, which extracts hierarchical features for WP forecasting. Then conditional mixture Copula (Con-mCopula) function integrated with multiobjective optimizer is established for WP interval prediction, which reduces the WP interval prediction accuracy error as some information neglected. The wind power cluster dataset from Germany was selected for experiments. Based on the results, for both datasets, the designed prediction system can achieve better point and interval prediction performance resulting in a reduction of mean absolute error (MAE) by approximately 40 % compared to other existing components in the market. This study deconstructs the “Black Box” in deep network for energy forecasting through interpretability analysis, which accelerates the development of AI technology in WP forecasting system and provides decision-makers with trustworthy WP forecasting assistance.},
  archive      = {J_ASOC},
  author       = {Jinchang Li and Jiapeng Chen and Zheyu Chen and Ying Nie and Aiting Xu},
  doi          = {10.1016/j.asoc.2024.112007},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112007},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term wind power forecasting based on multi-scale receptive field-mixer and conditional mixture copula},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A wave forecasting method based on probabilistic diffusion
LSTM network for model predictive control of wave energy converters.
<em>ASOC</em>, <em>164</em>, 112006. (<a
href="https://doi.org/10.1016/j.asoc.2024.112006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate long-term prediction of wave excitation forces is critical for optimizing wave energy converters, enabling enhanced control, and improving energy absorption efficiency. Traditional prediction models often employ deterministic conservative approaches, overlooking uncertainties. This paper introduces a novel prediction method based on the probabilistic diffusion model, offering a more precise prediction while accounting for uncertainty. Notably, our approach goes beyond previous works by simultaneously achieving control objectives for maximizing energy absorption, contrasting with methodologies solely focused on prediction and control tasks. The proposed method utilizes long short-time memory to extract temporal information from historical wave excitation observation. The hidden features are then processed through a diffused probabilistic-based unit. Subsequently, a time-scale neural network is developed for wave excitation moment prediction, followed by the application of nonlinear model predictive control to maximize the energy absorption. The methodology is validated on the 1:20 Wavestar prototype devices through numerical simulations. Results indicate that the predicted excitation moment closely align with measured values. In cases 4, 5, 6, the proposed method yields a substantial improvement in maximum control energy absorption–22%, 16%, and 31%, respectively–compared to traditional prediction methods. The proposed approach not only achieves a more accurate wave excitation moment prediction with uncertainty consideration but also introduces advanced control strategies in a full-scale plant application. This work contributes significantly to the field by bridging the gap between precise prediction and effective control in wave energy conversion systems.},
  archive      = {J_ASOC},
  author       = {Yongxiang Lei},
  doi          = {10.1016/j.asoc.2024.112006},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112006},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A wave forecasting method based on probabilistic diffusion LSTM network for model predictive control of wave energy converters},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient angle-based twin random vector functional link
classifier. <em>ASOC</em>, <em>164</em>, 112005. (<a
href="https://doi.org/10.1016/j.asoc.2024.112005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random vector functional link (RVFL) has always proven to be an excellent classifier in various application areas of machine learning. In this work, inspired by RVFL and its twin variant, i.e., twin RVFL (TRVFL), we propose an efficient angle-based twin random vector functional link (ATRVFL) to address the binary classification problem. ATRVFL is a general classification model in which the first optimization problem can be found by solving a quadratic programming problem (QPP). The second problem can be described as an unconstrained minimization problem (UMP) which is solved by a system of linear equations. ATRVFL finds the solution by solving a QPP and a UMP instead of solving two QPPs. Therefore, its training time is significantly less than TRVFL. To maximize the angle between the normals of the two hyperplanes, the second hyperplane is chosen to be close to its class. The objective of introducing the idea of angle is to maximise the distance between two hyperplanes to improve the ability to discriminate them in geometric spaces. Experimental analyses are performed on a leaf dataset and 25 real-world benchmark datasets (RWDs) collected from the UCI repository of various fields like medical, biological, etc. The results are evaluated based on classification accuracy, the area under the curve, G-mean and F1-score. The results of ATRVFL are compared with Support Vector Machine (SVM), RVFL, Twin-RVFL (TRVFL), RVFL with ε-insensitive Huber loss ( ε ε -HRVFL), ensemble deep learning-based RVFL network (edRVFL) and Intuitionistic fuzzy random vector functional link classifier (IFRVFLC). The classification accuracy of the proposed model on the leaf dataset is 91.093 % and the highest F1-score of 0.768. Overall, based on experimental analysis it is evident that the proposed ATRVFL shows comparative or better performance than SVM, RVFL, TRVFL, ε-HRVFL, IFRVFLC and edRVFL.},
  archive      = {J_ASOC},
  author       = {Upendra Mishra and Deepak Gupta and Barenya Bikash Hazarika},
  doi          = {10.1016/j.asoc.2024.112005},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112005},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient angle-based twin random vector functional link classifier},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A spatiotemporal chaos based deep learning model
watermarking scheme. <em>ASOC</em>, <em>164</em>, 112004. (<a
href="https://doi.org/10.1016/j.asoc.2024.112004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With deep learning techniques achieving great results in modern industry, the intellectual property (IP) protection for deep learning models has attracted the attention of academics and engineers. However, training a commercially viable deep learning model usually needs professional resources and time. Once a malicious user clones, illegally distributes and uses the model, it can infringe on the model owner&#39;s IP and even steal its market share. Among the existing IP protection methods, scholars prefer the black-box watermarking approaches, of which the content of the trigger set and the label are the key part of the watermarking technique. However, most schemes do not consider the security and invisibility of the trigger set, which allows attackers to easily trigger the model by creating a fake trigger set, thereby committing a fraudulent ownership claim attack and claiming the ownership belongs to themselves. To overcome these drawbacks, we proposed a spatiotemporal chaotic data annotation method. Firstly, the unpredictability and acyclicity of chaos make the model resistant to fraudulent ownership claim attacks, statistical inference and other common machine learning attacks; Secondly, the trigger set and parameters are independent of each other, guaranteeing the security of the key; Thirdly, the spatiotemporal chaotic system provides a large key space, which meets the commercialization needs of deep learning models. Theoretical analysis and experimental results show that our scheme has security, practicality and robustness. To further validate the superiority of the proposed method, we also compare it with the Logistic chaotic annotation watermarking-based method, and the results show that our method performs better in terms of robustness, effectiveness, completeness, fidelity, security and practicality.},
  archive      = {J_ASOC},
  author       = {Dehui Wang and Shuang Zhou and Yingqian Zhang},
  doi          = {10.1016/j.asoc.2024.112004},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112004},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatiotemporal chaos based deep learning model watermarking scheme},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stabilized GAN models training with kernel-histogram
transformation and probability mass function distance. <em>ASOC</em>,
<em>164</em>, 112003. (<a
href="https://doi.org/10.1016/j.asoc.2024.112003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image generation using generative adversarial networks (GANs) has been extensively researched in recent years. Despite active developments, the chronic issue of training instability in GANs remains unresolved. To alleviate this problem, this study proposes a model named probability mass function GANs (PMF-GAN), which handles the inherent limitation of GANs. The PMF-GAN framework employs kernels, histogram transformation, and probability mass function (PMF) distance for distribution learning. The configuration of PMF-GAN kernel and PMF distance offers flexibility, allowing for optimal settings tailored to datasets and experimental environments. In this study, experiments were conducted using the gaussian kernel across five different distances. The experiments demonstrated that PMF-GAN outperforms the baselines in terms of visual quality and evaluation metrics, such as Inception score and Frechet Inception distance (FID). For example, in the CIFAR-10 dataset, Euclidean-based PMF-GAN applying with 3 bins showed a 21.5 % and 32.8 % improvement in Inception score and FID, respectively, compared to conventional WGAN-GP. Similarly, in the AFHQ dataset with the same settings, the improvements were 56.9 % and 61.5 %. As a result, this study presents the potential to achieve stable training processes in GAN models with modified loss function structures. The flexibility of the proposed model allows for simultaneous application to various models, contributing to the overall improvement of generative model training processes in the future.},
  archive      = {J_ASOC},
  author       = {Jangwon Seo and Hyo-Seok Hwang and Minhyeok Lee and Junhee Seok},
  doi          = {10.1016/j.asoc.2024.112003},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112003},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stabilized GAN models training with kernel-histogram transformation and probability mass function distance},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An incremental learning approach to dynamic parallel machine
scheduling with sequence-dependent setups and machine eligibility
restrictions. <em>ASOC</em>, <em>164</em>, 112002. (<a
href="https://doi.org/10.1016/j.asoc.2024.112002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing the tardiness of a parallel machine scheduling problem has been actively studied in modern manufacturing systems. In particular, dynamic parallel machine scheduling problems (DPMSPs) have gained much attention since rescheduling is required to address unpredictable events such as unexpected job arrivals and machine breakdowns, which usually occur in real-world manufacturing systems. To deal with such unpredictable events, many researchers have employed deep supervised learning (SL). However, it is still challenging to solve a DPMSP since SL requires a large number of high-quality schedules for the training neural networks. In this paper, we propose an incremental learning-based scheduling method (ILS) in which neural networks (NNs) are periodically trained by utilizing schedules built from the updated NNs in previous training intervals. Furthermore, to perform training without reducing solution space within a dynamic environment, the proposed method is designed to consider the dependency between consecutive allocations on a machine for the sum of setup time and tardiness. To verify the effectiveness of the proposed method, extensive experiments are carried out in static and dynamic scheduling problems. The experiment results demonstrate that ILS outperforms the existing methods such as dispatching rules, metaheuristics, and DRL-based methods. Additionally, we show that the proposed input features are effective in appropriately selecting job-machine pairs for assigning jobs on eligible machines through deep Shapley additive explanations analysis.},
  archive      = {J_ASOC},
  author       = {Donghun Lee and In-Beom Park and Kwanho Kim},
  doi          = {10.1016/j.asoc.2024.112002},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112002},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An incremental learning approach to dynamic parallel machine scheduling with sequence-dependent setups and machine eligibility restrictions},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Underwater image enhancement via cross-wise transformer
network focusing on pre-post differences. <em>ASOC</em>, <em>164</em>,
112000. (<a href="https://doi.org/10.1016/j.asoc.2024.112000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater images often suffer from color deviations and blurred details. To address these issues, many methods employ networks with an encoder/decoder structure to enhance the images. However, the direct skip connection overlooks the differences between pre- and post-features, and deep network learning introduces information loss. This paper presents an underwater image enhancement network that focuses on pre-post differences. The network utilizes a multi-scale input and output framework to facilitate the underwater image enhancement process. A novel cross-wise transformer module (CTM) is introduced to guide the interactive learning of features from different periods, thereby enhancing the emphasis on detail-degraded regions. To compensate for the information loss within the deep network, a feature supplement module (FSM) is devised for each learning stage. FSM merges the multi-scale input features, effectively enhancing the visibility of underwater images. Experimental results across several datasets demonstrate that the integrated modules yield significant enhancements in network performance. The proposed network exhibits outstanding performance in both visual comparisons and quantitative metrics. Furthermore, the network also exhibits good adaptability in additional visual tasks without the need for parameter tuning. Code and models are released in https://github.com/WindySprint/CTM .},
  archive      = {J_ASOC},
  author       = {Zhixiong Huang and Jinjiang Li and Xinying Wang and Zhen Hua and Shenglan Liu and Lin Feng},
  doi          = {10.1016/j.asoc.2024.112000},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {112000},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Underwater image enhancement via cross-wise transformer network focusing on pre-post differences},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing earth dam slope stability prediction with
integrated AI and statistical models. <em>ASOC</em>, <em>164</em>,
111999. (<a href="https://doi.org/10.1016/j.asoc.2024.111999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces an innovative approach integrating artificial intelligence (AI) and statistical modelling techniques to enhance the prediction of earth dam slope stability. Utilizing advanced methodologies, including Classification and Regression Tree (CART), Classification and Regression Random Forests (CRRF), and Multiple Linear Regression (MLR), this research provides a comprehensive, data-driven analysis for slope stability prediction. The integration of AI with traditional statistical models significantly improves the predictive accuracy over conventional methods. The findings reveal the model&#39;s capability in terms of reliability and precision in predicting slope stability, demonstrating its potential as a powerful tool in geo-engineering. Additionally, this work highlights the effective application of AI in complex geo-environmental systems analysis, opening new avenues for research and practice in the field.},
  archive      = {J_ASOC},
  author       = {Abolfazl Baghbani and Roohollah Shirani Faradonbeh and Yi Lu and Amin Soltani and Katayoon Kiany and Hasan Baghbani and Hossam Abuel-Naga and Pijush Samui},
  doi          = {10.1016/j.asoc.2024.111999},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111999},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing earth dam slope stability prediction with integrated AI and statistical models},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A credibility integration evaluation approach of complex
simulation systems based on type-2 fuzzy set and perceptual computing.
<em>ASOC</em>, <em>164</em>, 111994. (<a
href="https://doi.org/10.1016/j.asoc.2024.111994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credibility evaluation is an important research topic in the simulation field and the credibility integration of complex simulation systems is a difficult problem that needs to be broken through. The existing credibility integration methods can only deal with the deterministic numerical evaluation data, which are described by numbers and intervals. We introduce perceptual computing for the first time to solve the credibility integration problem, and propose a novel credibility integration evaluation approach, which can deal with the evaluation data containing both numerical and linguistical descriptions. Firstly, we propose the credibility transformation approach and the weight transformation approach to map the evaluation data into interval type-2 fuzzy sets. Then, the crdibility integration perceptual computer with perceptual computer as the prototype is proposed to aggregate the indicators and the credibility label is generated. Finally, experiments are presented to demonstrate the effectiveness and performance of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Huan Zhang and Wei Li and Ping Ma and Ming Yang},
  doi          = {10.1016/j.asoc.2024.111994},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A credibility integration evaluation approach of complex simulation systems based on type-2 fuzzy set and perceptual computing},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing the consistency of performance scores reported for
binary classification problems. <em>ASOC</em>, <em>164</em>, 111993. (<a
href="https://doi.org/10.1016/j.asoc.2024.111993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary classification is a fundamental task in machine learning, with applications across various scientific domains. Whether conducting fundamental research or refining practical applications, scientists typically assess and rank classification techniques based on performance metrics such as accuracy , sensitivity , and specificity . However, reported performance scores may not always provide a reliable basis for research ranking. The unreliability can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup with a specific number of positive and negative test items, performance scores can only assume specific, interrelated values. Based on this observation, in this paper, we introduce numerical techniques to assess the consistency of reported performance scores with the assumed experimental setup. Importantly, the proposed approaches do not rely on statistical inference. Instead, they use numerical methods (interval computing and integer linear programming) to identify inconsistencies with certainty. Through three applications in different fields of medicine, we demonstrate how the proposed tests can detect inconsistencies, thereby safeguarding the integrity of research fields. The power analyses of the tests in these applications show at least 71% of power when the performance scores are reported to four decimal places. In the investigated areas, the tests have so far identified inconsistencies in more than 100 scientific papers. To benefit the scientific community, we have made the consistency tests available in the open-source Python package mlscorecheck .},
  archive      = {J_ASOC},
  author       = {Attila Fazekas and György Kovács},
  doi          = {10.1016/j.asoc.2024.111993},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111993},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Testing the consistency of performance scores reported for binary classification problems},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-branch encoder-decoder network for image tampering
localization. <em>ASOC</em>, <em>164</em>, 111992. (<a
href="https://doi.org/10.1016/j.asoc.2024.111992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tampered images with false information can mislead viewers and pose security issues. Tampering traces in images are difficult to detect. To locate tampering traces effectively, a dual-domain deep-learning-based image tampering localization method based on RGB and frequency stream branches is proposed in this work. The former branch learns and extracts tampered features on the image and content features of the tampered region. The latter branch extracts tampered features from the frequency domain to complement the RGB stream branch. In addition, an attention mechanism is used to integrate the features from both branches at the fusion stage. In the experiments, the F 1 score of the proposed method outperformed those of the baselines on the NIST16 dataset (with a 15.3 % improvement), and the AUC score outperformed those of the baselines on the NIST16 and COVERAGE datasets (improvements of 3.9 % and 4.7 %, respectively). This study provides a beneficial alternative to image tampering localization techniques.},
  archive      = {J_ASOC},
  author       = {Yuling Luo and Ce Liang and Sheng Qin and Junxiu Liu and Qiang Fu and Su Yang},
  doi          = {10.1016/j.asoc.2024.111992},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111992},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-branch encoder-decoder network for image tampering localization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimization framework with dimensionality reduction
using markov chain monte carlo and genetic algorithms for groundwater
potential assessment. <em>ASOC</em>, <em>164</em>, 111991. (<a
href="https://doi.org/10.1016/j.asoc.2024.111991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited samples and high-dimensional feature spaces often hinder the accuracy of machine learning (ML) models in regional groundwater potential assessment (GPA). This study proposes a novel framework, the GPA with Dimensionality Optimization (GPADO), that optimizes feature dimension reduction to enhance prediction performance. Taking the Jianghan Basin as an example, data on nine continuous variables and five categorical variables influencing the region&#39;s GPA were gathered, expanding the feature set to 37 through One-hot encoding for categorical variables. Three scenarios were devised to assess prediction outcomes following various dimensionality reduction approaches. Comparative analysis revealed that a hybrid dimension reduction method, incorporating both continuous and categorical variables, yielded the highest validation set accuracy. Consequently, genetic algorithm and Markov Chain Monte Carlo methods were employed to determine the optimal solution and uncertainties associated with four unknown parameters: the chosen dimension reduction method for continuous and categorical variables, and the number of dimensions retained. Results indicated that utilizing singular value decomposition to reduce categorical variables to three dimensions, coupled with principal component analysis reducing continuous variables to three dimensions, produced the highest model validation accuracy of 0.834 within the GPADO framework. This optimal configuration facilitated automated ML training, resulting in a final validation set accuracy of 0.851 and a test set accuracy of 0.836. The resulting model provided a more precise spatial distribution of groundwater potential and demonstrated the GPADO framework&#39;s effectiveness in improving GPA accuracy, particularly in data-scarce regions. The GPADO framework offers a valuable approach for enhancing GPA studies.},
  archive      = {J_ASOC},
  author       = {Zitao Wang and Chao Yue and Jianping Wang},
  doi          = {10.1016/j.asoc.2024.111991},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An optimization framework with dimensionality reduction using markov chain monte carlo and genetic algorithms for groundwater potential assessment},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vehicle routing and scheduling problem with order acceptance
for pharmaceutical refrigerated logistics. <em>ASOC</em>, <em>164</em>,
111983. (<a href="https://doi.org/10.1016/j.asoc.2024.111983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a delivery problem in a pharmaceutical cold chain. In this problem, each customer orders multiple pharmaceutical products and requires a due date for each order. Each product has sensitive characteristics of storage temperature range and shelf life. A refrigerated vehicle routing and scheduling problem with order acceptance is formulated as a mixed-integer-linear-programming model. In the model, the acceptance of each pharmaceutical order and the route and schedule of a limited number of refrigerated vehicles is simultaneously determined to maximize the total profit. A hybrid genetic algorithm embedding an insertion heuristic with modification operation is proposed. The performance of the proposed algorithm is evaluated and verified by comparing other metaheuristics. Managerial insights about various experimental environments for the order acceptance are suggested by conducting a sensitivity analysis.},
  archive      = {J_ASOC},
  author       = {Seung Jae Lee and Byung Soo Kim},
  doi          = {10.1016/j.asoc.2024.111983},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111983},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vehicle routing and scheduling problem with order acceptance for pharmaceutical refrigerated logistics},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable scale fuzzy β-covering group approximation space
and variable scale multi-granulation FCRSs with applications.
<em>ASOC</em>, <em>164</em>, 111982. (<a
href="https://doi.org/10.1016/j.asoc.2024.111982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of rough set, fuzzy β β -covering rough set (F β β CRS) and multi-granulation rough set are two important concepts, and their applications are extensive in feature selection, data classification analysis and (group) decision making. In this paper, from the perspective of variable scale (different β β -parameters are selected for different coverings or attributes) and multi-granulation, a new concept of variable scale fuzzy β β -covering group approximation space (VS-F β β GAS) is proposed and four models of multi-granulation F β β CRS are established based on VS-F β β GAS. Moreover, traditional attribute reduction is extended to δ δ -approximate reduction, and a novel attribute reduction algorithm based δ δ -approximate reduction is introduced. Through the experimental analysis on an extensive collection of publicly available datasets, the obvious advantages of the models and algorithm in the data classification analysis are demonstrated. In addition, to establish the aforementioned the models of multi-granulation F β β CRS, ( I , SO )-fuzzy rough set model (single granulation) based on fuzzy implication and semi-overlap function is proposed, which theory extends the traditional fuzzy rough set model and has greater flexibility and adaptability.},
  archive      = {J_ASOC},
  author       = {Xiaofeng Wen and Fuchun Sun and Xiaohong Zhang and Mengyuan Li},
  doi          = {10.1016/j.asoc.2024.111982},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111982},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variable scale fuzzy β-covering group approximation space and variable scale multi-granulation FCRSs with applications},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Graph generative adversarial networks with evolutionary
algorithm. <em>ASOC</em>, <em>164</em>, 111981. (<a
href="https://doi.org/10.1016/j.asoc.2024.111981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph adversarial Networks (GANs) have shown state-of-the-art results in numerous application domains. While GANs are difficult to be trained to generate distribution from data descriptions. In order to solve this problem, GraphGAN is an innovative graph representation learning framework in which generative models and discriminative models are trained against minimax game based on game theory. However, existing GraphGAN is often affected by mode collapse and gradient problem. This paper proposed a novel GANs framework, called graph generative adversarial networks with evolutionary algorithm (EGraphGAN), for enhancing GANs training performance in graph structure learning and improving the generator’s competence in generating high-quality data distribution during iterative training process. The generator is regarded as an evolutionary body to continuously mutate and evolve in the environment (discriminator). The discriminator acts as an environment to evaluate the fitness of the individuals generated by generator through the fitness function. During this training process, only individuals with good fitness in each epoch can be retained for the next stage of training. Experiments on multiple challenging datasets showed that EGraphGAN achieves convincing performance and decreases the negative impact of mode collapse and gradient anomalies. The source code is available at https://github.com/codeedit/EGraphGAN .},
  archive      = {J_ASOC},
  author       = {Pengda Wang and Zhaowei Liu and Zhanyu Wang and Zongxing Zhao and Dong Yang and Weiqing Yan},
  doi          = {10.1016/j.asoc.2024.111981},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111981},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph generative adversarial networks with evolutionary algorithm},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DILA: Dynamic gaussian distribution fitting and imitation
learning-based label assignment for tiny object detection.
<em>ASOC</em>, <em>164</em>, 111980. (<a
href="https://doi.org/10.1016/j.asoc.2024.111980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of deep learning technology, significant achievements have been made in the field of general object detection. However, challenges still remain in the detection of tiny objects. There are two main drawbacks: (1) static prior information makes the localization of tiny objects relatively fixed; (2) Intersection over Union (IoU) is highly sensitive to deviations in tiny objects. To this end, we propose a Dynamic Gaussian Distribution Fitting and Imitation Learning-Based Label Assignment (DILA) strategy for Tiny Object Detection. Specifically, to address the positional deviation of effective receptive fields at different network layers during Gaussian modeling, DILA first designs an Adaptive Dynamic Calculation Strategy (ADCS) to compute estimation factors for the effective receptive field in different feature spaces, dynamically modeling prior information using Gaussian distribution. Then, DILA introduces a new Balance of Gaussian Scaling-aware Metric (BGSM) to measure the similarity between tiny bounding boxes and predefined anchors, instead of using IoU, which is highly sensitive to tiny pixel shifts, for sample assignment, thereby providing a more accurate basis for label assignment. Finally, a Detail Information Imitation Compensation Module (DIM) is presented to improve and compensate for the detailed information of tiny objects that troubles label assignment, achieving balanced learning for tiny objects. The proposed DILA strategy can be seamlessly integrated into various anchor-based detectors. Extensive experiments were conducted on three publicly available datasets for tiny object detection. The results indicate that when DILA is embedded into Faster RCNN, it outperforms other state-of-the-art methods in terms of detection performance for tiny objects, achieving an improvement in average precision of 10.3%, 1.5%, and 4.2% on AI-TOD, SODA-D, and VisDrone2019, respectively. The source codes and results are available at: https://github.com/chnu-cpl/DILA .},
  archive      = {J_ASOC},
  author       = {Penglei Chen and Jiangtao Wang and Zhiwei Zhang and Cheng He},
  doi          = {10.1016/j.asoc.2024.111980},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111980},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DILA: Dynamic gaussian distribution fitting and imitation learning-based label assignment for tiny object detection},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A memory interaction quadratic interpolation whale
optimization algorithm based on reverse information correction for
high-dimensional feature selection. <em>ASOC</em>, <em>164</em>, 111979.
(<a href="https://doi.org/10.1016/j.asoc.2024.111979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a key technique for data dimensionality reduction, and there are many challenges in facing the exponential expansion phenomenon of high-dimensional decision space. In order to improve the quality of feature selection, we propose a memory interaction quadratic interpolation whale optimization algorithm based on reverse information correction (RQWOA). First, in order to improve the convergence capability of the Whale Optimization Algorithm (WOA) in high-dimensional space, we propose a quadratic interpolation mechanism with memory information interaction, improve the quality of the datum points used for quadratic interpolation through the proposed memory interaction mechanism, and accelerate the convergence speed and accuracy of the algorithm with the help of the quadratic interpolation on the algorithm exploitation capability. In addition, to address the invalid probability flip in the feature selection subset when mapping to the discrete space, we propose a reverse information correction mechanism to judge and correct each dimension in the feature subset by using the reverse complementary information provided by individuals with poor fitness values in the population, so as to better remove invalid or redundant features and improve the quality of feature selection. Finally, in the 15 high-dimensional feature datasets compared with the comparison methods, the accuracy of RQWOA is improved by 7.49 % on average, and the number of features is compressed by 80.42 % on average, which indicates that the RQWOA algorithm has stronger feature identification and redundancy removal capabilities and can better solve the feature selection problem.},
  archive      = {J_ASOC},
  author       = {Fahui Miao and Yong Wu and Guanjie Yan and Xiaomeng Si},
  doi          = {10.1016/j.asoc.2024.111979},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111979},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memory interaction quadratic interpolation whale optimization algorithm based on reverse information correction for high-dimensional feature selection},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A CNN pruning approach using constrained binary particle
swarm optimization with a reduced search space for image classification.
<em>ASOC</em>, <em>164</em>, 111978. (<a
href="https://doi.org/10.1016/j.asoc.2024.111978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have exhibited exceptional performance in a range of computer vision tasks. However, these deep CNNs typically demand significant computational resources, which not only hinders their practical deployment but also contributes to a considerable carbon footprint. To tackle this issue, several filter pruning methods based on evolutionary algorithms have been proposed to provide significant memory and energy savings during CNN inference. However, due to the curse of high dimensionality in the structure of deep CNNs, the search space expands dramatically, presenting significant challenges for these methods. This paper proposes a novel algorithm called BPSO-FPruner for CNN filter pruning. BPSO-FPruner utilizes a constrained binary particle swarm optimization algorithm for filter pruning, incorporating a new initialization strategy based on filter weighting information and a reduced search space strategy. Extensive validation using VGG, ResNet, DenseNet, and MobileNetv2 architectures on the CIFAR-10, CIFAR-100, and Tiny ImageNet datasets demonstrates the effectiveness of BPSO-FPruner in reducing model computational costs and carbon footprint emissions while maintaining or improving performance.},
  archive      = {J_ASOC},
  author       = {Jihene Tmamna and Emna Ben Ayed and Rahma Fourati and Amir Hussain and Mounir Ben Ayed},
  doi          = {10.1016/j.asoc.2024.111978},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111978},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A CNN pruning approach using constrained binary particle swarm optimization with a reduced search space for image classification},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Twinning quality sensors in wastewater treatment process via
optimized echo state network-based soft sensors. <em>ASOC</em>,
<em>164</em>, 111977. (<a
href="https://doi.org/10.1016/j.asoc.2024.111977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of a large amount of quality-related but hard-to-measure variables usually makes effective monitoring of industrial processes difficult, and even impossible. Soft computing techniques and digital twins can revolutionize standard approaches to solve this issue, even though industrial data are frustrated by nonlinearity, disturbances, and dynamic behaviors. In this light, this paper proposes a data-driven soft sensor with the help of the Fourier amplitude sensitivity test (FAST) and improved probabilistic regularized echo state network (IPRESN), then to twin the quality hardware sensors. Within this framework, the initial step is to rely on the FAST method to measure the collaborated importance of multiple process variables with respect to each quality variable. This quantification facilitates the selection of auxiliary variables for the training of IPRESN. Furthermore, a probabilistic regularization method is learned to improve the robust performance of the echo state network (ESN) by re-designing a new objective function to minimize the mean and variance of the modeling error. Additionally, the whale optimization algorithm (WOA) is improved and then used to optimize the critical hyper-parameters of ESN, thus mitigating the risks associated with suboptimal hyperparameter selection. The effectiveness of the proposed method is verified through a Benchmark Simulation Model 2 (BSM2) and a full-scale wastewater treatment plant data set. The findings show the potential of the proposed method in facilitating digital twin implementations for hardware sensors.},
  archive      = {J_ASOC},
  author       = {Gang Fang and Yiqi Liu},
  doi          = {10.1016/j.asoc.2024.111977},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111977},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Twinning quality sensors in wastewater treatment process via optimized echo state network-based soft sensors},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic harris hawks optimizer based on historical
information and tournament strategy and its application in numerical
optimization of blast furnace ingredients. <em>ASOC</em>, <em>164</em>,
111976. (<a href="https://doi.org/10.1016/j.asoc.2024.111976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harris hawks optimizer (HHO) is a new meta-heuristic optimization algorithm, which is inspired by the cooperative behavior and chasing style of Harris hawks in nature. However, HHO can neither balance exploration and exploitation well nor fully use historical information in the face of complex optimization problems. In order to alleviate the shortcomings of HHO, a dynamic Harris hawks optimizer based on historical information and tournament strategy (DHHO-HITS) is proposed in this paper, in which a dynamic parameter is proposed to adjust the behavior of the Harris hawks in exploration and exploitation. The concept of “archive” is added to store the historical optimal solutions, and then a randomly selected historical optimal solution in the “archive” is used to guide the Harris hawk population, which improves the utilization rate of historical information and the accuracy of the solution. The tournament strategy is used to select a new generation of the population to avoid premature convergence. To verify the performance of DHHO-HITS, the CEC2020 benchmark suite is used to analyze the selection of parameters, the influence of the control parameters, the influence of three improved mechanisms, and the dynamic properties of the algorithm. Then, DHHO-HITS is compared with 20 other algorithms on multiple dimensions of the CEC2017 benchmark suite, CEC2020 benchmark suite, and CEC2021 benchmark suite. Further, the proposed DHHO-HITS is applied to three standard engineering problems. These test results show that DHHO-HITS outperforms most competitors in numerical optimization. Finally, DHHO-HITS is used to solve the numerical optimization of blast furnace ingredients. The simulation results based on actual data show that the optimized ingredient scheme can reduce the CO 2 CO2 emissions of the blast furnace while meeting multiple constraints: for each ton of iron output, the CO 2 CO2 emissions are reduced by 85.7177 kg, accounting for about 9.97% of the total emissions.},
  archive      = {J_ASOC},
  author       = {Zhendong Liu and Yiming Fang and Le Liu and Shuidong Ma},
  doi          = {10.1016/j.asoc.2024.111976},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111976},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic harris hawks optimizer based on historical information and tournament strategy and its application in numerical optimization of blast furnace ingredients},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive overview of the applications of kernel
functions and data-driven models in regression and classification tasks
in the context of software sensors. <em>ASOC</em>, <em>164</em>, 111975.
(<a href="https://doi.org/10.1016/j.asoc.2024.111975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven models can reduce the number of hardware sensors in a process plant by acting as low-cost substitutes for hardware sensors. Since some data-driven models have difficulty dealing with nonlinear data, kernel functions have been integrated into the data-driven models due to their capability to handle this nonlinear behavior of data. However, the existing review studies on kernel functions and data-driven models for regression and classification are still limited. Moreover, for kernel functions, most research studies have only focused on the radial basis function group, such as gaussian and hyperbolic tangent kernel functions. Considering these research gaps, this review study aims to summarize the most up-to-date cumulative studies on kernel functions, their application categories, and their integration with data-driven models. Different from other existing review studies, this study discussed the characteristics, advantages, and disadvantages of different kernel functions. Additionally, this study also summarizes and critically reviews data-driven models for regression and classification tasks, including their advantages and disadvantages. Moreover, this review discovers the state of the art of different kernel functions that were used in data-driven models for regression and classification. Besides, this review study also found that the existing studies on kernel functions are mostly used for the classification task rather than the regression task. In addition, the gaussian kernel is found to be the most applied kernel function in various applications. Lastly, it is recommended to emphasize integrating different kernel functions with adaptive data-driven models in different industrial applications.},
  archive      = {J_ASOC},
  author       = {Joyce Chen Yen Ngu and Wan Sieng Yeo and Teck Fu Thien and Jobrun Nandong},
  doi          = {10.1016/j.asoc.2024.111975},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111975},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive overview of the applications of kernel functions and data-driven models in regression and classification tasks in the context of software sensors},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning with local spatiotemporal structure preserving
for soft sensor development of complex industrial processes.
<em>ASOC</em>, <em>164</em>, 111974. (<a
href="https://doi.org/10.1016/j.asoc.2024.111974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven soft sensors have emerged as indispensable tools for predicting quality variables in complex industrial processes because of their cost-effectiveness and ease of maintenance. In particular, soft sensors based on deep learning have been utilized in extensive research and successful applications in recent years. However, traditional deep learning methods capture hierarchical data features by minimizing global fitting errors, neglecting the local structural characteristics implied in the original data. In this paper, we propose a new deep learning method for soft sensor development. Utilizing autoencoders as the foundational architecture of our network, a new semisupervised strategy is adopted for layerwise pretraining optimization. On the one hand, more representative data features are extracted by maintaining the local spatiotemporal structure of the data; on the other hand, layer-by-layer supervised learning is employed to identify the critical features that are aligned with the ultimate task, which aids in obtaining the optimal network parameters and improving the resulting prediction accuracy. Subsequently, a local spatiotemporal structure-preserving stacked semisupervised autoencoder (LSP-SuAE) is established. To evaluate the feasibility and effectiveness of the proposed approach, experiments are carried out in a real industrial process. A soft sensor based on the LSP-SuAE is developed to predict the rate of ethylbenzene conversion during the dehydrogenation of styrene. The experimental results demonstrate that, compared to five other common or similar data-driven modeling methods, LSP-SuAE exhibits higher prediction accuracy and better stability.},
  archive      = {J_ASOC},
  author       = {Xiao Wang and Xiaomei Qi and Yong Zhang},
  doi          = {10.1016/j.asoc.2024.111974},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111974},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning with local spatiotemporal structure preserving for soft sensor development of complex industrial processes},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy logic and biometric-based lightweight cryptographic
authentication for metaverse security. <em>ASOC</em>, <em>164</em>,
111973. (<a href="https://doi.org/10.1016/j.asoc.2024.111973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Metaverse advances, robust authentication systems become more critical. Traditional security measures are not efficient and adaptable for metaverse scenarios. In this context, this study proposed a two-phase security framework that combines fuzzy logic and convolutional neural network (CNN) techniques for biometric authentication and a lightweight Cryptographic protocol for securing the Metaverse. In the first stage, the CNN model is used to extract accurate biometric signatures from hand tremor data. This innovative method takes into account the inherent unpredictability of biometrics. During the second phase, a cryptographic scheme is used to provide safe mutual authentication between the user and the Metaverse infrastructure. Our findings, which have been verified by BAN Logic analysis, show that the proposed framework achieves a high level of accuracy in user authentication and has robust defensive capabilities against typical cyber-attacks.},
  archive      = {J_ASOC},
  author       = {Brij B. Gupta and Akshat Gaurav and Varsha Arya},
  doi          = {10.1016/j.asoc.2024.111973},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111973},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy logic and biometric-based lightweight cryptographic authentication for metaverse security},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy price prediction based on decomposed price dynamics:
A parallel neural network approach. <em>ASOC</em>, <em>164</em>, 111972.
(<a href="https://doi.org/10.1016/j.asoc.2024.111972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing uncertainties of the world due to geographic tensions, and weather conditions challenge the traditional method to achieve a reliable price prediction of energy future for both asset pricing and risk management. Following the initial success of deep learning models in energy price prediction, we attempt to establish a better architecture of neural networks to improve the prediction accuracy. We propose a novel Parallel Hybrid Neural Network (PHNN) model that utilizes independent sub-networks to effectively capture the distinct features of various sequences. Empirical results demonstrate that the PHNN model exhibits a significant performance enhancement of 16.68%, 14.09%, and 2.34% over the EMD–LSTM, the informer model, and the single LSTM model, respectively. In particular, the PHNN outperforms the single LSTM , which is trained on the same inputs, by 2.34% overall while by a remarkable 4.11% during event periods. This suggests that the PHNN derives notable benefits from its distinct architecture, particularly during the initial phase of extreme events characterized by significant price trend changes. Additionally, the study explores the potential benefits of incorporating additional event-tracking indicators for energy futures price forecasting . However, the findings suggest that these indicators may not consistently provide effective complementary information.},
  archive      = {J_ASOC},
  author       = {Min Zhu and Siyue Zheng and Yu Guo and Yuping Song},
  doi          = {10.1016/j.asoc.2024.111972},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111972},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy price prediction based on decomposed price dynamics: A parallel neural network approach},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Molecular representation contrastive learning via
transformer embedding to graph neural networks. <em>ASOC</em>,
<em>164</em>, 111970. (<a
href="https://doi.org/10.1016/j.asoc.2024.111970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction has shown great performance using graph neural networks (GNNs). However, due to the lack of expansion potential and the scarcity of available labeling data, GNNs are unable to generate appropriate molecular representation. In this study, we propose MolFG, a new contrastive learning (CL) pre-training framework for predicting molecular properties. Meanwhile, we also propose FormerGraph, an effective molecular graph representation strategy, aiming to devise an effective method for learning information regarding molecular features. After pre-training on 10 million unlabeled molecules and then fine-tuning multiple types of downstream tasks to predict molecular properties. The encouraging results revealed that MolFG could effectively extract meaningful chemical insights to generate interpretable representations and differentiate chemically plausible molecular similarities. On most molecular benchmark datasets, MolFG rivals or surpasses supervised learning methods with sophisticated feature engineering. Compared to the previous best supervised model, MolFG demonstrates an average 7.5% gain in ROC-AUC on 7 classification tasks and a 1.9% decrease in scaled average error on 6 regression tasks. Numerous experimental outcomes on downstream tasks demonstrate that the MolFG model can significantly enhance its effectiveness in predicting molecular properties.},
  archive      = {J_ASOC},
  author       = {Yunwu Liu and Ruisheng Zhang and Tongfeng Li and Jing Jiang and Jun Ma and Yongna Yuan and Ping Wang},
  doi          = {10.1016/j.asoc.2024.111970},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111970},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Molecular representation contrastive learning via transformer embedding to graph neural networks},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generation of black-box adversarial attacks using many
independent objective-based algorithm for testing the robustness of deep
neural networks. <em>ASOC</em>, <em>164</em>, 111969. (<a
href="https://doi.org/10.1016/j.asoc.2024.111969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have become increasingly ubiquitous in our daily lives, finding applications in areas such as image recognition, voice recognition, and natural language processing. However, a growing concern revolves around the vulnerability of DNNs to adversarial examples—malicious inputs that can compromise the safety and accuracy of their outcomes. Existing studies predominantly fall into two categories: white-box and black-box techniques. White-box techniques require detailed internal information about the model, often focusing on gradient-based methods. In contrast, black-box techniques, which emulate real-world scenarios more closely, only rely on input–output knowledge. This study focuses specifically on black-box strategies, with key contributions from Single-Objective Variant of Differential Evolution (Pixel-SOO) and Multi-Objective Variant of Differential Evolution (Pixel-MOO) algorithms. While these approaches show promise, they suffer from drawbacks like long execution times and being unable to generate instances for adversarial purposes. To address these challenges, we introduce a novel archive-based Many Independent Objective (MIO) algorithm for using the first time in this context. Our proposed algorithm identifies the most vulnerable image pixels through the MIO algorithm, enabling efficient label flip attacks with minimized attempts. Furthermore, we balance exploration and exploitation by incorporating an adaptive parameter mechanism. The effectiveness of the proposed algorithm is assessed by employing VGG (VGG16 and VGG19) and ResNet (ResNet50, ResNet101, and ResNet152) architectures, both of which are convolutional neural network (CNN) models. The success criterion for the algorithms is to minimize the number of pixel changes while achieving high flip rates with a minimal number of requests to the network. A comprehensive analysis of the experimental results reveals that our algorithm consistently outperforms Pixel-SOO and Pixel-MOO, exhibiting an average speedup of three times compared to Pixel-SOO and seven times compared to Pixel-MOO. In most runs, adversarial attacks are generated with fewer pixel changes than Pixel-MOO and Pixel-SOO. In addition, our findings are openly accessible on GitHub to ensure transparency and reproducibility and encourage future research efforts.},
  archive      = {J_ASOC},
  author       = {Omur Sahin},
  doi          = {10.1016/j.asoc.2024.111969},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111969},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generation of black-box adversarial attacks using many independent objective-based algorithm for testing the robustness of deep neural networks},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cooperative defense of autonomous surface vessels with
quantity disadvantage using behavior cloning and deep reinforcement
learning. <em>ASOC</em>, <em>164</em>, 111968. (<a
href="https://doi.org/10.1016/j.asoc.2024.111968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Surface Vessels (ASVs) excel at undertaking hazardous tasks, garnering significant attention recently. Particularly, ASV cooperative defense is a crucial application for protecting harbors and combating smugglers. Here, ASVs intercept intruders from reaching a protected region. Unlike most research, which assumes defenders with numerical advantages, this work considers a more practical defense mission with fewer defenders, defender damages, and intruders employing evasion strategies. However, interception challenges are also introduced, including ASV underactuated dynamics, a limited interception time window, and environmental nonstationarity. Directly applying existing defense methods to such missions may not achieve success. To surmount the challenges, we propose an ASV decision-making framework by integrating supervised learning and deep reinforcement learning. Initially, supervised learning uses actions from a bi-level controller to train ASVs, addressing underactuated dynamics and aiding policy convergence. Subsequently, deep reinforcement learning explores more effective policies to enhance interception rates. Furthermore, hybrid rewards are meticulously designed to drive policy optimizations while mitigating environmental nonstationarity. Finally, numerical simulations are carried out to verify the effectiveness of our approach.},
  archive      = {J_ASOC},
  author       = {Siqing Sun and Tianbo Li and Xiao Chen and Huachao Dong and Xinjing Wang},
  doi          = {10.1016/j.asoc.2024.111968},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111968},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cooperative defense of autonomous surface vessels with quantity disadvantage using behavior cloning and deep reinforcement learning},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An activity level based surrogate-assisted evolutionary
algorithm for many-objective optimization. <em>ASOC</em>, <em>164</em>,
111967. (<a href="https://doi.org/10.1016/j.asoc.2024.111967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing expensive many-objective optimization problems (MaOPs) is a formidable challenge owing to their intricate objective spaces and high computational demands. Surrogate-assisted evolutionary algorithms (SAEAs) have gained prominence because of their ability to tackle MaOPs efficiently. They achieve this by using surrogate models to approximate objective functions, significantly reducing their reliance on costly evaluations. However, the effectiveness of many SAEAs is hampered by their reliance on various surrogate models and optimization strategies, which often result in suboptimal prediction accuracy and optimization performance. This study introduces a novel approach: an activity level based surrogate-assisted reference vector guided evolutionary algorithm specifically designed for expensive MaOPs. Utilizing the Kriging model and an angle penalty distance criterion, this algorithm effectively filters solutions that require evaluation using the original function. It employs a fixed number of training sets,that are updated via a two-screening strategy that leverages activity levels to refine population screening. This process ensures that the reference vector progressively aligns more closely with the Pareto fronts,which is enhanced by the deployment of adjusted adaptive reference vectors, thereby improving the screening precision. The proposed algorithm was tested against six contemporary algorithms using the DTLZ, WFG, and MaF test suites. The experimental results show that the proposed method outperforms other algorithms in most problems. Furthermore, its application to the cloud computing task scheduling problem underscores its practical value, demonstrating its notable effectiveness. The experimental outcomes attest to the robust performance of the algorithm across both test scenarios and real-world applications.},
  archive      = {J_ASOC},
  author       = {Jeng-Shyang Pan and An-Ning Zhang and Shu-Chuan Chu and Jia Zhao and Václav Snášel},
  doi          = {10.1016/j.asoc.2024.111967},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111967},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An activity level based surrogate-assisted evolutionary algorithm for many-objective optimization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive extreme learning machine using soft computing fuzzy
propositions—validating operating state of solar energy system.
<em>ASOC</em>, <em>164</em>, 111966. (<a
href="https://doi.org/10.1016/j.asoc.2024.111966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper soft computing fuzzy proposition/rule based adaptive extreme learning is proposed. This article illustrates the impact of agricultural solar panel (AgSP) and its operating condition/state during the farming seasons and hence safety of energy system sustained. Extreme learning machine (ExLM) used as learning machine (LM) for artificial intelligence (AI) training to predict the operating state of applied AgSP green energy unit. soft computing Fuzzy rules are framed for every instant of leaning machine to identifying the lower mismatch conditions and avoid the false prediction. Higher and lower limits of adaptive resonance theory (ART) support LM (ART-LM) is properly used by providing unusual dust formation (UnDFR) data and usual/normal dust formation (UsDFR) data for AI training. Outcome of AI neural net is determined by resonant neuron net (RSNN). Predicted variable from ART-LM is calculated with higher and lower limits of dataset captured from applied energy common node (CN). Many test conditions of dust formation on solar photovoltaic (PV) has been validated to predict the state of applied energy system. For any UnDFR confirmation, entire AgSP based distributed generator (DG) unit will isolate from common node/utility grid. Proposed ART-LM based AI algorithm has been validated even for charging state events of electrical vehicle (EV) during the abnormal conditions.},
  archive      = {J_ASOC},
  author       = {K. Harinadha Reddy},
  doi          = {10.1016/j.asoc.2024.111966},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111966},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive extreme learning machine using soft computing fuzzy propositions—Validating operating state of solar energy system},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human movement science-informed multi-task spatio temporal
graph convolutional networks for fitness action recognition and
evaluation. <em>ASOC</em>, <em>164</em>, 111963. (<a
href="https://doi.org/10.1016/j.asoc.2024.111963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rise of health consciousness, people’s demand for fitness has steadily increased. Utilizing automated human action recognition technology to monitor users’ movements during exercise continuously would help prevent situations where incorrect movements lead to injuries while working out. Skeleton-based human action recognition methods can overcome the susceptibility of past color-based and depth-based methods to various external backgrounds and noise, becoming a more successful solution in recent years. In this study, the auto-fitness advisor system we propose not only identifies the category of the action but also assesses the quality of the action and provides suggestions. We integrate human movement science, such as the Five Primary Kinetic Chains (5PKC), which defines the primary physiological principles in human movement, to enhance the accuracy of fitness action recognition by providing a more precise relationship between the human skeleton and muscles. For assessing the quality of movements and providing suggestions, we have designed a multi-task objective function within our model. Overall, the proposed model is a multi-task model based on Spatio Temporal Graph Convolutional Networks (ST-GCN), which employs the Five Primary Kinetic Chains (5PKC) as a partitioning strategy for skeletal information. In our experiments, we not only collected a certain amount of datasets in gyms to validate the performance of our model but also compared it with other current methods using existing public datasets.},
  archive      = {J_ASOC},
  author       = {Jia-Wei Chang and Ming-Hung Chen and Hao-Shang Ma and Hao-Lan Liu},
  doi          = {10.1016/j.asoc.2024.111963},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111963},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human movement science-informed multi-task spatio temporal graph convolutional networks for fitness action recognition and evaluation},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate technique for the prediction and classification
of brain tumor using deep shallow network. <em>ASOC</em>, <em>164</em>,
111962. (<a href="https://doi.org/10.1016/j.asoc.2024.111962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors, a major cause of death, carry a substantial socio-economic burden. It is essential to distinguish between various types of tumors, such as gliomas, meningiomas , and pituitary tumors , using MRI data. This assists radiologists and eliminates the need for risky histology biopsies. The objective of the proposed work is to create an automated system capable of distinguishing tumor types. This system integrates a deep, shallow network with the ResNet18 model for feature extraction and machine learning methods for classification on images from the Kaggle dataset. The standalone ResNet18 model produced an accuracy of 91.42 % and to enhance the accuracy, features extracted from the pool5 layer of the ResNet18 architecture were examined by chi-square algorithm. The optimal deep features extracted using the chi-square algorithm were integrated with 17 machine learning classifier techniques comprising variants of SVM , KNN , and ensemble techniques. The proposed method predicted the classes, yielding an accuracy of 99.9 % for cubic SVM among the variants of SVM , 94 % for cosine KNN among the variants of KNN, and 99.8 % for the Adaboost algorithm only, with a maximum of the top 10 features justified using local interpretable model-agnostic (LIME) and gradient-weighted class activation mapping (Grad-CAM) visualization techniques.},
  archive      = {J_ASOC},
  author       = {Gayathri Devi Krishnamoorthy and Kishore Balasubramanian},
  doi          = {10.1016/j.asoc.2024.111962},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111962},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multivariate technique for the prediction and classification of brain tumor using deep shallow network},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy control charts for individual observations to analyze
variability in health monitoring processes. <em>ASOC</em>, <em>164</em>,
111961. (<a href="https://doi.org/10.1016/j.asoc.2024.111961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In healthcare monitoring, the quality of medical services and patient outcomes are significantly influenced by the unnatural variations emphasizing the importance of effective control and monitoring strategies. In such scenarios the quality is compromised when a shift in the medical process is not detected timely. In this study, we commenced a comprehensive analysis of the variability present in the patient&#39;s hematocrit levels. We introduced a novel approach using fuzzy control charts for individual measurements to detect shift within hematocrit levels effectively. For the initial forecasting of hematocrit level variability, we employed the exponential smoothing method by using R software. Following this, we proposed a set of fuzzy control charts intended for individual measurements, including the fuzzy moving average control chart, fuzzy weighted moving control chart, and fuzzy moving range control chart. These control charts are defined with fuzzy control rules, allowing them to analyze and interpret the small shifts in the process precisely. Also, to measure the process&#39;s capability for providing a deeper understanding of the processes, fuzzy process capability indices are introduced. A Monte Carlo simulation approach is employed to obtain the performance metrics. Finally, a case study sourced from Kaggle was conducted to evaluate the performance of the proposed fuzzy control charts for assessing hematocrit levels.},
  archive      = {J_ASOC},
  author       = {Muhammad Usman Aslam and SongHua Xu and Muhammad Noor-ul-Amin and Sajid Hussain and Muhammad Waqas},
  doi          = {10.1016/j.asoc.2024.111961},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111961},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy control charts for individual observations to analyze variability in health monitoring processes},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving satellite image data downlink scheduling problem
with family attribute via a bi-stage differential evolutionary
algorithm. <em>ASOC</em>, <em>164</em>, 111960. (<a
href="https://doi.org/10.1016/j.asoc.2024.111960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the asynchronous development of observation and transition capabilities, the original image data ( OID ) obtained by a one-time observation cannot be completely transmitted in one transmission cycle between the EOS and GS , which is named a visible time window ( VTW ). First, it is needed to segment the OID into several segmented image datasets ( SIDs ), which are then transmitted in several VTWs . Furthermore, this can cause the satellite image data downlink scheduling problem ( SIDSP ). This study defines an innovative SIDSP as a satellite image data downlink scheduling problem with the family attribute ( SIDSPFA ), where massive OID are segmented using a fast segmentation operator, and all SIDs and other no-segmented OID are transmitted in the second step. In addition, two optimization objectives, namely the failure rate ( FR ) and Segmentation Time ( ST ) of the image data transmission, are defined to formalize the SIDSPFA as a bi-objective discrete optimization model. Further, a bi-stage differential evolutionary algorithm ( DE-CD ) is developed. The results of the extensive simulation tests verify the efficiency of the models, strategies, algorithms, and operators used in this study.},
  archive      = {J_ASOC},
  author       = {Zhongxiang Chang and Zhongbao Zhou and Xiaolu Liu},
  doi          = {10.1016/j.asoc.2024.111960},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111960},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving satellite image data downlink scheduling problem with family attribute via a bi-stage differential evolutionary algorithm},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evolutionary knowledge transfer framework for solving
workgroup reconfiguration problem in aircraft final assembly.
<em>ASOC</em>, <em>164</em>, 111959. (<a
href="https://doi.org/10.1016/j.asoc.2024.111959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Final assembly is a crucial aspect of aircraft manufacturing. The introduction of reconfigurable workgroups to aircraft final assembly lines (AFALs) significantly increases assembly flexibility and efficiency. However, the number of reconfigurations and their corresponding assignments in the constraint-complex AFAL must be considered jointly during the assignment of reconfigurable workgroups. This presents a great difficulty when optimizing the reconfiguration strategy. In this study, an optimization framework based on evolutionary knowledge transfer (EKT) is proposed to efficiently search the metameric-encoded search space. EKT extracts and transfers valuable knowledge in different reconfiguration sub-problems according to the temporal relations between encodings. Meanwhile, the scope of the EKT is constrained based on the historical individual changes of different sub-problems and different regions of the objective space. The experimental results show that the proposed framework outperforms state-of-the-art algorithms on real-world production data. The behavior and mechanism of the EKT approach during the optimization process are also investigated in detail.},
  archive      = {J_ASOC},
  author       = {Hongxia Cai and Qiucheng Ye and Qijie Zhao and Lilan Liu},
  doi          = {10.1016/j.asoc.2024.111959},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111959},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolutionary knowledge transfer framework for solving workgroup reconfiguration problem in aircraft final assembly},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive masked autoencoder transformer for image
classification. <em>ASOC</em>, <em>164</em>, 111958. (<a
href="https://doi.org/10.1016/j.asoc.2024.111958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have exhibited exceptional performance across a broad spectrum of visual tasks. Nonetheless, their computational requirements often surpass those of prevailing CNN-based models. Token sparsity techniques have been employed as a means to alleviate this issue. Regrettably, these techniques often result in the loss of semantic information and subsequent deterioration in performance. In order to address these challenges, we propose the Adaptive Masked Autoencoder Transformer (AMAT), a masked image modeling-based method. AMAT integrates a novel adaptive masking mechanism and a training objective function for both pre-training and fine-tuning stages. Our primary objective is to reduce the complexity of Vision Transformer models while concurrently enhancing their final accuracy. Through experiments conducted on the ILSVRC-2012 dataset, our proposed method surpasses the original ViT by achieving up to 40% FLOPs savings. Moreover, AMAT outperforms the efficient DynamicViT model by 0.1% while saving 4% FLOPs. Furthermore, on the Places365 dataset, AMAT achieves a 0.3% accuracy loss while saving 21% FLOPs compared to MAE. These findings effectively demonstrate the efficacy of AMAT in mitigating computational complexity while maintaining a high level of accuracy.},
  archive      = {J_ASOC},
  author       = {Xiangru Chen and Chenjing Liu and Peng Hu and Jie Lin and Yunhong Gong and Yingke Chen and Dezhong Peng and Xue Geng},
  doi          = {10.1016/j.asoc.2024.111958},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111958},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive masked autoencoder transformer for image classification},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine and deep learning methods for concrete strength
prediction: A bibliometric and content analysis review of research
trends and future directions. <em>ASOC</em>, <em>164</em>, 111956. (<a
href="https://doi.org/10.1016/j.asoc.2024.111956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This review paper provides a detailed evaluation of the existing landscape and future trends in applying machine learning and deep learning approaches for predicting concrete strength in construction engineering. The study contextualizes the investigation of machine learning and deep learning in concrete strength prediction, emphasizing the need for precise strength forecasting in construction. This hybrid review uses quantitative analysis of an extensive collection of 1005 research publications from the Scopus database (2010−2023) to identify clusters, hotspots, and gaps in this area, giving a systematic way to analyze the field&#39;s dynamics. This review reveals major research clusters such as concrete characteristics, sustainability, error analysis, and optimization. It identifies research hotspots like compressive strength prediction, reinforced concrete, and neural networks. The review illuminates future research paths, ethical concerns, and environmental implications. It emphasizes the relevance of fairness, bias reduction, and sustainability in developing and deploying machine and deep learning models in the construction sector and the necessity for specialized models in forecasting concrete durability, sustainable concrete strength, and shear strength.},
  archive      = {J_ASOC},
  author       = {Raman Kumar and Essam Althaqafi and S Gopal Krishna Patro and Vladimir Simic and Atul Babbar and Dragan Pamucar and Sanjeev Kumar Singh and Amit Verma},
  doi          = {10.1016/j.asoc.2024.111956},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111956},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Machine and deep learning methods for concrete strength prediction: A bibliometric and content analysis review of research trends and future directions},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retweeting behavior prediction based on dynamic bayesian
network classifier in microblogging networks. <em>ASOC</em>,
<em>164</em>, 111955. (<a
href="https://doi.org/10.1016/j.asoc.2024.111955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, predicting user retweeting behavior in microblogging networks has gained considerable attention. Solving this problem allows for understanding the underlying mechanism of information diffusion and analyzing diffusion&#39;s evolution concerning a particular original microblog over time. Time-series data from microblogging networks are ubiquitous, and user retweeting behavior prediction is primarily considered a classification task. Consequently, there is a significant demand for accurate classification of time-series data. Several influential factors affect user retweeting behavior prediction in microblogging networks. However, much of the existing research focuses on studies that neglect the impact of users&#39; indirect social influence strength. This neglect leads to inaccurate forecasts of indirect retweeting behavior for a particular original microblog. To address this, this paper presents a new time-sensitive measure of social influence strength based on a combination of users&#39; temporal behavior patterns and the shortest cascade path length. Second, it proposes a prediction model based on Dynamic Bayesian Network derivative classifiers called V-DBNC to accurately predict user retweeting behavior over time. The proposed time-sensitive social influence strength factor and other factors, such as individual behavior and similarity-based driving factors, are applied to the V-DBNC model to achieve accurate user retweeting behavior prediction in the microblogging network. The joint density of factors in this classifier is estimated using a multivariate Gaussian kernel function with smoothing parameters. The V-DBNC model is empirically optimized by splitting the smoothing parameters into different-scale intervals and using the model averaging method to select the optimal classifier. Additionally, the proposed model&#39;s relatively simple structure helps overcome the overfitting problem and allows it to accumulate classification information through iterative evolution, promoting generalization. The real Twitter microblogging dataset is used to evaluate the performance of V-DBNC. The experimental results demonstrated that the proposed model outperforms other compared approaches when dealing with the classification of time-series data.},
  archive      = {J_ASOC},
  author       = {Rahebeh Mojtahedi Safari and Amir Masoud Rahmani and Sasan H. Alizadeh},
  doi          = {10.1016/j.asoc.2024.111955},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111955},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Retweeting behavior prediction based on dynamic bayesian network classifier in microblogging networks},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A coevolutionary algorithm using self-organizing map
approach for multimodal multi-objective optimization. <em>ASOC</em>,
<em>164</em>, 111954. (<a
href="https://doi.org/10.1016/j.asoc.2024.111954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Multi-Objective Problems (MMOPs) are frequently encountered in the real world. Traditional Multimodal Multi-Objective Evolutionary Algorithms (MMEAs) often find multiple Pareto optimal solutions with the same objective values. However, in real-world problems, there often exist multiple global optimal solutions and local optimal solutions at the same time. Ensuring that these solution sets are obtained simultaneously is the concern of most current researchers. To address this issue, this paper proposes a novel multimodal multi-objective evolutionary algorithm named CoSOMEA. In the CoSOMEA, a Self-organizing map (SOM) neural network is used to extract the information of decision space to ensure better exploration of the global optima and exploitation of the local optima. Meanwhile, coevolutionary mechanism are used to ensure a balance between the exploration and exploitation in order to avoid the algorithm falling into local areas. The three test suites named IDMP, IDMP_ee and MMF are adopted to verify the effectiveness of proposed algorithm. Experimental results demonstrate that the CoSOMEA exhibits competitive performance in solving MMOPs compared to other state-of-the-art MMEAs.},
  archive      = {J_ASOC},
  author       = {Zongli Liu and Yuze Yang and Jie Cao and Jianlin Zhang and Zuohan Chen and Qingyang Liu},
  doi          = {10.1016/j.asoc.2024.111954},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111954},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A coevolutionary algorithm using self-organizing map approach for multimodal multi-objective optimization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Power system loading margin enhancement based on a
multi-objective TCSC placement to mitigate the risk of blackouts.
<em>ASOC</em>, <em>164</em>, 111953. (<a
href="https://doi.org/10.1016/j.asoc.2024.111953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving power systems, loading margin ( LM LM ) enhancement has a significant effect on the mitigation of the risk of cascading line outages and thereby the occurrence of large blackouts. On the other hand, flexible alternating current transmission system (FACTS) devices, especially thyristor-controlled series compensated (TCSC) devices, are more effective than other mitigation strategies such as constructing new lines because of environmental and economic reasons. In this paper, a multi-objective framework is presented to maximize the LM LM with a minimum installation cost of TCSCs. The optimization problem is solved by using the ɛ-constraint method. The benefit of each Pareto solution is calculated by the probabilistic risk assessment, and the most preferred one is selected by a benefit/cost analysis. The proposed method is developed based on a modified alternating current (AC) version of the ORNL–Pserc–Alaska (OPA) model inspired by the self-organized criticality (SOC) theory in complex power grids. Using the benefit/cost criterion, it is revealed that by the proposed approach, a larger value for the most preferred Pareto solution is obtained in comparison with the defined transmission expansion planning (TEP) scenarios. It confirms the suitability of the proposed method to suppress the cascading outages instead of expensive and time-consuming policies such as building new lines.},
  archive      = {J_ASOC},
  author       = {Mohamad Reza Tavakoli and Akbar Ebrahimi},
  doi          = {10.1016/j.asoc.2024.111953},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111953},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Power system loading margin enhancement based on a multi-objective TCSC placement to mitigate the risk of blackouts},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive multi-population artificial bee colony algorithm
based on fitness landscape analysis. <em>ASOC</em>, <em>164</em>,
111952. (<a href="https://doi.org/10.1016/j.asoc.2024.111952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the multi-population mechanism stands out as an effective method to improve the artificial bee colony (ABC) algorithm. However, many existing multi-population ABC variants still exhibit some deficiencies, such as a fixed number of subpopulations and a single type of population partition method. Hence, in this work, a new multi-population ABC variant, called ABC-AMP, is proposed by designing an adaptive multi-population mechanism. For ABC-AMP, to determine the number of subpopulations and the type of population partition method, the fitness landscape analysis technique is first used to identify the problem features. Then, based on the identified features, the number of subpopulations and the most suitable type of population partition methods are determined adaptively. Meanwhile, to preserve the search experience, an improved dual-elite search strategy is crafted in the scout bee phase. To evaluate the performance of ABC-AMP, extensive experiments are conducted on two renowned test suites (CEC2013 and CEC2017) and three practical optimization problems. In comparison with four multi-population ABC variants, five other state-of-the-art ABC variants and four non-ABC variants, the comparative results demonstrate that ABC-AMP outperforms its competitors, showcasing a competitive performance.},
  archive      = {J_ASOC},
  author       = {Xinyu Zhou and Xiaocui Zhang and Weifeng Gao and Hui Wang and Yong Ma},
  doi          = {10.1016/j.asoc.2024.111952},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111952},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive multi-population artificial bee colony algorithm based on fitness landscape analysis},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Boosting cattle face recognition under uncontrolled scenes
by embedding enhancement and optimization. <em>ASOC</em>, <em>164</em>,
111951. (<a href="https://doi.org/10.1016/j.asoc.2024.111951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate individual cattle identification is crucial for modern precision cattle farming. However, practical applications encounter challenges due to factors such as shooting distance and angles, cattle movements, weather conditions, and cattle face posture. Existing models struggle with low recognition accuracy for low-recognizable images and poor robustness to facial pose variations, presenting urgent problems that need to be addressed. In this study, a novel cattle face recognition method was proposed, aiming to solve the aforementioned issues by analyzing and optimizing the embedding distribution of cattle faces. Firstly, MobileFaceNet was employed as the feature extraction network to sufficiently extract discriminative representations. Secondly, due to the tendency of low-recognizable samples to cluster in specific areas of the embedding space, an Embedding Enhancement Module (EEM) was proposed. This module drives embedding features away from ineffective embedding spaces, thereby enhancing the model&#39;s ability to extract identity features from less recognizable cattle faces. Finally, an Embedding Optimization Module (EOM) was proposed, which utilized a Sub-Center method to alleviate the model&#39;s learning difficulty during early training stages and achieved &quot;pose-irrelevant&quot; clusters by merging Sub-Centers, enhancing robustness to facial pose variations. Experimental results confirmed the effectiveness of the proposed method, with it outperforming the baseline by up to 2.12 % in accuracy, achieving a recognition accuracy of 98.69 %. Furthermore, the application of the proposed method in cattle farms further demonstrated the significant potential value of this work.},
  archive      = {J_ASOC},
  author       = {Xingshi Xu and Hongxing Deng and Yunfei Wang and Shujin Zhang and Huaibo Song},
  doi          = {10.1016/j.asoc.2024.111951},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111951},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Boosting cattle face recognition under uncontrolled scenes by embedding enhancement and optimization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on interactive evolutionary
computation in the first two decades of the 21st century. <em>ASOC</em>,
<em>164</em>, 111950. (<a
href="https://doi.org/10.1016/j.asoc.2024.111950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive evolutionary computation (IEC) has demonstrated significant success in addressing numerous real-world problems that are challenging to quantify mathematically or are inadequately evaluated using conventional computational models. This success arises from IEC’s ability to effectively amalgamate evolutionary computation (EC) algorithms with expert knowledge and user preferences. These problems encompass the creative and personalized generation of products, art, and sound; the design optimization of communication systems, environments, and pharmaceuticals; and expert support in areas such as portfolio selection and hearing aid fitting, among others. Despite significant advancements in IEC over the past two decades, no major comprehensive survey encompassing all aspects of IEC research has been conducted since 2001. This article aims to address this gap by providing a comprehensive survey and an enriched definition and scope of IEC, along with innovative ideas for future research in this field. The proposed IEC definition more clearly reflects the mechanism and current research status of the IEC. Additionally, the survey categorizes IEC research into five distinct directions from a problem-oriented perspective: interactive evolutionary computation algorithms, IEC algorithm improvements, evolutionary multi-objective optimization (EMO) with IEC, human perception studies with IEC, and IEC applications. Each direction is meticulously explored, elucidating its contents and key features, while providing a concise summary of pertinent IEC studies. Finally, the survey investigates several promising future trends in IEC, analyzing them through the lens of these five directions and considering the current perspective of computational intelligence, artificial intelligence, and human-machine interaction.},
  archive      = {J_ASOC},
  author       = {Yanan Wang and Yan Pei},
  doi          = {10.1016/j.asoc.2024.111950},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111950},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive survey on interactive evolutionary computation in the first two decades of the 21st century},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Occult lymph node metastasis prediction in non-small cell
lung cancer based self-supervised pretrained and hyperbolic theory.
<em>ASOC</em>, <em>164</em>, 111949. (<a
href="https://doi.org/10.1016/j.asoc.2024.111949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting occult lymph node metastasis in non-small cell lung cancer (NSCLC) patients is pivotal for tailoring appropriate surgical and therapeutic interventions. This prognostic factor remains underexplored, largely due to the intricate variability of occult lymph node characteristics and the absence of a pathologically confirmed predictive dataset. Addressing this gap, we retrospectively assembled a dataset of occult lymph node metastases (TJ-OLNM) from NSCLC patients who underwent chest Computed Tomography (CT) scans at Tongji Hospital, Tongji University from 2016 to 2021. Utilizing this dataset, we developed a novel self-supervised learning model, the Occult Lymph Node Metastasis Network (OLNM-Net), which leverages hyperbolic metric few-shot learning to enhance the prediction accuracy of occult metastases. Our comprehensive evaluations demonstrate that OLNM-Net significantly outperforms existing models in predicting occult lymph node metastasis, offering new insights into the preoperative assessment of NSCLC and advancing the application of machine learning in medical diagnostics.},
  archive      = {J_ASOC},
  author       = {Haizhou Xu and Jiaqi Wu and Yujia Yu and Wenkai Huang and Jiong Ni},
  doi          = {10.1016/j.asoc.2024.111949},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111949},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Occult lymph node metastasis prediction in non-small cell lung cancer based self-supervised pretrained and hyperbolic theory},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of mat-heuristics for combinatorial optimisation
problems: Variants, trends and opportunities. <em>ASOC</em>,
<em>164</em>, 111947. (<a
href="https://doi.org/10.1016/j.asoc.2024.111947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey paper presents an overview of recent application of mat-heuristics on combinatorial optimisation problems (COPs) from 2018 to 2024. In this review, we categorise the mat-heuristics into six categories based on three integration types (loose, tight and multi) and two approaches (direct and decomposition). Descriptive statistics reveal that tight integration mat-heuristics are widely favoured. It is also observed that direct approaches are more commonly employed compared to decomposition approaches, perhaps due to the complexity involved in the latter. Next, we briefly present the mechanism of each mat-heuristic and its performance in a comparison to other state-of-the-art solution methodologies. CPLEX emerges as the predominant solver. Mat-heuristics have demonstrated their versatility across COPs, consistently achieving or setting new best-known solutions (BKS). We analyse highly effective mat-heuristics and outline the implementation strategies employed by those that managed to set new BKS. In addition, we discuss the advantages and challenges of utilising mat-heuristics as a solution methodology, as well as future research opportunities in this domain.},
  archive      = {J_ASOC},
  author       = {Chong Man Ngoo and Say Leng Goh and San Nah Sze and Nasser R. Sabar and Mohd Hanafi Ahmad Hijazi and Graham Kendall},
  doi          = {10.1016/j.asoc.2024.111947},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111947},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey of mat-heuristics for combinatorial optimisation problems: Variants, trends and opportunities},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A sine cosine algorithm guided by elite pool strategy for
global optimization. <em>ASOC</em>, <em>164</em>, 111946. (<a
href="https://doi.org/10.1016/j.asoc.2024.111946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When handling global optimization problems by metaheuristic algorithms (MAs), an important yet difficult assignment is to keep a tradeoff between the swarm’s diversity and convergence. Hence, this paper develops an enhanced sine cosine algorithm called EPSCA to achieve the above target. In EPSCA, to balance diversity and convergence, the elite pool strategy and Brownian motion are introduced to modify the position updating formula of the original SCA. Then the pattern search method serves as a local search tool to reinforce the quality of the current best solution. Additionally, an efficient mutation operator is devised to discourage premature convergence. The comparative analysis between the developed EPSCA and other state-of-the-art techniques is conducted on 30 CEC2017 benchmarks. Besides, EPSCA is employed to identify the core parameters of photovoltaic (PV) systems. The experimental evidence based on 30 independent runs demonstrated that EPSCA can perform better than four state-of-the-art SCA variants and five other well-known methodologies. The source code of EPSCA is publicly available at https://github.com/denglingyun123/EPSCA .},
  archive      = {J_ASOC},
  author       = {Lingyun Deng and Sanyang Liu},
  doi          = {10.1016/j.asoc.2024.111946},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111946},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A sine cosine algorithm guided by elite pool strategy for global optimization},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic path planning of autonomous bulldozers using
activity-value-optimised bio-inspired neural networks and adaptive cell
decomposition. <em>ASOC</em>, <em>164</em>, 111944. (<a
href="https://doi.org/10.1016/j.asoc.2024.111944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous bulldozers encounter critical challenges in dynamic path planning in complex construction environments such as dynamic obstacles and narrow passages. Bio-inspired neural networks (BINN) are commonly employed for real-time path planning in dynamic environments. However, the algorithm is not feasible enough to calculate the positive input of neighbour cells, thus resulting in unreasonable distribution of activity values in the obstacle-dense regions. Additionally, the use of regular grids results in excessive computation. These drawbacks cause the algorithm to plan irrational paths and reduce computational efficiency. This study proposed an activity-value-optimised BINN and adaptive cell decomposition for the dynamic path planning of autonomous bulldozers. First, an adaptive cell decomposition method was proposed to model a highly intricate and constantly evolving environment, resulting in a reduction in the computational workload of the BINN. Second, the calculation of the activity value is optimised by enhancing the shunt equation of the BINN, which considers both obstacles and grid levels. This improvement enabled more reasonable distribution results. The smoothness of dynamic path planning is refined by incorporating the turning radius, which can be more effectively integrated with local path planning. Compared with the original BINN algorithm and other baseline algorithms, a practical large-scale hydropower project application demonstrated that the proposed method can effectively reduce the number of computational iterations, enrich the travel direction, and improve path quality while ensuring dynamic obstacle avoidance.},
  archive      = {J_ASOC},
  author       = {Xiangyun Meng and Haojun Gao and Jiajun Wang and Xiaoling Wang and Hongling Yu and Jun Zhang},
  doi          = {10.1016/j.asoc.2024.111944},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111944},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic path planning of autonomous bulldozers using activity-value-optimised bio-inspired neural networks and adaptive cell decomposition},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of concrete compressive strength from
non-destructive tests using a customized neural network and genetic
algorithm. <em>ASOC</em>, <em>164</em>, 111941. (<a
href="https://doi.org/10.1016/j.asoc.2024.111941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach for deriving an equation aimed at predicting concrete compressive strength, leveraging two non-destructive test results: ultrasonic pulse velocity and rebound hammer value. Traditional regression methods often fail to capture the complex non-linear relationships crucial for accurate concrete CS estimation, leading to limited effectiveness. In contrast, while machine learning models excel at mapping these non-linear relationships from non-destructive test results, their intricate internal mechanisms can be opaque, posing challenges for practical application and comprehension by engineers. This study aims to bridge these gaps by developing an estimation equation derived from the decoded weights of a neural network specifically designed to capture the intricate nonlinear mapping relationship. Initially, the equation is intricate and lengthy, comprising multiple terms. To streamline and refine it, a genetic algorithm is employed, focusing solely on the most pivotal terms. Consequently, the refined prediction equation demonstrates superior estimation performance with an RMSE of 3.40 MPa, an MAE of 2.70 MPa, an R 2 of 0.92, and an R of 0.97 compared to several existing formulations. Furthermore, a comparative analysis is undertaken to assess the impact of the degree of equation simplification on its predictive accuracy. The findings offer insights into the pivotal roles of exponential function terms in enhancing prediction performance, as well as elucidating trigonometric function terms contribute the model’s complex non-linear mapping capability.},
  archive      = {J_ASOC},
  author       = {Jun Su Park and Sinwon Park and Byung Kwan Oh and Taehoon Hong and Dong-Eun Lee and Hyo Seon Park},
  doi          = {10.1016/j.asoc.2024.111941},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111941},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Estimation of concrete compressive strength from non-destructive tests using a customized neural network and genetic algorithm},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainability analysis: An in-depth comparison between
fuzzy cognitive maps and LAMDA. <em>ASOC</em>, <em>164</em>, 111940. (<a
href="https://doi.org/10.1016/j.asoc.2024.111940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, it has become very relevant that machine learning techniques can provide an explanation of the results they generate, which is even more relevant in certain domains, called critical, such as health and energy, among others. For this reason, several explainability methods have been generated in the literature. Some of them are Local Interpretable Model-Agnostic Explanations (LIME) and Feature Importance, which have been used in a wide range of problems. The objective of this work is to analyze the explainability capacity of the Learning Algorithm for Multivariate Data Analysis (LAMDA) and Fuzzy Cognitive Maps (FCM), which have been used with great interest due to their interpretability, management of uncertainty during the inference process, simplicity in its use, among other things. For the development of this work, data from two critical domains have been considered, health (COVID-19 and Dengue datasets) and energy (energy price dataset), for which prediction/classification models have been developed using LAMDA and FCM techniques. Afterward, two explainability techniques were used to analyze the explainability provided by each method, one based on the LIME method and the other on the feature importance method, the latter adapted to our work by being based on the permutation of values. Finally, the work proposes two new explainability methods, one based on causal inference and the other on the degrees of membership of the variables to the classes. The latter, in particular, allows doing an explainability analysis by class. The new explainability methods reproduce the results of well-known explainability methods in the literature such as LIME and feature importance, with less execution cost, and also, with an explainability analysis by class. This work opens the doors to new work on class explainability. Furthermore, we see that machine learning approaches based on causal or fuzzy relationships are quite self-explanatory, but specific explainability methods such as those we propose in the work allow us to study particular aspects, such as highly important variables, which general explainability methods do not allow us to do, such as LIME and feature importance.},
  archive      = {J_ASOC},
  author       = {Diego Benito and Carlos Quintero and Jose Aguilar and Juan Marcos Ramírez and Antonio Fernández-Anta},
  doi          = {10.1016/j.asoc.2024.111940},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111940},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainability analysis: An in-depth comparison between fuzzy cognitive maps and LAMDA},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trisection-fusion and fusion-trisection methods of three-way
conflict analysis with pythagorean fuzzy information. <em>ASOC</em>,
<em>164</em>, 111939. (<a
href="https://doi.org/10.1016/j.asoc.2024.111939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basic task of Pawlak conflict analysis is to cluster agents based on their attitudes towards various issues. When ratings of agents are imprecise and uncertain, for example, as represented by a Pythagorean fuzzy situation table, single-measure based methods fall short in ensuring the accuracy of agent clusters. Multi-measure based conflict analysis methods are more effective for generating higher-quality agent clusters. The objective of this paper is to propose two new multi-measure based methods under uncertainty represented by Pythagorean fuzzy sets. We introduce three distinct conflict measures regarding an issue by leveraging the maximum positive and negative agents. The first measure is based on support degrees, the second measure incorporates opposition degrees, and the third measure considers both support and opposition degrees. These measures trisect a set of agents into three disjoint coalitions concerning an individual issue. For multiple issues, we propose two models by combing the two fundamental tasks, namely, trisection and fusion. A trisection-fusion model amalgamates a family of trisections generated from multiple issues based on conflict measures regarding single issues. Matrix representations of trisections are provided, and trisection fusions are transformed into a series of matrix operations. A fusion-trisection model trisects the set of agents according to fused conflict measures on multiple issues. To demonstrate the value of the two models, we apply them in assisting decision-makers in Hunan Province, China, to adjust development plans. The experimental results show that multi-measure based models offer decision-makers more comprehensive guidance.},
  archive      = {J_ASOC},
  author       = {Guangming Lang and Weiping Ding and Duoqian Miao and Hamido Fujita and Yiyu Yao},
  doi          = {10.1016/j.asoc.2024.111939},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111939},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Trisection-fusion and fusion-trisection methods of three-way conflict analysis with pythagorean fuzzy information},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Realistic aggregate based on rough textures with deep
learning. <em>ASOC</em>, <em>164</em>, 111938. (<a
href="https://doi.org/10.1016/j.asoc.2024.111938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model construction is a critical element in simulating the mesoscale structure of concrete. This study introduces an advanced deep learning approach that excels in generating highly realistic aggregates with diverse and heterogeneous surface roughness. The methodology leverages a synergy between Gaussian Process Regression (GPR) and Convolutional Neural Networks (CNNs) to imbue textures with remarkable variations, all originating from a single source texture. Initially, aggregates are randomly generated through the cell fracture technique, and subsequently, the Catmull–Clark subdivision algorithm is applied to refine and smoothen their surfaces. The integration of displacement mapping, in conjunction with the generated texture, results in the creation of intricately textured rough surfaces. Significantly, this research presents an innovative technique for introducing a wide spectrum of surface roughness variations to aggregates, greatly enhancing our ability to construct highly accurate mesoscale models for concrete. This groundbreaking development not only enriches the domain of concrete modeling but also clears a path for elevated levels of precision and sophistication in the realm of concrete structure analysis.},
  archive      = {J_ASOC},
  author       = {Linmei Wu and Peng Liu},
  doi          = {10.1016/j.asoc.2024.111938},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111938},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Realistic aggregate based on rough textures with deep learning},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pareto-optimality based black widow spider algorithm for
energy efficient flexible job shop scheduling problem considering new
job insertion. <em>ASOC</em>, <em>164</em>, 111937. (<a
href="https://doi.org/10.1016/j.asoc.2024.111937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of flexible job shops is on the rise, driven by the decreasing life cycle of consumer products. In dynamic flexible job shops, the occurrence of disruptive events like machine breakdown, tool wear, and new job arrivals is common, emphasizing the critical need for efficient scheduling to achieve productivity and cost-effectiveness. Additionally with the growing concerns about energy consumption and global warming, the imperative to contemplate workshops with lower energy usage is becoming increasingly significant. Therefore, in this study a multi-objective dynamic flexible job shop scheduling problem (MODFJSP) with insertion of new jobs for optimization objectives: makespan, total energy consumption and schedule instability is addressed. A novel multi-objective black widow spider algorithm (MOBWSA), inspired by the mating behavior of black widow spiders, is proposed. The MOBWSA ensures an optimal balance between minimizing makespan and energy consumption through its superior evolutionary processes of procreation, mutation and cannibalism. Furthermore, a strategic on/off strategy is implemented to enhance energy efficiency. The disruptions in schedule at rescheduling stage are addressed through the proposed sigmoidal weighted instability function. To handle the multi-objective nature of the problem this work incorporates Pareto-optimality approach in combination with novel hybrid crowding distance metric. The optimization is further aided by exploitation of search space through an ensemble of local search operators. Testing and evaluation of the MOBWSA with 30 benchmark problems is conducted, and its performance for multiple metrics is compared with recently published state-of-the-art algorithms. The results establish the effectiveness and efficiency of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Kashif Akram and Muhammad Usman Bhutta and Shahid Ikramullah Butt and Syed Husain Imran Jaffery and Mushtaq Khan and Alam Zeb Khan and Zahid Faraz},
  doi          = {10.1016/j.asoc.2024.111937},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111937},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pareto-optimality based black widow spider algorithm for energy efficient flexible job shop scheduling problem considering new job insertion},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Visualization and classification of mushroom species with
multi-feature fusion of metaheuristics-based convolutional neural
network model. <em>ASOC</em>, <em>164</em>, 111936. (<a
href="https://doi.org/10.1016/j.asoc.2024.111936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the correct mushroom species with the necessary ecological characteristics is critical to continue mushroom production, which is essential in gastronomy. The mushroom farmers and collectors technique may help identify toxic mushrooms by detecting poisonous mushrooms using images of different mushroom species with distinctive morphological features. However, it can be not easy to distinguish between species. This paper used a dataset of 6714 mushroom images obtained from nine different mushroom species to classify the mushroom species. For a more straightforward comprehension of mushroom images and feature extraction by reanalysis of data sets, data visualization was performed using Grad-CAM, LIME, and Heatmap methods. Residual block-based Convolutional Neural Network (CNN) architectures are trained to automatically classify the concatenated feature map obtained from the Grad-CAM, LIME, and Heatmap methods. After extracting the deep features of the images from each architecture, the Atom Search Optimization (ASO) algorithm has been used to select the most distinctive features. The 6714×9000 size of the concatenated feature map was reduced to 6714×600 using the ASO algorithm. Classification results were evaluated using six different classifiers based on the feature map obtained to determine the mushroom species. The nine classes of mushroom species were classified successfully with 95.45 % accuracy using the proposed model with the ASO algorithm and KNN classifier. The methodology introduces novel visualization techniques for interpreting CNN-based models’ decisions in mushroom species classification tasks. Using metaheuristics-based CNN models with multi-feature fusion techniques allows the model to leverage diverse sources of information, potentially enhancing its ability to discriminate between mushroom species and achieve higher classification accuracy than existing methods. This study can advance the mushroom species classification field by introducing new methodologies, improving classification accuracy, providing insights into model interpretability, and facilitating knowledge transfer to related fields.},
  archive      = {J_ASOC},
  author       = {Erdal Özbay and Feyza Altunbey Özbay and Farhad Soleimanian Gharehchopogh},
  doi          = {10.1016/j.asoc.2024.111936},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111936},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Visualization and classification of mushroom species with multi-feature fusion of metaheuristics-based convolutional neural network model},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristics-guided active learning for optimizing
reaction conditions of high-performance methane conversion.
<em>ASOC</em>, <em>164</em>, 111935. (<a
href="https://doi.org/10.1016/j.asoc.2024.111935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Converting greenhouse gases into value-added chemical compounds has been widely studied in chemical science and engineering for sustainable industry. In particular, nonoxidative coupling of methane (NOCM) that transforms methane into useful chemical compounds has received great interests because methane is a naturally abundant greenhouse gas. NOCM has received significant attention in sustainable industry for addressing the climate crisis. However, there are complex optimization problems in maximizing the efficiency of the nonoxidative methane conversion. Although conventional data-driven methods have been successfully applied to various engineering problems, a data-driven optimization of NOCM remains a challenging problem because expensive chemical experiments should be performed to collect prior data for model training. To avoid the expensive costs of chemical experiments, we propose an active learning method that performs training data augmentation and model re-training without pre-defined unlabeled experimental data. To this end, we combine active learning with metaheuristic algorithms to perform active learning with statistically augmented data. We applied the proposed method to a high-throughput screening task to discover new reaction conditions of high-performance NOCM, and the high-throughput screening error was significantly reduced by 69.11%.},
  archive      = {J_ASOC},
  author       = {Gyoung S. Na and Hyun Woo Kim},
  doi          = {10.1016/j.asoc.2024.111935},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111935},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristics-guided active learning for optimizing reaction conditions of high-performance methane conversion},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding and exploring promising search space for the 0–1
multidimensional knapsack problem. <em>ASOC</em>, <em>164</em>, 111934.
(<a href="https://doi.org/10.1016/j.asoc.2024.111934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 0–1, Multidimensional Knapsack Problem (MKP) is a classical NP-hard combinatorial optimization problem with many engineering applications. In this paper, we propose a novel algorithm combining evolutionary computation with the exact algorithm to solve the 0–1 MKP. It maintains a set of solutions and utilizes the information from the population to extract good partial assignments. To find high-quality solutions, an exact algorithm is applied to explore the promising search space specified by the good partial assignments. The new solutions are used to update the population. Thus, the good partial assignments evolve towards a better direction with the improvement of the population. Extensive experimentation with commonly used benchmark sets shows that our algorithm outperforms the state-of-the-art heuristic algorithms, TPTEA and DQPSO , as well as the commercial solver CPlex . It finds better solutions than the existing algorithms and provides new lower bounds for 10 large and hard instances.},
  archive      = {J_ASOC},
  author       = {Jitao Xu and Hongbo Li and Minghao Yin},
  doi          = {10.1016/j.asoc.2024.111934},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111934},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Finding and exploring promising search space for the 0–1 multidimensional knapsack problem},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AUV path planning in a three-dimensional marine environment
based on a novel multiple swarm co-evolutionary algorithm.
<em>ASOC</em>, <em>164</em>, 111933. (<a
href="https://doi.org/10.1016/j.asoc.2024.111933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous underwater vehicles (AUVs) are rapidly advancing in ocean exploration. High-performance path planning techniques are essential for AUVs. Path planning for AUVs is a multi-faceted challenge, necessitating careful consideration of safety, energy consumption and the influence of sea currents to ensure the development of a high-quality trajectory that satisfies all operational criteria. Existing path planning algorithms have problems such as incomplete consideration of influencing factors, computational complexity and weak applicability. Therefore, we propose a swarm intelligence optimisation algorithm based on multiple swarm co-evolution (MCO) to address the issue of AUV path planning in a three-dimensional marine environment. First, a three-dimensional marine environment model and the corresponding path evaluation mechanism are established. Second, the MCO rule is established to ensure that the MCO has balanced exploration and exploitation capabilities. A shared dynamic optimal particle between populations is introduced to ensure information exchange between populations. In addition, the cross-integration mutation strategy has been proposed for promoting the fusion of two populations of superior genes to ensure the inheritance of superior paternal genes to the offspring. Finally, four comparison experiments are designed, and the experiments compare eight commonly used intelligent search algorithms and improved versions. The results of the experiments proved that the MCO has excellent three-dimensional marine environment path planning capability, with robustness and search capabilities superior to other swarm intelligence optimisation algorithms.},
  archive      = {J_ASOC},
  author       = {Zhilei Liu and Dayong Ning and Jiaoyi Hou and Fengrui Zhang and Gangda Liang},
  doi          = {10.1016/j.asoc.2024.111933},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111933},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AUV path planning in a three-dimensional marine environment based on a novel multiple swarm co-evolutionary algorithm},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scale-adaptive mask r-CNN strategy for foreground particle
segmentation and geometrical analysis of granular aggregates.
<em>ASOC</em>, <em>164</em>, 111931. (<a
href="https://doi.org/10.1016/j.asoc.2024.111931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance segmentation algorithms play an increasingly crucial role in evaluating the geometrical characteristics of granular particles in geotechnical engineering. However, the accurate segmentation of the foreground particles of densely packed granular aggregates from multi-scale images remains challenging due to the time-consuming, laborious manual annotation process. This study proposes a scale-adaptive mask regional convolutional neural network (Mask R-CNN) strategy method for the automatic segmentation and geometrical analysis of granular aggregates in multi-scale images with less manual labeling. This new Mask R-CNN strategy involves an iterative process of automatic annotation of large-scale images and updating the preliminary model initially trained by small-scale images. The large-scale images are initially decomposed into a series of small-scale blocks. Subsequently, the preliminary model automatically identifies and annotates foreground particles within these small blocks. The segmentation results of the small blocks are then integrated into the original size, serving as annotations for the large-scale images. Finally, a multi-scale model can be obtained by retraining the preliminary model with annotated large-scale images and repeating the decomposition and integration process. Experiments based on densely packed ballast particles and cobble particles were conducted to validate the effectiveness of the proposed strategy. The results indicate that the trained multi-scale model exhibits satisfactory performance in segmenting foreground particles from densely packed granular aggregates, achieving a detection rate of 90.66 % and an error rate of 18.81 %. The proposed framework provides a feasible and effective tool for onsite inspection and the morphology analysis of densely packed granular aggregates.},
  archive      = {J_ASOC},
  author       = {Haoran Zhang and Zhen-Yu Yin and Ning Zhang and Xiang Wang and Zhi Ding},
  doi          = {10.1016/j.asoc.2024.111931},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111931},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A scale-adaptive mask R-CNN strategy for foreground particle segmentation and geometrical analysis of granular aggregates},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual optimization approach in discrete hopfield neural
network. <em>ASOC</em>, <em>164</em>, 111929. (<a
href="https://doi.org/10.1016/j.asoc.2024.111929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Having effective learning and retrieval phases of satisfiability logic in Discrete Hopfield Neural Network models ensures optimal synaptic weight management, which consequently leads to the production of optimal final neuron states. However, the problem with this model is that different initial states can affect the biasedness of the retrieval phase since the model memorizes final states without generating new ones and produces suboptimal final neuron states. To date, there is no recent research that solves this issue by improving both phases in the Discrete Hopfield Neural Network that involves first-order satisfiability logic. Therefore, this research contributes to the improvement of the learning and retrieval phases by integrating the Hybrid Differential Evolution Algorithm and Swarm Mutation respectively. This research utilizes Y-Type Random 2 Satisfiability, which combines first and second-order clauses to expand the storage capacity of DHNN models, facilitating the retrieval of optimal final neuron states. To evaluate the effectiveness of the Hybrid Differential Evolution Algorithm and Swarm Mutation in the learning and retrieval phases, several performance metrics are employed in terms of synaptic weight management, learning errors, testing errors, energy profiles, solution variations, and similarity for 10 different cases. Quantitative evaluations show that the proposed model successfully enhances the optimization of both phases, ranking first compared to 10 recent algorithms for all metrics. In terms of convergence analysis, the proposed model progressed fast towards the optimal solution with only one iteration for all cases. Additionally, the proposed model can generate a 100 % global minima ratio when dealing with a high number of neurons for Case 5.},
  archive      = {J_ASOC},
  author       = {Yueling Guo and Nur Ezlin Zamri and Mohd Shareduwan Mohd Kasihmuddin and Alyaa Alway and Mohd. Asyraf Mansor and Jia Li and Qianhong Zhang},
  doi          = {10.1016/j.asoc.2024.111929},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111929},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual optimization approach in discrete hopfield neural network},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-series weather prediction in the red sea using ensemble
transformers. <em>ASOC</em>, <em>164</em>, 111926. (<a
href="https://doi.org/10.1016/j.asoc.2024.111926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting Sea Surface Temperature (SST) and Wind Speed (WS) in the Red Sea is crucial for maritime safety, climate research, and ecosystem monitoring. However, modeling SST and WS is challenging due to the region’s complex and dynamic oceanic and atmospheric processes. This study introduces advanced predictive models based on Transformers to enhance the accuracy of SST and WS predictions by capturing time dependencies and modeling sequential data. A novel stacked architecture, named StackPred, improves the performance of individual Transformer models. Additionally, wavelet-based multiscale filtering is applied during preprocessing to separate relevant signals from noise, further enhancing prediction accuracy. The prediction performance is validated using publicly available SST and WS data from ten different locations in the Red Sea. Results consistently demonstrate the superiority of the proposed stacked deep learning models over existing techniques, including LSTM (Long Short-Term Memory), BiLSTM (Bidirectional LSTM), GRU (Gated Recurrent Unit), BIGRU (Bidirectional GRU), and single Transformers. Moreover, integrating wavelet denoising with StackPred significantly improves predictive performance compared to using data without denoising. The StackPred model achieves strong performance with an average R 2 R2 score of 99.83 for predicting SST and 99.96 for WS.},
  archive      = {J_ASOC},
  author       = {Mohamad Mazen Hittawe and Fouzi Harrou and Mohammed Amine Togou and Ying Sun and Omar Knio},
  doi          = {10.1016/j.asoc.2024.111926},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111926},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Time-series weather prediction in the red sea using ensemble transformers},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A memory-guided jaya algorithm to solve multi-objective
optimal power flow integrating renewable energy sources. <em>ASOC</em>,
<em>164</em>, 111924. (<a
href="https://doi.org/10.1016/j.asoc.2024.111924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional optimal power flow problem (OPF) usually centers on thermal generators, which have limited fuel for power generation, while emissions from the network system are commonly overlooked. However, the rising appreciation of renewable energy sources, valued for their sustainability, abundance, and environmental friendliness, has sparked increasing interest in the power systems domain. Consequently, there is a growing trend of integrating renewable energy sources into the electrical grid. This work explores the adaptation of the standard IEEE-30 bus system by incorporating renewable energy sources as a case study. This involves the replacement of traditional thermal generators located on buses 5 and 11 with wind generators, while bus 13 is substituted with solar generators. Addressing the uncertainty and intermittence inherent in renewable energy sources (RES), Weibull and lognormal probability density functions are employed for RES availability estimation. Integrating RES into the optimal power flow problem is framed as a multi-objective optimization task. A novel meta-heuristic optimization approach termed the Memory-Guided Jaya algorithm (MG-Jaya), is specifically tailored to address diverse challenges in multi-objective optimal power flow (MOOPF) incorporating RES. A smart memory-based strategy is incorporated into the algorithm to enhance solution optimality, convergence properties, and exploitation capabilities. Furthermore, a suitable mechanism aimed at efficiently finding Pareto-optimal solutions is proposed to address the multi-objective optimization optimal power flow (MOOPF) problem. Besides, to further evaluate the performance of the proposed approach in addressing complex, larger-scale issues, the study opts to utilize another test system of greater magnitude, namely the IEEE-57 bus system. To assess its effectiveness, the approach is compared against 14 recently introduced metaheuristics, which have garnered significant citations. To ensure fair comparisons, parameter configurations for all algorithms are automated using the parameter tuning tool iterated racing (irace). The experimental outcomes are analyzed using various nonparametric statistical techniques, including the Hyper-volume test, the Wilcoxon signed-rank test, and the critical difference plot. Furthermore, three authenticity criteria - Generational Distance (GD), Spacing Parameter (SP), and Diversity Metric (DM) - are employed to analyze the obtained Pareto-optimal solutions. Simulation results show that the proposed MG-Jaya algorithm demonstrates competitive capabilities. It effectively handles multi-objective and non-convex optimization problems. When compared to other approaches, it outperforms them in terms of solution optimality and feasibility.},
  archive      = {J_ASOC},
  author       = {Masoud Ahmadipour and Zaipatimah Ali and Vigna K. Ramachandaramurthy and Hussein Mohammed Ridha},
  doi          = {10.1016/j.asoc.2024.111924},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111924},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memory-guided jaya algorithm to solve multi-objective optimal power flow integrating renewable energy sources},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Instance reduction algorithm based on elitist min-max ant
colony optimization technique. <em>ASOC</em>, <em>164</em>, 111923. (<a
href="https://doi.org/10.1016/j.asoc.2024.111923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the advancement of technology has made data processing quite easier, it is still an enormous task, especially if the volume is large and can affect the learning capability of the classifier. Instance Reduction techniques can be employed on data to attain best results while making a substantial difference in terms of data storage needs, and processing speed. The proposed novel algorithm – Elitist Min-Max Ant Colony based Instance Reduction (ÆIR), can be attributed to the hybridization of bio-inspired optimization algorithms and Machine Learning algorithms for instance reduction. The primary features of ÆIR are the introduction of new hyperparameters such as the elitist ants, the min-max bounding of pheromones, and the incorporation of the length of the reduced set, all which aid in electing better instances for the reduced set. To evaluate the performance of the proposed ÆIR algorithm, 18 benchmark datasets of various dimensions were used alongside 4 different classifiers: Random Forest, Decision Tree, K-Nearest Neighbor and Naïve Bayes. The experiments demonstrated the effectiveness of ÆIR in reducing the size of the instances. On an average, approximately 50 % reduction in the number instances was observed. This can result in an equivalent decline in terms of the storage requirements, while contemporaneously improving the prediction accuracy.},
  archive      = {J_ASOC},
  author       = {S. Geethanjali and S. Sasikala},
  doi          = {10.1016/j.asoc.2024.111923},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111923},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Instance reduction algorithm based on elitist min-max ant colony optimization technique},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing anomaly detection in 3D MRI scans: The role of
ConvLSTM in medical image analysis. <em>ASOC</em>, <em>164</em>, 111919.
(<a href="https://doi.org/10.1016/j.asoc.2024.111919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of Medical Images (MI), particularly the detection and classification of anomalies in 3D MRI (Magnetic Resonance Imaging) scans, plays a critical part in timely intervention and personalized therapy plans. In our paper, a comprehensive methodology for anomaly detection in 3D MRI scans of the brain is proposed that combines advanced Deep Learning (DL) techniques, including Convolutional Long Short-Term Memory (ConvLSTM) model, with efficient statistical feature extraction from segmented anomaly regions. The data collection phase utilizes three main datasets namely the Brain Tumor Segmentation Challenge (BRATS), the Federated Tumor Segmentation Challenge (FETS), and the Medical Segmentation Decathlon (MSD). The research begins with preprocessing steps including image resizing, intensity normalization, and alignment of anomaly region segmentation masks. The targeted anomaly regions within the samples are segmented using U-Net architecture and then follow statistical feature extraction procedure. Dimensionality reduction methods such as Principal Component Analysis (PCA) and Recursive Feature Elimination (RFE) are utilized to streamline the feature space. The ConvLSTM is then used to classify the anomalies using both convolution and recurrent layers to capture spatiotemporal patterns in MRI data. The model is fine-tuned and iterated for better classification performance using Adam optimizer. The statistical evaluation results with accuracy of 98.9 % showed that the designed ConvLSTM method is more suitable for clinical diagnosis and treatment design in detecting anomalies in 3D MRI images.},
  archive      = {J_ASOC},
  author       = {Anuradha Durairaj and E.S. Madhan and M. Rajkumar and Syed Shameem},
  doi          = {10.1016/j.asoc.2024.111919},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111919},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing anomaly detection in 3D MRI scans: The role of ConvLSTM in medical image analysis},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-based visual speech recognition towards realistic avatars
and lip-reading applications in the metaverse. <em>ASOC</em>,
<em>164</em>, 111906. (<a
href="https://doi.org/10.1016/j.asoc.2024.111906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse, a virtually shared digital world where individuals interact, create, and explore, has witnessed rapid evolution and widespread adoption. Communication between avatars is crucial to their actions in the metaverse. Advances in natural language processing have allowed for significant progress in producing spoken conversations. Within this digital landscape, the integration of Visual Speech Recognition (VSR) powered by deep learning emerges as a transformative application. This research delves into the concept and implications of VSR in the metaverse. This study focuses on developing realistic avatars and a lip-reading application within the metaverse, utilizing Artificial Intelligence (AI) techniques for visual speech recognition. Visual Speech Recognition in the metaverse refers to using deep learning techniques to comprehend and respond to spoken language, relying on the visual cues provided by users&#39; avatars. This multidisciplinary approach combines computer vision and natural language processing, enabling avatars to understand spoken words by analyzing the movements of their lips and facial expressions. Key components encompass the collection of extensive video datasets, the employment of 3D Convolutional Neural Networks (3D CNNs) combined with ShuffleNet and Densely Connected Temporal Convolutional Neural Networks (DC-TCN) called (CFS-DCTCN) to model visual and temporal features, and the integration of contextual understanding mechanisms. The two datasets Wild (LRW) dataset and the GRID Corpus datasets are utilized to validate the proposed model. As the metaverse continues its prominence, integrating Visual Speech Recognition through deep learning represents a pivotal step towards forging immersive and dynamic virtual worlds where communication transcends physical boundaries. This paper contributes to the foundation of technology-driven metaverse development and fosters a future where digital interactions mirror the complexities of human communication. The proposed model achieves 99.5 % on LRW and 98.8 % on the GRID dataset.},
  archive      = {J_ASOC},
  author       = {Ying Li and Ahmad Sobri Hashim and Yun Lin and Puteri N.E. Nohuddin and K. Venkatachalam and Ali Ahmadian},
  doi          = {10.1016/j.asoc.2024.111906},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111906},
  shortjournal = {Appl. Soft. Comput.},
  title        = {AI-based visual speech recognition towards realistic avatars and lip-reading applications in the metaverse},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Eligibility traces in an autonomous soccer robot with
obstacle avoidance and navigation policy. <em>ASOC</em>, <em>164</em>,
111889. (<a href="https://doi.org/10.1016/j.asoc.2024.111889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the thematic literature of machine learning suggests, reinforcement learning falls between the two methods of supervised learning and unsupervised learning. In this method, the learning agent receives a reward or punishment from the environment according to its action. Therefore, the learning agent interacts with the environment through trial and error and learns to choose the optimal action to achieve its goal. In the meantime, the eligibility traces are considered as one of the main mechanisms of reinforcement learning in receiving delayed rewards. In the use of conventional reinforcement learning methods, when the learning agent achieves a goal, only the value function of the last state-action pair changes, but all the trace states are affected based on the eligibility traces. In other words, the delayed rewards are distributed throughout the trace. Like the ant pheromone effect, this method can increase learning speed empirically to some extent. A soccer robot on the field encounters moving obstacles such as balls, rival robots, and home robots, and fixed obstacles such as gates and flags, so its environment is in a very dynamic state. Therefore, due to the dynamic environment, it is an important issue for autonomous soccer robots to avoid obstacles and should be considered in real play. The main idea of this study is to determine the appropriate traces for the robot to move towards the ball and ultimately to score with the approach of avoiding obstacles in simulating a real soccer match. The desired results obtained from a game played indicate a high level of online experience and decision-making power in the face of new situations.},
  archive      = {J_ASOC},
  author       = {Seyed Omid Azarkasb and Seyed Hossein Khasteh},
  doi          = {10.1016/j.asoc.2024.111889},
  journal      = {Applied Soft Computing},
  month        = {10},
  pages        = {111889},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Eligibility traces in an autonomous soccer robot with obstacle avoidance and navigation policy},
  volume       = {164},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Efficient cross-modality feature interaction for
multispectral armored vehicle detection. <em>ASOC</em>, <em>163</em>,
111971. (<a href="https://doi.org/10.1016/j.asoc.2024.111971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting armed vehicles from a UAV platform is challenging due to the complexity of ground environment. This paper presents a dual-stream multispectral armored vehicle detection method to tackle this problem. First, considering that there is a paucity of datasets containing multispectral armored vehicle images, a multispectral armored vehicle detection dataset is constructed for this study. The dataset consists of 5853 pairs of RGB and infrared images, featuring a total of 15,878 instances of armored vehicles. Then, a cross-modal feature interaction module is designed to enable efficient feature interaction between multispectral images. This module uses the cross-modal channel-wise feature difference method to model the channel differences between the two modal features and obtains the cross-modal channel difference matrix. The cross-modal channel difference matrix is then employed to extract the unique features of the two modal features, allowing for efficient cross-modal feature interaction by complementing each other&#39;s unique features. Experiment results demonstrate that the proposed model has excellent detection performance and is capable of coping with various challenges brought by complex ground environments.},
  archive      = {J_ASOC},
  author       = {Jie Zhang and Tian-qing Chang and Li-yang Zhao and Jin-dun Ma and Bin Han and Lei Zhang},
  doi          = {10.1016/j.asoc.2024.111971},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111971},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient cross-modality feature interaction for multispectral armored vehicle detection},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ant colony optimization-based method for energy-efficient
cutting trajectory planning in axial robotic roadheader. <em>ASOC</em>,
<em>163</em>, 111965. (<a
href="https://doi.org/10.1016/j.asoc.2024.111965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional cutting trajectory of the axial robotic roadheader (spiral or reciprocating) is too simple to meet the requirements of adaptive planning. We proposed a method for planning the cutting trajectory of an axial robot roadheader based on ant colony optimization, which enables the machine to automatically adapt to the rock characteristics of the tunnel face, solves the problems of high energy consumption and low cutting efficiency in existing methods. This method uses machine vision to capture the fractures in the tunnel face, construct a grid map of the fractures, and calculate the fracture parameters within each grid, including the numbers, length, width, and density, through the connected domain method. Among them, the fracture parameters is used as input and the geological strength index as output. The BPNN is established to predict GSI of the grid environment. Based on this, an improved ant colony optimization for full coverage path planning is introduced, the cutting trajectory can be planned adaptively according to the geological conditions. The method is validated on a axial robotic roadheader, demonstrating its ability to adaptively plan cutting paths based on the geological conditions of the tunnel face. Compared to traditional methods, it reduces axial robotic roadheader energy consumption. This research enhances the intelligence level of coal mining robots, reduces energy consumption, and increases equipment lifespan.},
  archive      = {J_ASOC},
  author       = {Zheng Dong and Xuhui Zhang and Wenjuan Yang and Mengyu Lei and Chao Zhang and Jicheng Wan},
  doi          = {10.1016/j.asoc.2024.111965},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111965},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ant colony optimization-based method for energy-efficient cutting trajectory planning in axial robotic roadheader},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Roulette wheel-based level learning evolutionary algorithm
for feature selection of high-dimensional data. <em>ASOC</em>,
<em>163</em>, 111948. (<a
href="https://doi.org/10.1016/j.asoc.2024.111948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection in high-dimensional data is a large-scale sparse and discrete optimization problem. Most evolutionary algorithms are designed to tackle continuous optimization problems. However, when dealing with high-dimensional feature selection tasks, they often suffer from poor population diversity and are computationally expensive. To address these challenges, this work introduces a roulette wheel-based level learning evolutionary algorithm (RWLLEA). RWLLEA integrates two key components. Firstly, it employs a leveled population mode. Individuals from higher levels provide guidance to those at lower levels during the evolutionary process, thereby exploring the potential combinatorial effects among features. Secondly, recognizing the characteristics associated with the high-dimensional feature selection task, a roulette wheel-based update method is devised to dynamically reduce search space and harmonize the algorithm&#39;s exploitation and exploration capacities across different stages. The performance of the proposed method was evaluated by comparing it with six other feature selection techniques across a range of fifteen diverse datasets. The experimental findings demonstrate that the proposed method can achieve a reduced feature set with a shorter runtime and exhibit superior classification accuracy.},
  archive      = {J_ASOC},
  author       = {Huan Ma and Min Li and Siyu Lv and Lei Wang and Shaobo Deng},
  doi          = {10.1016/j.asoc.2024.111948},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111948},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Roulette wheel-based level learning evolutionary algorithm for feature selection of high-dimensional data},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarially attack feature similarity for fine-grained
visual classification. <em>ASOC</em>, <em>163</em>, 111945. (<a
href="https://doi.org/10.1016/j.asoc.2024.111945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained visual classification (FGVC) strives to distinguish images from distinct sub-classes within the same overarching meta-class, which is significant in various practical applications. Existing works mainly employ attention mechanisms to learn discriminative feature representations of objects under weakly supervised learning. In this paper, we argue that this likehood-based attention learning manner often outputs an inadequate feature representation since the available image-level labels fail to provide an explicit supervisory signal for attention learning, especially when the fine-grained images share a small and inconsistent inter-class variance. To alleviate this issue, we consider this challenging task from the perspective of attacking the feature representation between similar sub-classes to maximize the feature discriminativeness via learning adversarial examples, and propose an Adversarial-Aware Fine-Grained Visual Classification Network (A 2 Net). Specifically, we first propose an adversarial attack module based on projected gradient descent, which appends multiple-scale adversarial perturbations to simulate subclass examples with different similarities. Then, we introduce an adversarial attention generation module that estimates the effect of attention learned on adversarial examples and legitimate examples for the final class prediction through causal inference. The adversarial attention generation module is encouraged to maximize the effect, which provides powerful supervision to capture more attention indicating the discriminative parts. We further propose an adversarial-aware module to learn the feature-level differences between legitimate and adversarial examples, which helps enhance the semantic boundaries of class-specific features for accurate FGVC. The extensive array of experiments conducted serves to underscore the efficacy of the proposed A 2 Net, outperforming state-of-the-art FGVC methods on CUB-200–2011, FGVC-Aircraft, Stanford Cars, Stanford Dogs, and NABirds benchmarks.},
  archive      = {J_ASOC},
  author       = {Yupeng Wang and Can Xu and Yongli Wang and Xiaoli Wang and Weiping Ding},
  doi          = {10.1016/j.asoc.2024.111945},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111945},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarially attack feature similarity for fine-grained visual classification},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated scheduling of multi-constraint open shop and
vehicle routing: Mathematical model and learning-driven brain storm
optimization algorithm. <em>ASOC</em>, <em>163</em>, 111943. (<a
href="https://doi.org/10.1016/j.asoc.2024.111943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a surge of interest in integrated production and distribution scheduling problems which can achieve an overall optimization of the production and distribution activities. However, integrated scheduling of open shop and distribution receives rare attention in existing studies. This work proposes an integrated scheduling problem of multi-constraint open shop and vehicle routing to minimize maximum completion time, where group and transportation operations are considered together in the production process. All jobs are divided into multiple groups, and then handled in an open shop with multiple machines. Subsequently, the jobs are delivered to their corresponding customers. First, a mixed integer programming model is formulated to define the problem. Second, a Q-learning-driven brain storm optimization algorithm is developed to address the formulated model. A Q-learning method is employed to choose search strategies for generating new individuals rather than using fixed probability parameters straightforwardly as basic brain storm optimizers. In addition, the solution encoding, heuristic decoding, population initialization, clustering, new individual generation and selection methods are specially devised in consideration of problem-specific knowledge. At last, the developed model and algorithm are verified by addressing a set of benchmark instances, and comparison experiments are conducted with an exact solver CPLEX and four meta-heuristics from existing literature. The results validate the competitive advantages of the formulated model and algorithm in solving the considered problems.},
  archive      = {J_ASOC},
  author       = {Yaping Fu and Yifeng Wang and Kaizhou Gao and Ponnuthurai Nagaratnam Suganthan and Min Huang},
  doi          = {10.1016/j.asoc.2024.111943},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111943},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrated scheduling of multi-constraint open shop and vehicle routing: Mathematical model and learning-driven brain storm optimization algorithm},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024g). Graph recurrent neural networks-integrated real-time
prediction of key displacements for fire-induced collapse early warning
of steel frames. <em>ASOC</em>, <em>163</em>, 111942. (<a
href="https://doi.org/10.1016/j.asoc.2024.111942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes the FRAME-Net, which can extract spatial-temporal features of discrete structures like planar steel frames, for early warning systems of fire-induced building collapse. FRAME-Net, based on graph neural networks and recurrent neural networks, seamlessly integrates feature extraction techniques, advanced graph convolution operations, and an enhanced graph long short-term memory mechanism, resolving the critical issue that the actual state of burning buildings, e.g., fire scenario, load case, material properties, etc ., cannot be identified accurately and rapidly. The temperatures of steel components and easy-to-measure displacements are inputs, and hard-to-measure displacements at the top and interior of steel frames under fire can be predicted in real time. Numerical examples indicate that the trained agent accurately predicts the hard-to-measure displacements for the trained steel frame under completely unknown datasets, showcasing its potential in real-world unknown fire scenarios. Besides, the trained intelligent agent can be directly applied to untrained steel frames with different topological forms without re-training, which significantly reduces computational costs and is beneficial for swift emergency responses. Subsequently, satisfactory results predicted by the trained agent without re-training for the steel frame under the real fire test further underscores the promising generalization capability of the trained agent to adapt to unknown fire scenarios in real-world steel frames. Finally, the feasibility of the proposed framework to facilitate the implementation of the early-warning systems of fire-induced collapse is proved by comparing the predicted and actual remaining building collapse time, thereby assisting in reducing casualties during the rescue process and contributing to smart firefighting advancements.},
  archive      = {J_ASOC},
  author       = {Yao Wang and Guo-Qiang Li and Shaojun Zhu},
  doi          = {10.1016/j.asoc.2024.111942},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111942},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph recurrent neural networks-integrated real-time prediction of key displacements for fire-induced collapse early warning of steel frames},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-level guided evolution algorithm for solving fuzzy
flexible job shop problem. <em>ASOC</em>, <em>163</em>, 111932. (<a
href="https://doi.org/10.1016/j.asoc.2024.111932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional flexible job shop scheduling problems (FJSP) are mostly limited to deterministic environments. In actual production, some uncertain factors lead to changes in job processing time. When solving multi-objective fuzzy flexible job shop scheduling problems (MOFFJSP), the existing evolutionary algorithms do not fully utilize information transfer between levels during the population evolution process. Therefore, a multi-level guided evolutionary (MLGE) optimization method is proposed to solve MOFFJSP. In the proposed MLGE method, decomposition and dominance techniques are combined to balance the diversity and convergence performance of the algorithm. Meanwhile, the idea of the Jaya algorithm approaching good individuals and distancing poor individuals is introduced. Combined with improved Jaya operator operations, it is used to guide individual evolution, preserving and inheriting solutions that perform well in terms of convergence and diversity. Finally, numerous experiments are carried out to evaluate the effectiveness of MLGE. The results show that MLGE can provide promising results for MOFFJSP. Additionally, it shows how MLGE outperforms other comparison algorithms in terms of convergence and variety of solutions.},
  archive      = {J_ASOC},
  author       = {Zeyin Guo and Lixin Wei and Jinlu Zhang and Ziyu Hu and Hao Sun and Haijun Che},
  doi          = {10.1016/j.asoc.2024.111932},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111932},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-level guided evolution algorithm for solving fuzzy flexible job shop problem},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semantic differences and psychological behavior in
multi-criteria group decision-making: Do they need consideration?
<em>ASOC</em>, <em>163</em>, 111930. (<a
href="https://doi.org/10.1016/j.asoc.2024.111930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Multi-Criteria Group Decision-Making (MCGDM), linguistic evaluation is widely used due to its flexibility. In particular, Personalized Individual Semantics (PIS), namely decision-makers hold heterogeneous understandings of the same word, has been extensively studied to elicit numeric meaning to the linguistic scale. However, few studies consider the need to tailor linguistics to the different indicators used, as well as the semantic difference in the same level linguistic that experts use when situations change. The traditional PIS models overlook the psychological aspect of the decision makers. To solve these problems, this paper proposes a method that considers semantic differences arising from the decision-makers and the indicators and incorporates the experts&#39; psychological behavior. We apply this method to assess the alternatives for a smart refrigerator product service system. The results suggest that a large semantic difference, when evaluating the linguistic indicators, can influence the final outcome. We validate the flexibility and the effectiveness of the method by comparing against the base case of no semantic difference, using the Euclidean and Manhattan distance measures.},
  archive      = {J_ASOC},
  author       = {Qun Wang and Guozhu Jia and Mark Goh and Zeyu Jiao and Wenyan Song},
  doi          = {10.1016/j.asoc.2024.111930},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111930},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semantic differences and psychological behavior in multi-criteria group decision-making: Do they need consideration?},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised dual-layer 2D normalizing flow method for
industrial anomaly detection. <em>ASOC</em>, <em>163</em>, 111928. (<a
href="https://doi.org/10.1016/j.asoc.2024.111928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the shortcomings in feature representation and abstraction ability of NFs in the field of unsupervised industrial anomaly detection, as well as the ambiguity in determining the decision boundary between normal and anomaly features, this study proposes the SDNF. This method is dedicated to accurately identifying anomalies by mastering the differences in the distribution of normal and anomaly features. The adopted AFS enables the model to more clearly define the decision boundaries, while the application of the DNF enhances the feature representation and abstraction capabilities. By utilizing the mapping principle of NF, we successfully distinguished between normal and anomaly features, achieving efficient anomaly localization. Furthermore, the ESAM is introduced into the DNF’s sub-networks, allowing the model to focus more on anomaly features, thereby significantly improving the performance of anomaly detection. Experimental validation shows that this method significantly improves the image-level and pixel-level AUC metrics on the MVTec AD dataset, reaching 99.33 % and 98.48 % respectively, surpassing previous research results in terms of the accuracy and quality of anomaly detection. While ensuring high-precision anomaly detection, this research also balances the complexity and inference efficiency of the model, contributing to the advancement of industrial anomaly detection. The related code has been made open source, for more details please visit https://github.com/FutureIAI .},
  archive      = {J_ASOC},
  author       = {Zhenlian Miao and Guangzhu Chen and Xiaojuan Liao and Jiu Dai and Yumeng He},
  doi          = {10.1016/j.asoc.2024.111928},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111928},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised dual-layer 2D normalizing flow method for industrial anomaly detection},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nature-inspired optimization prey–predator algorithm for
soil slope stability analysis with physically informed initial
population generation. <em>ASOC</em>, <em>163</em>, 111927. (<a
href="https://doi.org/10.1016/j.asoc.2024.111927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In soil slope stability, locating the critical slip surface with the minimum safety factor is a difficult and complicated optimization problem. Most methods fail when proper bounds are not applied to the decision variable. For the first time, the paper uses the prey–predator algorithm to analyze slopes stability with circular slip surfaces making use of meaningful rules, from an engineering point of view, to define these bounds. The Fellenius method based on limit equilibrium technique also serves as the constraint of the mathematical problem adopted to solve the task. A pseudo-analytical study is also carried out on four benchmark literature problems to test the performance and to certify the accuracy of the results obtained with the new application of the prey–predator algorithm. We show that the solutions obtained are accurate and should be assumed as a reference comparing to other methods.},
  archive      = {J_ASOC},
  author       = {Leonardo Leonetti and Maria Elena Bruni and Ernesto Ausilio},
  doi          = {10.1016/j.asoc.2024.111927},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111927},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Nature-inspired optimization prey–predator algorithm for soil slope stability analysis with physically informed initial population generation},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep label embedding learning for classification.
<em>ASOC</em>, <em>163</em>, 111925. (<a
href="https://doi.org/10.1016/j.asoc.2024.111925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The one-hot 0/1 encoding method is the most popularized encoding method of class labels for classification tasks. Despite its simplicity and popularity, it comes with limitations and weaknesses, like failing to capture the inherent uncertainty in data labels, and making classifiers more prone to overfitting. In this paper, these shortcomings are tackled with a framework for learning soft label embeddings. Two variants are proposed: first, a learnable general-class embedding which aims to capture information regarding inter-class similarities, and second, a neural architecture which can be added to any neural classifier and aims to learn inter-instance similarities. The inherent uncertainty in data labels is thus somewhat alleviated, allowing the network to focus on incorrectly classified samples, instead of difficult but correctly classified ones. The experimental study on multiple classification benchmarks of increasing difficulty, using neural networks of varying depth and width, show that the proposed method leads to better classification accuracy, highlighting its ability to generalize to unseen samples.},
  archive      = {J_ASOC},
  author       = {Paraskevi Nousi and Anastasios Tefas},
  doi          = {10.1016/j.asoc.2024.111925},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111925},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep label embedding learning for classification},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Intelligent diagnosis method for machine faults based on
federated transfer learning. <em>ASOC</em>, <em>163</em>, 111922. (<a
href="https://doi.org/10.1016/j.asoc.2024.111922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent fault diagnosis model based on federated learning can effectively solve the problem of fault data privacy and sharing, and ignores the difference of fault data distribution. Transfer learning can avoid the difference of data distribution. Combining the advantages of transfer learning and federated learning, a fault diagnosis model with data privacy based on federated transfer learning is proposed to achieve cross-domain fault diagnosis without sharing the data. In the constructed model, the local models on the client are firstly established based on deep convolution neural network to extract the feature of the source and target domains. The alignment loss is introduced to minimize the similar feature distribution differences among different source domains and target domain. The parameters of local models are fused and updated to generate a global model, which can not only identify the fault types in the target domain, but also retain the ability to recognize the fault types in the source domain. The two experiments, including different bearings with same feature distribution and label and the bearing and planetary gear in the same transmission system with similar feature distribution, are used to verify the effectiveness of the proposed model. The experiments suggest that the fault diagnosis model based on federated transfer learning can reduce the difference of the newly added fault type data distribution, and can accurately recognize the fault data of the source domain and target domain. Compared with the traditional diagnosis model based on deep learning, transfer learning and federated learning, the proposed model can effective perform the cross-domain fault diagnosis with data privacy.},
  archive      = {J_ASOC},
  author       = {Zhinong Li and Zedong Li and Fengshou Gu},
  doi          = {10.1016/j.asoc.2024.111922},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111922},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent diagnosis method for machine faults based on federated transfer learning},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time evaluation of object detection models across open
world scenarios. <em>ASOC</em>, <em>163</em>, 111921. (<a
href="https://doi.org/10.1016/j.asoc.2024.111921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection models have been experiencing significant improvements over the years due to advancements in deep learning techniques, increased availability of large-scale annotated datasets, and computational resources. Different object detection models have varying levels of accuracy, speed, and robustness. With the increasing complexity and diversity of object detection models, it becomes a problem for researchers and practitioners to choose the most suitable model for their specific needs. This research paper outlines the escalating demand for robust comparison of object detection models in response to rapidly advancing technology. This evaluation helps in identifying the strengths and weaknesses of these models and selecting the most suitable one for a specific task. This highlights a significant challenge stemming from the lack of recent comparative studies on object detection models across various image qualities, object sizes, and training data sizes. The above challenges are tackled by a meticulous evaluation of three state-of-the-art object detection models: YOLO-v8, Faster R-CNN with ResNet 50 and 101 backbones, and End-to-End Object Detection Transformers (DETR) utilizing ResNet 50 and 101 backbones by employing a rigorous assessment framework encompassing mean Average Precision (mAP), accuracy, and inference speed. This study focuses on thoroughly examining how well the models perform across three different datasets: TACO, PlastOPol, and TACO 4.5. These datasets consist of open-world images captured in real-time from various locations. They include 1500, 2500, and 6500 images respectively, depicting real-world environments with varying lighting conditions and complex backgrounds. The results identify YOLOv8 as the superior model for high and medium-quality images, while Faster R-CNN performs better for low-quality images. However, DETR&#39;s accuracy falls short compared to other models. The paper fills a crucial gap in understanding model performance across varying image qualities and object sizes and helps in taking informed decisions in object detection systems.},
  archive      = {J_ASOC},
  author       = {Puneet Goswami and Lakshita Aggarwal and Arun Kumar and Rahul Kanwar and Urvi Vasisht},
  doi          = {10.1016/j.asoc.2024.111921},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111921},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Real-time evaluation of object detection models across open world scenarios},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A study of uncertain 4D transportation problems with rough
interval parameters and additional real-life factors. <em>ASOC</em>,
<em>163</em>, 111920. (<a
href="https://doi.org/10.1016/j.asoc.2024.111920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In developing countries like India and Nepal, a vehicle document verification system is implicated at the checkpoints and border crossings that allows users to authenticate and verify the validity of vehicle related documents such as registration certificates, driving licenses, pollution under control certificates, and insurance policies. This results in the formation of long queues of the vehicles and hence a significant time wastage. These vehicles waiting in the long queues, continue to emit carbon dioxide and other greenhouse gases into the atmosphere. To address these issues, the government has introduced the Automated Number Plate Recognition (ANPR) technology which plays a significant role in the efficient and accurate document verification by identifying and validating vehicle registration details. It also automates toll collection by capturing the vehicle’s number plate, reducing waiting times and congestion at toll barriers. To analyze all these benefits, in this study, for the first time, ANPR technology has been introduced in the multi-objective model of transportation problem and its impact on various objectives has been observed. Also, real-life factors: fixed-charge cost, carbon emission, multiple routes have been considered in the proposed model and the problem is named as multi-objective fixed-charge four dimensional green transportation problem. Rough set theory has been employed as a tool for effectively addressing the indeterminacy and facilitating the modeling of real-life transportation problems. Two methods: the expected value method and the rough chance constrained programming method, are proposed to transform the suggested model of multi-objective transportation problem into a deterministic form. To solve these transformed models, a new multi-objective optimization approach based on the neutrosophic theory, named as neutrosophic goal programming has been introduced. To further demonstrate the practicality of the proposed study, an application for the apple fruit transportation has been solved. Detailed comparisons between obtained results are done on the basis of different modeling perspectives, different solution techniques and also with/and without ANPR technology. Based on the outcomes, the suggested neutrosophic goal programming technique proved to be superior to the existing techniques in the literature.},
  archive      = {J_ASOC},
  author       = {Shivani and Deepika Rani and Rizk M. Rizk-Allah},
  doi          = {10.1016/j.asoc.2024.111920},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111920},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A study of uncertain 4D transportation problems with rough interval parameters and additional real-life factors},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Aggregate-aware model with bidirectional edge generation for
medical image segmentation. <em>ASOC</em>, <em>163</em>, 111918. (<a
href="https://doi.org/10.1016/j.asoc.2024.111918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of lesion areas plays an important role in medical imaging-assisted diagnosis and treatment. Accurate boundary information can help doctors develop precise surgical plans and improve patient prognosis. However, automatic segmentation methods often struggle to accurately segment edges due to the random shape, size, and location of regions of interest (ROI). This problem is compounded in medical images, where the difference in pixel intensity between foreground and background is significantly smaller than in natural images. In this study, we propose an a ggregate-aware m odel with b idirectional e dge g eneration (Ambeg) for medical image segmentation. To overcome the problem of blurred edges between foreground and background in medical images, we design a deep learning model via a multi-task learning strategy and obtain richer visual features to guide segmentation. Furthermore, an Edge Feature Fusion (EFF) module is developed to combine spatial correlation information of lesion edges between adjacent images for more accurate edge segmentation. Finally, we design a new evaluation metric, the Boundary DSC segmentation consistency measure, to evaluate the edge segmentation accuracy of medical image segmentation methods. We utilize dilation and erosion operations in morphological methods to construct lesion edge labels. In addition, we use expansion and erosion rates to regulate the dimensions of the edge region to assess the requirements of different diseases for edge segmentation accuracy. The proposed approach is particularly noteworthy for achieving state-of-the-art results on medical image segmentation datasets, including BraTS 2022 (MRI), BraTS 2020 (MRI) and COVID-19–20 (CT), which have different modalities of datasets. It has an impressive Hausdorff distance of 4.62 mm and a sensitivity score of 92.45 % on BraTS 2020. Compared with existing assessment methods such as Dice score, Boundary DSC segmentation consistency measure focuses on the hard-to-segment lesion edge region rather than the easy-to-segment lesion center region, which provides a more comprehensive reference for physicians to choose automatic segmentation methods. In addition, since the boundary shapes of medical images are complex and diverse, we utilize morphological methods to obtain the boundary labels to ensure the smoothness of the boundary. Moreover, the approach is easy to implement and has a low computational cost, making it an attractive option for practical medical imaging applications.},
  archive      = {J_ASOC},
  author       = {Shiqiang Ma and Xuejian Li and Jijun Tang and Fei Guo},
  doi          = {10.1016/j.asoc.2024.111918},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111918},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Aggregate-aware model with bidirectional edge generation for medical image segmentation},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of municipal solid waste as a source of energy
production using fuzzy decision system. <em>ASOC</em>, <em>163</em>,
111917. (<a href="https://doi.org/10.1016/j.asoc.2024.111917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the major causes of global environmental deterioration is the enormous amount of municipal solid waste generated as a result of increasing industrialization and expanding populations. Managing the limited land resources of metropolitan areas and the growing volumes of waste generated as a result of development has become increasingly challenging for decision makers. Anaerobic digestion, gasification, pyrolysis, and incineration are among methods that help to derive energy from the various types of municipal wastes. Using these processes, waste is converted into energy, which is then used to produce power. It reduces the burden on landfills and helps in the fight against pollution in the environment. This article proposes the optimal municipal waste that can be used to derive energy using fuzzy multiple criteria decision making. This area of operational research focuses on achieving the best outcomes in intricate situations with a variety of indicators, competing goals, and requirements. This tool is gaining popularity in the sector of energy extraction because it allows decision makers to make judgments while considering all criteria and objectives at the same time. Finally, sensitive and comparative study is provided to determine the sturdiness of the proposed system.},
  archive      = {J_ASOC},
  author       = {Chakkarapani Sumathi Thilagasree and Thippan Jayakumar and Krishnan Suvitha and Michael Sandra and Dragan Pamucar and Vladimir Simic and Jeonghwan Jeon},
  doi          = {10.1016/j.asoc.2024.111917},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111917},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis of municipal solid waste as a source of energy production using fuzzy decision system},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Temporal graph convolutional network for multi-agent
reinforcement learning of action detection. <em>ASOC</em>, <em>163</em>,
111916. (<a href="https://doi.org/10.1016/j.asoc.2024.111916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most action detection techniques process untrimmed action videos using temporal context, without explicitly consider the inherent spatio-temporal context information, leading to limited accuracy in the cases with big spatial complexity and temporal redundancy. To this end, by intuitively dealing with the problem following a two-stage ”clip selection + clip classification” scheme, this paper proposes to formulate action detection as a Markov process and builds up a multi-agent reinforcement learning framework capturing global structural relationships of videos to optimize the selection and classification, simultaneously and progressively. In particular, a temporal graph convolutional network is constructed to represent the spatio-temporal correlations of video clips, which are initialized by evenly sampling, and further adjusted via learning the rewards adaptively for multi-agent cooperation. Multi-head dot-product attention mechanism is adopted to integrate the relations of latent CNN features of interacting agents. Our framework is jointly learnt by fusing the objectives of clip selection policy and clip recognition. The proposed method comprises a novel graph convolutional network based spatio-temporal semantic observation module which captures topological features among nearby agents, and a new policy module that segments actions according to the rewards from the objectives of action recognition. Extensive experiments are conducted on ActivityNet v1.3 and THUMOS14, with 30.13% and 55.4% mAP obtained, demonstrate the applicability and superiority of our approach.},
  archive      = {J_ASOC},
  author       = {Liangliang Wang and Jiayao Liu and Ke Wang and Lianzheng Ge and Peidong Liang},
  doi          = {10.1016/j.asoc.2024.111916},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111916},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal graph convolutional network for multi-agent reinforcement learning of action detection},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). Matrix-based approximation dynamic update approach to
multi-granulation neighborhood rough sets for intuitionistic fuzzy
ordered datasets. <em>ASOC</em>, <em>163</em>, 111915. (<a
href="https://doi.org/10.1016/j.asoc.2024.111915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth and rapid changes in the use of data, information systems are constantly evolving. Timely dynamic updates have become imperative with real-time monitoring of data increasingly common. Effectively characterizing the approximation space in dynamic environments is of significant concern. This paper investigates a dynamic update mechanism for generalized multi-granulation neighborhood dominant rough sets, based on a matrix form, in an intuitionistic fuzzy ordered information table. We first define support and inclusion functions to construct the model of generalized multi-granulation neighborhood dominant rough sets. Additionally, we analyze the dynamic update process in which objects are added or removed in matrix form. Corresponding dynamic update algorithms are proposed based on generalized multi-granulation neighborhood dominant rough sets. Finally, to validate the effectiveness of the matrix-based dynamic approximation update algorithm, eight UCI datasets are used to perform experiments. The results verify that our matrix-based dynamic update algorithm is effective in approximating updates for dynamic intuitionistic fuzzy ordered information datasets.},
  archive      = {J_ASOC},
  author       = {Xiaoyan Zhang and Jinghong Wang and Jianglong Hou},
  doi          = {10.1016/j.asoc.2024.111915},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111915},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Matrix-based approximation dynamic update approach to multi-granulation neighborhood rough sets for intuitionistic fuzzy ordered datasets},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image manipulation detection and localization using
multi-scale contrastive learning. <em>ASOC</em>, <em>163</em>, 111914.
(<a href="https://doi.org/10.1016/j.asoc.2024.111914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current image tampering detection methods rely on various forgery footprints, such as JPEG artifacts and edge inconsistencies, and use algorithms related to image segmentation. However, these methods have several issues, including over-fitting, focusing on only a few specific forgery footprints, and emphasizing semantically relevant information while ignoring tampering traces. This paper proposes a model for image manipulation detection and localization based on multi-scale contrast learning(MSCL-Net). The model utilizes the differences in feature distributions between tampered and untampered regions to extract a comprehensive tamper trace. It uses a dual-stream structured encoder that incorporates both RGB raw images and SRM noise features. A Feature Cross-Fusion Module (FCFM) is proposed to fuse features for improving feature representation of tampered information. The decoding process involves the use of an Adaptive Self-Attention Module (ASAM) to filter and aggregate relevant context from coarse feature maps. Additionally, a Supervised Contrastive Learning Module (SCLM) is used to expand the difference between tampered and untampered areas. The loss function for multi-loss fusion comprises classification loss, segmentation loss, and multi-scale supervised contrastive loss. This improves the network&#39;s understanding of global differences, reduces false positives, weakens semantic information, and enhances the model&#39;s ability to locate tampered regions of varying sizes. Extensive experiments are conducted across multiple datasets, demonstrating that our model is robust against attacks and resilient to false-positive predictions at both the image-level and pixel-level. Furthermore, its overall performance exceeds that of state-of-the-art alternatives in reliably detecting and localizing tampered images.},
  archive      = {J_ASOC},
  author       = {Ruyi Bai},
  doi          = {10.1016/j.asoc.2024.111914},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111914},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image manipulation detection and localization using multi-scale contrastive learning},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A double exponential particle swarm optimization with
non-uniform variates as stochastic tuning and guaranteed convergence to
a global optimum with sample applications to finding optimal exact
designs in biostatistics. <em>ASOC</em>, <em>163</em>, 111913. (<a
href="https://doi.org/10.1016/j.asoc.2024.111913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature-inspired metaheuristic algorithms, like Particle Swarm Optimization (PSO), are powerful general-purpose optimization tools but they invariably do not come with rigorous theoretical justifications and can fail to find a global optimum. By treating PSO as a random search optimization process and repairing the famous Global Search Convergence Theorem by imposing an additional condition in the proof, we create a novel theory-based algorithm called the Double Exponential Particle Swarm optimization algorithm (DExPSO) that converges to a global optimum. In particular, we show the common practice of using uniform variates as stochastic components in PSO and related algorithms does not satisfy the sufficient conditions in DExPSO and hence may provide a reason why PSO and other nature-inspired algorithms, like QPSO, LcRiPSO, and CSO can fail to converge. Additionally, in more complicated design problems, we show that DExpSO tends to converge to the support points of the optimal design more frequently and faster than PSO and its variants do. Moreover, there is a possibility to modify other PSO variants to DExPSO variants, and such hybridization offers promising improvement in the quality of the global search. Our applications include finding designs that minimize the integrated mean squared prediction error and locally D D -optimal exact designs for a 68-compartmental model to assess radioactive particles retained in the human lung after exposure. Because PSO, and more generally, metaheuristics are used across disciplines, including ecology, pharmacokinetics and pharmacodynamics studies, agriculture, engineering, and computer science, there are potentially broad and deep implications of our results.},
  archive      = {J_ASOC},
  author       = {Milan Stehlík and Ping-Yang Chen and Weng Kee Wong and Jozef Kiseľák},
  doi          = {10.1016/j.asoc.2024.111913},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111913},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A double exponential particle swarm optimization with non-uniform variates as stochastic tuning and guaranteed convergence to a global optimum with sample applications to finding optimal exact designs in biostatistics},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical and density-based clustering of geographical
flows for crowd movement patterns recognition. <em>ASOC</em>,
<em>163</em>, 111912. (<a
href="https://doi.org/10.1016/j.asoc.2024.111912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sensors and communication technologies, it has become easy to collect large-scale and long-term crowd movement positioning data, which brings new opportunities for studying crowd movement patterns. This paper proposes a novel Statistical and Density-Based Clustering algorithm (SDBC) to identify implicit significant spatial aggregation patterns in geographical flow data. Unlike existing flow clustering algorithms, this method identifies the hot spots of origin-destination (OD) flows based on local spatial statistics and density-growing clustering. It also evaluates the significance of the identified geographic flow clusters, effectively reducing the identification of spurious clusters generated by chance in data. In our method, the spatial neighborhood of each flow is first obtained based on spatial proximity, temporal similarity, and directional similarity. Then, the number of flows in the spatial neighborhood of each flow is calculated and used as the density measure. Based on this, high-density flows are automatically detected using local spatial aggregation statistics, and a hierarchical density-based clustering strategy is developed to merge adjacent high-density flows to generate candidate flow clusters. Finally, we perform permutation tests to infer the statistical significance of each flow cluster and eliminate the candidate clusters generated by chance. Experiments on synthetic data and real-world taxi trajectory data were conducted to evaluate the effectiveness of the proposed method. Results show that the proposed method can accurately identify the statistically significant flow clusters of different shapes and densities and performs better than the available state-of-the-art flow clustering algorithms.},
  archive      = {J_ASOC},
  author       = {Jianbo Tang and Yuxin Zhao and Xuexi Yang and Min Deng and Huimin Liu and Chen Ding and Ju Peng and Xiaoming Mei},
  doi          = {10.1016/j.asoc.2024.111912},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111912},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Statistical and density-based clustering of geographical flows for crowd movement patterns recognition},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature fusion technology based on serum FTIR spectra
combined with chaos theory in the disease auxiliary diagnosis.
<em>ASOC</em>, <em>163</em>, 111911. (<a
href="https://doi.org/10.1016/j.asoc.2024.111911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaos theory is a mathematical theory that studies nonlinear dynamical systems and has found extensive applications in the disease auxiliary diagnosis. FTIR spectra is a technique based on infrared spectroscopy that provides information about molecular vibrations, rotations, and vibrational-rotational energy levels by recording the absorption spectrum of a sample in the infrared radiation range. This technology has gained attention for its extensive applications in the disease auxiliary diagnosis. However, due to the limited amount of molecular information captured by FTIR spectra and intricate clinical diagnostic scenarios, this study introduces an innovative approach by combining FTIR spectra with chaos theory. This novel method for disease prediction is proposed and validated using FTIR spectra datasets from various diseases, including glioma, non-small cell lung cancer (NSCLC), and systemic lupus erythematosus (SLE). The experimental results demonstrate that the proposed Low-rank Tensor Features Fusion-BiGRU (LTFF-BiGRU) model achieves competitive outcomes in three datasets. Comparing the spectral features, inputting spectral-chaotic fusion features into LTFF-BiGRU models can effectively improve the average Accuracy (Acc) by 3.5 %, average Precision (Pre) by 3.30 %, average Sensitivity (Sen) by 2.37 %, average Specificity (Spe) by 4.07 %, average F1 score by 3.10 %, and average Area Under the ROC Curve (AUC) by 3.23 %. Through low-rank tensor fusion, the correlations and interaction patterns between different feature data can be effectively captured, thus extracting a more comprehensive and enriched feature representation to enhance disease diagnosis results further. This research marks the first demonstration of chaotic characteristics in FTIR spectra and pioneers the exploration of employing low-rank tensor fusion between spectral features and chaotic features. The research signifies a crucial step in integrating FTIR spectra with chaos theory in the disease auxiliary diagnosis, paving the way for further exploration in this promising interdisciplinary field.},
  archive      = {J_ASOC},
  author       = {Yang Du and Cheng Chen and Chen Chen and Yue Liu and Lijun Wu and Enguang Zuo and Xiaoyi Lv},
  doi          = {10.1016/j.asoc.2024.111911},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111911},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature fusion technology based on serum FTIR spectra combined with chaos theory in the disease auxiliary diagnosis},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Phase-wise attention GCN for recommendation denoising.
<em>ASOC</em>, <em>163</em>, 111910. (<a
href="https://doi.org/10.1016/j.asoc.2024.111910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Graph Convolution Network (GCN) has been widely applied in recommendation systems. Compared to traditional collaborative filtering methods that only aggregate information of neighboring nodes, GCN-based recommendation systems can aggregate information of high-order neighborhoods of a node to achieve better performance. However, GCN-based recommendation systems have faced the over-smoothing problem. The over-smoothing problem causes the performance of recommender systems first to rise and then decrease as the number of stacked layers increases. Previous GCN-based recommender systems have provided a few solutions to over-smoothing. We believe that there are still two problems: (1) most models focus on aggregating diverse information, with less consideration of filtering noise; (2) most models ignore the fact that there are differences in different stages of the process of aggregating information using GCN. We introduce a Phase-wise Attention mechanism to address these issues and propose a P hase-wise A ttention GCN (PAGCN) recommendation model. Considering the different characteristics of different stages during the aggregation process of the GCN-based recommendation systems, our model uses different information aggregation methods in different graph convolution layers and adopts targeted aggregation methods. In this way, high-order neighborhood information can be more controlled to improve the performance and effectiveness of our model. The experimental results on real-world datasets show that our model outperforms various baselines, demonstrating the reasonableness of our method.},
  archive      = {J_ASOC},
  author       = {Peng Zhou and Yachao Cui and Xiaoxu Guo and Jiabing Wei and Han Cao},
  doi          = {10.1016/j.asoc.2024.111910},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111910},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Phase-wise attention GCN for recommendation denoising},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mitigating adversarial threats in deep CT image diagnosis
models via a dual-stage inference-time defense. <em>ASOC</em>,
<em>163</em>, 111909. (<a
href="https://doi.org/10.1016/j.asoc.2024.111909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI), particularly deep learning (DL) and machine learning (ML) have revolutionized disease diagnosis using complex medical images such as X-rays and CT scans, significantly improving accuracy in identifying various medical conditions. However, concerns arise regarding the trustworthiness of these models due to their vulnerability to adversarial attacks, potentially leading to inaccurate predictions. In response, we propose an innovative pipeline defensive approach that integrates denoising techniques like Total Variation Minimization (TVM) and Non-local (NL) means, followed by a fuzzy logic-based image transformer called Fuzzy Image Transformations (FIT). Deployed during the inferencing phase as a preprocessing module, this strategy aims to fortify DL medical diagnosis models, addressing adversarial vulnerabilities and ensuring their reliability in critical healthcare applications. Our focus is narrowed down to COVID-19 diagnosis, where we initially developed a model for accurately classifying lung CT images, covering both normal and COVID-19 pneumonia cases. Our experimental procedures, employing the Stratified K-Fold cross-validation method with K=5, systematically evaluate the model’s susceptibility to benchmark adversarial attacks such as the Fast Gradient Sign Method (FGSM), revealing a significant drop in aggregate average metrics across all folds under adversarial conditions. Specifically, precision (P), recall (R), accuracy (A), and F1-score (F) metrics decrease from 95.86 %, 96.16 %, 95.85 %, and 96.00–12.54 %, 13.13 %, 12.63 %, and 12.83 %, respectively. Notably, our proposed pipeline defense approach demonstrates substantial efficacy in enhancing diagnostic models, with average P, R, A, and F metrics improving impressively to 92.65 %, 92.39 %, 92.65 %, and 92.51 %, respectively, when applied to the same adversarial images. These promising results underscore the potential of our proposed pipeline defense techniques to enhance the resilience and reliability of AI-based diagnostic systems.},
  archive      = {J_ASOC},
  author       = {Burhan Ul Haque Sheikh},
  doi          = {10.1016/j.asoc.2024.111909},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111909},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mitigating adversarial threats in deep CT image diagnosis models via a dual-stage inference-time defense},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning with spinally shared layers.
<em>ASOC</em>, <em>163</em>, 111908. (<a
href="https://doi.org/10.1016/j.asoc.2024.111908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer-learned models have achieved promising performance in numerous fields. However, high-performing transfer-learned models contain a large number of parameters. In this paper, we propose a transfer learning approach with parameter reduction and potential high performance. Although the high performance depends on the nature of the dataset, we ensure the parameter reduction. In the proposed SpinalNet shared parameters, all intermediate-split-incoming parameters except the first-intermediate-split contain a shared value. Therefore, the SpinalNet shared parameters network contains three parameter groups: (1) first input-split to intermediate-split parameters, (2) shared intermediate-split-incoming parameters, and (3) intermediate-split-to-output-split parameters. The total number of parameters becomes lower than the SpinalNet and traditional fully connected layers due to parameter sharing. Besides the overall accuracy, this paper compares the precision, recall, and F1-score of each class as performance criteria. As a result, both parameter reduction and potential performance improvement become possible for the ResNet-type models, VGG-type traditional models, and Vision Transformers. We applied the proposed model to MNIST, STL-10, and COVID-19 datasets to validate our claims. We also provided a posterior plot of the sample from different models for medical practitioners to understand the uncertainty. Example model training scripts of the proposed model are also shared to GitHub.},
  archive      = {J_ASOC},
  author       = {H.M. Dipu Kabir and Subrota Kumar Mondal and Syed Bahauddin Alam and U. Rajendra Acharya},
  doi          = {10.1016/j.asoc.2024.111908},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111908},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Transfer learning with spinally shared layers},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust knowledge distillation based on feature variance
against backdoored teacher model. <em>ASOC</em>, <em>163</em>, 111907.
(<a href="https://doi.org/10.1016/j.asoc.2024.111907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from large well-trained deep neural networks (DNNs), model compression has captured special attention for computing resource limited equipment, especially edge devices. Knowledge distillation (KD) is one of the widely used compression techniques for edge deployment, by obtaining a lightweight student model from a well-trained teacher model released on public platforms. However, it has been empirically noticed that the backdoor in the teacher model will be transferred to the student model during the process of KD. Although numerous KD methods have been proposed, most of them focus on the distillation of a high-performing student model without robustness consideration. Besides, some research adopts KD techniques as effective backdoor mitigation tools, but they fail to perform model compression at the same time. Consequently, it is still an open problem to well achieve two objectives of robust KD, i.e. , student model’s performance and backdoor mitigation. To address these issues, we propose RobustKD , a robust knowledge distillation that compresses the model while mitigating backdoor based on feature variance. Specifically, RobustKD distinguishes the previous works in three key aspects: (1) effectiveness - by distilling the feature map of the teacher model after detoxification, the main task performance of the student model is comparable to that of the teacher model; (2) robustness - by reducing the characteristic variance between the teacher model and the student model, it mitigates the backdoor of the student model under backdoored teacher model scenario; (3) generic - RobustKD still has good performance in the face of multiple data models ( e.g. , WRN 28-4, Pyramid-200) and diverse DNNs ( e.g. , ResNet50, MobileNet). Comprehensive experiments are conducted on four datasets, six models, two distillation methods, and two backdoor attack methods, compared with four baselines, and the results verified that the proposed method achieves the state-of-the-art performance in both aspects of accuracy and robustness. In addition, RobustKD is still effective when adaptive attacks are considered. The code of RobustKD is open-sourced at https://github.com/Xming-Z/RobustKD .},
  archive      = {J_ASOC},
  author       = {Jinyin Chen and Xiaoming Zhao and Haibin Zheng and Xiao Li and Sheng Xiang and Haifeng Guo},
  doi          = {10.1016/j.asoc.2024.111907},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111907},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust knowledge distillation based on feature variance against backdoored teacher model},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Emission reduction investment for green products of a
manufacturing system via optimal control theory and artificial
hummingbird algorithm. <em>ASOC</em>, <em>163</em>, 111905. (<a
href="https://doi.org/10.1016/j.asoc.2024.111905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Now, due to the global industrialization, carbon emission from the industrial section imposes a severe impact on the current environment. Due to the increasing awareness of environmental and sustainability issues, green products, also known as eco-friendly, include a considerable impact on the today’s market economy. Keeping mind of these two important issues of the current scenario, a production inventory model is developed in which time-dependent production rate is considered. In this work, the level of greenness of the products, selling price and time, influences the customers’ demand rate. Again, two different cases are considered due to the business assimilation of defectively manufactured items: rework and salvage policies. Additionally, the company uses pollution reduction technologies to lower emissions from the manufacturing firms in order to lower emission taxes. The Euler-Lagrange equation is used to solve an optimal control problem that arises in the model as time-dependent production rate is taken into the account. The optimisation problem corresponding to the proposed model is solved with the help of artificial hummingbird algorithm (AHA) as the system&#39;s average profit functions for both of the considered scenarios are highly nonlinear in nature w. r. to its decision variables. Further, the results obtained from AHA are compared with the help of other well-known metaheuristics. Finally, sensitivity analysis is conducted to arrive at meaningful conclusions and implications for the competent manager of the manufacturing firm.},
  archive      = {J_ASOC},
  author       = {Fleming Akhtar and Hachen Ali and Firdausi Khatun and Subhajit Das and Ali Akbar Shaikh},
  doi          = {10.1016/j.asoc.2024.111905},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111905},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Emission reduction investment for green products of a manufacturing system via optimal control theory and artificial hummingbird algorithm},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance mapping overlap complexity metric for
class-imbalance problems. <em>ASOC</em>, <em>163</em>, 111904. (<a
href="https://doi.org/10.1016/j.asoc.2024.111904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data complexity for class-imbalance problems is a hot topic in the field of data mining. Classical data complexity measures use techniques such as kNN, which computes the nearest neighbors of each instance. However, the kNN-based method to obtain the nearest neighbors of all instances is an NP-hard problem, which is not conducive to large-scale data complexity calculations. In order to solve this problem, from the global and local perspectives, four distance map complexity metric methods are proposed, and the complexity values is called the distance map overlap index (DMOI). Firstly, we use Mahalanobis distance or standardized Euclidean distance to calculate the distance of the instance to the center point of the data set, and arrange by the distance value of each instance. Then, according to the sequential label vector, iteratively searches the number of map cut-points for each class. Finally, according to the number of map cut-points of different classes, the DMOI of the data set is calculated. Experiments on 50 class-imbalanced datasets show that the DMOI method outperforms state-of-the-art complexity measures for class-imbalanced problems. Although there is still a gap between DMOI and ONB for rule-based or tree-based classifiers, the use of Pearson correlation coefficient shows that DMOI can effectively approximate ONB, and there is a strong positive correlation between them.},
  archive      = {J_ASOC},
  author       = {Qi Dai and Jian-wei Liu and Yong-hui Shi},
  doi          = {10.1016/j.asoc.2024.111904},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111904},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distance mapping overlap complexity metric for class-imbalance problems},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-criteria group emergency decision-making method
considering knowledge granularity. <em>ASOC</em>, <em>163</em>, 111903.
(<a href="https://doi.org/10.1016/j.asoc.2024.111903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergencies are typically characterized by abruptness, time urgency, and complexity, which give rise to challenges such as incomplete information, compromised information effectiveness, and reduced efficiency. To address these issues, this study proposes a novel multi-criteria group emergency decision-making (MCGEDM) method considering knowledge granularity. Within the framework of a hierarchical criterion system (HCS), decision makers&#39; (DMs&#39;) judgment information is extracted using belief distributions (BDs) on knowledge chunks, based on the conceptualization of knowledge granularity. This enables DMs to make judgments as effectively as possible, thereby improving efficiency in terms of time and enhancing the effectiveness of information. The generalized combination (GC) rule is applied for individual information fusion within basic nests, demonstrating internal revision and complementation of information. Automatic parameter determination methods are proposed to enhance the effectiveness of information and the efficiency of MCGEDM. Finally, the proposed method is demonstrated through a simulative case of an oil spill emergency, and the subsequent sensitivity analysis and comparisons verify its feasibility and effectiveness.},
  archive      = {J_ASOC},
  author       = {Su-Su Wang and Yuan-Wei Du},
  doi          = {10.1016/j.asoc.2024.111903},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111903},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-criteria group emergency decision-making method considering knowledge granularity},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SCGAFusion: A skip-connecting group convolutional attention
network for infrared and visible image fusion. <em>ASOC</em>,
<em>163</em>, 111902. (<a
href="https://doi.org/10.1016/j.asoc.2024.111902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of infrared and visible image fusion is to synthesize an image that contains complementary information as much as possible in paired infrared and visible images. However, due to using the convolution operation with local receptive field to extract features, most of the existing deep learning-based fusion methods fail to effectively exploit global contextual information in source images and so obtain a limited fusion performance. To address this issue, this paper proposes an end-to-end skip-connecting group convolutional attention network, termed as SCGAFusion, to fuse infrared and visible images. In SCGAFusion, a group convolutional attention block (GAB) is purposefully developed to promote the extraction and utilization capability of the fusion network for informative features. GAB introduces group convolutional layer to obtain hierarchical features, and employs a residual non-local attention module to capture long-range dependencies between pixels. In this way, it can not only focus more on the important regions or details, but also leverage both local neighborhood and global contextual information. Additionally, a hierarchical feature compensation mechanism based on skip connections is devised to integratedly exploit both local and global features. Experimental results on several public datasets qualitatively and quantitatively demonstrate the advantage of SCGAFusion over other state-of-the-art fusion methods.},
  archive      = {J_ASOC},
  author       = {Danchen Zhu and Jingbin Ma and Dong Li and Xiaoming Wang},
  doi          = {10.1016/j.asoc.2024.111902},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111902},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SCGAFusion: A skip-connecting group convolutional attention network for infrared and visible image fusion},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated learning for spiking neural networks by hint-layer
knowledge distillation. <em>ASOC</em>, <em>163</em>, 111901. (<a
href="https://doi.org/10.1016/j.asoc.2024.111901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on the federated Spiking Neural Network (SNN) has attracted increasing attention because of its advantages of low-power consumption and privacy security. However, existing federated SNN researches rely primarily on the Federated Average (FedAvg) strategy, transferring full network parameters between the server and clients and incurring substantial communication. To address this issue, we propose a Hint-layer Distillation-based Spiking Federated Learning (HDSFL) framework that reduces the communication cost by transferring knowledge and losslessly compressing the spiking tensor. To compensate for the information loss due to the binary representation of spikes in knowledge distillation, we introduce the hint-layer information instead of just soft-label distribution. To aggregate the knowledge effectively across clients on the server, we process the weighted knowledge aggregation on the spiking knowledge representation based on local performance. The experimental results on four classical and large-scale benchmarks show that our method reduces the communication cost by about 1–2 orders of magnitude compared to conventional methods, while achieving comparable accuracies. Especially on the CIFAR-10 dataset, our method achieves the same accuracy as FedAvg using only 20% of the communications, and consumes only 5.8% communications of FedAvg in a single round.},
  archive      = {J_ASOC},
  author       = {Xiurui Xie and Jingxuan Feng and Guisong Liu and Qiugang Zhan and Zhetong Liu and Malu Zhang},
  doi          = {10.1016/j.asoc.2024.111901},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111901},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Federated learning for spiking neural networks by hint-layer knowledge distillation},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broad fractional-order echo state network with slime mould
algorithm for multivariate time series prediction. <em>ASOC</em>,
<em>163</em>, 111900. (<a
href="https://doi.org/10.1016/j.asoc.2024.111900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, considering the infinite memory capability of fractional-order differential equations and the advantages of broad echo state network, a broad fractional-order echo state network (BFO-ESN) is proposed to build the multivariate time series prediction model. Firstly, the Pearson correlation coefficient method is utilized to filter the input data set and the output data set, which can clearly determine the number of reservoirs through the correlation between the two data sets. Secondly, the idea of fractional order is introduced into the broad echo state network, and then the internal characteristics of different learning tasks can be fully reflected. Thirdly, the reservoir parameters of BFO-ESN are optimized by using the slime mould algorithm (SMA). Finally, three numerical examples and a photovoltaic (PV) system are used to illustrate the effectiveness of BFO-ESN with SMA, and then some different optimization algorithms in the PV system are given to illustrate the advantage of SMA.},
  archive      = {J_ASOC},
  author       = {Xianshuang Yao and Huiyu Wang and Zhanjun Huang},
  doi          = {10.1016/j.asoc.2024.111900},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111900},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad fractional-order echo state network with slime mould algorithm for multivariate time series prediction},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconstruction of gene regulatory networks using graph
neural networks. <em>ASOC</em>, <em>163</em>, 111899. (<a
href="https://doi.org/10.1016/j.asoc.2024.111899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gene regulatory network (GRN) inference, a longstanding challenge in computational biology, aims to construct GRNs from genomic data. Graph Neural Networks (GNNs) are well-suited for this task due to their ability to leverage both node features and topological relationships. This research systematically evaluated various GNN variants, gradually narrowing the focus through a filtering process. The study considered multiple design aspects, including layers, epochs, decoders, activation functions, graph structures, aggregation methods, skip connections, dropout, and hidden dimensions. Ultimately, two promising models emerged, one based on the Chebyshev spectral graph convolutional operator and the other on the Hypergraph convolutional operator, demonstrating state-of-the-art performance. Notably, hypergraphs demonstrated superior performance on real datasets with higher-order dependencies, while the Chebyshev model showed greater generalization across both simulated and real datasets. The code for this research is available online at https://github.com/EmmaDPaul/GRN-inference-using-GNN .},
  archive      = {J_ASOC},
  author       = {Emma Paul M. and Jereesh A.S. and G. Santhosh Kumar},
  doi          = {10.1016/j.asoc.2024.111899},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111899},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reconstruction of gene regulatory networks using graph neural networks},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Molecular sharing and molecular-specific representations for
multimodal molecular property prediction. <em>ASOC</em>, <em>163</em>,
111898. (<a href="https://doi.org/10.1016/j.asoc.2024.111898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction plays a crucial role in drug discovery and development. However, traditional experimental measurements and Quantitative Structure-Activity Relationship (QSAR) models are often expensive, time-consuming, and data acquisition is challenging. To overcome these limitations and challenges, this study innovatively proposes a fusion molecular property prediction method called molecular property prediction model (MSSP) to address the non-uniqueness of Simplified Molecular Input Line Entry System (SMILES) string representation and the difficulty of capturing global information in molecular graphs. This method extracts multiple fingerprint features and utilizes graph neural network encoding to map different modalities of molecules into molecular sharing and molecular-specific representation spaces, achieving modal alignment and fusion of molecules by combining molecular invariance and representation specificity. To enhance the interpretability and visualization capabilities of the model, graph attention mechanisms are introduced, enabling the identification and inference of important chemical fragments within molecules. Experimental results on publicly available cell line phenotype and kinase activity datasets demonstrate that MSSP outperforms the current state-of-the-art methods in molecular property prediction. Additionally, MSSP exhibits strong competitiveness across nine benchmark molecular property prediction datasets. Furthermore, in the task of predicting SRC kinase data properties, this study successfully screens promising therapeutic compounds from compound libraries by validating the predictions of the MSSP model and combining them with traditional methods such as molecular docking and molecular dynamics simulations. Multiple potential Lyn inhibitors have been discovered through this approach. The application of MSSP model is helpful to discover new molecules with new drug properties or functions, accelerate the process of drug discovery, save time and resources, and provide important guidance for drug discovery.},
  archive      = {J_ASOC},
  author       = {Xuecong Tian and Sizhe Zhang and Ying Su and Wanhua Huang and Yongzheng Zhang and Xuan Ma and Keao Li and Xiaoyi Lv and Chen Chen and Cheng Chen},
  doi          = {10.1016/j.asoc.2024.111898},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111898},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Molecular sharing and molecular-specific representations for multimodal molecular property prediction},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GRASP method for the bi-objective multiple row equal
facility layout problem. <em>ASOC</em>, <em>163</em>, 111897. (<a
href="https://doi.org/10.1016/j.asoc.2024.111897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bi-Objective Multiple Row Equal Facility Layout Problem considers both quantitative and qualitative objectives that are very useful in many scenarios like the factory design. In this work, a new multi-objective GRASP approach is proposed which applies an ensemble of four different constructive methods followed by the combination of two local search procedures, improving the results from the state of the art. Due to the superiority of this proposal, a new dataset of larger problem instances is generated, providing detailed metrics of the obtained solutions.},
  archive      = {J_ASOC},
  author       = {Nicolás R. Uribe and Alberto Herrán and J. Manuel Colmenar},
  doi          = {10.1016/j.asoc.2024.111897},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111897},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GRASP method for the bi-objective multiple row equal facility layout problem},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A residual autoencoder-based transformer for fault detection
of multivariate processes. <em>ASOC</em>, <em>163</em>, 111896. (<a
href="https://doi.org/10.1016/j.asoc.2024.111896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of high-dimensional and noisy process signals reduces the effectiveness of conventional fault detection methods in industrial processes. Based on the hypothesis that data collected from normal and faulty processes has different characteristics, unsupervised deep neural networks, e.g., autoencoders, have been widely applied in process fault detection and achieved good performance. Many variants have been proposed to improve feature learning by combining different network structures. In this paper, a new transformer model, residual autoencoder-based transformer, is proposed for process fault detection. Firstly, autoencoder and transformer are integrated for better unsupervised feature learning of process signals. Secondly, linear embedding and attention mechanisms with bias are proposed to generate effective features from process signals. Finally, residual connections are constructed between the encoder and decoder of RATransformer to address overfitting in training. Four industrial cases are used to test the performance of RATransformer for process fault detection. The results show that the fault detection rate of RATransformer is at least 1 % higher than other comparison methods. Moreover, the testing results show that the model structure improves the fault detection performance of RATransformer. The complex models like RATransformer can be used in the industrial process when sufficient normal process data is available. An end-to-end training method can be further developed to improve the applicability of RATransformer in process fault detection in the future.},
  archive      = {J_ASOC},
  author       = {Jilin Shang and Jianbo Yu},
  doi          = {10.1016/j.asoc.2024.111896},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111896},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A residual autoencoder-based transformer for fault detection of multivariate processes},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vital node identification in complex networks based on
autoencoder and graph neural network. <em>ASOC</em>, <em>163</em>,
111895. (<a href="https://doi.org/10.1016/j.asoc.2024.111895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying vital nodes in complex networks is one of the critical research problems in network science. The emergence of deep learning has spurred the development of numerous approaches for vital node detection, particularly those leveraging graph embedding techniques. However, current methodologies often rely on partial network information to create latent representations of nodes, neglecting critical topological aspects of graphs. In this paper, we propose a novel fusion model, named AGNN , which merges Autoencoder and Graph Neural Network (GNN) architectures to fully integrate the structural details of networks into graph embeddings, thereby enhancing the model’s efficacy. Initially, a graph convolutional network-based Autoencoder is introduced to produce latent representations of nodes that comprehensively incorporate the network’s topological characteristics. Subsequently, AGNN uses the listMLE as the loss function to optimize the overall rank prediction model by combining the GNN and Susceptible–Infected–Recovered models. AGNN is trained on the synthetic network generated by the Barabási–Albert (BA) model without loss of generality and is then transferred to the real-world networks for vital node identification. We conduct extensive experiments on fifteen real-world network datasets in three metrics. AGNN has the highest average Kendall’s τ τ coefficient at 0.7059, 5% above the second best. Additionally, it achieves the highest average monotonicity index of 0.9977, outperforming the runner-up at 0.9843. The experimental findings affirm the effectiveness of AGNN compared to benchmark and state-of-the-art techniques.},
  archive      = {J_ASOC},
  author       = {You Xiong and Zheng Hu and Chang Su and Shi-Min Cai and Tao Zhou},
  doi          = {10.1016/j.asoc.2024.111895},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111895},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vital node identification in complex networks based on autoencoder and graph neural network},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A method for seismic fault identification based on
self-training with high-stability pseudo-labels. <em>ASOC</em>,
<em>163</em>, 111894. (<a
href="https://doi.org/10.1016/j.asoc.2024.111894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The imaging principle of seismic image is different from that of natural image. There are many problems on seismic images, such as limited resolution, complex reflection feature and strong uncertainty, which leads to significant difference in the research emphases of computer vision on seismic image. In response to the above issues, the semi-supervised semantic segmentation method based on self-training with high-stability pseudo-labels is proposed for seismic fault identification. Specifically, the self-training strategy Fault-Seg-ST is proposed to decouple Teacher–Student model predictions by adding random noise to unlabeled images. The selective retraining strategy Fault-Seg-SST is proposed to evaluate the reliability of pseudo-labels based on the stability of model predictions. The contrastive loss function is proposed to learn pixel-level feature representation with intra class affinity and inter class separability. The Fault-Seg-SST has achieved the optimal segmentation performance on both DeepLabV2 (Dice 86%, mIoU 77%) and UNet with ResNet 18 (Dice 83%, mIoU 76%). Experimental results show that the fault identification method based on self-training with high-stability pseudo-labels demonstrates the superiority of selective retraining strategy, and the fault identification model trained with Fault-Seg-SST strategy can achieve the fine-grained fault segmentation.},
  archive      = {J_ASOC},
  author       = {Kewen Li and Xiao Li and Ruonan Yin and Liechong Wang},
  doi          = {10.1016/j.asoc.2024.111894},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111894},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A method for seismic fault identification based on self-training with high-stability pseudo-labels},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Class conditioned text generation with style attention
mechanism for embracing diversity. <em>ASOC</em>, <em>163</em>, 111893.
(<a href="https://doi.org/10.1016/j.asoc.2024.111893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of artificial intelligence and natural language processing (NLP), natural language generation (NLG) has significantly advanced. Its primary aim is to automatically generate text in a manner resembling human language. Traditional text generation has mainly focused on binary style transfers, limiting the scope to simple transformations between positive and negative tones or between modern and ancient styles. However, accommodating style diversity in real scenarios presents greater complexity and demand. Existing methods usually fail to capture the richness of diverse styles, hindering their utility in practical applications. To address these limitations, we propose a multi-class conditioned text generation model. We overcome previous constraints by utilizing a transformer-based decoder equipped with adversarial networks and style-attention mechanisms to model various styles in multi-class text. According to our experimental results, the proposed model achieved better performance compared to the alternatives on multi-class text generation tasks in terms of diversity while it preserves fluency. We expect that our study will help researchers not only train their models but also build simulated multi-class text datasets for further research.},
  archive      = {J_ASOC},
  author       = {Naae Kwon and Yuenkyung Yoo and Byunghan Lee},
  doi          = {10.1016/j.asoc.2024.111893},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111893},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Class conditioned text generation with style attention mechanism for embracing diversity},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Tiny drone object detection in videos guided by the
bio-inspired magnocellular computation model. <em>ASOC</em>,
<em>163</em>, 111892. (<a
href="https://doi.org/10.1016/j.asoc.2024.111892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting drones in infrared videos is highly desired in many realistic scenarios, e.g. , unauthorized drone monitoring around airports. Nevertheless, automated drone detection is rather challenging when the targets appear as tiny objects ( ≤ 10 × 10 ≤10×10 pixels) against complex backgrounds. Conventional object detection algorithms, which mainly use static visual features, can hardly distinguish tiny objects from undesired artefacts in complex backgrounds. To alleviate this problem, we learn from the early biological visual pathway (including the parvocellular and magnocellular pathways), which process static and motion information simultaneously. Therefore, we propose a magnocellular inspired method for video tiny-object detection (Magno-VTOD) that integrates both static and motion visual information. The Magno-VTOD firstly employs a retinal magnocellular computation model to extract the motion strength of moving objects. The motion responses are then used to enhance the areas of the flying tiny drones effectively and efficiently, thereby facilitating the subsequent target detection procedure. We implement the video tiny-object detection method based on the widely adopted deep neural networks guided by the magnocellular computation model. Experimental results obtained on the large-scale Anti-UAV dataset (304451 video frames) validate that the proposed Magno-VTOD method significantly outperforms the competing state-of-the-art object detection methods on the tiny drone detection task. Particularly, the AP value is increased by 15.4% for tiny object detection, and by 17.1%/13.7% against wood/mountain backgrounds.},
  archive      = {J_ASOC},
  author       = {Gang Wang and Xin Yang and Liang Li and Kai Gao and Jin Gao and Jia-yi Zhang and Da-jun Xing and Yi-zheng Wang},
  doi          = {10.1016/j.asoc.2024.111892},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111892},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tiny drone object detection in videos guided by the bio-inspired magnocellular computation model},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision transformer-based robust learning for cloth-changing
person re-identification. <em>ASOC</em>, <em>163</em>, 111891. (<a
href="https://doi.org/10.1016/j.asoc.2024.111891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain of Cloth-Changing Person Re-identification has garnered substantial attention recently owing to its pivotal role in security applications. The essence of Cloth-Changing Person Re-identification lies in acquiring information unaffected by clothing variations, such as gait and silhouette. Most studies focus on the Convolutional Neural Network models. However, main drawback of Convolutional Neural Network is the limitation of receptive field. Methods based on Convolutional Neural Network primarily focus on modeling local information, making it complex to extract the discriminative part of multiple spatial contexts. Therefore, we proposed a Vision Transformer-based Cloth-Changing Person Re-identification framework called TransRL-ReID. Specifically, we encoded an image as a sequence of patches and constructed a Vision Transformer-based framework. In addition, a module named DropAll was designed to reduce local bias by adaptive adjustment of attention weights for each patch. We conducted extensive experiments on the real-world benchmark PRCC dataset. TransRL-ReID achieves 66.7% on Rank-1 and 60.0% on mean Average Precision, outperforming previous Convolutional Neural Network-based methods. After visual analysis of the results, we found that TransReID could extract information unrelated to clothing, such as gait and silhouette, and paid less attention to clothing information. DropAll improves the model’s ability to fit pedestrian distributions.},
  archive      = {J_ASOC},
  author       = {Chen Xue and Zhongliang Deng and Wangwang Yang and Enwen Hu and Yao Zhang and Shuo Wang and Yiming Wang},
  doi          = {10.1016/j.asoc.2024.111891},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vision transformer-based robust learning for cloth-changing person re-identification},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Variable neighborhood genetic algorithm for multi-order
multi-bin open packing optimization. <em>ASOC</em>, <em>163</em>,
111890. (<a href="https://doi.org/10.1016/j.asoc.2024.111890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving e-commerce landscape, efficient packaging and logistics reduce costs and enhance customer satisfaction. This study addresses the problem of dynamic bin size optimization in e-commerce logistics by proposing a series of intelligent algorithms. Considering real-world constraints such as item separation requirements, a Mixed Integer Programming Model for Multi-Order Multi-Box Open-Dimension Rectangular Packing (MOMB-ODRPP) is formulated. The Stacked Clustering Algorithm (SCA) series, One-Dimensional Fixed Stacked Clustering Algorithm (ODF-SCA), Two-Dimensional Fixed Stacked Clustering Algorithm (TDF-SCA), and Variable Neighborhood Descent Spatial Ordering Algorithm (VND-SOA) series are employed to solve the MOMB-ODRPP model and improve order packing rates and optimize bin sizes. Computational experiments using real-world data from JD’s e-commerce operations reveal that the TDF-SCA algorithm series outperforms the ODF-SCA series by approximately 5% in Case 4. In contrast, the VND-SOA-S1 and VND-SOA-S2 algorithms achieve improvements of 0.83% and 0.76%, respectively, over the TDF-SCA-P2 algorithm in Cases 4 and 11. The comparative analysis highlights the practical implications of bin size optimization, with Case 11 providing a more viable option for standardizing bin sizes in e-commerce logistics.},
  archive      = {J_ASOC},
  author       = {Jianglong Yang and Huwei Liu and Kaibo Liang and Li Zhou and Junhui Zhao},
  doi          = {10.1016/j.asoc.2024.111890},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111890},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Variable neighborhood genetic algorithm for multi-order multi-bin open packing optimization},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A scholars’ personality traits augmented multi-dimensional
feature fusion scholarly journal recommendation model. <em>ASOC</em>,
<em>163</em>, 111888. (<a
href="https://doi.org/10.1016/j.asoc.2024.111888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Journal recommendation is a popular research topic in academic resource recommendation. However, the reliability of the current model depends on rich features in the dataset, and ignores the issue of model performance being degraded by sparse sample features. To tackle this issue, inspired by personality trait-based recommendation techniques, we propose a P ersonality T rait-augmented M ulti-dimensional F eature F usion J ournal R ecommendation (PTMFFJRec) model that integrates scholars’ personality traits and multi-dimensional deep semantics, and utilize linguistic features and the big-5 personality model to estimate the personality of the scholars. This is the first multi-dimensional feature model that incorporates Transfer Learning, BERT, and GCN techniques to recommend academic journals based solely on the abstracts and titles of submitted manuscripts. Experimental results on the real-world Scopus’s dataset demonstrate that PTMFFJRec outperforms advanced benchmark models, specifically, surpassing the baseline models in metrics of MAP, MRR, Recall@20 and Diversity.},
  archive      = {J_ASOC},
  author       = {Xiaojun Li and Bilin Shao and Genqing Bian},
  doi          = {10.1016/j.asoc.2024.111888},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111888},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A scholars’ personality traits augmented multi-dimensional feature fusion scholarly journal recommendation model},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge-preserving image deraining network using cumulative
feature aggregation. <em>ASOC</em>, <em>163</em>, 111887. (<a
href="https://doi.org/10.1016/j.asoc.2024.111887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an edge-preserving image deraining network using a wavelet feature aggregation method. Wavelet subbands re correlated with each other, and the high-frequency subband in the horizontal direction is the least affected by rain streak contamination. On this basis, we introduce a single image deraining network that cumulatively aggregates wavelet subband features according to their importance. The network architecture primarily comprises a wavelet feature aggregation block and a residue channel guide block. The aggregation of features with the cumulative wavelet feature aggregation block moves downward and upward, and a long short-term memory-based multiscale attentive rain streak removal block is developed to serve as the backbone for rain streak removal. We use a residual channel map based on the low-frequency subband to construct guide features that assist in rain streak removal. A repetitive image restoration framework that incorporates two proposed blocks is used to iteratively improve rainy images. We test the proposed network on various image datasets and compare the deraining performance with those of existing methods The experimental results demonstrate that the performance of the proposed scheme is superior to that of other tested deraining methods. https://github.com/SYChoi98/CWFANet/},
  archive      = {J_ASOC},
  author       = {So Young Choi and Su Yeon Park and Il Kyu Eom},
  doi          = {10.1016/j.asoc.2024.111887},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111887},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Edge-preserving image deraining network using cumulative feature aggregation},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depthwise separable axial asymmetric wavelet convolutional
neural networks. <em>ASOC</em>, <em>163</em>, 111886. (<a
href="https://doi.org/10.1016/j.asoc.2024.111886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinterpreting wavelet multi-resolution analysis as CNN methods to endow them with the capacity for high-level semantic feature extraction has emerged as a research topic in deep sparse representations. We explore the fundamental operations of wavelet transform and convolution from a projective standpoint, then design a highly interpretable, exceptionally lightweight, and fully learnable deep neural network architecture known as Depthwise Separable Axial Asymmetric Wavelet Convolutional Neural Networks (DSAWCN). This network acquires wavelets filters customized for specific image tasks, thus providing adaptive and efficient multi-scale feature extraction strategies. We examined the behavior and the impact of the parameters in the proposed method using three general texture image datasets and four bark texture image datasets. The findings indicate that this exceptionally lightweight network surpasses current wavelet convolutional networks in terms of classification accuracy and achieves performance comparable to some eminent CNN models.},
  archive      = {J_ASOC},
  author       = {Tonghao Wang and Shijiao Gao and Yukang Huo and Piercarlo Cattani and Shuli Mei},
  doi          = {10.1016/j.asoc.2024.111886},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111886},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Depthwise separable axial asymmetric wavelet convolutional neural networks},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction maintenance based on vibration analysis and deep
learning — a case study of a drying press supported on a hidden markov
model. <em>ASOC</em>, <em>163</em>, 111885. (<a
href="https://doi.org/10.1016/j.asoc.2024.111885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of this paper is to describe a methodology that was developed to support maintenance decision-making methods based on equipment condition. Condition-Based Maintenance allows to increase equipment availability and maximize investments. This is mainly due to the prevention of unexpected equipment downtime. By avoiding turning on/off industrial equipment, production flows are more efficient, allowing manufacturers to improve the quality of the end-product. The industry aims more and more to correspond satisfying customer expectations. We argue that the methodology developed in this paper adds value to the existing literature, namely because the fact that it is possible to anticipate the state of an equipment without a large amount of support data. In other words, although one could find information gaps regarding the occurrence of failures, it was possible to accurately assess the state of the equipment. This approach is robust, as it can be used in distinct equipment with different sensors, making this methodology generalizable for Condition-Based Maintenance. The paper presents the validation of the preceding through a case study on drying presses in the paper industry. To do so, three states were adopted, namely: “Proper function”; “Alert state”; and “Equipment failure”. The methodology follows a series of steps, going through the collection of values from vibration sensors, imputation of values using Deep Artificial Neural Networks through on-line sensors, until reaching the last stage of classification carried out by the Hidden Markov Model. Through optimized observations from the previous steps, it was possible to define the hidden states through the Viterbi algorithm, which corresponds to the health states of the equipment. Additionally, it was possible to demonstrate that the proposed methodology can accurately characterize the condition states of the equipment based on the data obtained and can be generalized to other types of equipment.},
  archive      = {J_ASOC},
  author       = {Alexandre Martins and Inácio Fonseca and José Torres Farinha and João Reis and António J. Marques Cardoso},
  doi          = {10.1016/j.asoc.2024.111885},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111885},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prediction maintenance based on vibration analysis and deep learning — A case study of a drying press supported on a hidden markov model},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantum fuzzy decision-making for analyzing the service
progress life cycle of renewable energy innovation investments.
<em>ASOC</em>, <em>163</em>, 111884. (<a
href="https://doi.org/10.1016/j.asoc.2024.111884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovative and effective solutions can be produced by using advanced technologies to increase renewable energy innovation investments. However, many actions taken to increase these investments may cause the costs of enterprises to reach an unmanageable level. As a result, a priority analysis is needed to identify the most critical issues, but in literature, there are limited studies in the literature regarding this issue. Accordingly, the purpose of this study is to evaluate the service progress life cycle of renewable energy innovation investments. A novel model is presented in this process while integrating different approaches. First, the service progress life cycle of renewable innovation investments is weighted. In this scope, two-generation technology S-curve and facial action coding system-based Quantum Spherical fuzzy M-SWARA are considered. Secondly, the renewable innovation investment alternatives are ranked. In this framework, the integer patterns and facial action coding system-based Quantum Spherical fuzzy TOPSIS are taken into consideration. Additionally, these investment alternatives are also ranked by using VIKOR technique to test the validity of the proposed model results. Moreover, a sensitivity analysis is conducted by considering 8 different cases so that coherency of the results can be tested. The main contribution of this manuscript is the consideration of the FACS system to increase both the originality and effectiveness of the proposed model and the development of new technique named M-SWARA. Additionally, conducting a priority evaluation helps to identify the efficient investment strategies to increase renewable energy innovation. It is defined that the same findings can be achieved by both TOPSIS and VIKOR techniques. Furthermore, sensitivity analysis also indicates the same rankings as well. It is understood that the proposed model provides coherent and reliable findings. The findings denote that maturity is the most critical stage of the service progress life cycle of the renewable energy investments. Moreover, advanced technologies are found as the best alternative of the renewable innovation investments for the service progress life cycle. Hence, it is recommended that R&amp;D activities should be prioritized for the provision of advanced technologies in the renewable energy sector. For this purpose, research centers, universities, private sector companies and government institutions can work for the development of innovative and advanced technologies.},
  archive      = {J_ASOC},
  author       = {Gang Kou and Dragan Pamucar and Hasan Dinçer and Serhat Yüksel},
  doi          = {10.1016/j.asoc.2024.111884},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111884},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantum fuzzy decision-making for analyzing the service progress life cycle of renewable energy innovation investments},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Contrastive variational auto-encoder driven convergence
guidance in evolutionary multitasking. <em>ASOC</em>, <em>163</em>,
111883. (<a href="https://doi.org/10.1016/j.asoc.2024.111883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge transfer is at the core of the Evolutionary Multitasking (EMT) problem, as it exploits the interaction of inter-task common knowledge to accelerate task convergence. However, existing research on EMT is generally based solely on the framework of evolutionary computation, with limited integration of deep learning models. Additionally, few studies have found that utilizing deep learning models to generate individuals for transfer and guide the evolutionary trajectory may promote better convergence of algorithms. To address this research gap, we introduce the MFEA-VC (Multifactorial Evolutionary-Variational Auto-Encoder and Contrastive Learning) algorithm. Individuals are categorized based on task-label and inputted into a VAE, with sampling along feature dimensions. The VAE effectively guides the population towards better search areas by learning the latent trends of the distribution and generating transferred individuals. Simultaneously, a new training objective based on contrastive learning is proposed. This objective regulates the similarity between individuals from the same and different task-label, finely controlling individual features in the latent space. This approach makes the generated superior individuals more interpretable. To verify the superiority of MFEA-VC, we conduct comprehensive empirical studies on multi-task single-objective scenarios. We also validate the effectiveness of our improved loss function through theoretical analysis. The results demonstrate that, compared to state-of-the-art multifactorial algorithms, our method significantly enhances the global search capability during the early evolution stages, achieves excellent convergence results, and exhibits strong adaptability to heterogeneous tasks.},
  archive      = {J_ASOC},
  author       = {Ruilin Wang and Xiang Feng and Huiqun Yu},
  doi          = {10.1016/j.asoc.2024.111883},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111883},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive variational auto-encoder driven convergence guidance in evolutionary multitasking},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A many-objective evolutionary algorithm combining simplified
hypervolume and a method for reference point sampling based on angular
relationship. <em>ASOC</em>, <em>163</em>, 111881. (<a
href="https://doi.org/10.1016/j.asoc.2024.111881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the path for an evolutionary algorithm is crucial for its performance. Current methods for sampling reference points guiding evolutionary algorithms are inadequate for dealing with convex and concave Pareto fronts,and the uniformity of sampling results decreases significantly in high-dimensional spaces. In this paper,we propose a reference point sampling method based on angular relationships to tackle these issue. And we propose the concept of optimally distributed individuals based on the IGD indicator to ensure the distribution of the evolutionary process and prevent the algorithm from converging to local optima. Additionally,we introduce a novel method for calculating individuals’ fitness within the population,ensuring convergence,uniformity,and distribution of the evolutionary algorithm,thereby enhancing selection pressure among non-dominated individuals. Experimental results on diverse benchmark test problems demonstrate that the proposed algorithm competes favorably with six advanced evolutionary algorithms for many-objective optimization.},
  archive      = {J_ASOC},
  author       = {Tao Chao and Shuai Wang and Songyan Wang and Ming Yang},
  doi          = {10.1016/j.asoc.2024.111881},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111881},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A many-objective evolutionary algorithm combining simplified hypervolume and a method for reference point sampling based on angular relationship},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics informed machine learning model for inverse dynamics
in robotic manipulators. <em>ASOC</em>, <em>163</em>, 111877. (<a
href="https://doi.org/10.1016/j.asoc.2024.111877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of robotic modelling, the challenge of parameter estimation using limited joint monitoring data presents a substantial hurdle for both traditional physics-based methods (PBMs) and machine learning (ML) techniques. PBMs grapple with modelling uncertainties, variable working conditions, diverse robotic configurations, and incomplete parameter information. ML methods face hurdles in maintaining physical consistency, interpretability, and the need for extensive training data. In response to these challenges, this paper proposes a novel approach, the Equation Embedded Neural Network (E2NN), enhanced by an innovative Liquid mechanism, which effectively blends the strengths of PBMs and ML to surmount their inherent limitations. Its primary contributions encompass: (1) a pioneering review and synthesis of hybrid frameworks integrating physical principles with ML, (2) the development and rigorous validation of the E2NN framework, and (3) the introduction of a novel physics regulator and dynamic Liquid mechanism. Particularly, the proposed E2NN leverages inverse dynamics equations to construct specialized neural network layers, featuring activation functions and interconnections expressed as composition operators, thus explicitly encoding physical knowledge. This architectural choice proves especially well-suited for tasks involving inverse dynamics and dynamic planning of robotic manipulators. The accompanying Liquid mechanism allows for dynamic adaptation of interlayer connections in response to input data, enabling real-time adjustments to changing inputs and equations of motion, ultimately enhancing flexibility and performance. Quantitative assessments of E2NN reveal its compelling performance, yielding a Mean Absolute Error (MAE) of 0.10716, closely aligned with the Benchmark Deep Residual Shrinkage Network’s (DRSN) MAE of 0.10415, showcasing its competitive efficacy while achieving higher computational efficiency and a more compact model size. Robustness evaluations further confirm E2NN’s adaptability, as it attains a Mean Squared Error (MSE) of 0.3, outperforming DRSN’s 1.1 under varying working conditions. E2NN excels in torque trajectory fitting, achieving an impressive accuracy rate of 97.1%, underscoring its practical effectiveness. Furthermore, E2NN excels in torque prediction and parameter identification for inverse dynamics models, particularly when confronted with limited joint data and variable friction conditions. It substantially improving the discernment of robot dynamics and enhancing its applicability in real-world trajectory fitting.},
  archive      = {J_ASOC},
  author       = {Weikun Deng and Fabio Ardiani and Khanh T.P. Nguyen and Mourad Benoussaad and Kamal Medjaher},
  doi          = {10.1016/j.asoc.2024.111877},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111877},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Physics informed machine learning model for inverse dynamics in robotic manipulators},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-guided dual-channel graph neural networks for the
prediction of compound–protein interaction. <em>ASOC</em>, <em>163</em>,
111875. (<a href="https://doi.org/10.1016/j.asoc.2024.111875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compound–Protein Interaction (CPI) serves as essential indicators for efficiently screening potential candidate drugs. Previous studies have typically focused on modeling CPIs either from intramolecular or intermolecular interactions, disregarding the diversity of interactions and the fine dependencies between these two types of interactions, thereby limiting the accuracy of CPI predictions. We argue that properly considering both intramolecular and intermolecular interactions allows for a more comprehensive understanding of the interactions between compounds and proteins. To this end, we propose a novel approach called Co-guided Dual-channel Graph Neural Network (CDGN) for CPI predictions. CDGN simultaneously captures various CPI information from intramolecular and intermolecular interactions using a dual-channel aggregating mechanism. Furthermore, to model the complicated relationships between the two interactions, we design a co-guided learning scheme to model the CPIs between intramolecular and intermolecular interactions, enhancing the learning of each other. Finally, we predict CPIs based on the rich interaction information from dual channels. Exhaustive experimental studies on two benchmarks verify the superiority of CDGN in CPI predictions. In particular, CDGN achieves outstanding performance with RMSE evaluation metrics of 1.263 and 1.626 on the publicly available PDBbind and CSAQ-HiQ datasets, respectively.},
  archive      = {J_ASOC},
  author       = {Zheyu Wu and Huifang Ma and Bin Deng and Zhixin Li and Liang Chang},
  doi          = {10.1016/j.asoc.2024.111875},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111875},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-guided dual-channel graph neural networks for the prediction of compound–protein interaction},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enforcing high frequency enhancement in deep networks for
simultaneous depth estimation and dehazing. <em>ASOC</em>, <em>163</em>,
111873. (<a href="https://doi.org/10.1016/j.asoc.2024.111873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image dehazing and monocular depth estimation algorithms are crucial for environmental perception for autonomous driving. However, few algorithms can simultaneously perform single-image dehazing and depth estimation to solve the problems of perceiving surrounding information for unmanned vehicles in haze. The current algorithm based on the Transformer and Convolutional Neural Networks cannot effectively extract high frequency information and low frequency information in the image, resulting in poor image dehazing and depth estimation ability. Therefore, we design the DADENet network to solve these problems. We enhanced sensitivity to high-frequency information by incorporating Sobel operators and Gaussian blur into the residual unit structure, resulting in the ResNet-Edge-Encoder block. Furthermore, we improved the capability to extract both high-frequency and low-frequency information from feature maps by effectively combining depthwise convolutions with attention structures in the Transformer-Edge-Decoder block. Additionally, we designed the Depth-Transmission loss function to leverage the relationship between depth maps and transmission maps, thereby enhancing the accuracy of both single-image dehazing and depth estimation tasks. Extensive experiments on the NYU, KITTI and Citycapes datasets demonstrate that our DADENet algorithm effectively performs single-image dehazing and depth estimation, surpassing classical and state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xing Wei and Xiufen Ye and Xinkui Mei and Junting Wang and Heming Ma},
  doi          = {10.1016/j.asoc.2024.111873},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111873},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enforcing high frequency enhancement in deep networks for simultaneous depth estimation and dehazing},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Buffeting reliability of high-rise bridge tower in mountain
area based on CNN-BiLSTM. <em>ASOC</em>, <em>163</em>, 111872. (<a
href="https://doi.org/10.1016/j.asoc.2024.111872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the wind-induced vibration of structures has received a great deal of scholarly attention due to the increasing scale of large structures. In this study, an algorithm that combines a stochastic pseudo excitation method (SPEM), a convolutional neural network (CNN) and a bidirectional long short-term memory network (BiLSTM), named as SPEM-CNN-BiLSTM (SCBL), is proposed. The SCBL is a surrogate model consisting of three different computational modules, in which the SPEM can provide training samples for CNN-BiLSTM (CBL) network architecture, a convolutional layer extracts features of stochastic parameters and a BiLSTM handles time histories of bridge response. The accuracy of the SCBL prediction results is ensured by the set error threshold. The model also accounts for structural uncertainty in the inputs and is adapted during the training step to be more suitable for reliability analysis. According to the output qualified prediction data, the dynamic reliability based on the 99 % threshold is calculated by obtaining its probability density function, and the wind-induced vibration responses of uncertain bridge towers are studied. Firstly, the wind environment of the bridge tower was analyzed, and the average wind speed of 20 m/s among the bridge area was obtained by field measurements, and Computational Fluid Dynamics (CFD). Secondly, the proposed artificial neural network algorithm is applied to bridge jitter reliability analysis and compared with the traditionally recognized methods: the Monte Carlo method (MCM) and the stochastic pseudo excitation method (SPEM). The results of SCBL calculations are closer to the results of MCM than those of other methods. Additionally, the computational efficiency of the method is improved by 40 times over MCM and 2 times over SPEM. Finally, the data generated by the SCBL method will be used for reliability analysis. Uncertainties in structure, wind speed, and yaw angle all contribute to the uncertainty and dispersion of the stochastic response prediction. Among them, the uncertainty in wind speed has the greatest impact on the reliability.},
  archive      = {J_ASOC},
  author       = {Siyu Zhu and Rui Yi and Yongle Li and Xinyu Xu},
  doi          = {10.1016/j.asoc.2024.111872},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111872},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Buffeting reliability of high-rise bridge tower in mountain area based on CNN-BiLSTM},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multivariate migrating birds optimization algorithm based
on disjunctive graph neighborhood for scenic spot vehicle scheduling.
<em>ASOC</em>, <em>163</em>, 111870. (<a
href="https://doi.org/10.1016/j.asoc.2024.111870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the group travel, reasonable vehicle scheduling needs to consider constraints such as traffic distance and route conflict, which is common in many vehicle transportation scheduling systems, and it is crucial for operators to improve operational efficiency and provide a higher quality service experience. In this paper, a scenic spot vehicle scheduling problem is designed based the real-world scenario. In order to improve the search efficiency and maintain the diversity of solutions , an intelligent scheduling algorithm based on improved migrating birds optimization(gMBO) is proposed. The gMBO applies a neighborhood structure based on disjunctive graph to accelerate the solution search in the touring phase by avoiding redundancy. Besides, leveraging the left–right sequential queue characteristics of the MBO algorithm, gMBO utilizes two mechanisms to enhance the interaction between queues in the leader replacement stage, which can expand the search space of solutions and at the same time to maintain population diversity . Finally, we consider using the POX crossover operator in the individual, it is well adapted to the characteristics of the problem can reduce the generation of unreasonable solutions. The computational results show that the neighborhood structure is feasible. Considering the problem of scenic spot vehicle scheduling in practical urban applications, the multivariate migrating birds optimization algorithm based on disjunctive graph neighborhood is more effective than the MILP and other three meta-heuristic algorithms, and the optimal solution is obtained under the same stopping criteria, with an average RPD of 2.16. It has the advantages of fast convergence and good robustness.},
  archive      = {J_ASOC},
  author       = {Rong Fei and Zilong Wang and Junhuai Li and Facun Zhang and Hailong Peng and Junzhi Cheng},
  doi          = {10.1016/j.asoc.2024.111870},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111870},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multivariate migrating birds optimization algorithm based on disjunctive graph neighborhood for scenic spot vehicle scheduling},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Carbon futures return forecasting: A novel method based on
decomposition-ensemble strategy and markov process. <em>ASOC</em>,
<em>163</em>, 111869. (<a
href="https://doi.org/10.1016/j.asoc.2024.111869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of carbon futures return is essential for global climate policy and financial markets, yet it remains a significant challenge due to the complex nature of these returns. This paper proposes a novel hybrid model for carbon futures return forecasting based on decomposition-ensemble strategy and Markov process. This model can effectively capture nonlinear, nonstationary and volatile features of carbon prices and provide accurate point and interval forecasts. We use the Moth–Flame Optimization (MFO) algorithm and the Average Maximum Envelope Entropy (AMEE) function to optimize the parameters of Variational Mode Decomposition (VMD), which is a data decomposition technique that can separate the regular components and the random temporal component (noise) in the time series. We use Support Vector Regression (SVR) models to forecast the regular subsequences obtained by VMD, and Markov processes to quantify and forecast the random temporal component. We combine these forecasts to obtain the final prediction results, which incorporate both the regular and random information in the data. The proposed model supports both point and interval predictions, which are derived by summing the results of all the sub-models. We conduct extensive experiments to validate our approach and compare it with other models. The results show that our model outperforms other models in terms of prediction accuracy, reliability, and economic value. Our model can provide significant economic value for carbon futures investors and help them make favorable decisions in investment.},
  archive      = {J_ASOC},
  author       = {Yuan Zhao and Weiguo Zhang and Xue Gong and Xiufeng Liu},
  doi          = {10.1016/j.asoc.2024.111869},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111869},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Carbon futures return forecasting: A novel method based on decomposition-ensemble strategy and markov process},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A resilience control method for mitigating the sudden change
in online group opinion based on q-learning and PSO. <em>ASOC</em>,
<em>163</em>, 111867. (<a
href="https://doi.org/10.1016/j.asoc.2024.111867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s era, individuals can speak and communicate online anytime and anywhere. The Online Group Opinion (OGO), formed by the online statements of netizens, is regarded as a complex system. OGO is easy to get rapid and unpredictable sudden changes under external stimuli such as social emergencies, which may further lead to online violence, catalyze spread of emergencies, posing significant threats to social organization and cybersecurity. This paper proposes an innovative method to mitigate sudden changes in OGO using Q-learning from Reinforcement Learning and the Particle Swarm Optimization (PSO) algorithm, aiming to mitigate these sudden changes by controlling the resilience of the OGO. First, an OGO resilience index based on catastrophe theory and resilience theory is established to portray OGO sudden changes phenomena. Next, a resilience control algorithm integrated with Q-learning to optimize particles’ parameter update process is proposed, called QLPSOND. QLPSOND is used to control the system’s resilience to improve its ability to resist external stimuli, thus mitigating the sudden changes crisis. Through comparisons with other three algorithms on two online forums datasets, our proposed QLPSOND resilience control method demonstrates significant advantages in addressing complex social control issues, with average fitness improvement of 37.59 % and 7.98 %, and best fitness improvement of 31.66 % and 10.28 %, respectively, compared to most competitive state-of-the-art baseline. Additionally, this paper further discusses the practical significance of resilience control strategies and management implications. These efforts can extend the opinion dynamics approaches in online environments and assist enterprises and governments in monitoring and managing online communities.},
  archive      = {J_ASOC},
  author       = {Di Wu and Bin Hu and Xiaomeng Ma and Zhichao Wang},
  doi          = {10.1016/j.asoc.2024.111867},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111867},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A resilience control method for mitigating the sudden change in online group opinion based on Q-learning and PSO},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian ART for incomplete datasets. <em>ASOC</em>,
<em>163</em>, 111865. (<a
href="https://doi.org/10.1016/j.asoc.2024.111865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive Resonance Theory (ART) models allow for categorizing data in a fast and incremental manner. In particular, the Bayesian ART leverages Bayesian methodology to capture complex relationships between categories. While Bayesian methods are well-known for handling uncertainty, Bayesian ART models are at the mercy of a recurring foe: missing data. When data is missing, practitioners must rely on off-the-shelf solutions to impute the data before using Bayesian ART, effectively truncating the uncertainty quantification. To overcome such limitation, we (I) estimate the distribution of missing data entries using a Gaussian mixture model and (II) modify the three steps of Bayesian ART (category choice, matching, and update) to propagate the uncertainty around the missing entries. Experiments in a variety of tabular datasets show that, in general, our novel methodology leads to better results than using off-the-shelf imputation solutions. The performance gap becomes especially noticeable as the number of missing data entries increases.},
  archive      = {J_ASOC},
  author       = {Alan L.S. Matias and João Paulo P. Gomes and César Lincoln C. Mattos and Ajalmar R. Rocha Neto and Diego Mesquita},
  doi          = {10.1016/j.asoc.2024.111865},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111865},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bayesian ART for incomplete datasets},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning algorithms for dynamic pricing
and inventory management of perishable products. <em>ASOC</em>,
<em>163</em>, 111864. (<a
href="https://doi.org/10.1016/j.asoc.2024.111864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A perishable product has a limited shelf life, and inefficient management often leads to waste. This paper focuses on dynamic pricing and inventory management strategies for perishable products. By implementing effective inventory control and the right pricing policy, it is possible to maximize expected revenue. However, the exponential growth of the problem size due to the shelf life of the products makes it impractical to use methods that guarantee optimal solutions, such as Dynamic Programming (DP). Therefore, approximate solution algorithms become necessary. We use Deep Reinforcement Learning (DRL) algorithms to address the dynamic pricing and ordering problem for perishable products, considering price and age-dependent stochastic demand. We investigate Deep Q Learning (DQL) solutions for discrete action spaces and Soft Actor-Critic (SAC) solutions for continuous action spaces. To mitigate the negative impact of the stochastic environment inherent in the problem, we propose two different DQL approaches. Our results show that the proposed DQL and SAC algorithms effectively address inventory control and dynamic pricing for perishable products, even when products of different ages are offered simultaneously. Compared to dynamic programming, our proposed DQL approaches achieve an average approximation of 95.5 % and 96.6 %, and reduce solution times by 71.5 % and 79.9 %, respectively, for the largest problem. In addition, the SAC algorithm achieves on average 4.6 % and 1.7 % better results and completes the task 56.1 % and 48.2 % faster than the proposed DQL algorithms.},
  archive      = {J_ASOC},
  author       = {Tuğçe Yavuz and Onur Kaya},
  doi          = {10.1016/j.asoc.2024.111864},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111864},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning algorithms for dynamic pricing and inventory management of perishable products},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal solutions to granular fuzzy relation equations with
fuzzy logic operations. <em>ASOC</em>, <em>163</em>, 111861. (<a
href="https://doi.org/10.1016/j.asoc.2024.111861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy relation equations are commonly utilized to describe the fuzzy relationship between the antecedent and the consequent parts of complex data environment, and play a vital role in fuzzy system modeling. An interesting topic is to determine fuzzy relation equations based on existing fuzzy logic operations, including the t -norms and t -conorms ( s -norms). However, fuzzy rule-based models developed in form of numeric values still face significant challenges in accuracy and interpretability. This study aims to introduce the idea of granular computing to formulate granular fuzzy relation equations based on the fuzzy logic operations and evaluate the performance of granular fuzzy relation equations using the general principle of justifiable granularity. A two-phase development of granular fuzzy relation equations is designed: In the first phase, fuzzy relation equations are constructed based on t -norms and t -conorms; in the second phase, granular augmentation of fuzzy relation equations is realized by allocating a certain level of information granularity as the granular parameters of fuzzy relations. The capability of granular outputs is evaluated with the aid of two conflicting criteria—coverage ( cov ) and specificity ( sp ). The optimal solutions to granular fuzzy relation equations are explored using combinations of optimization algorithms. The originality of this study lies on the development of granular fuzzy relation equations with fuzzy logic operations, which provides a human-centric platform of fuzzy system modeling to enhance accuracy and interpretability. Experimental studies are carried out based on the UCI and KEEL machine learning datasets to report on the feasibility and performance of the proposed model.},
  archive      = {J_ASOC},
  author       = {Dan Wang and Kai Yu and Xiubin Zhu and Zhenhua Yu},
  doi          = {10.1016/j.asoc.2024.111861},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111861},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal solutions to granular fuzzy relation equations with fuzzy logic operations},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systematic review of software engineering uses of
multi-criteria decision-making methods: Trends, bibliographic analysis,
challenges, recommendations, and future directions. <em>ASOC</em>,
<em>163</em>, 111859. (<a
href="https://doi.org/10.1016/j.asoc.2024.111859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correctly adhering to the processes within the software development life cycle (SDLC), from analysis and design to coding and testing, is vital for ensuring the successful and efficient creation of high-quality software applications. These structured phases provide a systematic approach to software development, facilitating clear communication, reducing errors, and improving collaboration among development teams. For the proper and correct use of SDLC processes, it is essential for both software engineers and software decision makers to perform the correct and needed actions while performing each software process, and one method of facilitating that is multicriteria decision making (MCDM). This study aims to provide a systematic review of the use of MCDM within the field of software engineering (SE), encompassing methodologies such as fuzzy MCDM, AHP, TOPSIS, DEMATEL, and other methods, with a deliberate focus on software engineering development processes. To ensure the high quality of this review, a methodical and structured literature search process was performed with strict selection criteria, resulting in the identification of 32 contributions on the applications of MCDM in SE from various databases, including Scopus, ScienceDirect, IEEE Xplore digital library (IEEE), and Web of Science (WOS). The selected papers were taxonomized into seven main categories, with some divided into subcategories. This paper presents a systematic and comprehensive analysis of the aforementioned studies, investigating the challenges, motivations, and recommendations found within each, thereby paving the way for potential future research. Bibliometric analysis is also provided to show concise quantitative analysis of related bibliographic information, which draws several key insights into publication trends. Finally, a critical analysis of the current literature and existing research is presented, while also addressing relevant research gaps.},
  archive      = {J_ASOC},
  author       = {Aws A. Magabaleh and Lana L. Ghraibeh and Afnan Y. Audeh and A.S. Albahri and Muhammet Deveci and Jurgita Antucheviciene},
  doi          = {10.1016/j.asoc.2024.111859},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111859},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Systematic review of software engineering uses of multi-criteria decision-making methods: Trends, bibliographic analysis, challenges, recommendations, and future directions},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solution of maximum scatter traveling salesman problem
through evolutionary approaches. <em>ASOC</em>, <em>163</em>, 111858.
(<a href="https://doi.org/10.1016/j.asoc.2024.111858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with a variant of the traveling salesman problem (TSP) called the maximum scatter traveling salesman problem (MSTSP). The goal of MSTSP is to find a Hamiltonian cycle that maximizes the minimum length edge of the cycle. This problem has real-world applications in domains such as manufacturing and medical imaging. Both symmetric and asymmetric versions of the problem are considered in this paper. To address this problem, we have developed two evolutionary approaches. Our first approach is based on a genetic algorithm, whereas the second approach is based on a differential evolution algorithm. Initial solution generation procedure, variation operators (crossover and mutation) used in our approaches are designed considering the characteristics of this problem. The solutions obtained through variation operators are improved further through a local search that is specially adapted for MSTSP. For benchmarking, we have compared the performance of our proposed approaches with the state-of-the-art approaches available in the literature. Our approaches obtained better quality solutions in a shorter time for both symmetric and asymmetric versions of the problem, thereby clearly demonstrating their effectiveness in solving MSTSP.},
  archive      = {J_ASOC},
  author       = {Alok Singh and Sebanti Majumder},
  doi          = {10.1016/j.asoc.2024.111858},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111858},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solution of maximum scatter traveling salesman problem through evolutionary approaches},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-driven co-evolutionary exploration algorithm for
computationally expensive constrained multi-objective problems.
<em>ASOC</em>, <em>163</em>, 111857. (<a
href="https://doi.org/10.1016/j.asoc.2024.111857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted multi-objective optimization algorithms have attracted widespread attention due to their outstanding performance in computationally expensive real-world problems. However, there is relatively little research about multi-objective optimization with complex and expensive constraints. Hence, a data-driven co-evolutionary exploration (DDCEE) algorithm is presented in this paper for the above-mentioned problems, where Radial Basis Functions are utilized to train dynamically updated surrogate models for each objective and constraint. Specifically, a data-driven co-evolutionary exploration framework is proposed to fully utilize and mine the potential available information of RBF models, and RBF models are constantly updated to guide co-evolutionary in discovering valuable feasible regions and achieving global optimization. In co-evolutionary exploration, one population focuses on exploring the entire space without considering constraints, while the other population focuses on exploring feasible regions and collaborating by sharing their respective offspring. Reference vectors are introduced in co-evolutionary exploration to divide the objective space into several sub-regions for further selection. Furthermore, an adaptive selection of promising samples strategy is presented to reasonably utilize the information of solutions with good convergence and enhance the convergence and diversity of the Pareto front. After comprehensive experiments on constrained multi/many-objective benchmark cases and an engineering application problem, DDCEE shows more stable and impressive performance when compared with five state-of-art algorithms.},
  archive      = {J_ASOC},
  author       = {Wenyi Long and Peng Wang and Huachao Dong and Jinglu Li and Chongbo Fu},
  doi          = {10.1016/j.asoc.2024.111857},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111857},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven co-evolutionary exploration algorithm for computationally expensive constrained multi-objective problems},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exploratory study of self-supervised pre-training on
partially supervised multi-label classification on chest x-ray images.
<em>ASOC</em>, <em>163</em>, 111855. (<a
href="https://doi.org/10.1016/j.asoc.2024.111855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper serves as the first empirical study on self-supervised pre-training on partially supervised learning, an emerging yet unexplored learning paradigm with missing annotations. This is particularly important in the medical imaging domain, where label scarcity is the main challenge of practical applications. To promote the awareness of partially supervised learning, we leverage partially supervised multi-label classification on chest X-ray images as an instance task to illustrate the challenges of the problem of interest. Through a series of simulated experiments, the empirical findings validate that solving multiple pretext tasks jointly in the pre-training stage can significantly improve the downstream task performance under the partially supervised setup. Further, we propose a new pretext task, reverse vicinal risk minimization, and demonstrate that it provides a more robust and efficient alternative to existing pretext tasks for the instance task of interest.},
  archive      = {J_ASOC},
  author       = {Nanqing Dong and Michael Kampffmeyer and Haoyang Su and Eric Xing},
  doi          = {10.1016/j.asoc.2024.111855},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111855},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An exploratory study of self-supervised pre-training on partially supervised multi-label classification on chest X-ray images},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Evaluation and selection of e-learning websites using
intuitionistic fuzzy confidence level based dombi aggregation operators
with unknown weight information. <em>ASOC</em>, <em>163</em>, 111850.
(<a href="https://doi.org/10.1016/j.asoc.2024.111850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the surge in online learning has eclipsed traditional classroom teaching, elevating the prominence of e-learning platforms. However, the quest for a reliable methodology to evaluate these platforms has remained pressing. While several studies have explored this domain, none have integrated decision experts’ confidence levels. This study bridges this gap by seamlessly incorporating confidence levels into Dombi aggregation operators within an intuitionistic fuzzy framework. The study introduces two innovative operators: the confidence-level intuitionistic fuzzy Dombi weighted averaging operator (CIFDWA) and the confidence-level intuitionistic fuzzy Dombi weighted geometric operator (CIFDWG). These operators are employed to form a novel multi-attribute group decision-making (MAGDM) approach, which is demonstrated by applying them to select the optimal e-learning website in a real-world scenario. The “Stepwise Weight Assessment Ratio Analysis (SWARA)” method is utilized to determine criteria weights. The findings highlight the critical importance of sub-criteria such as topic diversity, pricing, content credibility, data security, and user interface simplicity. The analysis reveals Unacademy as the leading platform, closely followed by Byju’s. Through meticulous comparative analysis and sensitivity evaluations, we underscore the stability and coherence of the proposed model. The study’s novelty lies in its innovative approach to integrating confidence levels in multi-attribute group decision-making scenarios, promising broader applications across various real-world contexts. Educational advisers and management can leverage this methodology to empower students with informed choices.},
  archive      = {J_ASOC},
  author       = {Mijanur Rahaman Seikh and Prayosi Chatterjee},
  doi          = {10.1016/j.asoc.2024.111850},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111850},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation and selection of E-learning websites using intuitionistic fuzzy confidence level based dombi aggregation operators with unknown weight information},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integration of object detection and semantic segmentation
based on convolutional neural networks for navigation and monitoring of
cyanobacterial blooms in lentic water scenes. <em>ASOC</em>,
<em>163</em>, 111849. (<a
href="https://doi.org/10.1016/j.asoc.2024.111849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lentic waters, such as lakes, lagoons, reservoirs, and wetlands are characterized by their absence of current. In recent decades, they have been threatened by pollution and scarcity due to various environmental factors. Therefore, they require frequent monitoring to ensure their health and purity, especially to control the proliferation of harmful cyanobacteria (pollutants). Machine Vision Systems (MVS) on board Autonomous Surface Vehicles (ASVs) is a good option for automatic image processing in this context. ASVs must navigate safely, and obstacle detection is essential. In addition, the segmentation of pollutants in water is crucial. We propose an architecture based on convolutional neural networks that integrates both object detection and semantic segmentation. The goal is to simultaneously extract all available global information to detect objects and amorphous textures (cyanobacterial patches and water bodies), considering their variations in size, pose, and appearance. The architecture includes two branches: object detection and semantic segmentation, sharing the same backbone and neck. We evaluate the model on our dataset and the results show that it can holistically understand lentic water scenes with high accuracy, and the integration of the attention mechanism improves its overall performance.},
  archive      = {J_ASOC},
  author       = {Fredy Barrientos-Espillco and María J. Gómez-Silva and Eva Besada-Portas and Gonzalo Pajares},
  doi          = {10.1016/j.asoc.2024.111849},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111849},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integration of object detection and semantic segmentation based on convolutional neural networks for navigation and monitoring of cyanobacterial blooms in lentic water scenes},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithmic stock trading based on ensemble deep neural
networks trained with time graph. <em>ASOC</em>, <em>163</em>, 111847.
(<a href="https://doi.org/10.1016/j.asoc.2024.111847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial forecasting is generally implemented by analyzing the time series data related to the stock. This is accomplished widely with deep neural networks (DNNs) since DNNs can directly extract the related information that is otherwise hard to obtain. Time series is the core data representation of financial forecasting problem since it comes naturally. However recent studies show that even if time series representation is necessary, it still lacks certain aspects related to the problem. One of them is the relationship between the stocks of the market which can be captured through graph representation. Therefore, DNNs might solve the financial forecasting problem better when graph and time series representations are combined. In this study, we present different graph representations that can be used for this purpose. We also present an ensemble network that gives an investment strategy related to the stock market from stock predictions. Our proposed model returns an average of 20.09% annual profit on DOW30 dataset through daily buy–sell decisions based on close prices. Therefore, it can serve as a daily financial investment strategy, offering higher annual returns than conventional heuristic approaches.},
  archive      = {J_ASOC},
  author       = {Muhammed Yilmaz and Mustafa Mert Keskin and Ahmet Murat Ozbayoglu},
  doi          = {10.1016/j.asoc.2024.111847},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111847},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Algorithmic stock trading based on ensemble deep neural networks trained with time graph},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A discrete moth-flame optimization algorithm for multiple
automated guided vehicles scheduling problem in a matrix manufacturing
workshop. <em>ASOC</em>, <em>163</em>, 111846. (<a
href="https://doi.org/10.1016/j.asoc.2024.111846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing need for customized and varied production has highlighted the significance of smart and automated factories in the manufacturing sector. In this particular context, the scheduling of multiple Automated Guided Vehicles (AGVs) plays a pivotal role in enhancing the efficiency of operations within an intelligent manufacturing shop. This study centers on the material handling process within a matrix manufacturing shop with the objective of identifying the most cost-effective transportation routes for materials. To accomplish the specified objective, this research aims to identify an optimal solution for reducing transportation costs. In particular, this study formulates a mixed-integer linear programming model and introduces a novel discrete variant of the moth-flame optimization (MFO) algorithm, named DMFO, to address the scheduling problem. The DMFO algorithm incorporates several significant enhancements. Firstly, a population initialization method is proposed, which combines a nearest-neighbor-based adaptive heuristic and a random sorting technique to ensure the formation of a well-structured population. Secondly, the flame generation mechanism and spiral flight search processes within the MFO have been redefined to achieve a more optimal balance between exploration and exploitation. A neighborhood search mechanism is subsequently devised, employing the concept of neighborhood relevance to accelerate the convergence process. Additionally, a heuristic approach is introduced to reduce the computational cost. Moreover, a population regeneration mechanism is proposed to avoid the algorithm falling into a local optimum. To validate the effectiveness of the DMFO, a comparative analysis is conducted using a dataset of 110 real-world factory instances. In this analysis, eight well-established optimization algorithms are employed. The simulation results consistently demonstrate that the relative percentage deviation (RPD) value of the DMFO tends to approach 0% more closely compared to other algorithms, thereby substantiating the effectiveness of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Junhai Zeng and Wei Xie and Mi Pan},
  doi          = {10.1016/j.asoc.2024.111846},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111846},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete moth-flame optimization algorithm for multiple automated guided vehicles scheduling problem in a matrix manufacturing workshop},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating part of speech information in span
representation for named entity recognition. <em>ASOC</em>,
<em>163</em>, 111844. (<a
href="https://doi.org/10.1016/j.asoc.2024.111844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named Entity Recognition (NER) plays a pivotal role in knowledge extraction and improving the intelligence of edge computing. The effectiveness of span-based NER models predominantly depends on the representation of spans. Existing methods primarily utilize semantic features to represent spans, often neglecting other vital information. This paper proposes a method incorporating Part of Speech (POS) information into span representations to overcome this limitation. Central to this methodology is a span POS encoder designed to extract the POS feature of spans. For migrating the method to edge devices, this paper introduces a fast span POS encoder, which significantly reduces the time complexity of POS feature extraction. Building upon this innovation, a span-based NER model named IPSI ( I ncorporating P art of S peech I nformation in span representation) is developed, exhibiting outstanding performance on nested and flat datasets. Comparison the original and fast span POS encoders reveals that while the fast encoder slightly compromises performance, it markedly accelerates the training and inference processes. Finally, through a series of experiments and sample analyses, this article explores the intrinsic mechanism through which the span POS feature influences entity recognition and further illustrates the importance of the POS feature.},
  archive      = {J_ASOC},
  author       = {Ziyuan Cui and Zhongwen Guo and Yujun Lan and Xiaomei Li and Huafeng Dai},
  doi          = {10.1016/j.asoc.2024.111844},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111844},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporating part of speech information in span representation for named entity recognition},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving unbalanced image classification through
fine-tuning method of reinforcement learning. <em>ASOC</em>,
<em>163</em>, 111841. (<a
href="https://doi.org/10.1016/j.asoc.2024.111841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classification, especially unbalanced image classification, holds considerable promise for practical applications. Existing research focuses mainly on enhancing the effectiveness of classifiers through approaches such as data resampling and loss function adjustments. To date, most of the approaches address the class unbalanced issue by transforming unbalanced data distributions into balanced [id=SecondEdit]ones. Consequently, a critical challenge is to directly develop high-performance classifiers that are adaptive to diverse unbalanced data distributions. In this paper, for unbalanced image classification tasks, we propose a Reinforcement Learning Fine-tuning approach to Unbalanced image Classification (RLF-UC). Specifically, we train classification pretraining models on [d=FirstEdit]fivethree unbalanced datasetsid=SecondEdit], and train [d=FirstEdit]the correspondinga reward function model designed to optimize fine-tuning policy. Then, we train a reinforcement learning fine-tuning classification model and optimize its policy to maximize the cumulative expected reward. Finally, we guide the model to prioritize minority category knowledge and incorporate distribution distance constraints, which are derived from the disparities between the [d=SecondEdit]pre-trainedpretrained model and the fine-tuning model to dynamically adjust the fine-tuning classifier model. Experimental results on [d=FirstEdit]fivethree reprocessed unbalanced image datasets demonstrate that our RLF-UC method provides comparable or better classification and generalization capabilities than other baselines.},
  archive      = {J_ASOC},
  author       = {Jin-Qiang Wang and Lan Guo and Yuanbo Jiang and Shengjie Zhang and Qingguo Zhou},
  doi          = {10.1016/j.asoc.2024.111841},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111841},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving unbalanced image classification through fine-tuning method of reinforcement learning},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RCFI-net: A reliable correspondences evaluation and feature
interaction network for fast and accurate point cloud registration.
<em>ASOC</em>, <em>163</em>, 111839. (<a
href="https://doi.org/10.1016/j.asoc.2024.111839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud registration is a crucial task in computer vision, robotics, and safeguarding cultural artifacts in the digital realm. However, maintaining a balance between efficiency and accuracy during the 3D point cloud registration process continues to be a challenge. To solve this issue, a R eliable C orrespondences Evaluation and F eature I nteraction Network called RCFI-Net has been proposed for fast and precise registration. Our model use a two-stage approach to improve registration efficiency. In the first stage, a Transformer with position encoding network is employed to reinforce point features. Overlapping masks are learned based on the attention mechanism to identify overlapping areas, followed by the sampling of interest points that have high scores in these areas to accelerate registration. In the second stage, non-distinguishing points are eliminated, and a triangulated descriptor is introduced to further differentiate inliers and outliers and find reliable correspondences. Our proposed method has been evaluated on both public datasets and real Terracotta Warriors data, with results showing that it outperforms traditional and feature-learning methods in terms of accuracy, efficiency, and robustness.},
  archive      = {J_ASOC},
  author       = {Haibo Zhang and Linqi Hai and Xu Wang and Xizhi Wang and Mingquan Zhou},
  doi          = {10.1016/j.asoc.2024.111839},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111839},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RCFI-net: A reliable correspondences evaluation and feature interaction network for fast and accurate point cloud registration},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification of malware for security improvement in IoT
using heuristic aided adaptive multi-scale and dilated ResneXt with
gated recurrent unit. <em>ASOC</em>, <em>163</em>, 111838. (<a
href="https://doi.org/10.1016/j.asoc.2024.111838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of malware in the Internet of Things (IoT) realm exploits sensitive IoT devices that lead to extensive malicious attacks that pose a significant danger to the integrity of the Internet ecosystem. Addressing this threat effectively requires a robust system for classifying and attributing IoT malware, serving as vital initial steps toward implementing countermeasures for attack prevention and mitigation. So, in this work, a new malware classification technique using deep learning is developed for solving different kinds of malware in IoT. The required images for the malware classification process are gathered from online databases and given to the developed Adaptive Multi-scale and Dilated ResneXt with Gated Recurrent Unit (AMDR-GRU)-based malware classification process. Here, the last layer of the ResneXt is modified with GRU, and several parameters are optimized in the suggested AMDR-GRU model with the support of the designed Intensified Random Parameter-based Chameleon Swarm Algorithm (IRPCSA) to enhance the classification performance. Finally, the developed AMDR-GRU model offered the classified outcome and the obtained classification results are compared with different existing malware classification models to prove the effectiveness of the developed model over others. The developed model offered 99.59 % of precision. The result proved that the developed model is used to classify different malware without reverse engineering, binary code analysis, and feature engineering. Moreover, by effectively classifying malware, this technique can contribute to enhancing the security of IoT devices, protecting them from potential threats and vulnerabilities. This has significant implications for ensuring the privacy, integrity, and overall safety of IoT systems.},
  archive      = {J_ASOC},
  author       = {J. Jagadeesan and S. Nandhini and B. Sathiyaprasad},
  doi          = {10.1016/j.asoc.2024.111838},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111838},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification of malware for security improvement in IoT using heuristic aided adaptive multi-scale and dilated ResneXt with gated recurrent unit},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Slime mould algorithm with mechanism of leadership and
self-phagocytosis for multilevel thresholding of color image.
<em>ASOC</em>, <em>163</em>, 111836. (<a
href="https://doi.org/10.1016/j.asoc.2024.111836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Threshold based segmentation method is widely used because of its simplicity, high efficiency and easy implementation. However, as the number of thresholds increases, the algorithm&#39;s search space also increases, increasing computational complexity, which reduces computational efficiency and segmentation accuracy. Additionally, the existing multi-threshold (MT) segmentation methods still struggle with slow convergence speed and low solution accuracy. The optimum threshold for the image is determined in this study using the Slime Mould Algorithm with the Mechanism of Leadership and Self-phagocytosis (SMA-MLS). Firstly, the initial population production method based on Logistic - Tent mapping is introduced to enhance the diversity of algorithm solutions. Secondly, the position update formula with leadership mechanism is proposed to improve the convergence speed and accuracy. In addition, the adaptive combined mutation mechanism balances the exploration and exploitation ability. Finally, the self-phagocytosis mechanism maintains population diversity and heritability. A series of benchmark test suites are employed to evaluate the performance of the algorithm. Experimental results demonstrate that SMA-MLS does show good performance. It is superior in terms of convergence accuracy, convergence speed and stability. In the MT segmentation experiments, by using Kapur as the objective function and working with color images, SMA-MLS proved to be effective in the MT segmentation problem of images. Moreover, SMA-MLS will be applied to mechanical parameter optimization, neural network and other fields in future research.},
  archive      = {J_ASOC},
  author       = {Jinling Bei and Jiquan Wang and Haohao Song and Hualong Liu},
  doi          = {10.1016/j.asoc.2024.111836},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111836},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Slime mould algorithm with mechanism of leadership and self-phagocytosis for multilevel thresholding of color image},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristic-based cost-effective predictive modeling for
DevOps project success. <em>ASOC</em>, <em>163</em>, 111834. (<a
href="https://doi.org/10.1016/j.asoc.2024.111834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the decade, DevOps practices have gained popularity within software development organizations for managing the dynamic behavior of system development. Implementing DevOps introduces risks and challenges that increase the difficulties in software development activities, which consequently leads to project failures. This study presents a probability-based predictive model to estimate the success or failure of DevOps projects based on the 13 most significant features identified from literature and data gathered from the DevOps practitioners using a survey questionnaire. The Naïve Bayes Classifier and Logistic Regression (LR) models allied with Grey Wolf Optimization (GWO) have been used to measure the efficiency with the cost of implementing DevOps practices. The results of the study highlighted that NBC with GWO increased the success probability from 0.4954 to 0.9971, with the cost rising from 0.2577 to 0.5000. Similarly, LR with GWO also presented an increase in success probability from 0.2880 to 0.9839, along with an increase in cost from 0.2423 to 0.3558. In conclusion, the developed prediction model based on identified features could help DevOps software development practitioners to implement DevOps projects cost-effectively and successfully.},
  archive      = {J_ASOC},
  author       = {Ankur Kumar and Mohammad Nadeem and Mohammad Shameem},
  doi          = {10.1016/j.asoc.2024.111834},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111834},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic-based cost-effective predictive modeling for DevOps project success},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-population auxiliary multiobjective coevolutionary
algorithm for constrained multiobjective optimization problems.
<em>ASOC</em>, <em>163</em>, 111827. (<a
href="https://doi.org/10.1016/j.asoc.2024.111827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key to solving constrained multiobjective optimization problems (CMOPs) lies in maintaining the feasibility, convergence, and diversity of the population. In recent years, various constraint handling techniques (CHTs) and strategies have been proposed to enhance the performance of constrained multiobjective evolutionary algorithms (CMOEAs). However, most of these algorithms face difficulties in dealing with problems that have large infeasible regions and discontinuous small feasible regions, as they have trouble crossing large infeasible regions while simultaneously maintaining the convergence and diversity of the population. To tackle this issue, this paper proposes a dual-population auxiliary coevolutionary algorithm with an enhanced operator, denoted as DAEAEO. Auxiliary population 1 employs an improved ϵ ϵ -constraint handling technique to provide high-quality feasible solutions for the main population. Auxiliary population 2 adopts the non-dominated sorting method to provide favorable objective information for the main population to help it cross the infeasible region. In addition, to further improve diversity, each population adopts an enhanced operator and a genetic operator to generate offspring, respectively. Finally, knowledge transfer between offspring is realized. Compared to six state-of-the-art CMOEAs on DASCMOPs, LIR-CMOPs, DOC test suites, and two real-world problems, the proposed DAEAEO achieved superior performance, especially for CMOPs with large infeasible regions and discontinuous small feasible regions.},
  archive      = {J_ASOC},
  author       = {Zhao He and Hui Liu},
  doi          = {10.1016/j.asoc.2024.111827},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111827},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-population auxiliary multiobjective coevolutionary algorithm for constrained multiobjective optimization problems},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acute leukemia prediction and classification using
convolutional neural network and generative adversarial network.
<em>ASOC</em>, <em>163</em>, 111819. (<a
href="https://doi.org/10.1016/j.asoc.2024.111819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute Leukemia (AL) is a type of cancer that affects the blood cells and can be fatal if not detected and treated early. Flow cytometry is a key means of detecting AL, but requires manual processing by trained professionals, applying the manual gating method to huge amounts of raw data, making it time-consuming and labor-intensive. This study uses the deep learning Convolutional Neural Network (CNN) method for classification prediction based on two-dimensional graph for the parameter combinations of flow cytometry data to differentiate AL as normal, Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL) three categories. This approach normalizes raw data, removes outliers, and further augments data using Generative Adversarial Networks (GAN) to generate two-dimensional scattergrams of different parameter combinations generated by four methods. We then assess post-training classification prediction accuracy to identify the most effective preprocessing method and parameter combination. Experimental results obtain accuracy rates of 73–86 %. In the two-dimensional scatter plot of different parameter combinations generated by the four methods, the combination of CD3 and CD7 cell population parameters had the best classification and prediction accuracy, with image classification and identification reaching a high of 86 % through the use of GAN-based data augmentation. Advances in medical testing technology has reduced analysis times and increased data throughput. At the same time, artificial intelligence techniques are increasingly used for analysis and detection, performing large numbers of repetitive actions to reduce the potential for subjective assessment error from manual analysis, thus allowing for the earlier detection and treatment of disease.},
  archive      = {J_ASOC},
  author       = {Jiunn-Woei Lian and Chi-Hung Wei and Mu-Yen Chen and Ching-Chan Lin},
  doi          = {10.1016/j.asoc.2024.111819},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111819},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Acute leukemia prediction and classification using convolutional neural network and generative adversarial network},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intuitionistic fuzzy twin proximal SVM with fuzzy hyperplane
and its application in EEG signal classification. <em>ASOC</em>,
<em>163</em>, 111816. (<a
href="https://doi.org/10.1016/j.asoc.2024.111816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twin support vector machine (TSVM) is a contemporary machine learning technique to tackle classification and regression problems. However, TSVM lacks the ability to differentiate between support vectors and noises since it neglects the positional information of input data samples, and hence it is sensitive to noises. Additionally, it fails to consider uncertainties associated with the data, which reduces its capacity for generalization. To address these drawbacks, we propose a novel fuzzy hyperplane based intuitionistic fuzzy twin proximal support vector machine. The first significant feature of the proposed approach is that it gives an intuitionistic fuzzy number based on the relevance to each data vector. This efficiently reduces the impact of noise and outliers by leveraging the local neighborhood information within the data points by incorporating membership and non-membership weights. Secondly, all the parameters present in the model are fuzzy variables, including the offset term and the elements of the normal vector. The suggested fuzzy hyperplane successfully reflects the inherent ambiguity prevalent in real-world categorization problems by reflecting vagueness in the input data through the use of fuzzy variables. The model’s efficiency is enhanced by solving two systems of linear equations to obtain two non-parallel classifiers rather than solving two quadratic programming problems as in standard TSVM. Utilizing non-linear kernel functions within the feature space enables the method to effectively identify complex patterns or non-linear relationships within the datasets. In order to demonstrate the effectiveness of the suggested approach, extensive computer experiments have been conducted on a set of eighteen benchmark datasets with both linear and non-linear kernels. In addition, rigorous statistical analysis, including Friedman and post-hoc Nemenyi tests, have been employed to assess the significance of the observed performance differences. Furthermore, we also performed numerical experiments utilizing linear, Gaussian, and polynomial kernels to classify electroencephalogram (EEG) signals. The outcomes of the experiment are analyzed in terms of average accuracy, processing time, and F F -measure. The results demonstrate that the proposed method outperforms existing methods and achieves better generalization.},
  archive      = {J_ASOC},
  author       = {Yash Arora and S.K. Gupta},
  doi          = {10.1016/j.asoc.2024.111816},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111816},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intuitionistic fuzzy twin proximal SVM with fuzzy hyperplane and its application in EEG signal classification},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient and lightweight off-policy actor–critic
reinforcement learning framework. <em>ASOC</em>, <em>163</em>, 111814.
(<a href="https://doi.org/10.1016/j.asoc.2024.111814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of current off-policy actor–critic methods, the state–action pairs in an experience replay buffer (called historical behaviors) cannot be used to improve the policy, and the target network and the clipped double Q-learning techniques need to be used to evaluate the policy. The framework limits the policy learning capability in complex environments, and needs to maintain four critic networks. As a result, we propose an efficient and lightweight off-policy actor–critic (EL-AC) framework. In the policy improvement, an efficient off-policy likelihood ratio policy gradient algorithm with historical behaviors reusing (PG-HBR) is proposed, which promotes the agent to learn an approximately optimal policy by using the historical behaviors. Moreover, a theoretically interpretable universal critic network is designed. It can approximate the action-value and the state-value functions simultaneously, so as to obtain the advantage function in PG-HBR. In the policy evaluation, we develop the algorithms of low-pass filtering for target state-values and adaptive controlling algorithm for overestimation bias, which can evaluate the policy efficiently and accurately using only one universal critic network. Extensive evaluation results indicate that EL-AC outperforms the state-of-the-art off-policy actor–critic methods in terms of approximately optimal policy learning and neural network storage space occupation, and it is more suitable for policy learning in complex environments.},
  archive      = {J_ASOC},
  author       = {Huaqing Zhang and Hongbin Ma and Xiaofei Zhang and Bemnet Wondimagegnehu Mersha and Li Wang and Ying Jin},
  doi          = {10.1016/j.asoc.2024.111814},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111814},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient and lightweight off-policy actor–critic reinforcement learning framework},
  volume       = {163},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reinforcement learning based fractional fuzzy controller for
photovoltaic systems. <em>ASOC</em>, <em>162</em>, 111878. (<a
href="https://doi.org/10.1016/j.asoc.2024.111878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an innovative artificial nonlinear control technique for photovoltaic (PV) systems. A boost converter extracts maximum power from the PV system under different weather conditions. A fuzzy logic maximum power point (MPP) finding algorithm is used to determine the reference voltage of the PV system under various atmospheric conditions. It is assumed that the nonlinear dynamic model, parameters, and uncertainties of the converter are unknown. To estimate a precise model that contains nonlinearities, model uncertainties, and disturbances of the converter, proximal policy optimization (PPO) with a special reward function is applied. PPO is optimal in policy learning and sampling, fast to implement, and is a simple reinforcement learning (RL) algorithm. Then, estimations of dynamics and uncertainties are used to enhance the robustness of the suggested fractional order sliding mode controller. Furthermore, the controller coefficients are optimally adjusted by the PPO algorithm. The Lyapunov theorem is used for proving the control system&#39;s closed-loop stability. Also, the proposed methods can be generalized and used for other control systems. The simulation is conducted by MATLAB to evaluate the suggested controller&#39;s performance effectiveness. Robustness against the presence of parameter uncertainty is investigated. Moreover, the suggested technique is also compared with sliding mode controller.},
  archive      = {J_ASOC},
  author       = {Ali Sharifi and Hadi Delavari},
  doi          = {10.1016/j.asoc.2024.111878},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111878},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcement learning based fractional fuzzy controller for photovoltaic systems},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-strategy surrogate-assisted social learning particle
swarm optimization for expensive optimization and applications.
<em>ASOC</em>, <em>162</em>, 111876. (<a
href="https://doi.org/10.1016/j.asoc.2024.111876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary algorithms (EAs) require extensive fitness evaluations, which constitutes a barrier to solving computationally complex problems. In contrast, surrogate-assisted evolutionary algorithms (SAEAs) have the potential to solve complex expensive optimization problems. This paper proposes a surrogate-assisted social learning particle swarm optimization (SASLPSO) to handle expensive optimization problems. An adaptive local surrogate (ALS) strategy introduced in SASLPSO is introduced to accurately fit the landscape near the global optimum. ALS consists of two layers of surrogate, the part of the particles closest to the optimal particle and the closest to the optimal particle among the particles that have been eliminated historically. The proposed SASLPSO effectively combines global surrogate (GS) and adaptive local surrogate (ALS) to balance global exploration and local exploitation. Furthermore, a novel random group-based pre-screening (RGBPS) strategy is proposed to screen promising particles for real function evaluation. The proposed SASLPSO is compared with four other state-of-the-art SAEAs on 30D, 50D, and 100D benchmark functions. In addition, the significance of the SASLPSO algorithm was also verified using the Wilcoxon rank test. The test results on the benchmark function show that the SASLPSO algorithm performs better than other comparison algorithms, especially when dealing with high-dimensional benchmark functions. To further validate the effectiveness of the SASLPSO algorithm in solving expensive optimization problems, it was also applied to feature selection problems and real-world engineering problems. In addition, in the real application of node deployment in 3D wireless sensor networks, the highest coverage rate of the SASLPSO algorithm can reach 99.96%, confirming its performance advantages in solving real application problems. Finally, in the application of network intrusion detection, SASLPSO has shown more advantages in multiple metrics, proving its versatility.},
  archive      = {J_ASOC},
  author       = {Shu-Chuan Chu and Xu Yuan and Jeng-Shyang Pan and Bor-Shyh Lin and Zne-Jung Lee},
  doi          = {10.1016/j.asoc.2024.111876},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111876},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-strategy surrogate-assisted social learning particle swarm optimization for expensive optimization and applications},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous pbest-guided comprehensive learning particle
swarm optimization. <em>ASOC</em>, <em>162</em>, 111874. (<a
href="https://doi.org/10.1016/j.asoc.2024.111874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The balance between exploration and exploitation in the particle swarm optimization (PSO) algorithm is often discussed but has not been well solved. To further improve the capabilities of exploration and exploitation in the search space, the aim of this study is to focus on a novel comprehensive learning strategy for the PSO algorithm, which is named the heterogeneous pbest-guided comprehensive learning particle swarm optimizer (HPBPSO). In this algorithm, the population is divided into two subpopulations. In the exploration subpopulation, a pbest-guided mechanism is designed for obtaining better particles’ personal best information. Meanwhile, the non-elite personal best particles learn from elite personal best particles by using dynamic crossover strategy. In the exploitation subpopulation, in order to fully leverage the benefits of learning from high-quality individuals, the learning process is divided into two distinct categories: elite individual learning and intragroup learning. Furthermore, intergroup perturbation is designed to accelerate convergence performance. The experiments on classical functions are firstly used to verify the effectiveness and mutuality of the proposed strategies. Moreover, the performance of the proposed algorithm is evaluated on two well-known benchmarks and medical image segmentation problem. According to the statistical results, the HPBPSO algorithm is not only better than the other existing state-of-the-art PSO variants and metaheuristic evolutionary algorithms, but also applicable to real-world optimization problems.},
  archive      = {J_ASOC},
  author       = {Xiaoding Meng and Hecheng Li},
  doi          = {10.1016/j.asoc.2024.111874},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111874},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heterogeneous pbest-guided comprehensive learning particle swarm optimization},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility as a service: An exploration of exact and heuristic
algorithms for a new multi-modal multi-objective journey planning
problem. <em>ASOC</em>, <em>162</em>, 111871. (<a
href="https://doi.org/10.1016/j.asoc.2024.111871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility as a Service (MaaS) is a term coined for the development and implementation of multi-modal trip planner recommendation systems. Multi-modal journeys can include public transport, private transport and hire-able e-scooters which can beneficially augment public transport journeys. This study proposes and tackles a new and more general multi-modal multi-objective journey planning problem than considered previously. Our aim is to generate Pareto sets of multi-modal journeys which minimise the objectives of cost, travel time, inconvenience, CO 2 emissions, and calorie expenditure, as well as find journeys that optimally balance the trade-offs between them. Commuters can then choose between green, cheap, fast, car-free or convenient journeys. This work proposes implicit enumeration and heuristic algorithms which are analysed in terms of their theoretical time complexity, empirically in terms of solution time and optimality gap using a series of Manhattan and random structure transport networks. We show that our algorithms can generate solutions of equal quality to RAPTOR—a prominent existing journey planning algorithm. For this problem we reveal that the number of network nodes is a huge computational bottleneck, leading to the suggestion that future research can benefit from limiting the number of network nodes that are considered as transfer points. While our heuristic can generate good quality Pareto sets quicker than the enumeration procedure for the largest instances considered, we show that the enumeration procedure with pruning rules can still be the most effective strategy for this particular problem.},
  archive      = {J_ASOC},
  author       = {Christopher Bayliss and Djamila Ouelhadj},
  doi          = {10.1016/j.asoc.2024.111871},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111871},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mobility as a service: An exploration of exact and heuristic algorithms for a new multi-modal multi-objective journey planning problem},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). T-spherical fuzzy TOPSIS method based on distance measures
and hamacher heronian mean averaging aggregation operators and its
application to waste management. <em>ASOC</em>, <em>162</em>, 111868.
(<a href="https://doi.org/10.1016/j.asoc.2024.111868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {T-Spherical fuzzy sets provide a robust framework for handling uncertainty in real-world decision-making. This paper addresses the construction of T-Spherical fuzzy Heronian mean averaging aggregation operators using novel Hamacher operations. The study mathematically verifies the properties of these operators and develops new distance measures such as Hamming, Euclidean and Jaccard for quantifying dissimilarity between T-Spherical fuzzy numbers. Furthermore, the TOPSIS method is implemented using the proposed operators and distance measures to solve a practical decision-making problem in selecting the best transportation firm for medical waste disposal. A sensitivity analysis is made to assesses the efficacy of the proposed operators, while a comparison study with existing methods is exhibited to establish their efficiency.},
  archive      = {J_ASOC},
  author       = {A. Thilagavathy and S. Mohanaselvi},
  doi          = {10.1016/j.asoc.2024.111868},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111868},
  shortjournal = {Appl. Soft. Comput.},
  title        = {T-spherical fuzzy TOPSIS method based on distance measures and hamacher heronian mean averaging aggregation operators and its application to waste management},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based restoration of nonlinear motion blurred
images for plant classification using multi-spectral images.
<em>ASOC</em>, <em>162</em>, 111866. (<a
href="https://doi.org/10.1016/j.asoc.2024.111866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There have been various plant image-based studies for segmentation, deblurring, super-resolution reconstruction, and classification. However, nonlinear motion blur in thermal images was not considered in the existing studies on plant classification. Nonlinear motion blur occurs in images due to camera or plant movements, and it causes the degradation of plant classification accuracy. Moreover, nonlinear motion blur in images gets worse when both camera and plant movements occur simultaneously. In this case, it becomes difficult to recognize plants, and the performance of plant image classification becomes very low. Therefore, to reduce the nonlinear motion blur, a thermal and visible light plant images-based deblurring network (TVPD-Net) is proposed in this study. In addition, a thermal and visible light plant images-based classification network (TVPC-Net) is also proposed to improve the plant classification performance on deblurred images. Experimental results revealed that the proposed TVPD-Net achieved 21.21 and 22.53 of the peak signal-to-noise ratio (PSNR), and 0.726 and 0.737 of the structural similarity index measure (SSIM) on both visible light and thermal plant image datasets which were self-collected, respectively. Moreover, the proposed TVPC-Net with deblurred images by TVPD-Net achieved 92.52 % (top-1 accuracy) and 87.73 % (harmonic mean of precision and recall (F1-score)). In addition, the experimental results on an open dataset named Hyperspectral Flower Dataset (HFD100) revealed that the proposed plant classification method achieved 90.94 % of top-1 accuracy and 86.21 % of F1-score. The accuracies of the proposed methods are greater than those of the state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Ganbayar Batchuluun and Jin Seong Hong and Seung Gu Kim and Jung Soo Kim and Kang Ryoung Park},
  doi          = {10.1016/j.asoc.2024.111866},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111866},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based restoration of nonlinear motion blurred images for plant classification using multi-spectral images},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A short-term power load forecasting system based on data
decomposition, deep learning and weighted linear error correction with
feedback mechanism. <em>ASOC</em>, <em>162</em>, 111863. (<a
href="https://doi.org/10.1016/j.asoc.2024.111863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power load forecasting enables Independent System Operators (ISOs) to precisely quantify the demand patterns of users and achieve efficient management of the smart grid. However, with the increasing variety of power consumption patterns, the power load data displays increasingly irregular characteristics, which posing great challenges for accurate load forecasting. In order to solve above problem, a novel power load forecasting system is proposed based on data denoising, customized deep learning and weighted linear error correction. Specifically, we first proposed an improved optimization algorithm IGWO-JAYA which enhanced the Grey Wolf Optimizer ( GWO ) algorithm by using Halton low-discrepancy sequence and the mechanism of JAYA algorithm. In data denoising, the proposed optimizer was employed to optimize the Variational Mode Decomposition ( VMD ), enabling data-driven intelligent denoising. The customized deep learning framework contained multi-layer Convolution Neural Network ( CNN ), Bi-directional Long Short-Term Memory ( Bi-LSTM ) and Multi-Head Attention mechanism . The effective integration of these layers can significantly improve the capacity for nonlinear fitting of deep learning. In weighted linear error correction, the IGWO-JAYA algorithm was employed to determine the appropriate weight for point forecasting values and residual forecasting values. By weighting them, the forecasting precision has been further enhanced. To verify the forecasting ability of the system, we conducted experiments on power load datasets from four states in Australia and found that it has the best performance compared with all rivals. In the discussion, we demonstrated the convergence efficiency of the IGWO-JAYA algorithm by CEC test function.},
  archive      = {J_ASOC},
  author       = {Zhaochen Dong and Zhirui Tian and Shuang Lv},
  doi          = {10.1016/j.asoc.2024.111863},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111863},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A short-term power load forecasting system based on data decomposition, deep learning and weighted linear error correction with feedback mechanism},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A PSO-algorithm-based dual consensus method for large-scale
group decision making and its application in medical imaging equipment
purchasing. <em>ASOC</em>, <em>162</em>, 111862. (<a
href="https://doi.org/10.1016/j.asoc.2024.111862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of social activities requires an expanding number of people to be involved in decision-making, so that the large-scale multi-attribute group decision-making problems have gained widespread attention. After overviewing the researches of large-scale multi-attribute group decision-making methods, we have found that: The existing methods only consider consensus based on evaluation information of alternatives, while ignoring the consensus on importance of attributes. Thus, in order to tackle the issues and describe the hesitancy of decision makers in the decision-making process, this paper proposes a novel dual consensus method in large-scale multi-attribute group decision making under hesitant fuzzy linguistic environment. In the consensus reaching process, the method considers not only the consensus on the evaluation information of alternatives, but also the consensus on the importance of attributes, where both consensus reaching processes are implemented by the particle swarm optimization algorithm. The subjective weights of attributes are derived from the consensus reaching process of preference matrices, and the objective weights of attributes are obtained from the consensus reaching process of decision matrices, so as to acquire the comprehensive weights. After that, the overall ranking of alternatives can be obtained by TODIM. Finally, the proposed method is applied to a case study of medical imaging equipment purchasing decision-making, and the comprehensive analysis is provided to clarify advantages of the proposed method.},
  archive      = {J_ASOC},
  author       = {Tong Wu and Zeshui Xu and Yuanhang Zheng},
  doi          = {10.1016/j.asoc.2024.111862},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111862},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A PSO-algorithm-based dual consensus method for large-scale group decision making and its application in medical imaging equipment purchasing},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tracklet-switch and imperceivable adversarial attack against
pedestrian multi-object tracking trackers. <em>ASOC</em>, <em>162</em>,
111860. (<a href="https://doi.org/10.1016/j.asoc.2024.111860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though achieving aggressive progress, there are only a few explorations on the robustness of Multi-Object Tracking (MOT) trackers. Most of the existing MOT research focuses on pedestrian tracking, yet there is little research on its adversarial attack, hindering the robustness improvement study on these systems. It is also challenging to attack these systems since various mature association algorithms have been designed to be robust against errors during the tracking. In this work, we analyze the vulnerability of typical pedestrian MOT trackers and propose a novel adversarial attack method called Tracklet-Switch Attack (TraSA) against the complete tracking pipeline. By perturbing very few frames, the proposed TraSA can spoof the advanced deep pedestrian trackers ( i.e ., FairMOT and ByteTrack), causing them to fail to track the targets in subsequent frames. Specifically, TraSA learns an effective perturbation generator to make the tracker confuse intersecting trajectories by attacking very few frames, then keeps the error across frames to the end of the sequences without any more perturbation. In our method, two new losses are proposed: PushPull works on the re-identification (re-ID) branch to perturb two approaching pedestrian detection boxes, while CenterLeaping works on the detection branch to perturb pedestrian features to make their trajectories switch. We conduct extensive experiments on three typical MOT-Challenge datasets and two popular trackers to show the superiority of our method. TraSA achieves 91.58%, 91.05% and 95.65% average attack success rates on 2DMOT15, MOT17, and MOT20, respectively, outperforming the runner-up by 2.64%, 14.46% and 22.67%, respectively. Meanwhile, we use a smaller number of frames, 4.11 on the average, over all datasets, while other methods use at least 6.74 average number of frames. Moreover, our method yields much lower L 2 L2 distance.},
  archive      = {J_ASOC},
  author       = {Delv Lin and Qi Chen and Chengyu Zhou and Kun He},
  doi          = {10.1016/j.asoc.2024.111860},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111860},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tracklet-switch and imperceivable adversarial attack against pedestrian multi-object tracking trackers},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient flexible voxel-based two-stage network for 3D
object detection in autonomous driving. <em>ASOC</em>, <em>162</em>,
111856. (<a href="https://doi.org/10.1016/j.asoc.2024.111856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D object detection from the LiDAR point cloud plays an important role in autonomous driving. It is difficult to balance inference speed and detection accuracy when performing 3D point cloud object detection due to the large size of point cloud data and its unstructured storage, which makes it difficult to represent its features. To address the challenge, we propose a two-stage point cloud object detector, AFV-RCNN. In stage-1, the attention flexible voxel feature encoding layer is introduced, which utilizes flexible voxels to enhance feature encoding speed and focuses on foreground points to be detected through voxel attention. In stage-2, the multi-level and grid-based multi-scale RoI (Region of Interest) feature fusion module is designed. It directly extracts complete 3D structures from 3D region proposals and focuses on both local and global features through multi-scale partitioning. In the training stage, GHM-C Loss is applied to address the challenges associated with imbalanced target categories and the imbalance between difficult and easy samples in the classification task. We evaluate the model on the public KITTI Dataset and Waymo Open Dataset. The mAP in KITTI for 3D detection is 73.41% and inference on a single GPU reaches 30.0 fps. Compared with other state-of-the-art methods, AFV-RCNN achieves both the inference speed of a single-stage detector and the detection accuracy of a two-stage detector. It ensures higher detection accuracy while efficiently processing the point cloud.},
  archive      = {J_ASOC},
  author       = {Fanyue Sun and Guoxiang Tong and Yan Song},
  doi          = {10.1016/j.asoc.2024.111856},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111856},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient flexible voxel-based two-stage network for 3D object detection in autonomous driving},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning model for FaceSwap and face-reenactment
deepfakes detection. <em>ASOC</em>, <em>162</em>, 111854. (<a
href="https://doi.org/10.1016/j.asoc.2024.111854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the higher availability of multimedia content on social websites, together with lightweight deep learning (DL) empowered tools like Generative Adversarial Networks (GANs) has caused the generation of realistic deepfakes. Such fabricated data has the potential to spread disinformation, revenge porn, initiate monetary scams, and can result in adverse immoral and illegal societal issues, etc. Hence, the accurate identification of deepfakes is mandatory to discriminate between real and manipulated content. In this work, we have presented a DL-based approach namely a unified network for FaceSwap (FS) and Face-Reenactment (FR) Deepfakes Detection (AUFF-Net). More clearly, both the spatial and temporal information from the video samples are used to detect two types of visual manipulations i.e., FS and FR. For this reason, a novel DL framework namely the Inception-Swish-ResNet-v2 model is introduced as a feature extractor for computing the information at the spatial level. While the Bi-LSTM model is utilized to measure the temporal information. Additionally, 3 dense layers are included at the last of the model structure to suggest a discriminative group of the feature vector We performed extensive experimentation on a challenging dataset namely the FaceForensic++, and attainede average accuracy values of 99.21 %, and 98.32 % for FS, or FR, respectively. Furthermore, we introduced an explainability module to show the reliable keypoints selection capability of our technique. Moreover, we have performed a cross-dataset evaluation to show the generalization power of our approach. Both the qualitative and quantitative results have confirmed the effectiveness of the suggested approach for visual manipulation categorization under the occurrence of various adversarial attacks.},
  archive      = {J_ASOC},
  author       = {Marriam Nawaz and Ali Javed and Aun Irtaza},
  doi          = {10.1016/j.asoc.2024.111854},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111854},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning model for FaceSwap and face-reenactment deepfakes detection},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The bi-long short-term memory based on multiscale and
mesoscale feature extraction for electric load forecasting.
<em>ASOC</em>, <em>162</em>, 111853. (<a
href="https://doi.org/10.1016/j.asoc.2024.111853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate power load prediction is beneficial to the efficient use of electric energy and the orderly development of power systems. Given the strong volatility and complexity of power load series, a hybrid load forecasting method based on multiscale and mesoscale information fusion, signal decomposition, model optimization, and bi-long-short-term memory (BiLSTM) is proposed. Firstly, the load sequence is analyzed on different time scales, and the extracted multi-scale information and mesoscale information are fused to improve the perception ability. Secondly, the empirical wavelet transform (EWT) with adaptive decomposition ability is used to decompose the sequence and extract the rich feature information. Thirdly, the complexity, volatility, and uncertainty of each mode component were analyzed, the data features were fully mined, and the feature fusion was carried out by the TOPISIS evaluation method. The BiLSTM model and the GWO-BiLSTM model are used to predict the low-frequency component and the high-frequency component, respectively. The optimization of Grey Wolf optimization (GWO) algorithm can improve the BiLSTM model&#39;s ability to learn long-term time series. Finally, the analysis of application examples shows that compared with various prediction models, the prediction error of mixed model EWT-SGEO-BiLSTM is the smallest, MAPE is as low as 1.07 %, and goodness of fit R 2 is 0.99 which verifies the accuracy and applicability of the intelligent model.},
  archive      = {J_ASOC},
  author       = {Guo-Feng Fan and Jin-Wei Li and Li-Ling Peng and Hsin-Pou Huang and Wei-Chiang Hong},
  doi          = {10.1016/j.asoc.2024.111853},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111853},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The bi-long short-term memory based on multiscale and mesoscale feature extraction for electric load forecasting},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LiDAR point cloud simplification algorithm with fuzzy
encoding-decoding mechanism. <em>ASOC</em>, <em>162</em>, 111852. (<a
href="https://doi.org/10.1016/j.asoc.2024.111852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth in the density of acquired point cloud data, point cloud processing tasks will face tremendous challenges. LiDAR point cloud simplification is a key phase in addressing this issue, which effectively promotes the development of LiDAR technology in many engineering fields. In this study, an innovative point cloud simplification algorithm with the fuzzy encoding-decoding mechanism is proposed. In the developed scheme, an approach for curvature estimation is first designed on the basis of the k-neighbor searching and principal component analysis. Then, a collection of feature point sets is set up with the ordered curvatures. Subsequently, a Fuzzy C-Means clustering based encoding mechanism is employed to capture the level point cloud structures in depth and establish a reasonable and streamlined strategy for point clouds. Each feature point set and non-feature point set are encoded into a prototype matrix and a partition (membership) matrix. The membership degree of each feature point to its prototype becomes the basis for the simplification strategy. Finally, the simplification result of the point cloud is formed through merging the simplification results of all subsets. The method proposed in this study effectively preserves the point cloud features and ensures a uniform distribution of the simplified point cloud. A comparative analysis of the point cloud simplification is conducted. The experimental results demonstrate that the developed algorithm outperformed other point cloud simplification algorithms.},
  archive      = {J_ASOC},
  author       = {Ao Hu and Kaijie Xu and Witold Pedrycz and Mengdao Xing},
  doi          = {10.1016/j.asoc.2024.111852},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111852},
  shortjournal = {Appl. Soft. Comput.},
  title        = {LiDAR point cloud simplification algorithm with fuzzy encoding-decoding mechanism},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Considering personalized individual semantics with ordinal
and cardinal consensus reaching processes via three-way decision and
regret theory. <em>ASOC</em>, <em>162</em>, 111851. (<a
href="https://doi.org/10.1016/j.asoc.2024.111851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The seamless integration of computerized methodologies into industrial engineering problem-solving is pivotal for optimizing efficiency. In the specific domain of multi-attribute group decision-making (MAGDM) with probabilistic linguistic term sets (PLTSs), these methodologies offer systematic approaches to consensus building, ensuring effective decision processes in intricate scenarios. Within the realm of PLTSs, the consensus-reaching process (CRP) for MAGDM is gaining prominence. This paper addresses this evolving area by proposing ordinal and cardinal CRPs within the framework of PLTSs, specifically incorporating the regret theory (RT) of three-way decisions (TWD). The paper introduces an initial distance formula under PLTSs, providing a complementary approach to assess similarity relations among decision-makers (DMs). To account for diverse semantics across DMs, personalized individual semantics (PIS) is integrated into the CRP, recognizing variations in DMs’ alternatives and attributes. To enhance realism, the paper introduces the concepts of individual alternative sets and individual attribute sets. Additionally, the paper integrates ordinal and cardinal consensus, establishing a dynamic feedback adjustment mechanism grounded in the principles of RT and TWD. The method’s reasonableness is validated through a real case study, and a comparative analysis with the existing methods underscores the superiority of the approach presented in this paper.},
  archive      = {J_ASOC},
  author       = {Yu Wang and Jianming Zhan and Chao Zhang and Muhammet Deveci},
  doi          = {10.1016/j.asoc.2024.111851},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111851},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Considering personalized individual semantics with ordinal and cardinal consensus reaching processes via three-way decision and regret theory},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy rough sets-based data-driven approach for
quantifying local and overall fuzzy relations between variables for
spatial data. <em>ASOC</em>, <em>162</em>, 111848. (<a
href="https://doi.org/10.1016/j.asoc.2024.111848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring the relationships between variables is a crucial component in comprehending geographical phenomena. Most existing methods ignore the vagueness hidden in spatial data when quantifying this relation, which may lead to a partial or even wrong understanding of geographical phenomena as vagueness is an intrinsic property of them. This paper uses fuzzy rough sets for quantifying local and overall variable relationships to address this limitation, relying on the consistent degree between variables. This approach uses a sliding window to scan the entire study area and build a local region for each object. The local variable relation is quantified using the local average membership degree to the positive region for each object during the scan. The overall variable relation in the whole study area is quantified using the median value of the local consistent degree between variables in every local region, and the entropy of the normalized local consistent degree is used to measure the corresponding spatial heterogeneity. The proposed method can detect and compare local and overall variable relations. Comparison experiments on five publicly accessible datasets demonstrate the effectiveness of the proposed method and show that it can reveal patterns missed by geographically weighted regression and geographical detectors, as it models rather than ignores vagueness uncertainty.},
  archive      = {J_ASOC},
  author       = {Hexiang Bai and Junhao Jing and Deyu Li and Yong Ge},
  doi          = {10.1016/j.asoc.2024.111848},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111848},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy rough sets-based data-driven approach for quantifying local and overall fuzzy relations between variables for spatial data},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural network accelerated-group african vulture
optimization algorithm for unit commitment considering uncertain wind
power. <em>ASOC</em>, <em>162</em>, 111845. (<a
href="https://doi.org/10.1016/j.asoc.2024.111845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unit commitment (UC) of power systems serves to plan out the starting and shutdown of units and the power generation of units for a future period of time. However, the UC problem for large-scale systems faces the problems of long solution time and insufficiently accurate solution. Uncertainty in wind power generation poses a great challenge in solving the unit combination problem. This work proposes a deep neural network accelerated-double layer optimization method (DNNA-DOM) to improve the solution efficiency, accuracy, and economy of UC. The outer layer of DNNA-DOM utilizes the proposed deep neural network accelerated-group African vulture optimization algorithm (DNNA-GAVOA) to optimize the on-off condition of conventional coal-fired power units. The inner layer of DNNA-DOM adopts quadratic programming to optimize the solution of the economic dispatch problem of load distribution. The GAVOA optimizes the simulated African vultures in four groups to obtain the optimal solution after diverse exploration, exploitation, and exploitation activities, with low complexity and high accuracy. This work first evaluates the performance of GAVOA by solving seven unimodal functions. Furthermore, the 10-unit simulation by incorporating wind power curves and related constraints is optimized through the DNNA-DOM. The results show that GAVOA outperforms the African vulture optimization algorithm (AVOA) and traditional metaheuristics like the particle swarm algorithm and gray wolf algorithm in terms of lower operating cost and optimization stability. The DNNA-DOM combined with DNNA-GAVOA in the outer layer results in an average daily cost reduction of $36.30 and a 45.1997 % speed increase compared to AVOA in the outer layer.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Wenyu Ding},
  doi          = {10.1016/j.asoc.2024.111845},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111845},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep neural network accelerated-group african vulture optimization algorithm for unit commitment considering uncertain wind power},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning for sim-to-real policy transfer
of VTOL-UAVs offshore docking operations. <em>ASOC</em>, <em>162</em>,
111843. (<a href="https://doi.org/10.1016/j.asoc.2024.111843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel Reinforcement Learning (RL) approach for sim-to-real policy transfer of Vertical Take-Off and Landing Unmanned Aerial Vehicle (VTOL-UAV). The proposed approach is designed for VTOL-UAV landing on offshore docking stations in maritime operations. VTOL-UAVs in maritime operations encounter limitations in their operational range, primarily stemming from constraints imposed by their battery capacity. The concept of autonomous landing on a charging platform presents an intriguing prospect for mitigating these limitations by facilitating battery charging and data transfer. However, current Deep Reinforcement Learning (DRL) methods exhibit drawbacks, including lengthy training times, and modest success rates. In this paper, we tackle these concerns comprehensively by decomposing the landing procedure into a sequence of more manageable but analogous tasks in terms of an approach phase and a landing phase. The proposed architecture utilizes a model-based control scheme for the approach phase, where the VTOL-UAV is approaching the offshore docking station. In the Landing phase, DRL agents were trained offline to learn the optimal policy to dock on the offshore station. The Joint North Sea Wave Project (JONSWAP) spectrum model has been employed to create a wave model for each episode, enhancing policy generalization for sim2real transfer. A set of DRL algorithms have been tested through numerical simulations including value-based agents and policy-based agents such as Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) respectively. The numerical experiments show that the PPO agent can learn complicated and efficient policies to land in uncertain environments, which in turn enhances the likelihood of successful sim-to-real transfer.},
  archive      = {J_ASOC},
  author       = {Ali M. Ali and Aryaman Gupta and Hashim A. Hashim},
  doi          = {10.1016/j.asoc.2024.111843},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111843},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning for sim-to-real policy transfer of VTOL-UAVs offshore docking operations},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Three-way decision based island harmony search algorithm
for robust flow-shop scheduling with uncertain processing times depicted
by big data. <em>ASOC</em>, <em>162</em>, 111842. (<a
href="https://doi.org/10.1016/j.asoc.2024.111842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses an uncertain two-machine permutation flow-shop scheduling problem (2PFSP) with total weighted tardiness and common due date. Uncertain processing times are described by a large set of discrete scenarios, which is a type of big data. The objective is to minimize the schedule performance under the worst-case scenario. Identifying the worst-case scenario for each evaluated schedule is quite time-consuming in the situation that the scenario set size is large so that the objective evaluation might be computationally expensive. To handle this difficulty, three-way decision is used to preprocess the large-size scenario set to get a reduced scenario set so that the concept of surrogate worst-case scenario is adopted. A hybrid harmony search algorithm of combining three-island framework and the scenario-based local search is developed to solve the discussed problem. Based on the single-scenario knowledge of 2PFSP, a problem-specific scenario-dependent neighborhood structure is constructed under the surrogate worst-case scenario. An extensive experiment was carried out. The computational results show that the application of surrogate worst-case scenario based on three-way decision is effective in reducing the time consuming while keeping schedule performance evaluation. Being compared to the worst-case scenario objective evaluation, for an example in the case of the middle bad-scenario ratio, the surrogate worst-case scenario objective evaluation made the solution algorithm save 12.95 % in average CPU time for all instances while the relative performance difference is only 1.809 % in average. Being compared to possible alternative algorithms derived from the state-of-the-art algorithms, the developed algorithm is advantageous for the addressed problems.},
  archive      = {J_ASOC},
  author       = {Bing Wang and Pengfei Zhang and Xiaozhi Wang and Quanke Pan},
  doi          = {10.1016/j.asoc.2024.111842},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111842},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decision based island harmony search algorithm for robust flow-shop scheduling with uncertain processing times depicted by big data},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-stage evolutionary algorithm assisted by
multi-archives for constrained multi-objective optimization.
<em>ASOC</em>, <em>162</em>, 111840. (<a
href="https://doi.org/10.1016/j.asoc.2024.111840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the widespread existence of constrained multi-objective optimization problems (CMOPs) in real life, many researchers start to research the constrained multi-objective evolutionary algorithms (CMOEAs). Therefore, a variety of CMOEAs have emerged. However, some of them still suffer from great difficulties when coping with CMOPs with complex feasible regions. To solve the issue, this article puts forward a two-stage evolutionary algorithm assisted by multi-archives for constrained multi-objective optimization, called MA-TSEA. In MA-TSEA, the evolution process is divided into two stages. In Stage 1, non-dominated solutions obtained by non-dominated sorting based on ( M + 1 ) (M+1) objectives ( i.e., M M objectives and the constraint violation degree ( C V CV ) ) are stored in a temporary archive and merged with the parent population to improve the exploration ability of population. Then, MA-TSEA drives the populations to evolve from diverse directions by a multi-objective evolutionary algorithm, while the feasible solutions are saved in a formal archive. Next, the formal archive is updated to improve convergence and diversity. In Stage 2, the main population and archive population cooperate to evolve towards the constrained Pareto front (CPF), where the formal archive and population of Stage 1 are respectively used as the main population and archive population. The experimental studies on five benchmark test suites and five real-world applications demonstrate the superiority of MA-TSEA over the other seven state-of-art CMOEAs.},
  archive      = {J_ASOC},
  author       = {Wenjuan Zhang and Jianchang Liu and Wei Zhang and Yuanchao Liu and Shubin Tan},
  doi          = {10.1016/j.asoc.2024.111840},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111840},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-stage evolutionary algorithm assisted by multi-archives for constrained multi-objective optimization},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep RegNet-150 architecture for single image super
resolution of real-time unpaired image data. <em>ASOC</em>,
<em>162</em>, 111837. (<a
href="https://doi.org/10.1016/j.asoc.2024.111837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single Image Super-Resolution (SISR) is a fundamental computer vision task aimed at enhancing the spatial resolution and quality of low-resolution images. In recent years, deep learning techniques have revolutionized the field of SISR, enabling remarkable advancements in image enhancement. Typically, the Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) based deep learning techniques are employed for the SISR. However, the existing CNN and GAN architectures lag in providing maximum performance in terms of SSIM and PSNR. In this paper, a regression deep learning architecture is designed for the SISR. The proposed Deep Regression Network with 150 (Deep RegNet-150) is designed as a U-net. The proposed Deep RegNet-150 includes up-sampling and down-sampling portions with Residual Channel Attention Block (RCAB) for SISR. The RCAB acts as the building block of image super resolution (SR) by including ReLU and Conv layer. The performance of the proposed technique is evaluated on different image datasets, including a text image dataset and an unpaired dataset. The performance in terms of SSIM and PSNR for 4X and 8X SR is evaluated. The proposed Deep RegNet-150 attained maximum PSNR / SSIM values of 43.99 / 0.989 and 42.73 / 0.987 for 4X and 8X SR of the benchmark dataset, respectively. The 4X and 8X SR of unpaired datasets attained 46.13 / 0.991 and 45.97 / 0.990 of PSNR /SSIM, respectively. Ultimately, it proves that the Deep RegNet-150 is an effective deep learning architecture for the SISR.},
  archive      = {J_ASOC},
  author       = {S. Karthick and N. Muthukumaran},
  doi          = {10.1016/j.asoc.2024.111837},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111837},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep RegNet-150 architecture for single image super resolution of real-time unpaired image data},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A context-ensembled refinement network for image
segmentation of coated fuel particles. <em>ASOC</em>, <em>162</em>,
111835. (<a href="https://doi.org/10.1016/j.asoc.2024.111835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thickness of the coating layer of coated fuel particles plays a vital role in the reliable operation of a high-temperature gas-cooled reactor. Coating thickness remains extremely challenging to measure it efficiently, accurately, and robotically. The existing methods struggle with poorly refined segmentation results and easy adhesion of boundaries, complicating the accurate image segmentation of coating layers, which is fundamental to achieving the measurement goal. To overcome these issues, we designed the Context-Ensembled Refinement Network. This network refines the segmentation results to accurately extract different coating regions, leveraging its context fusion module and boundary refinement module to effectively utilize multiscale contextual information and provide strong boundary refinement capability. Experimental results demonstrate that our proposed method outperforms recent state-of-the-art methods, achieving an mIoU value of 95.8 %, and holds promising potential for automatic measurement of coating thickness.},
  archive      = {J_ASOC},
  author       = {Zhaochuan Hu and Jie Zhang and Hang Zhang and Chao Jiang and Ning Chen and Zhiyuan Yang and Jian Liu},
  doi          = {10.1016/j.asoc.2024.111835},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111835},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A context-ensembled refinement network for image segmentation of coated fuel particles},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A picture fuzzy CIMAS-ARTASI model for website performance
analysis in human resource management. <em>ASOC</em>, <em>162</em>,
111826. (<a href="https://doi.org/10.1016/j.asoc.2024.111826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The central emphasis of human resource management resides in the precise delineation of job responsibilities, engaging in systematic inquiry to identify well-suited candidates, and discerningly electing the most qualified individual from the pool whose qualifications align with the job description. The selection of a well-suited candidate is dependent on the creation of an appropriate candidate pool. Digital platforms are preferred over traditional approaches to reach human resources. The main objective of this research is to identify the most suitable digital platform for posting job advertisements based on website performance. The multiple-attribute group decision-making approach is adopted in this research, considering both qualitative and quantitative criteria. A decision support system for website selection is developed, where expert importance levels are calculated based on picture fuzzy sets (PFS). The weight levels of criteria are determined using the introduced PFS-based criteria importance assessment (CIMAS) method. The ranking of websites is calculated using the proposed PFS-based alternative ranking technique based on adaptive standardized intervals (ARTASI) method. These methods are hybridized into the PFS-CIMAS-ARTASI model. Additionally, an algorithm for this hybrid model is developed. A case study is conducted to demonstrate the applicability of the PFS-CIMAS-ARTASI hybrid model for website performance calculations. Robustness tests based on various sensitivity analysis scenarios are performed. The research results indicate that PFS-CIMAS-ARTASI is both applicable and robust. Comprehensive managerial implications are presented and elaborated on.},
  archive      = {J_ASOC},
  author       = {Karahan Kara and Galip Cihan Yalçın and Esra Gökçen Kaygısız and Vladimir Simic and Ali Şahin Örnek and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2024.111826},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111826},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A picture fuzzy CIMAS-ARTASI model for website performance analysis in human resource management},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractional-order q-learning based on modal decomposition and
convolutional neural networks for voltage control of smart grids.
<em>ASOC</em>, <em>162</em>, 111825. (<a
href="https://doi.org/10.1016/j.asoc.2024.111825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To maintain uniformity in the time scale of the control system, a coordinated first-level voltage control (FVC) framework is proposed for the voltage controller of three-state energy (TSE) models. To ensure control accuracy, this study combines complementary ensemble empirical mode decomposition (CEEMD) with adaptive noise (CEEMDAN), fractional-order proportional-integral-derivative (FO-PID) with convolutional neural networks (CNNs), Q-learning as CFOQL. Firstly, the CEEMDAN decomposes the historical voltage deviation data. The decomposed intrinsic mode functions (IMFs) are converted into RGB images with 30×30 pixels to train the CNNs. The residual function (RF) is utilized to train Q-learning. The implemented IMFs on the trained CNNs output, can generate regulating commands to adjust the differential and integral coefficients of FO-PID control. Similarly, the implemented RF on the trained Q-learning output can generate regulating commands. Then, the trained CNNs output the differential and integral coefficients for FO-PID control; Q-learning generates the regulating commands based on the RF. Finally, the regulation commands generated by Q-learning are combined with the regulation commands generated by the FO-PID as the total regulation command. The proposed controller can effectively reduce the voltage deviations of smart grids (SGs) with higher control accuracy. Significantly, in the IEEE 2736-bus system, the error integration criterion indices obtained by the proposed controller are at least 7.53 %, 1.89 %, 13.80 %, and 14.61 % less than those of the proportional-integral-derivative (PID) algorithm, FO-PID algorithm, R(λ) algorithm, and Q(λ) algorithm, respectively.},
  archive      = {J_ASOC},
  author       = {Linfei Yin and Nan Mo},
  doi          = {10.1016/j.asoc.2024.111825},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111825},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fractional-order Q-learning based on modal decomposition and convolutional neural networks for voltage control of smart grids},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A group consensus method based on social network and
three-way decision under multi-scale information systems. <em>ASOC</em>,
<em>162</em>, 111824. (<a
href="https://doi.org/10.1016/j.asoc.2024.111824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper synthesizes and analyses relevant data at multiple levels (i.e., multi-scale information systems (MSIS)) by fusing social network (SN) and three-way decision (TWD). It enables us to effectively address the complexity and uncertainty intrinsic to rich decision making environments. In addition, in group decision making (GDM) it is likely, often desirable, that agents view problems from different perspectives. Therefore, they need to reach consensus through negotiated discussions and changes in opinions. Such discussions will stop when either the group reaches a satisfactory consensus, or its members’ willingness to adjust reaches a maximum value. Within this context, the goal of this article is to investigate the consensus reaching process (CRP) of decision makers in an MSIS setting, and to establish a least-cost consensus method by fusing SN and TWD perspectives, referred to as the CRP-SN-MSIS method. First, optimal scale combinations in MSISs are filtered based on the spirit of TWD. In this way, we obtain multiple decision makers from a GDM viewpoint. Then, a measure method of their trust relations is designed to establish a social network among decision makers. Subsequently, the detailed process of the CRP-SN-MSIS method is presented and applied to a case study targeting a real dataset. Finally, the applicability and superiority of the designed method is fully validated through both qualitative and quantitative arguments.},
  archive      = {J_ASOC},
  author       = {Yibin Xiao and Xueling Ma and José Carlos R. Alcantud and Jianming Zhan},
  doi          = {10.1016/j.asoc.2024.111824},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111824},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A group consensus method based on social network and three-way decision under multi-scale information systems},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ship collision risk assessment: A multi-criteria
decision-making framework based on dempster–shafer evidence theory.
<em>ASOC</em>, <em>162</em>, 111823. (<a
href="https://doi.org/10.1016/j.asoc.2024.111823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ship collision risk modelling is dominated by the utilization of the experts’ experiential insights and knowledge. To construct an effective assessment model for evaluating potential ship collision risks grounded in expert judgments, this paper introduces a novel multi-criteria decision-making framework based on Dempster–Shafer evidence theory (DSET). Firstly, a novel index-based assessment system is provided to generate the potential collision risk value of ships based on automatic identification system (AIS) data, which encompasses ship condition parameters, deviation coefficient, and frequency coefficient. Further, to generate the weights of these indicators, a framework to extract expert opinions is provided, which includes two parts: a) converting experts’ opinions into D-S framework through hesitant fuzzy linguistic terms and b) generating experts’ opinions using improved DSET. Regarding part a, a methodology is proposed to evaluate the trustworthiness of experts by considering both the level of uncertainty in their opinions and the degree of similarity between their viewpoints. Concerning part b, strategies are integrated to ascertain the most suitable weight for the basic probability assignment, thereby mitigating conflicts within the evidence. Finally, based on the collected AIS data, the proposed framework has been applied to six of the world&#39;s busiest waterways: (1) the Strait of Malacca, (2) the English Channel, (3) the Panama Canal, (4) the Suez Canal, (5) the Danish Straits, and (6) the Strait of Hormuz. Subsequently, comprehensive analytical results and recommendations are furnished, such as wide and smooth channels showing elevated risk.},
  archive      = {J_ASOC},
  author       = {Nanxi Wang and Kum Fai Yuen and Jun Yuan and Duowei Li},
  doi          = {10.1016/j.asoc.2024.111823},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111823},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ship collision risk assessment: A multi-criteria decision-making framework based on Dempster–Shafer evidence theory},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining dynamic adaptive snake algorithm with perturbation
and observation for MPPT in PV systems under shading conditions.
<em>ASOC</em>, <em>162</em>, 111822. (<a
href="https://doi.org/10.1016/j.asoc.2024.111822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photovoltaic (PV) systems play an increasingly vital role in the global new energy landscape. However, when PV arrays are affected by partial shading, their power-voltage (P-U) characteristic curves exhibit dynamic multi-peak features, posing challenges to traditional Maximum Power Point Tracking (MPPT) techniques, including slow convergence speed, low tracking efficiency, and oscillations around the Maximum Power Point (MPP), leading to unnecessary power losses. Therefore, this study proposes a dual-layer control MPPT technology that integrates dynamic adaptive snake optimization algorithm with variable step perturbation and observation (ISO-IP&amp;O) to address the MPP extraction problem of PV systems under complex weather conditions. This study compares and analyzes the proposed ISO-IP&amp;O controller with current renowned MPPT techniques, including Grey Wolf Optimization combined with P&amp;O (GWO-P&amp;O), Improved Squirrel Search Algorithm (ISSA), Original Snake Optimization (SO), Horse Herd Optimization (HHO), and Adaptive Factor Selection Multi-Swarm Particle Swarm Optimization (FMSPSO), validating the effectiveness of the proposed ISO-IP&amp;O controller. Simulation results demonstrate that the ISO-IP&amp;O technology achieves an average tracking efficiency of 99.91 % under various shading conditions, with an average convergence speed of only 0.07 seconds. Compared with other existing methods, the ISO-IP&amp;O technology exhibits superior performance in terms of GMPP tracking speed, maximum power tracking efficiency, and stability. Experimental validation on the HIL+RCP physical experimental platform shows that under eight partial shading conditions(PSC), the proposed method still achieves the highest average tracking efficiency of 99.68 %, with the fastest average tracking speed of 0.66 seconds. The ISO-IP&amp;O controller achieves rapid, robust, and efficient GMPP tracking under various complex shading conditions, contributing to the enhancement of PV system performance.},
  archive      = {J_ASOC},
  author       = {Chunliang Mai and Lixin Zhang and Xue Hu},
  doi          = {10.1016/j.asoc.2024.111822},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111822},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining dynamic adaptive snake algorithm with perturbation and observation for MPPT in PV systems under shading conditions},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural architecture search via similarity adaptive guidance.
<em>ASOC</em>, <em>162</em>, 111821. (<a
href="https://doi.org/10.1016/j.asoc.2024.111821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary neural network architecture search (ENAS) has attracted the attention of many experts due to its global optimization capabilities to automatically search for convolutional neural network architectures based on the target task. The current search space for ENAS is not to design a fully structured network, but to search for smaller cell architectures to reduce search costs. However, blind search strategies do not effectively utilize the potential experience of the population. In order to utilize the potential experience learned by the current population to guide the evolutionary search of the population, we propose a similarity guided neural network architecture search algorithm based on cell architecture, which utilizes the similarity between pairwise architectures in the population as empirical knowledge learned by the population. Our proposed algorithm provides a novel method for calculating architecture similarity, which calculates architecture similarity separately from the cell and macro-structure. Then we decouple the connections and operations in the cell and calculate connection and operation similarity separately. In addition, we propose adaptive similarity selection and binary tournament selection strategies to enhance the algorithm’s global and local search capabilities and effectively explore the search space. Finally, we design an improved single-point crossover operator to enhance the local search ability of the evolutionary operator. The experimental results show that SAGNAS is a competitive algorithm that achieves 97.44% and 81.60% in CIFAR10 and CIFAR100 with only 1.9 GPU-days spent.},
  archive      = {J_ASOC},
  author       = {Yu Xue and Jiajie Zha and Mohamed Wahib and Tinghui Ouyang and Xiao Wang},
  doi          = {10.1016/j.asoc.2024.111821},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111821},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neural architecture search via similarity adaptive guidance},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scheduling techniques for addressing uncertainties in
container ports: A systematic literature review. <em>ASOC</em>,
<em>162</em>, 111820. (<a
href="https://doi.org/10.1016/j.asoc.2024.111820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ubiquitous uncertainties significantly impact the formulation and execution of the scheduling plan in container ports. Analyzing the recent two decades of peer-reviewed literature, this systematic review critically identifies key trends and approaches for tackling scheduling uncertainties in container ports. It constructs a comprehensive classification framework including six categories with 13 sub-categories of uncertainties, five categories with 11 sub-categories of uncertainty features, four scheduling approaches with 20 methods, and seven categories with 18 subcategories of optimization objectives. Under the framework, the review analyzes four main port scheduling problems concerning uncertainties—quay crane scheduling, berth allocation and crane assignment and scheduling, integrated scheduling of quay cranes and internal vehicles, and integrated scheduling of quay cranes, internal vehicles, and yard cranes—highlighting their unique characteristics and latest developments. A visual analysis illustrates the underlying relationships between optimization objectives, scheduling problems, uncertainties, and resolution methods for the first time. It is recommended to employ a proactive scheduling approach when the features of uncertain parameters are known. Conversely, when the features of uncertainties are unknown, using reactive scheduling methods is advisable. The review concludes with suggestions for future research, outlining 8 new areas for problem-solving, 10 methodological innovations, and 3 practical implications derived from the analysis.},
  archive      = {J_ASOC},
  author       = {Wenfeng Li and Lei Cai and Lijun He and Wenjing Guo},
  doi          = {10.1016/j.asoc.2024.111820},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111820},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scheduling techniques for addressing uncertainties in container ports: A systematic literature review},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). The fusion of fuzzy theories and natural language
processing: A state-of-the-art survey. <em>ASOC</em>, <em>162</em>,
111818. (<a href="https://doi.org/10.1016/j.asoc.2024.111818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a drastic surge in natural language processing (NLP), which is a popular research orientation in artificial intelligence. In contrast to precise numbers, human language is very complex and diverse, with millions of expressions, both spoken and written. It is due to this ambiguity and imprecision that most of the problems in NLP relating to cognition, translation, and understanding are non-trivial. Fuzzy theory, which accepts the fact that ambiguity exists, aims to address and actively quantify conceptual vagueness into messages that can be processed by computers. Following the thread of recent studies, we systematically review the fusion of fuzzy theory and NLP technologies from the aspects of commonly used fuzzy theories in NLP, the NLP tasks fuzzy theories are applied to, the application fields of fusion and the basic paradigms of fusion. Towards the end of this paper, we delineate the constraints and obstacles encountered in current researches, while also endeavoring to suggest avenues for enhancement that may serve as a reference for subsequent scholarly inquiry.},
  archive      = {J_ASOC},
  author       = {Ming Liu and Hongjun Zhang and Zeshui Xu and Kun Ding},
  doi          = {10.1016/j.asoc.2024.111818},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111818},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The fusion of fuzzy theories and natural language processing: A state-of-the-art survey},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure preserved fast dimensionality reduction.
<em>ASOC</em>, <em>162</em>, 111817. (<a
href="https://doi.org/10.1016/j.asoc.2024.111817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many graph-based unsupervised dimensionality reduction techniques have raised concerns about their high accuracy. However, there is an urgent need to address the enormous time consumption problem in large-scale data scenarios. Therefore, we present a novel approach named Structure Preserved Fast Dimensionality Reduction (SPFDR). Firstly, the parameter-insensitive, sparse, and scalable bipartite graph is constructed to build the similarity matrix. Then, employing alternating iterative optimization, the linear dimensionality reduction matrix and the optimal similarity matrix preserved cluster structure are learned. The computational complexity of the conventional graph-based dimension reduction method costs O ( n 2 d + d 3 ) O(n2d+d3) , yet the proposed approach is O ( n d m + n m 2 ) O(ndm+nm2) , wherein n n , m m , and d d are the number of instances, anchors, and features, respectively. Eventually, experiments conducted with multiple open datasets will provide convincing evidence for how effective and efficient the proposed method is.},
  archive      = {J_ASOC},
  author       = {Jihai Yi and Huiyu Duan and Jikui Wang and Zhengguo Yang and Feiping Nie},
  doi          = {10.1016/j.asoc.2024.111817},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111817},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structure preserved fast dimensionality reduction},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Facial expression recognition using reversible neural
network. <em>ASOC</em>, <em>162</em>, 111815. (<a
href="https://doi.org/10.1016/j.asoc.2024.111815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a concept of Reversible Neural Network, unexplored as yet, is introduced. When identifying facial expressions using a set of salient features such Reversible Neural Network is found to play a crucial role in recognizing the facial expressions into six basic categories. These salient characteristics are extracted by utilizing distance and shape signatures from three benchmark datasets to produce a noteworthy feature set. Such distance and shape signatures are utilized for finding stability indices capable of identifying a particular expression quite effectively. Statistical metrics like skewness, entropy, and moment are computed using distance and shape signatures in order to ensure the discriminative behavior in the selected feature set. The proposed Reversible Neural Network typically receives such discriminative feature set and transforms it into six basic facial expressions. The proposed network is tested and validated on the MMI, CK+, and JAFFE data sets in order to conduct our experiment and confirm the efficacy of facial expression identification. Additionally, its superiority in the context of facial expression recognition is justified by comparison with the state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Asit Barman and Paramartha Dutta},
  doi          = {10.1016/j.asoc.2024.111815},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111815},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Facial expression recognition using reversible neural network},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abstraction and decision fusion architecture for
resource-aware image understanding with application on handwriting
character classification. <em>ASOC</em>, <em>162</em>, 111813. (<a
href="https://doi.org/10.1016/j.asoc.2024.111813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resource-aware image understanding aims to achieve a soft equilibrium by effectively managing the constraints imposed by computational resources while improving the inferential capabilities of image processing systems. It serves a broad range of applications, spanning from embedded systems and IoT devices to budget-friendly smartphones and tablets. The proposed Abstraction and Decision Fusion Architecture (ADFA) addresses this problem through three tiers: abstraction, computation, and decision fusion. The first tier contains various views to generate different abstractions from the original data. These views are processed independently by an array of lightweight models forming the computation tier. They make independent decisions, where the final output is produced using a decision fusion tool in the last tier. To assess the capability of the proposed architecture, we have developed several ADFA-based models for the classification of handwriting data. In this regard, we first defined three data abstractions. Then, we trained support vector machines and fully connected neural networks according to the abstractions, which led to a set of independent basic models. Finally, an adaptive neuro-fuzzy inference system is employed as their hub to increase the accuracy by performing the decision fusion. Our experiments on the EMNIST dataset verify the efficiency and high accuracy of the proposed architecture in recognizing handwritten data, where the model size and the number of Multiply-Accumulate (MAC) operations are significantly smaller than state-of-the-art computational models.},
  archive      = {J_ASOC},
  author       = {Mohammad K. Fallah and Mohammadreza Najafi and Saeid Gorgin and Jeong-A Lee},
  doi          = {10.1016/j.asoc.2024.111813},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111813},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Abstraction and decision fusion architecture for resource-aware image understanding with application on handwriting character classification},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive integral neurodynamic model for dynamic nonlinear
optimization problems with equality constraints and its application.
<em>ASOC</em>, <em>162</em>, 111812. (<a
href="https://doi.org/10.1016/j.asoc.2024.111812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The zeroing neurodynamic model (ZND) is a specific recursive neurodynamic model designed to tackle dynamic issues. In previous research, the ZND model has exhibited substantial advancements in convergence and robustness compared with its original version. However, the structure of the recursive neurodynamic model has scope for improvement. In this paper, we propose an adaptive integral neurodynamic model (AIND) by solving dynamic nonlinear optimization problems with equational constraints (DNO-EC). Unlike ZND, the AIND model employs an adaptive integral with stability feedback, thus having a simpler structure compared to nonlinear projection functions with parallel outputs. Furthermore, we design a harmonic power-exponential (HPE) projection function, which enables the model to complete distributed convergence in finite time, regardless of the system’s initial state. Through, which is herein detailed through rigorous convergence analyses. Additionally, in-depth assumptions about potential complex noise are presented in the robust analysis section. Finally, the accuracy and superiority of the AIND model are verified by numerical experiments, and a motion generation of the manipulator is successfully implemented in simulation and physical experiments.},
  archive      = {J_ASOC},
  author       = {Yang Si and Dongyang Fu and Difeng Wang and Shangfeng Du and Yiyu Chen},
  doi          = {10.1016/j.asoc.2024.111812},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111812},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive integral neurodynamic model for dynamic nonlinear optimization problems with equality constraints and its application},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing berth-crane allocation considering tidal effects
using chaotic quantum whale optimization algorithm. <em>ASOC</em>,
<em>162</em>, 111811. (<a
href="https://doi.org/10.1016/j.asoc.2024.111811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the normalization of the global epidemic, shipping has gradually resumed, resulting in a surge in port throughput. Terminal managers are having great difficulty in making container ports be stable, orderly, and efficient. For small and medium-sized ports, making full use of the tidal water level to increase the operation time of large ships significantly improves the efficiency of port operations. Therefore, considering the impact of tidal factors on the operation of container ports, this paper firstly proposes a new berths and quay cranes allocation optimization model, T-B&amp;QC, that minimizes the distance between the actual berthing berth and the preferred berth, the port time cost of ships and the number of quay crane movements as the optimization objectives, under constraints that consider tidal factors. Then, to solve the T-B&amp;QC model using chaotic mapping and quantum theory, the Whale Optimization Algorithm (WOA) is integrated by using the Chaotic Quantum Rotating Gate Algorithm (CQRGA) and the Quantum Not Gate Algorithm (QNGA), whale coding rules are designed, and the Feasible-Integer Processing Algorithm (WF-IP) is established. Afterwards, the Chaotic Quantum Whale Optimization Algorithm, CQWOA, is proposed. Finally, the CQWOA is used to develop a new berth and quay crane allocation optimization method, T-B&amp;QC_CQWOA. Subsequently, data from an actual seaport container port is used to test the reliability and superiority of the proposed distributing approach and the established optimizing algorithm. Numerical results demonstrate that the proposed distributing approach outperforms the classical distribution models that are selected herein, and the CQWOA yields a coordinated schedule of higher quality than the others in the process of solving the model.},
  archive      = {J_ASOC},
  author       = {Ming-Wei Li and Rui-Zhe Xu and Zhong-Yi Yang and Yi-Hsuan Yeh and Wei-Chiang Hong},
  doi          = {10.1016/j.asoc.2024.111811},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111811},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing berth-crane allocation considering tidal effects using chaotic quantum whale optimization algorithm},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interval-valued spherical fuzzy CIMAS-WISP group
decision-analytic model for blockchain platform selection in digital
projects. <em>ASOC</em>, <em>162</em>, 111810. (<a
href="https://doi.org/10.1016/j.asoc.2024.111810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital projects aspiring to reach target audiences are executed through decentralized and trustworthy blockchain platforms (BPs). Once the objectives and target audience of a digital project are defined, the selection of suitable BPs is undertaken. The primary objective of this research is to develop a decision support system that aids in the selection of BPs for transferring digital data and assets. Numerous quantitative parameters determine the performance of BPs, alongside qualitative parameters indicating their performance. In this study, the aim is to determine the performance of BPs based on both qualitative and quantitative parameters. Within this scope, a multi-criteria decision-making approach and interval-valued spherical fuzzy (IVSF) sets are adopted. IVSF sets are utilized to determine expert importance levels. The IVSF-criteria importance assessment (CIMAS) method is introduced for the weighting of criteria. IVSF-CIMAS enables the determination of reliability levels in calculating criterion weights. The IVSF-simple weighted sum product (WISP) method is formulated to obtain the performance ranking of BPs. Thus, in this research, the IVSF-CIMAS-WISP hybrid model is developed, and an algorithm for this novel decision-analytic model is presented. A case study is developed focusing on BP selection for a digital project to demonstrate the applicability of the proposed hybrid model. The robustness of IVSF-CIMAS-WISP is tested through extensive sensitivity analysis scenarios. According to the research results, the applicability of the IVSF-CIMAS-WISP hybrid method is supported and its robustness is confirmed. The research findings provide numerous insights for project managers and practitioners.},
  archive      = {J_ASOC},
  author       = {Karahan Kara and Galip Cihan Yalçın and Vladimir Simic and Çağatay Korkuç and İlhan Çiçek and Erkan Afacan and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2024.111810},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111810},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interval-valued spherical fuzzy CIMAS-WISP group decision-analytic model for blockchain platform selection in digital projects},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learned features selection algorithm: Removal operation
of anomaly feature maps (RO-AFM). <em>ASOC</em>, <em>162</em>, 111809.
(<a href="https://doi.org/10.1016/j.asoc.2024.111809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class/Regression Activation Maps (CAMs/RAMs; AMs) are often embedded into Convolutional Neural Networks (CNNs) for checking activated regions on input images at estimation. CNNs sometime generate unreliable AMs, such as activated regions, are inappropriate. Because AM is calculated by stacking many feature maps generated by the final convolutional layer, when there are Anomaly Feature Maps (AFMs), unreliable AMs can be generated. For example, suppose we have a CNN that evaluates the heart. In this case, the feature maps that focus on regions unrelated to the heart (e.g., shoulders and esophagus) are AFMs. Additionally, we have a hypothesis that the estimation accuracy of CNNs is increased by removing AFMs. However, methods for automatically detecting and removing AFMs have not been sufficiently studied in previous research to improve the performance of CNNs. Therefore, we propose a method named “Removal Operation of Anomaly Feature Maps (RO-AFMs)” to automatically detect and remove AFMs. When applying an RO-AFM to the Global Average Pooling (GAP) feature vectors of a CNN, dimensions of the GAP vector are reduced. Therefore, an RO-AFM is regarded as a deep-feature selection algorithm. From the results of adopting an RO-AFM to a Regression CNN (R-CNN) for estimating pulmonary artery wedge pressure, which is one of the measurement score for representing cardiac anomaly state, improved reliability of AM and estimation accuracy were verified. A comparison of RO-AFM and the existing methods, i.e., Lasso and the Feature Selection Layer (FSL), indicated that RO-AFM performed slightly better on the estimation accuracy. The computation time required for RO-AFM to evaluate all features was 1.833 s on average, confirming that RO-AFM is a lightweight process. Therefore, RO-AFM is useful for constructing a medical CNN that emphasizes explainability (e.g., CNNs for estimating the risk of a disease or a test value from chest X-ray or computed tomography images).},
  archive      = {J_ASOC},
  author       = {Yuto Omae and Yohei Kakimoto and Yuki Saito and Daisuke Fukamachi and Koichi Nagashima and Yasuo Okumura and Jun Toyotani},
  doi          = {10.1016/j.asoc.2024.111809},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111809},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learned features selection algorithm: Removal operation of anomaly feature maps (RO-AFM)},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampling-based test scenarios searching method for
autonomous system robustness evaluation. <em>ASOC</em>, <em>162</em>,
111808. (<a href="https://doi.org/10.1016/j.asoc.2024.111808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness evaluation for autonomous systems is essential before implementing them in safety-critical fields. Searching the challenging test scenarios by adaptive sampling algorithm in the transition regions between different performance modes, named performance boundary, becomes an efficient method to evaluate the robustness of systems. This paper proposes a novel model-independent nearest neighbors adaptive sampling algorithm considering gradient and distance, named GDNNAS. GDNNAS designs a special evaluation criterion to identify samples within boundary regions and a searching method to generate new samples. A novel elimination criterion is proposed to ensure the quality of samples and an adaptive stopping criterion is proposed to stop the sampling process adaptively. Considering the lack of unified data sets and metrics to assess adaptive sampling methods for searching the performance boundary, we proposed a comparison platform consisting of nine simulated systems under test (SSUT) as adaptive sampling benchmarks and four evaluation metrics to evaluate the adaptive sampling method quantitatively. We assess GDNNAS and two other adaptive sampling algorithms with the nine SSUTs. The experimental result proves that samples generated by GDNNAS are precisely located at and cover the performance boundary regions. In addition, we analyze the effectiveness of the comparison platform. Experimental result shows that the proposed SSUTs and evaluation metrics can comprehensively reflect the performance of the algorithm from various aspects and reveal the characteristics of the adaptive sampling algorithms. To verify the practicability of GDNNAS, a path planning system is used for testing GDNNAS, and the experiment result proves that GDNNAS performs outstandingly in practical system robustness evaluations.},
  archive      = {J_ASOC},
  author       = {Hui Lu and Shiqi Wang and Yuxuan Zhang and Shi Cheng},
  doi          = {10.1016/j.asoc.2024.111808},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111808},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sampling-based test scenarios searching method for autonomous system robustness evaluation},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ranking-based multi/many-objective evolutionary optimization
with hierarchical evaluation rules and its application in water
distribution network. <em>ASOC</em>, <em>162</em>, 111807. (<a
href="https://doi.org/10.1016/j.asoc.2024.111807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To solve multi/many-objective optimization problems characterized by complex Pareto fronts, we propose a two-phase ranking-based multi-objective evolutionary algorithm with hybrid convergence rules. Firstly, we introduce a novel hybrid convergence criterion by integrating Pareto dominance, localized pruning power (LPP), and distance-based measure. Secondly, we design a two-phase ranking-based selection strategy. Specifically, we employ the acute angle between objective vector and reference vector to divide the current population into multiple sub-populations. In the first-phase ranking, the solutions within each subpopulation are sorted according to the hybrid convergence rule. The best solutions are chosen based on the angle-based diversity criterion into separate fronts. In the second-phase selection, a specific number of solutions with best diversity are further selected to the next generation based upon their front-level assignments. To obtain better diversity of selected solutions, we employ the maximum angle between solutions and the sum of normalized objectives of solutions to assist the selecting process. The experimental results, which encompass benchmark problems involving up to 10 objectives as well as two real-world application instances, validating our proposed algorithm’s competitiveness and effectiveness.},
  archive      = {J_ASOC},
  author       = {Dongyu Wang and Lianbo Ma},
  doi          = {10.1016/j.asoc.2024.111807},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111807},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ranking-based multi/many-objective evolutionary optimization with hierarchical evaluation rules and its application in water distribution network},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated detection of COVID-19 and pneumonia diseases using
data mining and transfer learning algorithms with focal loss from chest
x-ray images. <em>ASOC</em>, <em>162</em>, 111806. (<a
href="https://doi.org/10.1016/j.asoc.2024.111806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pneumonia is a lung infection that produces severe inflammation, which can be caused by viruses, bacteria, or fungi. The global occurrence of pneumonia cases has been notably affected by the outbreak of Coronavirus Disease 2019 (COVID-19). Recent studies have demonstrated the capability of Deep Learning (DL) algorithms in detecting COVID-19 and pneumonia in medical images like X-rays. In this study, we employ four different Chest X-ray (CXR) datasets to evaluate four pre-trained Convolutional Neural Network (CNN) models: InceptionResNet V2, Inception V3, Xception, and MobileNet. These models were fine-tuned using either the Cross-Entropy (CE) loss function only for balanced datasets or separately using both the CE and Focal Loss (FL) functions for imbalanced datasets. For the first dataset, the InceptionResNet V2 model achieved the highest multi-classification accuracy of 88.63%. The superior model for the second and fourth datasets is Inception V3, with 94.35% and 97.67% multi-classification accuracy, respectively. For the third dataset, the best model is Xception, with a binary classification accuracy of 100.00%. Our results emphasize the significance of using the FL function in solving class imbalance problems in DL models. Additionally, it highlights the effectiveness of individual DL models in detecting different pneumonia infections using different CXR datasets.},
  archive      = {J_ASOC},
  author       = {Rana Khattab and Islam R. Abdelmaksoud and Samir Abdelrazek},
  doi          = {10.1016/j.asoc.2024.111806},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111806},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated detection of COVID-19 and pneumonia diseases using data mining and transfer learning algorithms with focal loss from chest X-ray images},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of machine learning methods in software
testing. <em>ASOC</em>, <em>162</em>, 111805. (<a
href="https://doi.org/10.1016/j.asoc.2024.111805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The quest for higher software quality remains a paramount concern in software testing, prompting a shift towards leveraging machine learning techniques for enhanced testing efficacy. The objective of this paper is to identify , categorize , and systematically compare the present studies on software testing utilizing machine learning methods. This study conducts a systematic literature review (SLR) of 40 pertinent studies spanning from 2018 to March 2024 to comprehensively analyze and classify machine learning methods in software testing. The review encompasses supervised learning, unsupervised learning, reinforcement learning, and hybrid learning approaches. The strengths and weaknesses of each reviewed paper are dissected in this study. This paper also provides an in-depth analysis of the merits of machine learning methods in the context of software testing and addresses current unresolved issues. Potential areas for future research have been discussed, and statistics of each review paper have been collected. By addressing these aspects, this study contributes to advancing the discourse on machine learning&#39;s role in software testing and paves the way for substantial improvements in testing efficacy and software quality.},
  archive      = {J_ASOC},
  author       = {Sedighe Ajorloo and Amirhossein Jamarani and Mehdi Kashfi and Mostafa Haghi Kashani and Abbas Najafizadeh},
  doi          = {10.1016/j.asoc.2024.111805},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111805},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic review of machine learning methods in software testing},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Automatic time series forecasting model design based on
pruning. <em>ASOC</em>, <em>162</em>, 111804. (<a
href="https://doi.org/10.1016/j.asoc.2024.111804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Time Series Forecasting (TSF) model design which aims to help users to efficiently design forecasting models for the given time series data scenarios, is a novel research topic to be urgently solved. In this paper, we propose AutoTS algorithm trying to utilize the existing design skills and design efficient search methods to effectively solve this problem. In AutoTS, we extract effective design experience from the existing TSF works. We allow the effective combination of design experience from different sources, to create an effective search space containing a variety of TSF models to support different TSF tasks. Considering the huge search space, in AutoTS, we propose a two-stage pruning strategy to reduce the search difficulty and improve the search efficiency. Specifically, at the beginning of the search phase, we apply the vertical pruning method to quickly optimize each module of the TSF model. In the middle of the search, we turn to apply the horizontal pruning method to filter out less effective options for each module according to the learned experience and optimize modules. Moreover, in AutoTS, we introduce the knowledge graph to reveal associations between module options. We make full use of this relational information to learn higher-level features of each module option, to further improve the search quality. Experimental results show that AutoTS is well-suited for the TSF area. On some general datasets, our algorithms discover the assembled model with better performance. Additionally, compared to existing NAS algorithms, they exhibit higher search efficiency and greater search potential than the manually designed ones.},
  archive      = {J_ASOC},
  author       = {Chunnan Wang and Xingyu Chen and Chengyue Wu and Hongzhi Wang},
  doi          = {10.1016/j.asoc.2024.111804},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111804},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic time series forecasting model design based on pruning},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust autonomous vehicle control by leveraging multi-stage
MPC and quantized CNN in HIL framework. <em>ASOC</em>, <em>162</em>,
111802. (<a href="https://doi.org/10.1016/j.asoc.2024.111802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel path planning and path-following system designed explicitly for real-world driving conditions in autonomous vehicles. The pipeline considers uncertainties in the dynamic parameters of the vehicle, which are common in real-world scenarios. To address these uncertainties, a robust multi-stage model predictive control approach is developed to generate optimal control outputs. A customized convolutional neural network, Nano-FastSCNN, quantized to half-precision, is employed for accurately segmenting left and right lane boundaries. To ensure that our visual planner is robust against adverse weather conditions, we have rigorously evaluated its generalization capabilities under challenging weather types. The centerline of these lanes is then selected as the desired path and transformed into the camera reference frame by leveraging the camera projection matrix. The controller utilizes this reference trajectory to guide the vehicle along the desired trajectory. To demonstrate real-time processing capabilities, the entire pipeline, including the vision system module and controller, is implemented in a hardware-in-the-loop setup utilizing three NVIDIA Jetson devices to demonstrate the scalability of our architecture. Comprehensive evaluations demonstrate that the pipeline achieves an inference speed of 15.6 FPS, even on the less powerful NVIDIA Jetson Nano. Moreover, the system successfully alleviates the detrimental effects of uncertainties and reliably follows the desired path.},
  archive      = {J_ASOC},
  author       = {Amir Khosravian and Masoud Masih-Tehrani and Abdollah Amirkhani and Salman Ebrahimi-Nejad},
  doi          = {10.1016/j.asoc.2024.111802},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111802},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust autonomous vehicle control by leveraging multi-stage MPC and quantized CNN in HIL framework},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In-air handwriting system based on multi-scale channel
attention network and monocular vision. <em>ASOC</em>, <em>162</em>,
111801. (<a href="https://doi.org/10.1016/j.asoc.2024.111801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-air handwriting based on a monocular camera is an innovative and promising modality for human–computer interaction, offering a plethora of potential applications. However, existing in-air handwriting systems based on monocular camera suffer a significant challenge in determining the spatial coordinates of fingertips using two-dimensional images from a monocular camera. Since the size of the fingertip is very small and has very few discriminative features. To tackle this challenge, we propose a Multi-Scale Channel Attention Network (MSCAN). Through weighting multi-scale channels, the MSCAN facilitates the concentration of target detection models on high-resolution, small-scale channels, thus enhancing fingertip localization precision. We integrate the MSCAN with the YOLOv5s model to realize a novel in-air handwriting system based on monocular vision. We conducted comparative experiments on a self-constructed fingertip dataset and several publicly available small-target datasets. Experimental results show that the proposed method can effectively improve the accuracy of fingertip and small-target detection. The detection rate of fingertips reaches 98%, indicating that the proposed in-air handwriting system enables users to write freely and smoothly.},
  archive      = {J_ASOC},
  author       = {Xiwen Qu and Minhong Ye and Wei Zhao},
  doi          = {10.1016/j.asoc.2024.111801},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111801},
  shortjournal = {Appl. Soft. Comput.},
  title        = {In-air handwriting system based on multi-scale channel attention network and monocular vision},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive uniform search framework for constrained
multi-objective optimization. <em>ASOC</em>, <em>162</em>, 111800. (<a
href="https://doi.org/10.1016/j.asoc.2024.111800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive uniform search framework designed for constrained multi-objective optimization. The framework comprises three key components: a global uniform exploration strategy, a local greedy exploitation strategy, and a search switch mechanism. These components work together to facilitate comprehensive exploration of promising areas while maintaining a balance between global exploration and local exploitation. Specifically, the global uniform exploration strategy ensures even distribution within promising areas, preventing any oversights during exploration. The local greedy exploitation strategy divides these areas into sub-areas and employs a feasibility-led constraint handling technique to enhance efficiency in identifying optimal solutions. Additionally, the search switch dynamically adjusts the search strategy between global exploration and local exploitation. Numerical simulations on various benchmark suites and real-world problem demonstrate the strong performance of the framework in addressing constrained multi-objective optimization problems. The comparison results show that compared with eight recently proposed algorithms, the proposed framework is more robust in solving diverse constrained multi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Jiawei Yuan and Shuiping Yang and Wan-Lin Yan},
  doi          = {10.1016/j.asoc.2024.111800},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111800},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive uniform search framework for constrained multi-objective optimization},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end dimensionality reduction and regression from 3D
geological uncertainties to estimate oil reservoir simulations.
<em>ASOC</em>, <em>162</em>, 111799. (<a
href="https://doi.org/10.1016/j.asoc.2024.111799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Managing geological uncertainties in reservoir engineering involves significant challenges mainly due to the prohibitive computational costs of traditional simulation methods. These simulations, essential for generating geological models, often require extensive computational resources and can take days or weeks to complete. The computational load limits the number of viable models while the high dimensionality of properties compounds the challenge, and the abundance of producing wells, each associated with different objective functions, further complicates the problem. Current methodologies have focused on training an Artificial Neural Network (ANN) for each producer well to create a proxy model. Instead, we propose the development of a single ANN designed to simultaneously predict the behavior of multiple objective functions. This work proposes a novel end-to-end deep neural network that can handle 3D geological uncertainties and replicate the simulator’s response for several cumulative production curves. This approach leverages 3D convolutions to process spatial and depth dimensions within heterogeneous reservoirs, including diverse geostatistical realizations. The end-to-end solution was successfully implemented in a Brazilian offshore field and accurately replicated the simulator’s behavior and yielded results that outperformed state-of-the-art methods. The results indicate correlations that exceeded 0.95 between the proxy response and the numerical simulator. Additionally, our approach demonstrated robustness even when trained with a limited number of simulation models, and it notably reduced computational costs. The findings highlight our architecture’s capacity to integrate dimensional reduction and regression analysis within a unified framework, effectively predicting different fluid behaviors in the reservoir and showcasing robustness against high dimensionality and sparse data.},
  archive      = {J_ASOC},
  author       = {Jeanfranco Farfan and Gabriel Cirac and Guilherme Daniel Avansi and Célio Maschio and Denis José Schiozer and Anderson Rocha},
  doi          = {10.1016/j.asoc.2024.111799},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111799},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end dimensionality reduction and regression from 3D geological uncertainties to estimate oil reservoir simulations},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic gaussian mutation beetle swarm optimization method
for large-scale weapon target assignment problems. <em>ASOC</em>,
<em>162</em>, 111798. (<a
href="https://doi.org/10.1016/j.asoc.2024.111798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weapon target assignment is a crucial issue for firepower resources optimization in modern warfare. Such a problem is complicated, multi-constrained, strongly nonlinear, NP-complete and the existing studies did not consider the suitability between different weapons and targets. In this paper, a novel weapon target assignment model is established that involves the weapon-target suitability and is closer to the real combat scenarios. Then, in view of that the conventional weapon target assignment methods are difficult to be applied in the large-scale problems efficiently, this work proposes a dynamic Gaussian mutation beetle swarm optimization algorithm with rule-based chaotic initialization. With the assistance of the dynamic parameter adjustment strategies and Gaussian mutation, the improved algorithm has fast convergence speed and high convergence accuracy, and it can solve the weapon target assignment problems with excellent optimization capabilities. Besides, the rule-based chaotic initialization strategy is embedded in this algorithm to generate high-quality population with better diversity. Finally, two comparative simulation cases of different initialization methods and algorithms for solving the large-scale weapon target assignment problems are designed. The results demonstrate that the proposed approach can provide more superior assignment schemes than its competitors with enhanced efficiency.},
  archive      = {J_ASOC},
  author       = {Han Xu and An Zhang and Wenhao Bi and Shuangfei Xu},
  doi          = {10.1016/j.asoc.2024.111798},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111798},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic gaussian mutation beetle swarm optimization method for large-scale weapon target assignment problems},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mobility-aware proactive video caching based on asynchronous
federated learning in mobile edge computing systems. <em>ASOC</em>,
<em>162</em>, 111795. (<a
href="https://doi.org/10.1016/j.asoc.2024.111795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advance of the Internet of Things (IoT) and the surging user-centric smart devices, video services such as short videos, real-time video streaming, and Virtual Reality (VR) gaming are emerging. Traditional cloud-centric caching is no longer able to satisfy the low-latency needs of mobile user groups. Predicting the most popular content in advance and caching it on edge devices such as small base stations (SBS) is a promising solution to alleviate network congestion and supply better Quality of Service (QoS) for mobile users. However, frequently changing popular content, limited edge caching resources, privacy of user data, and user mobility pose significant challenges to edge video caching. To tackle these challenges, we propose an efficient asynchronous federated learning (AFL)-based mobility-aware proactive video caching scheme (AF-MPVC), which significantly improves the overall network performance while preventing the risk of user data leakage. First, we model the proactive video caching problem in mobile edge computing systems and consider the impact of user mobility on AFL, representing the problem as an autoencoder (AE) model loss minimization problem. Then, a proactive video caching algorithm is proposed based on a filtering model, which uses the hidden features obtained from the trained AE model to compute the user similarity to find the popular videos that are most likely to be accessed by users and combine them with the most frequently requested videos. Meanwhile, we take into account the short duration of high mobility users staying on the current SBS and cache some videos to neighboring edge servers to have more redundant space to store more less popular videos. Finally, experiments on three real-world datasets demonstrate that the presented caching scheme outperforms other baseline caching schemes with respect to cache hit rate and latency reduction.},
  archive      = {J_ASOC},
  author       = {Zhen Qian and Yiming Feng and Chenglong Dai and Wei Li and Guanghui Li},
  doi          = {10.1016/j.asoc.2024.111795},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111795},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mobility-aware proactive video caching based on asynchronous federated learning in mobile edge computing systems},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight deep convolutional neural network model for
skin cancer image classification. <em>ASOC</em>, <em>162</em>, 111794.
(<a href="https://doi.org/10.1016/j.asoc.2024.111794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models, particularly transformers and convolutional neural networks (CNNs), have been commonly used to achieve high classification accuracy for image data. Since introducing transformers, researchers have predominantly embraced these models to obtain impressive classification rates with novel approaches. In light of this scenario, we present a novel lightweight CNN called TurkerNet. Our primary objective is to attain a superior classification performance while minimizing the number of trainable parameters. TurkerNet comprises four essential components: the input block, residual bottleneck block, efficient block, and output block. To evaluate the performance of our proposed model, we conducted experiments using an open-access image dataset, specifically curated to include skin cancer images classified into two categories: benign and malignant. Our proposed (TurkerNet) model achieved a remarkable testing accuracy of 92.12% on this public dataset. Our model performed better than state-of-the-art techniques developed for automated skin cancer detection. Moreover, our proposed TurkerNet is a lightweight model. In this aspect, the presented TurkerNet is highly accurate with low trainable parameters.},
  archive      = {J_ASOC},
  author       = {Turker Tuncer and Prabal Datta Barua and Ilknur Tuncer and Sengul Dogan and U. Rajendra Acharya},
  doi          = {10.1016/j.asoc.2024.111794},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111794},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight deep convolutional neural network model for skin cancer image classification},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy-based dilemma assessment in confrontation analysis
with applications to the walkerton drinking water crisis. <em>ASOC</em>,
<em>162</em>, 111793. (<a
href="https://doi.org/10.1016/j.asoc.2024.111793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global population continues to rise and the processes of globalization gather momentum, the conflicting nature of water management issues is becoming increasingly apparent. The purpose of this paper is to introduce a fuzzy-based dilemma assessment method into the methodology confrontation analysis to analyze and assess the conflict problems. More specifically, the inherent logic of dilemma classification and mathematical definitions to the dilemmas are furnished initially to assist in understanding the mechanism of dilemma generation and assessment. Next, the statements of decision-makers (DMs) are quantified and assigned to a specific interval of fuzzy values, which serve to denote the degree of preference regarding their position and intention, as well as the intensity of doubts. Based on that, the dilemma intensity is calculated using the logic derived from the dilemma itself and the dilemma effect is obtained through the application of the same quantitative methodology. Then, to circumvent the complexities associated with parameter discussions, a four quadrant analysis framework is employed, utilizing dilemma intensity and dilemma effect as the two axes to assess dilemma priority. Finally, the Walkerton Drinking Water Crisis is presented to demonstrate the effectiveness and feasibility of the aforementioned approach.},
  archive      = {J_ASOC},
  author       = {Zihui Liu and Bingfeng Ge and Yuming Huang and Zeqiang Hou and Wanying Wei and Hui Xie},
  doi          = {10.1016/j.asoc.2024.111793},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111793},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy-based dilemma assessment in confrontation analysis with applications to the walkerton drinking water crisis},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Support vector regression-based heteroscedastic models for
cryptocurrency risk forecasting. <em>ASOC</em>, <em>162</em>, 111792.
(<a href="https://doi.org/10.1016/j.asoc.2024.111792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we aimed to combine support vector regression (SVR) with an autoregressive (AR) model and a generalized autoregressive conditional heteroscedastic (GARCH) or asymmetric GARCH model. We compared the performances of these combined models with the maximum likelihood estimation (MLE)-based models under a normal, Student’s t , or skewed Student’s t distributional assumption. We employed such models to protect the heteroscedastic risks of four major cryptocurrencies, namely, Bitcoin, Ethereum, Tether, and Binance Coin, before and during COVID-19. We accomplished this task by forecasting their volatility, Value-at-Risk (VaR), and Expected Shortfall (ES). The empirical results revealed that in terms of the root-mean-square error (RMSE) and mean-absolute error (MAE) metrics, the SVR-based models with a polynomial or radial basis kernel function, which is nonlinear, resulted in a more accurate volatility forecast compared to the MLE-based models under any distributional assumption. The superiority of the former models was more apparent when applied to Tether exhibiting the most stable volatility. This supports evidence of nonlinear characteristics in the cryptocurrency return and volatility series. A combination between SVR and an asymmetric GARCH model also confirms the presence of an asymmetrically nonlinear relationship between the returns and volatility. In addition, the correct-VaR and Kupiec’s backtesting results showed that the SVR-based models tended to produce the VaR forecasts for the four cryptocurrencies with better accuracy at the 1%, 5%, and 10% significance levels compared to the MLE-based ones. The use of the correct-ES metric also leads to the superior ES forecasting performance of the former models at any level. However, McNeil and Frey’s backtesting results demonstrated their worse ES forecasting performance at the 1% level. As the COVID-19 pandemic progressed, we also found an increase in the Bitcoin and Binance Coin market risks as well as a decrease in the Ethereum and Tether market risks.},
  archive      = {J_ASOC},
  author       = {Intan Muchtadi-Alamsyah and Robin Viltoriano and Ferdinand Harjono and Martha Nazaretha and Martin Susilo and Ade Bayu and Bony Josaphat and Arief Hakim and Khreshna Syuhada},
  doi          = {10.1016/j.asoc.2024.111792},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111792},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Support vector regression-based heteroscedastic models for cryptocurrency risk forecasting},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RDCSAE-RKRVFLN: A unified deep learning framework for robust
and accurate DOA estimation. <em>ASOC</em>, <em>162</em>, 111791. (<a
href="https://doi.org/10.1016/j.asoc.2024.111791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an innovative unified deep learning (DL) model, “reduced deep convolutional stack autoencoder (RDCSAE)-robust kernel-based random vector functional link network (RKRVFLN)” (RDCSAE-RKRVFLN), for addressing practical challenges like phase and gain uncertainty, mutual coupling effects, and element position errors in direction of arrival (DOA) estimation. The RDCSAE utilizes deep convolutional neural network (DCNN) and stack autoencoder (SAE) for robust feature extraction. The RKRVFLN integrates a concise SAE output with kernel-based learning. Thus, the integration of RDCSAE (suitable for unsupervised feature extraction) and RKRVFLN (for supervised learning) in the developed model results in efficient use of limited training data. Performance metrics of this method include among others the accuracy (separation between sources), root mean square error (RMSE), signal-to-noise ratio (SNR), etc. The method performs better than existing data driven methods in terms of computational efficiency, faster learning speed, enhanced model generalization, higher accuracy, and shorter event regression time and empirical methods. In the presence of above mentioned four perturbations, this method successfully resolves multiple sources with a resolution of 0 . 3 ° 0.3° with a RMSE of 0.00376. For real time application, the paper implements RDCSAE-RKRVFLN on a high-speed reconfigurable Xilinx Virtex-5 field-programmable gate array (FPGA) realizing a digital DOA estimator for one source. The hardware estimates the DOA with a maximum error of 0.01953125 which follows closely the simulation results indicating the efficiency and feasibility of the method.},
  archive      = {J_ASOC},
  author       = {Priyadarshini Raiguru and Bhanja Kishor Swain and Susanta Kumar Rout and Mrutyunjaya Sahani and Rabindra Kishore Mishra},
  doi          = {10.1016/j.asoc.2024.111791},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111791},
  shortjournal = {Appl. Soft. Comput.},
  title        = {RDCSAE-RKRVFLN: A unified deep learning framework for robust and accurate DOA estimation},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Serial cascaded deep feature extraction-based adaptive
attention dilated model for crop recommendation framework.
<em>ASOC</em>, <em>162</em>, 111790. (<a
href="https://doi.org/10.1016/j.asoc.2024.111790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective crop farming depends on wise selection of crops. It is an essential factor that has to be fulfilled before beginning an agricultural endeavor. Conventionally, the crop that has to be grown is selected without considering the location and cultivated site’s characteristics by only considering its profit and demand on the market. Choosing the best crop for the circumstances can minimize the need for additional fertilizer and water for irrigation and help in attaining enhanced crop yield. Therefore, choosing the right crop is crucial for a successful agricultural situation. Thus, a novel crop recommendation model by considering the soil and geographical conditions is developed to aid the farmers in choosing the appropriate crop for the right condition so that the overall production can be enhanced to increase the overall profit and decrease the losses faced by the farmers. At first, a certain geographical area is selected, and the ideal parameters for growing a particular plant are gathered from the standard database. Next, the deep optimal features are extracted using a Serial Cascaded network in which an autoencoder is cascaded with a “Dimensional Convolutional Neural Network (1DCNN)” from the gathered data. The obtained deep features are optimally selected using the developed Modified Movement Territory of Fire Hawk Optimizer (MMTFHO). These optimally selected features are given to the Adaptive and Attention-based Hybrid Network (AAHNet) in which “Gated Recurrent Unit (GRU), and Long Short Term Memory (LSTM)” are utilized for choosing the right crop for the provided geographical condition. The parameters in the AAHNet are optimized using the same enhanced MMTFHO algorithm for improving the precision of the appropriate crop selection process. The final prediction of crops for the given geographical condition is obtained from the AAHNet. The final or overall rating of the recommended approach regarding accuracy metrics is 96.73%.},
  archive      = {J_ASOC},
  author       = {Durai Arumugam S S L and Praveen Kumar R},
  doi          = {10.1016/j.asoc.2024.111790},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111790},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Serial cascaded deep feature extraction-based adaptive attention dilated model for crop recommendation framework},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of conventional and quantum computing for
predicting mortality based on small early-onset colorectal cancer data.
<em>ASOC</em>, <em>162</em>, 111781. (<a
href="https://doi.org/10.1016/j.asoc.2024.111781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing integrated with machine learning (ML) offers novel solutions in various fields, including healthcare. The synergy between quantum computing and ML in classification exploits unique data patterns. Despite theoretical advantages, the empirical application and effectiveness of quantum computing on small medical datasets remains underexplored. This retrospective study from a tertiary hospital used data on early-onset colorectal cancer with 93 features and 1501 patients from 2008 to 2020 to predict mortality. We compared quantum support vector machine (QSVM) models with classical SVM models in terms of number of features, number of training sets, and outcome ratio. We evaluated the model based on the area under the curve in the receiver operating characteristic curve (AUROC). We observed a mortality rate of 7.6 % (96 of 1253 subjects). We generated the mortality prediction model using 11 clinical variables, including cancer stage and chemotherapy history. We found that the AUROC difference between the conventional and quantum methods was the maximum for the top 11 variables. We also showed the AUROC in QSVM (mean [standard deviation], 0.863 [0.102]) outperformed all the number of trials in the conventional SVM (0.723 [0.231]). Compared to the conventional SVM, the QSVM showed robust performance, consistent with the AUROC, even in the unbalanced case. Our study highlights the potential of quantum computing to improve predictive modeling in healthcare, especially for rare diseases with limited available data. The advantages of quantum computing, such as the exploration of Hilbert space, contributed to the superior predictive performance compared to conventional methods.},
  archive      = {J_ASOC},
  author       = {Jae Yong Yu and Woo Seob Sim and Jae Yeob Jung and Si Heon Park and Han Sang Kim and Yu Rang Park},
  doi          = {10.1016/j.asoc.2024.111781},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111781},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of conventional and quantum computing for predicting mortality based on small early-onset colorectal cancer data},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benchmarking of industrial wastewater treatment processes
using a complex probabilistic hesitant fuzzy soft schweizer–sklar
prioritized-based framework. <em>ASOC</em>, <em>162</em>, 111780. (<a
href="https://doi.org/10.1016/j.asoc.2024.111780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of this research work is to effectively handle the intricate and uncertain nature of decision-making in the context of industrial wastewater treatment. The prioritization of wastewater treatment is of utmost importance in order to save the environment, promote human health, ensure adherence to legal regulations, conserve resources, and foster overall sustainability. The demand for efficient and sustainable wastewater treatment processes is increasing globally, but selecting appropriate treatment technologies for industrial effluents is challenging due to its complex nature. Our study aims to provide a systematic framework for evaluating and selecting treatments based on efficacy, affordability, and environmental impact. Aggregation operators are one of the fundamental ideas in the framework of information fusion for addressing real-life problems. Numerous scholars have made significant contributions to the development of aggregation operators specifically designed for multi-parameter decision-making (MPDM) whenever circumstances are characterized by uncertainty. Regrettably, the current operators can only be employed within strict limits and constraints. Therefore, this research formulates novel prioritized aggregation operators that alleviate the restrictive constraints associated with the current operators. After this, we present a new methodology that combines the Schweizer–Sklar prioritized aggregation operators with a complex probabilistic hesitant fuzzy soft framework. For this purpose, we develop the averaging aggregation operators and geometric aggregation operators. Following that, we proceed to examine the theorems and properties associated with them by providing rigorous proofs. Then, we develop a novel decision-making technique with a numerical example based on the wastewater treatment process. Through this novel technique, the best process is the activated sludge process. We perform the validity tests to evaluate the feasibility and effectiveness of MPDM techniques.},
  archive      = {J_ASOC},
  author       = {Muhammad Saqib and Shahzaib Ashraf and Hafiz Muhammad Athar Farid and Vladimir Simic and Muneeba Kousar and Erfan Babaee Tirkolaee},
  doi          = {10.1016/j.asoc.2024.111780},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111780},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Benchmarking of industrial wastewater treatment processes using a complex probabilistic hesitant fuzzy soft Schweizer–Sklar prioritized-based framework},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial sample attacks algorithm based on
cycle-consistent generative networks. <em>ASOC</em>, <em>162</em>,
111778. (<a href="https://doi.org/10.1016/j.asoc.2024.111778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the application of deep neural networks in the field of image classification has become increasingly widespread. However, existing researches have exposed the inherent vulnerabilities and opacity of these network models, rendering them susceptible to carefully crafted adversarial attacks. This poses significant security risks to their practical deployment. Given that the majority of models in real-world applications operate in a black-box manner, current research on black-box attacks targeting classification models remains inadequate, facing significant challenges such as incomplete robustness evaluations, low success rates of black-box attacks, and excessive resource consumption during the attack process. To address these challenges, this paper innovatively proposes a black-box attack algorithm based on a cycle-consistent generative network. This algorithm directly generates adversarial perturbations through a generative network that integrates an attention mechanism, deeply learning the complex relationship between adversarial samples and clean sample data. It supports both targeted and untargeted attack modes. Experimental validation on the SAT dataset demonstrates that our method achieves an average white-box attack success rate exceeding 96% in classification tasks and an average success rate of up to 69.4% in black-box transfer attacks, exhibiting excellent transferability across different models. This research contributes significantly to enhancing the effectiveness of black-box attacks, reducing attack costs, and providing novel ideas and methods for enhancing the security of deep learning models.},
  archive      = {J_ASOC},
  author       = {Haoqi Gao and Xing Yang and Yihua Hu and Zhenyu Liang and Haoli Xu and Bingwen Wang and Hua Mu and Yangyang Wang},
  doi          = {10.1016/j.asoc.2024.111778},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111778},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adversarial sample attacks algorithm based on cycle-consistent generative networks},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of the time-distributed layer in the controller
of memory-augmented neural networks to classify brain activities into
motor imagery and motor execution. <em>ASOC</em>, <em>162</em>, 111771.
(<a href="https://doi.org/10.1016/j.asoc.2024.111771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-Computer Interface (BCI) systems create a bridge between the human brain and the outside world, potentially rendering traditional methods of information transmission obsolete in the not-so-distant future. One of the key research areas in BCI is the classification of brain activity in electroencephalographic (EEG) data. On the other hand, new memory-augmented neural networks, such as the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC), have demonstrated their impressive abilities in solving complex tasks. Therefore, it is useful to evaluate the capability of memory-augmented neural networks to enhance the classification of brain activity within EEG signals. Previous methods have suffered from low accuracy and generalizability in classifying brain activities; primarily due to a lack of proper classification of Motor Imagery/Execution brain activities, an inability to extract valuable information at different time steps in time series data, and a failure to learn from longer dependencies. This article introduces TDMANN (Time-Distributed Memory Augmented Neural Network), a framework that leverages the principles of NTM and DNC for the binary classification of brain activities in EEG signals. The controller component of the memory-augmented neural network is enhanced with a time-distributed approach, which significantly improves the performance of the network in binary classification tasks involving motor imagery/execution brain activities by extracting valuable information at each time step. The benchmark datasets used in this study are EEGmmidb BCI2000 (Imagery/Execution), BCI IV 2B, and BCI IV 2A, all containing motor imagery/execution brain activity data in EEG format. The results demonstrate that the classification accuracy achieved by the proposed DNC@TDMANN method exhibits a maximum improvement of 23.03% compared to baseline research works. The NTM@TDMANN method also shows a maximum improvement accuracy of 22.5%.},
  archive      = {J_ASOC},
  author       = {Morteza Karimian-Kelishadrokhi and Faramarz Safi-Esfahani},
  doi          = {10.1016/j.asoc.2024.111771},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111771},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of the time-distributed layer in the controller of memory-augmented neural networks to classify brain activities into motor imagery and motor execution},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dual-population coevolutionary algorithm for balancing
convergence and diversity in the decision space in multimodal
multi-objective optimization. <em>ASOC</em>, <em>162</em>, 111770. (<a
href="https://doi.org/10.1016/j.asoc.2024.111770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many multimodal multi-objective evolutionary algorithms (MMEAs) are effective in solving multimodal multi-objective problems (MMOPs), which have multiple equivalent Pareto optimal sets (PSs) mapping to the same Pareto optimal front (PF). Due to the existence of the global convergence-first mechanism, these MMEAs will remove the solutions that can improve the diversity of the decision space but have poor convergence and even lead to the loss of PS when encountering MMOPs with an imbalance between convergence and diversity in the decision space (MMOP-ICD) or an MMOP with a local PS (MMOPL). We propose a new dual-population coevolutionary algorithm to address these issues. The auxiliary population helps the main population locate areas where equivalent PSs may exist, and the main population focuses on balancing convergence and diversity in the decision space. When updating the auxiliary population, a strength local convergence quality (SLCQ) is used to explore the distribution of the equivalent PSs. When updating the main population, the new niche-based truncation strategy first deletes the solutions that contribute less to convergence. Then, a distance-based subset selection method balances the diversity between the decision and objective spaces. The comparison results show the overall performance of the proposed algorithm is significantly better than other state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Zhipan Li and Huigui Rong and Shengxiang Yang and Xu Yang and Yupeng Huang},
  doi          = {10.1016/j.asoc.2024.111770},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111770},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dual-population coevolutionary algorithm for balancing convergence and diversity in the decision space in multimodal multi-objective optimization},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective response strategies based on adaptive selection
for dynamic multi-objective evolutionary optimization. <em>ASOC</em>,
<em>162</em>, 111756. (<a
href="https://doi.org/10.1016/j.asoc.2024.111756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multiobjective optimization problems (DMOPs) change over time, which require Evolutionary algorithms (EA) track Pareto-optimal solution (PS) or/and Pareto-optimal front (PF) in a constantly change environment. Prediction-based algorithms are the most common method to solve DMOPs. However, a single elaborate predictor is not always suitable for extracting changing pattern of different DMOPs, and not to mention DMOPs with unpredictable changes. To overcome these limitations, a simple yet effective algorithm, response strategies based on adaptive selection (RSAS), are proposed in this paper. When a change occurs, RSAS provides diversified solutions by different proposed strategies, that is, a center-guided self-correcting prediction, an individual-based prediction, and a precision-controllable mutation. Based on the quality of their generated solution set, an adaptive selection mechanism can adjust the selection probability of these three strategies. Since RSAS consists of not merely two different prediction strategies but also a mutation strategy, which can be more responsive to predictable and unpredictable changes. To validate the performance of RSAS, DMOP benchmarks in CEC2018 with recommended configurations are adopted. Compared to four state-of-the-art algorithms, the experimental results show that RSAS is effective and efficient.},
  archive      = {J_ASOC},
  author       = {Xiaoli Li and Anran Cao and Kang Wang},
  doi          = {10.1016/j.asoc.2024.111756},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective response strategies based on adaptive selection for dynamic multi-objective evolutionary optimization},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective brainstorm optimization algorithm based on
bayesian inference learning automaton for solving CVRP with time
windows. <em>ASOC</em>, <em>162</em>, 111755. (<a
href="https://doi.org/10.1016/j.asoc.2024.111755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new Bayesian inference-based learning automaton with a multi-objective brainstorm algorithm to find the best solutions for combinatorial optimization problems such as vehicle routing problems. Initially, a robust deep k-means algorithm is used to solve the consumer allocation problem to various clusters. Then, a multi-objective brainstorm optimization algorithm is used to determine non-dominated solutions and thereby, the Pareto front of the solution space can be formed. The proposed method uses an external archive to store the attained non-dominated solutions. In addition, a reinforcement learning algorithm is used for improving the overall convergence of the brainstorm optimization algorithm, and it can be used to discover good designs for the parameters of the brainstorm algorithm and ensure the best transition decisions by balancing the exploration and exploitation phases. Here, the control of two phases can be handled by Q-table in reinforcement learning. Subsequently, the Bayesian inference-based learning automaton is employed to learn the optimal actions by placing the agent in a complex environment. The proposed method is implemented in the Python working platform, and the performance of the proposed approach is validated based on Solomon’s benchmark problems. Additionally, the effectiveness of the proposed method can be validated by comparing it with other algorithms in terms of Pareto optimal results, hypervolume, generational distance, inverted generation distance, space and spread. As a result, the proposed method has obtained better generational distance and inverted generation distance of 0.0669 and 0.0004 than other competitive methods, and it is superior for solving vehicle routing problems.},
  archive      = {J_ASOC},
  author       = {Harinandan Tunga and Samarjit Kar and Debasis Giri},
  doi          = {10.1016/j.asoc.2024.111755},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111755},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective brainstorm optimization algorithm based on bayesian inference learning automaton for solving CVRP with time windows},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information fusion-based bayesian optimized heterogeneous
deep ensemble model based on longitudinal neuroimaging data.
<em>ASOC</em>, <em>162</em>, 111749. (<a
href="https://doi.org/10.1016/j.asoc.2024.111749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fusion of multimodal longitudinal data is difficult but crucial for enhancing the accuracy of deep learning models for disease identification and helps provide tailored and patient-centric decisions. This study explores the fusion of multimodal data to detect the progression of Alzheimer’s disease (AD) using ensemble learning. We propose a heterogeneous ensemble framework of Bayesian-optimized time-series deep learning models to identify progressive deterioration of brain damage. Experimental results show that the heterogeneous ensemble of three models with patient’s temporal data outperforms all other variants of ensemble models by achieving an average performance of 95% for accuracy. We also propose a novel explainability approach, which enables domain experts and practitioners to better comprehend the model&#39;s final decision. The visual explainability of infected brain regions and the model&#39;s robustness is evaluated by our two medical domain experts showing its promising use in real medical environment. To evaluate the model’s generalizability and robustness, our optimized model is tested on a dataset with different distribution. The experiments demonstrate that the proposed model, which was trained on ADNI data, exhibits reliable generalization to NACC data with an average precision of 90%, recall of 91%, F1-score of 89%, AUC of 88%, and accuracy of 88%.},
  archive      = {J_ASOC},
  author       = {Nasir Rahim and Shaker El-Sappagh and Haytham Rizk and Omar Amin El-serafy and Tamer Abuhmed},
  doi          = {10.1016/j.asoc.2024.111749},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111749},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Information fusion-based bayesian optimized heterogeneous deep ensemble model based on longitudinal neuroimaging data},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep recurrent reinforcement learning approach for
enhanced MPPT in PV systems. <em>ASOC</em>, <em>162</em>, 111728. (<a
href="https://doi.org/10.1016/j.asoc.2024.111728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under partial shading condition (PSC), for efficient usage, solar photovoltaic (PV) systems are required to operate at global maximum power point (GMPP). A maximum power point tracking (MPPT) controller is usually employed for this task. In this work, for MPPT under PSC, a recurrent deep reinforcement learning (DRL) approach has been developed and investigated. The optimized neural network, augmented by long short-term memory (LSTM), is trained using the proximal policy optimization (PPO) DRL algorithm. The MPPT performance of the proposed scheme was assessed using the GMPP attainment accuracy (%), on several realistic static and dynamic test cases. The presented investigations revealed the proposed controller&#39;s average MPPT accuracy of 97.79 % for the considered random static PSCs. Similarly, average MPPT accuracies of 95.69 % under changing irradiance, 95.70 % under change in temperature and 95.77 % under changes in both irradiance and temperature were recorded. These accuracies are found to be significantly superior to recent reported works. The resulting performance enhancement is attributed to the inclusion of LSTM, which allowed the agent to retain information about past states and actions, enabling it to make informed decisions. Therefore, based on the presented investigations, it is concluded that the proposed PPO-LSTM approach is an excellent MPPT alternative for PV systems.},
  archive      = {J_ASOC},
  author       = {Archit Wadehra and Siddhant Bhalla and Vicky Jaiswal and K.P.S. Rana and Vineet Kumar},
  doi          = {10.1016/j.asoc.2024.111728},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111728},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep recurrent reinforcement learning approach for enhanced MPPT in PV systems},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Oversampling framework based on sample subspace
optimization with accelerated binary particle swarm optimization for
imbalanced classification. <em>ASOC</em>, <em>162</em>, 111708. (<a
href="https://doi.org/10.1016/j.asoc.2024.111708">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the need to generate synthetic minority class samples to extend minority classes, the SMOTE-based oversampling methods have been favored for class-imbalanced classification. They usually generate unnecessary noise when training data are not well separated. Although filtering-based oversampling methods are recognized as effective solutions for addressing noise generation through employing specific noise filters based on instance selection methods to remove suspicious noise, they suffer from the following issues: a) noise filters heavily rely on strong assumptions, causing low robustness to different datasets; b) noise filters are specially designed for a specific oversampling method, and are not easily extended to others; and c) noise filters have a relatively high time consumption. To address noise generation while overcoming the above issues a)-c), an oversampling framework based on sample subspace optimization with accelerated binary particle swarm optimization (OF-SSO-ABPSO) is proposed. OF-SSO-ABPSO is a wrapping framework compatible with almost all the oversampling methods. First, in the framework, a SMOTE-based method is used to generate synthetic minority class samples. Second, a novel accelerated binary particle swarm optimization (ABPSO) algorithm with a new search space reduction strategy, a new particle update mechanism, and a new fitness function is proposed. Third, a novel ABPSO-based sample subspace optimization (SSO-ABPSO) method is proposed and used as a noise filter to remove suspicious noise from the training set and synthetic minority class samples. Experiments prove that, a) OF-SSO-ABPSO can improve 6 representative SMOTE variations by addressing noise generation, and b) OF-SSO-ABPSO outperforms 7 state-of-the-art filtering-based oversampling methods.},
  archive      = {J_ASOC},
  author       = {Junnan Li},
  doi          = {10.1016/j.asoc.2024.111708},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111708},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Oversampling framework based on sample subspace optimization with accelerated binary particle swarm optimization for imbalanced classification},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High directivity microstrip patch antenna design using
binary ebola search optimization for radio frequency identification
application. <em>ASOC</em>, <em>162</em>, 111682. (<a
href="https://doi.org/10.1016/j.asoc.2024.111682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microstrip patch antenna has gained significant popularity in wireless communication field, because of its compact size, less profile, and ease of installation. It is widely used in various applications such as Radio-Frequency Identification (RFID) systems, where dependable and long-range communication is made possible by the great directivity and efficiency. In this manuscript, a High Directivity Microstrip Patch Antenna design using Binary Ebola Search Optimization for Radio Frequency Identification Application (MPA-RFID-BESO) is proposed.The design of microstrip patch antennas is the difficulty in achieving high directivity while maintaining compact size and low profile. This research explains design and application-specific optimisation of the Microstrip Patch Antenna (MPA) of biomedical utilizing Binary Ebola Search optimization (BESO) algorithm. Microstrip patch antenna is designed to function in double and multi band applications due to its low cost, light weight, and ease of installation. MPA is made with flawed ground construction to lessen cross-polarized radiation emitted by microstrip patches and to achieve the necessary radiation parameters. Here, the cost of the materials is reduced by using a liquid crystal polymer substrate, and aerial performance is enhanced by using the appropriate geometrical parameters. The small design of the BESO optimised improves the performance of antenna as 50 mm×50 mm compact size. The MATLAB program uses high frequency to do the simulation method. The proposed antenna design is a preferable option for applications involving Radio Frequency Identification (RFID). The performance metrics, such as radiation pattern, constant gain, directivity, bandwidth, and antenna efficiency and return loss are measured and compared to the existing techniques. The proposed MPA-BA-BESO design provides 10.51%, 12.85% and 16.04% higher bandwidth and 23.63%, 47.86% and 32.06% higher gain compared with existing designs, like Designing MPA utilizing Moth–Flame optimization approach (MPA-UWB-MFOA), Predicting Rectangular Patch Microstrip Antenna Dimension utilizing Machine Learning (MPA-ANN), Dual Band Antenna Design and Prediction of Resonance Frequency Utilizing Machine Learning Algorithms (DBAD-RF-CNN) respectively. The proposed technology serve as a better design alternative for microstrip patch antennas in communication systems to address biological applications.},
  archive      = {J_ASOC},
  author       = {K.R. Prabha and M. Jagadeeswari},
  doi          = {10.1016/j.asoc.2024.111682},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111682},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High directivity microstrip patch antenna design using binary ebola search optimization for radio frequency identification application},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Immersive situational analysis method based on generalized
augmented grid statistic. <em>ASOC</em>, <em>162</em>, 111651. (<a
href="https://doi.org/10.1016/j.asoc.2024.111651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The governance of Virtual-Reality Integration, which seamlessly merges the virtual world (metaverse) with the physical reality, represents an emerging approach to addressing perception and comprehension challenges in complex computational environments. Such Virtual-Reality Integration systems have the capability to streamline data analysis complexity, offer real-time visualization, and provide user-centric interaction, thereby delivering crucial support for data analysis and profound decision-making in complex computational settings. In this paper, we introduce a real-time perception and interaction methodology that combines computer vision with Virtual-Reality Integration technology. We employ the Grid-ORB algorithm-based approach for high-precision feature extraction and three-dimensional registration tracking on resource-constrained devices, enabling the perception of physical entities. Furthermore, we utilize the Kriging method, augmented with a drift term, to fill gaps in numerical physical space data, aiding users in observing real-world physical values and trend fluctuations. To facilitate a unified cognitive experience for data and knowledge, we devise a user-centric interaction interface using augmented reality technology. Within this interface, users can interact with charts and controls through methods such as eye movement and gestures. Finally, we validate our system within a real thermodynamics experimental environment, with results demonstrating a significant enhancement in user efficiency for comprehending data and knowledge within complex environments.},
  archive      = {J_ASOC},
  author       = {Yue Zhang and Guihua Shan and Justin Zuopeng Zhang and Abhishek Behl and Dong Tian},
  doi          = {10.1016/j.asoc.2024.111651},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111651},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Immersive situational analysis method based on generalized augmented grid statistic},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A swarm-intelligence based formulation for solving nonlinear
ODEs: γβII-(2+3)p method. <em>ASOC</em>, <em>162</em>, 111424. (<a
href="https://doi.org/10.1016/j.asoc.2024.111424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A reverse engineering approach is taken to develop efficient numerical techniques for solving nonlinear ODEs through the numerical solution of a particular problem in engineering. The problem is the dynamic analysis of a mechanical system under a severely irregular seismic signal or earthquake load. It is a challenging problem which requires strong formulation to be tackled. Its solution is a rich source of information that could be exploited through artificial intelligence ( AI ) and statistical computation. This work devises a new strategy to conduct such a task and construct a series of 3∼5 points high-precision time integrators. The integrators belong to single-step two-derivative class of methods. The interconnectivity relationships between the weights of the integrators are formulated by linear regression and heuristics. These relationships make it possible to generate a wide spectrum of ODE -solving techniques in a continuous weighting space. In an innovative attempt, strong Hermite interpolators are constructed to couple with the integrators and improve the accuracy of the presented algorithms. They precisely evaluate the instep points of the integrator. All these issues are appropriately collected in the presented methodology. Four classes of weighting rules are presented for the integrators: 1) Symmetry weighting rule ( SWR ) 2) Consistency weighting rule ( CWR ), 3) Fundamental weighting rule ( FWR ), and 4) Auxiliary weighting rule ( AWR ). The first two rules are adopted from outward form of the well-configured quadratures available in literature. The third and the most important one, the so-called FWR , is disclosed here for the given integrator. It formulates the correlation between the weights of an optimal integrator. Finding the min-error solution of the vibration equation guides us to the FWR . Evolutionary grey wolf optimizer ( GWO ), accompanied with a statistical multilinear regression, extracts the FWR . This research pioneers the application of swarm-intelligence in formulation of the FWRs for the given two-derivative integrator. Finally, all the weighting rules and Hermite interpolators are combined into some unified algorithms so-called γ β II − q + r P γβII−q+rP algorithms to efficiently cope with initial value ODEs . Great performance of the presented methods is demonstrated through the numerical examples.},
  archive      = {J_ASOC},
  author       = {Mehdi Babaei},
  doi          = {10.1016/j.asoc.2024.111424},
  journal      = {Applied Soft Computing},
  month        = {9},
  pages        = {111424},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A swarm-intelligence based formulation for solving nonlinear ODEs: γβII-(2+3)P method},
  volume       = {162},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Many-objective optimization based mutual feed scheduling for
energy system of integrated energy station. <em>ASOC</em>, <em>161</em>,
111803. (<a href="https://doi.org/10.1016/j.asoc.2024.111803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The battery storage system of the integrated energy station contains electric vehicles as the distributed energy storage node of the regional power system can help the regional power system to achieve peak shaving and valley filling. This paper proposes a many-objective optimization based mutual feed model of the energy system of the integrated energy station. The model includes multiple modules of electric vehicle (EV), wind turbine (WT), photovoltaic (PV) and battery energy storage system (BESS). BESS was modeled according to the two behaviors of EVs charging and swapping. The model ensures the role of peak shaving and valley filling for regional power, reduces the loss of battery life in the battery storage system of the integrated energy station, and reduces the operating cost of the battery storage system. In the optimized results, the regional power system load peak-to-valley ratio by 28.72 %, and the daily battery charge/discharge life loss is minimized to 0.00036 %. In addition, for known many-objective optimization problems, this paper also proposes a new meta-heuristic many-objective optimization algorithm the non-dominated sorting hunter prey optimization (NSHPO for short) and compared with other four many-objective optimization algorithms by three indexes (Distribution range, Hypervolume index, and Running time). The simulation results show that compared with other algorithms, the HV metrics of NSHPO are lower by about 1.56 %-7.26 %, Spread metrics are better by about 0.39 %-2.73 %, and the proposed NSHPO algorithm is better in terms of convergence and distributivity.},
  archive      = {J_ASOC},
  author       = {Xiang Liao and Jun Ma and Zhiqiang Jiang and Jianzhong Zhou},
  doi          = {10.1016/j.asoc.2024.111803},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111803},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Many-objective optimization based mutual feed scheduling for energy system of integrated energy station},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight genetic algorithm with variable neighborhood
search for multi-depot vehicle routing problem with time windows.
<em>ASOC</em>, <em>161</em>, 111789. (<a
href="https://doi.org/10.1016/j.asoc.2024.111789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-depot green vehicle routing problem with time windows considering customer satisfaction (MDGVRPTW-CS) was constructed. To optimize it, we proposed a lightweight genetic algorithm with variable neighborhood search (LGAVNS), which takes the genetic algorithm as the upper algorithm and the variable neighborhood algorithm as the neighborhood search algorithm. In our constructed algorithm, the neighborhood search operators were applied to the optimal gene instead of all genes. Additionally, an enhanced crossover operator was developed to facilitate the effective transmission of gene information. Finally, we extended and optimized the parameters of the algorithm. Experimental results demonstrated that our approach exhibited significant advantages in terms of optimization effectiveness and convergence speed compared to six state-of-the-art algorithms, particularly when addressing MDGVRPTW-CS in large-scale scenarios. Moreover, the lightweight nature of our algorithm can be utilized for optimizing both the number and location of depots while further exploring the optimization potential of MDGVRPTW-CS.},
  archive      = {J_ASOC},
  author       = {Yukang Su and Shuo Zhang and Chengning Zhang},
  doi          = {10.1016/j.asoc.2024.111789},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111789},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight genetic algorithm with variable neighborhood search for multi-depot vehicle routing problem with time windows},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Connection density based clustering: A graph-based density
clustering method. <em>ASOC</em>, <em>161</em>, 111779. (<a
href="https://doi.org/10.1016/j.asoc.2024.111779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a graph-based density clustering framework is proposed that detects the boundary points of clusters rather than cluster exemplars in high density regions. The framework introduces the connection density to measure the density relationship between points, which depends on both a density metric and a distance metric, and a weighted graph is constructed based on the connection density. By cutting off edges with low connection density, points located at the boundary region become isolated points from the rest of the weighted graph, and each connected subgraph forms an initial cluster. For the generated isolated points, a graph-based label propagation strategy is designed, the stable point in initial clusters with higher connection density with an isolated point preferentially propagate labels. Finally, a novel connection density based clustering algorithm is proposed, called CDBC, which can automatically identify clusters of arbitrary shapes. The experimental results show that the proposed method outperforms other advanced clustering algorithms on several synthetic and real-world datasets.},
  archive      = {J_ASOC},
  author       = {Feng Xu and Mingjie Cai and Qingguo Li and Jie Zhou and Hamido Fujita},
  doi          = {10.1016/j.asoc.2024.111779},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111779},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Connection density based clustering: A graph-based density clustering method},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual hesitant fuzzy set in multi-objective transportation
problems in time sequence frame work. <em>ASOC</em>, <em>161</em>,
111777. (<a href="https://doi.org/10.1016/j.asoc.2024.111777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision makers may have different information in different situations when making decisions about the same attributes, depending on the circumstances. A time-sequential framework is necessary for navigating these kinds of circumstances. We established the notion of a Time-sequential dual hesitant fuzzy set (TS-DHFS) to illustrate the significance of time sequence in decision-making with the aid of a dual hesitant fuzzy set (DHFS), one of the important tools to handle the uncertainty embedded in the real world. The proposed set describes the hesitant situations more accurately by inducing a time sequence framework. We defined the fundamental operations, two score function for ranking and a series distance measures for the proposed set. For the pragmatic application of our proposed set, we devised the mathematical model in three different types of Time-sequential dual hesitant fuzzy multi-objective transportation problems (TS-DHF-MOTPs). The proposed three models are classified based on the nature of the parameters such as transportation cost (TC), transportation time (TT), availability, and demand. The goal of the proposed transportation systems is TC and TT simultaneously in a single framework. We established an approach utilizing one of the ranking functions that have been proposed, followed by the fuzzy programming (FP) and weighted sum technique (WST), to optimize the constructed models. Additionally, numerical computations are also carried out to emphasize the efficiency and benefits of MOTP under the TS-DHF environment.},
  archive      = {J_ASOC},
  author       = {M.K. Sharma and Sadhna Chaudhary},
  doi          = {10.1016/j.asoc.2024.111777},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111777},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual hesitant fuzzy set in multi-objective transportation problems in time sequence frame work},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Potential corrections to grey wolf optimizer. <em>ASOC</em>,
<em>161</em>, 111776. (<a
href="https://doi.org/10.1016/j.asoc.2024.111776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grey wolf optimizer (GWO), a well-known powerful algorithm that simulates the leadership hierarchy and hunting mechanisms of grey wolves in nature, has garnered significant attention from researchers recently. However, parts of GWO formulations have been shown to be unfit. Moreover, GWO generates outstanding results only for functions with optimal values of 0. Thus, in this paper, the inherent flaws of GWO are discussed and corrected variants are proposed to resolve its inherent flaws. The three corrections to the original GWO proposal made in this study include eliminating coefficient vector C , eliminating the absolute sign for factor D , and introducing a current-to-prey approach. Based on a numerical validation using CEC2005 and CEC2019 benchmark functions, one of the proposed corrected variants performs comparably with other popular optimization algorithms and handles high-dimensional functions exceptionally well. Numerical simulations have elucidated the efficacy of the suggested corrections in mitigating the inherent flaws present in the original GWO. The corrected variants of GWO proposed in this study may be useful in developing future GWO applications and other GWO variants.},
  archive      = {J_ASOC},
  author       = {Hsing-Chih Tsai and Jun-Yang Shi},
  doi          = {10.1016/j.asoc.2024.111776},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111776},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Potential corrections to grey wolf optimizer},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-pronged feature reduction in spectral clustering with
optimized landmark selection. <em>ASOC</em>, <em>161</em>, 111775. (<a
href="https://doi.org/10.1016/j.asoc.2024.111775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is widely employed for clustering data points, particularly for non-linear and non-convex structures in high-dimensional spaces. However, it faces challenges due to the high computational cost of eigen decomposition operations and the performance limitations with high-dimensional data. In this paper, we introduce BVA_LSC, a novel spectral clustering algorithm designed to address these challenges. Firstly, we incorporate an advanced feature reduction stage utilizing Barnes-Hut t-SNE and a deep Variational Autoencoder (VAE) to efficiently reduce the dimensionality of the data, thereby accelerating eigen decomposition. Secondly, we propose an adaptive landmark selection strategy that combines the Grey Wolf Optimizer (GWO) with a novel objective function and K-harmonic means clustering. This strategy dynamically determines an optimal number of landmarks, enhancing the representativeness of the data and reducing the size of the similarity matrix. We assess the performance of our algorithm on various standard datasets, demonstrating its superiority over state-of-the-art methods in terms of accuracy and efficiency.},
  archive      = {J_ASOC},
  author       = {Alireza Rouhi and Asgarali Bouyer and Bahman Arasteh and Xiaoyang Liu},
  doi          = {10.1016/j.asoc.2024.111775},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111775},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-pronged feature reduction in spectral clustering with optimized landmark selection},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FIAO: Feature information aggregation oversampling for
imbalanced data classification. <em>ASOC</em>, <em>161</em>, 111774. (<a
href="https://doi.org/10.1016/j.asoc.2024.111774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification performance often deteriorates when machine learning algorithms are trained on imbalanced data. Although oversampling methods have been successfully employed to address imbalanced data, existing approaches have limitations such as information loss, difficulty in parameter selection, and boundary effects when using and calculating nearest neighbors and densities. Therefore, this study introduces a novel oversampling method called Feature Information Aggregation Oversampling (FIAO). FIAO leverages feature information, including feature importance, feature density, and standard deviation, to guide the oversampling process. Initially, the feature information is employed to partition features into suitable intervals for feature generation. Subsequently, features are generated within these intervals. Finally, the generated features are integrated into the minority class data to achieve effective oversampling. The key advantage of FIAO lies in its ability to fully exploit the intrinsic information carried by the features themselves, thus circumventing issues related to parameter selection and boundary effects. To assess its efficacy, extensive experiments were conducted on 12 widely used benchmark datasets, comparing the performance of the proposed method against 10 popular resampling methods across four commonly used classifiers. The experimental results show that the proposed FIAO method shows ideal results in multiple application scenarios and achieves optimal performance.},
  archive      = {J_ASOC},
  author       = {Fei Wang and Ming Zheng and Xiaowen Hu and Hongchao Li and Taochun Wang and Fulong Chen},
  doi          = {10.1016/j.asoc.2024.111774},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111774},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FIAO: Feature information aggregation oversampling for imbalanced data classification},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DPC-DNG: Graph-based label propagation of k-nearest
higher-density neighbors for density peaks clustering. <em>ASOC</em>,
<em>161</em>, 111773. (<a
href="https://doi.org/10.1016/j.asoc.2024.111773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) algorithm is a novel density-based clustering approach that effectively determines cluster centers from a decision graph and groups objects by assigning non-center objects to the same cluster as their nearest higher-density neighbor. Although DPC can allocate clusters of arbitrary shapes, its single-chain label propagation mechanism has the risk of “chain error”, where an object assigned an incorrect label causes its subordinates to also be assigned the same incorrect label. Hence, DPC is unable to effectively group objects that are located in overlapping areas between clusters, which leads to unsatisfactory clustering results. To address this issue, this study proposes the graph-based label propagation of k k -nearest higher-density neighbor for density peaks clustering (DPC-DNG). DPC-DNG extends the single-chain label propagation of DPC to a graph-based multi-chain label propagation that assigns labels to objects from their k k -nearest higher-density neighbors. First, based on k k -nearest higher-density neighbors and the selected cluster centers, a symmetric density-based neighbor graph (DNG) is constructed. Second, to assign labels to objects, a classic graph-based label propagation mechanism is utilized in conjunction with DNG. To validate our method, we carry out comprehensive experiments on 6 synthetic and 12 real datasets. Statistically speaking, the results show that our method has improved the clustering performance of DPC and exhibits promising performance over other state-of-the-art DPC-related methods.},
  archive      = {J_ASOC},
  author       = {Yan Li and Lingyun Sun and Yongchuan Tang},
  doi          = {10.1016/j.asoc.2024.111773},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111773},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DPC-DNG: Graph-based label propagation of k-nearest higher-density neighbors for density peaks clustering},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collaborative optimization decision making of cement
grinding process operational indicators considering dual dynamic
problem. <em>ASOC</em>, <em>161</em>, 111772. (<a
href="https://doi.org/10.1016/j.asoc.2024.111772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cement grinding process requires monitoring of unit power consumption and specific surface area indicators to improve production efficiency and product quality. The strong coupling between operational variables necessitates careful adjustment of these variables to prevent production from becoming instability. Furthermore, the continuous and time-dependent nature of the process makes this adjustment particularly challenging. To address this dual dynamic problem, we have developed a multi-objective optimization model, with global optimization as the desired outcome, to optimize unit power consumption and specific surface area. Our optimization algorithm, termed “Optimization Algorithm - Dynamic Search Space and Rolling Time Domain” (OA-DSTR), considers the problem of dynamic production process. First, the algorithm uses a dynamic search space strategy, allowing for changes in the constraint range of the operational variables and introducing a fluctuation coefficient (FC) to measure solution rationality. Second, to deal with the time-dependent problem, we have added a rolling time domain strategy, enabling real-time monitoring of the cement grinding process. The experimental results show that OA-DSTR not only ensures global optimization, but also realizes the tracking of dynamic conditions, improving the stability of the cement grinding process and achieving a lower FC value.},
  archive      = {J_ASOC},
  author       = {Yonghang Li and Tianqi Yang and Xiaochen Hao and Jieguang Yang and Quanwei Sun},
  doi          = {10.1016/j.asoc.2024.111772},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111772},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Collaborative optimization decision making of cement grinding process operational indicators considering dual dynamic problem},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decoding the scientific creative-ability of subjects using
dual attention induced graph convolutional-capsule network.
<em>ASOC</em>, <em>161</em>, 111769. (<a
href="https://doi.org/10.1016/j.asoc.2024.111769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an increasing demand of creative individuals in scientific research, innovation sectors of software industries and industrial research/development sectors. On the other hand, there are requirements of analytical minded people in investigation departments, academia and management sectors. Unfortunately, classification of individuals into creative and analytical categories based on their behavioral response is not easy. This paper makes an attempt to classify people into 4 categories: Analytical, High Creative, Medium Creative and Low Creative from their brain response during their participation in a creativity-test based on convergent problems. The proposed classification problem involves two main phases. In the first phase, a brain connectivity map is constructed from the electroencephalogram (EEG) response of the brain using Pearson’s correlation technique. In the second phase, a set of three centrality features, namely degree, closeness and betweenness are extracted from the connectivity map and fed to a classifier model for categorizing the afore-said class labels. The classifier model here synergistically combines one Graph Convolution Network (to abstract the brain connectivity-based centrality features) and one Capsule Network (to undertake the classification task) to develop the proposed Dual Attention Induced Graph Convolutional-Capsule network (DAIGC-CapsNet). The novelty of the proposed classifier model lies in the dual attention module and a new routing algorithm. The dual attention modules includes a) a Mish Induced Attention Module (MI-AM) to guide the graph convolution layers to focus on the most significant node attributes, and b) a Fused Attention Module (F-AM) to ensure the transmission of the most relevant predictions from the primary capsule to the class capsule layers. The latter attention module combines the effects of two sub-modules (channel and spatial) that concentrate on determining &quot;what&quot; and &quot;where&quot; to prioritize within the channel and spatial dimensions of the primary capsules. Lastly, the coupling between the primary and class capsule layers is strengthened by a Sparsemax based routing algorithm. Experiments conducted yield fruitful and definitive outcomes that substantiate the effectiveness of the proposed framework with respect to its conventional counterparts. Moreover, statistical validation of the proposed classifier using Friedman’s test also proves its efficacy compared to its competitors.},
  archive      = {J_ASOC},
  author       = {Sayantani Ghosh and Amit Konar},
  doi          = {10.1016/j.asoc.2024.111769},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111769},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decoding the scientific creative-ability of subjects using dual attention induced graph convolutional-capsule network},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Miner selection in an internet of medical things framework
using fuzzy logic. <em>ASOC</em>, <em>161</em>, 111768. (<a
href="https://doi.org/10.1016/j.asoc.2024.111768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-enabled Internet of Medical Things (IoMT) is a secure, transparent, and decentralized network of medical devices which facilitates a patient-centric healthcare system for distributed exchange and management of medical records. The existing consensus protocols designed for a blockchain network such as Proof-of-Work, Proof-of-Stake, Proof-of-Negotiation, Proof-of-Authority, and Proof-of-X-Repute lack fast processing and security. This paper presents a scalable blockchain-enabled IoMT framework that uses gateway nodes to improve the performance. A novel hybrid consensus mechanism namely Proof-of-Context (PoCt) is also proposed for blockchain-enabled IoMT environment. The proposed PoCt consensus is a fast, reliable, and secure mechanism for block mining which consists of a fuzzy-based miner selection and a block validation process with multiple parameters. Fuzzy-based miner selection is done on the basis of context factor of a node, which is computed using the impact score, vote count and trust value. The trusted miner nodes are responsible for block generation and validation. The most trusted and efficient miner node having the highest neighbour density is being selected for block generation. The other miner nodes are responsible for block validation to prevent various malicious activities. The performance analysis of the PoCt consensus shows that it is comparatively 24% more efficient, 36% faster, 1.8% more reliable, and 65% less traffic overhead than fuzzy-based super leader nomination in PoS consensus mechanism, 65% faster than random-voting and game-theory based miner selection, 25% faster than Proof-of-Negotiation consensus, 6.1% more reliable and 58% less traffic overhead than Proof-of-Authority consensus mechanism for the network of 500 nodes. Security analysis of PoCt consensus algorithm ensures its resilience against various known mining attacks.},
  archive      = {J_ASOC},
  author       = {Namrata Singh and Ayan Kumar Das and Ditipriya Sinha},
  doi          = {10.1016/j.asoc.2024.111768},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111768},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Miner selection in an internet of medical things framework using fuzzy logic},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way graph convolutional network for multi-label
classification in multi-label information system. <em>ASOC</em>,
<em>161</em>, 111767. (<a
href="https://doi.org/10.1016/j.asoc.2024.111767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graph Convolutional Neural Networks (GCNs) have demonstrated a powerful capacity for relation processing. To improve the representation learning capability of GCNs, numerous studies have begun to excavate latent topological graphs based on node features, aiming to obtain richer representations of nodes. However, these graph mining methods based on global metrics, such as Euclidean distance, lack descriptions of uncertainty and fine granularity, leading to the loss of structural information, which affects the final representation of nodes. Therefore, this paper proposes the Three-way Graph Convolutional Neural Network (TW-GCN), which is based on multi-granularity and three-way decision. First, from the perspective of multi-granularity, each attribute is analyzed as a granule, and we establish relations of advantage, disadvantage, and uncertainty for objects under each attribute granule. Three-way relation captures the complex connections between nodes from multiple perspectives, improves the comprehensive understanding of graph data, and avoids the information loss caused by traditional two-way processing. Second, a three-way graph convolutional layer is constructed to capture uncertainty and ambiguity by performing graph convolution on different topological graphs. The final node representation is obtained through message passing on the node’s neighbor graph, which considers not only the global structure, but also effectively captures local structural features. Finally, we employ the TW-GCN for multi-label classification within information systems to validate the model’s validity based on multiple indicators.},
  archive      = {J_ASOC},
  author       = {Bin Yu and Hengjie Xie and Yu Fu and Zeshui Xu},
  doi          = {10.1016/j.asoc.2024.111767},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111767},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way graph convolutional network for multi-label classification in multi-label information system},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of extracting biomass energy using a strategic
decision support system. <em>ASOC</em>, <em>161</em>, 111766. (<a
href="https://doi.org/10.1016/j.asoc.2024.111766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity significantly influences the long-term development of society. Traditional power generation technologies have encountered numerous challenges. The rising demand for power generation and the scarcity of fossil fuel resources have prompted the adoption of renewable energy sources. Biomass is a renewable energy source that can be utilized to generate power and promote a more sustainable environment. This paper presents improved Pythagorean Probabilistic Hesitancy Fuzzy Set (PPHFS) multi-criteria information fusion processes. The development of connection numbers (CN) and the proposal of a distance measure between CNs have been initiated. The suggested solution involves a distance measure based on the CN score function. Set-pair analysis (SPA) is a prior uncertainty theory that incorporates three elements of CN and overlaps with the PPHFS. Biomass energy generation poses a fuzzy multi-criteria decision-making (MCDM) problem that requires a wide range of resources for evaluation. The study introduces a novel Projection Ranking by Similarity to the Referencing Vector (PRSRV) and Simultaneous Evaluation of Criteria and Alternatives (SECA) to effectively handle data ambiguity and generate weights for all criteria in the MCDM problem. The criteria’s weights were found to fall within the range of 0.2122 and 0.0232. The proposed method indicates that animal residues are the most desirable resource for bioenergy production, followed by municipal solid residues, agricultural residues, forest and crop residues, industrial residues, and algae. The research aims to assist decision-makers and organizations in evaluating and prioritizing various energy sources on a sustainable scale. The method presented exhibits superior data adaptability compared to other MCDM approaches.},
  archive      = {J_ASOC},
  author       = {Krishnan Suvitha and Samayan Narayanamoorthy and Michael Sandra and Dragan Pamucar and Vladimir Simic and Daekook Kang},
  doi          = {10.1016/j.asoc.2024.111766},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111766},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of extracting biomass energy using a strategic decision support system},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The weight fuzzy judgment method for the benchmarking
sustainability of oil companies. <em>ASOC</em>, <em>161</em>, 111765.
(<a href="https://doi.org/10.1016/j.asoc.2024.111765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The environmental, social, and economic challenges associated with the massive activities of the oil and gas industry require analysis and evaluation of companies&#39; sustainability priorities. Evaluating the environmental performance of oil companies requires the application of strict mathematical tools and models that provide rigorous results. Multi-criteria decision-making (MCDM) is a rigorous tool that evaluates the performance and practices of oil and sustainable energy companies. However, classical multi-criteria decision analysis has problems related to inconsistency because of the subjective nature of pairwise comparisons. This study responded to the literature call by utilizing the weight fuzzy judgment method (WFJM) to determine the weight coefficients of criteria with zero consistency. Such a method employed criteria values and expert judgment for decision-making to create a bridge between the mathematical approach and the human approach. This study aims to develop a solid decision support system by evaluating and benchmarking 11 companies utilizing the fuzzy judgment method based on fuzzy theory and the VIekriterijumsko KOmpromisno Rangiranje (VIKOR) method to determine sustainability priorities for oil and gas companies. The oil companies’ criteria were efficiently weighted by the fuzzy judgment method, and the organizational capabilities represented the greatest weight value of 0.128, whilst the low-weight criteria were the high costs of technologies at 0.080. Besides, the benchmarking results for oil companies revealed that COM 8 was the best, whilst COM 11 was the worst. The fuzzy judgment technique is a unique approach to solving complicated decision-making problems, and its influence can be significant. Furthermore, this method can be applied in sectors other than energy, including banking, medical services, and engineering, where decision-making is not straightforward and requires a systematic approach.},
  archive      = {J_ASOC},
  author       = {Yousif Raad Muhsen and Salah L. Zubaidi and Nor Azura Husin and Alhamzah Alnoor and Darko Božanić and Khalid S. Hashim},
  doi          = {10.1016/j.asoc.2024.111765},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111765},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The weight fuzzy judgment method for the benchmarking sustainability of oil companies},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized and robust privacy-preserving model using
blockchain-enabled federated deep learning in intelligent enterprises.
<em>ASOC</em>, <em>161</em>, 111764. (<a
href="https://doi.org/10.1016/j.asoc.2024.111764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Federated Deep Learning (FDL), multiple local enterprises are allowed to train a model jointly. Then, they submit their local updates to the central server, and the server aggregates the updates to create a global model. However, trained models usually perform worse than centralized models, especially when the training data distribution is non-independent and identically distributed (non-IID). Because non-IID data harms the accuracy and performance of the model. Second, due to the centrality of federated learning (FL) and the untrustworthiness of enterprises, traditional FL solutions are vulnerable to security and privacy attacks. Therefore, to tackle this issue, we propose F E D A N I L FEDANIL , a secure blockchain-enabled Fed erated Deep Le A r ni ng Mode l that improves enterprise models’ decentralized, performance, and tamper-proof properties, including two main phases. The first phase is proposed to address the non-IID challenge (label and feature distribution skew). In this phase, local models with similar data distributions are grouped into homogeneous clusters using the cosine similarity (CS) and affinity propagation (AP) techniques. Then, for each homogeneous cluster, Wasserstein Generative Adversarial Networks (WGAN) are used to deal with label and feature distribution skew. The second phase was adopted to address security and privacy concerns against poisoning and inference attacks via three steps. In the first step, data poisoning attacks are prevented by using CS. Then, in the second step, collude attacks were prevented by randomly selecting enterprises in the consortium blockchain. Finally, in the third step, model poisoning, membership inference, and reconstruction attacks have been prevented using the CKKS Fully Homomorphic Encryption (CKKS-FHE) technique and consortium blockchain. Extensive experiments were conducted using the Sent140, Fashion-MNIST, FEMNIST, and CIFAR-10 new real-world datasets to evaluate FedAnil’s robustness and performance. The simulation results demonstrate that F E D A N I L FEDANIL satisfies FDL privacy-preserving requirements. In terms of convergence analysis, the model parameter obtained with the F E D A N I L FEDANIL converges to the optimum of the model parameter. In addition, it performs better in terms of accuracy (more than 11, 15, and 24%) and computation overhead (less than 8, 10, and 15%) compared with baseline approaches, namely S H I E L D F L SHIELDFL , R V P F L RVPFL , and RFA, respectively. The F E D A N I L FEDANIL source code can be found on GitHub. 1},
  archive      = {J_ASOC},
  author       = {Reza Fotohi and Fereidoon Shams Aliee and Bahar Farahani},
  doi          = {10.1016/j.asoc.2024.111764},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111764},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decentralized and robust privacy-preserving model using blockchain-enabled federated deep learning in intelligent enterprises},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A weighted integration method based on graph representation
learning for drug repositioning. <em>ASOC</em>, <em>161</em>, 111763.
(<a href="https://doi.org/10.1016/j.asoc.2024.111763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-consuming and expensive nature of traditional drug discovery necessitates a cost-effective approach to facilitate disease treatment. Drug repositioning, discovering innovative applications for existing drugs, has become a viable strategy that is essential for facilitating drug discovery due to its cost-effectiveness and shorter development cycle. While existing methods assume neighbors of the target node are independent, they neglect potential neighbor interaction features. We propose a weighted integration method based on graph representation learning for drug repositioning (called WIGRL) to comprehensively consider neighborhood features and neighbor interaction features, with encoders designed for similarity networks of drugs and diseases, respectively, and a network of associations between the two. Firstly, WIGRL utilizes graph convolutional network modules to obtain the neighborhood properties of nodes in similar networks. Secondly, neighbor interaction properties in similar networks are captured by graph attention network modules. Next, projection encoders are introduced to represent the association features in the association network. Finally, a more representative, unified vector is formed by simultaneously fusing information from diverse networks. After that, the decoder receives this vector to predict associations. The findings of the experiments conducted on the Fdataset, Cdataset, and LRSSL benchmark datasets demonstrate that WIGRL outperforms the existing SOTA approaches in identifying the most real positive associations and obtains the most outstanding average metrics (AUROC of 0.9331 and AUPR of 0.5654). Notably, in the case study, WIGRL discovered new associations not recorded in the dataset, validated by clinical trials and authoritative sources. Additionally, it identified novel therapeutic candidates for two neurodegenerative diseases.},
  archive      = {J_ASOC},
  author       = {Haojie Lian and Pengju Ding and Chao Yu and Xinyu Zhang and Guozhu Liu and Bin Yu},
  doi          = {10.1016/j.asoc.2024.111763},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111763},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A weighted integration method based on graph representation learning for drug repositioning},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A gradual self distillation network with adaptive channel
attention for facial expression recognition. <em>ASOC</em>,
<em>161</em>, 111762. (<a
href="https://doi.org/10.1016/j.asoc.2024.111762">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) is widely applied in various real-world applications, such as security, human–computer interaction, and healthcare, leading to high demands for fast and accurate FER techniques. However, it remains a challenging task to design effective and efficient FER techniques that meet the requirements of real-world applications. In this paper, we propose a lightweight Gradual Self Distillation Network with adaptive channel attention (GSDNet) for accurate and efficient FER. Especially, we propose a novel gradual self distillation strategy which enables the network to learn from itself in a gradual and adaptive way. Specifically, the proposed GSDNet consists of a feature extraction backbone with multiple basic blocks. We plug an adaptive classifier after each basic block. Every two neighbor classifiers form “student &amp; teacher” relationship for gradual knowledge distillation. In particular, the gradual self distillation strategy enables the transfer of key knowledge from deep to shallow layers gradually. Besides, an Adaptive Channel Attention Module (ACAM) is designed to enhance the representation capability of each block for adaptively capturing important features and achieve better FER performance. Extensive experiments on three real-world datasets show that the proposed method GSDNet outperforms the baselines, including state-of-the-art methods. Specifically, the accuracy of GSDNet on the RAF-DB, Affect-net, and FERPlus datasets is 90.91%, 66.11%, and 90.32%, separately. The code is available at https://github.com/Emy-cv/GSDNet .},
  archive      = {J_ASOC},
  author       = {Xin Zhang and Jinlin Zhu and Dongjing Wang and Yueyun Wang and Tingting Liang and Hongbo Wang and Yuyu Yin},
  doi          = {10.1016/j.asoc.2024.111762},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111762},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gradual self distillation network with adaptive channel attention for facial expression recognition},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault diagnosis method via one vs rest evidence classifier
considering imprecise feature samples. <em>ASOC</em>, <em>161</em>,
111761. (<a href="https://doi.org/10.1016/j.asoc.2024.111761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key task of fault diagnosis is to establish the nonlinear mapping relationship between fault feature sequences (FFSs) and fault modes (FMs). Therefore, it is usually necessary to detect transit jump points of FFS to separate feature samples with the different trends. These samples and the their fault labels are combined to form training samples for a fault diagnosis model (FDM). However, due to changes in fault status and uncertainty in measurement, some feature samples will imprecisely point to multiple FMs (fault labels), hardly adopted in FDM. In order to solve such imprecision problem, this paper presents an information fusion method via One vs Rest (OvR) evidence classifiers. For each FFS, the multiple OvR evidence classifiers are designed to model the imprecise relationship between the samples and FMs. Then, a two-level fusion framework is proposed to integrate the evidence of all FFSs. By using the intersection operation between One (one FM) and Rest (the other FMs), the imprecision can be significantly reduced. A rotating machinery fault diagnosis experiment is given to illustrate that the proposed method have better fault classification accuracy compared with the traditional FDMs including SVM, BPNN, KNN, FCNN, RF and Bayes.},
  archive      = {J_ASOC},
  author       = {Xiaobin Xu and Haohao Guo and Zhenjie Zhang and Pengfei Shi and Wenguang Huang and Xiaoding Li and Georg Brunauer},
  doi          = {10.1016/j.asoc.2024.111761},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111761},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault diagnosis method via one vs rest evidence classifier considering imprecise feature samples},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting and compensating for small-sample thermal
information data in precision machine tools: A spatial-temporal
interactive integration network and digital twin system approach.
<em>ASOC</em>, <em>161</em>, 111760. (<a
href="https://doi.org/10.1016/j.asoc.2024.111760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thermal errors in ball screws present complex spatial-temporal characteristics and exhibit persistent long-term memory, significantly affecting the machining accuracy of the whole machine tool in applications. Deep learning has emerged as a promising approach to predict these errors using thermal data. However, the scarcity of extensive thermal datasets, typically due to the prohibitive costs and time required for experimental acquisition, hampers the comprehensive exploitation of spatial-temporal and long-term memory attributes. Moreover, the current thermal error compensation system does not support the deployment of deep learning-based thermal error models due to its weak real-time performance. These limitations lead to suboptimal prediction accuracy and inadequate error compensation. Addressing this challenge, a novel spatial-temporal interactive integration network is proposed, and this is a sophisticated model that synergistically blends a time memory gate and a spatial-temporal fusion gate within an advanced attention-based spatial-temporal graph convolutional framework. Our network ingeniously leverages spatial data to anchor long-term memory while employing temporal data for selective memory feature extraction, facilitating a refined integration of spatial-temporal features. Concurrently, the integration of gated recurrent units and a time attention layer meticulously extracts and enhances temporal features, bolstered by our innovative time memory gate, which is adept at handling small-sample scenarios and refining thermal error predictions. Critically, the digital twin system for thermal error compensation is constructed to improve the system’s real-time performance. The integration of this network into the digital twin system marks a pivotal advancement in thermal error compensation. Our empirical results underscore the system&#39;s remarkable robustness and superior predictive accuracy, demonstrating a significant reduction in positioning and machining errors (over 90% and 80% respectively) even with constrained thermal data inputs. These advancements not only delineate a substantial leap in predictive accuracy but also underscore our contribution to reducing operational costs and enhancing the efficacy of machine tools.},
  archive      = {J_ASOC},
  author       = {Zheng Wu and Chi Ma and Lang Zhang and Hongquan Gui and Jialan Liu and Zijie Liu},
  doi          = {10.1016/j.asoc.2024.111760},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111760},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting and compensating for small-sample thermal information data in precision machine tools: A spatial-temporal interactive integration network and digital twin system approach},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recurrent ensemble random vector functional link neural
network for financial time series forecasting. <em>ASOC</em>,
<em>161</em>, 111759. (<a
href="https://doi.org/10.1016/j.asoc.2024.111759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series forecasting is crucial in empowering investors to make well-informed decisions, manage risks effectively, and strategically plan their investment activities. However, the non-stationary and non-linear characteristics inherent in time series data pose significant challenges when accurately predicting future forecasts. This paper proposes a novel Recurrent ensemble deep Random Vector Functional Link (RedRVFL) network for financial time series forecasting. The proposed model leverages randomly initialized and fixed weights for the recurrent hidden layers, ensuring stability during training. Furthermore, incorporating stacked hidden layers enables deep representation learning, facilitating the extraction of complex patterns from the data. The proposed model generates the forecast by combining the outputs of each layer through an ensemble approach. A comparative analysis was conducted against several state-of-the-art models over financial time-series datasets, and the results demonstrated the superior performance of our proposed model in terms of forecasting accuracy and predictive capability.},
  archive      = {J_ASOC},
  author       = {Aryan Bhambu and Ruobin Gao and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2024.111759},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111759},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recurrent ensemble random vector functional link neural network for financial time series forecasting},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A linear directional optimum weighting (LDOW) approach for
parallel hybridization of classifiers. <em>ASOC</em>, <em>161</em>,
111754. (<a href="https://doi.org/10.1016/j.asoc.2024.111754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybridization of classifiers can often yield outperformed performance compared to its best individual component and mostly have more generalization ability. The majority of combined classifiers reported in the literature benefit the parallel or ensemble topology. The performance of such parallel hybrid classifiers significantly depends on their applied weighting approaches and the accuracy strongly relies on the classifiers&#39; weights. In the literature, several different weighting mechanisms have been developed to yield a higher classification rate, which can be generally categorized into three main categories individual classifiers, averaging-based, and optimization-based algorithms. Although these weighting mechanisms have been commonly and frequently used for parallel hybridization, none of them can guarantee that their obtained classification rate will be optimum. In addition, due to the use of iterative procedures, especially by meta-heuristic optimization-based algorithms, their computational time and cost are always unsatisfactory. In this paper, a linear direct optimal weighting (LDOW) approach is proposed in which it can be generally guaranteed that the optimum classification rate will be directly reached. In this way, the proposed approach can achieve the highest classification rate by the desired computational time and cost and both mentioned limitations of currently-used weighting algorithms can be simultaneously lifted. Empirical results of twenty-three different benchmark data sets from six dissimilar domains indicate that the proposed LDOW approach can yield more classification rate in all data sets and can averagely improve 5.36% the performance of its traditional linear direct non-optimal version with the same components. Furthermore, the proposed LDOW approach can averagely improve 6.87% the classification rate of the averaging weighting algorithms that are similar to the proposed algorithm are direct ones. In addition, the proposed model can even improve 2.80% the classification rate of nonlinear intelligent weighting algorithms, on average. Whereas, the proposed approach is a linear model and its complexity and computational cost are significantly lower than these nonlinear intelligent algorithms. Numerical results also indicate that the proposed model can averagely improve 6.08% the classification rate of meta-heuristic-based weighting algorithms. While the proposed model is a direct model and its computational cost is meaningfully lower than these iterative algorithms. Thus, in theory as well as in practice, it can be inferred that the proposed LDOW approach can be an efficient alternative weighting method for parallel hybridization in the classification field. This is particularly relevant when more accurate results are required or for big data situations where computational time and cost are critical factors to consider.},
  archive      = {J_ASOC},
  author       = {Zahra Hajirahimi and Mehdi Khashei and Negar Bakhtiarvand},
  doi          = {10.1016/j.asoc.2024.111754},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111754},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A linear directional optimum weighting (LDOW) approach for parallel hybridization of classifiers},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pythagorean fuzzy z̃-number-based neutrality aggregation
model for AI-enabled energy efficiency management. <em>ASOC</em>,
<em>161</em>, 111753. (<a
href="https://doi.org/10.1016/j.asoc.2024.111753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI)-enabled energy efficiency in buildings leverages AI to optimize energy consumption, enhancing sustainability and aiding decision-makers in selecting cost-effective and environmentally friendly solutions. Pythagorean fuzzy Z ̃ Z̃ number sets, capable of handling high uncertainty, play a crucial role in representing fuzzy information in decision-making. This research introduces novel operational laws and weighted aggregation operators, focusing on neutral addition and scalar multiplication. Comprehensive investigations into the properties of these laws lead to groundbreaking aggregation operators tailored for Pythagorean fuzzy Z ̃ Z̃ number information. These include weighted, ordered weighted, and hybrid neutral averaging aggregation operators, along with a novel approach for calculating attribute weights. Relationships and characteristics of these operators are explored, and an algorithm for solving multiple attribute group decision-making problems is provided. A practical example illustrates the approach’s superiority, supported by comparative results.},
  archive      = {J_ASOC},
  author       = {Shahzad Noor Abbasi and Shahzaib Ashraf and Maria Akram and Chiranjibe Jana and Vladimir Simic and Dragan Pamucar},
  doi          = {10.1016/j.asoc.2024.111753},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111753},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pythagorean fuzzy z̃-number-based neutrality aggregation model for AI-enabled energy efficiency management},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble feature selection using q-rung orthopair hesitant
fuzzy hamacher, einstein and dombi aggregation operators. <em>ASOC</em>,
<em>161</em>, 111752. (<a
href="https://doi.org/10.1016/j.asoc.2024.111752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article aims in addressing the issue of ensemble feature selection problem by modeling it as a multi-criteria decision making technique. To build such a model, initially, aggregation operators such as weighted arithmetic, weighted geometric, ordered weighted arithmetic, ordered weighted geometric aggregation of Hamacher, Einstein and Dombi operators in the q-rung orthopair hesitant fuzzy environment are proposed. The properties of these operators are also discussed to provide a more elaborate understanding of them. Such an approach to ensemble feature selection has not yet been carried out in the literature which adds to the novelty of our work. Validation is provided through comparison of the performance metrics with existing and base feature selection methods and also by carrying out statistical tests. Through this article, a model for ensemble feature selection incorporating the advantages of Einstein, Hamacher, Dombi and q-rung orthopair hesitant fuzzy set was constructed which was reflected in the results.},
  archive      = {J_ASOC},
  author       = {S. Kavitha and K. Janani and S.S. Mohanrasu and J. Satheeshkumar and T. Amudha and R. Rakkiyappan},
  doi          = {10.1016/j.asoc.2024.111752},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111752},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble feature selection using q-rung orthopair hesitant fuzzy hamacher, einstein and dombi aggregation operators},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving biobjective traveling thief problems with
multiobjective reinforcement learning. <em>ASOC</em>, <em>161</em>,
111751. (<a href="https://doi.org/10.1016/j.asoc.2024.111751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an end-to-end multiobjective reinforcement learning (MORL) approach to solve the biobjective traveling thief problems (TTP). A TTP involves a thief visiting cities and selecting items to maximize profit while minimizing travel time within a knapsack’s capacity. The study evaluates combinations of two architectures, namely the pointer network (PN) and attention mechanism (AM), with three MORL methods: deep reinforcement learning multiobjective algorithm (DRLMOA), multi-sample Pareto hypernetwork (PHN), and manifold-based policy search (MBPS). However, PN and AM cannot be directly used to predict two different sequences simultaneously: the city tour and the item selection. Therefore, a solution encoding and decoding scheme is proposed to solve TTP without substantially modifying PN and AM. The methods are trained on only small randomly generated problem instances based on Eil76 instances, and their performance is evaluated on various problem instances. The state-of-the-art non-dominated sorting-based customized random-key genetic algorithm (NDS-BRKGA) serves as the baseline. The experimental study demonstrates a competitive performance of the proposed methods compared to the baseline, particularly in instances with a high number of items. The proposed methods, especially PN-DRLMOA and AM-DRLMOA, also show promising generalization capabilities on different and larger graphs. Lastly, the proposed MORL methods significantly outperform NDS-BRKGA in terms of the solution generation running time.},
  archive      = {J_ASOC},
  author       = {Gemilang Santiyuda and Retantyo Wardoyo and Reza Pulungan},
  doi          = {10.1016/j.asoc.2024.111751},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111751},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving biobjective traveling thief problems with multiobjective reinforcement learning},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continual three-way decisions via knowledge transfer.
<em>ASOC</em>, <em>161</em>, 111750. (<a
href="https://doi.org/10.1016/j.asoc.2024.111750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional credit risk prediction, updates the model from scratch for time-changing data streams, resulting in significant computational, time, and storage consumption. For resource efficiency with acceptable prediction precision, we advocate incrementally training models by transferring knowledge and delayed decisions of low-confidence samples. To this end, we propose a novel multi-stage prediction approach, called Continual Three-Way Decisions, which focuses on knowledge accumulation and thresholds optimization between tasks from the perspective of uncertainty reduction. We utilize elastic weight consolidation to take previous important parameter information as transferred knowledge. Subsequently, we unify continual learning and dynamic three-way decisions through thresholds-based joint mechanism. Finally, the extensive experiments verify the efficacy of our proposed method, demonstrating up to a 38% reduction in running time and a 28% decrease in memory usage compared to isolated and static methods.},
  archive      = {J_ASOC},
  author       = {Xin Yang and Meijun Wu and Longsheng Chen and Gang Kou},
  doi          = {10.1016/j.asoc.2024.111750},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111750},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Continual three-way decisions via knowledge transfer},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image inpainting based on GAN-driven structure- and
texture-aware learning with application to object removal.
<em>ASOC</em>, <em>161</em>, 111748. (<a
href="https://doi.org/10.1016/j.asoc.2024.111748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel deep learning-based image inpainting framework consisting of restoring image structure and reconstructing image details from corrupted images is proposed. Most image inpainting methods in the literature aim at restoring image details, outlines, and colors, simultaneously, which may suffer from blurring, deformation, and unreasonable content recovery due to interference among various information. To solve these problems, a two-stage image inpainting deep neural network based on GAN (generative adversarial network) architecture is proposed. The proposed inpainting framework consists of two modules: (1) the first stage, called the structure-aware learning stage, aims at learning a GAN-based structure restoration network, focusing on recovering the low-frequency image component, including colors and outlines of the missing regions of the input corrupted image; and (2) the second stage, called the texture-aware learning stage, aims at learning a GAN-based detail refinement network, focusing on rebuilding the high-frequency image details and texture information. In particular, we also propose to remove details from the training images to better train the structure restoration network to avoid inadequate image structure recovery induced by richer image textures, where the detail reconstruction task is left to the second stage. This strategy achieves to balance the workload between the two stages and the image quality can be progressively enhanced through the two stages. Experimental results have shown that the proposed deep inpainting framework quantitatively and qualitatively achieves state-of-the-art performance on the well-known datasets, including the CelebA, Places2, and ImageNet datasets, compared with existing deep learning-based image inpainting approaches. More specifically, in terms of the two well-known image quality assessment metrics, PSNR (peak signal-to-noise ratio) and SSIM (structural similarity), the improvement percentage of the proposed method, compared with the baseline approach, respectively, ranges from 3.23 % to 11.12 %, and 1.95 % to 13.39 %. The improvements have been shown to stably and significantly outperform the compared state-of-the-art methods in most types of inpainting mask. We also show that the proposed method is applicable to image editing in object removal from a single image.},
  archive      = {J_ASOC},
  author       = {Chia-Hung Yeh and Hsin-Fu Yang and Mei-Juan Chen and Li-Wei Kang},
  doi          = {10.1016/j.asoc.2024.111748},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111748},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image inpainting based on GAN-driven structure- and texture-aware learning with application to object removal},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gradient descent algorithm for SNN with time-varying
weights for reliable multiclass interpretation. <em>ASOC</em>,
<em>161</em>, 111747. (<a
href="https://doi.org/10.1016/j.asoc.2024.111747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretation of the prediction is vital for mission critical tasks. Accurate interpretation relies upon the generalization accuracy of the model. In this paper, we propose a modified gradient descent learning algorithm to improve the generalization ability of a Spiking Neural Network with time-varying weights (SNN-t). This algorithm is referred to as GradST, can help towards improving the interpretation of multiclass classification problems. We have transformed the SNN-t to a Generalized Additive Model (GAM) to provide interpretation. The resultant Spiking Additive Model (SAM) has the generalization ability of SNN-t and the interpretable characteristics of GAM for multiclass problems. We also propose a post-processing method to enable better visualization of multiple shape functions of GAMs, towards better relative interpretation for multiclass classification problems. The post-processing method utilizes the properties of multiclass GAMs to visually modify the shape functions to establish the importance of the feature in multiclass setting. We first evaluate the performance of SNN-t, trained with GradST and the SAM generated from it, on large public datasets. The SNN-t trained with GradST has better generalization accuracy than other SNN-t classifier and consequently, the SAM generated from it has better generalization accuracy than other state-of-the-art multiclass GAMs. Improved accuracy in SAM implies more reliable interpretation. Then, we evaluate the proposed post-processing method for multiclass GAMs to provide relative interpretation. It is observed that relative interpretation of multiclass GAM is more meaningful and reliable.},
  archive      = {J_ASOC},
  author       = {Abeegithan Jeyasothy and Savitha Ramasamy and Suresh Sundaram},
  doi          = {10.1016/j.asoc.2024.111747},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111747},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A gradient descent algorithm for SNN with time-varying weights for reliable multiclass interpretation},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A k-means-teaching learning based optimization algorithm
for parallel machine scheduling problem. <em>ASOC</em>, <em>161</em>,
111746. (<a href="https://doi.org/10.1016/j.asoc.2024.111746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous increase of workshop production scale, traditional heuristic algorithms in solving the scheduling problem have the defects of unsatisfactory computing time and insufficient stability of the solution result. However, data mining has a good performance in solving large-scale scheduling problems. A data mining method was proposed for industrial big data to solve the problem of large-scale parallel machines scheduling. This methodology can obtain an effective initial solution for single-operation parallel machine scheduling problem by exploring the effective information in the historical scheduling data. Based on historical customer orders, the offline learning was used to continuously generate simulated data for learning, which makes up for the shortcomings of insufficient data. A TLBO framework (teaching-learning-based optimization) hybrid K-means algorithm was redesigned to enhance the accuracy of offline learning and the efficiency of data searching. In the online operation part, according to the optimal solutions for high-similarity manufacturing orders are the approximate solutions, the new customer order will be quickly matched with the most similar manufacturing order through similarity calculation, and then and then local search is performed. Finally, the globally optimal solution is obtained after screening. Experimental results show that the hybrid teaching–learning methodology can solve the large-scale parallel machines scheduling problem with a better learning performance and computational efficiency.},
  archive      = {J_ASOC},
  author       = {Yibing Li and Jie Liu and Lei Wang and Jinfu Liu and Hongtao Tang and Jun Guo and Wenxiang Xu},
  doi          = {10.1016/j.asoc.2024.111746},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111746},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A K-means-teaching learning based optimization algorithm for parallel machine scheduling problem},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A typical sample-driven learning framework for automatic
disease diagnosis. <em>ASOC</em>, <em>161</em>, 111745. (<a
href="https://doi.org/10.1016/j.asoc.2024.111745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disease diagnosis mainly depends on the doctor’s medical knowledge and clinical experience, which can be treated as a medical text classification task. We observe that existing data-driven methods always suffer from the distribution bias since a small amount of common diseases appear high-frequently, while most diseases are infrequent in real-world, which leads to an unbalanced data distribution in the disease diagnosis task. To address this problem, we propose a new learning framework, T ypical sample- D riven G raph N eural N etwork (TD-GNN) for disease knowledge representation and classification. In our framework, different from previous methods, each disease (label) is concretized and learned from several corresponding well-representative samples rather than full imbalance data. In addition, the contrastive learning strategy is utilized to enhance the distinguishable features learning between different diseases. In this study, we construct a real-world dataset covering 350 common diseases to evaluate the proposed learning method. The experimental results demonstrate that the proposed TD-GNN significantly outperforms the state-of-the-art baselines, especially for the majority of diseases in which only small samples can be collected from the real world. Additionally, our method can provide a sample-based interpretation for disease prediction learning.},
  archive      = {J_ASOC},
  author       = {Chenwei Yan and Xinxin You and Xiangling Fu and Xien Liu and Ji Wu},
  doi          = {10.1016/j.asoc.2024.111745},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111745},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A typical sample-driven learning framework for automatic disease diagnosis},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The vehicle routing problem with cross-docking and
scheduling at the docking station: Compact formulation and a general
variable neighborhood search metaheuristic. <em>ASOC</em>, <em>161</em>,
111744. (<a href="https://doi.org/10.1016/j.asoc.2024.111744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce the Vehicle Routing Problem with Cross-Docking and Scheduling at the Docking Station (VRPCDSD), which integrates pickup and delivery routing decisions with the cross-docking scheduling of the unloading and reloading of requests. We present a compact Mixed Integer Programming (MIP) formulation and a General Variable Neighborhood Search (GVNS) metaheuristic for the problem. The proposed GVNS employs several routing and scheduling neighborhoods that require problem-specific adjustments due to the integrated characteristic of the problem. Computational experiments indicate that the MIP formulation can solve instances with up to 31 nodes (15 requests) within the specified time limit when using a standard commercial solver. Furthermore, the GVNS metaheuristic can obtain high-quality solutions that at least match those encountered by the solver for the tested instances, achieving considerably large improvements over the latter especially as the number of requests increases. In fact, GVNS is able to find the optimal solution for all the instances for which the optimal is known. In addition, GVNS is a viable alternative for tackling larger instances of VRPCDSD and is very robust in that it obtains low deviations from the best-encountered solutions when multiple executions are performed. We also present some insights on how the different cross-docking operation costs impact the optimal solutions.},
  archive      = {J_ASOC},
  author       = {Vitor A.A. Souza and Rafael A. Melo and Geraldo R. Mateus},
  doi          = {10.1016/j.asoc.2024.111744},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111744},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The vehicle routing problem with cross-docking and scheduling at the docking station: Compact formulation and a general variable neighborhood search metaheuristic},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical adaptive modeling for kitchen waste detection in
complex scenes. <em>ASOC</em>, <em>161</em>, 111743. (<a
href="https://doi.org/10.1016/j.asoc.2024.111743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of kitchen waste is of great significance, which provides support for its subsequent full quantitative consumption and harmless treatment. In addition, manual sorting of kitchen waste is inefficient and toxic waste is harmful to human health, making automatic detection of kitchen waste technology crucial. Automatic detection of kitchen waste in complex scenes faces the challenge of diversity of category outlines and uneven distribution. In this paper, we propose a detector based on statistical adaptive modeling(namely SA-Det) for the automatic detection of kitchen waste in complex scenes. Firstly, to solve the issue of diversity of category outlines, we propose a category statistics adaptive (CSA) module. The CSA module constructs dynamic thresholds for each instance to accurately assign positive and negative samples by fusing category statistics and instance shape information, thereby improving detection performance. Moreover, to solve the issue of uneven distribution, we propose a distribution adaptive (DA) module, which dynamically adjusts the loss weights by adaptively sensing the number of labels during training process. Extensive experiments on our constructed kitchen waste dataset (KWD) demonstrate that SA-Det consistently and significantly improves the performance of existing state-of-the-art methods (e.g., Rotated_RetinaNet (Lin et al., 2017) and RoI_Transformer (Ding et al., 2019)) by around 2% to 3.5%.},
  archive      = {J_ASOC},
  author       = {Hao Feng and Leyuan Fang and Shuaiyu Ding and Junwu Yu and Min He and Lin Tang},
  doi          = {10.1016/j.asoc.2024.111743},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111743},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Statistical adaptive modeling for kitchen waste detection in complex scenes},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Uncertainty bottom impact optimization of power battery
pack with 3D star-shaped auxetic structure. <em>ASOC</em>, <em>161</em>,
111742. (<a href="https://doi.org/10.1016/j.asoc.2024.111742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the integration of the power battery pack with the vehicle body/chassis and the limited ground clearance, the importance of bottom safety protection becomes more pronounced. Therefore, exploring how to utilize the mechanical advantages of new structures and approaches to ensure both the bottom impact resistance and the lightweight nature of the power battery pack is a critical issue. To address the problem, this paper first designs a novel power battery pack featuring a three-dimensional (3D) star-shaped auxetic structure. Additionally, an improved hybrid surrogate method combining the adaptive t-distribution sparrow search algorithm (TSSA) and the generalized regression neural network (GRNN) is proposed. Finally, considering deviations and uncertainties of the design process of power battery pack, an uncertainty bottom impact optimization integrating the TSSA-GRNN surrogate model and NSGA-II algorithm is carried out to further enhance the comprehensive crashworthiness and robustness. The results demonstrate superior performance of the novel battery pack compared to conventional ones, with TSSA-GRNN achieving the highest accuracy and efficiency in constructing surrogate models. Moreover, the F p-b and the maximum von Mises stress values obtained by the uncertainty optimization are 17.99 kN and 93.75 MPa, respectively, representing reductions of 23.90% and 25.45% compared with the initial design. Most importantly, the robustness of the optimization is significantly improved, with the standard deviation (σ) of the constraint condition reaching 8. It can provide theoretical support and engineering references in electric vehicle safety design.},
  archive      = {J_ASOC},
  author       = {Weiwei Wang and Tianci Zhang and Yi He and Wenhao Zhang and Xiaomei Xu and Fei Ju},
  doi          = {10.1016/j.asoc.2024.111742},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111742},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertainty bottom impact optimization of power battery pack with 3D star-shaped auxetic structure},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Squeeze-and-excitation 3D convolutional attention recurrent
network for end-to-end speech emotion recognition. <em>ASOC</em>,
<em>161</em>, 111735. (<a
href="https://doi.org/10.1016/j.asoc.2024.111735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) is difficult since emotions are complex and dynamic processes involving multiple dimensions and sub-dimensions. Feature extraction is a challenging step in SER, where relevant features are extracted from the speech to identify emotional states accurately. Overcoming these challenges is essential to ensuring the effectiveness and robustness of the SER system. 3D-convolutional neural networks (3D-CNNs) are successfully used for feature extraction in SER. Speech signals can be converted into spectrogram-like representations where one axis represents time, another frequency, and the third can represent additional context or features. For modeling the dynamic nature of speech, a squeeze-and-excitation-based 3D-CNN model is employed for capturing temporal and spatial features of speech. Attention Gated Recurrent Units (AGRU) are applied to the extracted features for learning long-range temporal dependencies and selecting more informative features. The extraction and selection of the spatio-temporal feature representation lead to a hierarchical representation of the input speech. The fusion of squeeze-and-excitation 3D-CNN and AGRU (named SE3D-CARN) is evaluated on two datasets, EMO-BD and IEMOCAP, to identify various emotional states in speech. The proposed SER model reached an accuracy of 94.2% and 81.1% on the EMO-BD and IEMOCAP datasets, respectively.},
  archive      = {J_ASOC},
  author       = {Nasir Saleem and Hela Elmannai and Sami Bourouis and Aymen Trigui},
  doi          = {10.1016/j.asoc.2024.111735},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111735},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Squeeze-and-excitation 3D convolutional attention recurrent network for end-to-end speech emotion recognition},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Q-learning guided mutational harris hawk optimizer for
high-dimensional gene data feature selection. <em>ASOC</em>,
<em>161</em>, 111734. (<a
href="https://doi.org/10.1016/j.asoc.2024.111734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of high-throughput sequencing technology in recent years, the scale of high-dimensional gene sequence datasets has rapidly expanded. However, due to the high-dimensional nature of gene sequence data, researchers face the challenge of processing such complex data. One common preprocessing technique that can improve performance is feature selection, which selects the most relevant features from the original dataset, reducing its dimensionality. However, feature selection is often an NP-hard problem, and medical feature selection is mainly affected by the multiple attributes of features, which affect classification accuracy. In order to improve the generality of the algorithm, we propose RLHHO, which solves the feature selection problem of various medical gene datasets by combining Q-learning-guided mutation strategies. We also created binary RLHHO (bRLHHO) through conversion functions and evaluated its performance on 12 high-dimensional datasets. The experimental results show that bRLHHO is superior to the original HHO in improving classification accuracy and reducing the number of selected features. When processing high-dimensional medical gene datasets with over 1000 dimensions, bRLHHO can achieve good accuracy with fewer features. In summary, compared with other algorithms, including the original Harris Hawks Optimization (HHO), our proposed improved version of HHO, RLHHO, can handle various feature selection datasets and exhibits superior performance.},
  archive      = {J_ASOC},
  author       = {Lemin Peng and Xinru Li and Liang Yu and Ali Asghar Heidari and Huiling Chen and Guoxi Liang},
  doi          = {10.1016/j.asoc.2024.111734},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111734},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Q-learning guided mutational harris hawk optimizer for high-dimensional gene data feature selection},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Sample-analysis based adversarial attack with saliency map.
<em>ASOC</em>, <em>161</em>, 111733. (<a
href="https://doi.org/10.1016/j.asoc.2024.111733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of deep learning, the vulnerability of neural networks has attracted considerable attention, raising reliability and security concerns. Therefore, research on the robustness of neural networks has become increasingly critical. In this paper, we propose a novel sample-analysis based robustness evaluation method that overcomes the drawbacks of existing techniques, such as solving difficulty, single strategy, and loose radius. Our algorithm comprises two parts: robustness evaluation and adversarial attacks. Specifically, we introduce formal definitions of multiple sample types and a general solution to the problem of adversarial samples. We formulate a disturbance model-based description of adversarial samples in the adversarial attack algorithm and utilize saliency map to solve them. Our experimental results demonstrate that our adversarial attack algorithm not only achieves a high attack success rate in a relatively small disturbance range but also generates multiple adversarial examples for each clean example. Our algorithm can evaluate the robustness of complex datasets and models, overcome the lack of a single strategy in solving adversarial examples, and provide a more accurate radius of robustness.},
  archive      = {J_ASOC},
  author       = {Dian Zhang and Yunwei Dong and Yun Yang},
  doi          = {10.1016/j.asoc.2024.111733},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111733},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sample-analysis based adversarial attack with saliency map},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the artificial bee colony algorithm with a
proprietary estimation of distribution mechanism for protein–ligand
docking. <em>ASOC</em>, <em>161</em>, 111732. (<a
href="https://doi.org/10.1016/j.asoc.2024.111732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protein–ligand docking problem plays an essential role in structure-based drug design. The challenge for a protein–ligand docking method is how to execute an efficient conformational search to explore a well-designed scoring function. In this study, we improved the artificial bee colony (ABC) algorithm and proposed an approach called ABC-EDM to solve the protein–ligand docking problem. ABC-EDM employs the scoring function of the classical AutoDock Vina to evaluate a solution during docking simulation. ABC-EDM adopts the search framework of the canonical ABC algorithm to execute conformational search. By further investigating the characteristics of the protein–ligand docking problem, a proprietary search mechanism inspired by estimation of distribution algorithm, i.e., estimation of distribution mechanism (EDM), is designed to enhance the performance of ABC-EDM. To verify the effectiveness of the proposed ABC-EDM, we compare it with three variants of the ABC algorithm, three evolutionary computation algorithms, and AutoDock Vina. The experimental results show that ABC-EDM can effectively solve the protein–ligand docking problem, and it can achieve a success rate 5% higher than AutoDock Vina on the GOLD dataset. This study reveals that taking advantage of problem-specific information about the protein–ligand docking problem to enhance a docking method contributes to solving this problem.},
  archive      = {J_ASOC},
  author       = {Shuangbao Song and Cheng Tang and Zhenyu Song and Jia Qu and Xingqian Chen},
  doi          = {10.1016/j.asoc.2024.111732},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111732},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improving the artificial bee colony algorithm with a proprietary estimation of distribution mechanism for protein–ligand docking},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recurrent neural networks-guided vector-valued synchronized
key exchange for secure and privacy-preserving communication in
industrial internet of things. <em>ASOC</em>, <em>161</em>, 111731. (<a
href="https://doi.org/10.1016/j.asoc.2024.111731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an innovative approach to the problem of key exchange in the Industrial Internet of Things (IIoT) implementation. Recurrent Neural Networks (RNNs) and vector-valued neural synchronization are the key components of the proposed secure transmission method. Drive-response methods are integrated to improve speed in important applications, meeting the continuous need for effective cryptographic key exchange between IIoT devices. Conventional algorithms face drawn-out assessment procedures that jeopardize neural synchronization concealment. This work examines how random input vector generation and synchronization issues in RNNs with drive-response are affected by proportional and non-proportional delays. Furthermore, it delves into an unexplored domain, investigating the synchronization of response-based RNN systems without delays and drive-response-based RNN systems with various proportional delays. It also suggests a simplified analysis of Artificial Neural Networks (ANNs) synchronization, organizing ANNs for session key switch-over using an RNN system. To achieve polynomial and non-polynomial synchronization in the proposed driver response systems, this method offers several benefits: it introduces a polynomial synchronization theory for RNNs to generate synchronized input vectors for ANN synchronization; it uses inequality assessment techniques and Lyapunov formulas to derive relevant control inputs and time-dependent conditions; it establishes the relationship between polynomial and non-polynomial synchronization; and it provides numerical examples demonstrating its effectiveness. It also builds a neural network for generating session keys throughout the IIoT network by aligning vector-valued ANNs in a reciprocal manner. This strategy outperforms previous approaches in the literature, validated through simulations, balances resilience against attacks with minimal computational load, resulting in more effective and resilient industrial applications.},
  archive      = {J_ASOC},
  author       = {Arindam Sarkar},
  doi          = {10.1016/j.asoc.2024.111731},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111731},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recurrent neural networks-guided vector-valued synchronized key exchange for secure and privacy-preserving communication in industrial internet of things},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image-based security techniques for water critical
infrastructure surveillance. <em>ASOC</em>, <em>161</em>, 111730. (<a
href="https://doi.org/10.1016/j.asoc.2024.111730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security-related image analysis of critical infrastructures involves using advanced technologies to analyze visual data to identify potential threats, vulnerabilities, or abnormal activities. This type of analysis can be crucial for safeguarding critical infrastructure such as water management systems, power plants, transportation systems, communication networks, and more. In this paper, we propose two-factor-based authentication and pose-based monitoring tasks for water critical infrastructures. In the first phase, we detect and recognize the faces of authorized persons and check their liveness for anti-spoofing by eye-clicking checking and multimodal ensemble methods. In the second phase, we create our dataset, WCI-Pose, and we use four pose-based approaches – OpenPose, AlphaPose, Kapao, and YOLOv8 – for human action recognition. Experimental results show that OpenPose algorithm provides more favorable results compared to the other algorithms with an accuracy of 92.4%. SVM with multimodal data has the best performance score with an accuracy of 92% for face anti-spoofing.},
  archive      = {J_ASOC},
  author       = {Seda Balta Kaç and Süleyman Eken and Deniz Dural Balta and Musa Balta and Murat İskefiyeli and İbrahim Özçelik},
  doi          = {10.1016/j.asoc.2024.111730},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111730},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image-based security techniques for water critical infrastructure surveillance},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-aware transferable adversarial attacks against image
classification. <em>ASOC</em>, <em>161</em>, 111729. (<a
href="https://doi.org/10.1016/j.asoc.2024.111729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to white-box adversarial attacks, black-box adversarial attacks are more applicable in practical scenarios and have received significant attention. However, most existing black-box attacks are optimized at the output layer, and the generated adversarial samples are difficult to transfer from the surrogate model to the target model. Existing adversarial attacks indiscriminately perturb features in intermediate layers, which are prone to fall into local optima of surrogate models and have limited transferability. In this paper, we propose a transferable targeted adversarial attack framework based on a feature-aware triplet, which achieves a better tradeoff between attack ability and transferability by perturbing salient features and constructing representative sample features. The importance-weighted optimization objective interferes with the salient object-aware features of images in a targeted manner and guides adversarial perturbations in the optimization process to pull the intermediate features of samples toward a target class while pushing them far from a source class to achieve targeted transferable attacks. Moreover, a construction method of a feature library with the weighted average of feature importance is built to obtain more expressive intermediate features of the target and source classes. The modified target features and source features are fed into the triplets to guide the optimization objectives to find more transferable adversarial samples. Extensive experiments on the ImageNet-compatible dataset verify the effectiveness of the proposed method, e.g., improving the untargeted success rate by 1.8% and the targeted success rate by 2.3% against normally trained models as compared to the existing methods.},
  archive      = {J_ASOC},
  author       = {Shuyan Cheng and Peng Li and Keji Han and He Xu},
  doi          = {10.1016/j.asoc.2024.111729},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111729},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-aware transferable adversarial attacks against image classification},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deadlock prevention and multi agent path finding algorithm
considering physical constraint for a massive fleet AGV system.
<em>ASOC</em>, <em>161</em>, 111725. (<a
href="https://doi.org/10.1016/j.asoc.2024.111725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces deadlock avoidance algorithms for Automated Guided Vehicles (AGVs) within warehouse fulfillment systems. Numerous AGVs are tasked with transporting requests to deliver shelves to workers stationed at various locations. The pathfinding challenge within such systems is identified as the Multi-Agent Pickup and Delivery (problem) and has been extensively explored in computer science and AI. Nevertheless, most prior research was conducted in environments that are discretized and allow circular movements of AGVs, potentially leading to deadlocks in real-world applications. Our proposed algorithm leverages a dynamic path block method and path reservation. Specifically, it limits movement to certain edges when AGVs navigate along reserved paths, while the remaining AGVs plan their routes using unrestricted edges. We show that our algorithm not only effectively prevents deadlocks but also scales well in environments with a high number of AGVs.},
  archive      = {J_ASOC},
  author       = {Chang Hyun Chung and Young Jae Jang},
  doi          = {10.1016/j.asoc.2024.111725},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111725},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deadlock prevention and multi agent path finding algorithm considering physical constraint for a massive fleet AGV system},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Deep fuzzy nets approach for energy efficiency optimization
in smart grids. <em>ASOC</em>, <em>161</em>, 111724. (<a
href="https://doi.org/10.1016/j.asoc.2024.111724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using smart grids has become crucial for achieving efficient and sustainable energy management. One of the main challenges in smart grids is optimizing energy efficiency by managing and controlling electricity generation, transmission, and distribution. The Deep Fuzzy Nets (DFN) approach has been proposed as a novel technique that combines the capabilities of deep learning and fuzzy login to optimize energy efficiency in smart grids. The proposed approach utilizes a deep learning architecture to learn the complex relationships between various parameters within the smart grid system. The fuzzy logic component handles uncertainties and imprecision’s in the data, making the DFN approach well-suited for real-world energy management applications. The proposed approach can provide accurate and reliable predictions and enhancing energy efficiency in a dynamic and evolving smart grid environment. The proposed deep fuzzy nets approach reached 91% sensitivity, 94.45% specificity, 92/37% prevalence threshold, and 96.54% critical success index. This approach has been tested in various energy systems and has demonstrated capabilities to improve system-level energy efficiency while still giving users control of their energy usage. As energy efficiency optimization in intelligent grids continues to be a primary focus of energy research, deep fuzzy nets could provide a powerful solution for energy optimization.},
  archive      = {J_ASOC},
  author       = {Abdullah Baz and J. Logeshwaran and Yuvaraj Natarajan and Shobhit K. Patel},
  doi          = {10.1016/j.asoc.2024.111724},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111724},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep fuzzy nets approach for energy efficiency optimization in smart grids},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Safe drug recommendation through forward data imputation and
recurrent residual neural network. <em>ASOC</em>, <em>161</em>, 111723.
(<a href="https://doi.org/10.1016/j.asoc.2024.111723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug recommendation is crucial for aiding clinical decisions and promoting the scientific and rational use of drugs. The goal of drug recommendation is to provide safe and effective drug combinations for patients based on their historical medical records. Previous research heavily relies on electronic health records (EHRs) for data analysis. However, incomplete information and the presence of abnormal data in EHRs often lead to imprecise patient characterization. And it is challenging to learn patient representations and efficiently recommend safe drug combinations through EHRs. To tackle these concerns, this paper presents FDIRNet model to improve the performance of drug recommendation. FDIRNet employs forward data imputation to deal with missing and abnormal data of single-visit and multi-visits. Meanwhile, FDIRNet utilizes recurrent neural networks and residual neural networks to capture the changing patterns of a patient’s historical information. In addition, a novel loss function is proposed, which is combined with a Drug-Drug Interaction (DDI) loss strategy to mitigate DDI, thereby improving the overall robustness of the model. FDIRNet is evaluated on the publicly available dataset and it exhibits significant improvements over previous methods. We have observed distinct increases of 3.1%, 1.6%, and 3.1% in Jaccard, F1 score, and PRAUC, respectively.},
  archive      = {J_ASOC},
  author       = {Junping Liu and Zhiju Wan and Xinrong Hu and Qiang Zhu},
  doi          = {10.1016/j.asoc.2024.111723},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111723},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Safe drug recommendation through forward data imputation and recurrent residual neural network},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Processing 2D barcode data with metaheuristic based CNN
models and detection of malicious PDF files. <em>ASOC</em>,
<em>161</em>, 111722. (<a
href="https://doi.org/10.1016/j.asoc.2024.111722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portable Document Format (PDF) is a file format created to create portable and printable documents across platforms. PDF files are one of the most widely used application types in computer-based systems. Thanks to the functionality that PDF files provide, they are used by many users around the world. Malware developers can exploit PDF files due to various factors. Malware can integrate embedded files, JavaScript, PDF files, etc. As a result, PDFs are susceptible to security vulnerabilities in computer-based systems. In this study, we utilised the CIC-Evasive-PDFMal2022 dataset, made accessible by the Canadian Cybersecurity Institute in 2022, that includes two categories, namely benign and malicious. In the preprocessing step, the proposed model transformed text-based PDF parameter data into the 2D PDF417 barcode. 2D Convolutional Neural Network (CNN) models (MobileNetV2, ResNet18, and ShuffleNet) are trained using the dataset generated by the preprocessing step. CNN is a type of artificial neural network used in image recognition, processing, and classification. Type/class based feature sets were then obtained by each CNN model. In the last step, the metaheuristic optimization method (Honey Badger Algorithm) was used. Thanks to this method, the best performing feature set was determined among the feature sets of the types extracted from each CNN model. It was then classified by the softmax method, and an overall accuracy of 99.73% was achieved. The proposed approach has successfully trained 1D data with 2D CNNs. In addition, with the barcode imaging technique, direct understanding of the data by the users is prevented.},
  archive      = {J_ASOC},
  author       = {Mesut Toğaçar and Burhan Ergen},
  doi          = {10.1016/j.asoc.2024.111722},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111722},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Processing 2D barcode data with metaheuristic based CNN models and detection of malicious PDF files},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classical vs. Neural network-based PCA approaches for lossy
image compression: Similarities and differences. <em>ASOC</em>,
<em>161</em>, 111721. (<a
href="https://doi.org/10.1016/j.asoc.2024.111721">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper describes three lossy data compression techniques based on the principal component analysis (PCA), which are compared using the image compression task. The presented approach uses both classical PCA method based on the eigen-decomposition of the image data covariance matrix and two different neural network structures. The first neural structure is a two-layer feed-forward network with supervised learning acting as the so-called autoencoder, and the second one is a single-layered network with an unsupervised learning method commonly known as the generalized Hebbian algorithm. For each compression method mentioned above, the influence of the number of image segments (frames) and the number of eigenvalues/eigenvectors on the compression ratio and the image quality are examined using three different gray-scale test images. The paper also addresses a vital implementation issue regarding selecting appropriate data types to represent the compressed data. The paper’s main conclusion is that the classical PCA method outperforms its neural counterparts, both in terms of the decompressed image quality and the time required to perform the compression procedure. The positive aspect of using neural networks as a tool for PCA-based lossy data compression is that they do not require calculating the correlation matrix explicitly and thus can be used in online data acquisition schemes.},
  archive      = {J_ASOC},
  author       = {Krzysztof Bartecki},
  doi          = {10.1016/j.asoc.2024.111721},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111721},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classical vs. neural network-based PCA approaches for lossy image compression: Similarities and differences},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Portfolio optimization with mental accounts under uncertain
random environment and butterfly optimization algorithm with adaptive
strategies. <em>ASOC</em>, <em>161</em>, 111720. (<a
href="https://doi.org/10.1016/j.asoc.2024.111720">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In complex security markets, uncertainty and randomness may coexist. The investors hold different risk attitudes toward different investment objectives in reality. In order to reflect this phenomenon, this paper applies mental accounts to portfolio optimization in an uncertain random environment. Firstly, considering the influence of transaction costs and diversification degree, this paper establishes an uncertain random mean-absolute semi-deviation-entropy bi-objective optimization model. Then the bi-objective model is converted to two single-objective models through three steps. Secondly, the equivalent forms of the models are deduced when the return rates of random risky securities are assumed to be normal random variables and the return rates of uncertain risky securities are assumed to be linear and zigzag uncertain variables. Furthermore, we propose an improved butterfly optimization algorithm (IBOA) to solve the two single-objective models. Finally, numerical simulations are presented to analyze the practicability and effectiveness of the models with different mental accounts and the IBOA algorithm. The results indicate that the IBOA algorithm is effective and putting money into more mental accounts may gain higher returns.},
  archive      = {J_ASOC},
  author       = {Bo Li and Yayi Huang},
  doi          = {10.1016/j.asoc.2024.111720},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111720},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Portfolio optimization with mental accounts under uncertain random environment and butterfly optimization algorithm with adaptive strategies},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating reinforcement learning and metaheuristics for
safe and sustainable health tourist trip design problem. <em>ASOC</em>,
<em>161</em>, 111719. (<a
href="https://doi.org/10.1016/j.asoc.2024.111719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to address the critical challenge of enhancing safety within the domain of health and wellness tourism through the framework of sustainable travel design. Given the rising demand for secure and sustainable tourism experiences, this investigation explores the integration of safety considerations into the strategic planning and execution of tourist journeys. The central focus of this research revolves around the development and application of the Modified Reinforcement Learning-Artificial Multiple Intelligence System (MRL-AMIS). Operating within a multi-objective framework, the objective of this study is to optimize essential parameters encompassing environmental sustainability, economic viability, tourist preferences, and societal acceptability, with a specific emphasis on safety. The efficacy of the proposed methodology is rigorously evaluated across various tourist group scenarios, providing a comprehensive assessment of its applicability. Our computational analyses unequivocally demonstrate the superiority of the proposed approach in enhancing safety, surpassing existing methods such as genetic algorithms, differential evolution algorithms, particle swarm optimization, etc., by 15%-25%. These findings underscore the effectiveness of the proposed method in seamlessly integrating safety considerations into the realm of sustainable tourism planning. This study highlights the potential of the proposed method as an advanced tool in the strategic planning of sustainable tourism, particularly within the context of health and wellness tourism. It constitutes a significant contribution to the field of sustainable tourism by offering an innovative approach that prioritizes safety while upholding the principles of environmental stewardship, economic viability, and societal sustainability.},
  archive      = {J_ASOC},
  author       = {Rapeepan Pitakaso and Kanchana Sethanan and Chen-Fu Chien and Thanatkij Srichok and Surajet Khonjun and Natthapong Nanthasamroeng and Sarayut Gonwirat},
  doi          = {10.1016/j.asoc.2024.111719},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111719},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating reinforcement learning and metaheuristics for safe and sustainable health tourist trip design problem},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective anti-submarine decision support system based on
heuristic rank-based dijkstra and adaptive threshold partitioning
mechanism. <em>ASOC</em>, <em>161</em>, 111718. (<a
href="https://doi.org/10.1016/j.asoc.2024.111718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Submarines possess strong covert striking capabilities, making anti-submarine warfare (ASW) a global naval priority. The Hidden Markov Anti-Submarine Model (HMASM) finds crucial applications in dynamic and uncertain ASW. The model delineates ASW into two phases: partitioning and search path planning. Current partitioning algorithms often include cells leading to redundant and competitive search, determined empirically. Additionally, in HMASM, the path planning algorithms based on genetic algorithm (GA) are time-consuming and exhibits unstable performance. This paper proposes a novel solution to these issues. For partitioning, a Reassigned k-Nearest Neighbour (RKNN) algorithm is introduced, identifying and reallocating units causing repeated and competing searches. For search path planning, heuristic rules for Dijkstra&#39;s cost function and goal point selection transform the NP-hard problem of maximizing the expected number of detections (ED) into a deterministic algorithm-compatible form. A heuristic rank-based selection model, Rank-Based Dijkstra under the Hidden Markov Model (HMM-R-Dijkstra), considering distance and probability, is added to Dijkstra. Furthermore, an Adaptive Threshold Partitioning (ATP) dynamically monitors searcher exploration by setting variables and thresholds to determine optimal partitioning timing, preventing untimely and excessive partitioning. Combining RKNN, HMM-R-Dijkstra, and ATP forms R-HRD-ATP, optimizing all parameters using parallel structures. Through three comparative experiments, R-HRD-ATP&#39;s performance steadily improves. Experiments comparing R-HRD-ATP to GA, Sparrow Search Algorithm (SSA)-GA, and Ant Colony Optimization (ACO)-GA reveal performance enhancements of 25–70.99% and time savings of 56–295 times for our model. Importantly, no parameter adjustments are required. The success of R-HRD-ATP indicates that its heuristic rules can support the establishment of robust applications of deterministic path planning algorithms in HMASM.},
  archive      = {J_ASOC},
  author       = {Hanlin Li and Longxia Qian and Mei Hong and Haiping Huang and Yunxiang Zhang and Qinglun Yan},
  doi          = {10.1016/j.asoc.2024.111718},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111718},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective anti-submarine decision support system based on heuristic rank-based dijkstra and adaptive threshold partitioning mechanism},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remaining useful life prediction under variable operating
conditions via multisource adversarial domain adaptation networks.
<em>ASOC</em>, <em>161</em>, 111717. (<a
href="https://doi.org/10.1016/j.asoc.2024.111717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) methods have been extensively applied for intelligent remaining useful life prediction (RUL) tasks, addressing domain shift issues under variable operating conditions. Almost all existing methods focus on exploiting the single-source unsupervised domain adaptation (SUDA) approach which only learns prognostic knowledge from a single domain. Nevertheless, SUDA methods have significant limitations due to the disregard for the valuable information contained in historical data from numerous sources. Compared to conventional SUDA approaches, multisource unsupervised domain adaptation (MUDA) approaches can acquire invariant domain representation from a variety of sources and offer higher generalization in transfer tasks. Therefore, a novel MUDA framework multisource adversarial domain adaptation network (MADAN) is proposed for regression tasks. MADAN can efficiently organize and utilize multiple source domain data through a two-stage adversarial domain alignment, of which adversarial domain distribution alignment and inter-domain regression alignment modules are designed to minimize domain discrepancy and regressor discrepancy between each pair of source and target domains. Experimental results indicate that MADAN achieves accurate cross-domain predictions by using multiple source domains, and a comparison demonstrates that it performs better than the state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Junrong Du and Lei Song and Xuanang Gui and Jian Zhang and Lili Guo and Xuzhi Li},
  doi          = {10.1016/j.asoc.2024.111717},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111717},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Remaining useful life prediction under variable operating conditions via multisource adversarial domain adaptation networks},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simulation-based hybrid genetic algorithms for the
stochastic multi-mode resource-constrained project scheduling problem
with minimized financial risk. <em>ASOC</em>, <em>161</em>, 111716. (<a
href="https://doi.org/10.1016/j.asoc.2024.111716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Projects face uncertainties during implementation, and these uncertainties often lead to risk factors that disrupt successful project execution and cause a loss of profit. The stochastic project scheduling technique has been an effective avenue for reducing the risks in project management. Nevertheless, previous studies often neglected resource limits for minimizing the financial risks of projects in stochastic project scheduling problems. This study aims to optimize the conditional net present value at risk (CNPVaR), which measures a project’s expected loss of net present value (NPV) in a stochastic multi-mode resource-constrained project scheduling problem (SMRCPSP). Accordingly, considering the stochastic activity duration and cash flow, we construct a scenario-based optimization model for the SMRCPSP, where the solution (strategy) of the SMRCPSP is represented by a policy that includes an activity list and execution mode list; a scheduling policy is then employed to map the start times of activities at each decision time based on the solution. Three simulation-based hybrid genetic algorithms embedded with different iteration alternatives according to activity-based (AB) or resource-based (RB) scheduling policies are used. Furthermore, computational experiments are conducted to evaluate the proposed procedures through 480 instances. With respect to the CNPVaR, expected NPV, and algorithm stability metrics, the results of the experiments demonstrate that the genetic algorithm (GA) with local search based on the RB scheduling policy performed best compared with two other hybrids of the GA, the existing vibration damping optimization (VDO) method and a pure GA for large-sized projects. However, the VDO outperforms the other approaches according to the AB scheduling policy for small-scale projects. Additionally, a case study is conducted; the findings demonstrate that the risk attitude (confidence level) of project managers and the discount factor have impacts on the CNPVaR and expected NPV, but the CNPVaR is insensitive to the number of simulations.},
  archive      = {J_ASOC},
  author       = {Wanlin Liu and Haotian Zhang and Yumeng Chen and Chunli Qu and Jingwen Zhang},
  doi          = {10.1016/j.asoc.2024.111716},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111716},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simulation-based hybrid genetic algorithms for the stochastic multi-mode resource-constrained project scheduling problem with minimized financial risk},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Error-feedback three-phase optimization to configurable
convolutional echo state network for time series forecasting.
<em>ASOC</em>, <em>161</em>, 111715. (<a
href="https://doi.org/10.1016/j.asoc.2024.111715">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting is critical for many real-world applications. Convolutional echo state networks (CESNs) have shown intriguing time series modeling efficacy by combining convolutional neural network (CNN) and echo state network (ESN). However, current CESN models are tailored for the classification tasks and rely on elaborately designed neural architectures. To this end, we propose a novel configurable convolutional echo state network (CCESN) with an innovative error-feedback three-phase optimization (ETO) strategy for time series forecasting. The network is progressively constructed with heterogeneous modular subnetworks, including ESN, CNN, CESN, and reversed CESN modules. This scheme leverages the complementary feature extraction capabilities of convolutional and recurrent neural architectures. To adaptively evolve the CCESN, we propose a novel error-feedback three-phase optimization (ETO) strategy by selecting optimal subnetwork modules while step-wise tuning parameters. Comprehensive experiments are conducted on representative simulated and real-world datasets. The results indicate that ETO-CCESN can adaptively select and evolve heterogeneous subnetworks to acclimatize to varied scenarios, and thus demonstrate significant performance improvements, achieving a 45.69% average enhancement in forecasting accuracy compared to the existing CESN model, and surpassing the best baseline by 8.79% in terms of symmetric mean absolute percentage error across diverse forecasting tasks.},
  archive      = {J_ASOC},
  author       = {Xinze Zhang and Kun He and Qi Sima and Yukun Bao},
  doi          = {10.1016/j.asoc.2024.111715},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111715},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Error-feedback three-phase optimization to configurable convolutional echo state network for time series forecasting},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing medical image classification with generative AI
using latent denoising diffusion probabilistic model and wiener
filtering approach. <em>ASOC</em>, <em>161</em>, 111714. (<a
href="https://doi.org/10.1016/j.asoc.2024.111714">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of medical diagnostics, a paradigm shift is being catalysed by the advent of generative artificial intelligence. Medical X-ray, CT and MRI images are essential diagnostic tools used by healthcare professionals to assess various musculoskeletal conditions. However, obtaining a sufficient number of medical images for training deep learning models can be challenging due to limited access to labelled data. Hence, the authors propose a novel Latent diffusion process for synthesizing medical images that closely resemble real patient images, aiming to address the challenge of limited access to labelled data in medical diagnostics. Leveraging deep learning and generative modelling techniques, this method synthesizes high-fidelity images that closely mimic real patient scans. By introducing noise and subsequently training the model to denoise, the approach captures intricate patterns inherent in authentic medical images. Among the four datasets utilized, the Diabetes Retinopathy dataset demonstrates superior performance, achieving the highest Mean Structural Similarity Index (MSSIM) score of 0.57 (compared to the dataset baseline of 0.62) and an accuracy of 93.75% when passed through the proposed pipeline. The Cataract dataset, registered a MSSIM score of 0.51 (versus the dataset baseline of 0.53) and an accuracy score of 97.52%, while the Knee OA dataset follows closely with MSSIM and accuracy scores of 0.65 (in contrast to the dataset baseline of 0.63) and 68.66% respectively. The results obtained are then compared with the results generated by the other state of the art models.},
  archive      = {J_ASOC},
  author       = {Manas Ranjan Prusty and Rohit Madhavan Sudharsan and Philip Anand},
  doi          = {10.1016/j.asoc.2024.111714},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111714},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing medical image classification with generative AI using latent denoising diffusion probabilistic model and wiener filtering approach},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature-weight and cluster-weight learning in fuzzy c-means
method for semi-supervised clustering. <em>ASOC</em>, <em>161</em>,
111712. (<a href="https://doi.org/10.1016/j.asoc.2024.111712">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised clustering aims to guide the clustering by utilizing auxiliary information about the class labels. Among the semi-supervised clustering categories, the constraint-based approach uses the available pairwise constraints in some steps of the clustering procedure, usually by adding new terms to the objective function. Considering this category, Semi-supervised FCM (SSFCM) is a semi-supervised version of the fuzzy c -means algorithm, which takes advantage of fuzzy logic and auxiliary class distribution knowledge. Despite the performance enhancement caused by incorporating this extra knowledge in the clustering process, semi-supervised fuzzy approaches still suffer from some problems. All the data attributes in the feature space are assumed to have equal importance in the cluster formation, while some features may be more informative than others. Thus the feature importance issue is not addressed in the semi-supervised category. This paper proposes a novel Semi-Supervised Fuzzy c -means approach, which is designed based on Feature-Weight, and Cluster-Weight learning, named SSFCM-FWCW. Inspired by the SSFCM, a fuzzy objective function is presented, which is composed of (1) a semi-supervised term representing the external class knowledge; (2) a feature weighting; and (3) a cluster weighting. Both feature weights and cluster weights are determined adaptively during the clustering. Considering these two techniques leads to insensitivity to the initial center selection, insensitivity to noise, and consequently helps to form an optimal clustering structure. Experimental comparisons are carried out on several benchmark datasets to evaluate the proposed approach&#39;s performance, and promising results are achieved. The Matlab implementation source code of the proposed method is publicly accessible at https://github.com/Amin-Golzari-Oskouei/SSFCM-FWCW .},
  archive      = {J_ASOC},
  author       = {Amin Golzari Oskouei and Negin Samadi and Jafar Tanha},
  doi          = {10.1016/j.asoc.2024.111712},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111712},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature-weight and cluster-weight learning in fuzzy c-means method for semi-supervised clustering},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain tumor segmentation with missing MRI modalities using
edge aware discriminative feature fusion based transformer u-net.
<em>ASOC</em>, <em>161</em>, 111709. (<a
href="https://doi.org/10.1016/j.asoc.2024.111709">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor segmentation is an essential task for medical diagnosis and treatment planning. Multi-modal MRI provides complementary information that is essential for accurate segmentation of brain tumors, but missing modality images are a common problem in clinical practice. Existing segmentation methods often fail to generate accurate object boundaries and selectively fuse the tumor region, resulting in unreliable segmentation masks. In this work, we propose an Edge-aware Discriminative Feature Fusion Based Transformer U-Net (EA-DFFTU-Net) to segment the brain tumor effectively even in the absence of modalities. First, the MRI input data is pre-processed, and features are then extracted using a ResNet-50 encoder, which learns local spatial. We employ an Edge Feature Module (EFM) to acquire edge attention representations. These features are then transferred to the Discriminative Feature Fusion based Transformer U-Net (DFFTU-Net) that learns global contexts and captures multiscale features. We use a Discriminative Feature Fusion Module (DFFM) in the decoder of the DFFTU-Net to effectively fuse the multiscale features in order to obtain accurate segmentation. The performance of the proposed EA-DFFTU-Net segmentation method was determined by evaluating it and comparing the obtained results with those of existing brain tumor segmentation techniques.},
  archive      = {J_ASOC},
  author       = {B. Jagadeesh and G. Anand Kumar},
  doi          = {10.1016/j.asoc.2024.111709},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111709},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Brain tumor segmentation with missing MRI modalities using edge aware discriminative feature fusion based transformer U-net},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective particle swarm optimization based on local
ideal points. <em>ASOC</em>, <em>161</em>, 111707. (<a
href="https://doi.org/10.1016/j.asoc.2024.111707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous multi-objective evolutionary algorithms have recently been proposed for the leader selection or archive maintenance process via using reference sets. Despite the potential of this strategy has been demonstrated in the existing literatures, a significant drawback is that this methodology requires additional parameters or predefined reference points, which leads to increasing the complexity in real-world applications and reducing the versatility in addressing various optimization challenges. Novel to this study, a new convergence contribution (CC) evaluator without extra parameters and predefined reference points is presented for convergence evaluation, where the inspiration is drawn from the concept of an ideal point on a Pareto front and the divide-and-conquer technique. Specifically, the local ideal points are introduced in this paper by dynamically fabricating from the approximate Pareto front. Furthermore, the CC evaluator and parallel cell distance (PCD) are cooperatively integrated into a multi-objective particle swarm optimization (MOPSO/CP) to enhance both the global best solution selection and archive maintenance strategies. Comparative experiments on 21 benchmark test functions exhibited that the performances in terms of inverted generational distance and hypervolume of the proposed MOPSO/CP was the best among those of the chosen competitive algorithms. The significant role of the cooperative mechanism and the CC evaluator is further verified by ablation studies. The superiority of the designed algorithm against its competitors is unequivocally highlighted by the experimental results.},
  archive      = {J_ASOC},
  author       = {Yu Zhang and Wang Hu and Wen Yao and Xinyue Li and Junjie Hu},
  doi          = {10.1016/j.asoc.2024.111707},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111707},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective particle swarm optimization based on local ideal points},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic neural networks for incremental learning over
time-varying streaming data with application to air pollution
monitoring. <em>ASOC</em>, <em>161</em>, 111702. (<a
href="https://doi.org/10.1016/j.asoc.2024.111702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel algorithm for incremental learning over streaming data in a non-stationary environment. The idea refers to the applicability of Probabilistic Neural Networks (PNNs), commonly used as a fast and robust method for solving classification problems in a stationary scenario. It is well known that PNNs have solid mathematical foundations but they fail in the non-stationary scenario and are not able to deal with concept drift. In this paper, the method based on the orthogonal series expansions of unknown probability densities is proposed. This approach significantly extends the applicability of the PNNs. The nonparametric procedures called Incremental Probabilistic Neural Networks (IPNNs) for tracking drifting probability densities or conditional class densities, in the case of classification, are presented. We prove their convergence as the stream data size is growing to infinity. More precisely, we show convergence in probability and with probability one as the sample size tends to infinity. Compared to the existing literature, almost exclusively based on various heuristics, the proposed method is mathematically justified and has solid theoretical foundations. Simulations performed on synthetic and air pollution data confirm the effectiveness of the algorithm.},
  archive      = {J_ASOC},
  author       = {Danuta Rutkowska and Piotr Duda and Jinde Cao and Maciej Jaworski and Marek Kisiel-Dorohinicki and Dacheng Tao and Leszek Rutkowski},
  doi          = {10.1016/j.asoc.2024.111702},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111702},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Probabilistic neural networks for incremental learning over time-varying streaming data with application to air pollution monitoring},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Co-evolutionary traffic signal control using reinforcement
learning for road networks under stochastic capacity. <em>ASOC</em>,
<em>161</em>, 111701. (<a
href="https://doi.org/10.1016/j.asoc.2024.111701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A co-evolutionary traffic signal control using reinforcement learning approach (CORLA) is proposed for time-varying road networks under stochastic capacity. Classic reinforcement learning based traffic signal control cannot effectively reduce traffic congestion for large-scale road networks while standard evolutionary metaheuristics often suffer from significantly high computational cost. A co-evolutionary decomposition algorithm (CODA) is proposed to improve traffic mobility for urban road networks with time-varying traffic flow. To capture time-varying spatial evolution of traffic flow inside road links, a stochastic traffic model is presented. To fully consider road users’ response, a stochastic bi-level optimization problem (SBOP) is given where road users’ route choice can be fully taken into account. To efficiently implement CORLA in a large-scale road network against high-consequence realization for stochastic capacity, a coordinated co-evolutionary two phase control (CCTPC) is proposed. Numerical experiments are performed at a real-data city road network and various sizable traffic grids. As compared to state-of-the-art traffic signal control for various traffic conditions, obtained results showed that CCTPC exhibits sufficient gain of achieving road network performance and suffers from the least computational cost in all cases.},
  archive      = {J_ASOC},
  author       = {Suh-Wen Chiou},
  doi          = {10.1016/j.asoc.2024.111701},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111701},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Co-evolutionary traffic signal control using reinforcement learning for road networks under stochastic capacity},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of performance enhancement in ethereum fraud
detection using oversampling techniques. <em>ASOC</em>, <em>161</em>,
111698. (<a href="https://doi.org/10.1016/j.asoc.2024.111698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of cryptocurrencies and their decentralized nature, the risk of fraudulent activities within these ecosystems has become a pressing concern. This research paper focuses on Ethereum fraud detection using a dataset specifically curated for this purpose. The methodology encompasses essential steps, including data cleaning, correlation analysis, data splitting, and exploratory data analysis to understand the data characteristics. Subsequently, self-optimized machine learning models are trained with the Pycaret library while addressing the class imbalance using SMOTENC (Synthetic Minority oversampling Technique for Nominal and Continuous Data), ADA-SYN (Adaptive Synthetic Algorithm), and K-Means-SMOTE techniques. The performance of the various models is evaluated on test and validation datasets using metrics such as accuracy, precision, recall, and AUC (Area Under Curve). The study reveals that the ensemble models, particularly CATBoost (Categorical Boost) and LGBM (Light Gradient Boost Method), show exceptional efficiency, with accuracy ranging from 97% to 98.42% after oversampling. Moreover, these models exhibit higher F1 scores and AUC values, indicating their potential to detect fraud effectively. The validation metrics also lie in the same range, demonstrating that the models do not suffer from over-fitting. The experiment demonstrates the promise of ensemble models in Ethereum fraud detection, paving the way for deploying robust fraud detection systems in crypto-currency ecosystems. The results show that the K-Means SMOTE oversampling technique has the highest classification accuracy levels of 98.42% with an AUC of 99.82%.},
  archive      = {J_ASOC},
  author       = {Vaishali Ravindranath and M.K. Nallakaruppan and M. Lawanya Shri and Balamurugan Balusamy and Siddhartha Bhattacharyya},
  doi          = {10.1016/j.asoc.2024.111698},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111698},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluation of performance enhancement in ethereum fraud detection using oversampling techniques},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label disambiguation-based feature selection for partial
label learning via fuzzy dependency and feature discernibility.
<em>ASOC</em>, <em>161</em>, 111692. (<a
href="https://doi.org/10.1016/j.asoc.2024.111692">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial label learning is a multi-class classification issue in which each training instance is associated with a set of candidate labels. Feature selection is an effective method to improve the performance of the learning model, and at the same time, feature selection being a challenging problem in partial label learning due to the label ambiguity of partial labels. To tackle this challenge, this paper proposes a label disambiguation-based feature selection for partial label learning via fuzzy dependency and feature discernibility. Specifically, considering the high sensitivity of the fuzzy rough sets model to pseudo labels, a instance distribution-based label disambiguation method is presented to reduce the noise from the candidate labels. Based on this, a weighted fuzzy rough sets model is constructed in accordance with the distribution information of labels, and the function of fuzzy dependency is redefined. Then, the evaluation function of feature significance is obtained by fusing fuzzy dependency and feature discernibility for identifying the critical features. Finally, extensive experiments have confirmed the feasibility and effectiveness of the proposed method. Compared to other feature selection algorithms, the proposed method exhibits superior performance and enhances the generalization of partial label learning models. Furthermore, the feasibility of the proposed label disambiguation method is demonstrated through comparison with state-of-the-art label disambiguation methods.},
  archive      = {J_ASOC},
  author       = {Wenbin Qian and Jinfei Ding and Yihui Li and Jintao Huang},
  doi          = {10.1016/j.asoc.2024.111692},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111692},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Label disambiguation-based feature selection for partial label learning via fuzzy dependency and feature discernibility},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A decentralized federated learning based interoperable and
heterogeneity aware predictive optimization method for energy and
comfort in smart homes environment. <em>ASOC</em>, <em>161</em>, 111689.
(<a href="https://doi.org/10.1016/j.asoc.2024.111689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a known fact that a comfortable residential space or workspace is crucial to productivity and well-being. In smart home environments, sensors can be used to sense environment and occupant data and actuators can be controlled accordingly for maximizing thermal comfort. However, keeping in view the growing demand of energy and scarcity of energy generation sources, it is mandatory to optimize the energy usage complementary to optimize thermal comfort. In order for the developed method to be interoperable and heterogeneity aware across different edge device vendors and for ease of hardware connectivity in smart homes, a REST API based framework is developed. Sensors mounted with IoT devices collect data from its respective smart home environment and store the data in its local database. An edge server is used to control the connectivity among these IoT devices and to initiate the decentralized federated learning mechanism. Each IoT device train local deep learning models for predicting energy usage, indoor temperature, indoor humidity for maintaining a comfortable environment state using minimal energy. Models from each IoT devices are aggregated at each IoT device iteratively to make federated learning independent of central server. Global deep learning model is iteratively trained for each prediction requirement. Later, these models are used to predict indoor temperature, humidity and energy consumption for time stamp ( t+1 ), keeping in view environmental and occupant related sensed data at time stamp ( t ). Optimization of energy consumption and thermal comfort is mathematically modeled and based on the predictions of DL models, environment is controlled by handling actuators using a particle swarm optimization (PSO) mechanism. Relevant constraints are incorporated in PSO for limiting the positional variations in particles for efficient learning. The method continually learns and adapts to changes in the surroundings, providing real-time monitoring and control of thermal comfort to the end users. To validate our approach, data is collected and experiments are conducted in a real-time smart home’s environment. The results are compared with those obtained without using the proposed predictive optimization mechanism. The study demonstrates the effectiveness of proposed predictive optimization mechanism in providing a comfortable and productive smart home or workspace environment utilizing 38% less energy as compared to competitor.},
  archive      = {J_ASOC},
  author       = {Rongxu Xu and Salabat Khan and Wenquan Jin and Anam Nawaz Khan and Qazi Waqas Khan and Sunhwan Lim and Do Hyuen Kim},
  doi          = {10.1016/j.asoc.2024.111689},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111689},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decentralized federated learning based interoperable and heterogeneity aware predictive optimization method for energy and comfort in smart homes environment},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A joint local spatial and global temporal CNN-transformer
for dynamic facial expression recognition. <em>ASOC</em>, <em>161</em>,
111680. (<a href="https://doi.org/10.1016/j.asoc.2024.111680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike conventional video action recognition, Dynamic Facial Expression Recognition (DFER) tasks exhibit minimal spatial movement of objects. Addressing this distinctive attribute, we propose an innovative CNN-Transformer model, named LSGTNet, specifically tailored for DFER tasks. Our LSGTNet comprises three stages, each composed of a spatial CNN (Spa-CNN) and a temporal transformer (T-Former) in sequential order. The Spa-CNN extracts spatial features from images, yielding smaller-sized feature maps to alleviate the computational complexity for subsequent T-Former. The T-Former integrates global temporal information from the same spatial positions across different time frames while retaining the feature map dimensions. The alternating interplay between Spa-CNN and T-Former ensures a continuous fusion of spatial and temporal information, leading our model to excel across various real-world datasets. To the best of our knowledge, this is the first method to address the DFER challenge by focusing on capturing the temporal changes in muscles within local spatial regions. Our method has achieved state-of-the-art results on multiple in-the-wild datasets and datasets under laboratory conditions.},
  archive      = {J_ASOC},
  author       = {Linhuang Wang and Xin Kang and Fei Ding and Satoshi Nakagawa and Fuji Ren},
  doi          = {10.1016/j.asoc.2024.111680},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111680},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A joint local spatial and global temporal CNN-transformer for dynamic facial expression recognition},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised weight learning-based PSO framework for single
document extractive summarization. <em>ASOC</em>, <em>161</em>, 111678.
(<a href="https://doi.org/10.1016/j.asoc.2024.111678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for automatic text summarization is natural: there is a huge volume of information available online, which prompts for a widespread interest in extracting relevant information in a concise and understandable manner. Here, automated text summarization has been treated as an extractive single-document summarization problem in the proposed system. To solve this problem, a particle swarm optimisation (PSO) algorithm-based approach is suggested, with the goal of producing good summaries in terms of content coverage, informativeness, and readability. This paper introduces XSumm-PSO: a new approach based on PSO optimization technique in a supervised manner for extractive summarization. Further, this paper also contributes a new feature “incorrect word” that captures misspelled words in the candidate sentences. This feature is combined with nine existing features used by proposed model to generate error free summaries. As a result, the proposed XSumm-PSO framework produces superior performance achieving improvements of +2.7%, +0.8%, and +0.8% for ROUGE-1, ROUGE-2, and ROUGE-L scores, respectively, on DUC 2002 dataset, over state-of-the-art techniques. The corresponding improvements on the CNN/DailyMail dataset are +0.97%, +0.25%, and +0.49%. We also performed sample t-test, showing the proposed approach is statistically consistent across various runs.},
  archive      = {J_ASOC},
  author       = {Sangita Singh and Jyoti Prakash Singh and Akshay Deepak},
  doi          = {10.1016/j.asoc.2024.111678},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111678},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Supervised weight learning-based PSO framework for single document extractive summarization},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spherical search algorithm with memory-guided population
stage-wise control for bound-constrained global optimization problems.
<em>ASOC</em>, <em>161</em>, 111677. (<a
href="https://doi.org/10.1016/j.asoc.2024.111677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed Spherical Search (SS) algorithm replaces the traditional square search pattern with a spherical boundary to provide position-diverse solutions. The algorithm balances its exploration and exploitation performance by utilizing 2 exploration and exploitation sub-populations of equal size. SS has been proven to be highly competitive. However, we observed that when it is used to solve a variety of problems as well as during different searching stages, the fixed sub-population size limits its adaptability and flexibility for achieving continuous exploitation–exploration balance. The balance potential of two operators with distinct characteristics is underdeveloped. As a result, SS and its advanced variants are prone to still easily falling into local optima and lacks certain performance advantages over peer algorithms. In this paper, we further develop SS and propose a memory-guided population stage-wise control strategy based SS, called SSM. By our proposed memory-guided stage-wise evaluation mechanism, SS evaluates the exploitation–exploration balance extent in real time and thus adaptively optimizes and predicts better resource allocation ratio values between its 2 sub-populations and thus achieves significant performance advantages over peer algorithms. The experiments are conducted on 120 benchmark functions and 22 real-world problems, and the results show that SSM significantly outperforms other 13 state-of-the-art evolutionary algorithms. Additionally, we conduct analyses based on method characteristics, convergence process, solution quality robustness testing, population diversity, exploitation and exploration balance, and computational complexity.},
  archive      = {J_ASOC},
  author       = {Sichen Tao and Kaiyu Wang and Ting Jin and Zhengwei Wu and Zhenyu Lei and Shangce Gao},
  doi          = {10.1016/j.asoc.2024.111677},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111677},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spherical search algorithm with memory-guided population stage-wise control for bound-constrained global optimization problems},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep semi-supervised regression via pseudo-label filtering
and calibration. <em>ASOC</em>, <em>161</em>, 111670. (<a
href="https://doi.org/10.1016/j.asoc.2024.111670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) is a widely used model training paradigm that effectively utilizes a limited set of labeled data and a substantially larger pool of unlabeled data. Historically, the focus of SSL research has predominantly been on classification tasks, employing methods such as consistency regularization and pseudo-labeling. However, the direct application of these methods to regression tasks presents significant challenges, primarily due to the complexities associated with evaluating the reliability of pseudo-labels in a regression context. This paper introduces SimRegMatch, a novel semi-supervised regression (SSR) framework devised to overcome this specific challenge, by combining an uncertainty-based filtering mechanism with a similarity-based pseudo-label calibration approach. The former component is tasked with discerning which unlabeled examples possess pseudo-labels of sufficient reliability, achieved through the estimation of uncertainty levels. The latter component then refines these pseudo-labels by propagating information from labeled to unlabeled examples, thereby enhancing the overall quality of the pseudo-labels. The efficacy of SimRegMatch was rigorously tested through experiments conducted on the publicly available AgeDB dataset, which is centered around age prediction, as well as on a practical regression problem focused on the detection of interior noise levels in automobiles using accelerometer data. When benchmarked against current state-of-the-art methods in semi-supervised regression, SimRegMatch exhibited notable improvements in regression performance. Additionally, a series of ablation studies were carried out to dissect and understand the specific elements of the framework that were instrumental in achieving these performance enhancements. SimRegMatch addresses a pivotal issue in semi-supervised regression – the assessment of regression pseudo-label reliability – and substantially elevates model performance. By combining the strengths of uncertainty estimation and pseudo-label calibration, SimRegMatch emerges as a robust and versatile framework with significant potential for broad applicability in various SSR scenarios. A PyTorch implementation is publicly available at https://github.com/YongwonJo/SimRegMatch .},
  archive      = {J_ASOC},
  author       = {Yongwon Jo and Hyungu Kahng and Seoung Bum Kim},
  doi          = {10.1016/j.asoc.2024.111670},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111670},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep semi-supervised regression via pseudo-label filtering and calibration},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Prognosis prediction based on liver histopathological image
via graph deep learning and transformer. <em>ASOC</em>, <em>161</em>,
111653. (<a href="https://doi.org/10.1016/j.asoc.2024.111653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver cancer is one of the leading causes of cancer-related deaths globally. Accurately predicting the prognosis of liver cancer patients is crucial for improving their treatment and developing new anticancer drugs. However, analyzing whole slide images is time-consuming and labor-intensive for pathologists. Although deep learning methods can improve analysis efficiency, cancer prognosis prediction remains challenging due to the need for both histological features and context-aware representations to accurately infer patient survival probabilities. Several context-aware models based on graph neural networks have been proposed for weakly supervised deep learning. However, most of these methods extract WSI features using a classification network pretrained on ImageNet, which does not include cancer cell-level images. Additionally, most GNN-based methods employ a fixed number of graph convolutional layers, limiting their ability to learn multi-scale information. To address these limitations, we propose Multi-Trans-GACN, a context-aware parallel multi-scale GNN based on Transformers. Multi-Trans-GACN hierarchically aggregates instance-level histology features on different scales in the liver cancer microenvironment. A Transformer-based scale attention mechanism is utilized to combine the features extracted from different scales. We also propose a method that utilizes InceptionV3, pretrained on cellular-level liver cancer images, to construct graph structures for liver cancer images. We evaluated Multi-Trans-GACN on two liver cancer datasets. Compared to existing methods, our approach achieved significant improvements in the C-index by 5.3 and 2.50, demonstrating its superior performance in liver cancer prognosis prediction tasks. The code id available in https://github.com/z19991013/MTG .},
  archive      = {J_ASOC},
  author       = {Jiawei Zhang and Zhanquan Sun and Kang Wang and Chaoli Wang and Shuqun Cheng and Yu Jiang and Qing Bai},
  doi          = {10.1016/j.asoc.2024.111653},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111653},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prognosis prediction based on liver histopathological image via graph deep learning and transformer},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the use of the differential evolution algorithm for
truss-type structures optimization. <em>ASOC</em>, <em>161</em>, 111372.
(<a href="https://doi.org/10.1016/j.asoc.2024.111372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, bio-inspired numerical algorithms have emerged as an alternative for optimizing the structural design of trusses. The differential evolution algorithm (DEA) has demonstrated both good performance and ease of implementation. However, unlocking the full potential of DEA to address engineering problems poses a significant challenge, necessitating a strategic and informed definition of each component of the algorithm. This research systematically evaluates the influence of defining DEA components on improving the reliability of truss optimization. The algorithm structure of DEA was configured for five aspects: (I) the mutation operator, (II) inclusion of multi-modal techniques, (III) inclusion of parameter control techniques, (IV) definition of the initial population, and (V) local search heuristics. A comprehensive evaluation is conducted to assess the performance of 23 DEA configurations in optimizing eight planar and spatial trusses, varying in size from 10 to 163 elements. Assessment is based on key criteria such as optimal weight, robustness, and computational cost, providing a thorough basis for comparison. The results showed that no tested DEA configuration is the best for all trusses. Instead, the study revealed the presence of recommendable configurations, each tailored to the specific complexities and scales inherent in various truss structures. The integration of multimodal and local search techniques proves particularly advantageous for larger trusses, amplifying the algorithm’s exploratory capabilities to effectively navigate and uncover optimal regions. In contrast, using parameter control technique was more effective in optimizing smaller trusses, capitalizing on the rapid exploration of potential optimal areas in a smaller search space. There was a mutation operator that produced the best results for large trusses and good results for smaller structures. This operator uses the target vector as the base vector and guides its movement from a best-based difference vector, achieving a balance between the exploratory stage and user-defined constraints that promote the exploitation of potentially optimal areas. No discernible impact was observed when the initial population heuristic proposed was used. Finally, this study underscores the feasibility of DEA configurations for optimizing trusses of varying complexities, as proved by a comparison with results from the literature.},
  archive      = {J_ASOC},
  author       = {Oscar Contreras-Bejarano and Jesús Daniel Villalba-Morales},
  doi          = {10.1016/j.asoc.2024.111372},
  journal      = {Applied Soft Computing},
  month        = {8},
  pages        = {111372},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the use of the differential evolution algorithm for truss-type structures optimization},
  volume       = {161},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-hop temporal knowledge graph reasoning with
multi-agent reinforcement learning. <em>ASOC</em>, <em>160</em>, 111727.
(<a href="https://doi.org/10.1016/j.asoc.2024.111727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graph (KG) is a key component of artificial intelligence. In recent years, many large-scale knowledge graphs have been produced and put into practical applications. At present, researchers have proposed many methods to reason facts that do not exist in knowledge graphs using the existing information. However, most traditional reasoning methods lack interpretability, and cannot get the reasoning paths. Therefore, the multi-hop path reasoning method of knowledge graph has gradually become a hot spot. This method finds the next relations through the reinforcement learning agent, gets the whole path, and scores the path. Although these multi-hop path reasoning methods make the reasoning method interpretable, these multi-hop path reasoning methods focus on the choice of relations, ignoring the importance of entities in the path reasoning process. Moreover, with the development of the Temporal Knowledge Graph (TKG), traditional multi-hop path reasoning methods cannot effectively process time information. To solve these problems, a multi-hop path reasoning method of temporal knowledge graph based on multi-agent reinforcement learning is proposed, which is named MA-TPath. MA-TPath uses two agents to perform relation selection and entity selection iteratively. Meanwhile, considering the diversity of temporal reasoning paths, we propose a new type of reward function. In MA-TPath, two agents employ the Long Short-Term Memory Networks (LSTM) to capture current results from the environment and output the corresponding action vectors to the environment through activation functions. Experimentally, MA-TPath outperforms all existing models in 4 out of 4 indicators on the YAGO dataset, 3 out of 4 indicators on the GDETL-5 dataset and the ICEWS-250 dataset, and 1 out of 4 indicators on the ICEWS05–15 dataset. Analysis of the temporal reasoning path indicates that MA-TPath not only surpasses other state-of-the-art reasoning methods over four public temporal knowledge graph datasets but provides rationality for results.},
  archive      = {J_ASOC},
  author       = {Luyi Bai and Mingzhuo Chen and Qianwen Xiao},
  doi          = {10.1016/j.asoc.2024.111727},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111727},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-hop temporal knowledge graph reasoning with multi-agent reinforcement learning},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint learning strategy of multi-scale multi-task
convolutional neural network for aero-engine prognosis. <em>ASOC</em>,
<em>160</em>, 111726. (<a
href="https://doi.org/10.1016/j.asoc.2024.111726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remaining useful life (RUL) prediction and health status (HS) assessment are two key tasks in aero-engine prognostics and health management (PHM) system. However, existing deep learning-based prognostic models perform RUL prediction and HS assessment tasks separately, without considering the correlation between these two tasks. Secondly, traditional deep learning can only extract single-scale features, which limits the ability to extract complex degradation features from high-dimensional condition monitoring data. Therefore, this work proposes a multi-scale and multi-task convolutional neural network for joint learning of aero-engine RUL prediction and HS assessment. Firstly, multi-sensor data with multiple cycles are converted into image samples to integrate more condition monitoring information that is beneficial to prognosis. Then, the multi-scale feature fusion block is designed as the shared network for multi-task, utilizing convolutional layers with filters of different sizes to enhance the ability to extract complex degradation features from high-dimensional condition monitoring data. And a multi-layer concatenation block is constructed to integrate multi-scale features at different levels to fully utilize the important information at different levels. On this basis, a multi-task joint learning block is constructed and a joint loss function is developed for joint learning of RUL prediction and HS assessment. Finally, experiments on two engine degradation datasets, CMAPSS and N-CMAPSS, demonstrate that the proposed network has excellent RUL prediction and HS assessment performance, and outperforms other state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Liang Zhou and Huawei Wang and Shanshan Xu},
  doi          = {10.1016/j.asoc.2024.111726},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111726},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint learning strategy of multi-scale multi-task convolutional neural network for aero-engine prognosis},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simulation-based genetic algorithm for a semi-automated
warehouse scheduling problem with processing time variability.
<em>ASOC</em>, <em>160</em>, 111713. (<a
href="https://doi.org/10.1016/j.asoc.2024.111713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For warehouse operations, efficiently scheduling the available resources is crucial to improve the productivity and customer satisfaction. This paper proposes a simulation-based evolutionary algorithm for order scheduling and multi-robot task assignment in a robotic mobile fulfillment system. The algorithm proactively deals with the effects of the processing time variability by evaluating schedules based on both its system performance as well as its robustness under uncertain conditions. The algorithm implements an efficient resource allocation method and a variance reduction technique to reduce the overall computational burden. The experimental results show that the techniques to reduce the computational time are effective and can significantly reduce the amount of simulations required for the fitness evaluation. If a candidate schedule is allocated insufficient simulation replications it can lead to an inaccurate estimate of its long-term average performance. This could lead to an average performance loss of 7.3 %. Furthermore, the proactive scheduler is able to generate schedules that are more robust compared to deterministically generated. A reduction in the average operational cost of about 5 % can be reached, compared to a deterministically generated schedule. The paper reveals the relevance of identifying and modeling uncertainty when designing schedules in an operational system, rather than looking for optimal schedules for ideal scenarios.},
  archive      = {J_ASOC},
  author       = {Sander Teck and Reginald Dewil and Pieter Vansteenwegen},
  doi          = {10.1016/j.asoc.2024.111713},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111713},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A simulation-based genetic algorithm for a semi-automated warehouse scheduling problem with processing time variability},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GPU-accelerated adaptation of the PSO algorithm for
multi-objective optimization applied to artificial neural networks to
predict energy consumption. <em>ASOC</em>, <em>160</em>, 111711. (<a
href="https://doi.org/10.1016/j.asoc.2024.111711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization research often confronts the challenge of developing time consuming processes. This article introduces an innovative approach that leverages the computational power of Graphics Processing Units (GPUs) to speed up that optimization process. We present an innovative adaptation of Particle Swarm Optimisation (PSO) to meet the requirements of multiobjective optimization problems. This approach aims to leverage the strengths of a multi-objective approach to perform energy consumption prediction using neural networks. By employing GPU parallel techniques, our method not only speeds up the optimization process but also enhances the efficiency of neural network training execution. The main advantage of our approach lies in its dual ability to simultaneously optimizing neural network architectures by determining the minimum number of hidden neurons and fitting the weights of the networks in order to achieve the lowest error. Preliminary results suggest a notable enhancement in prediction accuracy of forecasting electric energy consumption, as a result of optimizing the architecture and parameters of the neural network using the proposed method. This PSO adaptation stands out for its ability to address complex problems, increase efficiency and produce accurate predictions, making it a novel solution in Machine Learning heuristic methods for application in the solution of advanced prediction problems with time constraints from time series.},
  archive      = {J_ASOC},
  author       = {J.R.S. Iruela and L.G.B. Ruiz and D. Criado-Ramón and M.C. Pegalajar and M.I. Capel},
  doi          = {10.1016/j.asoc.2024.111711},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111711},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GPU-accelerated adaptation of the PSO algorithm for multi-objective optimization applied to artificial neural networks to predict energy consumption},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complex hilly terrain agricultural UAV trajectory planning
driven by grey wolf optimizer with interference model. <em>ASOC</em>,
<em>160</em>, 111710. (<a
href="https://doi.org/10.1016/j.asoc.2024.111710">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the limitations of agricultural UAV in performing trajectory planning in complex hilly terrain, a trajectory planning model based on hilly characteristic terrain and agricultural scheduling requirements is proposed. For obtaining an efficient and stable flight operations solution, a multi mechanism grey wolf algorithm based on the interferometric image enhancement model (IIE-GWO) is proposed to improve the population diversity and balance the algorithm&#39;s ability of exploration and exploitation. By comparing the IIE-GWO with twelve significant meta-heuristic algorithms on the CEC2017 functions in a comparative experiment, the results show that the IIE-GWO possesses excellent optimization seeking ability and search stability. Meanwhile, in the trajectory planning part, the effectiveness of the model and algorithms was tested by eight flight tasks with different scales and challenges. The experimental results show that IIE-GWO is able to generate effective trajectory paths, and the flight cost of the agricultural operations performed in the hilly terrain is low, and the trajectory planning capability is more adaptable and effective.},
  archive      = {J_ASOC},
  author       = {Xinyu Liu and Peng Shao and Guangquan Li and Liuxi Ye and Haoyuan Yang},
  doi          = {10.1016/j.asoc.2024.111710},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111710},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complex hilly terrain agricultural UAV trajectory planning driven by grey wolf optimizer with interference model},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal topic modeling from social media data using deep
transfer learning. <em>ASOC</em>, <em>160</em>, 111706. (<a
href="https://doi.org/10.1016/j.asoc.2024.111706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As social media platforms grow rapidly, multi-modal data is becoming more and more prevalent. A user can better understand events by analyzing multimodal data for topics. Automatic topic detection from multimodal data can potentially have tremendous value for advertising and government agencies for whom public opinion matters for strategic decisions and policy making. However , multimodal topic detection is complicated for two reasons: (1) The nature of the multimodal data varies from one medium to another, and (2) The noisy nature of webdata. Conventional topic models are ineffective in dealing with these two problems. This paper proposes, a framework for multimodal topic modeling for social media data that uses topics extracted using Latent Dirichlet Allocation (LDA) and patterns found from images using transfer learning. The proposed framework makes use of textual as well as visual data for topic detection. The experiments are conducted on the benchmark datasets: Flickr8k, Flickr30k, and MCG WEBV. The proposed work outperformed other techniques in terms of accuracy (0.63), precision (0.75), recall (0.97), F-Measure (0.85), Bleu-1(0.68),METEOR (0.17) , ROUGE-L (0.49), and CIDEr (0.573). The proposed work is compared to state-of-the-art methods to demonstrate its accuracy.},
  archive      = {J_ASOC},
  author       = {Seema Rani and Mukesh Kumar},
  doi          = {10.1016/j.asoc.2024.111706},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111706},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-modal topic modeling from social media data using deep transfer learning},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparsify dynamically expandable network via variational
dropout. <em>ASOC</em>, <em>160</em>, 111705. (<a
href="https://doi.org/10.1016/j.asoc.2024.111705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new method for lifelong learning referred to as Sparsify Dynamically Expandable Network (SDEN) via Variational Dropout, which explores a sparse model while preserving the performance. Dynamically Expandable Network (DEN) can learn a sequence of tasks via performing network retraining, network expansion by adding only the necessary neurons, and network split to effectively prevent semantic drift in an online manner. To overcome point estimation of parameters in DEN, Bayesian Compression for DEN is developed under the Bayesian framework. However, this method demands more time for model training and testing. To improve the model efficiency, we propose SDEN under the efficient sparse learning framework. We validate our SDEN in the lifelong learning scenarios with multiple frequently used benchmarks, on which it can obtain comparable classification accuracy, and less training and testing time compared with the comparison methods. Furthermore, our method can also learn a more sparse network structure which means fewer network parameters.},
  archive      = {J_ASOC},
  author       = {Yang Yang and Jie Huang and Dexiu Hu},
  doi          = {10.1016/j.asoc.2024.111705},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111705},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sparsify dynamically expandable network via variational dropout},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SwinYOLOv7: Robust ship detection in complex synthetic
aperture radar images. <em>ASOC</em>, <em>160</em>, 111704. (<a
href="https://doi.org/10.1016/j.asoc.2024.111704">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using satellite-based SAR (Synthetic Aperture Radar) imagery to detect and track ships is a formidable challenge. However, accurate analysis is hampered by inherent difficulties such as obscured edges, multiple targets, varying dimensions and complex backgrounds. These factors contribute to a poor signal-to-noise ratio. Various artificial intelligence models have been developed to improve these problems, especially with YOLO-based models. To overcome the above challenges and achieve robust performance, this study presents a model called SwinYOLOv7, a novel fusion of the YOLOv7 framework with the Features Pyramid Network and the Swin Transformer using ground-breaking anchor-free detection algorithms. This innovative method aims to increase the accuracy of vessel detection while reducing the impact of background clutter. The proposed model improved by YOLOv7 examined three different datasets in detail: SRSDD-v1.0, HRSID, and SSDD to optimize the performance of the model. The training process was consistently verified using superior recall, precision, and F1-score values, which can be easily compared with previous studies. The results show that the model using the Swin Transformer attention mechanism and using an image size of 640×640 achieves the highest accuracy of 96.59%. Alternative attention mechanisms, including the Squeeze-and-Excitation Network (SEnet), the Convolutional Block Attention Module (CBAM), Channel Attention (CA) and Efficient Channel Attention (ECA), deliver poorer accuracy rates. The combination of YOLOv7 and Swin Transformer yielded encouraging results that enabled the proposed model to outperform the current benchmarking models. Therefore, the proposed model provides a compelling solution to ensure accurate vessel identification in complex search and rescue scenarios.},
  archive      = {J_ASOC},
  author       = {Muhammad Yasir and Liu Shanwei and Xu Mingming and Wan Jianhua and Shah Nazir and Qamar Ul Islam and Kinh Bac Dang},
  doi          = {10.1016/j.asoc.2024.111704},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111704},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SwinYOLOv7: Robust ship detection in complex synthetic aperture radar images},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dual-stage and dual-population cooperative evolutionary
algorithm for solving constrained multiobjective problems.
<em>ASOC</em>, <em>160</em>, 111703. (<a
href="https://doi.org/10.1016/j.asoc.2024.111703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the search process, the characteristics of the feasible regions encountered by the population continually change in Constrained Multiobjective Optimization Problems (CMOPs). This variability poses a challenge for traditional evolutionary algorithms, which often struggle to adapt to the diverse problem characteristics of the encountered feasible regions. To overcome this limitation, we propose a Dual-Stage and Dual-Population Cooperative Evolutionary Algorithm (DDCEA) to address CMOPs characterized by diverse feasible regions. DDCEA employs a dual-stage mechanism to adapt the offspring generation strategy and establishes two distinct populations to evaluate offspring using constraint-sensitive and constraint-free strategies. Comparative analyses reveal that DDCEA surpasses chosen state-of-the-art CMOEAs in adapting to the changing feasible regions and then approximating the constrained Pareto fronts.},
  archive      = {J_ASOC},
  author       = {Wenguan Luo and Xiaobing Yu and Gary G. Yen},
  doi          = {10.1016/j.asoc.2024.111703},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111703},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual-stage and dual-population cooperative evolutionary algorithm for solving constrained multiobjective problems},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-start simulated annealing strategy for data lake
organization problem. <em>ASOC</em>, <em>160</em>, 111700. (<a
href="https://doi.org/10.1016/j.asoc.2024.111700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Data Lake Organization Problem consists of optimized data navigation structures generation to reduce the user’s time exploring all available data. The goal is to find a data organization that maximizes the expected probability of table discovery during user navigation. For this problem, we propose a simulated annealing metaheuristic and compare it with the Organize literature solution on benchmark instances. The instances are Socrata Open Data Lake samples with varying topics and open data from government entities worldwide. To validate our proposal, we performed a statistical analysis using a non-parametric test, which confirmed the dominance of our proposition over the state-of-the-art. Our proposal was more efficient and increased the expected probability of table discovery up to 15%. Thus, our strategy can find better solutions in the benchmarks evaluated even without exhaustively analyzing all of them and more effectively exploring the space of solutions.},
  archive      = {J_ASOC},
  author       = {Danilo Fernandes and Geymerson S. Ramos and Rian G.S. Pinheiro and Andre L.L. Aquino},
  doi          = {10.1016/j.asoc.2024.111700},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111700},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-start simulated annealing strategy for data lake organization problem},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixed-batch scheduling to minimize total tardiness using
deep reinforcement learning. <em>ASOC</em>, <em>160</em>, 111699. (<a
href="https://doi.org/10.1016/j.asoc.2024.111699">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the issue of scheduling batch machine to minimize total tardiness. Vacuum heat treatment allows multiple jobs to be processed as a batch, as long as they do not exceed the machine&#39;s weight and size limits; the weight and size of the jobs both impact the batch processing time. By considering the differences in the release time and due date for each job, a mixed-integer linear programming model is developed and validated using small-scale instances. To tackle large-scale scheduling problems, an intelligent algorithm based on deep reinforcement learning is proposed. Double deep Q-learning networks are designed, and the environmental state feature matrix is extracted as input parameters for the networks. Four job-sorting rules and three scheduling time windows are defined, resulting in twelve action rules within the action space. The most suitable action rule is dynamically selected based on the current batch&#39;s environmental status during the scheduling process. Through extensive comparison using large-scale random data, the proposed algorithm demonstrates significantly improved scheduling performance compared to the benchmark algorithms.},
  archive      = {J_ASOC},
  author       = {JinDian Huang},
  doi          = {10.1016/j.asoc.2024.111699},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111699},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mixed-batch scheduling to minimize total tardiness using deep reinforcement learning},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing collaborative learning automata to guide
multi-objective optimization based inverse analysis for structural
damage identification. <em>ASOC</em>, <em>160</em>, 111697. (<a
href="https://doi.org/10.1016/j.asoc.2024.111697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural damage identification based on physical models is often transformed into an optimization problem that minimizes the difference between measurement information of structure being monitored and the model prediction in the parametric space. However, the objective function in this context often exhibits multimodality, involving high-dimensional variables due to the reliance on finite element models for damage identification. These features pose challenges to optimization algorithms, where entrapment in local solutions can lead to false positives and false negatives in damage identification. In this research, we propose a reinforcement learning based multi-swarm optimizer to tackle such challenges in pursuit of a small yet diverse solution set that can capture the true damage scenario as one of the solutions. The proposed method leverages the flexibility of the particle swarm optimizer and incorporates novel strategies of metaheuristics to realize targeted improvement. To enable the particle swarm to adaptively select the appropriate search strategy based on the current environment, we adopt the learning automata technique, which sidesteps the need for reward strategy selection that is usually ad hoc at each step of the search. The integration harnesses the automatic learning and self-adaptation capabilities of learning automata, enabling the particles to navigate based on environmental signals. This leads to accumulated probabilities tied to advantageous movements, fostering an adaptive exploration of particles in the search space. The proposed approach is first validated through implementing into benchmark test cases with comparisons. It is then applied to structural damage identification with piezoelectric admittance experimental signals. `The results highlight the capability of the algorithm to identify a small solution set with high accuracy to match the actual damage scenario.},
  archive      = {J_ASOC},
  author       = {Yang Zhang and Kai Zhou and Jiong Tang},
  doi          = {10.1016/j.asoc.2024.111697},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111697},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Harnessing collaborative learning automata to guide multi-objective optimization based inverse analysis for structural damage identification},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Metaheuristics exposed: Unmasking the design pitfalls of
arithmetic optimization algorithm in benchmarking. <em>ASOC</em>,
<em>160</em>, 111696. (<a
href="https://doi.org/10.1016/j.asoc.2024.111696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work unveils design flaws within most metaheuristics, with a specific focus on issues associated with the arithmetic optimization algorithm (AOA). Despite being a simple metaheuristic optimizer inspired by mathematical operations, AOA holds promise for addressing complex real-world applications. However, a thorough analysis of its search mechanism reveals a heavy dependence on problem bounds for the quality of solutions obtained by AOA. Additionally, discrepancies between algorithm descriptions and implementations in AOA can mislead users and impede progress within the metaheuristic community. Experimental simulations conducted on various standard benchmarks including their shifted versions indicate a structural bias in AOA, leading to artificially high accuracy in fitting standard test functions but poor performance when applied to shifted benchmarks. Finally, we give a critical cause analysis and conclude this article by highlighting valuable research avenues in this field.},
  archive      = {J_ASOC},
  author       = {Lingyun Deng and Sanyang Liu},
  doi          = {10.1016/j.asoc.2024.111696},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111696},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristics exposed: Unmasking the design pitfalls of arithmetic optimization algorithm in benchmarking},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way decisions with dual hesitant fuzzy covering-based
rough set and their applications in medical diagnosis. <em>ASOC</em>,
<em>160</em>, 111695. (<a
href="https://doi.org/10.1016/j.asoc.2024.111695">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of partition, fuzzy β β -covering can provide a more realistic and accurate description of incomplete information. In this paper, we mainly integrate the idea of fuzzy β β -covering with dual hesitant fuzzy (DHF) information and construct some novel three-way decision (3WD) models with DHF covering-based rough set. Firstly, we propose the notions of DHF β β -covering, DHF β β -minimal description and DHF β β -maximal description, and then construct three types of DHF neighborhood operators. Meanwhile, we introduce DHF conditional probability by using DHF neighborhood operator. Secondly, in terms of DHF conditional probability, DHF covering-based probabilistic rough set, DHF covering-based decision-theoretic rough set and their 3WD models are established. In light of DHF neighborhood operator, we propose two pairs of optimistic and pessimistic DHF decision evaluation functions and build two 3WD models through them. Lastly, a numerical example is employed to elaborate the application of the above models, which is effective and credible to medicine screening for Alzheimer’s disease.},
  archive      = {J_ASOC},
  author       = {Wei Li and Xiaolei Wang and Bin Yang},
  doi          = {10.1016/j.asoc.2024.111695},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111695},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decisions with dual hesitant fuzzy covering-based rough set and their applications in medical diagnosis},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Matheuristics for mixed-model assembly line balancing
problem with fuzzy stochastic processing time. <em>ASOC</em>,
<em>160</em>, 111694. (<a
href="https://doi.org/10.1016/j.asoc.2024.111694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our work aims to investigate methods for solving the mixed-model assembly line balancing problem (MALBP) under uncertainty with the objective of minimizing the number of workstations. Specifically, we model task processing time as fuzzy stochastic variables (FRVs) due to the inherent uncertainties and variations in the manufacturing environment. Additionally, we introduce a ranking method for FRVs and propose a mathematical model to address MALBP. The recently developed Red Fox Optimization (RFO) algorithm is also discretized for the first time to support solving this problem. Finally, matheuristic algorithms combine a metaheuristic such as the popular Genetic Algorithm (GA), Particle Swarm Optimization (PSO), or the Discretized Red Fox Optimization (DRFO) algorithm with the Mixed-Integer Programming (MIP) model to generate the best solution in a reasonable time. Our comparative results demonstrate that the GA-MIP combination outperforms the others in both objective value and computational time.},
  archive      = {J_ASOC},
  author       = {Truong Tran Mai Anh and Nguyen Van Hop},
  doi          = {10.1016/j.asoc.2024.111694},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111694},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Matheuristics for mixed-model assembly line balancing problem with fuzzy stochastic processing time},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach to extract topological information from
intuitionistic fuzzy sets and their application in obtaining a natural
hierarchical clustering algorithm. <em>ASOC</em>, <em>160</em>, 111691.
(<a href="https://doi.org/10.1016/j.asoc.2024.111691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological data analysis (TDA) is a powerful mathematical framework that extracts valuable insights about the shape and structure of complex datasets by identifying and analyzing underlying topological features, including connected components, holes, voids, and higher-dimensional counterparts. TDA achieves this by constructing a simplicial complex from the data, comprising simple shapes like points, lines, triangles, and higher-dimensional counterparts. The Vietoris–Rips complex (VRC), a widely used method, constructs simplicial complexes by measuring the distances between data points in a metric space. While the link between Intuitionistic Fuzzy Sets (IFSs) and TDA is clear through intuitionistic fuzzy distance measures, the literature has not explored the application of TDA for extracting topological features from uncertain and vague datasets using IFSs. In this paper, we propose a novel approach that leverages Intuitionistic Fuzzy Distance Measures (IFDMs) to generate the Intuitionistic Fuzzy Vietoris–Rips Complex (IFVRC) of IFSs. Specifically, we investigate the persistence of invariant relations of topological features between IFVRCs constructed using different distances, including Atanassov’s Euclidean distance ( l 2 I F S l2IFS ), Boran and Akay’s distance ( D D ), and Liu’s distance measure ( d L dL ), over artificially generated IFSs. Furthermore, we demonstrate that the generation of IFVRC can be viewed as a Natural Hierarchical Clustering Algorithm (NHCA) for IFSs. Finally, we apply the proposed IFVRC as a Natural Hierarchical Clustering Algorithm to cluster intuitionistic fuzzy datasets related to Car and Building materials. Moreover, we incorporate a relatively large dataset, the GTZAN dataset from the Kaggle datasets repository, to classify music genres based on their topological information.},
  archive      = {J_ASOC},
  author       = {Mohd Shoaib Khan},
  doi          = {10.1016/j.asoc.2024.111691},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111691},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An approach to extract topological information from intuitionistic fuzzy sets and their application in obtaining a natural hierarchical clustering algorithm},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traffic trajectory generation via conditional generative
adversarial networks for transportation metaverse. <em>ASOC</em>,
<em>160</em>, 111690. (<a
href="https://doi.org/10.1016/j.asoc.2024.111690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transportation Metaverse, by integrating real and virtual vehicular networks, brings significant benefits to the development of smart cities. However, the difficulty and high cost of conducting large-scale traffic and driving simulations in the transportation Metaverse via realistic data collection and fusion from the physical world directly result in the lack of spatial–temporal traffic data and privacy concerns, which significantly hinder the development of the transportation Metaverse. Hence, in these situations, it becomes essential to produce high-quality, large-scale trajectory data to support relevant applications. However, the deficiency of data-driven method is the heavy dependence on high-quality historical data. Prior knowledge can help reduce learning difficulties and overfitting problems with small amounts of data. In our study, we use a hybrid framework, the Travel Demand Conditioning Generative Adversarial Network (TD-GAN), which combines data-driven and knowledge-driven approaches to address the issue of traffic trajectory generation. First, we employ a conditional mechanism to incorporate prior knowledge of travel demand to reduce learning difficulties. Second, non-standard convolutional module and multi-headed self-attention module are developed to capture spatial–temporal correlations. The experimental results show that our model outperforms the baseline models.},
  archive      = {J_ASOC},
  author       = {Xiangjie Kong and Junhui Bi and Qiao Chen and Guojiang Shen and Tachia Chin and Giovanni Pau},
  doi          = {10.1016/j.asoc.2024.111690},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111690},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Traffic trajectory generation via conditional generative adversarial networks for transportation metaverse},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group decision making with incomplete triangular fuzzy
multiplicative preference relations for evaluating third-party reverse
logistics providers. <em>ASOC</em>, <em>160</em>, 111688. (<a
href="https://doi.org/10.1016/j.asoc.2024.111688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The strategic management of reverse logistics (RL) is essential for enterprises to enhance their operational efficiency, customer satisfaction, and sustainability performance in today&#39;s competitive marketplace. Many manufacturing firms have to cooperate with professional RL providers to overcome resource constraints and technological limitations, ultimately driving business success. Thus, it is critical for every enterprise to select the most suitable third-party RL provider (3PRLP). This paper provides a novel group decision making (GDM) method with incomplete triangular fuzzy multiplicative preference relations (TFMPRs) to cope with the selection of the most optimal 3PRLP. Firstly, a definition of acceptable incomplete TFMPRs is given. Then, the sufficient and necessary condition of an acceptable incomplete TFMPR is proposed. By analyzing the properties of consistent TFMPRs, a graph-based algorithm is designed to estimate the unknown elements in incomplete TFMPRs. Based on the proposed acceptable consistency definition of TFMPRs, an optimization model is set up to improve the consistency degree of inconsistent TFMPRs. The optimal normalized triangular fuzzy multiplicative weight vector (Tri-MWV) is obtained by computing two analytic expressions and solving a linear programming model. To measure the closeness degree of two TFMPRs, the concept of logarithmic correlation coefficient ( LCC ) between two TFMPRs is proposed. Combining the incomplete preference information in incomplete TFMPRs with the LCC s of any two TFMPRs, an algorithm of computing experts’ weights is displayed. Subsequently, a novel method of GDM with incomplete TFMPRs is presented. Lastly, a practical example of evaluating 3PRLPs is conducted to illustrate the effectiveness of the proposed GDM method with incomplete TFMPRs.},
  archive      = {J_ASOC},
  author       = {Xianjuan Cheng and Changxiong Chen and Shuping Wan},
  doi          = {10.1016/j.asoc.2024.111688},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111688},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Group decision making with incomplete triangular fuzzy multiplicative preference relations for evaluating third-party reverse logistics providers},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive reinforcement learning-based control using proximal
policy optimization and slime mould algorithm with experimental tower
crane system validation. <em>ASOC</em>, <em>160</em>, 111687. (<a
href="https://doi.org/10.1016/j.asoc.2024.111687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel optimal reference tracking control approach resulted from the combination of a popular policy gradient Reinforcement Learning (RL) algorithm, namely Proximal Policy Optimization (PPO), and a metaheuristic Slime Mould Algorithm (SMA). One of the most important parameters in the PPO-based RL process is the learning rate, which has a big impact on how the parameters of the actor neural network (NN) are iteratively updated. In every episode of the RL process, the weights and the biases of the actor NN are multiplied with the learning rate, determining how much the learning agent will step into a certain direction computed based on previous experiences. The classical PPO algorithm usually relies on fixed values for the learning rates which rarely change, or not at all, during the learning process. However, its main drawback is that the learning agent cannot take advantage of positive momentum in the learning process by accelerating towards good learning experiences or slow down and quickly change the direction in the case of consecutive negative learning experiences. The main objective of the combination proposed in this paper is to create an adaptive SMA-based PPO approach applied to control systems, which instead of using fixed learning rate values, it uses the SMA to compute optimal values of the learning rates in each time step of the learning process based on the progress of the learning agent. This paper investigates if the adaptive SMA-based PPO control approach can be considered as an alternative to the classical PPO version, which employs fixed values of the learning rate. A comparison is carried out using control system performance indices gathered while performing an optimal reference tracking control task on tower crane system laboratory equipment.},
  archive      = {J_ASOC},
  author       = {Iuliu Alexandru Zamfirache and Radu-Emil Precup and Emil M. Petriu},
  doi          = {10.1016/j.asoc.2024.111687},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111687},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive reinforcement learning-based control using proximal policy optimization and slime mould algorithm with experimental tower crane system validation},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine vision approach with temporal fusion strategy for
concrete vibration quality monitoring. <em>ASOC</em>, <em>160</em>,
111684. (<a href="https://doi.org/10.1016/j.asoc.2024.111684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In concrete construction, ensuring the quality of vibration is paramount for maintaining the strength, durability, and quality of structures. This study proposed a method for monitoring the vibration quality of vibrating robots to replace subjective human judgment. The method employed an improved EfficientNetV2 model to classify the vibration quality into four levels: unqualified, middle, qualified, and over-vibration. A temporal fusion strategy was introduced, employing time-domain probability fusion to enhance the accuracy and stability of the results. Additionally, image patching was applied to reduce computational complexity while preserving feature integrity. Experimental results demonstrate that the proposed method outperforms common mainstream models, achieving an accuracy of 96.47%, with a relatively small parameter size of 13.8 M. Compared to non-temporal fusion strategy, the accuracy is improved by 2.06%. This research has been successfully applied in practical engineering, providing a reliable means of quality assurance for concrete structures and demonstrating potential application prospects in the field of engineering construction.},
  archive      = {J_ASOC},
  author       = {Tan Li and Hong Wang and Dongxu Pan and Jiasheng Tan and Junxu Hou and Lingjie Kong and Jingbo Liu},
  doi          = {10.1016/j.asoc.2024.111684},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111684},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A machine vision approach with temporal fusion strategy for concrete vibration quality monitoring},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep residual neural network model for synchronous motor
fault diagnostics. <em>ASOC</em>, <em>160</em>, 111683. (<a
href="https://doi.org/10.1016/j.asoc.2024.111683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronous motors play a significant role in a wide range of industrial applications. Their reliable operation is paramount. Any faults in synchronous motors can lead to costly downtime, decreased productivity, and potential safety hazards. By accurately diagnosing and classifying faults, we can proactively address issues before they escalate, ensuring the smooth operation of synchronous motors and minimizing the risk of equipment failure. The accurate diagnosis and fault detection in synchronous motors pose a significant challenge in their operation and maintenance. In the existing models, the feature data at various depths are not thoroughly extracted to maximize their feature extraction capability. Additionally, they employ a single support vector machine to make the final decision on the output. The single support vector machine may not consistently produce more accurate outcomes. Therefore, this paper proposes a novel fault diagnosis model based on a deep residual neural network and multiple support vector machines to diagnose mechanical and electrical faults of synchronous motors. The proposed model improves upon existing fault diagnosis models in two key aspects. Firstly, by employing a deep neural network, the model is able to effectively process and extract fault features from the motor fault dataset, capturing more nuanced information that may be missed by existing models. Secondly, the use of multiple support vector machines enhances the decision-making capability of the model, allowing for more accurate fault diagnosis. By combining these two aspects, the proposed model achieves superior diagnostic performance compared to single support vector machine-based models. Our proposed model has been rigorously evaluated using mechanical and electrical fault datasets, and the results of experimental tests clearly demonstrate its superior diagnostic performance when compared to existing fault diagnosis models. The synergy of deep neural networks and multiple support vector machines not only improves fault detection accuracy but also enhances the robustness and generalizability of the model, making it a valuable tool for real-world industrial applications.},
  archive      = {J_ASOC},
  author       = {S. Ida Evangeline and S. Darwin and E. Fantin Irudaya Raj},
  doi          = {10.1016/j.asoc.2024.111683},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111683},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep residual neural network model for synchronous motor fault diagnostics},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining traditional and spiking neural networks for
energy-efficient detection of eimeria parasites. <em>ASOC</em>,
<em>160</em>, 111681. (<a
href="https://doi.org/10.1016/j.asoc.2024.111681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of bacterial and viral microbes is pivotal for both human and animal well-being in the public health services and for veterinary care. Even in a laboratory, the isolation of microorganisms requires time-consuming procedures and expert technicians. However, the rise of Machine Learning and Deep Learning has seen a surge in the application of techniques that can be applied outside the laboratory to classify microorganisms using microscopy images. Yet, despite their success in various domains, Deep Learning approaches tend to have high energy demands, which can in some contexts limit their application, increasing both costs and environmental concerns. In this study, a novel hybrid methodology was proposed in which an Artificial Neural Network was combined with a Spiking Neural Network. A quality measure was proposed to assess the effectiveness of the hybrid models, in which their energy efficiency, energy consumption patterns, performance levels and accuracy were all considered. The synergy of both methods markedly reduced the energy footprint of deep-learning models programmed to detect microorganisms, increasing their environmental sustainability and the feasibility of their use in places with little or no electricity supply. The efficacy of our model was demonstrated through the detection and the classification of different species of the Eimeria parasite on chicken and rabbit farms.},
  archive      = {J_ASOC},
  author       = {I.X. Vázquez and B.W.D. Ayasi and H. Seker and J. Luengo and J. Sedano and A.M. García-Vico},
  doi          = {10.1016/j.asoc.2024.111681},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111681},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining traditional and spiking neural networks for energy-efficient detection of eimeria parasites},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective model for selecting response strategies of
primary and secondary project risks under interval-valued fuzzy
uncertainty. <em>ASOC</em>, <em>160</em>, 111679. (<a
href="https://doi.org/10.1016/j.asoc.2024.111679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risks are uncertain events that can affect criteria such as project cost, quality, and completion time and ultimately lead to project failure. That is why the project risk management process, which is one of the most fundamental parts of project management, must be done properly. In this paper, a new multi-objective fuzzy model is proposed to respond appropriately to the primary and secondary project risks. The contributions, such as risk interdependency, considering the project with multi-mode activities, resource considerations, as well as attention to interval-valued fuzzy uncertainties are regarded simultaneously for the first time. The purpose of the proposed multi-objective mathematical model is to minimize cost, quality reduction, and project completion time. Then, a new two-stage uncertain solution approach is introduced in this paper. In this solution approach, using an equivalent method in the first step and then using an extended multi-choice goal programming method in the second step, for both lower and upper limits of the membership functions for interval-valued fuzzy numbers, the computational processes are performed. To show the application and effectiveness of the proposed model, a case study adapted from the literature is given. Finally, sensitivity analysis is conducted to analyze the behavior of the developed mixed-integer programming model. The analysis of obtained results indicates that the utilization of the proposed model leads to better and more appropriate solutions.},
  archive      = {J_ASOC},
  author       = {Elham Ahmadi and Seyed Meysam Mousavi and Samira Khojasteh Eghbali},
  doi          = {10.1016/j.asoc.2024.111679},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111679},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective model for selecting response strategies of primary and secondary project risks under interval-valued fuzzy uncertainty},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Differential evolutionary particle swarm optimization with
orthogonal learning for wind integrated optimal power flow.
<em>ASOC</em>, <em>160</em>, 111662. (<a
href="https://doi.org/10.1016/j.asoc.2024.111662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a novel variant of particle swarm optimization (PSO), which improves its balance of exploration and exploitation by modifying neighborhood topology, self-adaptive parameter strategies and deep search, namely differential evolutionary evolution PSO with orthogonal learning (OL), i.e., DEEPSO-OL in short. Evolutionary computing can explore the solution space efficiently because of its self-evolving attribute as iteration continues. The OL enhances its exploitation by focusing on deeper search for promising solutions. It utilizes the concept of orthogonal experimental design (OED) which predicts the best combination of control variables without exhaustive evaluation of all possible combinations. In addition, to avoid premature convergence in a local optimum, a stochastic star topology for particles is proposed. Such topology ensures just enough communication among the best performing particles, while encouraging them to explore other spaces. The efficacy of the algorithm is evaluated through real-world scenarios such as optimal power flow (OPF) and wind integrated OPF, which are hard to solve with classical mathematical methods. The proposed algorithm is run on a modified IEEE 30-bus test system and compared to the state-of-the-art evolutionary computing algorithms for a variety of cost objective functions with high levels of non-linearity and non-convexity. The DEEPSO-OL demonstrates its performance to generate more accurate feasible solutions and construct promising and efficient search method for real-world complex optimization problems.},
  archive      = {J_ASOC},
  author       = {Wenlei Bai and Fanlin Meng and Ming Sun and Haoxiang Qin and Richard Allmendinger and Kwang Y. Lee},
  doi          = {10.1016/j.asoc.2024.111662},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111662},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Differential evolutionary particle swarm optimization with orthogonal learning for wind integrated optimal power flow},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human-inspired similarity control system: Enhancing
line-following robot perception. <em>ASOC</em>, <em>160</em>, 111660.
(<a href="https://doi.org/10.1016/j.asoc.2024.111660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Inspired Control (HIC) holds promise for endowing machines with human-like cognition, decision-making, and adaptability. In this study, we employ a fusion of cognitive modeling, machine learning, and control theory as the foundational architecture and empirically validate its suitability for robot control within the realm of HIC. Fuzzy logic stands out as a viable approach for HIC control, wherein control rules can be devised drawing from human intuitive inspiration. Specifically, this study explores similarity inference control systems in robotics, with the objective of enhancing line-following control as an alternative to fuzzy systems. The experimental optimization results provide insights into the advantages and limitations of the similarity inference control system. Despite achieving performance comparable to that of traditional two-stage fuzzy control systems, careful consideration of noise sensitivity is paramount. While the similarity inference approach streamlines implementation and obviates the necessity for expert-designed fuzzy rules , its susceptibility to noise can compromise performance, particularly in noisy environments . These considerations are pivotal for the development of control systems aimed at mitigating noise sensitivity, enhancing task-specific performance, and ensuring the adaptability and robustness of line-following robots. To tackle this challenge, we both discuss and experimentally evaluate potential solutions and their applicability in this paper.},
  archive      = {J_ASOC},
  author       = {Yukinobu Hoshino and Yuka Nishiyama and Toshimi Yamamoto and Yuki Shinomiya and Namal Rathnayake and Tuan Linh Dang},
  doi          = {10.1016/j.asoc.2024.111660},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111660},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human-inspired similarity control system: Enhancing line-following robot perception},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using hill climb assembler encoding neural networks to
control follower vehicles in an underwater swarm. <em>ASOC</em>,
<em>160</em>, 111647. (<a
href="https://doi.org/10.1016/j.asoc.2024.111647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents the application of neural networks evolving under the Hill Climb Assembler Encoding (HCAE) algorithm to control follower autonomous underwater vehicles that are members of a swarm consisting of one leader vehicle and a group of followers. The leader is responsible for global navigation and guiding the swarm, while low-cost followers unable to cover longer distances on their own follow the leader. To locate the leader, the followers only use information about the distance to it. Directional information is unavailable to the followers. Moreover, information about the distance is transmitted to followers with a frequency depending on the number of followers: the more followers, the lower the frequency. In addition to tracking the leader, the followers must also be able to avoid collisions with other followers and the leader. To this end, they are equipped with short-range sensors looking around each follower. The simulations presented in the paper were carried out for different swarm sizes, different sensor ranges, different collision distances, and variable leader speed, and showed high effectiveness of the proposed neural solution. The most common strategy for the followers when following a leader was to circle it at a certain safe distance depending on the sensor range.},
  archive      = {J_ASOC},
  author       = {Tomasz Praczyk},
  doi          = {10.1016/j.asoc.2024.111647},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111647},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using hill climb assembler encoding neural networks to control follower vehicles in an underwater swarm},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the fusion of soft-decision-trees and concept-based
models. <em>ASOC</em>, <em>160</em>, 111632. (<a
href="https://doi.org/10.1016/j.asoc.2024.111632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of eXplainable Artificial Intelligence (XAI), the generation of interpretable models that are able to match the performance of state-of-the-art deep learning methods is one of the main challenges. In this work, we present a novel interpretable model for image classification that combines the power of deep convolutional networks and the transparency of decision trees. We explore different training techniques where convolutional networks and decision trees can be trained together using gradient-based optimization methods as usually done in deep learning environments. All of this results in a transparent model in which a soft decision tree makes the final classification based on human-understandable concepts that are extracted by a convolutional neural network. We tested the proposed solution on two challenge image classification datasets and compared them with the state-of-the-art approaches, achieving competitive results.},
  archive      = {J_ASOC},
  author       = {David M. Rodríguez and Manuel P. Cuéllar and Diego P. Morales},
  doi          = {10.1016/j.asoc.2024.111632},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111632},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the fusion of soft-decision-trees and concept-based models},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multitask augmented random search in deep reinforcement
learning. <em>ASOC</em>, <em>160</em>, 111605. (<a
href="https://doi.org/10.1016/j.asoc.2024.111605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) has gained significant popularity in recent years for its ability to solve complex control problems. However, most existing RL algorithms are designed to train policies for each environment in isolation, limiting their applicability to real-world scenarios with many related environments. Recently, many multitask optimization algorithms have been proposed and successfully applied to a wide range of optimization problems. However, existing studies mainly focus on solving multiple continuous function benchmarks and overlook the potential of tailoring towards RL. In this paper, we propose a simple multitask optimization algorithm called Multitask Augmented Random Search (MARS) that trains multiple RL agents together and exploits the performance surplus from highly correlated tasks. MARS is a modification of the simple random search Augmented Random Search (ARS) algorithm, which has been shown to outperform complicated methods in solving continuous control MuJoCo environments such as Soft Actor-Critic (SAC), Proximal Policy Optimization (PPO), and Trusted Region Policy Optimization (TRPO). The experimental results also demonstrate that our proposed algorithm is more consistent in solving different instances of MuJoCo benchmark than ARS, Multifactorial Evolution Algorithm (MFEA), and Adaptive MFEA RL (AMFEARL) within the same number of training episodes.},
  archive      = {J_ASOC},
  author       = {Le Tien Thanh and Ta Bao Thang and Le Van Cuong and Huynh Thi Thanh Binh},
  doi          = {10.1016/j.asoc.2024.111605},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111605},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multitask augmented random search in deep reinforcement learning},
  volume       = {160},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lossless and lossy remote sensing image
encryption-compression algorithm based on DeepLabv3+ and 2D CS.
<em>ASOC</em>, <em>159</em>, 111693. (<a
href="https://doi.org/10.1016/j.asoc.2024.111693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of remote sensing technology, the amount of remote sensing image (RSI) has increased sharply. The explosion in data volume requires higher standards of image compression and encryption technology. This paper proposes a lossless and lossy encryption-compression method for RSI. Because RSIs contain important and unimportant information, using a normal algorithm could ignore their important information or reduce the computational efficiency, so we use the trained model by DeepLabv3+ to segment the region of interest (ROI) and the region of non-interest (RONI), and implement lossless and lossy algorithms respectively. The whole algorithm is based on the newly proposed hyperchaotic system 2D Tent coupled Infinite collapse map (2D TICM). According to the 2D TICM, the dynamic 3D Latin cube is generated. This cube is then used for dynamic scrambling algorithms between multiple planes (3D LMBS), within planes (3D LMIS), and for the multiple dynamic S boxes substitution algorithm. Due to the large size of RSI, the image is divided into blocks, and MATLAB parallel mechanism is used to encrypt each block at the same time. First, the lossy part is scrambled by 3D LMIS, then compressed with 2D compressive sensing (2D CS). Finally, each block is substituted with a different S box. A CS reconstruction algorithm combining decryption and reconstruction, 2D projection gradient chaotic decryption algorithm (2D PG-CD), is proposed. The lossless part embeds the encryption into JPEG-LS, encrypts while compressing, and then encrypts according to 3D LMBS and S boxes. The algorithm in this paper considers the characteristics of RSI, experiments show that it has good security and compression performance.},
  archive      = {J_ASOC},
  author       = {Hao Zhang and Shi-xian Nan and Zi-hao Liu and Jie Yang and Xiu-fang Feng},
  doi          = {10.1016/j.asoc.2024.111693},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111693},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lossless and lossy remote sensing image encryption-compression algorithm based on DeepLabv3+ and 2D CS},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Evaluating edge artificial intelligence-driven supply chain
management platforms using collaborative large-scale fuzzy information
fusion. <em>ASOC</em>, <em>159</em>, 111686. (<a
href="https://doi.org/10.1016/j.asoc.2024.111686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of edge artificial intelligence (AI) has brought about revolutionary changes in supply chain management (SCM). It not only provides real-time data processing capabilities but also accelerates the decision-making process, injecting more innovative elements into SCM. In this context, SCM platforms require new technological developments and effective evaluations, as they collectively drive the efficient coordination of complex business processes. SCM platforms enable the seamless coordination of various supply chain elements, facilitating streamlined operations and enhanced decision-making processes. The evaluation of these platforms not only serves to validate their performance and effectiveness, but also contributes to continual improvements in design and application, and addressing the evolving demands within the realms of global commerce and dynamic market conditions. The ongoing assessment and enhancement of SCM platforms play a crucial role in optimizing resource allocation, improving production efficiency, and fostering adaptability to changing market dynamics. However, the acquisition of assessment data often introduces imprecise data. Additionally, the large-scale nature and bounded rationality of decision-makers (DMs) significantly impact the evaluation of SCM platforms. This article aims to explore a collaborative large-scale information fusion approach and provide a robust fuzzy framework for evaluating SCM platforms. This approach employs a combination of spherical fuzzy sets (SFSs), large-scale group decision-making (LSGDM), behavioral theories and three-way decisions (TWD) to thoroughly explore collaborative large-scale information fusion and its practical application in assessing SCM platforms. It introduces an innovative spherical fuzzy (SF) LSGDM technique and integrates TWD with the inclusion of prospect theory (PT) and regret theory (RT) to mitigate potential decision risks. The developed collaborative large-scale information fusion approach is assessed for validity, effectiveness and practicality in the context of evaluating SCM platforms using online data. Experimental results demonstrate that this approach provides reasonable evaluation outcomes, considering uncertain information processing capabilities, large-scale characteristics, bounded rationality and decision risks.},
  archive      = {J_ASOC},
  author       = {Chao Zhang and Jingjing Zhang and Arun Kumar Sangaiah and Deyu Li and Wentao Li},
  doi          = {10.1016/j.asoc.2024.111686},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111686},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evaluating edge artificial intelligence-driven supply chain management platforms using collaborative large-scale fuzzy information fusion},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Density-based clustering with boundary samples verification.
<em>ASOC</em>, <em>159</em>, 111685. (<a
href="https://doi.org/10.1016/j.asoc.2024.111685">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density-based clustering is a widely explored domain in the field of machine learning, with numerous methods proposed to address challenges in clustering under varying density conditions. These methods primarily categorize samples based on their local density. However, challenges arise when dealing with boundary samples. The density of certain boundary samples is typically lower, which can lead to misclassification as noise. Moreover, in situations where two clusters have similar densities and are in close proximity, accurately classifying the boundary points between them becomes a challenging task. In this paper, we introduce an enhanced approach based on k k -nearest neighbors to address this challenge within density-based clustering. In our method, upon the formation of a new cluster, we identify boundary samples by examining the spatial relationships between each sample and its k k -nearest neighbors, as well as their connections to the newly established cluster. Once all clusters are formed, we refine the classification of these boundary samples by adjusting or retaining their assigned labels based on their k k -nearest neighbors. Experimental evaluations on synthetic and real-world datasets demonstrate the effectiveness of our proposed method.},
  archive      = {J_ASOC},
  author       = {Jie Peng and Yong Chen},
  doi          = {10.1016/j.asoc.2024.111685},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111685},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Density-based clustering with boundary samples verification},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pareto fronts relationship identification-based two-stage
constrained evolutionary algorithm. <em>ASOC</em>, <em>159</em>, 111674.
(<a href="https://doi.org/10.1016/j.asoc.2024.111674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Striking a balance between diverse constraints and conflicting objectives is one of the most crucial issues in solving constrained multi-objective optimization problems (CMOPs). However, it remains challenging to existing methods, due to the reduced search space caused by the constraints. For this issue, this paper proposes a Pareto fronts relationship identification-based two-stage constrained evolutionary algorithm called RITEA, which balances objective optimization and constraint satisfaction by identifying and utilizing the relationship between the unconstrained Pareto front (UPF) and the constrained Pareto front (CPF). Specifically, the evolutionary process is divided into two collaborative stages: training stage and reinforcement stage. In the training stage, a relationship identification method is developed to estimate the relationship between UPF and CPF, which guides the population search direction. In the reinforcement stage, the corresponding evolutionary strategies are designed based on the identified relationship to enhance the accurate search on the CPF. Furthermore, a dynamic preference fitness function (termed DPF ) is designed to adaptively maintain the balance of search preference between convergence and diversity. Compared to seven state-of-the-art algorithms on 36 benchmark CMOPs in three popular test suites, RITEA obtains 77.8% of the best IGD values and 66.7% of the best HV values. The experimental results show that RITEA exhibits highly competitively when dealing with CMOPs.},
  archive      = {J_ASOC},
  author       = {Kaiwen Zhao and Xiangrong Tong and Peng Wang and Yingjie Wang and Yue Chen},
  doi          = {10.1016/j.asoc.2024.111674},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111674},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pareto fronts relationship identification-based two-stage constrained evolutionary algorithm},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). PipeTransUNet: CNN and transformer fusion network for
semantic segmentation and severity quantification of multiple sewer pipe
defects. <em>ASOC</em>, <em>159</em>, 111673. (<a
href="https://doi.org/10.1016/j.asoc.2024.111673">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous development of urbanization, the service life of sewer pipes is gradually approaching a critical threshold. Defects within pipe networks can significantly affect the municipality operations and residents&#39; quality of life. Towards efficient and automatic sewer pipeline inspection, an integrated framework was proposed for semantic segmentation and severity quantification of multiple sewer pipe defects using the PipeTransUNet model that fuses convolutional neural networks with Transformer. The capability of the fusion network to extract and localize sewer defects was further enhanced by incorporating the convolutional block attention module and improving the activation function. In extensive experiments, PipeTransUNet shows enough competitiveness after hyperparameter selection, architecture tweaks, and comparison with other state-of-the-art models. Specifically, PipeTransUNet outperformed other models in terms of both quantitative and visual evaluations, with mean intersection over union, the mean of pixel accuracy, mean pixel accuracy, mean recall, mean F1-score, mean specificity, mean Kappa, and frequency-weighted intersection over union values reaching 71.92%, 84.90%, 80.74%, 85.93%, 83.05%, 84.44%, 55.55%, and 91.32%, respectively. A severity level assessment method for different sewer defects was developed based on PipeTransUNet and compared with expert reviews&#39; results to demonstrate its feasibility and effectiveness. Moreover, the in-depth image features extracted from the segmentation head of our proposed model were visually evaluated and interpreted using pixel-level gradient-weighted class activation mapping. In summary, PipeTransUNet canrecognize complex defects well and provide a solid foundation for inspecting and maintaining sewer pipe networks.},
  archive      = {J_ASOC},
  author       = {Mingze Li and Mingchao Li and Qiubing Ren and Heng Li and Lei Xiao and Xin Fang},
  doi          = {10.1016/j.asoc.2024.111673},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111673},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PipeTransUNet: CNN and transformer fusion network for semantic segmentation and severity quantification of multiple sewer pipe defects},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end learning of adaptive coded modulation schemes for
resilient wireless communications. <em>ASOC</em>, <em>159</em>, 111672.
(<a href="https://doi.org/10.1016/j.asoc.2024.111672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive modulation and coding schemes play a crucial role in ensuring robust data transfer in wireless communications, especially when faced with changes or interference in the transmission channel. These schemes involve the use of variable coding rates, which can be achieved normally through code puncturing or shortening, and have been adopted in 4G and 5G communication standards. In recent works, auto-encoders for wireless communications have demonstrated the ability to learn short code representations that achieve gains over conventional codes. Such a methodology is attractive as it can learn optimal representations under a variety of channel conditions. However, due to its structure the auto-encoder does not currently support multiple code rates with a single model. This article draws upon the discipline of multi-task learning, as it applies to deep learning and therefore devises a branching architecture for the auto-encoder and custom training algorithm in training transmitter and receiver for adaptive modulation and coding. In this article we aim to demonstrate improvements in Block Error Rate over conventional methods in the Additive White Gaussian Noise channel, and to analyse the performance of the model under Rayleigh fading channels without retraining the auto-encoder on the new channel. This article demonstrates a novel approach towards training auto-encoder models to jointly learn adaptive modulation and coding schemes framed as a multi-task learning problem. The research outcomes extend end-to-end learning approaches to the design of adaptive wireless communications systems.},
  archive      = {J_ASOC},
  author       = {Christopher P. Davey and Ismail Shakeel and Ravinesh C. Deo and Ekta Sharma and Sancho Salcedo-Sanz and Jeffrey Soar},
  doi          = {10.1016/j.asoc.2024.111672},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111672},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end learning of adaptive coded modulation schemes for resilient wireless communications},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate time series anomaly detection via separation,
decomposition, and dual transformer-based autoencoder. <em>ASOC</em>,
<em>159</em>, 111671. (<a
href="https://doi.org/10.1016/j.asoc.2024.111671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series usually have entangled temporal patterns and various anomaly types. Meanwhile, they often contain both continuous and discrete features. Many existing methods directly model correlations in complex multivariate time series to conduct anomaly detection. Decomposing time series into different components, such as the overall trend and fluctuations, can contribute to better extracting semantic information and detecting anomalies. Existing decomposition-based anomaly detection methods still have several limitations. First, they directly decompose all features without considering that discrete features are unsuitable for decomposition because they do not have trends or fluctuations. Second, they adopt the same networks for different components with different characteristics, limiting their ability to extract semantic information. Moreover, due to the nature of Transformers, existing reconstruction-based methods using Transformers rarely form information bottlenecks, reducing the differentiation between the reconstruction errors of normal data and anomalies. This paper proposes a multivariate time series anomaly detection method with separation, decomposition, and dual Transformer-based autoencoder (SDDformer). Different from existing methods, SDDformer separates continuous and discrete features and only decomposes continuous features into trend and residual components. Considering the different characteristics of different components, SDDformer adopts Crossformer and the vanilla Transformer as the backbone of two different autoencoders to reconstruct the trend and residual components. Information bottlenecks are better formed using an extra token as the latent variable between the encoder and the decoder. SDDformer regards reconstructing a discrete feature as a classification task and calculates Cross-Entropy as its reconstruction error. Three different metrics are adopted in this paper to compare SDDformer with a variety of typical anomaly detection methods on public data sets, and the experimental results prove that SDDformer can achieve state-of-the-art performance.},
  archive      = {J_ASOC},
  author       = {Shiyuan Fu and Xin Gao and Baofeng Li and Feng Zhai and Jiansheng Lu and Bing Xue and Jiahao Yu and Chun Xiao},
  doi          = {10.1016/j.asoc.2024.111671},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111671},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multivariate time series anomaly detection via separation, decomposition, and dual transformer-based autoencoder},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Two-stage short-term wind power probabilistic prediction
using natural gradient boosting combined with neural network.
<em>ASOC</em>, <em>159</em>, 111669. (<a
href="https://doi.org/10.1016/j.asoc.2024.111669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate wind power probabilistic prediction reflects the uncertainty information of wind power generation, which is the foundation for optimal scheduling of power systems. This study proposes a two-stage probabilistic prediction model combining natural gradient boosting and neural network for accurate uncertainty estimation of short-term output in a wind farm. In the first stage, the selected input features containing historical and future information are fed into a neural network for representation learning. In the second stage, the extracted abstract features are concatenated with the original features, and a natural gradient boosting model is employed to acquire short-term probabilistic forecasts. The experimental results using data from two real wind farms indicate that the proposed hybrid model can generate accurate, sharp, and reliable forecasts. After performing the successive day-ahead prediction task for a month in the first wind farm, the average root mean square error and mean absolute error of the proposed model in the point prediction were 0.1330 and 0.1070, respectively, which were 6.21%–57.29% and 2.96%–62.03% lower than those of comparative models. In addition, the model’s forecasting probability density curves demonstrate high reliability; its coverage probability and the mean width percentage of the interval prediction results under the 90% confidence level were 0.9094 and 0.3696, respectively, which were more suitable than those of five other probabilistic prediction models.},
  archive      = {J_ASOC},
  author       = {Siyi Zhang and Mingbo Liu and Min Xie and Shunjiang Lin},
  doi          = {10.1016/j.asoc.2024.111669},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111669},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage short-term wind power probabilistic prediction using natural gradient boosting combined with neural network},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristic optimization based- ensemble learners for the
carbonation assessment of recycled aggregate concrete. <em>ASOC</em>,
<em>159</em>, 111661. (<a
href="https://doi.org/10.1016/j.asoc.2024.111661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the enhanced prevalence of carbonation, a process accelerating steel reinforcement corrosion in recycled aggregate concrete (RAC) compared to natural aggregate concrete. Traditional carbonation depth assessment methods in RAC are noted for being labor-intensive, costly, and requiring specialized expertise. There is a noted deficiency in the application of machine learning techniques for accurately predicting carbonation depth in RAC, a gap this study aims to fill. Utilizing the extreme gradient boosting (XGBoost) technique, recognized for its efficacy in ensemble machine learning, this study innovates in modeling carbonation depth in RAC. It emphasizes the criticality of hyperparameter optimization of the XGBoost algorithm for maximizing model accuracy. To achieve this, three novel metaheuristic optimization algorithms, including reptile search algorithm (RSA), Aquila optimizer (AO), and arithmetic optimization algorithm (AOA), were introduced as global optimizers for tunning the XGBoost hyperparameters. The study was underpinned by a comprehensive database compiled from extensive literature, facilitating the development of an accurate RAC carbonation depth model. Through rigorous evaluations, including sensitivity analyses, the Wilcoxon signed-rank test, and runtime comparisons, the synthesized models demonstrated exceptional accuracy, with coefficients of determination exceeding 0.95. The XGBoost-AO algorithm, in particular, showcased superior performance, with the XGBoost-RSA algorithm providing efficient predictions considering runtime. SHapley Additive exPlanations (SHAP) interpretation highlighted environmental conditions as significant carbonation depth influencers. A user-friendly graphical user interface was developed, enhancing the practical utility of the findings for predicting carbonation depth progression in RAC over time. This research significantly advances the predictive accuracy for carbonation depth in RAC, contributing to the sustainable management of concrete infrastructures and emphasizing the integration of advanced machine learning techniques with metaheuristic optimization for environmental and structural engineering advancements.},
  archive      = {J_ASOC},
  author       = {Emadaldin Mohammadi Golafshani and Ali Behnood and Taehwan Kim and Tuan Ngo and Alireza Kashani},
  doi          = {10.1016/j.asoc.2024.111661},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111661},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic optimization based- ensemble learners for the carbonation assessment of recycled aggregate concrete},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A clustering-based adaptive undersampling ensemble method
for highly unbalanced data classification. <em>ASOC</em>, <em>159</em>,
111659. (<a href="https://doi.org/10.1016/j.asoc.2024.111659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance issue is prevalent in various practical classification tasks. A high unbalanced rate will significantly decrease the classification performance of unbalanced learning. However, existing methods for highly unbalanced data classification still face two key difficulties: (1) fairly learning key information, and (2) maintaining consistency. To address these difficulties, we propose a novel majority clustering-based adaptive undersampling enhanced ensemble classification method, which integrates undersampling and ensemble techniques. In the adaptive undersampling process, we first consider the spatial distribution of majority samples to ensure distribution consistency. We then consider an adaptive sampling rate and introduce a feedback mechanism to obtain more representative majority samples from each cluster. In the classifier ensemble process, multiple ensemble iterations are introduced to achieve fair attention to key information in different classes. Finally, six kinds of experiments are conducted on 17 real highly unbalanced datasets from multiple fields. Experimental results demonstrate that the proposed method outperforms existing methods in terms of effectiveness, robustness, and adaptability.},
  archive      = {J_ASOC},
  author       = {Xiaohan Yuan and Chuan Sun and Shuyu Chen},
  doi          = {10.1016/j.asoc.2024.111659},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111659},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A clustering-based adaptive undersampling ensemble method for highly unbalanced data classification},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label reusing based graph neural network for unbalanced
classification of personalized driver genes in cancer. <em>ASOC</em>,
<em>159</em>, 111658. (<a
href="https://doi.org/10.1016/j.asoc.2024.111658">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a big challenge to identify personalized driver genes (PDGs) for understanding tumor heterogeneity of cancer individual patients. From the perspective of machine learning, identifying PDGs is an inherent class imbalance issue due to the fewer known driver genes than most passenger ones. However, existing machine learning based methods including unsupervised and supervised learning based methods ignore the importance of limited well-established cancer tissue specific driver genes(CSDGs) for this class imbalance issue. Here we converted the PDG prediction issue to a semi-supervised classification task and a novel method (namely PersonalizedGNN) was developed to identify PDGs by using graph attention neural network and label reuse strategy in personalized gene interaction network (PGIN). PersonalizedGNN effectively utilizes the structure information of PGIN and the limited well-established CSDG information for achieving promising performance. Using the breast cancer and lung cancer datasets from The Cancer Genome Atlas, we validated our method and compared it with other advanced methods. PersonalizedGNN showed outstanding potential in identifying cancer driver genes in terms of prediction precision. Furthermore, we could discover subtype-specific de novo cancer driver genes and in vitro cell-based assays for a novel driver gene FZD7 in lung squamous cell carcinoma cells further validated the PersonalizedGNN. In summary, PersonalizedGNN offers a new effective perspective for discovering PDGs by considering information of prior known CSDGs in PGIN which help researchers understand tumor heterogeneity of cancer individual patients.},
  archive      = {J_ASOC},
  author       = {Han-Wen Wan and Meng-Han Wu and Wen-Shan Zhao and Han Cheng and Ying Bi and Xian-Fang Wang and Xiang-Rui Zhang and Yan Li and Wei-Feng Guo},
  doi          = {10.1016/j.asoc.2024.111658},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111658},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Label reusing based graph neural network for unbalanced classification of personalized driver genes in cancer},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way decision method based on triangular norms in
incomplete information systems and its applications in medical
diagnosis. <em>ASOC</em>, <em>159</em>, 111657. (<a
href="https://doi.org/10.1016/j.asoc.2024.111657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision, as an extension of traditional two-way decision, was proposed by Yao in 2009, which can effectively avoid unnecessary losses caused by incorrect decisions in the decision-making process. Meanwhile, incomplete hybrid information systems represent the database of the relationship between objects and attributes, which refers to a system with multiple data and missing data. In this paper, based on the fact that many existing literatures involving incomplete hybrid information systems did not fully consider the impact of different conditional attributes on decision attributes and lacked effective aggregation methods to integrate weights and distances, we propose a new three-way decision method to deal with incomplete hybrid information systems with the help of triangular norms. First, in incomplete hybrid information systems, we redefine the distance between two objects based on conditional attributes and give the calculation formula of different data attributes in conditional attributes. At the same time, we define a new weight calculation method based on the relationship between conditional attributes and decision attributes. And then, by the distance between different conditional attributes and their corresponding weights, the hybrid distance is obtained using triangular norms. Furthermore, we use the hybrid distance to get the tolerance relation on the target set of an incomplete hybrid information system, thereby, using this tolerance relation, we get a new decision theoretic rough set model and the corresponding decision rules. Finally, by two experiments involving medical diagnosis, we demonstrate that our model has better classification ability, lower misclassification rate, and better stability compared to other corresponding models, thereby confirming that the proposed model provides a new and effective method for handling incomplete hybrid information systems in practical applications.},
  archive      = {J_ASOC},
  author       = {Yanlong Tang and Junsheng Qiao},
  doi          = {10.1016/j.asoc.2024.111657},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111657},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way decision method based on triangular norms in incomplete information systems and its applications in medical diagnosis},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distance metric learning-based multi-granularity
neighborhood rough sets for attribute reduction. <em>ASOC</em>,
<em>159</em>, 111656. (<a
href="https://doi.org/10.1016/j.asoc.2024.111656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is a hot research topic in data mining, in which rough set theory-based attribute reduction methods have been widely focused. The neighborhood rough set (NRS) model has good generalization performance and practicality in uncertainty reasoning, so it is often used for attribute reduction in recent years. Calculating the distance between samples in different attribute spaces is a key step in the NRS-based attribute reduction methods, which directly affects the performance of the reduction algorithm. However, the NRS model uses a fixed computational paradigm in calculating the distance between samples and does not consider the influence of labels in the attribute spaces on the distance calculation, which is not conducive to improving the performance of the reduction algorithm. Distance metric learning takes full account of the labels information in the multi-dimensional attribute space, and it can learn the distance between samples by taking into account the integrated principle that samples with the same label are closer and samples with different labels are further away, which helps to reduce the classification uncertainty. Inspired by this, this paper firstly incorporates distance metric learning into the NRS model from the perspective of multi-dimensional attribute space and proposes a distance metric learning-based multi-granularity neighborhood rough set (DmlMNRS) model. The related properties of the DmlMNRS model are also introduced and proved. Then, the DmlMNRS-based attribute reduction criterion and the significance of the attributes are defined. A DmlMNRS-based heuristic attribute reduction (DMNHAR) algorithm is designed based on this. Finally, experiments are performed on fifteen publicly datasets, and the experimental results show that the proposed algorithm has better robustness and classification performance.},
  archive      = {J_ASOC},
  author       = {Shaoguo Cui and Gengsen Li and Binbin Sang and Weihua Xu and Hongmei Chen},
  doi          = {10.1016/j.asoc.2024.111656},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111656},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Distance metric learning-based multi-granularity neighborhood rough sets for attribute reduction},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oxygen starvation control of proton exchange membrane fuel
cell through fusion control strategy. <em>ASOC</em>, <em>159</em>,
111655. (<a href="https://doi.org/10.1016/j.asoc.2024.111655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oxygen excess ratio (OER) is regarded as a significant indicator of the safety and efficient operation of the open-cathode proton exchange membrane fuel cell (PEMFC) system. This is because inappropriate OER is prone to the phenomenon of oxygen starvation and oxygen saturation under load conditions change frequently. To avoid the phenomenon, this paper integrates linear active disturbance rejection control (LADRC) and proportional integral differential (PID) control to regulate OER of fuel cell air supply system, which effectively combines model-based and data-driven LADRC structure. As the controller parameters directly affect the control performance and it is difficult to obtain the optimal control parameters. Especially, when the load conditions change, the control parameters are difficult to adapt the load changes. To this end, a snake optimization (SO) algorithm with easy implementation and fast convergence is used to realtime update the controller parameters, which can further improve the control effects. Firstly, an equivalent linear model of fuel cell air supply system is established. Secondly, a hybrid LADRC method is developed by combining the merits of the LADRC and PID methods. Thirdly, the SO algorithm is used to optimize the controller parameters, which can achieve the optimal control parameters. Finally, the results revealed that the proposed method has a faster response with less noise sensitivity against the load changes and stronger robustness than the other methods.},
  archive      = {J_ASOC},
  author       = {Zhihua Deng and Ming Chen and Qihong Chen and Haijiang Wang},
  doi          = {10.1016/j.asoc.2024.111655},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111655},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Oxygen starvation control of proton exchange membrane fuel cell through fusion control strategy},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic design of interpretable control laws through
parametrized genetic programming with adjoint state method gradient
evaluation. <em>ASOC</em>, <em>159</em>, 111654. (<a
href="https://doi.org/10.1016/j.asoc.2024.111654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the application of a Local Search (LS) enhanced Genetic Programming (GP) algorithm to the control scheme’s design task. The combination of LS and GP aims to produce an interpretable control law as similar as possible to the optimal control scheme reference. Inclusive Genetic Programming (IGP), a GP heuristic capable of promoting and maintaining the population diversity, is chosen as the GP algorithm since it proved successful on the considered task. IGP is enhanced with the Operators Gradient Descent (OPGD) approach, which consists of embedding learnable parameters into the GP individuals. These parameters are optimized during and after the evolutionary process. Moreover, the OPGD approach is combined with the adjoint state method to evaluate the gradient of the objective function. The original OPGD was formulated by relying on the backpropagation technique for the gradient’s evaluation, which is impractical in an optimization problem involving a dynamical system because of scalability and numerical errors. On the other hand, the adjoint method allows for overcoming this issue. Two experiments are formulated to test the proposed approach, named Operator Gradient Descent - Inclusive Genetic Programming (OPGD-IGP): the design of a Proportional–Derivative (PD) control law for a harmonic oscillator and the design of a Linear Quadratic Regulator (LQR) control law for an inverted pendulum on a cart. OPGD-IGP proved successful in both experiments, being capable of autonomously designing an interpretable control law similar to the optimal ones, both in terms of shape and control gains.},
  archive      = {J_ASOC},
  author       = {Francesco Marchetti and Gloria Pietropolli and Federico Julian Camerota Verdù and Mauro Castelli and Edmondo Minisci},
  doi          = {10.1016/j.asoc.2024.111654},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111654},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automatic design of interpretable control laws through parametrized genetic programming with adjoint state method gradient evaluation},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A graph attention network with contrastive learning for
temporal review-based recommendations. <em>ASOC</em>, <em>159</em>,
111652. (<a href="https://doi.org/10.1016/j.asoc.2024.111652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, review-based recommendations have recently attracted extensive attention from researchers. However, applying reviews to improve recommendation performance still faces some problems that remain unsolved: (1) Most existing works cannot capture the time-varying user preferences and high-order collaborative features. (2) Most of the existing works often use supervised learning to train models with a limited interaction between users and items, which is not sufficient to learn the more accurate recommendation models. To overcome these problems, we propose a new G raph A ttention N etwork with C ontrastive L earning for temporary review-based recommendation, named GANCL. Specifically, to capture dynamic user preferences and high-order collaborative features, we design a user–item bipartite graph with time-series review information and ratings as its edges, and then use the graph attention and different gating mechanisms to extract the corresponding features. To make full use of the limited interaction between the users and items, we use the contrastive learning paradigm for the nodes and edges in the bipartite graph to more effectively model the user–item interaction. A large number of experiments on five public data from Amazon prove that the performance of the GANCL is improved by 2.76% and 2.83% respectively compared with the state-of-the-art algorithms in MSE and MAE.},
  archive      = {J_ASOC},
  author       = {Peilin Yang and Yingyuan Xiao and Wenguang Zheng and Yanli Liu and Ching-Hsien Hsu},
  doi          = {10.1016/j.asoc.2024.111652},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111652},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A graph attention network with contrastive learning for temporal review-based recommendations},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-strategy enhanced marine predators algorithm with
applications in engineering optimization and feature selection problems.
<em>ASOC</em>, <em>159</em>, 111650. (<a
href="https://doi.org/10.1016/j.asoc.2024.111650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel meta-heuristic optimization algorithm named Clustering Wavelet Opposition-based Marine Predators Algorithm (CWOMPA) to address some limitations present in the well-established Marine Predators Algorithm (MPA). CWOMPA incorporates three key strategies: a fuzzy clustering approach for escaping local optima, using wavelet basis function-based impact coefficient adjustment for elites to prevent premature convergence, and finally an adaptive opposition-based learning strategy for maintaining population diversity. Compared with some recent meta-heuristic algorithms, extensive evaluations conducted affirm that CWOMPA achieves the best Friedman rank, 4.30 and 1.95 respectively, on 23 benchmark functions and the CEC 2017 benchmark set. Not only does CWOMPA demonstrate significant effectiveness on six constrained problems from the CEC 2020 real-world benchmarks, but also when applied to 14 medical datasets, it gains superior Friedman ranks in terms of selected features, classification accuracy, F-Score, and objective function value, 2.5, 2.25, 2.96, 2.93 respectively, outperforming other common methods. Finally, CWOMPA exhibits the highest classification accuracy across all datasets and the best F-Score performance in nine datasets compared to traditional feature selection algorithms. The obtained results reveal that CWOMPA is a powerful and versatile optimization algorithm with significant potential in various real-world applications, including feature selection for medical datasets.},
  archive      = {J_ASOC},
  author       = {Kamran Rezaei and Omid Solaymani Fard},
  doi          = {10.1016/j.asoc.2024.111650},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111650},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-strategy enhanced marine predators algorithm with applications in engineering optimization and feature selection problems},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Quantile regression network-based cross-domain prediction
model for rolling bearing remaining useful life. <em>ASOC</em>,
<em>159</em>, 111649. (<a
href="https://doi.org/10.1016/j.asoc.2024.111649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning improves remaining useful life (RUL) prediction accuracy across domains by aligning data distributions for different operating conditions. However, the uncertainty caused by the complex working conditions and stochastic degradation process of rolling bearings is not considered, leading to poor credibility of the prediction results and affecting the development of the predictive maintenance strategy. In response to this problem, the paper proposes a deep subdomain adaptation time-quantile regression network (DSATQRN) model to compress rolling bearing uncertainty intervals of RUL across prediction. The model uses deep subdomain adaptation to align the feature distribution and introduces temporal correlation to construct a temporal quantile regression network to obtain interval prediction results. Finally, the uncertainty in the prediction results is quantified by kernel density estimation. The model was experimented with using the open XJTU-SY Bearing Datasets and IEEE PHM 2012 Challenge Datasets. It verifies the performance of the proposed model from three aspects: point prediction accuracy, interval prediction suitability, and probabilistic prediction overall performance. The experimental results show that the average interval coverage of the proposed model on the two datasets is 91.25% and 90.43%, and the average prediction interval width is 16.65% and 13.69%, respectively. It is demonstrated that the cross-domain prediction results of the DSATQRN possess high prediction accuracy and narrow prediction interval, and the model has good robustness and reliability.},
  archive      = {J_ASOC},
  author       = {Ting Zhang and Honglei Wang},
  doi          = {10.1016/j.asoc.2024.111649},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111649},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quantile regression network-based cross-domain prediction model for rolling bearing remaining useful life},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development of real-time brain-computer interface control
system for robot. <em>ASOC</em>, <em>159</em>, 111648. (<a
href="https://doi.org/10.1016/j.asoc.2024.111648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based brain-computer interfaces (BCI) have been considered a prevailing non-invasive method for collecting human biomedical signals by attaching electrodes to the scalp. However, it is difficult to detect and use these signals to control an online BCI robot in a real environment owing to environmental noise. In this study, a novel state recognition model is proposed to determine and improve EEG signal states. First, a Long Short-Term Memory Convolutional Neural Network (LSTM-CNN) was designed to extract EEG features along the time sequence. During this process, errors caused by the randomness of the mind or external environmental factors may be generated. Thus, an actor-critic based decision-making model was proposed to correct these errors. The model consists of two networks that can be used to predict the final signal state based on both the current signal state probability and past signal state probabilities. Subsequently, a hybrid BCI real-time control system application is proposed to control a BCI robot. The Unicorn Hybrid Black EEG device was used to acquire brain signals. A data transmission system was constructed using OpenViBE to transfer data. An EEG classification system was built to classify the BCI commands. In this experiment, EEG data from five subjects were collected to train and test the performance and reliability of the proposed control system. The system records the time spent by the robot and the moving distance. Experimental results were provided to demonstrate the feasibility of the real-time control system. Compared to similar BCI studies, the proposed hybrid BCI real-time control system can accurately classify seven BCI commands in a more reliable and precise manner. Overall, the offline testing accuracy was 87.20%. When we apply the proposed system to control a BCI robot in a real environment, the average online control accuracy is 93.12%, and the mean information transmission rate is 67.07 bits/min, which is better than those of some state-of-the-art control systems. This shows that the proposed hybrid BCI real-time control system demonstrated higher reliability, which can be used in practical BCI control applications.},
  archive      = {J_ASOC},
  author       = {Yang An and Johnny Wong and Sai Ho Ling},
  doi          = {10.1016/j.asoc.2024.111648},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111648},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Development of real-time brain-computer interface control system for robot},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating normalization in preference-based evolutionary
multi-objective optimization using a reference point. <em>ASOC</em>,
<em>159</em>, 111646. (<a
href="https://doi.org/10.1016/j.asoc.2024.111646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalization of objectives plays a crucial role in evolutionary multi-objective optimization (EMO) to handle objective functions with different scales, which can be found in real-world problems. Although the effect of normalization methods on the performance of EMO algorithms has been investigated in the literature, that of preference-based EMO (PBEMO) algorithms is poorly understood. Since PBEMO aims to approximate a region of interest, its population generally does not cover the Pareto front in the objective space. This property may make normalization of objectives in PBEMO difficult. This paper investigates the effectiveness of three normalization methods in three representative PBEMO algorithms. We present a bounded archive-based method for approximating the nadir point. First, we demonstrate that the normalization methods in PBEMO perform significantly worse than that in conventional EMO in terms of approximating the ideal point, nadir point, and range of the PF. Then, we show that PBEMO requires normalization of objectives on problems with differently scaled objectives. Our results show that there is no clear “best normalization method” in PBEMO, but an external archive-based method performs relatively well.},
  archive      = {J_ASOC},
  author       = {Ryoji Tanabe},
  doi          = {10.1016/j.asoc.2024.111646},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111646},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Investigating normalization in preference-based evolutionary multi-objective optimization using a reference point},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A probabilistic reliable linguistic model for
blockchain-based student information management system assessment.
<em>ASOC</em>, <em>159</em>, 111645. (<a
href="https://doi.org/10.1016/j.asoc.2024.111645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing necessity for robust security and trust in systems has prompted the widespread adoption of blockchain technology in student information management systems (SIMS). While existing literature has concentrated on the development of blockchain-based student information management systems (BC-SIMS), a notable gap remains in the evaluation of these systems. This study addresses this gap by employing a Multi-Criteria Decision-Making (MCDM) approach. Recognizing the inherent uncertainties in BC-SIMS evaluation due to inadequate information, the study introduces the Probabilistic Reliable Linguistic Term Sets (PRLTSs) theory to address these problems. As a result, the Measurement of Alternatives and Ranking according to the Compromise Solution (MARCOS) method is extended into the Probabilistic Reliable Linguistic environment, resulting in the Probabilistic Reliable Linguistic-Measurement of Alternatives and Ranking according to the Compromise Solution (PRL-MARCOS) method. This method supports the evaluation and selection decisions on BC-SIMS in higher education institutions. The study further designs the Probabilistic Reliable Linguistic-Coefficient of Variation (PRL-CV) method to determine the relative importance of selection criteria. Twenty selection criteria for BC-SIMS are identified through a literature review, and an illustrative case study is employed to assess the practicality and validity of the proposed PRL-CV-MARCOS approach in a higher educational institution. Experimental results highlight cost reduction (0.0778), interoperability (0.0717), security (0.0628), service improvement (0.0557) and responsiveness (0.0538) as the top five criteria for BC-SIMS selection, with BC-SIMS P 5 P5 identified as the optimal solution for managing student information. Finally, a comprehensive comparative and sensitivity analysis verifies the stability and effectiveness of the proposed decision support model.},
  archive      = {J_ASOC},
  author       = {Kwame Omono Asamoah and Adjei Peter Darko and Collins Opoku Antwi and Collins Sey and Abigail Hyiaman Osei and Xiaodong Ma and Jia Zhu},
  doi          = {10.1016/j.asoc.2024.111645},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111645},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A probabilistic reliable linguistic model for blockchain-based student information management system assessment},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A reservoir computing model based on nonlinear spiking
neural p systems for time series forecasting. <em>ASOC</em>,
<em>159</em>, 111644. (<a
href="https://doi.org/10.1016/j.asoc.2024.111644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear spiking neural P (NSNP) systems are a novel class of neural-like computing models that exhibit nonlinear dynamic behaviours. Further, reservoir computing (RC) is a new type of recurrent neural network (RNN) whose dynamic behaviour is provided by a reservoir of neurones. An interesting question is whether NSNP systems can be embedded into an RC framework to develop an efficient prediction model for time series forecasting problems. To address this problem, this paper first defines a variant of NSNP systems with two outputs, called NSNP-TO systems, in which a nonlinear spiking mechanism with two outputs is the key feature. Based on NSNP-TO systems, a new variant of RC models is proposed based on NSNP systems, called the RC-NSNP model. Owing to the use of spiking neurones, the RC-NSNP model comprises state-updating and output equations that are different from those of the existing RC models. Using a similar structure, the proposed RC-NSNP model can be implemented in the RC framework, and a similar training method can be utilised to train it. Time series forecasting tasks are performed to verify the effectiveness of the proposed RC-NSNP model. The proposed RC-NSNP model achieved the best performance on nine of the twelve time series problems and obtained the second-best performance on the other two time series.},
  archive      = {J_ASOC},
  author       = {Lifan Long and Chenggang Guo and Xin Xiong and Hong Peng and Jun Wang},
  doi          = {10.1016/j.asoc.2024.111644},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111644},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A reservoir computing model based on nonlinear spiking neural p systems for time series forecasting},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of cloud-to-ground lightning and intra-cloud
lightning based on their radiated electric field signatures using
different types of neural networks and machine learning classifiers.
<em>ASOC</em>, <em>159</em>, 111643. (<a
href="https://doi.org/10.1016/j.asoc.2024.111643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scope of this paper is to test the efficiency of three different neural networks in the area of cloud-to-ground/intra-cloud lightning classification using a not very large data set of electric field data. Effective neural network learning verified for a small lightning event dataset but with whole 2-seconds particular record length is an important feature of fast and efficient procedures implemented into lightning location systems among which discrimination between cloud-to-ground and intra-cloud lightning is one of the most important task. Measurement data obtained at the Lightning Observatory in Rzeszow were used for the analysis. About 100 extremely low/medium frequency bandwidth electric field of cloud-to ground lightning registrations and the same number of intra-cloud lightning events were taken into account. During the study 81% of those registrations were dedicated for learning of neural networks while the remaining 19% were put into the testing set. All data were preselected manually with consideration of the electric field variation and information from the lightning location system database. Lightning events were selected uniformly with the well-pronounced presence of their basic components and reported distance to the registration station which ensured a wide variation of expected lightning electric field signatures. The multilayer perceptron neural network, the radial basis function neural network and the convolutional neural network were tested and compared in different configurations while distinguishing lightning. The results were compared with other conventional classification approaches, such as traditional machine learning methods as well as one a little more contemporary architecture, the long short-term memory neural network. The multilayer perceptron neural network achieved the best detection accuracy overall. Presented research is an extension of lightning identification studies available so far mainly based on the convolutional neural network by the results achieved using the multilayer perceptron neural network, the radial basis function neural network, the long short-term memory neural network and several machine learning-based classification methods.},
  archive      = {J_ASOC},
  author       = {G. Karnas and G. Dralus and G. Maslowski},
  doi          = {10.1016/j.asoc.2024.111643},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111643},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Identification of cloud-to-ground lightning and intra-cloud lightning based on their radiated electric field signatures using different types of neural networks and machine learning classifiers},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological numbers of fuzzy soft graphs and their
applications in globalizing the world by mutual trade. <em>ASOC</em>,
<em>159</em>, 111642. (<a
href="https://doi.org/10.1016/j.asoc.2024.111642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research draft, by keeping in view, the importance of Zagreb numbers, first we defined some familiar graph families and their degrees in fuzzy soft environment, and then by converting these two novel topological numbers in fuzzy soft framework, their formulae are calculated for these graphical networks. An application of international trade using two kinds of routes is also discussed in the article, and by comparing the results efficiency of both the numbers is compared and analyzed how topological descriptors are useful in examining the progress and profit in the countries, that are a part of the project. Graphs are used in the modeling of links and processes within physical, biological, social, and information systems. A fuzzy graph is the most adaptable tool in mathematics, which describes the fuzzy relationship between objects. Fuzzy graphs are very helpful to give more accuracy to the system as compared to crisp graphs. A classification of the universe with respect to certain specified parameters is known as a soft set. It is a new method for simulating ambiguity and uncertainty. Soft set theory was created by Molodtsov in 1999 as a method for simulating vagueness and uncertainty. Soft set theory is presently used widely in academia to address decision-making issues. The concept of soft graphs is used to provide a parameterized point of view for graphs. In many areas of mathematics and chemistry, as well as in network theory, spectral graph theory, and molecular chemistry, the Zagreb numbers are crucial graph parameters. Due to the importance and significance of Zagreb numbers, in this research article we define these graph parameters in fuzzy soft environment and then derived the fundamental formula for first and second Zagreb numbers of some popular graph families. Also by keeping in view the importance of graph theory in solving decision making problems in business industry, and the most generic behavior of fuzzy soft graph due to its parameterized nature we use our work in application of decision making in trade among different nations.},
  archive      = {J_ASOC},
  author       = {Shabana Anwar and Muhammad Azeem and Muhammad Kamran Jamil},
  doi          = {10.1016/j.asoc.2024.111642},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111642},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Topological numbers of fuzzy soft graphs and their applications in globalizing the world by mutual trade},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). 3D medical image segmentation based on semi-supervised
learning using deep co-training. <em>ASOC</em>, <em>159</em>, 111641.
(<a href="https://doi.org/10.1016/j.asoc.2024.111641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, artificial intelligence has been applied to 3D COVID-19 medical image diagnosis, which reduces detection costs and missed diagnosis rates with higher predictive accuracy, and diagnostic efficiency. However, the limited size and low quality of clinical 3D medical image samples have hindered the segmentation performance of 3D models. Therefore, we propose a 3D medical image segmentation model based on semi-supervised learning using co-training. Multi-view and multi-modal images are generated using spatial flipping and windowing techniques to enhance the spatial diversity of 3D image samples. A pseudo label generation module based on confidence-weights is employed to generate reliable pseudo labels for non-annotated data, thereby increasing the sample size and reducing overfitting. The proposed approach utilizes a three-stage training process: firstly, training a single network based on annotated data; secondly, incorporating non-annotated data to train a dual-modal network and generate pseudo labels; finally, jointly training six models in three dimensions using both annotated and pseudo labels generated from multi-view and multi-modal images, aiming to enhance segmentation accuracy and generalization performance. Additionally, a consistency regularization loss is applied to reduce noises and accelerate convergence of the training. Moreover, a heatmap visualization method is employed to focus on the attention of features at each stage of training, providing effective reference for clinical diagnosis. Experiments were conducted on an open dataset of 3D COVID-19 CT samples and a non-annotated dataset from TCIA, including 771 NIFTI-format CT images from 661 COVID-19 patients. The results of 5-fold cross-validation show that the proposed model achieves a segmentation accuracy of Dice=73.30 %, ASD=10.633, Sensitivity=63.00 %, and Specificity=99.60 %. Compared to various typical semi-supervised learning 3D segmentation models, it demonstrates better segmentation accuracy and generalization performance.},
  archive      = {J_ASOC},
  author       = {Jingdong Yang and Haoqiu Li and Han Wang and Man Han},
  doi          = {10.1016/j.asoc.2024.111641},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111641},
  shortjournal = {Appl. Soft. Comput.},
  title        = {3D medical image segmentation based on semi-supervised learning using deep co-training},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Railway network delay evolution: A heterogeneous graph
neural network approach. <em>ASOC</em>, <em>159</em>, 111640. (<a
href="https://doi.org/10.1016/j.asoc.2024.111640">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate delay evolution prediction plays a pivotal role in train rescheduling decision-making for the railway network. Existing studies on delay prediction predominantly centered around predicting delays for each train in the subsequent stations (i.e., following a train-oriented perspective). Furthermore, train operations in the railway network involve different types of entities (stations, trains, etc.), making the current graph/network models with homogenous nodes (i.e., the same kind of nodes) incapable of effectively capturing the interactions between the entities. This paper develops a network-oriented model to investigate the train delay evolution on railway networks, by predicting the delays of running trains in the network after a given time interval. The proposed model combines the GraphSAGE graph neural network (GNN) and the heterogeneous graph neural network (HetGNN) architecture, thus called SAGE-Het, enabling it to capture interactions between heterogeneous nodes (i.e., different types of nodes) based on different edges (e.g., edges between trains, trains, and stations). Additionally, SAGE-Het allows for flexible inputs in contrast to conventional machine learning techniques, whose inputs must meet the consistent dimension requirement (e.g., in the form of rectangular or grid-like arrays). The performance and robustness of the suggested SAGE-Het model are assessed in experiments on the data from two sub-networks of the China railway network. The experimental results demonstrate that SAGE-Het outperforms the existing delay prediction methods and some advanced HetGNNs used for prediction tasks in other domains; SAGE-Het demonstrates excellent scalability, capable of handling various types of nodes; the predictive performances of SAGE-Het under different prediction time horizons (10/20/30 min ahead) all exhibit better performance over other baselines; the accuracies are over 90 % under the permissible 3-minute errors for the three prediction time horizons. Specifically, the impact of train interactions on delay evolution is investigated based on the flexible input characteristic of the proposed model. The results illustrate that train interactions become subtle with the increase of train headways. This finding directly contributes to decision-making in situations where conflict resolution or train-canceling actions are needed.},
  archive      = {J_ASOC},
  author       = {Zhongcan Li and Ping Huang and Chao Wen and Wei Dong and Yindong Ji and Filipe Rodrigues},
  doi          = {10.1016/j.asoc.2024.111640},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111640},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Railway network delay evolution: A heterogeneous graph neural network approach},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interpretable short-term carbon dioxide emissions
forecasting based on flexible two-stage decomposition and temporal
fusion transformers. <em>ASOC</em>, <em>159</em>, 111639. (<a
href="https://doi.org/10.1016/j.asoc.2024.111639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon emissions play a pivotal role in exacerbating the global warming crisis and driving climate change. Accurate and consistent projections of carbon emissions are of utmost importance for nations worldwide, as they shape emission reduction strategies and expedite the pursuit of carbon peaking and carbon neutrality goals. While previous studies have focused on hybrid methodologies for carbon emission forecasting, yielding commendable predictive performance, these approaches often overlook the significance of internal interpretability within the forecasting models. In light of this gap, this study introduces a groundbreaking and elucidating hybrid carbon emission forecasting model that amalgamates the two-stage layer decomposition method, adaptive differential evolution with optional external archive (JADE), and temporal fusion transformers (TFT). To begin with, a series of sub-sequences is derived by employing a flexible two-stage decomposition strategy, which leverages a linear-nonlinear decomposition criterion to thoroughly extract the fluctuating characteristics inherent in the carbon emission series. Subsequently, the JADE algorithm intelligently and efficiently optimizes the parameter combinations within the TFT model, ensuring both stability and reliability of the prediction framework. Empirical investigations conclusively demonstrate the remarkable applicability and efficacy of the proposed model in short-term carbon emission forecasting. By delving into the interpretability of the model&#39;s results, the study enhances the capacity of policymakers to devise well-informed strategies based on comprehensive insights gleaned from the forecasting process.},
  archive      = {J_ASOC},
  author       = {Binrong Wu and Huanze Zeng and Zhongrui Wang and Lin Wang},
  doi          = {10.1016/j.asoc.2024.111639},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111639},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interpretable short-term carbon dioxide emissions forecasting based on flexible two-stage decomposition and temporal fusion transformers},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A distributed evolutionary based instance selection
algorithm for big data using apache spark. <em>ASOC</em>, <em>159</em>,
111638. (<a href="https://doi.org/10.1016/j.asoc.2024.111638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instance selection is an important preprocessing technology in data mining and machine learning. In this paper, we proposed a novel evolutionary based instance selection algorithm for big data. First, we defined a coarse granularity chromosome structure to reduce the size of search space and costs of chromosome operations(recombination and mutation, etc.). Then a stratified evolution strategy was proposed to remove the hyper parameter in classic fitness function and achieve precise control over the reduction ratio of instances. Finally, a sampling-based fitness function was proposed to reduce the time complexity. Experimental results shown that our new algorithm is efficient to complete the instance selection task on data set with millions of instances in minutes-level. The 10-fold cross-validation also proved that the selection results on many datasets have high nearest neighbor classification accuracy.},
  archive      = {J_ASOC},
  author       = {Liyang Qin and Xiaoli Wang and Linzi Yin and Zhaohui Jiang},
  doi          = {10.1016/j.asoc.2024.111638},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111638},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A distributed evolutionary based instance selection algorithm for big data using apache spark},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Delay-sensitive task offloading and efficient resource
allocation in intelligent edge–cloud environments: A discretized
differential evolution-based approach. <em>ASOC</em>, <em>159</em>,
111637. (<a href="https://doi.org/10.1016/j.asoc.2024.111637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of smart wireless devices (WDs) has enormously increased over the last few years due to the advancement of 5G/B5G networks. The advanced applications of such smart WDs, e.g., augmented reality, virtual reality, online gaming, etc., demand excessive resources. Although the WDs are equipped with limited resources, the evolution of edge computing and offloading techniques enables the WDs to offload their resource-intensive tasks to the nearby edge node. These edge nodes might experience higher loads and delays when WDs generate a huge number of tasks. Moreover, the wireless channel bandwidth and transmission data rate of the wireless channels are also limited. Therefore, optimizing the use of available bandwidth as a valuable resource and reducing latency emerge as crucial objectives while offloading tasks. In this paper, a delay-aware resource-constrained offloading problem for edge–cloud systems is mathematically formulated as a 0–1 integer linear programming, and it is shown to be NP-complete. Then, a delay-aware resource-constrained offloading algorithm based on a discretized differential evolution (DARC-DE) is designed. The objectives of the DARC-DE are to maximize the utilization of the resources as bandwidth and minimize the delay. The vectors are efficiently encoded along with the decoding technique. The fitness function is designed by considering execution, offloading, queuing, transmission delay, and bandwidth utilization. The DARC-DE is shown to be executed in polynomial time. To evaluate DARC-DE, extensive simulation is performed in two different scenarios with varying numbers of tasks and edges. Simulation results demonstrate that the proposed DARC-DE can minimize total delay by 15% to 40% in comparison to particle swarm optimization, genetic algorithm, and bees algorithm, respectively. Simulation results also indicate a significant improvement in bandwidth utilization. Taguchi method and alternative average convergence rate are conducted. The statistical tests—analysis of variance, post-hoc, and Friedman tests—are also performed.},
  archive      = {J_ASOC},
  author       = {Biswadip Bandyopadhyay and Pratyay Kuila and Mahesh Chandra Govil and Marlom Bey},
  doi          = {10.1016/j.asoc.2024.111637},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111637},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Delay-sensitive task offloading and efficient resource allocation in intelligent edge–cloud environments: A discretized differential evolution-based approach},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correlation-driven multi-level learning for anomaly
detection on multiple energy sources. <em>ASOC</em>, <em>159</em>,
111636. (<a href="https://doi.org/10.1016/j.asoc.2024.111636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced metering infrastructure (AMI) has been widely used as an intelligent energy consumption measurement system. Electric power was the representative energy source collected by AMI; most existing studies to detect abnormal energy consumption have focused on a single energy source, i.e., power. Recently, other energy sources such as water, gas, and heating have also been actively collected. As a result, it is necessary to develop a unified methodology for anomaly detection across multiple energy sources; however, research efforts have rarely been made to tackle this issue. In this study, we propose a new correlation-driven multi-level learning model for anomaly detection on multiple energy sources. The distinguishing property of the model incorporates multiple energy sources in multi-levels based on the strength of the correlations between them. Our model is scalable to integrate arbitrary new energy sources, with further performance improvement, considering both correlated and non-correlated sources. Through extensive experiments on real-world datasets consisting of three to five energy sources, we demonstrate that the proposed model clearly outperforms the existing multimodal learning and recent time-series anomaly detection models and makes further performance improvement as more energy sources are integrated, showing the scalability of the proposed model.},
  archive      = {J_ASOC},
  author       = {Taehee Kim and Jae-Seok Jang and Hyuk-Yoon Kwon},
  doi          = {10.1016/j.asoc.2024.111636},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111636},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correlation-driven multi-level learning for anomaly detection on multiple energy sources},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG emotion recognition based on the TimesNet fusion model.
<em>ASOC</em>, <em>159</em>, 111635. (<a
href="https://doi.org/10.1016/j.asoc.2024.111635">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, emotion recognition based on electroencephalogram (EEG) has become an important research field. This paper proposes an innovative multi-scale emotion recognition method (MS-ERM), which is based on a deep learning model. First, we divide the EEG signal into time windows of 0.5 s in different frequency bands to extract the differential entropy feature and embed the feature into the brain electrode map to express spatial information. Then, the features of each segment are used as input to the new deep learning model (MS-TimesNet). The model combines multi-scale convolution and TimesNet network to effectively extract dynamic time features, cross-channel spatial features, and complex time features in 2D space. Through extensive tests on the DEAP dataset, we prove that this method is superior to existing methods in terms of sentiment classification performance. In the arousal and valence classification, the average classification accuracy of subject-dependent tests reached 91.31% and 90.45%, respectively, while in subject-independent tests, the average classification accuracy was 86.66% and 85.40%, respectively. Code is available at this repository: https://github.com/hyao0827/MS-ERM.},
  archive      = {J_ASOC},
  author       = {Luyao Han and Xiangliang Zhang and Jibin Yin},
  doi          = {10.1016/j.asoc.2024.111635},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111635},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG emotion recognition based on the TimesNet fusion model},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correlation analysis of sampled wafer profile maps based on
a deep reconstruction model. <em>ASOC</em>, <em>159</em>, 111634. (<a
href="https://doi.org/10.1016/j.asoc.2024.111634">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the semiconductor manufacturing process, various kinds of metrology and test data can form different types of wafer maps. By analyzing the correlation of multiple types of wafer maps, especially for the wafer bin maps (WBMs) and the wafer profile maps, the faulty inline manufacturing steps strongly correlated with end-of-line yield can be found for yield improvement. This paper proposes a correlation analysis method based on a deep reconstruction model to integrate the knowledge among multiple types of wafer maps for root cause analysis. First, the sparsely sampled wafer profile maps are restored to original wafer profile maps through the deep reconstruction model for obtaining more information. And then, the correlation between the WBMs and the reconstructed profile maps is calculated. The process step whose wafer profile maps have the highest correlation with WBMs is highly related to the fault. Experiments on the real-world dataset demonstrate that the proposed method can restore the wafer profile map well and has high accuracy in matching the relevant process wafer profile map based on the WBMs.},
  archive      = {J_ASOC},
  author       = {Yuting Kong and Dong Ni},
  doi          = {10.1016/j.asoc.2024.111634},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111634},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Correlation analysis of sampled wafer profile maps based on a deep reconstruction model},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-learning based infrared ship object detection model for
generalization to unknown domains. <em>ASOC</em>, <em>159</em>, 111633.
(<a href="https://doi.org/10.1016/j.asoc.2024.111633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared images exhibit considerable variations in probability distributions, stemming from the utilization of distinct infrared sensors and the influence of diverse environmental conditions. The variations pose great challenges for deep learning models to detect ship objects and adapt to unseen maritime environments. To address the domain shift problem, we propose an end-to-end infrared ship object detection model based on meta-learning neural network to improve domain adaptation for target domain where data is not available at training phase. Different from existing domain generalization methods, the novelty of our model lies in the effective exploitation of meta-learning and domain adaptation, ensuring that the extracted domain-independent features are meaningful and domain-invariant at the semantic level. Firstly, a double gradient-based meta-learning algorithm is designed to solve the common optimal descent direction between different domains through two gradient updates in the inner and outer loops. The algorithm enables extraction of domain-invariant features from the pseudo-source and pseudo-target domain data. Secondly, a domain discriminator with dynamic-weighted gradient reversal layer (DWGRL) is designed to accurately classify domain-invariant features and provide additional global supervision information. Finally, a multi-scale feature aggregation method is proposed to improve the extraction of multi-scale domain-invariant features. It can effectively fuse local features at different scales and global features of targets. Extensive experimental results conducted in real nighttime water surface scenes demonstrate that the proposed model achieves very high detection accuracy on target domain data, even no target domain data was used during the training phase. Compared to the existing methods, our method not only improves the detection accuracy of infrared ships by 18%, but also exhibits the smallest standard deviation with a value of 0.93, indicating its superior generalization performance.},
  archive      = {J_ASOC},
  author       = {Hui Feng and Wei Tang and Haixiang Xu and Chengxin Jiang and Shuzhi Sam Ge and Jianhua He},
  doi          = {10.1016/j.asoc.2024.111633},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111633},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-learning based infrared ship object detection model for generalization to unknown domains},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based convolution neural network for magnetic tile
surface defect classification and detection. <em>ASOC</em>,
<em>159</em>, 111631. (<a
href="https://doi.org/10.1016/j.asoc.2024.111631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effectively identifying surface defects in magnetic tiles has proven to be highly challenging due to limited sample availability and irrelevant background interference, which also plays a crucial role in significantly influencing the lifespan and reliability of permanent magnet motors. To address these challenges, our study draws inspiration from a comprehensive analysis of the retinal attention mechanism and proposes three guiding criteria: multi-level resolution, what to look for, and where to look at. These criteria are utilized as foundational principles to enhance the representation learning capability of designed neural network structures through the incorporation of the retinal attention mechanism. Subsequently, based on these guiding criteria, we introduce a novel convolutional retinal attention block (CRAB) to learn discriminative and robust feature representations for magnetic tile surface defect classification and detection. The proposed CRAB comprises three modules: multi-resolution module (MRM), global attention aggregation module (GAAM), and local attention aggregation module (LAAM), designed to extract discriminative and robust features by refining meaningful information and suppressing redundant ones. Comprehensive experimental results across image classification and object detection tasks demonstrate that the proposed CRAB outperforms existing methods such as SE, ECA, and CBAM, and can effectively amplify the representation power across various backbone networks, including VGG-16, GoogLeNet, ResNet-18, and ResNet-50. An evaluation on surface defect classification and detection tasks for industrial magnetic tiles further shows that CRAB achieves accuracies of 99.50% and 96.98%, respectively. These results emphasize the promising application prospects of the proposed method in detecting industrial surface defects amid expansive and inconsequential backgrounds. The code of the proposed method is available at: https://github.com/KWflyer/CRAB .},
  archive      = {J_ASOC},
  author       = {Ju Li and Kai Wang and Mengfan He and Luyao Ke and Heng Wang},
  doi          = {10.1016/j.asoc.2024.111631},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111631},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based convolution neural network for magnetic tile surface defect classification and detection},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective genetic algorithm embedded with
reinforcement learning for petrochemical melt-flow-index production
scheduling. <em>ASOC</em>, <em>159</em>, 111630. (<a
href="https://doi.org/10.1016/j.asoc.2024.111630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the petrochemical industry, product type conversion can cause capacity loss and produce transition products, leading to waste. Our study aims to minimize these transition products and tardiness in meeting customer due dates. We introduce the Engineering Experience Heuristic (EEH), which utilizes scheduling reference sheets to identify key production traits and constraints, thereby generating efficient schedules. Further building on the EEH, we propose a non-dominated sorting genetic algorithm II enhanced with reinforcement learning (NSGAeRL) to address the multi-objective scheduling optimization problem. The incorporation of reinforcement learning (RL), particularly through a model-based Markov Decision Process (MDP), allows for the dynamic adjustment of genetic algorithm (GA) parameters, including crossover and mutation mechanisms, based on the optimal policy derived from RL. We validate our proposed methodologies through empirical studies and numerical experiments. The results demonstrate that NSGAeRL outperforms standard benchmarking algorithms, offering superior performance in minimizing transition products and scheduling tardiness.},
  archive      = {J_ASOC},
  author       = {Chia-Yen Lee and Chieh-Ying Ho and Yu-Hsin Hung and Yu-Wen Deng},
  doi          = {10.1016/j.asoc.2024.111630},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111630},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective genetic algorithm embedded with reinforcement learning for petrochemical melt-flow-index production scheduling},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep factor asset pricing with policy guidance based on
multi-source heterogeneous information. <em>ASOC</em>, <em>159</em>,
111629. (<a href="https://doi.org/10.1016/j.asoc.2024.111629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel latent factor pricing model to extract latent pricing factors and corresponding factor loadings from multi-source heterogeneous information through a deep learning architecture. Notably, we pioneer the extraction of policy pricing factors from China’s national strategies (“Five-Year Plans”, “Government Work Reports”, and “Monetary Policy Reports”) using natural language processing and a dynamic topic model. The proposed mixed-frequency deep factor asset pricing (MIDAS-DF) model learns from mixed-frequency heterogeneous data and captures nonlinear joint patterns between inputs and outputs, providing more nuanced insights into asset pricing. The empirical analyses of the Chinese A-share market from January 1, 2003 to July 31, 2022 show that the MIDAS-DF model outperforms competing models in pricing individual stocks, various test portfolios, and investment portfolios. The results also demonstrate that low-frequency policy information anchors long-term pricing trends, while high-frequency market and sentiment information refine short-term pricing accuracy. They work together to enhance the pricing performance.},
  archive      = {J_ASOC},
  author       = {Zezhou Wang and Qifa Xu and Cuixia Jiang},
  doi          = {10.1016/j.asoc.2024.111629},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111629},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep factor asset pricing with policy guidance based on multi-source heterogeneous information},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BWM integrated VIKOR method using neutrosophic fuzzy sets
for cybersecurity risk assessment of connected and autonomous vehicles.
<em>ASOC</em>, <em>159</em>, 111628. (<a
href="https://doi.org/10.1016/j.asoc.2024.111628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connected and autonomous vehicles (CAVs) have gained significant importance in intelligent transportation systems. However, CAVs are associated with significant cyber-risks caused by different threat-agents, which require robust mitigation strategies for secure operation. Different categories of threat-agents come with varying resources, motivation, and skills that must be addressed effectively for the secure operation of CAVs. This study offers a comprehensive analysis of cybersecurity risks from different categories of threat-agents targeting CAVs. A novel hybrid multi-criteria decision making (MCDM) technique has been developed to prioritize the threat-agent categories as part of the cyber defense mechanism. In this study, the major threat-agent categories are identified from the existing literature and cybersecurity reports. Then expert opinion is collected on the likelihood and severity of cyber-attacks undertaken by different threat-agent categories. Single-valued neutrosophic fuzzy sets (SVNFS) are implemented to consolidate the subjectivity of the linguistic opinions given by the experts. Then Best-Worst Method (BWM) is used to determine the relative criteria weightage on the cyber-attack consequences. These weights are then supplied to the ViseKriterijum-ska Optimizacija I Kompromisno Resenj (VIKOR) method for ranking the categories of threat-agents based on the risk perceived by the experts. A case study is presented with five categories of threat-agents and five loss criteria targeting CAVs to validate the viability of the proposed model. The result shows that insider attackers pose maximum risk for cybersecurity of CAVs. The proposed technique will provide helpful insights to the decision-makers for formulating effective defense strategies against cyber-attackers.},
  archive      = {J_ASOC},
  author       = {Bhosale Akshay Tanaji and Sayak Roychowdhury},
  doi          = {10.1016/j.asoc.2024.111628},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111628},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BWM integrated VIKOR method using neutrosophic fuzzy sets for cybersecurity risk assessment of connected and autonomous vehicles},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic multi-model transfer based short-term load
forecasting. <em>ASOC</em>, <em>159</em>, 111627. (<a
href="https://doi.org/10.1016/j.asoc.2024.111627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of renewable energy sources in power systems has resulted in increased complexity in dispatch management, necessitating higher accuracy in short-term load forecasting. Although numerous deep learning models have been developed, their training data requirements and difficulties in effectively combining different model structures remain challenges. This study proposes a multi-model transfer-based approach for dynamic short-term load forecasting to address these issues. The method consists of two phases: (1) conducting correlation analysis to determine transferability between datasets and implementing an adaptive parameter transfer policy to optimize data utilization and training efficiency, and (2) constructing a multi-model cooperation platform to fully leverage the strengths of different models and enhance forecasting accuracy. Experimental case studies demonstrate the effectiveness of the proposed method in improving forecasting accuracy when historical load data is limited. This research contributes to advancing load forecasting techniques by leveraging transfer learning and model cooperation, facilitating efficient power system operations in the context of renewable energy integration.},
  archive      = {J_ASOC},
  author       = {Ling Xiao and Qinyi Bai and Binglin Wang},
  doi          = {10.1016/j.asoc.2024.111627},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111627},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic multi-model transfer based short-term load forecasting},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graphormer based contrastive learning for recommendation.
<em>ASOC</em>, <em>159</em>, 111626. (<a
href="https://doi.org/10.1016/j.asoc.2024.111626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Graph Neural Networks (GNNs) have become a powerful tool for graph representation in recommendation systems. Graphormer integrates its graph structure into a Transformer to provide state-of-the-art performance for graph prediction tasks. Meanwhile, self-supervised contrastive learning has succeeded in processing highly sparse data. Despite these successes, most graph contrastive methods perform random enhancement, such as node/edge perturbation, on a user-item interaction graph, or perform heuristic enhancement techniques, such as user clustering, to generate a new contrastive view. These methods cannot preserve the inherent semantic structure well and are susceptible to noise. In this paper, we propose a Graphormer-based graph contrastive learning method, GO-GCL, which reduces the influence of noise while retaining the inherent semantic structure to some extent, and improves the versatility and robustness of the recommendation based on contrastive learning. The model is represented by Graphormer and then uses singular value decomposition for contrastive enhancement. The experiments on Yelp datasets show that our model has a significant improvement over existing GCL-based approaches. In-depth analyses show the superiority and robustness of GO-GCL in data sparsity and prevalence bias.},
  archive      = {J_ASOC},
  author       = {Jing Wang and Jiangtao Ren},
  doi          = {10.1016/j.asoc.2024.111626},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111626},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graphormer based contrastive learning for recommendation},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An α-cut intervals based fuzzy best–worst method for
multi-criteria decision-making. <em>ASOC</em>, <em>159</em>, 111625. (<a
href="https://doi.org/10.1016/j.asoc.2024.111625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address ambiguity arising from linguistic decision judgments, Guo and Zhao introduced a model of the well-known Multi-Criteria Decision-Making (MCDM) method, Best-Worst Method (BWM), using fuzzy sets, known as Fuzzy Best–Worst Method (FBWM). Despite its versatility, FBWM has limitations, such as criteria-weights being independent of fuzzy comparison values’ shape and reliance on approximate fuzzy arithmetic operations. In response to these limitations, we introduce a novel model of Fuzzy Best–Worst Method based on α α -cut intervals. In this model, the calculation of optimal weights involves formulating an optimization problem that utilizes exact fuzzy arithmetic operations defined in terms of α α -cut intervals. This approach aims to optimize the entire shape of fuzzy comparison values simultaneously. However, it turns out that, although we have proven the existence of optimal solution for this problem, solving it proves to be challenging. To resolve this issue, we approximate optimal weight set(s) using triangular fuzzy weight sets and assess the Error of Approximation (EA). The approximate weight set with the desired EA is then defuzzified to obtain a crisp weight set. In situations where there are multiple approximate weight sets with the same EA, interval-weights are calculated. Following that, we establish the necessary conditions for a Fuzzy Pairwise Comparison System (FPCS) to be consistent. Using these conditions, we calculate a lower bound of the Consistency Index (CI), providing a measure of the accuracy of weights. Finally, we discuss numerical examples and a real-world scenario, ranking of risk factors in supply chain 4.0, to validate the effectiveness of the proposed model.},
  archive      = {J_ASOC},
  author       = {Harshit M. Ratandhara and Mohit Kumar},
  doi          = {10.1016/j.asoc.2024.111625},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111625},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An α-cut intervals based fuzzy Best–Worst method for multi-criteria decision-making},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explainable deep inherent learning for multi-classes skin
lesion classification. <em>ASOC</em>, <em>159</em>, 111624. (<a
href="https://doi.org/10.1016/j.asoc.2024.111624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is often a lack of explanation when artificial intelligence (AI) is used to diagnose skin lesions, which makes the physician unable to interpret and validate the output; thus, diagnostic systems become significantly less safe. In this paper, we proposed a deep inherent learning method to classify seven types of skin lesions. The proposed deep inherent learning was validated using different explanation techniques. Explainable AI (X-AI) was used to explain decision-making processes at the local and global levels. In addition, we provide visual information to help physicians trust the proposed method. The challenging dataset, HAM10000, was used to evaluate the proposed method. Medical practitioners can better understand the mechanisms of black-box AI models using our simple, stage-based X-AI framework. They can trust the proposed method because the rationale for its decisions is explained.},
  archive      = {J_ASOC},
  author       = {Khalid M. Hosny and Wael Said and Mahmoud Elmezain and Mohamed A. Kassem},
  doi          = {10.1016/j.asoc.2024.111624},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111624},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explainable deep inherent learning for multi-classes skin lesion classification},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Antenna modeling based on meta-heuristic intelligent
algorithms and neural networks. <em>ASOC</em>, <em>159</em>, 111623. (<a
href="https://doi.org/10.1016/j.asoc.2024.111623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As wireless communication technology continues to advance, the antenna, as an essential front-end device in radio communication system, is surrounded by more and more complex electromagnetic wave environments with increasing variety, resulting in greater demand for antennas and higher design requirements. While the traditional antenna design methods suffered from the disadvantage of low design efficiency, a powerful tool for accelerating antenna design is the modelling of antennas with neural networks. Aiming to enhance the modeling accuracy of neural network, multiple novel meta-heuristic swarm intelligent algorithms are introduced and part of them are modified for the purpose of applying to optimizing network’s weights and biases so as to raise the antenna model`s prediction precision on the basis of neural network. Specifically, the intelligent algorithms and their improvement directions include the strategy of optimizing weights and biases for neural networks with seagull optimization algorithm, optimizing the weights and biases of neural network with the improved butterfly algorithm fused with reverse learning, and the artificial rabbit algorithm optimizing the neural network weights and biases. In addition, two intelligent optimization algorithms that are already more mature: the particle swarm algorithm and the genetic algorithm are added to compare with the above three algorithms. The accuracy of neural network prediction before and after the optimisation of neural network by seagull algorithm, the butterfly algorithm incorporating reverse learning, the artificial rabbit algorithm, the particle swarm algorithm, and the genetic algorithm are got through the results respectively. The results of the experiments displayed that the neural network optimized of the improved butterfly algorithm incorporating reverse learning has a prediction accuracy of 99.69 % with stable results, the optimised neural network prediction accuracy of the seagull algorithm reaches 99.51%, and the optimised neural network prediction accuracy of the artificial rabbit algorithm is 99.49 %. The remaining two traditional algorithms optimized neural network accuracy is 83.1 % and 99.43 % respectively. Therefore, the improved butterfly algorithm incorporating reverse learning is the most effective among these three new algorithms applied to the field of antenna prediction. Moreover, the running time of the network optimized by different algorithms is quite distinct, among which the neural network optimized by the improved butterfly algorithm incorporating reverse learning takes the shortest time, which increases the prediction efficiency of the network by more than 70%. In summary, the application of the fused reverse learning improved butterfly algorithm in optimizing neural network predictions yields the shortest processing time and highest accuracy. This not only enables faster and more precise antenna design but also holds greater significance for the field of antenna design and analysis.},
  archive      = {J_ASOC},
  author       = {Ju Huang and Jingchang Nan and Mingming Gao and Yifei Wang},
  doi          = {10.1016/j.asoc.2024.111623},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111623},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Antenna modeling based on meta-heuristic intelligent algorithms and neural networks},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale learnable key-channel attention network for
point cloud classification and segmentation. <em>ASOC</em>,
<em>159</em>, 111622. (<a
href="https://doi.org/10.1016/j.asoc.2024.111622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 3D point cloud, a common format of 3D data, finds extensive applications in fields like remote sensing, surveying, robotics, and more. Addressing the challenges posed by insufficient weighted feature information and the offset between weighted results and task expectations in the attention mechanism, this paper proposes a Multi-scale Learnable Key-channel Attention Network (MLKNet). First, we introduce a feature feedback-repair module to mitigate the impact of information loss in the feature embedding process. This module aims to fully embed the original data into a high-dimensional feature space, ensuring a rich supply of feature information for subsequent transformer modules. Second, an efficient hierarchical local feature encoder extract and aggregate local features from point clouds at various scales, thereby significantly enhancing the model&#39;s capability to represent geometric structures. Third, a novel learnable key-channel attention module allows tasks to influence the feature selection and weighting process directly, make the highlighted features as close to task expectations as possible, effectively enhancing the network&#39;s perception of global semantic information. Our method was benchmarked on various tasks where we achieved overall accuracy (OA) of 92.3% on the ModelNet40 classification task and achieved instance mean intersection over union (ins. mIoU) of 87.6% on the ShapeNet-part segmentation task. The results indicate the superior performance of our method.},
  archive      = {J_ASOC},
  author       = {Jie Zhao and Yian Liu and Bin Wu},
  doi          = {10.1016/j.asoc.2024.111622},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111622},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale learnable key-channel attention network for point cloud classification and segmentation},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated skip convolutional network with residual
learning and feature extraction for point and interval prediction of
solar radiation. <em>ASOC</em>, <em>159</em>, 111621. (<a
href="https://doi.org/10.1016/j.asoc.2024.111621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spinning reserve based on solar radiation prediction can ensure the secure operation of large-scale grid-connected photovoltaic systems, but irrational spinning reserve can lead to substantial economic losses and even grid collapse. Therefore, it is imperative to identify characteristic patterns of solar radiation and establish an effective solar radiation prediction model. However, existing hybrid models often struggle to efficiently recognize and leverage input features, compromising the robustness of the model. To address this limitation, this study proposes an integrated skip-convolutional network with residual learning and feature extraction (InSCNet), which enhances the capability to represent information data and thereby improves the stability and accuracy of solar radiation prediction by employing a series of feature extraction and prediction blocks with residual learning. InSCNet includes a sophisticated feature extraction module to effectively address the underutilization of feature information, and it incorporates residual learning, skip-convolution, and recursive networks in the prediction module, reducing the risk of gradient vanishing or gradient explosion while enhancing prediction accuracy. Additionally, an interval estimation method with adaptively chosen distributions is introduced, which extends the prediction interval estimation method. The proposed InSCNet is rigorously evaluated using datasets from three major cities in Pakistan. Experimental results demonstrate that InSCNet outperforms existing solutions in both point and interval predictions.},
  archive      = {J_ASOC},
  author       = {Xiaojing Sun and Wei Liu and Kang Wang and Jingquan Chen},
  doi          = {10.1016/j.asoc.2024.111621},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111621},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated skip convolutional network with residual learning and feature extraction for point and interval prediction of solar radiation},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fault transfer diagnosis of rolling bearings across
different devices via multi-domain information fusion and multi-kernel
maximum mean discrepancy. <em>ASOC</em>, <em>159</em>, 111620. (<a
href="https://doi.org/10.1016/j.asoc.2024.111620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current deep learning-based intelligent diagnosis algorithms depend on large amounts of well-labeled data, but they may not perform well in engineering practice where the fault data available from a single device is limited. The significant difference in data distribution between different devices makes it more difficult to transfer diagnosis across devices. Therefore, aiming at the problem that the great difference in the data distribution of source and target domains in cross-device transfer diagnosis leads to the low diagnosis accuracy, this paper investigates a deep transfer network model based on multi-domain information fusion and multi-kernel maximum mean discrepancy (MK-MMD) for fault diagnosis of rolling bearings across different devices. Firstly, to address the problem that single-domain information is difficult to adequately characterize rolling bearing health status information of different devices, a multi-domain information feature extractor based on multi-attention mechanism is constructed to capture the important features of vibration signals in different transform domains. Secondly, the extracted multi-domain features are input into the bidirectional gated recurrent unit (BiGRU) network for feature fusion, and MK-MMD is used to narrow the distribution distance between the source domain and target domain to obtain domain invariant features, and then achieve the transfer diagnosis of rolling bearing faults between different devices. Finally, the method investigated is verified by rolling bearing fault transfer diagnosis tests between different devices, and the results show that the suggested method can adapt to the feature distribution between different domains, and improve the transfer diagnosis accuracy between different devices.},
  archive      = {J_ASOC},
  author       = {Jimeng Li and Zhangdi Ye and Jie Gao and Zong Meng and Kai Tong and Shancheng Yu},
  doi          = {10.1016/j.asoc.2024.111620},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111620},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fault transfer diagnosis of rolling bearings across different devices via multi-domain information fusion and multi-kernel maximum mean discrepancy},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semi-supervised mixture model of visual language multitask
for vehicle recognition. <em>ASOC</em>, <em>159</em>, 111619. (<a
href="https://doi.org/10.1016/j.asoc.2024.111619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective vehicle detection is extremely vital in cities. However, existing vehicle recognition models exhibit limited functionality and have not been effectively integrated with language models to aid in vehicle monitoring within cities. To address these challenges, this paper proposes a semi-supervised mixture model with detection, segmentation, and question-answering capabilities, referred to as the Multi-task Visual Language Model (MTVLM), for vehicle recognition. Further, a vehicle dataset containing 8425 vehicle images from 13 different categories is introduced, which is collected across various urban scenarios and weather conditions. MTVLM consists of a detector, a language model, and a segmentation model. The detector YOLODS is based on an improved You Only Look Once algorithm (YOLOv4), which has a lower number of parameters and higher detection accuracy. The detection results of YOLODS are then used as prompts for the pre-trained language model, a distilled version of Bidirectional Encoder Representations from Transformers (DistilBERT), to answer questions related to the detected vehicles. Furthermore, the coordinates of bounding boxes obtained by YOLODS are input into the Segment Anything Model (SAM) as prompts to segment vehicles from images. Experimental results show that MTVLM achieves a mean average precision (mAP) of 97.12 % on the vehicle dataset, surpassing other state-of-the-art (SOTA) detectors. The high-precision detector also ensures highly accurate text prompts for input to DistilBERT and SAM. By employing text prompts, the potential and performance of the pre-trained models DistilBERT and SAM can be fully explored and utilized to better complete question-answering and segmentation tasks.},
  archive      = {J_ASOC},
  author       = {Wenjin Liu and Shudong Zhang and Lijuan Zhou and Ning Luo and Min Xu},
  doi          = {10.1016/j.asoc.2024.111619},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111619},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-supervised mixture model of visual language multitask for vehicle recognition},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary simultaneous under and oversampling of
instances for dealing with class-imbalance datasets in multilabel
problems. <em>ASOC</em>, <em>159</em>, 111618. (<a
href="https://doi.org/10.1016/j.asoc.2024.111618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel classification has recently attracted great attention from the data mining research community. Multilabel classification is concerned with learning where each instance can be associated with multiple classes (or labels). Class-imbalance problems appear in any classification task when the class distribution of the instances is very different. In multilabel classification, this problem is ubiquitous, as a large percentage of labels suffer from a class-imbalanced distribution. The adaptation of single-label methods to deal with the class-imbalance problem in multilabel learning is problematic as many of their basic concepts are not easily transferred. In this paper, we propose the use of evolutionary computation to simultaneously oversample the minority class and undersample the majority class for multilabel problems. Letting the algorithm autonomously select the instances to undersample and oversample allows us to extend these two successful paradigms to the multilabel task. An extensive comparison setup of 35 datasets shows the advantages of using this approach to deal with class-imbalance datasets for multilabel problems compared with previously published methods as well as the basic classification algorithms with the original datasets.},
  archive      = {J_ASOC},
  author       = {Nicolás García-Pedrajas and José M. Cuevas-Muñoz and Aida de Haro-García},
  doi          = {10.1016/j.asoc.2024.111618},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111618},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary simultaneous under and oversampling of instances for dealing with class-imbalance datasets in multilabel problems},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective discrete artificial bee colony for the
rectangular cutting problem with guillotine in transformer
manufacturing. <em>ASOC</em>, <em>159</em>, 111617. (<a
href="https://doi.org/10.1016/j.asoc.2024.111617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses a two-dimensional rectangular cutting problem with guillotine constraint, applied to the transformer industry. The problem requires a set of rectangular items to be cut from larger and variable-sized rectangles, known as bins. The goal is to minimize the number of bins used to cut all required items. An effective discrete artificial bee colony is proposed to solve the problem. The approach uses a decimal encoding method where each integer represents a type of bin or item, and decodes the list of integers by a constructive heuristic to generate cutting patterns with guillotine-cut constraint. In addition, a copy strategy is designed to improve the solution quality. The approach’s performances and the effectiveness of the strategy are evaluated by three datasets generated from real-world data. The experimental results show that the discrete artificial bee colony is superior to other competitive algorithms, and that the copy strategy can produce higher quality solutions.},
  archive      = {J_ASOC},
  author       = {Qiang Luo and Yunqing Rao and Bing Du},
  doi          = {10.1016/j.asoc.2024.111617},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111617},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective discrete artificial bee colony for the rectangular cutting problem with guillotine in transformer manufacturing},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-swarm surrogate model assisted PSO algorithm to
minimize distribution network energy losses. <em>ASOC</em>,
<em>159</em>, 111616. (<a
href="https://doi.org/10.1016/j.asoc.2024.111616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational analysis of real-world engineering problems often relies on time-consuming simulation calculations. This presents challenges in balancing computational burden and precision when applying traditional metaheuristic algorithms to large-scale complex problems. While there have been numerous surrogate-assisted metaheuristic optimization algorithms proposed, most of them are designed for solving expensive problems within 30–50 dimensions. In this paper, a novel approach called Parallel-multi-swarm Gaussian Process Regression surrogate-based particle swarm optimization (Parallel-MS-GPRS-PSO) is proposed for high-dimensional problems. This approach combines a standard particle swarm optimization (PSO) algorithm, multi-swarm cooperative PSO (MSCPSO), and a GPR surrogate-based PSO (PSO-GPR). By integrating PSO-GPR and MSCPSO, the proposed method aims to effectively explore and exploit the search space while enhancing the global and local performance of the surrogate model. To validate the effectiveness of the proposed algorithm, it is compared with several existing PSO algorithms on seven benchmark functions and an engineering problem. The results illustrate that the Parallel-MS-GPRS-PSO approach outperforms the existing algorithms. Moreover, the method shows promising performance in computationally expensive engineering optimization problems with 288 variables.},
  archive      = {J_ASOC},
  author       = {Goli Vamsi Priya and Sanjib Ganguly},
  doi          = {10.1016/j.asoc.2024.111616},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111616},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-swarm surrogate model assisted PSO algorithm to minimize distribution network energy losses},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-attention-LSTM method for dam deformation prediction
based on CEEMDAN optimization. <em>ASOC</em>, <em>159</em>, 111615. (<a
href="https://doi.org/10.1016/j.asoc.2024.111615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structural deformation of a dam directly affects its lifespan and safety, making its accurate prediction crucial. Traditional prediction methods often overlook the nonlinearity and non-smoothness of deformation data. Moreover, the irregular intervals within the historical deformation data used for model training can reduce prediction accuracy. To address these issues, we propose a hybrid deep learning model that uses signal decomposition and reconstruction to enhance dam deformation prediction. This model employs a long short-term memory (LSTM) neural network optimized using a complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN). The self-attention mechanism in the LSTM model effectively captures the temporal features of dam deformation, alleviating the difficulties generated by the irregular intervals within the historical data used in model training. Furthermore, considering the lag effect of influencing factors on dam deformation and the differences among various measurement points, we propose a CEEMDAN-based feature selection method. Using 13 years worth of data from the Shuibuya Dam, we evaluate the accuracy and effectiveness of the CEEMDAN-Self-attention-LSTM model using indicators, such as MAE, RMSE, MAPE, and R 2 R2 , and compared it with existing models. The experimental results show that this model reduces prediction error by more than 53.62%.},
  archive      = {J_ASOC},
  author       = {Shuo Cai and Huixin Gao and Jie Zhang and Ming Peng},
  doi          = {10.1016/j.asoc.2024.111615},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111615},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A self-attention-LSTM method for dam deformation prediction based on CEEMDAN optimization},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-direction guided mutation-driven stable swarm
intelligence algorithm with translation and rotation invariance for
global optimization. <em>ASOC</em>, <em>159</em>, 111614. (<a
href="https://doi.org/10.1016/j.asoc.2024.111614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there have been notable advancements in the theoretical research of existing meta-heuristic algorithms and in the development of novel algorithms. However, the research on the definition and construction methods of the so-called “mutation” operator, a concept widely used in existing meta-heuristic algorithms, remains notably insufficient. In addition, how to directly construct a meta-heuristic algorithm under the guidance of stability and invariance analysis continues to pose a challenge that has yet to resolve. This paper carefully investigates and rigorously defines the concepts of “mutation operator” and “mutation-driven meta-heuristic algorithm” as found in the literature. According to these definitions, a novel meta-heuristic algorithm, called Multi-direction guided Mutation-driven Stable swarm Intelligence algorithm with Translation and Rotation invariance (M 2 SITRI) is proposed, and the corresponding mutation operator is carefully designed based on the analysis of translation invariance, rotation invariance, order-1 stability, and order-2 stability. The stability and invariance properties of the M 2 SITRI algorithm are validated through numerical simulations, and its superior solving performance is demonstrated by benchmarking it against several other well-known existing meta-heuristic algorithms across a variety of test problems.},
  archive      = {J_ASOC},
  author       = {Haoxin Wang and Libao Shi},
  doi          = {10.1016/j.asoc.2024.111614},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111614},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-direction guided mutation-driven stable swarm intelligence algorithm with translation and rotation invariance for global optimization},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous increase of parameters of an experimental
absorption system: Neural network inverse optimization methodology with
multi-inputs. <em>ASOC</em>, <em>159</em>, 111606. (<a
href="https://doi.org/10.1016/j.asoc.2024.111606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Absorption Heat Transformer (AHT) has become an alternative proposal to weaken the trend of thermal pollution because it contributes to the recovery of residual heat and the use of solar thermal energy for its activation. Recently, compact designs have been developed that share two heat exchangers in one shell to reduce heat losses and save maintenance costs. The evaluation of the performance of AHTs depends on multiple parameters, which have been optimized using models based on ideal conditions. Multivariable optimization with artificial intelligence has considered experimental data to obtain feasible conditions related to the operation of the AHT. However, these models have focused only on determining one parameter at a time, reflecting a limitation. This study aims to develop a new optimization strategy to simultaneously maximize two relevant parameters associated with the efficiency of a compact experimental AHT. The optimization methodology is applied to increment the values of the heat generated in the absorber (Q AB ) and the Exergy Coefficient of Performance (ECOP) using the same objective function. This objective function is resolved when reaching the desired output parameters by determining multiple optimal conditions of the AHT. Initially, the modeling of the experimental data was carried out using an artificial neural network (ANN) for the diagnosis and prediction of the Q AB and the ECOP. The model was validated by comparing the experimental values of Q AB and ECOP against predicted values through a linear regression model, with a satisfactory result of R 2 &gt;0.98. Subsequently, the multivariable inverse artificial neural network methodology for multiple output parameters (ANNim-m) was used and coupled with the Particle Swarm Optimization algorithm (ANNim-m-PSO) to generate the new multivariate optimization strategy and improve Q AB and ECOP parameters simultaneously. Finally, 4 random tests with different initial operating conditions were optimized. Based on the optimization of the 4 tests, the Q AB heat load was increased by 87.7 %, 54.2 %, 30.79 %, and 22.66 % from initial experimental conditions of 2.7, 9.0, 14.23 and 23.29 kW, respectively. In the case of ECOP, elevations of 28.5 %, 23.4 %, 15.4 %, and 16.18 % were obtained for initial values of 0.3801, 0.455, 0.5180, and 0.5729, respectively. It is determined that the parameters Q AB and ECOP have a high sensitivity when the inlet temperature of the absorber of the external circuit (TAB) decreases. The results reveal that it is feasible to use the ANNim-m-PSO model, because the optimization was able to significantly maximize both Q AB and ECOP parameters of the 40 kW experimental AHT.},
  archive      = {J_ASOC},
  author       = {R.A. Conde-Gutiérrez and A. Márquez-Nolasco and U. Cruz-Jacobo and D. Colorado-Garrido and J.A. Hernández},
  doi          = {10.1016/j.asoc.2024.111606},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111606},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Simultaneous increase of parameters of an experimental absorption system: Neural network inverse optimization methodology with multi-inputs},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-attention and asymmetric multi-layer perceptron-gated
recurrent unit blocks for protein secondary structure prediction.
<em>ASOC</em>, <em>159</em>, 111604. (<a
href="https://doi.org/10.1016/j.asoc.2024.111604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein secondary structure prediction (PSSP) is one of the most prominent and widely-conducted tasks in Bioinformatics. Deep neural networks have become the primary methods for building PSSP models in the last decade due to their potential to enhance PSSP performances. However, there is room for improvement in PSSP as previous studies have yet to reach the theoretical limit of PSSP model performance. In this work, we propose a PSSP model called SADGRU-SS, which is built with a novel and unique deep learning architecture that utilizes self-attention, asymmetric multi-layer perceptron (MLP)-gated recurrent unit (GRU) blocks, and a dense block for solving the PSSP problem. Our experiment results show that using self-attention in the SADGRU-SS architecture has successfully increased SADGRU-SS performance. Moreover, installing self-attention in the frontmost position of the networks produces better performance than locating it in other positions. Using the asymmetric configuration in the MLP-GRU blocks results in more excellent performance than the symmetric ones. Our model is trained using the standard CB6133-filtered dataset. We evaluate the performance of our model using the standard CB513 test dataset. Our experiment shows that the performance of our model on 8-state PSSP outstands other PSSP models. The model achieves 70.74% and 82.78% prediction accuracy in the 8-state and 3-state PSSP, respectively.},
  archive      = {J_ASOC},
  author       = {Dewi Pramudi Ismi and Reza Pulungan and Afiahayati},
  doi          = {10.1016/j.asoc.2024.111604},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111604},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-attention and asymmetric multi-layer perceptron-gated recurrent unit blocks for protein secondary structure prediction},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic mutation late acceptance hill climbing aided red fox
optimization for metabolomic biomarkers selection from lung cancer
patient sera. <em>ASOC</em>, <em>159</em>, 111602. (<a
href="https://doi.org/10.1016/j.asoc.2024.111602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate selection of serum metabolomic biomarkers for early lung cancer screening remains a significant challenge in the clinical context. Consequently, this study introduces the Red Fox Optimization (RFO) that integrates Dynamic Mutation Late Acceptance Hill Climbing (DM-LAHC), with the aim of selecting a panel of serum metabolomic biomarkers suitable for distinguishing between benign and malignant pulmonary nodules. The key innovation is the dynamic adjustment of the mutation probability in the Late Acceptance Hill Climbing algorithm, which greatly enhances the local search capabilities. And the RFO&#39;s reproduction mechanism has been improved through the utilization of a more efficient interpolation form. The biomarker selection model employs a multi-objective fitness function that takes into account both accuracy and quantity. After this, the optimal model yielded a biomarker panel, including Inosine, Hippuric acid, Alanine, and other metabolites. This model demonstrates outstanding performance on an independent test dataset, achieving a fitness value of 0.9136, an AUC (Area Under the Curve) of 0.9926, a sensitivity of 0.9643, and a specificity of 0.9412. Furthermore, the clinical net benefit is highlighted across various risk thresholds by decision curve analysis. These results underscore the significance of DM-LAHC aided RFO in the selection of serum metabolomic biomarkers for lung cancer. The supporting source codes of this work can be found at: https://github.com/zzl2022/DM-LAHC-aided-RFO .},
  archive      = {J_ASOC},
  author       = {Shuli Guo and Zhilei Zhao and Lina Han and Lei Wu and Xiaowei Song and Anil Baris Cekderi},
  doi          = {10.1016/j.asoc.2024.111602},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111602},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic mutation late acceptance hill climbing aided red fox optimization for metabolomic biomarkers selection from lung cancer patient sera},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable benchmarks and performance measures for dynamic
multi-objective optimization. <em>ASOC</em>, <em>159</em>, 111600. (<a
href="https://doi.org/10.1016/j.asoc.2024.111600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic multi-objective optimization problems (DMOPs) can be utilized to model certain real-world problems that have a dynamic nature. Algorithms for solving DMOPs can be evaluated and improved by comparing their performance on different benchmarks. However, some existing benchmarks for DMOPs have the limitation of non-uniform weights for decision variables. Additionally, dynamic many-objective optimization problems (DMaOPs) involve more than three objectives, but only a few existing benchmarks can be extended to accommodate DMaOPs. Furthermore, some existing performance measures for DMOPs may not effectively compare the relative performance differences between multiple algorithms or evaluate the search uniformity among different objectives. In this paper, we propose improvements to an existing benchmark for DMOPs by expanding the impact range of decision variables. Moreover, a benchmark framework that can be extended to accommodate DMaOPs is proposed, thus addressing a research gap between the optimization of DMOPs and DMaOPs. Additionally, a set of performance measures for DMOPs are proposed, which can evaluate the relative performance and search uniformity of multi-objective optimization algorithms. By comparing the performance of state-of-the-art and commonly used algorithms on test problems, we can gain a better understanding of the characteristics and strengths and weaknesses of the algorithms and test problems.},
  archive      = {J_ASOC},
  author       = {Baiqing Sun and Changsheng Zhang and Haitong Zhao and Zhang Yu},
  doi          = {10.1016/j.asoc.2024.111600},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111600},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scalable benchmarks and performance measures for dynamic multi-objective optimization},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ensemble deep learning model for human activity analysis
using wearable sensory data. <em>ASOC</em>, <em>159</em>, 111599. (<a
href="https://doi.org/10.1016/j.asoc.2024.111599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lately, the continuous temporal data from motion sensors in wearable devices has been great interest for the research community due to its demand for analyzing human activities in several applications e.g. healthcare, sports, and surveillance. Numerous solo deep learning models have been proposed in the literature to extract an adequate feature representation from temporal sensory data, however, they are limited to encode only one aspect of the data, and not sufficient to capture the in-depth association between the patterns. This paper presents an ensemble deep model to encode the complex patterns and dependencies in temporal data. An ensemble model refers to the merging or blending of separate deep models, aiming to utilize their unique strengths and abilities to construct a more resilient and effective solution. The contributions of this paper are two folds: First, a systematic evaluation of five different ensemble models is presented to encode the raw sensory data. Secondly, this study proposed a lightweight, multi-layer, and hybrid LSTM-GRU model to recognize the human activities using time series sensory data. Specifically, the raw sensory data is fed into a two-layer LSTM followed by GRU layers. To avoid over-fitting and accelerate convergence, we employed dropout and batch normalization layers. An extensive evaluation of all the five ensemble models is carried out on two large benchmark datasets, namely: UCI-HAR and WISDM. The proposed LSTM-GRU architecture achieved the recognition accuracy of 99.06% and 96.61% on the WISDM and UCI-HAR, respectively. The performance comparison with existing state-of-the-art techniques confirms the superiority and efficacy of the proposed algorithm.},
  archive      = {J_ASOC},
  author       = {Sheeza Batool and Muhammad Hassan Khan and Muhammad Shahid Farid},
  doi          = {10.1016/j.asoc.2024.111599},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111599},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ensemble deep learning model for human activity analysis using wearable sensory data},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A discrete artificial bee colony algorithm and its
application in flexible flow shop scheduling with assembly and machine
deterioration effect. <em>ASOC</em>, <em>159</em>, 111593. (<a
href="https://doi.org/10.1016/j.asoc.2024.111593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real production, assembly and machine deterioration are extremely common phenomenon and flexible flow shop scheduling problem (FFSP) is also extensively investigated. However, assembly FFSP with machine deterioration effect (AFFSP-DE) is neglected. This paper addresses an AFFSP-DE with makespan criterion and proposes a discrete artificial bee colony (DABC) algorithm. In DABC, the number of employed (onlooker) bees is dynamically changeable according to evolutionary quality to adaptively allocate computing resources for two kinds of bees, and a new scout bee phase is designed to improve convergence speed. Finally, the relationships between makespan and different deterioration rates are analyzed systematically by extensive experiments, and the results demonstrate that DABC outperforms the existing algorithms over 82% test instances in solving the AFFSP-DE.},
  archive      = {J_ASOC},
  author       = {Ming Li and Ching-Ter Chang and Zhi Liu},
  doi          = {10.1016/j.asoc.2024.111593},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111593},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A discrete artificial bee colony algorithm and its application in flexible flow shop scheduling with assembly and machine deterioration effect},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The selection of reform models for provincial rural credit
cooperatives unions in china using an extended CPT-TODIM method based on
novel type-2 fuzzy numbers. <em>ASOC</em>, <em>159</em>, 111585. (<a
href="https://doi.org/10.1016/j.asoc.2024.111585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of a suitable reform model for provincial rural credit cooperatives unions (PRCCUs) is a pivotal aspect of China’s ongoing financial reform endeavors. This involves optimizing internal processes, enhancing efficiency, minimizing operational costs, elevating service quality, strengthening financial innovation capabilities, and promoting the healthy growth of rural financial markets. However, current research on this matter primarily concentrates on qualitative analysis. As of yet, a scientifically grounded decision-making framework from a quantitative standpoint has not been developed. This absence of quantitative analysis results in inaccuracies in research findings, thereby hindering the practical applicability of outcomes. To tackle this formidable challenge, we propose an innovative quantitative analysis framework employing the multi-criteria group decision-making method. This framework, which builds upon the cumulative prospect theory (CPT), the TODIM method, and the most recent advances in type-2 fuzzy numbers (T2FNs), is referred to as the T2FNs-CPT-TODIM. Initially, we define a novel score function, followed by the introduction of a fresh method for computing the distance between the T2FNs. Subsequently, our model pioneers an exclusive weight allocation strategy for decision-makers, grounded in the principles of Dice Similarity. Furthermore, the objective weights of criteria can be computed utilizing the Distance Correlation-based CRITIC approach. We then optimize the rankings by applying the advantage of gains and losses preference in the CPT-TODIM. Ultimately, we apply our proposed model to the practical task of selecting the optimal reform model for a specific PRCCU. Thorough comprehensive sensitivity and comparative analyses, the results validate the theoretical robustness and practical efficacy of our proposed method.},
  archive      = {J_ASOC},
  author       = {Wen Li and Luqi Wang and Zhiliang Ren and Obaid Ur Rehman},
  doi          = {10.1016/j.asoc.2024.111585},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111585},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The selection of reform models for provincial rural credit cooperatives unions in china using an extended CPT-TODIM method based on novel type-2 fuzzy numbers},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PURF: Improving teacher representations by imposing
smoothness constraints for knowledge distillation. <em>ASOC</em>,
<em>159</em>, 111579. (<a
href="https://doi.org/10.1016/j.asoc.2024.111579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is one of the most persuasive approaches to model compression that transfers the representational expertise from large deep-learning teacher models to a small student network. Although numerous techniques have been proposed to improve teacher representations at the logits level, no study has examined the weaknesses in the representations of the teacher at the feature level during distillation. On the other hand, in a trained deep-learning model, all the kernels are not uniformly activated to make specific predictions. Transferring this knowledge may result in a student learning a suboptimal intrinsic distribution and restrict the existing distillation methods from exploiting their highest potential. Motivated by these issues, this study analyses the generalization capability of teachers with or without a uniformly activated channel distribution. Preliminary investigations and theoretical analyses show that partly uniforming or smoothing feature maps offer improved representation that enriches the generalization capability. Based on these observations, it is hypothesized that distillation-based explicit supervision using smoothed feature maps and cross-entropy loss plays a significant role in improving generalization. Hence, this paper proposes a novel technique called P artly U nified R ecalibrated F eature ( PURF ) map distillation. The proposed method recalibrates the feature maps by intercommunicating the representational cues among nearest-neighbor channels. PURF increases the performance of state-of-the-art knowledge distillation (KD) methods across architectures by improving generalization, model compression, few-shot training, transferability, and robustness transfer on standard benchmark datasets. PURF achieves 1.51% average accuracy improvements on seven diverse architectures in image classification tasks. PURF increases the performance of the state-of-the-art knowledge distillation methods by an average accuracy of 1.91% across architectures. Moreover, PURF achieves an average of 2.02%, and 0.96% higher accuracy in transferability and robustness tasks, respectively, on standard benchmark datasets.},
  archive      = {J_ASOC},
  author       = {Md Imtiaz Hossain and Sharmen Akhter and Choong Seon Hong and Eui-Nam Huh},
  doi          = {10.1016/j.asoc.2024.111579},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111579},
  shortjournal = {Appl. Soft. Comput.},
  title        = {PURF: Improving teacher representations by imposing smoothness constraints for knowledge distillation},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A moving metaverse: QoE challenges and standards
requirements for immersive media consumption in autonomous vehicles.
<em>ASOC</em>, <em>159</em>, 111577. (<a
href="https://doi.org/10.1016/j.asoc.2024.111577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of Autonomous Vehicles (AVs) has blurred the distinction between drivers and passengers, resulting in increased demand for in-car entertainment. Simultaneously, social networking platforms are evolving to adopt AR (Augmented Reality)/VR (Virtual Reality)-centric metaverses. Self-driving cars will free up the drivers and passengers to engage in leisure activities such as watching VR videos and playing games while travelling. Before driverless cars can become a “third living space”, open challenges confront the provision of immersive content (e.g., VR videos and gaming) with a high level of Quality of Experience (QoE). Current autonomous vehicular network technologies focus on navigation and safety and rely on ubiquitous high-speed 5G/6G network connectivity to provide a seamless streaming experience in AVs. The compounded risks of cyber- and motion-sickness are identified as barriers to the enjoyment of in-car VR entertainment technologies. The influence of AV car networks in different traffic conditions is identified as a key factor to minimize a person’s risk of sickness and increase overall QoE. However, assessing the QoE of immersive media consumption in AV to its full potential is a difficult task that requires proper standard guidelines and directions. Existing surveys focus only on a specific metaverse or transportation system. These surveys lack a comprehensive view of the entire immersive media consumption process, QoE factors and aspects, and QoE standards for assessing immersive content in vehicular networks. To address these issues, we present an overview of the research landscape in the context of QoE and network standards that highlights the need for experimental studies to inform and consolidate the methodologies and recommendations for standardization of immersive content QoE assessment in AVs. Research and user studies have not yet yielded standards or recommendations describing assessment methodologies to evaluate immersive systems, such as exist for traditional video and images. A framework is outlined to provide a comprehensive understanding of the QoE influencing factors and aspects that will assist in adapting, augmenting, and producing standards for immersive entertainment in autonomous vehicles.},
  archive      = {J_ASOC},
  author       = {Muhammad Shahid Anwar and Ahyoung Choi and Sadique Ahmad and Khursheed Aurangzeb and Asif Ali Laghari and Thippa Reddy Gadekallu and Andrew Hines},
  doi          = {10.1016/j.asoc.2024.111577},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111577},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A moving metaverse: QoE challenges and standards requirements for immersive media consumption in autonomous vehicles},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy deep learning for modeling uncertainty in character
recognition using EEG signals. <em>ASOC</em>, <em>159</em>, 111575. (<a
href="https://doi.org/10.1016/j.asoc.2024.111575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of brain signals for character recognition is a challenging task associated with uncertainties. This study aims to present an Improved Type-2 Fuzzy Deep Learning (IT2FDL) framework for uncertainty management in character recognition using EEG signals by learning rules from each other in the learning layer. The Evolutionary Algorithm (EA) was employed for this study&#39;s optimum design of the proposed IT2FDL model. This algorithm was used to determine the T2F set parameters. The model parameters were tuned by Parallel Swarm Optimization (PSO) Algorithm. The present framework optimized the fusion layer of the model. This layer is a multi-layered Recurrent Type2 Fuzzy Neural Network (RT2FNN) with a learning layer composed of RNN cells. For performance evaluation, the models were examined using three public databases: BCI Competition III-Dataset II, BCI Competition II-Dataset IIb, and EEG Dataset. The optimized IT2FDL model outperformed the counterpart models with a significant improvement of test score of 2.5% for Competition III-Dataset II at epoch 15, 2.5% for Competition II-Dataset IIb at epoch 3, and 1% for EEG Dataset at epoch 15. The proposed IT2FDL-PSO (GRU) model showed a mean test score of 96.2% at epoch 15 for all 58 subjects, which outperformed other models with many subjects for character recognition.},
  archive      = {J_ASOC},
  author       = {Farzaneh Latifi and Rahil Hosseini and Arash Sharifi},
  doi          = {10.1016/j.asoc.2024.111575},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111575},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy deep learning for modeling uncertainty in character recognition using EEG signals},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Profit maximization through integrated order acceptance and
scheduling: A metaheuristic approach. <em>ASOC</em>, <em>159</em>,
111570. (<a href="https://doi.org/10.1016/j.asoc.2024.111570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronization of production capacity with demand plays a significant role in maximizing profit. Firms attempt to align their activities, requiring that businesses be occasionally turned away. A trade-off exists between the revenue gained by a specific order and all related processing costs. We solve an important order acceptance and scheduling problem arising from this business model. A firm that receives multiple orders must determine: (i) whether to accept or reject any order, (ii) how to assign accepted orders to identical parallel machines, (iii) the sequence of accepted orders, and (iv) when to schedule order starting times. The objective is to maximize the total profit, measured as the difference between revenue and tardiness penalty costs. We propose an effective hybrid algorithm (GA-RP-TS) that combines a refining process (RP), a genetic algorithm (GA), and a tabu search (TS) step to tackle this NP-hard problem. The hybrid GA-RP-TS employs GA’s strong global search ability, uses RP to improve a solution, and performs exploitation with TS. In particular, the proposed GA-RP-TS balances the intensification and diversification ability of metaheuristics and evolutionary algorithms. We evaluate the performance of the proposed GA-RP-TS by testing benchmark instances. Results show that the proposed GA-RP-TS outperforms the state-of-the-art algorithm in solving large-sized instances. GA-RP-TS obtains 22 new best solutions out of the 54 tested benchmark instances, besides helping prove optimality for two instances. In addition, GA-RP-TS consumes less computation time. Sensitivity analysis shows that the excellent performance of GA-RP-TS comes from the hybridization of its three components, i.e., GA, RP, and TS.},
  archive      = {J_ASOC},
  author       = {Yantong Li and Lianhua Tang and Danyu Bai and Leandro C. Coelho},
  doi          = {10.1016/j.asoc.2024.111570},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111570},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Profit maximization through integrated order acceptance and scheduling: A metaheuristic approach},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy-based predictive deep reinforcement learning for
robust and constrained optimal control of industrial solar thermal
plants. <em>ASOC</em>, <em>159</em>, 111432. (<a
href="https://doi.org/10.1016/j.asoc.2024.111432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating distributed solar fields (DSFs) into conventional heat and power plants (CHPs) of industries is mostly constrained by the availability of a real-time capable control scheme. Safe and efficient operation of industrial DSF requires the supply of a fluctuating and periodically available energy at the required temperature while reducing losses and ensuring operational constraint. Reinforcement learning (RL), specifically Q-learning methods, are being recently applied for such control tasks. However, existing RL approaches cannot be directly applied to continuous domain control tasks or tasks with operation constraints. While using deep learning in model-free RL can remove these limitations, tractability and scalability boundaries on performing the deep learning are becoming significant, with scarce data and multi-objective concerns. In contrast, model-based deep RL schemes can be applied for sample efficiency and satisfying operational constraints, but their modeling framework is not general and accurate. To address these challenges, the work here develops a hybrid fuzzy convolution model (HFCM) that takes full advantages of data, models (dynamic and steady-state), and prior knowledge on industrial DSF. The HFCM is then extended for use in deep deterministic policy gradient (DDPG) algorithm to learn the DSF control task. This is done so by solving a multi-objective optimization problem, which is formulated as a constrained Markov decision process (CMDP) with continuous state and actions. Some practical and relevant findings were made with this HFCM. Firstly, it allowed the DDPG agent to learn independently a temperature state and a disturbance state, using only temperature measurement and single realizations of state vector. It also permitted the testing of an operational state violation strategy on a DDPG actions, and at the same time, the correction of the effects at a safety layer, if needed. Furthermore, the proposed control strategy simultaneously reduced mean temperature tracking error by 24%–51% and energy gain by 13.9%–17.35% when compared respectively to model-free DDPG and MPC baselines.},
  archive      = {J_ASOC},
  author       = {Fitsum Bekele Tilahun},
  doi          = {10.1016/j.asoc.2024.111432},
  journal      = {Applied Soft Computing},
  month        = {7},
  pages        = {111432},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy-based predictive deep reinforcement learning for robust and constrained optimal control of industrial solar thermal plants},
  volume       = {159},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Physics-informed neural networks with skip connections for
modeling and control of gas-lifted oil wells. <em>ASOC</em>,
<em>158</em>, 111603. (<a
href="https://doi.org/10.1016/j.asoc.2024.111603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks, while powerful, often lack interpretability. Physics-Informed Neural Networks (PINNs) address this limitation by incorporating physics laws into the loss function, making them applicable to solving Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs). The recently introduced PINC framework extends PINNs to control applications, allowing for open-ended long-range prediction and control of dynamic systems. In this work, we enhance PINC for modeling highly nonlinear systems such as gas-lifted oil wells. By introducing skip connections in the PINC network and refining certain terms in the ODE, we achieve more accurate gradients during training, resulting in an effective modeling process for the oil well system. Our proposed improved PINC demonstrates superior performance, reducing the validation prediction error by an average of 67% in the oil well application and significantly enhancing gradient flow through the network layers, increasing its magnitude by four orders of magnitude compared to the original PINC. Furthermore, experiments showcase the efficacy of Model Predictive Control (MPC) in regulating the bottom-hole pressure of the oil well using the improved PINC model, even in the presence of noisy measurements.},
  archive      = {J_ASOC},
  author       = {Jonas Ekeland Kittelsen and Eric Aislan Antonelo and Eduardo Camponogara and Lars Struen Imsland},
  doi          = {10.1016/j.asoc.2024.111603},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111603},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Physics-informed neural networks with skip connections for modeling and control of gas-lifted oil wells},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive framework for designing and learning fuzzy
cognitive maps at the granular level. <em>ASOC</em>, <em>158</em>,
111601. (<a href="https://doi.org/10.1016/j.asoc.2024.111601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Cognitive Maps (FCM) possesses interpretability and fuzzy reasoning capability. It is widely applied in addressing time series prediction problems and various algorithms for learning FCM are proposed. Information granulation method can transfer knowledge from numerical data to granular knowledge. However, how to learn FCM better at the granular level is a challenge. Therefore, we develop a comprehensive framework, which consists of six stages: encoding, granulation, learning, prediction, degranulation and decoding. In this framework, we extract three-dimensional information data from univariate time series data for encoding. It is granulated by the fuzzy C-means clustering algorithm. Then the FCM is constructed at the granular level. In the learning stage, we propose a model based on the adaptive loss function to learn FCM which can be solved by alternating direction multiplier method and quadratic programming method. After learning the FCM, time series prediction is carried out. The degranulation function converts the information granulation back into numerical data. We do some experiments on twelve real datasets, the results demonstrate the effectiveness of the comprehensive framework.},
  archive      = {J_ASOC},
  author       = {Qimin Zhou and Yingcang Ma and Zhiwei Xing and Xiaofei Yang},
  doi          = {10.1016/j.asoc.2024.111601},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111601},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive framework for designing and learning fuzzy cognitive maps at the granular level},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Design element extraction of plantar pressure imaging
employing meta-learning-based graphic convolutional neural networks.
<em>ASOC</em>, <em>158</em>, 111598. (<a
href="https://doi.org/10.1016/j.asoc.2024.111598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting plantar pressure images intelligently can provide valuable insight for people with high blood pressure, making bespoke footwear requirements possible and resulting in more comfortable shoe designs. It is, however, difficult to extract design elements from a segmented image dataset. To address this challenge, we propose an ML-GNN model that segments plantar pressure images using metal-earning. The first part of the paper presents a method for extracting image features that reduce the complexity of the ML-GNN algorithm. To create the network structure, we propose optimization meta-based learning. Using a meta-learning-based graphic neural network, we enhance our mask-based CNN prediction model with VGG16 and CNN layers. We pre-processed the plantar pressure dataset using pressure-sensing data acquisition and compared the results. By defining standard image segmentation indices, we demonstrate the high effectiveness of our research. We have developed an ML-GNN model that improves the segmentation accuracy of plantar pressure images and can also be applied to other sensor image datasets. Through our shoe-last customization approach, we enable the shoe industry to manufacture shoes more efficiently, particularly for people with specific healthcare needs who require bespoke shoe designs. Our findings demonstrate the potential of intelligent image segmentation to advance the field of footwear design and improve the lives of people with specific health requirements.},
  archive      = {J_ASOC},
  author       = {Dan Wang and Zairan Li and Nilanjan Dey and Rubén González Crespo and Fuqian Shi and R. Simon Sherratt},
  doi          = {10.1016/j.asoc.2024.111598},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111598},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Design element extraction of plantar pressure imaging employing meta-learning-based graphic convolutional neural networks},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Corrigendum to “dual quaternion hand-eye calibration
algorithm for hunter-prey optimization based on twice
opposition-learning and random differential variation” appl. Soft
comput. J. 154 (2024) 111249. <em>ASOC</em>, <em>158</em>, 111597. (<a
href="https://doi.org/10.1016/j.asoc.2024.111597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Yun-tao Zhao and Wen Li and Wei-gang Li},
  doi          = {10.1016/j.asoc.2024.111597},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111597},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Corrigendum to “Dual quaternion hand-eye calibration algorithm for hunter-prey optimization based on twice opposition-learning and random differential variation” appl. soft comput. j. 154 (2024) 111249},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy minkowski distance-based fusion of convolutional
neural networks for gastrointestinal disease detection. <em>ASOC</em>,
<em>158</em>, 111595. (<a
href="https://doi.org/10.1016/j.asoc.2024.111595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate classification of gastrointestinal (GI) conditions from medical images is a critical task for facilitating timely diagnosis and effective treatment. However, the reliance on manual diagnosis introduces the possibility of human errors. In response, researchers have been tirelessly working to develop robust computerized methods that can significantly enhance diagnostic accuracy. In this study, we present a novel approach designed to elevate the precision of classification tasks through the utilization of a Fuzzy Minkowski Distance-based Ensemble Model. Our ensemble model is accurately constructed by integrating three well-established pre-trained convolutional neural network (CNN) models: MobileNet, ResNet101V2, and Xception. To enhance the robustness of these base models, we incorporate ResNeXt block, effectively amplifying their ability in feature extraction and representation. Furthermore, we extract probabilities from these models and aggregate them using a fuzzy Minkowski Distance approach. This technique serves to minimize error values between observed and ground-truth data, leading to a further enhancement in detection accuracy. For our experiments, we utilize a publicly available dataset containing 6000 endoscopy images. These images are not only enhanced through the application of image contrast enhancement techniques but are also subjected to augmentation processes, effectively enhancing the dataset&#39;s diversity and contributing to improved performance in the classification task. After conducting comprehensive assessments, we highlight the strength of our ensemble model compared to single base models and existing methods. The ensemble consistently achieves an impressive accuracy of 99.62%, showcasing its potential as a powerful tool for accurate diagnosis in the field of gastroenterology.},
  archive      = {J_ASOC},
  author       = {Sohaib Asif and Qurrat-ul-Ain},
  doi          = {10.1016/j.asoc.2024.111595},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111595},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy minkowski distance-based fusion of convolutional neural networks for gastrointestinal disease detection},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An end-to-end deep clustering method with consistency and
complementarity attention mechanism for multisensor fault diagnosis.
<em>ASOC</em>, <em>158</em>, 111594. (<a
href="https://doi.org/10.1016/j.asoc.2024.111594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep clustering methods have found successful applications in single-sensor data fault diagnosis. However, most of these methods employ separate optimization strategies that overlook the interaction between feature learning and clustering. Moreover, conventional deep learning methods for fault diagnosis often disregard the consistent and complementary information inherent in the multisensor data, resulting in unsatisfactory multisensor fault diagnosis performance. In this study, we introduce a novel End-to-end Deep Clustering Method with Consistency and Complementarity Attention Mechanism, termed EDCM-CCAM, tailored for multisensor fault diagnosis. Firstly, multiple deep autoencoder networks are utilized to concurrently extract the deep representation features from various sensor inputs. Secondly, we introduce a Consistency and Complementarity Attention Mechanism (CCAM) to facilitate multisensor feature fusion, accompanied by the design of two distinct loss functions to fully exploit the consistent and complementary information within multisensor data. Finally, fault pattern recognition in multisensor data is accomplished through Kullback-Leibler (KL) divergence-based clustering, while a joint optimization strategy is employed to simultaneously optimize all components of the EDCM-CCAM. The efficacy of the proposed method is validated on a gearbox dataset, demonstrating superior performance in multisensor fault diagnosis compared to alternative methods.},
  archive      = {J_ASOC},
  author       = {Zhangjun Wu and Gang Fang and Yifei Wang and Renli Xu},
  doi          = {10.1016/j.asoc.2024.111594},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111594},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An end-to-end deep clustering method with consistency and complementarity attention mechanism for multisensor fault diagnosis},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A matrix norm-based pythagorean fuzzy metric and its
application in MEREC-SWARA-VIKOR framework for solar panel selection.
<em>ASOC</em>, <em>158</em>, 111592. (<a
href="https://doi.org/10.1016/j.asoc.2024.111592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the escalating electricity demand from industrialization and modernization, it is imperative to explore clean and sustainable energy sources. Solar energy has become a feasible solution for this surging demand. Solar panels, vital components of solar power systems, are crucial in converting sunlight into electricity. However, selecting the most suitable solar panel is a complex task involving subjective and measurable parameters involving uncertainties. To address the uncertainties and ambiguities in real-life problems, the theory of Pythagorean fuzzy sets with distance measures has emerged as a flexible and superior tool. Nevertheless, many existing distance functions often fail to meet necessary metric conditions, leading to inaccurate and unreasonable outcomes. These limitations are pointed out by many literature. In addition, we observe a serious drawback of parameter dependency of some existing measures. This paper proposes a novel metric for Pythagorean fuzzy sets using the matrix norm to overcome these drawbacks. Additionally, we discuss various mathematical properties and provide geometrical representations for the proposed distance measure. To demonstrate the superiority of the proposed measure, we conduct a comparative study against existing distance measures. Furthermore, we develop a Pythagorean fuzzy-method based on the removal effects of criteria-stepwise weight assessment ratio analysis-vlse kriterijumska optimizacija kompromisno resenje (PF-MEREC-SWARA-VIKOR) method for selecting solar panel systems. Our approach incorporates the MEREC method for objective criteria, utilizes the SWARA method to calculate subjective weights, and employs the VIKOR method in the Pythagorean fuzzy context to determine the preference order of alternatives. We conduct sensitivity analysis and comparative studies with existing developed methods to validate our method. Highlight the effectiveness and advantages of our novel approach in selecting solar panels. Finally, exploring the future research directions and challenges associated with the proposed methodology.},
  archive      = {J_ASOC},
  author       = {Naveen Kumar and Juthika Mahanta},
  doi          = {10.1016/j.asoc.2024.111592},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111592},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A matrix norm-based pythagorean fuzzy metric and its application in MEREC-SWARA-VIKOR framework for solar panel selection},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-way trajectory privacy-preserving model based on
multi-feature fusion. <em>ASOC</em>, <em>158</em>, 111591. (<a
href="https://doi.org/10.1016/j.asoc.2024.111591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalent method in trajectory privacy protection through publishing k k -1 similar trajectories alongside a target trajectory, often relies on a single feature, which can compromise the balance between privacy, data utility, and processing efficiency. Addressing this, our study introduces a nuanced three-way decision model that integrates multiple trajectory features: staying areas, average velocity, and distance. This model begins by filtering candidate trajectories through staying areas and average velocity. Subsequently, it employs a three-way decision-making process based on a novel dynamic trajectory warping (DTrW) distance metric, which obviates the need for trajectory pre-alignment and mitigates information loss. This process classifies trajectories into accepted, rejected, or pending categories, with the final set compiled through a combined metric of average speed and staying areas. Comparative experiments on a classic real trajectory dataset validate the model’s superiority, demonstrating enhanced privacy protection, improved data utility, and increased efficiency over single-feature-based approaches and other k k -anonymity models.},
  archive      = {J_ASOC},
  author       = {Jianfeng Xu and Yiping Wei and Yingxiao Chen},
  doi          = {10.1016/j.asoc.2024.111591},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111591},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way trajectory privacy-preserving model based on multi-feature fusion},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research on multi-view clustering algorithm based on
sequential three-way decision. <em>ASOC</em>, <em>158</em>, 111590. (<a
href="https://doi.org/10.1016/j.asoc.2024.111590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering has achieved many applications in recent years. But the existing multi-view clustering methods face two problems, firstly, the traditional multi-view clustering uses a hard clustering method, which cannot describe the uncertainty between the samples and the clusters, and secondly how to perform effective incremental learning on multi-view data when the number of views increases. Therefore to address the above two problems, we firstly propose a three-way fuzzy spectral clustering algorithm based on fuzzy clustering, which can perform three-way clustering on multi-views and get soft clustering results, thus describing the uncertainty between samples and clusters. Then based on the sequential decision making approach, an incremental learning mechanism is designed for multi-view clustering when the number of views increases. Finally, these two works are combined to propose a multi-view clustering algorithm based on sequential three-way decision making. The experimental results demonstrate that the method proposed in this paper has better clustering accuracy and efficiency compared to the traditional multi-view clustering algorithm.},
  archive      = {J_ASOC},
  author       = {Yi Xu and Guoqing Niu},
  doi          = {10.1016/j.asoc.2024.111590},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111590},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research on multi-view clustering algorithm based on sequential three-way decision},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metallic surface defect recognition network based on global
feature aggregation and dual context decoupled head. <em>ASOC</em>,
<em>158</em>, 111589. (<a
href="https://doi.org/10.1016/j.asoc.2024.111589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface defect detection is a crucial inspection phase in ensuring industrial product quality and reliability, particularly in the context of metallic manufacturing components. While existing deep learning-based approaches have demonstrated some effectiveness, the design of certain network structures still lacks consideration for industrial scenarios. This paper proposed a complex metallic surface defect detection neural network which incorporates two innovatively designed modules along with other targeted enhancements. Firstly, the spatial pyramid pooling (SPP) structure is redesigned for richer global-local information fusion. A global feature fusion and redistribution module (GFAR) is designed to fully leverage feature information across different scales. With the combination of a novel global positional attention mechanism (GPAM), GFAR is capable of distinguishing defects from complex backgrounds. Moreover, the asymmetric convolution blocks (ACB) are employed to process the final features, enhancing the representational capacity for intricate defect shapes. Subsequently, the detection head is decoupled in both spatial and task domains to addresses the problems posed by inconsistent focus between classification and regression tasks. Finally, the reassignment of loss functions is undertaken for each respective task to enhance their alignment with the specific requirements of the defect detection task. Extensive ablation experiments are conducted on two steel surface defect datasets, NEU-DET and GC10-DET, to demonstrate the effectiveness of each module. The defects detection accuracy of our method respectively reaches 80.9 % and 84.8 % for the two datasets, which outperforms the majority of state-of-the-art detection neural networks.},
  archive      = {J_ASOC},
  author       = {Kefei Qian and Lai Zou and Zhiwen Wang and Wenxi Wang},
  doi          = {10.1016/j.asoc.2024.111589},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111589},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metallic surface defect recognition network based on global feature aggregation and dual context decoupled head},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A metaverse framework for IoT-based remote patient
monitoring and virtual consultations using AES-256 encryption.
<em>ASOC</em>, <em>158</em>, 111588. (<a
href="https://doi.org/10.1016/j.asoc.2024.111588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The convergence of Internet of Things (IoT) and metaverse technologies is revolutionizing healthcare. This study introduces a pioneering framework tailored for health monitoring within the metaverse. By reshaping remote patient monitoring and virtual consultations, the framework utilizes vital parameters like heart rate, blood pressure, and body temperature. It integrates IoT sensors, augmented reality (AR), and virtual reality (VR), establishing a cohesive metaverse environment for healthcare interactions. Notably, robust 256-bit AES encryption ensures data privacy and security. Our analysis highlights the pivotal role of metaverse architecture in healthcare, emphasizing the efficacy of AES-256 encryption in preserving patient confidentiality. Findings underscore the framework&#39;s potential to enhance remote patient care while upholding stringent data privacy standards. Moreover, it fosters trust among patients, healthcare providers, and regulatory bodies. In summary, this comprehensive framework marks a significant advancement in remote patient care, promising improved health outcomes and a secure foundation for healthcare in the metaverse.},
  archive      = {J_ASOC},
  author       = {Zainab Khalid Mohammed and Mazin Abed Mohammed and Karrar Hameed Abdulkareem and Dilovan Asaad Zebari and Abdullah Lakhan and Haydar Abdulameer Marhoon and Jan Nedoma and Radek Martinek},
  doi          = {10.1016/j.asoc.2024.111588},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111588},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A metaverse framework for IoT-based remote patient monitoring and virtual consultations using AES-256 encryption},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hacker group identification based on dynamic heterogeneous
graph node update. <em>ASOC</em>, <em>158</em>, 111587. (<a
href="https://doi.org/10.1016/j.asoc.2024.111587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the critical task of hacker identification within the cyber traceability system. While the latest hacker group identification method based on the heterogeneous graph attention network (HGHAN) holds promise in discovering hacker groups, its potential is hindered by the underutilization of node information and poor training efficiency. Particularly, attribute information dilution during node feature extraction and lengthy training for node embedding vector reassignment when new nodes are added have been observed. To rectify these shortcomings, the paper presents an improved model for hacker group identification. This novel approach leverages dynamic heterogeneous graph node updating to significantly boost efficiency without compromising the original model’s classification accuracy. The key aspects of the method involve pre-learning for node attribute training vectors, LSTM (Long Short-Term Memory) and attention mechanisms for node feature vector refinement, and introducing a sparse matrix and dynamic node update scheme. The experimental results demonstrate marked improvements in training efficiency and graph update processes while maintaining classification accuracy. This advancement signifies the improved HGHAN model’s capacity to adeptly navigate real-world network dynamics, assisting researchers in pinpointing malicious attackers amid cyber incidents.},
  archive      = {J_ASOC},
  author       = {Yijia Xu and Yong Fang and Cheng Huang and Zhonglin Liu and Weipeng Cao},
  doi          = {10.1016/j.asoc.2024.111587},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111587},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hacker group identification based on dynamic heterogeneous graph node update},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated test case generation for path coverage using
hierarchical surrogate-assisted differential evolution. <em>ASOC</em>,
<em>158</em>, 111586. (<a
href="https://doi.org/10.1016/j.asoc.2024.111586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of automated test case generation for path coverage (ATCG-PC), commonly used evolutionary optimization algorithms face significant computational overhead due to the necessity of executing the test program for individual fitness evaluations. To address this challenge and enhance efficiency while ensuring comprehensive coverage, this paper proposes a Hierarchical Surrogate-Assisted Differential Evolution (HSADE) approach. HSADE innovatively incorporates two surrogate models: a Global Surrogate Model (GSM) utilizing Gaussian Process Regression (GPR) for a macroscopic view of the fitness landscape, and a Local Surrogate Model (LSM) employing Radial Basis Function Network (RBFN) for precise fitness estimations in targeted subspaces. This hierarchical approach facilitates a two-phase optimization process: an initial coarse search to pinpoint areas of interest followed by a focused, fine-grained search for precision, where LSM is combined with differential evolution (DE) algorithm for fine optimization in these regions. To mitigate the risk of converging to local optima, HSADE employs Random Opposition-Based Learning (ROBL) to enhance the population diversity, leveraging the uncertainty assessment capabilities of the GPR. Our experimental validation, conducted on a diverse set of benchmark programs, demonstrates HSADE’s effectiveness in reducing the number of fitness evaluations and computational time required for generating high-quality test cases. Compared to established evolutionary algorithms, HSADE achieves superior path coverage with significantly less computational effort.},
  archive      = {J_ASOC},
  author       = {Lin Gao and Songyan Bai and Mingxing Liu and Fan Li},
  doi          = {10.1016/j.asoc.2024.111586},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111586},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated test case generation for path coverage using hierarchical surrogate-assisted differential evolution},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information granule optimization and co-training based on
kernel method. <em>ASOC</em>, <em>158</em>, 111584. (<a
href="https://doi.org/10.1016/j.asoc.2024.111584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-training was originally designed for multi-view data. Subsequent theoretical research has extended co-training to the application of single-view data. The construction of a feature subspace is one of the methods to expand single-view data into multi-view data, so the establishment of a feature subspace is the key to this approach. In this paper, the kernel method is used to form an implicit feature subspace, to perform multi-view co-training on single-view data. Because the attribute value in the feature subspace is not known, the subspace is a pseudo-view. To make the view adapt to the base classifier, this paper uses the neighborhood classifier as the base classifier and proposes an adaptive kernel function and three kernel parameter optimization methods according to the characteristics of the neighborhood classifier to build the feature subspace adapted to the neighborhood classifier. The decision of the neighborhood classifier needs to be made based on information granules generated by unlabeled objects. In the iteration of the feature subspace, we can continuously learn and optimize the information granule, and finally form what we expect, to get the implicit feature space corresponding to the granule and improve the accuracy of the base classifier. Finally, taking five data sets from UCI, and using accuracy and F1-score as evaluation indicators, we conduct a comparative experiment. The experimental results show that an adaptive kernel function, three kernel parameter optimization methods, and the co-training method presented in this paper are effective.},
  archive      = {J_ASOC},
  author       = {Yuzhang Bai and Jusheng Mi and Leijun Li},
  doi          = {10.1016/j.asoc.2024.111584},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111584},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Information granule optimization and co-training based on kernel method},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opinion formation over dynamic hierarchical networks with
acquaintances and strangers: A genetic variation based double-mechanism
framework. <em>ASOC</em>, <em>158</em>, 111583. (<a
href="https://doi.org/10.1016/j.asoc.2024.111583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the psychological phenomenon that agents generally adopt different opinions or action-update mechanisms when faced with different types of neighbors, we propose a novel double-mechanism framework over dynamic hierarchical networks to fill this gap. First, a novel multi-attribute genetic variation-based leader-influencer-follower (LIF) dynamic hierarchical network is developed. Second, we propose a double-mechanism framework in which a synchronous asymmetric Deffuant–Weisbuch model with opinion memory effect (MSADW) and a continuous opinion and discrete action (CODA) model with action memory effect (MCODA) are built, achieving the co-evolution of the agent’s attribute and the social network with acquaintances and strangers, respectively. Finally, the experimental results show that the memory effect can effectively weaken the bounded confidence rule and behavioral preference, accelerating the consensus of group opinions or actions. In addition, all agents attempt to obtain a higher out-degree during the co-evolution process to realize the hierarchical transition. The fitted hierarchical partition functions provide a basis for adjusting social structure from the inverted T-shaped to the pyramid-shaped structure and finally to the olive-shaped structure, which has insightful social interpretations.},
  archive      = {J_ASOC},
  author       = {Jianglin Dong and Jiangping Hu and Yiyi Zhao and Yuan Peng},
  doi          = {10.1016/j.asoc.2024.111583},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111583},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Opinion formation over dynamic hierarchical networks with acquaintances and strangers: A genetic variation based double-mechanism framework},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Integral reinforcement learning-based angular acceleration
autopilot for high dynamic flight vehicles. <em>ASOC</em>, <em>158</em>,
111582. (<a href="https://doi.org/10.1016/j.asoc.2024.111582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the synthesis of acceleration autopilots for high dynamic flight vehicles (HDFV), autopilots with feedback of angular acceleration (AFAA) have become more perspective with stringent requirements on response speed and high maneuverability, compared with autopilots with feedback of angular rate (AFAR). Integral reinforcement learning (IRL) method has now proved to be an effective technique for adaptive optimal control of partially unknown nonlinear systems. In this paper, a novel data-driven IRL algorithm with “actor–critic” structure is proposed for HDFV utilizing AFAA. As an advanced model-free approach, “actor–critic” based IRL algorithm learns optimal behaviors by observing the real-time responses from the environment under the action of nonoptimal control policies. Instead of solving algebraic Riccati equation directly, the control policy updates online via the solution of proposed IRL Bellman equation with sensed quantities. Numerical simulation is carried out to validate the effectiveness of proposed online IRL-based angular acceleration autopilot for HDFVs. Besides, the tracking performance under different wave commands, the robustness against parameter uncertainties and the noise attenuation capacity between classical optimal tracking approach and proposed IRL method are analyzed for AFAR and AFAA, respectively. Simulation results show that, angular acceleration autopilot with proposed integral RL algorithm possesses better tracking performance against various disturbances.},
  archive      = {J_ASOC},
  author       = {Yingxin Liu and Yuhui Hu and Kai Shen and Jiatai Qiu and Konstantin A. Neusypin},
  doi          = {10.1016/j.asoc.2024.111582},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111582},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integral reinforcement learning-based angular acceleration autopilot for high dynamic flight vehicles},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal design of adaptive model predictive control based on
improved GWO for autonomous vehicle considering system vision
uncertainty. <em>ASOC</em>, <em>158</em>, 111581. (<a
href="https://doi.org/10.1016/j.asoc.2024.111581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tuning issue of the parameters and the system uncertainty represent big challenges in most engineering applications. In this regard, a new tuning approach is developed for adaptive model predictive control (AMPC) of autonomous vehicles (AVs). The proposed control strategy tackles the uncertainty issue of the vision system due to the variation of the velocity, road curvature, time delay, and look ahead distance. The proposed AMPC is designed depending on a new improved grey wolf optimizer (IGWO) algorithm with different learning procedures and fitness-distance balance (FDB) approach to equilibrium the exploration and exploitation manners of the original GWO. This development of GWO based on FDB can increase the capability of the GWO for global search and avoid trapping in local states. Furthermore, the proposed IGWO does not require more adjustable parameters that can enhance the convergence to the best global solution rapidly. The new IGWO is compared with recent optimization algorithms such as the mayfly optimization algorithm (MA), chimp optimization algorithm (COA), dynamic arithmetic optimization algorithm (DAOA), whale optimization algorithm (WOA), jaya algorithm (JA), archimedes optimization algorithm (AOA), and equilibrium optimizer (EO) as well as the original GWO. Besides, the performance of the designed AMPC is compared with the fuzzy logic proportional integral (FLPI) controller and the traditional MPC. Various test scenarios are performed to assert the robustness and effectiveness of the designed AMPC against the variation of the velocity, road curvature, time delay, and look ahead distance. The results emphasize that the proposed AMPC can perform the best damping response with an overshoot of around 1.3 m and a settling time of around 1.12 s less than the FLPI controller and the traditional MPC.},
  archive      = {J_ASOC},
  author       = {Mahmoud Elsisi},
  doi          = {10.1016/j.asoc.2024.111581},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111581},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal design of adaptive model predictive control based on improved GWO for autonomous vehicle considering system vision uncertainty},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bearing fault diagnosis using gradual conditional domain
adversarial network. <em>ASOC</em>, <em>158</em>, 111580. (<a
href="https://doi.org/10.1016/j.asoc.2024.111580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the limited availability of accurately labeled data in fault diagnosis across various industrial scenarios, we proposed a Gradual Conditional Domain Adversarial Network (GCDAN) incorporating various fault categories and rotating speeds. We constructed a prototype system for collecting three-dimensional vibration data samples and modified the network structure to accommodate the input. Inspired by the generalization capability of cross-device scenarios, we adopted CDAN as the main component. To overcome the performance degradation caused by the source and target domains with substantial distribution differences, we introduced Gradual Domain Adaptation into our algorithm. Unlabeled data samples obtained from the intermediate domains were used to train a sequence of CDANs. Experimental comparison results confirmed the effectiveness of the 3-D input data and its network alteration. Additionally, GCDAN performed better over challenging transfer tasks compared to the existing state-of-art algorithms in terms of prediction accuracy and multi-class classification metrics.},
  archive      = {J_ASOC},
  author       = {Chu-ge Wu and Duo Zhao and Te Han and Yuanqing Xia},
  doi          = {10.1016/j.asoc.2024.111580},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111580},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bearing fault diagnosis using gradual conditional domain adversarial network},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing public transport system using biased random-key
genetic algorithm. <em>ASOC</em>, <em>158</em>, 111578. (<a
href="https://doi.org/10.1016/j.asoc.2024.111578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning the public transportation system of a city is a complex process that depends on various factors, including transportation modes, origin–destination demands, service quality and reliability, and operational costs. The vehicle frequency setting (FS) problem is a particularly challenging aspect of this planning process. This work proposes a novel methodology, based on biased random-key genetic algorithms (BRKGA), for optimizing the FS of a bus-based public transport system. The proposed approach considers two optimization models that aim to address the following key metrics: (i) passengers’ waiting time, and (ii) the operational cost for the concessionaire company, specifically the distance covered by buses. We apply our BRKGA methodology to a real case study using bus transport data from the city of Maceió (AL, Brazil). Our results demonstrate that, for each metric, the proposed methodology improves the performance of the city’s public transport system by over 10%, compared to the current configuration.},
  archive      = {J_ASOC},
  author       = {João Luiz Alves Oliveira and Andre L.L. Aquino and Rian G.S. Pinheiro and Bruno Nogueira},
  doi          = {10.1016/j.asoc.2024.111578},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111578},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing public transport system using biased random-key genetic algorithm},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structure-aware preserving projections with applications to
medical image clustering. <em>ASOC</em>, <em>158</em>, 111576. (<a
href="https://doi.org/10.1016/j.asoc.2024.111576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of dimensionality reduction (DR) in effectively handling high-dimensional data is becoming increasingly prominent. However, mainstream methods in projection learning exhibit a reliance on either global or local structures, resulting in the inability to effectively learn the discriminative projection. To address this issue, we propose a novel unsupervised DR algorithm, called Structure-Aware Preserving Projections (SAPP). Specifically, SAPP exploits a powerful combination of subspace clustering and graph construction to effectively capture the intrinsic structure of the data. It is able to find discriminative projections, which is mainly attributed to two key factors: (1) can capture the local structure of the data by constructing the nearest neighbor graph and introducing the grouping effect of the representation into projection learning; and (2) preserves the global structure and respects the complex relationship between data points by integrating least squares regression into the dimension reduction process. Moreover, SAPP proves to be a versatile framework that is easily extended to semi-supervised scenarios. Extensive experiments on medical image datasets confirm the effectiveness of our proposed method, showcasing superior clustering accuracy compared to state-of-the-art approaches.},
  archive      = {J_ASOC},
  author       = {Keyang Yu and Yike Zhu and Xuesong Yin and Ting Shu and Yigang Wang and Enliang Hu},
  doi          = {10.1016/j.asoc.2024.111576},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111576},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Structure-aware preserving projections with applications to medical image clustering},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Exposing the chimp optimization algorithm: A misleading
metaheuristic technique with structural bias. <em>ASOC</em>,
<em>158</em>, 111574. (<a
href="https://doi.org/10.1016/j.asoc.2024.111574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct a comprehensive, component-based analysis of the highly cited chimp optimization algorithm (ChOA). Aside from disassembling ChOA into its constituent elements and establishing connections with analogous components in existing methodologies like particle swarm optimization, we scrutinize the application of metaphors that inspired ChOA. Our findings reveal that the idea introduced in ChOA has long been present in the metaheuristic literature, and the purported novelty in the algorithm primarily stems from the adoption of distinct terminologies based on new metaphors. Furthermore, experiments on the CEC2021 benchmark set and several standard benchmark functions involving their shifted versions reveal a structural bias in ChOA, which results in artificially high accuracy when fitting standard test functions but leads to weak performance when applied to shifted benchmarks. Finally, a critical mathematical analysis is provided to elucidate the fundamental reason behind the manifestation of this bias.},
  archive      = {J_ASOC},
  author       = {Lingyun Deng and Sanyang Liu},
  doi          = {10.1016/j.asoc.2024.111574},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111574},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Exposing the chimp optimization algorithm: A misleading metaheuristic technique with structural bias},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified jaya optimization and TOPSIS for determining the
optimal frequency in LF-HVac. <em>ASOC</em>, <em>158</em>, 111573. (<a
href="https://doi.org/10.1016/j.asoc.2024.111573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel approach of hybrid modified Jaya and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) optimization (MJTO) for efficiently determining the optimal frequency of low-frequency HVac lines. The proposed technique takes frequency constraints and static synchronous compensator (STATCOM) equations into the account. The objective functions in this study are the minimization of power losses, voltage deviation, and voltage stability index of transmission system. This study examines two types of objective functions: the traditional quadratic function and the enhanced quadratic function. These functions are used to incorporate more accurate modeling that considers low frequency. The MJTO is employed for global searching and gradient-free optimization to determine optimal control settings. The Modified Newton-Raphson method is utilized to minimize the mismatch power between power flow (PF) and STATCOM equations. The proposed method utilizes a hybrid technique to handle inequality constraints by preserving feasible solutions while keeping the original objective function intact. The suggested methodology is successfully implemented in extensive benchmark testing of IEEE systems on the 39 Bus and 14 Bus. The optimal solutions obtained are validated by comparing them to recent algorithms as mentioned in literature. The analytical and Statistical results indicates that the proposed optimization strategy is more optimistic for the LF-HVac system.},
  archive      = {J_ASOC},
  author       = {Mukul Anand and Swapan Kumar Goswami and Debashis Chatterjee and Anagha Bhattacharya and Md. Jalil Piran},
  doi          = {10.1016/j.asoc.2024.111573},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111573},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified jaya optimization and TOPSIS for determining the optimal frequency in LF-HVac},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum consensus model with individual tolerance and mixed
DEA prospect cross-efficiency for multi-attribute group decision-making.
<em>ASOC</em>, <em>158</em>, 111572. (<a
href="https://doi.org/10.1016/j.asoc.2024.111572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experts are mostly not completely rational, and they generally exhibit different psychological attitudes and behaviors in multi-attribute group decision-making (MAGDM), which may lead to actual decisions deviating from the results obtained by traditional objective methods. Therefore, it is reasonable and necessary to capture individual tolerance and risk attitudes of experts when dealing with MAGDM problems. To achieve this intention, we design a MAGDM method that combines the maximum consensus model with individual tolerance and mixed DEA prospect cross-efficiency. In the consensus reaching process (CRP), we devise a maximum consensus model with individual tolerance, which can achieve a high consensus level with limited expert tolerance. In the selection process, we first define the prospect aggressive and benevolent cross-efficiency functions. Then, a mixed DEA prospect cross-efficiency ranking method is proposed, which reflects the attitudes of DMs (decision-makers) towards losses and gains. Moreover, this method avoids DMs’ hesitation and trouble in choosing between aggressive and benevolent DEA models. Subsequently, the algorithm and framework for the MAGDM method based on individual tolerance and risk attitude are proposed. Finally, the applicability of our method can be validated by an illustrative example. Sensitivity analysis and comparative analysis are supplied to indicate the rationality and superiority of our developed method.},
  archive      = {J_ASOC},
  author       = {Huayou Chen and Longlong Shao and Ligang Zhou and Jinpei Liu},
  doi          = {10.1016/j.asoc.2024.111572},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111572},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Maximum consensus model with individual tolerance and mixed DEA prospect cross-efficiency for multi-attribute group decision-making},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ranking products through online opinions: A text analysis
and regret theory-based approach. <em>ASOC</em>, <em>158</em>, 111571.
(<a href="https://doi.org/10.1016/j.asoc.2024.111571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of e-shopping, a list of similar products can be found with a large volume of valuable customer reviews online. However, it is generally difficult to compare various aspects of similar products effectively by understanding all relevant online opinions. To help consumers, in this study, how products are ranked according to online reviews is investigated. Firstly, an SC-LDA (Seed Constraint-Latent Dirichlet Allocation) model, which is an extension of the classical topic model LDA (Latent Dirichlet Allocation), is proposed to extract product features. The must-link and cannot-link seed constraints are invited to estimate the probability expansion/reduction value. They help to affect the topic allocation by additional constraints in Gibbs sampling for a higher accuracy on feature extraction. Secondly, an improved convolutional memory neural network model is devised to analyze the sentiment polarity. It takes the advantages of CNN (convolutional neural network) and Bi-LSTM (bidirectional Long Short-Term Memory) and performs dynamic pooling in CNN to prevent the loss of important features. Besides, the concept of group satisfaction degree is introduced, which makes products be compared according to the Regret Theory. It ranks products without a commonly applied reference point and take consumer psychology into considerations. Finally, in the case study, an illustrative example is presented to evaluate the proposed framework. Categories of experiments show that the proposed framework provides consumers with effective purchase suggestions. Permanent link to reproducible Capsule: https://doi.org/10.24433/CO.6445683.v1 and https://doi.org/10.24433/CO.2658577.v1 .},
  archive      = {J_ASOC},
  author       = {Kejia Chen and Jingjing Zheng and Jian Jin},
  doi          = {10.1016/j.asoc.2024.111571},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111571},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ranking products through online opinions: A text analysis and regret theory-based approach},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk measurement of aggregation approaches in multiple
attribute decision making under uncertain information. <em>ASOC</em>,
<em>158</em>, 111568. (<a
href="https://doi.org/10.1016/j.asoc.2024.111568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision-maker&#39;s judgment deviates from uncertain attribute information will lead to decision risk in multiple attribute decision making (MADM), and different aggregation approaches result in different risk levels. This paper aims to study the risk levels of aggregation operators in MADM with uncertain attribute information. We use the signal detection theory to characterize the decision-maker&#39;s noisy perceptions of multiple attributes to present his/her judgment deviation. Then, we establish the aggregation models to aggregate these perceptions based on the weighted averaging (WA) and the ordered weighted averaging (OWA) operators. Furthermore, a risk measurement model of each aggregation approach is constructed to measure the risk levels of the commission risk (CR), the omission risk (OR), and the overall risk. A numerical example is used to verify the validity of the proposed model, while simulation experiments are designed to compare the risk levels of the WA and OWA operators. The results reveal that the overall risk level of the WA operator is higher than that of the OWA operator when judging the quality of the alternative with high standards; otherwise, the WA operator is lower. This finding provides a scientific reference for aggregation approach selection under uncertain information. Permanent link to reproducible Capsule: https://doi.org/10.24433/CO.0911610.v1 .},
  archive      = {J_ASOC},
  author       = {Jiajia Jiang and Gaocan Gong and Lin Wang and Quanbo Zha},
  doi          = {10.1016/j.asoc.2024.111568},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111568},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk measurement of aggregation approaches in multiple attribute decision making under uncertain information},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Directional optimization of elevator scheduling algorithms
in complex traffic patterns. <em>ASOC</em>, <em>158</em>, 111567. (<a
href="https://doi.org/10.1016/j.asoc.2024.111567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elevator systems in buildings face challenges due to unpredictable passenger flow, which can make scheduling elevators complicated to optimize their operation. Most of the existing algorithms are developed based on pattern recognition and may not be effective in scenarios where patterns are difficult to classify, especially when elevator operations involve uncertain human behaviors. To address this issue, this paper proposes a time-dependent optimization model that considers long-term and multi-floor peaks, with a focus on peak floors. The proposed model accounts for dynamic scheduling patterns of elevators and represents passenger flow direction as a relation matrix. The proposed direction optimization method contains several functions to guide iteration direction and improve the efficiency of an iteration process based on classical algorithms. This method also ensures the stability of Markov chains by adjusting an iteration process. The feasibility of the proposed method is supported by the relevant theory, and experimental results show that the direction-optimized algorithms outperform classical algorithms, resulting in the superb operating efficiency of elevators at a lower cost. This paper contributes to the development of efficient algorithms for scheduling elevators in complex traffic patterns, which can improve performance of elevator group systems in buildings. The proposed method is not only limited to elevators but can also be extended to other transportation systems with flow requirements.},
  archive      = {J_ASOC},
  author       = {Yu Wu and Jianjun Yang},
  doi          = {10.1016/j.asoc.2024.111567},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111567},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Directional optimization of elevator scheduling algorithms in complex traffic patterns},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic metaheuristic selection via thompson sampling for
online optimization. <em>ASOC</em>, <em>158</em>, 111566. (<a
href="https://doi.org/10.1016/j.asoc.2024.111566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is acknowledged that no single heuristic can outperform all the others in every optimization problem. This has given rise to hyper-heuristic methods for providing solutions to a wider range of problems. In this work, a set of five non-competing low-level heuristics is proposed in a hyper-heuristic framework. The multi-armed bandit problem analogy is efficiently leveraged and Thompson Sampling is used to actively select the best heuristic for online optimization. The proposed method is compared against ten population-based metaheuristic algorithms on the well-known CEC’05 optimizing benchmark consisting of 23 functions of various landscapes. The results show that the proposed algorithm is the only one able to find the global minimum of all functions with remarkable consistency.},
  archive      = {J_ASOC},
  author       = {Alain Nguyen},
  doi          = {10.1016/j.asoc.2024.111566},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111566},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic metaheuristic selection via thompson sampling for online optimization},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Social-aware graph contrastive learning for recommender
systems. <em>ASOC</em>, <em>158</em>, 111558. (<a
href="https://doi.org/10.1016/j.asoc.2024.111558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems usually encounter the issue of sparse interaction data, which is commonly alleviated by social recommendation models based on graph neural networks. However, these models overlook the collaborative similarity relationship among items and fail to effectively integrate and process various graph structures. To address these issues, we propose a novel S ocial-aware G raph C ontrastive L earning R ecommendation model (SG-CLR). Specifically, we initially utilize data augmentation techniques to obtain different augmented views of user–item interaction. Secondly, a social-aware encoder is put forward to effectively capture both the influence diffusing within the social network and the attractiveness of items among the item collaborative similarity graph. Finally, we employ graph contrastive learning to maximize the consistency of node representation across different augmented views, and further focus on domain-shared information through joint training. Experimental results conducted on two real-world datasets demonstrate that the proposed SG-CLR outperforms the state-of-the-art baselines. Compared to the best baseline, SG-CLR improves the performance on the two datasets by 3.069% and 2.972%, respectively.},
  archive      = {J_ASOC},
  author       = {Yuanyuan Zhang and Junwu Zhu and Yonglong Zhang and Yi Zhu and Jialuo Zhou and Yaling Xie},
  doi          = {10.1016/j.asoc.2024.111558},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111558},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Social-aware graph contrastive learning for recommender systems},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hidden markov guided deep learning models for forecasting
highly volatile agricultural commodity prices. <em>ASOC</em>,
<em>158</em>, 111557. (<a
href="https://doi.org/10.1016/j.asoc.2024.111557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting agricultural commodity prices accurately is of utmost importance due to various factors such as perishability, seasonality, production uncertainty etc . Moreover, the substantial volatility that may be exhibited in time series further adds to the complexity and constitutes a significant challenge. In this paper, a Hidden Markov (HM) guided Deep Learning (DL) models has been developed on nonlinear and nonstationary price data of agricultural commodities for forecasting by considering technical indicators viz ., Moving Average (MA), Bollinger Bands (BB), Moving Average Convergence Divergence (MACD), Exponential MA (EMA) and Fast Fourier Transformation (FFT). HM Models (HMMs) can effectively handle the sequential dependencies and hidden states, while DL approach can learn complex patterns and relationships within the price series and thus the drawback of lack of generalization capability in the DL model has been overcome by HMM. In this study, the Potato price data of the Champadanga district of West Bengal, India has been utilized to assess the performance of the proposed technique. HMM has been combined with six baseline DL models viz ., Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), Bidirectional LSTM (BiLSTM) and Bidirectional GRU (BiGRU) for forecast modeling. Performance evaluation metrics viz ., Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE), Mean Absolute Error (MAE) and the insightful Diebold–Mariano (DM) test revealed that Hidden Markov hybridized with DL models surpassed baseline DL models in forecasting accuracy for 1-week, 4-week, 8-week and 12-week ahead DL predictions. The proposed approach holds significant promise for enhancing the precision of agricultural commodity price forecasting with far-reaching implications for various stakeholders such as farmers and planners.},
  archive      = {J_ASOC},
  author       = {G. Avinash and V. Ramasubramanian and Mrinmoy Ray and Ranjit Kumar Paul and Samarth Godara and G.H. Harish Nayak and Rajeev Ranjan Kumar and B. Manjunatha and Shashi Dahiya and Mir Asif Iquebal},
  doi          = {10.1016/j.asoc.2024.111557},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111557},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hidden markov guided deep learning models for forecasting highly volatile agricultural commodity prices},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effective combining source code and opcode for accurate
vulnerability detection of smart contracts in edge AI systems.
<em>ASOC</em>, <em>158</em>, 111556. (<a
href="https://doi.org/10.1016/j.asoc.2024.111556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automating transactions using smart contracts extends the functionality of blockchains and secures the decentralization of blockchains in edge AI systems. Whereas, since plenty of smart contracts are deployed to support various decentralized edge applications, the security vulnerabilities of smart contracts will lead to huge irreversible losses. To deal with this problem, many deep learning-based methods have been developed for vulnerability detection. However, most existing methods use only contract source codes for feature extraction, resulting in low accuracy. In contrast, we propose a method based on deep learning model to integrate both the features of contract source codes and opcodes for vulnerability detection. Particularly, the contextual features are extracted based on opcodes while the expert pattern features are extracted from the source codes. Using the real-world dataset of Ethereum smart contracts targeting reentrancy vulnerability, experiment results demonstrate that our method outperforms the state-of-the-art methods and achieves 96.89% accuracy and 95.41% F1-Score.},
  archive      = {J_ASOC},
  author       = {Huakun Huang and Longtao Guo and Lingjun Zhao and Haoda Wang and Chenkai Xu and Shan Jiang},
  doi          = {10.1016/j.asoc.2024.111556},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111556},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effective combining source code and opcode for accurate vulnerability detection of smart contracts in edge AI systems},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling of limit order book data with ordered fuzzy
numbers. <em>ASOC</em>, <em>158</em>, 111555. (<a
href="https://doi.org/10.1016/j.asoc.2024.111555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach to representing the Limit Order Book data at a given timestamp using the Ordered Fuzzy Numbers concept. The limit order book contains all buy and sell orders placed by investors, updated in real-time, for the most liquid securities, even several hundred times a minute. Due to its irregular nature (different and dynamic changes in the number of buy and sell orders), direct calculations on the order book data are not feasible without transforming it into feature vectors. Currently, most studies use a price level-based data representation scheme when applying deep learning models on limit order book data. However, this scheme has limitations, particularly its sensitivity to subtle perturbations that can negatively impact model performance. On the other hand, the ordered fuzzy number is a mathematical object (a pair of two functions) used to process imprecise and uncertain data. Ordered Fuzzy Numbers possess well-defined arithmetic properties. Converting the limit order book data to ordered fuzzy numbers allows the creation of a time series of ordered fuzzy numbers (order books) and use them for further calculations, e.g., to represent input data for deep learning models or employing the concept of fuzzy time series in various domains, such as defining liquidity measures based on limit order book data. In this paper, the proposed approach is tested using one-year market data from the Polish Stock Exchange for the five biggest companies. The DeepLOB model is employed to predict mid-price movement using different input data representations. The proposed representation of Limit Order Book data demonstrated remarkably stable out-of-sample prediction accuracy, even when subjected to data perturbation.},
  archive      = {J_ASOC},
  author       = {Adam Marszałek and Tadeusz Burczyński},
  doi          = {10.1016/j.asoc.2024.111555},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111555},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling of limit order book data with ordered fuzzy numbers},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble classifiers using multi-objective genetic
programming for unbalanced data. <em>ASOC</em>, <em>158</em>, 111554.
(<a href="https://doi.org/10.1016/j.asoc.2024.111554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic Programming (GP) can be used to design effective classifiers due to its built-in feature selection and feature construction characteristics. Unbalanced data distributions affect the classification performance of GP classifiers. Some fitness functions have been proposed to solve the class imbalance problem of GP classifiers. However, with the evolution of GP, single-objective GP classifiers evaluated by a single fitness function have poor generalization ability. Moreover, using the best evolved GP classifier for decision-making can easily lead to the possibility of misclassification. In this paper, multi-objective GP is used to optimize multiple fitness functions including AUC approximation (Wmw), Distance (Dist), and Complexity to evolve ensemble classifiers, which jointly determines the class labels of unknown instances. Experiments on sixteen datasets show that our multi-objective GP can significantly improve classification performance compared with single-objective GP, and our proposed ensemble classifiers evolved by multi-objective GP can further improve the classification performance than the single best GP classifier. Comparisons with six GP-based and five traditional machine learning algorithms show that our proposed approaches can achieve significantly better classification performance on most cases.},
  archive      = {J_ASOC},
  author       = {Wenyang Meng and Ying Li and Xiaoying Gao and Jianbin Ma},
  doi          = {10.1016/j.asoc.2024.111554},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111554},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble classifiers using multi-objective genetic programming for unbalanced data},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing investment portfolios with a sequential ensemble
of decision tree-based models and the FBI algorithm for efficient
financial analysis. <em>ASOC</em>, <em>158</em>, 111550. (<a
href="https://doi.org/10.1016/j.asoc.2024.111550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research presents a comprehensive, sequential ensemble framework meticulously crafted for optimizing investment portfolios, focusing on the construction industry. It employs decision tree-based and metaheuristic optimization algorithms to create an efficient stock-selection framework grounded in financial analysis. This approach offers a profitable investment strategy integrated with portfolio optimization while systematically lowering the portfolio investment threshold. In this framework, a web crawler was deployed to gather daily closing prices of stocks from the Taiwan Stock Exchange, along with monthly revenue data and financial statements. Subsequently, decision tree-based models were utilized to pinpoint the fundamental financial indicators with significant explanatory power over revenues. The stock selection conditions aligned with these indicators were optimized through a newly developed metaheuristic algorithm named the forensic-based investigation (FBI) optimizer. The optimal conditions were subsequently integrated with the equal-weighting scheme, mean-variance method, and hierarchical risk parity to identify the most effective investment portfolio strategies. Backtesting results showed that the proposed stock-portfolio investment strategy, optimized through machine learning and a metaheuristic algorithm, is well-suited for construction and all-stock categories. This study equips professional investment advisors or securities investment institutions with a decision-aid expert system for initial stock selection.},
  archive      = {J_ASOC},
  author       = {Jui-Sheng Chou and Ke-En Chen},
  doi          = {10.1016/j.asoc.2024.111550},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111550},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing investment portfolios with a sequential ensemble of decision tree-based models and the FBI algorithm for efficient financial analysis},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmented support vector regression with an autoregressive
process via an iterative procedure. <em>ASOC</em>, <em>158</em>, 111549.
(<a href="https://doi.org/10.1016/j.asoc.2024.111549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Support Vector Regression (SVR) technique can approximate intricate systems by addressing learning and estimation challenges within a reproducing kernel Hilbert space, devoid of reliance on specific parameter assumptions. However, when dealing with correlated data like time series, the SVR method often falls short in accounting for underlying temporal structures, leading to limited enhancements in prediction efficiency. We introduce an enhanced SVR method that considers temporal correlations (TemporalSVR) to overcome this constraint. Our proposed method extends kernel functions to include additional linear kernels, facilitating learning temporal patterns. Additionally, we develope an iterative training procedure for the augmented regression model. During model training, we estimate the hyper-parameter in the corresponding loss function using a ‘working’ likelihood approach, enhancing the generalization capabilities of the proposed regression. To demonstrate superior forecasting performance, we conduct extensive numerical simulations on both linear and nonlinear systems and the TemporalSVR achieves improvements ranging from 8% to 114% based on the RMSE ratio from the AR-X model. Furthermore, we investigate the forecasting performance of three basic models (NARX-NN, Statistical SVR, and SVR-ARIMA) and four deep learning (DL) techniques (Transformer, Informer, Reformer, Autormer, and Autoformer) by using a WTI forecasting study. Our proposed TemporalSVR achieves the smallest RMSE at 2.22 and attains the highest success ratio of stock direction prediction at 71.30%. All these numerical results highlight the effectiveness and advantages of our TemporalSVR in handling temporal data and making accurate predictions.},
  archive      = {J_ASOC},
  author       = {Jinran Wu and You-Gan Wang and Hao Zhang},
  doi          = {10.1016/j.asoc.2024.111549},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111549},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Augmented support vector regression with an autoregressive process via an iterative procedure},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A solution to multi objective stochastic optimal power flow
problem using mutualism and elite strategy based pelican optimization
algorithm. <em>ASOC</em>, <em>158</em>, 111548. (<a
href="https://doi.org/10.1016/j.asoc.2024.111548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind Energy is rapidly growing into one of the most popular energy options for power generation. However, the unpredictability of wind necessitates the employment of a distribution function when describing the wind environment. In the depiction of wind speeds, the two significant parameters of the Weibull distribution, such as the scale parameter and the shape parameter are often utilized. Therefore, it is essential to choose the most appropriate approach for estimating these parameters. Furthermore, the uncertainty in delegated power from variable wind speeds makes system management challenging. In this paper, the Weibull distribution parameters were determined using the Pelican Optimization Algorithm (POA). POA has gained extensive popularity among the research community and is currently used to address a wide range of optimization problems. However, the exploration and exploitation are not properly balanced in this algorithm. To overcome this problem a novel hybrid algorithm namely Mutualism phase based POA (MPOA) is proposed with the help of mutualism phase of Symbiotic Organisms Search (SOS) algorithm. To demonstrate the efficacy and robustness MPOA is applied on multi-objective Optimal Power Flow (MO-OPF) problems. For this study IEEE 30 bus standard system and modified IEEE 30 bus system consisting of two wind farms are taken. The purpose of this work is to examine the impact of changing the Weibull parameters, reverse and penalty cost coefficients on the overall generation cost while considering all the security aspects.},
  archive      = {J_ASOC},
  author       = {Bimal Kumar Dora and Sunil Bhat and Sudip Halder and Ishan Srivastava},
  doi          = {10.1016/j.asoc.2024.111548},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111548},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A solution to multi objective stochastic optimal power flow problem using mutualism and elite strategy based pelican optimization algorithm},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A binary sparrow search algorithm for feature selection on
classification of x-ray security images. <em>ASOC</em>, <em>158</em>,
111546. (<a href="https://doi.org/10.1016/j.asoc.2024.111546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today&#39;s world, especially in public places, strict security measures are being implemented. Among these measures, the most common is the inspection of the contents of people&#39;s belongings, such as purses, knapsacks, and suitcases, through X-ray imaging to detect prohibited items. However, this process is typically performed manually by security personnel. It is an exhausting task that demands continuous attention and concentration, making it prone to errors. Additionally, the detection and classification of overlapping and occluded objects can be challenging. Therefore, automating this process can be highly beneficial for reducing errors and improving the overall efficiency. In this study, a framework consisting of three fundamental phases for the classification of prohibited objects was proposed. In the first phase, a deep neural network was trained using X-ray images to extract features. In the subsequent phase, features that best represent the object were selected. Feature selection helps eliminate redundant features, leading to the efficient use of memory, reduced computational costs, and improved classification accuracy owing to a decrease in the number of features. In the final phase, classification was performed using the selected features. In the first stage, a convolutional neural network model was utilized for feature extraction. In the second stage, the Sparrow Search Algorithm was binarized and proposed as the binISSA for feature selection. Feature selection was implemented using the proposed binISSA. In the final stage, classification was performed using the K-Nearest Neighbors (KNN) and Support Vector Machine (SVM) algorithms. The performances of the convolutional neural network and the proposed framework were compared. In addition, the performance of the proposed framework was compared with that of other state-of-the-art meta-heuristic algorithms. The proposed method increased the classification accuracy of the network from 0.9702 to 0.9763 using both the KNN and SVM (linear kernel) classifiers. The total number of features extracted using the deep neural network was 512. With the application of the proposed binISSA, average number of features were reduced to 25.33 using the KNN classifier and 32.70 using the SVM classifier. The results indicate a notable reduction in the extracted features from the convolutional neural network and an improvement in the classification accuracy.},
  archive      = {J_ASOC},
  author       = {Ahmet Babalik and Aybuke Babadag},
  doi          = {10.1016/j.asoc.2024.111546},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111546},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A binary sparrow search algorithm for feature selection on classification of X-ray security images},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved learning efficiency of deep monte-carlo for complex
imperfect-information card games. <em>ASOC</em>, <em>158</em>, 111545.
(<a href="https://doi.org/10.1016/j.asoc.2024.111545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL) has achieved considerable success in games involving perfect and imperfect information, such as Go, Texas Hold’em, Stratego, and DouDiZhu. Nevertheless, training a state-of-the-art model for complex imperfect-information card games like DouDiZhu and Big2 remains resource and time-intensive. To address this challenge, this paper introduces two innovative methods: the Opponent Model and Optimized Deep Monte-Carlo (ODMC). These methods are designed to improve the training efficiency of Deep Monte-Carlo (DMC) for imperfect-information card games. The Opponent Model predicts hidden information, enhancing the agent’s learning speed in DMC compared to the original training that only utilizes observed information as input features. In ODMC, the Minimum Combination Search (MCS) is a heuristic search algorithm based on dynamic programming. It calculates the minimum combination of actions in the current state, and ODMC uses MCS to filter suboptimal actions in each state. This reduces the action space considered by DMC, resulting in faster training that focuses on evaluating the most promising actions. The effectiveness of the proposed approach is evaluated by examining two complex card games with imperfect information: DouDiZhu and Big2. Ablation experiments are conducted to evaluate both the Opponent Model (D+OM and B+OM) and ODMC (D+ODMC and B+ODMC), along with their combined variants (D+OMODMC and B+OMODMC). Furthermore, D+OMODMC and B+OMODMC are compared with state-of-the-art DouDiZhu and Big2 artificial intelligence (AI) programs, respectively. The experimental results demonstrate that the proposed methods achieve comparable performance to the original DMC, but with only 25.5% of the training time on the same device. These findings are valuable for mitigating both the equipment requirements and training time in complex imperfect-information card games.},
  archive      = {J_ASOC},
  author       = {Qian Luo and Tien-Ping Tan},
  doi          = {10.1016/j.asoc.2024.111545},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111545},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved learning efficiency of deep monte-carlo for complex imperfect-information card games},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel network for semantic segmentation of landslide areas
in remote sensing images with multi-branch and multi-scale fusion.
<em>ASOC</em>, <em>158</em>, 111542. (<a
href="https://doi.org/10.1016/j.asoc.2024.111542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Landslides pose significant risks as natural disasters, highlighting the importance of accurate mapping using remote sensing images for various practical applications. However, due to the challenges arising from incomplete and inaccurate boundary information of foreground landslide polygons, existing methods can only achieve suboptimal performance. To this premise, in this paper, we propose a segmentation network called GMNet that leverages global information extraction and multi-scale feature fusion to enhance the discrimination of landslides from other objects. Specifically, by employing a multi-branch mechanism, our method effectively captures global information, while an improved multi-scale feature fusion technique addresses the issue of varying scales in landslide polygons. Furthermore, semantic enhancement enhances the semantic information of low-level features, bridging the semantic gap and enhancing fusion efficacy. Experimental results demonstrate the effectiveness of our network in segmenting landslide areas accurately within the remote sensing image dataset. Especially, our F1_scores on three benchmarks outperform existing runner-ups by notable margins of 4.81%, 1.72%, and 1.16%, showcasing the value of our method in this domain.},
  archive      = {J_ASOC},
  author       = {Kai Wang and Daojie He and Qingqiang Sun and Lizhi Yi and Xiaofeng Yuan and Yalin Wang},
  doi          = {10.1016/j.asoc.2024.111542},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111542},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel network for semantic segmentation of landslide areas in remote sensing images with multi-branch and multi-scale fusion},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wireless sensor networks-based adaptive differential
evolution for multimodal optimization problems. <em>ASOC</em>,
<em>158</em>, 111541. (<a
href="https://doi.org/10.1016/j.asoc.2024.111541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In wireless sensor networks (WSN), we often detect the monitoring areas among different sensors so that the sensors can be switched on and off adaptively to save energy and extend their lifetime. Inspired by the principle of WSN, a WSN-based adaptive differential evolution (WSNADE) algorithm is proposed in this paper, together with a WSN-based adaptive niching technique (WANT) and two novel strategies called protection-based dual-scale mutation (PDM) strategy and multi-level reset (MLR) strategy, for solving multimodal optimization problems (MMOPs). In WANT, each individual is considered as a sensor with its monitoring area. If the monitoring areas of two individuals intersect, which means these two individuals monitor the similar area and should be partitioned into the same niche. In this way, WANT can adaptively form a niche for each individual, avoiding the sensitivity of niching parameters. Based on WANT, the PDM strategy is designed to select the appropriate mutation strategy for each individual. Besides, to save fitness evaluations (FEs) for exploring more promising areas, the MLR strategy is developed to store the promising individuals and reset the stagnant individuals. The experimental results on 20 multimodal benchmark test functions in CEC2015 multimodal competition show that the proposed WSNADE algorithm generally performs better than or at least comparable with other state-of-the-art multimodal algorithms, including the winner of the CEC2015 competition. Finally, WSNADE is applied to a real-world multimodal application in multiple competitive facilities location design (MCFLD) problem to illustrate its practical applicability.},
  archive      = {J_ASOC},
  author       = {Yi-Biao Huang and Zi-Jia Wang and Yu-Hui Zhang and Yuan-Gen Wang and Sam Kwong and Jun Zhang},
  doi          = {10.1016/j.asoc.2024.111541},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111541},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Wireless sensor networks-based adaptive differential evolution for multimodal optimization problems},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic path planning via dueling double deep q-network
(D3QN) with prioritized experience replay. <em>ASOC</em>, <em>158</em>,
111503. (<a href="https://doi.org/10.1016/j.asoc.2024.111503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path planning is a key requirement for mobile robots employed for different tasks such as rescue or transport missions. Conventional methods such as A* or Dijkstra to tackle path planning problem need a premise map of the robot&#39;s environment. Nowadays, dynamic path planning is a popular research topic, which drives mobile robots without prior static requirements. Deep reinforcement learning (DRL), which is another popular research area, is being harnessed to solve dynamic path planning problem by the researchers. In this study, Deep Q-Networks, which is a subdomain of DRL are opted to solve dynamic path planning problem. We first employ well known techniques Double Deep Q-Networks (D2QN) and Dueling Double Deep Q-Networks (D3QN) to train a model which can drive a mobile robot in environments with static and dynamic obstacles within 3 different configurations. Then we propose D3QN with Prioritized Experience Replay (PER) extension in order to further optimize the DRL model. We created a test bed to measure the performance of the DRL models against 99 randomly generated goal locations. According to our experiments, D3QN-PER method performs better than D2QN and D3QN in terms of path length and travel time to the goal without any collisions. Robot Operating System and Gazebo simulation environment is utilized to realize the training and testing environments, thus, the trained DRL models can be deployed to any ROS compatible robot seamlessly.},
  archive      = {J_ASOC},
  author       = {Mehmet Gök},
  doi          = {10.1016/j.asoc.2024.111503},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111503},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic path planning via dueling double deep Q-network (D3QN) with prioritized experience replay},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A federated recommendation algorithm based on user
clustering and meta-learning. <em>ASOC</em>, <em>158</em>, 111483. (<a
href="https://doi.org/10.1016/j.asoc.2024.111483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommendation is a typical application of federated learning, which can protect the privacy of users by exchanging models between users’ devices and central servers rather than users’ raw data. Recently, although some research in federated recommendation has made remarkable progress, there are still two major issues need to be addressed further due to the non-independent and identical distribution (Non-IID) data which is very common in federal recommendation systems. First, the communication load of the user device during training is heavy. Second, the trained local model lacks personalization. Aiming at the above problems, a federated recommendation algorithm based on user clustering and meta-learning, ClusterFedMet, is proposed to improve communication efficiency and recommendation personalization simultaneously. In ClusterFedMet, users are clustered into different clusters according to their data distribution, and user sampling are performed based on the clustering result, thus reduce harmful interference among users with different data distribution. The model is trained with meta-learning, which can generate more personalized local models. During meta-learning, a controller which can dynamically tune the hyperparameters for users is designed to achieve better performance. According to weights, gradients, and losses of each step, the controller can find a learning rate suitable for each user’s local data and model. We perform evaluations for the proposed algorithm on two public datasets, and the results demonstrate that our algorithm outperforms other advanced methods in terms of recommendation accuracy and communication efficiency.},
  archive      = {J_ASOC},
  author       = {Enqi Yu and Zhiwei Ye and Zhiqiang Zhang and Ling Qian and Meiyi Xie},
  doi          = {10.1016/j.asoc.2024.111483},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111483},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A federated recommendation algorithm based on user clustering and meta-learning},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spherical fuzzy sets based integrated DEMATEL, ANP, VIKOR
approach and its application for renewable energy selection in turkey.
<em>ASOC</em>, <em>158</em>, 111465. (<a
href="https://doi.org/10.1016/j.asoc.2024.111465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewable energy continues occupying the global agenda for the low-carbon transition. Identifying suitable sources for renewable energy involves different criteria and alternatives, which can be defined as a Multi-Criteria Decision-Making (MCDM) problem. MCDM methods can be enriched with Group Decision Making (GDM) to merge individual judgments into group opinions, as well as with fuzzy sets to better simulate human opinions. To handle uncertainties of DMs’ preferences, a recently developed fuzzy set extension, the Spherical Fuzzy Sets (SFSs), is combined in this study with different MCDM tools to establish a decision support tool for the energy source selection problem. This article proposes a novel MCDM approach that integrates SF-DEMATEL (Spherical Fuzzy Decision-Making Trial and Evaluation Laboratory), SF-ANP (Spherical Fuzzy Analytic Network Process), and SF-VIKOR (Spherical Fuzzy Vlse Kriterijumska Optimizacija Kompromisno Resenje) algorithms in a GDM environment and presents its application in a case study for selecting renewable energy sources in Turkey. Comparing wind, geothermal, solar, hydropower, and biogas, the proposed method highlighted both wind and solar energy as a compromise solution as the most suitable options for Turkey. It has been found that compared to similar fuzzy sets, SFS can better reflect the hesitation degree to address the lack of knowledge and errors in the membership function definitions. The robustness and plausibility of these findings are demonstrated with sensitivity and comparative analyses, suggesting this MCDM method can also be applied to similar problems in other settings.},
  archive      = {J_ASOC},
  author       = {Gülçin Büyüközkan and Yağmur Karabulut and Fethullah Göçer},
  doi          = {10.1016/j.asoc.2024.111465},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111465},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spherical fuzzy sets based integrated DEMATEL, ANP, VIKOR approach and its application for renewable energy selection in turkey},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Particle filter based on jaya optimisation for bayesian
updating of nonlinear models. <em>ASOC</em>, <em>158</em>, 111429. (<a
href="https://doi.org/10.1016/j.asoc.2024.111429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle filter (PF) is a powerful and commonly used filtering technique based on Sequential Monte Carlo framework. The main challenge in using PF for nonlinear state and parameter estimation is the degeneracy of particles. Although resampling techniques can solve this to some extent, it would still result in particle impoverishment when a limited number of particles are used thereby affecting the accuracy. Hence, a hybrid metaheuristic optimisation algorithm that combines the PF with Jaya optimisation, (PF-JAYA) has been proposed and implemented for joint state and parameter estimation for geotechnical engineering problems. The performance of PF-JAYA has been compared against the traditional Particle Filter with Sampling Importance Resampling (PF-SIR) technique. The synthetic examples show that PF-JAYA outperforms PF-SIR in terms of accuracy, rate of convergence, parameter identification and particle diversity. Furthermore, the performance of PF-JAYA is independent of the choice of prior distribution and due to its superior convergence proves to be efficient when working with sparse monitoring information. The performance of PF-JAYA on Bayesian updating of state and parameters of an elastoplastic model for a synthetic embankment case has also been evaluated where, along with PF-SIR, the Ensemble Kalman Filter (EnKF) is also chosen for comparison. Finally a further evaluation using the Lorenz ‘63 model, shows the superior performance of PF-JAYA in terms of accuracy and precision over the classical Data Assimilation techniques.},
  archive      = {J_ASOC},
  author       = {Amardeep Amavasai and Jelke Dijkstra},
  doi          = {10.1016/j.asoc.2024.111429},
  journal      = {Applied Soft Computing},
  month        = {6},
  pages        = {111429},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle filter based on jaya optimisation for bayesian updating of nonlinear models},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating temporal multi-head self-attention
convolutional networks and LightGBM for indoor air quality prediction.
<em>ASOC</em>, <em>157</em>, 111569. (<a
href="https://doi.org/10.1016/j.asoc.2024.111569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Friction between subway wheels and tracks and inadequate combustion of fuel are causes of respirable particulate matter. Restricted underground ventilation and high population density make it difficult for particulate matter to dissipate, posing a threat to human health. An effective data-driven model for indoor pollutant prediction can enhance preparedness for high pollution situations. This study introduces a feature extraction method that combines kernel principal component analysis with max-relevance and min-redundancy algorithm. The temporal convolutional network, enhanced with a multi-head self-attention mechanism, adeptly captures time series features and effectively manages attention weight allocation. Additionally, the incorporation of the light gradient boosting machine method enhances overall efficiency. The proposed framework for PM 2.5 concentration prediction was employed to a high-traffic subway station in Seoul. In the test set, the model demonstrated strong performance with evaluation metrics including an R 2 value of 0.92, RMSE of 6.02 μg/m 3 , MAE of 4.36, and MAPE of 20.58 μg/m 3 . Compared to the conventional LSTM, the proposed method reduces the RMSE by 20.5% and the MAPE by 49.05%. Notably, the model we propose demonstrates superior capabilities in managing large datasets and offers enhanced predictive accuracy compared to baseline models. It effectively addresses the limitations observed in models like LSTM, which often struggle with adequately capturing feature information, and overcomes the generalization weaknesses inherent in models such as the Transformer. This advancement significantly boosts the efficiency of environmental monitoring and fosters both automation and intelligence in the analysis of environmental data.},
  archive      = {J_ASOC},
  author       = {Yifeng Lu and Jinyong Wang and Dongsheng Wang and ChangKyoo Yoo and Hongbin Liu},
  doi          = {10.1016/j.asoc.2024.111569},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111569},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incorporating temporal multi-head self-attention convolutional networks and LightGBM for indoor air quality prediction},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentiment analysis on a low-resource language dataset using
multimodal representation learning and cross-lingual transfer learning.
<em>ASOC</em>, <em>157</em>, 111553. (<a
href="https://doi.org/10.1016/j.asoc.2024.111553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affect Sensing is a rapidly growing field with the potential to revolutionize human–computer interaction, healthcare, and many more applications. Multimodal Sentiment Analysis (MSA) is a recent research area that exploits the multimodal nature of video data for affect sensing. However, the success of a multimodal framework depends on addressing the challenges associated with integrating diverse modalities and selecting informative features. We propose a novel multimodal representation learning framework using multimodal autoencoders that learns a comprehensive representation of the underlying heterogeneous modalities. Affect Sensing is even more challenging in low-resource languages because annotated video datasets and language-specific models are limited. To address this concern, we introduce Multimodal Sentiment Analysis Corpus in Tamil (MSAT), a small-sized dataset in the Tamil language for MSA, and exhibit how a novel technique involving cross-lingual transfer learning in a multimodal setting, leverages the knowledge gained by training the model on a larger English MSA dataset to fine-tune a much smaller Tamil MSA dataset. Our transfer learning model achieves significant gain in the Tamil dataset by a large margin. Our experiments demonstrate that we can build efficient, generalized models for low-resource languages by using the existing MSA datasets.},
  archive      = {J_ASOC},
  author       = {Aruna Gladys A. and Vetriselvi V.},
  doi          = {10.1016/j.asoc.2024.111553},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111553},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentiment analysis on a low-resource language dataset using multimodal representation learning and cross-lingual transfer learning},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A particle swarm optimization and prior knowledge fusion
seismic damage prediction of concrete structures. <em>ASOC</em>,
<em>157</em>, 111552. (<a
href="https://doi.org/10.1016/j.asoc.2024.111552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-driven algorithm is developed to predict local seismic damage distribution of concrete structures based on the measured global structural response. Based on the algorithm, the signal of reaction force and displacement at one location is only necessary for seismic damage prediction. The algorithm is established based on the improved particle swarm optimization with two innovative strategies. One is the probabilistic mutation procedure, which can consider the prior knowledge of the positive correlation between the strain/stress level and damage level in the seismic damage optimization process. Another is the dynamic condition-based mutation and cross procedure, which can increase the diversity of the particle swarm in the optimization process to get rid of the possible local optimum. A representative example of a concrete column under cyclic load is designed and modeled to examine the performance of the algorithm. The prediction results based on the algorithm are compared with the traditional particle swarm optimization and the previous damage inversion algorithm based on ant colony optimization. The comparison results support that the local seismic damage distribution prediction based on the algorithm is closer to the corresponding experimental result. In addition, the error of the predicted macroscopic response in the final seismic stage based on the algorithm is 1.7%. The prediction error of the traditional particle swarm optimization algorithm is 12.6%, and the prediction error of the previous damage inversion algorithm based on ant colony optimization is 8.5%. The ability of the proposed algorithm is supported, which can be capable of seismic damage prediction of concrete structures subjected to earthquakes.},
  archive      = {J_ASOC},
  author       = {Bin Sun and Yan Li and Tong Guo},
  doi          = {10.1016/j.asoc.2024.111552},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111552},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization and prior knowledge fusion seismic damage prediction of concrete structures},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-head attention ResUnet with sequential sliding windows
for sea surface height anomaly field forecast: A regional study in north
atlantic ocean. <em>ASOC</em>, <em>157</em>, 111551. (<a
href="https://doi.org/10.1016/j.asoc.2024.111551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and efficient prediction of Sea surface height anomaly (SSHA) field is very important for operational marine monitoring and engineering. It is also a vital indicator of better understanding global climate changes and ocean dynamics. Many traditional SSHA forecasting methods focused primarily on single grid-point based predictions, and some classical Recurrent Neural Network/Long-Short-Term-Memory based machine learning approaches can cost heavily computational consumption, also the SSHA prediction by only employing the observations in single locations would lead to large potential uncertainties. This study proposed a novel purely Convolutional neural network (CNN) based Unet structure with Multi-head attention mechanism and Residual CNN blocks to accomplish daily SSHA variabilities prediction with higher accuracy and better computational efficiency. Specifically, the satellite altimetry observations aggregated by Data Unification and Altimeter Combination System (DUACS) from a sub-region with very high occurring frequently of storm surges and mesoscale/small-scale eddies in North Atlantic Ocean (NAO) was employed to verify the practicability and functionality of the proposed model. In addition, several existed deep learning approaches were adopted to implement a better comparison. Experimental results in this study demonstrated that the proposed methodology can achieve superior prediction performance amongst the four deep learning methods, and specifically, presenting significant superiorities on the aspect of computational costs compared to recurrent-based neural networks. Very high accurate SSHA predictions with averaged RMSE as 0.018 m and Correlation coefficient as 0.99 for 1 day ahead forecasting, and corresponded 0.107 m, 0.80 for 24days ahead forecasting was obtained by the proposed approach. It took about less than 2 minutes to fulfil a 5-year SSHA field forecasts together with optimal model training process, Additional forecasting experiments based on different seasonal SSHA sequential patterns reveal that the proposed purely CNN-based technique show pretty well generalization capability. This study provides an alternative promising-prospect on the application of purely CNN-based deep learning methods in sea surface variabilities forecasting.},
  archive      = {J_ASOC},
  author       = {Zeguo Zhang and Jianchuan Yin and Lijun Wang},
  doi          = {10.1016/j.asoc.2024.111551},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111551},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-head attention ResUnet with sequential sliding windows for sea surface height anomaly field forecast: A regional study in north atlantic ocean},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parallel chimp optimization algorithm based on
tracking-learning and fuzzy opposition-learning behaviors for data
classification. <em>ASOC</em>, <em>157</em>, 111547. (<a
href="https://doi.org/10.1016/j.asoc.2024.111547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chimp optimization algorithm (ChOA), which simulates the social behaviors of chimps, is a novel swarm intelligence algorithm for solving global optimization problems. ChOA has the advantages of fast convergence and avoiding falling into local optimum. However, the global search capability is weakened and the time overhead is too large when solving complex optimization problems. In order to improve the overall performance of ChOA, a parallel chimp optimization algorithm based on tracking-learning and fuzzy opposition-learning behaviors (PChOA) is proposed in this paper. First, a tracking-learning behavior is designed to improve the search accuracy. Second, a fuzzy opposition-learning behavior is adopted to enhance the global search capability. Third, a parallel computing architecture is developed to accelerate computational speed. Moreover, the convergence of our proposed PChOA has been analyzed theoretically. To validate the effectiveness of PChOA, it is applied to solve classification problem. The experimental results demonstrate that the classification performance of our proposed algorithm outperforms six other state of the art algorithms on most used datasets. Meanwhile, the time overhead of PChOA is significantly reduced in the environment of parallel computing. When the number of processors is increased to 16, PChOA costs less time than NBTree which is the fastest comparison algorithm in the experiment.},
  archive      = {J_ASOC},
  author       = {Zhaolin Lai and Guangyuan Li and Xiang Feng and Xiaochun Hu and Caoqing Jiang},
  doi          = {10.1016/j.asoc.2024.111547},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111547},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A parallel chimp optimization algorithm based on tracking-learning and fuzzy opposition-learning behaviors for data classification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A GAN-based method for diagnosing bodywork spot welding
defects in response to small sample condition. <em>ASOC</em>,
<em>157</em>, 111544. (<a
href="https://doi.org/10.1016/j.asoc.2024.111544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the hidden nature and complexity of resistance spot welding weld nugget formation, how to avoid the time-consuming and money-consuming problem of traditional defect diagnosis methods and accurately grasp the weld nugget status is still an urgent problem. In this paper, an improved GAN model is proposed to solve the corresponding problem by combining the weld nugget defects with the dynamic resistance curve. Aiming at the problem that traditional GAN algorithms are prone to pattern collapse, this paper utilizes a variational autoencoder integrated with a channel attention mechanism as the generator part of the generative adversarial network, which helps the model pay better attention to the high-weight part of the defective sample data and combines the encoding and decoding processes to highlight defective features, thus reconstructing the defective samples with higher quality. Convolutional neural networks are then utilized to identify the features of the generated samples and diagnose the type of weldment defects. The test results show that the proposed scheme is highly reliable and the model outperforms other schemes in diagnosing welded nugget defects under the same conditions, avoiding undesirable effects such as underfitting. The validation of the actual dataset shows that, compared with other diagnostic methods that generally have an accuracy rate of less than 75%, the accuracy of the weld nugget defects diagnosis of this paper&#39;s method reaches more than 94%, which is a positive impetus to the development of auto body welding diagnosis.},
  archive      = {J_ASOC},
  author       = {Chen Geng and Sheng Buyun and Fu Gaocai and Chen Xiangxiang and Zhao Guangde},
  doi          = {10.1016/j.asoc.2024.111544},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111544},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A GAN-based method for diagnosing bodywork spot welding defects in response to small sample condition},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Carbon emission price point-interval forecasting based on
multivariate variational mode decomposition and attention-LSTM model.
<em>ASOC</em>, <em>157</em>, 111543. (<a
href="https://doi.org/10.1016/j.asoc.2024.111543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of carbon emission prices is a crucial yet challenging task, given the complexity of the trading market and the variability of influencing factors. To comprehensively address factors related to carbon prices while considering interval forecasts associated with uncertainty, this study introduces a novel multistep ahead point-interval forecasting framework for carbon emission prices. This framework integrates multifactor selection, multivariate decomposition and reconstruction, an intelligent point and interval forecasting network, and an evaluation system. First, influencing factors are selected using a recursive feature elimination algorithm based on the extreme gradient boosting estimator. Second, original carbon prices and related influencing factors are decomposed into a set of multimode components (MMCs) using multivariate variational mode decomposition and sample entropy reconstruction techniques. Third, each row of MMCs is predicted using a long short-term memory model with an attention mechanism, and final point predictions are produced through simple addition. Finally, interval predictions are derived using enhanced kernel density estimation (KDE), which can expand the upper and lower bounds of the prediction interval. The stability and robustness have been tested in European Union Allowance (EUA) price forecasting. The results show that the proposed model is superior to other benchmark models, with MAPEs of 0.83%, 3.54%, and 3.82% in 1-step, 15-step, and 30-step ahead forecasting, respectively. Additionally, the proposed enhanced KDE demonstrates excellent performance; in 1-step ahead forecasting, the F-value is 1.86, 1.82, and 1.75 at the 80%, 90%, and 95% confidence levels, respectively. This proves that the proposed framework can effectively improve the point forecasting performance of EUA prices and quantify the prediction uncertainty.},
  archive      = {J_ASOC},
  author       = {Liling Zeng and Huanling Hu and Huajun Tang and Xuejing Zhang and Dabin Zhang},
  doi          = {10.1016/j.asoc.2024.111543},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111543},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Carbon emission price point-interval forecasting based on multivariate variational mode decomposition and attention-LSTM model},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristic computing performances based gudermannian neural
network to solve the eye surgery corneal model. <em>ASOC</em>,
<em>157</em>, 111540. (<a
href="https://doi.org/10.1016/j.asoc.2024.111540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current work is related to present the solutions of the corneal shape-based eye surgery model (CSESM) by applying the novel procedures of Gudermannian neural network (GNN) along with the hybrid optimization of the global and local approaches of heuristic genetic algorithm (GA) and sequential quadratic programing (SQP), i.e., GNN-GASQP. An error function is constructed using the terminologies of the differential model along with the corresponding boundary conditions of the CSESM and then the optimization of the parameter is approved by the global operator GA at the start and then local refinements of SQP is implemented. Six different cases of the CSESM have been numerically treated using the GNN-GASQP and the scheme’s correctness is performed through the numerical Runge-Kutta (RK) results. The analysis based small and larger neurons is also implemented to authenticate the stability of GNN-GASQP. Moreover, the analysis through statistics using different measures of root mean square error, Theil’s inequality coefficient and variance account for is presented to check the consistency of GNN-GASQP for solving the CSESM.},
  archive      = {J_ASOC},
  author       = {Zulqurnain Sabir and Muhammad Umar and Hafiz Abdul Wahab and Shahid Ahmad Bhat and Canan Unlu},
  doi          = {10.1016/j.asoc.2024.111540},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111540},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristic computing performances based gudermannian neural network to solve the eye surgery corneal model},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brownian motion based multi-objective particle swarm
optimization methodology and application in binary classification.
<em>ASOC</em>, <em>157</em>, 111539. (<a
href="https://doi.org/10.1016/j.asoc.2024.111539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The particle swam optimization (PSO) method has been widely applied in evolutionary computation and related industrial areas. However, the optimization performance of traditional PSO is usually influenced by particle updating displacement and velocity, which is randomly distributed and makes the solution solving process inefficient. To deal with this challenge, a Brownian particle motion and Euler-Maruyama (EM) methodology-based model is proposed to improve the performance of the multi-objective PSO. The EM based PSO principles and algorithm framework are introduced by theoretical analysis and mathematical modeling, and then the optimization performance of four comparing PSO models is tested, while the feasibility of the new PSO 4 is verified by typical benchmark problems. Afterwards, the characterizations of the EM based PSO method are investigated from the perspective of particle motion mechanism analysis, binary classification application and various optimization performance comparison. The result manifests that the new PSO 4 features higher accuracy and less running time among 24 related optimization methods for 4 typical standard datasets of Ionosphere, Breast, Diabetes and Wine. Finally, the multi-objective optimization and classification application potential of the Brownian motion and EM based PSO 4 is demonstrated, and the future work regarding the new PSO model is discussed.},
  archive      = {J_ASOC},
  author       = {Shiwei Liu and Yong Liu and Qiaohua Wang and Weiguo Lin and Yanhua Sun and Lingsong He},
  doi          = {10.1016/j.asoc.2024.111539},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111539},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Brownian motion based multi-objective particle swarm optimization methodology and application in binary classification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Credit risk prediction based on an interpretable three-way
decision method: Evidence from chinese SMEs. <em>ASOC</em>,
<em>157</em>, 111538. (<a
href="https://doi.org/10.1016/j.asoc.2024.111538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit risk prediction can provide essential tools for use in commercial banking credit and credit-related decision-making. This paper proposes a three-way decision method based on prospect theory and evidence theory for predicting credit risk. The first problem in this study is determining the optimal classification boundary, and the second is effectively predicting the sample default status within an uncertain boundary. To address the limitation of the SVDD model, which is that it does not consider the aggregation degree, a new sample-weighted support vector data description (SW-SVDD) model is constructed by ranking samples according to their relative membership degree. The classification boundaries of the default and nondefault samples are determined according to the maximum prediction accuracy of the SW-SVDD model. The samples are divided into definite boundary nondefault, definite boundary default, and uncertain boundary samples. The default status of samples falling into definite boundaries is predicted by the SW-SVDD model. The three-way decision method combining prospect theory and evidence theory predicts the default status of samples falling into uncertain boundaries. This paper also proposes a new interpretability method based on the default probability, nondefault probability, and optimal threshold point obtained by the three-way decision model. The empirical results show that the proposed three-way decision method is a model that balances accuracy and interpretability. It has a higher classification performance than traditional models and can reveal the key features that lead to each customer&#39;s default. The proposed three-way decision method can enhance the accuracy and reliability of risk assessments, enabling financial institutions to make more informed lending decisions and more effectively manage credit portfolios.},
  archive      = {J_ASOC},
  author       = {Meng Pang and Fengjuan Wang and Zhe Li},
  doi          = {10.1016/j.asoc.2024.111538},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111538},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Credit risk prediction based on an interpretable three-way decision method: Evidence from chinese SMEs},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-granularity hierarchical network for long- and
short-term forecasting on multivariate time series data. <em>ASOC</em>,
<em>157</em>, 111537. (<a
href="https://doi.org/10.1016/j.asoc.2024.111537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate time series forecasting is a significant research problem in many fields such as economics, finance and transportation, where simultaneous long- and short-term forecasting is required. However, current techniques are typically limited to a single short-term or a long-term forecast. To address the limitation, a novel multi-granularity hierarchical network, GNet-LS, is proposed for long- and short-term forecasting on multivariate time series data, which takes into account the separate role of internal correlation and external relationship. First, the original time series sequence is divided into multiple granular sequences based on downsampling, to reduce error accumulation caused by long-term prediction. In order to discover the external relationships between variables, the CNN module slides over the sequence of variables. The global CNN and local CNN are built to implement periodic and nonperiodic extraction, respectively. Next, a self-attention module is used to model dependencies between the output of local CNN and global CNN. The LSTM networks and attention mechanisms are used to mine internal correlation of the target variable on time series. Then, multiple granular external relationships and internal correlation are obtained in parallel. Finally, external relationships and internal correlation are fused together by splicing and overlay to obtain both long-term and short-term forecasts. The experimental results demonstrate that the proposed GNet-LS outperforms a bunch of compared methods in terms of RSE, CORR, MAE and RMSE.},
  archive      = {J_ASOC},
  author       = {Hong Yu and Zongqiang Wang and Yongfang Xie and Guoyin Wang},
  doi          = {10.1016/j.asoc.2024.111537},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111537},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-granularity hierarchical network for long- and short-term forecasting on multivariate time series data},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic graph structure identification method of
spatio-temporal correlation in an aluminum electrolysis cell.
<em>ASOC</em>, <em>157</em>, 111536. (<a
href="https://doi.org/10.1016/j.asoc.2024.111536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic correlation analysis of cell-spatial information (distributed anode current signal, DACS) is of great significance in the regional-refined control of industrial aluminum electrolysis cell. Due to the strong-dynamic spatio-temporal correlation of DACS and the complex dynamic cell noise, the existing methods are difficult to effectively obtain the spatio-temporal correlation analysis results of aluminum electrolysis cell. To solve these problems, a dynamic graph structure identification method of spatio-temporal correlation (DGSI-StC) in an aluminum electrolysis cell is proposed. The identified dynamic graph structure is used to describe the dynamic correlation analysis results of cell-spatial information. Specifically, a novel strongly robust distance function Edit Distance on Real sequences with Adaptive threshold (At-EDR) is proposed to weaken the impact of complex dynamic cell noise on the dynamic correlation analysis of cell-spatial information. Under the guidance of aluminum electrolysis process mechanism knowledge, the matrix-representation of dynamic cell noise information obtained from production data is used as the adaptive threshold of At-EDR to construct the dynamic graph structure. Then, based on the single-layer classical graph convolutional network, a framework for optimizing the dynamic graph structure is designed to capture the strong-dynamic spatio-temporal correlation of DACS. The framework obtains the optimal dynamic graph structure by optimizing the hyper-parameters. Finally, the experimental results on the public datasets show that the robustness of At-EDR is superior to existing methods. Meanwhile the experimental results on multiple sets of industrial aluminum electrolysis production datasets show that DGSI-StC improves the classification accuracy by 2.82% compared with existing classification methods.},
  archive      = {J_ASOC},
  author       = {Yubo Sun and Xiaofang Chen and Lihui Cen and Weihua Gui and Chunhua Yang and Zhong Zou},
  doi          = {10.1016/j.asoc.2024.111536},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111536},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dynamic graph structure identification method of spatio-temporal correlation in an aluminum electrolysis cell},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complex q-rung orthopair fuzzy yager aggregation operators
and their application to evaluate the best medical manufacturer.
<em>ASOC</em>, <em>157</em>, 111532. (<a
href="https://doi.org/10.1016/j.asoc.2024.111532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary healthcare systems, the selection and judicious choice of the right medical device becomes paramount consideration for accurate diagnosis, patient care, mitigate risk and cost incurred on diagnosis of disease. Moreover, medical devices are requisite indispensable tools to help healthcare professionals for effective diagnosis, selection of the right treatment amongst different options, and continuous monitoring of the patient. Thus, to select the best manufacturing company is as equally important to achieve the desired goals. This paper presents a decision making algorithm based on fuzzy theory that helps the decision maker in selecting the most suitable worldwide medical device manufacturer. Fuzzy theory is extensively used in managing databases and analyzing data to summarize information and draw meaningful conclusions. This paper utilizes the aggregation operators within the context of the complex q-rung orthopair fuzzy environment by utilizing Yager t-norm and t-conorm operations. Some aggregation operators are applied, namely, complex q-rung orthopair fuzzy Yager weighted geometric operator, complex q-rung orthopair fuzzy Yager ordered weighted geometric operator, complex q-rung orthopair fuzzy Yager weighted averaging operator and complex q-rung orthopair fuzzy Yager ordered weighted averaging operator. Additionally, the notable properties of these operators are proved. These aggregation operators are generated specifically for multi-criteria decision-making. Decision making algorithm is used to choose the most relevant medical device manufacturer (alternative) to validate its efficacy. The results demonstrate the versatility of parameters, adaptability to multi-criteria decision making, and efficiency of the proposed aggregation operators and algorithm. Test with specific criteria is also conducted to verify the legality and validity of the decision making algorithm. At last, the acquired results underwent a comparative analysis against various established methodologies, confirming the method’s capacity to produce precise and accurate outcomes.},
  archive      = {J_ASOC},
  author       = {Shumaila Javeed and Mubashar Javed and Izza Shafique and Muhammad Shoaib and Mansoor Shaukat Khan and Lirong Cui and Sameh Askar and Ahmad M. Alshamrani},
  doi          = {10.1016/j.asoc.2024.111532},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111532},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complex q-rung orthopair fuzzy yager aggregation operators and their application to evaluate the best medical manufacturer},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter-efficient fine-tuning large language model
approach for hospital discharge paper summarization. <em>ASOC</em>,
<em>157</em>, 111531. (<a
href="https://doi.org/10.1016/j.asoc.2024.111531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text summarization in medical domain is one of the most crucial chores as it deals with the critical human information. Consequently the proper summarization and key point extraction from medical deeds using pre-trained Language models is now the key figure to be focused on for the researchers. But due to the considerable amount of real-world data and enormous amount of memory requirement to train the Large Language Models (LLMs), research on these models become challenging. To overcome these challenges multiple prompting and tuning techniques are being used. In this paper, effectiveness of prompt engineering and parameter efficient fine tuning is being studied to summarize the Hospital Discharge Summary (HDS) papers effectively, so that these models can accurately interprete medical terminologies and contexts, generate brief but compact summaries, and draw out concentrated themes, which opens new approaches for the application of LLMs in healthcare and making HDS more patient-friendly. In this research LLaMA 2 (Large Language Model Meta AI) has been considered as the base model. Also, the model has been fine-tuned using QLoRA (Quantized Low Rank Adapters), which can bring down the memory usage of LLMs without compromising the data quality. This study explores the way to use LLMs on HDS datasets without the hassle of memory usage using QLoRA, into electronic health record systems to further streamline the handling and retrieval of healthcare information.},
  archive      = {J_ASOC},
  author       = {Joyeeta Goswami and Kaushal Kumar Prajapati and Ashim Saha and Apu Kumar Saha},
  doi          = {10.1016/j.asoc.2024.111531},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111531},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parameter-efficient fine-tuning large language model approach for hospital discharge paper summarization},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024e). A PiRNA-disease association model incorporating sequence
multi-source information with graph convolutional networks.
<em>ASOC</em>, <em>157</em>, 111523. (<a
href="https://doi.org/10.1016/j.asoc.2024.111523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is growing evidence that PIWI-interacting RNA (piRNA) is widely involved in the proliferation, invasion, and metastasis of malignant tumors, playing an important regulatory role in numerous human physiological and pathological processes. Disease-associated piRNAs are expected to be biomarkers and novel therapeutic targets for early diagnosis and prognosis of malignant tumors. However, most previous computational models did not fully focus on the rich representation ability of multiple sources of information in piRNA sequences, which affected their performance in predicting piRNA-disease associations (PDAs). In this work, we propose a model, iSG-PDA, which combines the multi-source information of piRNA sequences with graph convolutional neural networks to predict potential PDAs. More specifically, we first fuse multi-source information including piRNA sequences and disease semantics to enhance the expressiveness of data, then deeply mine the advanced hidden features of PDA using graph convolutional networks, and finally exploit random forest to accurately determine the associations between piRNAs and diseases. In the golden standard dataset, the proposed model realized a prediction accuracy of 91.96% at the AUC of 0.9184. In ablation experiments and comparisons with other different models, iSG-PDA exhibits strong competitiveness. Moreover, the results of the case study indicate that 17 of the top 20 PDAs in the proposed model predictive score were confirmed. These preliminary results reveal that iSG-PDA is an effective computational method for predicting PDAs and can provide reliable disease candidate piRNAs for biological experiments.},
  archive      = {J_ASOC},
  author       = {Lei Wang and Zheng-Wei Li and Jing Hu and Leon Wong and Bo-Wei Zhao and Zhu-Hong You},
  doi          = {10.1016/j.asoc.2024.111523},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111523},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A PiRNA-disease association model incorporating sequence multi-source information with graph convolutional networks},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). An operator-inspired framework for metaheuristics and its
applications on job-shop scheduling problems. <em>ASOC</em>,
<em>157</em>, 111522. (<a
href="https://doi.org/10.1016/j.asoc.2024.111522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The job-shop scheduling problem (JSP) is a well-known combinatorial optimization problem in manufacturing systems. For the past two decades, real-number metaheuristics have been widely used to solve the JSP using the real-number transform methods. A limitation of the real-number metaheuristics is the premature convergence due to the stochasticity of the transform methods. To eliminate this limitation, a novel operator framework has been designed, building the bridge between the discrete optimization problem (JSP) and real-number metaheuristics (also called continuous metaheuristics). Specifically, this paper captures the core operators of the real-number metaheuristics, namely, addition, subtraction, and multiplication. Firstly, three new operators are reconstructed according to several simple neighborhood structures to solve the JSP robustly and effectively. The properties of the arithmetic (symmetry) are taken into account in the reconstruction of the proposed operators to avoid excessive redundant searches. Secondly, a positional similarity-based population diversity is presented for the JSP to demonstrate the intrinsic distinctions between the proposed operators and the transform methods during the evolutionary process. Finally, the results of five widely used benchmark test suites (185 instances) show that the proposed operators can achieve a better balance between exploration and exploitation than the current real-number transform methods.},
  archive      = {J_ASOC},
  author       = {Jiahang Li and Xinyu Li and Liang Gao},
  doi          = {10.1016/j.asoc.2024.111522},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111522},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An operator-inspired framework for metaheuristics and its applications on job-shop scheduling problems},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Research orientation and novelty discriminant for new
metaheuristic algorithms. <em>ASOC</em>, <em>157</em>, 111521. (<a
href="https://doi.org/10.1016/j.asoc.2024.111521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid rate of generating a new metaheuristic algorithm almost every month is causing increasing concerns and disputes about their novelty. To stop the disputes and steer algorithm design in a healthy direction, this article presents a discriminant method of novelty and a research orientation for metaheuristic algorithms. The novelty discriminant is implemented by two novel mathematical definitions of homologous algorithms and root algorithms. The two definitions are developed to divide algorithms into two classes according to a discrepancy in whether the reproduction operator of an algorithm is a linear combination of existing operators. Root algorithms are strongly innovative because of the novelty of their reproduction operators. A homologous algorithm is recognized as a novel algorithm only when the practical value and academic significance of the new combinatorial structure of the algorithm’s reproduction operator is clearly highlighted. So a research orientation that the study of a homologous algorithm should focus on how a certain metaphor evokes a new combinatorial structure can be developed. Moreover, numerical experiments should be conducted to analyze the relationship between its search behavior and its new combinatorial structure. Further work can be directed towards studying the systematization of existing knowledge about search behaviors of metaheuristic algorithms.},
  archive      = {J_ASOC},
  author       = {Zhongbo Hu and Qian Zhang and Yujie Wang and Qinghua Su and Zenggang Xiong},
  doi          = {10.1016/j.asoc.2024.111521},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111521},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Research orientation and novelty discriminant for new metaheuristic algorithms},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A priority-based self-guided serial–parallel genetic
algorithm for low-dose computed tomography. <em>ASOC</em>, <em>157</em>,
111520. (<a href="https://doi.org/10.1016/j.asoc.2024.111520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed tomography (CT) is a non-destructive evaluation technique to know the internal structure of the objects under scan. It has numerous applications in engineering as well as in the medical field. The prime objective of this manuscript is to reduce the radiation dose to the object under scanning and to reconstruct the low-noise CT images, even in limited-view projections data scenarios. The present manuscript proposes a novel priority-based self-guided serial-parallel hybrid genetic algorithm for low-dose CT reconstruction. The current algorithm combines serial and parallel processing elements to reduce the loss of diversity in the population and increase the convergence rate. The proposed algorithm uses a novel priority-based self-guided competition operator. It consists of two approaches for maintaining exploration and exploitation tradeoffs. Here, the first approach uses serial processing, whereas the second approach uses parallel processing. The algorithm also uses priority-based hybrid crossover (PHCO) and adaptive priority-based mutation operator (PMO) to generate the most suitable offspring in every generation. Experimental results reveal that the presented algorithm produces satisfactory results with limited-view projections. The presented algorithm also outperforms other limited-view CT reconstruction algorithms.},
  archive      = {J_ASOC},
  author       = {Raghavendra Mishra and Manish Kumar Bajpai},
  doi          = {10.1016/j.asoc.2024.111520},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111520},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A priority-based self-guided serial–parallel genetic algorithm for low-dose computed tomography},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FedMUP: Federated learning driven malicious user prediction
model for secure data distribution in cloud environments. <em>ASOC</em>,
<em>157</em>, 111519. (<a
href="https://doi.org/10.1016/j.asoc.2024.111519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is flourishing at a rapid pace. Significant consequences related to data security appear as a malicious user may get unauthorized access to sensitive data which may be misused, further. This raises an alarm-ringing situation to tackle the crucial issue related to data security and proactive malicious user prediction. This article proposes a Fed erated learning driven M alicious U ser P rediction Model for Secure Data Distribution in Cloud Environments ( FedMUP ). This approach firstly analyzes user behavior to acquire multiple security risk parameters. Afterward, it employs the federated learning-driven malicious user prediction approach to reveal doubtful users, proactively. FedMUP trains the local model on their local dataset and transfers computed values rather than actual raw data to obtain an updated global model based on averaging various local versions. This updated model is shared repeatedly at regular intervals with the user for retraining to acquire a better, and more efficient model capable of predicting malicious users more precisely. Extensive experimental work and comparison of the proposed model with state-of-the-art approaches demonstrate the efficiency of the proposed work. Significant improvement is observed in the key performance indicators such as malicious user prediction accuracy, precision, recall, and f1-score up to 14.32%, 17.88%, 14.32%, and 18.35%, respectively.},
  archive      = {J_ASOC},
  author       = {Kishu Gupta and Deepika Saxena and Rishabh Gupta and Jatinder Kumar and Ashutosh Kumar Singh},
  doi          = {10.1016/j.asoc.2024.111519},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111519},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FedMUP: Federated learning driven malicious user prediction model for secure data distribution in cloud environments},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SPINEX: Similarity-based predictions with explainable
neighbors exploration for regression and classification. <em>ASOC</em>,
<em>157</em>, 111518. (<a
href="https://doi.org/10.1016/j.asoc.2024.111518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of machine learning (ML) has witnessed significant advancements in recent years. However, many existing algorithms lack interpretability and struggle with high-dimensional and imbalanced data. This paper proposes SPINEX, a novel similarity-based interpretable neighbor exploration algorithm designed to address these limitations. This algorithm combines ensemble learning and feature interaction analysis to achieve accurate predictions and meaningful insights by quantifying each feature&#39;s contribution to predictions and identifying interactions between features, thereby enhancing the interpretability of the algorithm. To evaluate the performance of SPINEX, extensive experiments on 59 synthetic and real datasets were conducted for both regression and classification tasks. The results demonstrate that SPINEX achieves comparative performance and, in some scenarios, may outperform commonly adopted ML algorithms. The same findings demonstrate the effectiveness and competitiveness of SPINEX, making it a promising approach for various real-world applications.},
  archive      = {J_ASOC},
  author       = {M.Z. Naser and ‬‬‬Mohammad Khaled al-Bashiti and Ahmad Z. Naser},
  doi          = {10.1016/j.asoc.2024.111518},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111518},
  shortjournal = {Appl. Soft. Comput.},
  title        = {SPINEX: Similarity-based predictions with explainable neighbors exploration for regression and classification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards unbalanced multiclass intrusion detection with
hybrid sampling methods and ensemble classification. <em>ASOC</em>,
<em>157</em>, 111517. (<a
href="https://doi.org/10.1016/j.asoc.2024.111517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion Detection Systems (IDS) play a crucial role in securing computer networks against malicious activities. However, their efficacy is consistently hindered by the persistent challenge of class imbalance in real-world datasets. While various methods, such as resampling techniques, ensemble methods, cost-sensitive learning, data augmentation, and so on, have individually addressed imbalance classification issues, there exists a notable gap in the literature for effective hybrid methodologies aimed at enhancing IDS performance. To bridge this gap, our research introduces an innovative methodology that integrates hybrid undersampling and oversampling strategies within an ensemble classification framework. This novel approach is designed to harmonize dataset distributions and optimize IDS performance, particularly in intricate multi-class scenarios. In-depth evaluations were conducted using well-established intrusion detection datasets, including the Car Hacking: Attack and Defense Challenge 2020 (CHADC2020) and IoTID20. Our results showcase the remarkable efficacy of the proposed methodology, revealing significant improvements in precision, recall, and F1-score metrics. Notably, the hybrid-ensemble method demonstrated an exemplary average F1 score exceeding 98% for both datasets, underscoring its exceptional capability to substantially enhance intrusion detection accuracy. In summary, this research represents a significant contribution to the field of IDS, providing a robust solution to the pervasive challenge of class imbalance. The hybrid framework not only strengthens IDS efficacy but also illuminates the seamless integration of undersampling and oversampling within ensemble classifiers, paving the way for fortified network defenses.},
  archive      = {J_ASOC},
  author       = {Thi-Thu-Huong Le and Yeongjae Shin and Myeongkil Kim and Howon Kim},
  doi          = {10.1016/j.asoc.2024.111517},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111517},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards unbalanced multiclass intrusion detection with hybrid sampling methods and ensemble classification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic adaptive graph convolutional transformer with broad
learning system for multi-dimensional chaotic time series prediction.
<em>ASOC</em>, <em>157</em>, 111516. (<a
href="https://doi.org/10.1016/j.asoc.2024.111516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chaotic time series data is extensively applied in financial stocks, climate monitoring, and sea clutter, in which data fusion from various sources and multi-sensor information make accurate predictions of chaotic time series challenging under complex nonlinear conditions. Previous works focus on designing different model frameworks to capture the temporal dependence and extract richer nonlinear features to improve the accuracy of univariate chaotic time series prediction, which ignores the spatial dependence of multivariable. However, in this paper, we argue that spatial correlation among multiple variables is essential to improve the prediction accuracy of chaotic time series. To fill the gap, we innovatively propose a D ynamic A daptive G raph C onvolutional T ransformer with a B road L earning S ystem ( DAGCT-BLS ), a GCN and Transformer-based model utilizing multivariate spatial dependence for multi-dimensional chaotic time series forecasting. In DAGCT-BLS, the multivariate chaotic time series are reconstructed into the phase space, and the reconstructed data are rapidly feature-extracted using a cascade network BLS with frozen weights to maximize the retention of chaotic properties and nonlinear relationships. Then, the Dynamic Adaptive Graph Convolutional Network (DAGCN) is proposed to capture the spatial correlation among the multiple variables. Finally, improved multi-head attention of the Transformer Encoder is used to capture the temporal dependence of the phase point sequence. Experiments of our proposed model on three datasets (Lorenz, Rossler, and Sea clutter) show that DAGCT-BLS can achieve the best prediction performance and have strong interpretability, and multivariate-based joint modeling of chaotic time series helps to improve the prediction performance.},
  archive      = {J_ASOC},
  author       = {Lang Xiong and Liyun Su and Xiaoyi Wang and Chunquan Pan},
  doi          = {10.1016/j.asoc.2024.111516},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111516},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic adaptive graph convolutional transformer with broad learning system for multi-dimensional chaotic time series prediction},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive fault detection model based on variational
auto-encoders and unsupervised transfer learning. <em>ASOC</em>,
<em>157</em>, 111515. (<a
href="https://doi.org/10.1016/j.asoc.2024.111515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the problem of insufficient generalization of fault detection in traditional machine learning, an SDN controller fault detection method based on unsupervised transfer learning is proposed. The method mainly includes two parts. (1) A Gaussian mixture variational autoencoder based on the autoregressive flow is proposed. First, the encoder and decoder of variational autocoding are improved with gated recurrent units, and the improved variational autocoding can process time series data. Secondly, the gated recurrent unit is improved by using the gravitational search algorithm, which speeds up the search of the weight of the gated recurrent unit. Further, considering that the latent space of the variational autoencoder is a single Gaussian distribution, and the complex data in reality is often too simple to be represented by a single Gaussian distribution. (2) Aiming at the problem of poor generalization of fault detection models in practical scenarios, a domain adaptive fault detection algorithm based on multi-kernel maximum mean difference and intra-class distance constraints is proposed. Map the features into the manifold space to eliminate the distortion of the features in the original space. After mapping, the distance between fields needs to be measured, and the maximum mean difference of a single kernel cannot determine which kernel function is more suitable for the current task in practical applications. Therefore, the maximum mean difference based on multi-core is introduced to measure between the two fields. The experimental results show that the algorithm proposed improves the accuracy about 5% compared with the previous algorithm.},
  archive      = {J_ASOC},
  author       = {Fengjun Shang and Fengyin Sun and Jiayu Wen},
  doi          = {10.1016/j.asoc.2024.111515},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111515},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive fault detection model based on variational auto-encoders and unsupervised transfer learning},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Data-unbalanced traffic accident prediction via adaptive
graph and self-supervised learning. <em>ASOC</em>, <em>157</em>, 111512.
(<a href="https://doi.org/10.1016/j.asoc.2024.111512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic accident prediction is an important research problem, which can help to identify dangerous situations on the road in advance and take appropriate measures. Nonetheless, real-world traffic accident data suffers from a significant data unbalance problem, as accident occurrences vary unevenly in both spatial and data domains. This unbalance can easily lead to the prediction methods biased towards the side with more data. Recently, researchers have proposed a series of effective prediction methods based on deep learning and graph theory. Existing graph-based methods always adopt the predefined distance graph. However, these methods cannot fully capture the spatial correlations among regions that are far away from each other but share similar accident patterns. To address these challenges, we propose a traffic accident prediction method that combines Adaptive Graphs with Self-Supervised Learning (AGSSL). In the proposed method, we can adaptively construct graph structures to learn global spatial correlations among urban regions. Meanwhile, two self-supervised learning modules called Graph Infomax and Focal Contrastive Regularization are used to learn a robust representation of traffic accidents data under an unbalanced distribution. Experiment results show that AGSSL outperforms SOTA methods in traffic accident prediction.},
  archive      = {J_ASOC},
  author       = {Shun Wang and Yong Zhang and Xinglin Piao and Xuanqi Lin and Yongli Hu and Baocai Yin},
  doi          = {10.1016/j.asoc.2024.111512},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111512},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-unbalanced traffic accident prediction via adaptive graph and self-supervised learning},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrete-time zeroing neural network with quintic error mode
for time-dependent nonlinear equation and its application to robot arms.
<em>ASOC</em>, <em>157</em>, 111511. (<a
href="https://doi.org/10.1016/j.asoc.2024.111511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-dependent nonlinear equation (TDNE) arises in numerous engineering applications. Recently, zeroing neural network (ZNN) has been proven to be an effective alternative for solving the TDNE. In this paper, we present a new solution to the TDNE by using the discrete-time ZNN (DTZNN). Specifically, a special difference formula is first constructed via Taylor series expansion. Then, by utilizing such a formula to discretize the existing continuous-time ZNN model, the new DTZNN model is proposed to determine the TDNE solution. Theoretical analysis and numerical results further indicate the validity and superiority of the proposed DTZNN model in comparison with the previous models. Finally, the DTZNN practicality is presented by the simulation and experiment on the DOBOT arm using the proposed model.},
  archive      = {J_ASOC},
  author       = {Naimeng Cang and Hao Tang and Dongsheng Guo and Weidong Zhang and Weibing Li and Xuanxian Li},
  doi          = {10.1016/j.asoc.2024.111511},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111511},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discrete-time zeroing neural network with quintic error mode for time-dependent nonlinear equation and its application to robot arms},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024d). Cross-view multi-layer perceptron for incomplete multi-view
learning. <em>ASOC</em>, <em>157</em>, 111510. (<a
href="https://doi.org/10.1016/j.asoc.2024.111510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multi-view learning (IML) is an important and challenging issue. The recent popular matrix factorization methods learn the representation matrix that contains as much complete information as possible from incomplete data. However, these works focus more on mining intrinsic information from the remaining views but fail to exploit the latent and connotative consistency, complementarity, and diversity information across views simultaneously. Meanwhile, the commonly used mean completer or deleting incomplete views strategy generates high uncertainty samples. To overcome these limits, this paper presents a Cross-View Multi-Layer Perceptron (CVMLP). CVMLP integrates an auto-encoder module, cross-view classification loss, masked contrastive learning, and variance loss into a unified framework to learn IML problems. The auto-encoder and cross-view modules efficiently express consistency and diversity across views, mining structural information from within views to between views. Masked contrastive loss makes the model robust to missing views by establishing a contrastive relationship between the input and random masked data. The variance loss can reduce the uncertainty of the classification hyperplane. Extensive experiments demonstrate that CVMLP achieves superior performance.},
  archive      = {J_ASOC},
  author       = {Zhi Wang and Heng Zhou and Ping Zhong and Hui Zou},
  doi          = {10.1016/j.asoc.2024.111510},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111510},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cross-view multi-layer perceptron for incomplete multi-view learning},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A region-wise indoor localization system based on
unsupervised learning and ant colony optimization technique.
<em>ASOC</em>, <em>157</em>, 111509. (<a
href="https://doi.org/10.1016/j.asoc.2024.111509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based indoor localization has gained widespread attention in the recent past with the ubiquitous deployment of WLAN. However, for sustainable performance, it is crucial to identify and maintain the important WiFi Access Points (APs). Interestingly, the localization capabilities of APs differ even within their area of coverage depending on the indoor ambience and building properties. Thus, the significance of an AP to the localization performance can be better assessed if the entire region is divided into optimal number of clusters/subregions. This type of problem is hardly investigated in the literature. Consequently, in this paper, the aforementioned challenges are addressed from the perspective of expert systems through applying machine learning and meta-heuristic techniques. Accordingly, our contribution is three-fold. First, we have designed a Region-wise Indoor Localization System (RwILS) in which a sub-regional division algorithm is proposed using DBSCAN approach. Second, a sub-region-wise important AP selection algorithm is designed based on Ant Colony Optimization technique. Third, a location estimation approach is proposed that first identifies the sub-region and then predicts the location point in that sub-region using supervised learning classifiers. A publicly available dataset, JUIndoorLoc is used for experimental evaluation. The localization accuracy of RwILS is improved to 95.68% while the state-of-the-art classifiers give 71% to 83% accuracy. RwILS also outperforms some recent works which further validates its effectiveness. Thus, this proposed technique can lead to an effective expert system in the domain of indoor localization.},
  archive      = {J_ASOC},
  author       = {Priya Roy and Chandreyee Chowdhury},
  doi          = {10.1016/j.asoc.2024.111509},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111509},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A region-wise indoor localization system based on unsupervised learning and ant colony optimization technique},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A pareto dominance relation based on reference vectors for
evolutionary many-objective optimization. <em>ASOC</em>, <em>157</em>,
111505. (<a href="https://doi.org/10.1016/j.asoc.2024.111505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pareto dominance based approach is a classical method for solving multi-objective optimization problems (MOPs). However, as the number of objectives increases, the selection pressure drops sharply. Solutions with good convergence and diversity are hardly obtained. To tackle these issues, this paper proposes a Pareto dominance relation based on reference vectors (called PRV-dominance) for evolutionary many-objective optimization. In PRV-dominance, solutions in the population are divided into several subregions according to a set of uniform reference vectors. To enhance the convergence, a new convergence metric based on the ranking of objective function values is designed to determine the dominance relationship between two solutions. Then, the density in different subregions is considered to maintain the diversity. In order to verify the performance of our approach, WFG and MaF benchmark problems with 3, 5, 8, and 15 objectives are utilized. Experimental results demonstrate that the proposed PRV-dominance outperforms eight existing dominance relations in balancing convergence and diversity. An improved NSGA-II is suggested based on the proposed PRV-dominance, which shows the competitive performance when compared with six other state-of-the-art algorithms in solving many-objective optimization problems (MaOPs). The effectiveness of the proposed PRV-dominance is also verified on two other existing many-objective evolutionary algorithms.},
  archive      = {J_ASOC},
  author       = {Shuai Wang and Hui Wang and Zichen Wei and Feng Wang and Qingling Zhu and Jia Zhao and Zhihua Cui},
  doi          = {10.1016/j.asoc.2024.111505},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111505},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A pareto dominance relation based on reference vectors for evolutionary many-objective optimization},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining deep learning models for ozone pollution
prediction via embedded feature selection. <em>ASOC</em>, <em>157</em>,
111504. (<a href="https://doi.org/10.1016/j.asoc.2024.111504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient air pollution is a pervasive global issue that poses significant health risks. Among pollutants, ozone ( O 3 O3 ) is responsible for an estimated 1 to 1.2 million premature deaths yearly. Furthermore, O 3 O3 adversely affects climate warming, crop productivity, and more. Its formation occurs when nitrogen oxides and volatile organic compounds react with short-wavelength solar radiation. Consequently, urban areas with high traffic volume and elevated temperatures are particularly prone to elevated O 3 O3 levels, which pose a significant health risk to their inhabitants. In response to this problem, many countries have developed web and mobile applications that provide real-time air pollution information using sensor data. However, while these applications offer valuable insight into current pollution levels, predicting future pollutant behavior is crucial for effective planning and mitigation strategies. Therefore, our main objectives are to develop accurate and efficient prediction models and identify the key factors that influence O 3 O3 levels. We adopt a time series forecasting approach to address these objectives, which allows us to analyze and predict future O 3 O3 behavior. Additionally, we tackle the feature selection problem to identify the most relevant features and periods that contribute to prediction accuracy by introducing a novel method called the Time Selection Layer in Deep Learning models, which significantly improves model performance, reduces complexity, and enhances interpretability. Our study focuses on data collected from five representative areas in Seville, Cordova, and Jaen provinces in Spain, using multiple sensors to capture comprehensive pollution data. We compare the performance of three models: Lasso, Decision Tree, and Deep Learning with and without incorporating the Time Selection Layer. Our results demonstrate that including the Time Selection Layer significantly enhances the effectiveness and interpretability of Deep Learning models, achieving an average effectiveness improvement of 9% across all monitored areas.},
  archive      = {J_ASOC},
  author       = {M.J. Jiménez-Navarro and M. Martínez-Ballesteros and F. Martínez-Álvarez and G. Asencio-Cortés},
  doi          = {10.1016/j.asoc.2024.111504},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111504},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Explaining deep learning models for ozone pollution prediction via embedded feature selection},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensorless force estimation of teleoperation system based on
multilayer depth extreme learning machine. <em>ASOC</em>, <em>157</em>,
111494. (<a href="https://doi.org/10.1016/j.asoc.2024.111494">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The force feedback technology has a crucial impact on the precise control of teleoperation system. Robots in fields such as minimally invasive surgery and nuclear waste cannot integrate force sensors to obtain force feedback due to their end size and harsh working environment. In order to obtain end-effector force feedback in sensorless situation, a sensorless force estimation model based on artificial neural network is defined in this paper. For improving the model prediction ability, a new Multi-layer Depth Extreme Learning Machine (MDELM) is proposed. Firstly, the network structure of the extreme learning machine is redesigned, and the smoothing function and Gauss-Laplacian function are constructed as the feature extraction layer and enhancement layer of the MDELM model. Then, to further improve the prediction performance of MDELM, an improved Sooty Tern Optimization Algorithm (STOA) is introduced to optimize the parameters of the model. The results show that the proposed force estimator outperforms the existing model with an MAE of less than 0.205, which is at least 18.65% lower than the existing model. In addition, when the acceleration of the remote operating system is a fixed constant or the information is not available，a sensorless force feedback system can be constructed by combining motor torque, joint position and joint velocity parameter. In general, this study provides an effective force estimation solution for teleoperation system without force sensor.},
  archive      = {J_ASOC},
  author       = {Mingzhang Pan and Tiecheng Su and Ke Liang and Lu Liang and Qiye Yang},
  doi          = {10.1016/j.asoc.2024.111494},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111494},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sensorless force estimation of teleoperation system based on multilayer depth extreme learning machine},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A patent text-based product conceptual design
decision-making approach considering the fusion of incomplete evaluation
semantic and scheme beliefs. <em>ASOC</em>, <em>157</em>, 111492. (<a
href="https://doi.org/10.1016/j.asoc.2024.111492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conceptual design evaluation is a significant challenge in product development to select suitable conceptual schemes (CS). Relying on decision makers&#39; (DMs) personal experience for design information may result in incomplete and ambiguous evaluation semantics, creating an incomplete decision matrix. Previous approaches use attributes based on candidate schemes to complete the missing information, but neglect the mining of associated design information from external resources such as patents, reducing the beliefs objectivity after missing semantic completion. Besides, non-compensatory nature between DMs’ interactivity and criteria have not been well considered. To fill these issues, a patent text-based CS decision approach considering the fusion of incomplete semantics and scheme beliefs is proposed. First, using the beneficial effects text from patents as historical design data, constructing general evaluation criteria using the Latent Dirichlet Allocation topic model, and introducing the Apriori model to extract strong association rules between criteria to complete the incomplete semantics. Second, a fuzzy measure model for DMs is constructed based on intuitionistic fuzzy cross-entropy and Shapley value method, supporting the allocation of DMs&#39; interactive weights. Then, the fuzzy Dempster-Shafer evidence theory is used to transform the multi-criteria decision model into a belief fusion problem of CS that avoids aggregating the evaluation semantics of non-compensatory criteria and outputting the optimal CS with interval credibility. A practical case study of an in-pipe inspection robot will be employed to validate the proposed approach, sensitivity analysis and comparison results confirmed that the complement data have the reliability to avoid continuous investment in the evaluation process.},
  archive      = {J_ASOC},
  author       = {Liting Jing and Xiaoyan Fan and Di Feng and Congda Lu and Shaofei Jiang},
  doi          = {10.1016/j.asoc.2024.111492},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111492},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A patent text-based product conceptual design decision-making approach considering the fusion of incomplete evaluation semantic and scheme beliefs},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mathematical analysis of big data analytics under bipolar
complex fuzzy soft information. <em>ASOC</em>, <em>157</em>, 111481. (<a
href="https://doi.org/10.1016/j.asoc.2024.111481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this study is to develop new and efficient theories for handling complex and unreliable data in real-world scenarios. The proposed approach integrates two distinct theories: the Bipolar Complex Fuzzy Set (BCFS) and the Soft Set (SS), resulting in a novel and superior method compared to existing solutions. Furthermore, the value of big data analytics cannot be overstated as it provides businesses with the opportunity to use their data to find areas for development and progress. To gain insights from raw data, it must first be collected and organized. Data modeling represents these large, complex data sets in a visual format, such as a chart or diagram. However, the visualization process often results in uncertainties and inaccuracies, highlighting the need for a more robust approach to data analysis. To address this issue, this paper introduces the concept of Bipolar Complex Fuzzy Soft Relations (BCFSRs), a fuzzy decision-making method within a complex bipolar fuzzy environment that can have the advantages of both a Bipolar Complex fuzzy relation and a soft relation at the same time. In BCFSRs, an element can have degrees of membership in both the positive and negative directions, reflecting a more nuanced and versatile representation of uncertainty or ambiguity. These mathematical ideas are explained through the Cartesian product of two Bipolar Complex Fuzzy Soft Sets (BCFSSs) and serve to simplify the decision-making process by presenting the BCFSSs in a clear and concise manner. The inventive notion of the BCFSRs elucidates the combined positive and negative effects of anything with parameterization. To support organizations in making data-driven choices and obtain insights for strategic planning and inventiveness, the analysis of big data is essential in several industries, including financial services, marketing, and medical treatment, among others. To choose the optimal big data analytics for efficient operation, this research presents modeling methodologies based on BCFSRs, including the formulation and analysis of scoring functions. The validity of the proposed work is demonstrated through a comparison study with existing methods, providing valuable insights for effective data analysis.},
  archive      = {J_ASOC},
  author       = {Naeem Jan and Jeonghwan Gwak and Muhammet Deveci and Vladimir Simic and Jurgita Antucheviciene},
  doi          = {10.1016/j.asoc.2024.111481},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111481},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mathematical analysis of big data analytics under bipolar complex fuzzy soft information},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple-attribute group decision-making approach using
power aggregation operators with CRITIC-WASPAS method under
2-dimensional linguistic intuitionistic fuzzy framework. <em>ASOC</em>,
<em>157</em>, 111466. (<a
href="https://doi.org/10.1016/j.asoc.2024.111466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 2-dimensional linguistic intuitionistic fuzzy variables (2DLIFVs) provide an efficient tool to model the cognitive information of experts in practical circumstances while considering the reliability of the evaluation results. It is a hybrid model representing qualitative information, which derives from integrating the 2-dimensional linguistic variables (2DLVs) with linguistic intuitionistic fuzzy numbers (LIFNs). In this work, we first define a novel score function to establish a proper ranking order among the 2DLIFVs. Next, we define a new distance measure under a 2-dimensional linguistic intuitionistic fuzzy framework to determine the difference between 2DLIFVs. Some mathematical properties and special cases of the defined distance measure are also discussed. The power average (PA) operators provide aggregation tools, allowing aggregated arguments to support each other in the aggregation process. Motivated by the notion of power average (PA) and power geometric (PG), we define four new aggregation operators (AOs) to aggregate 2DLIFVs by considering information correlation in terms of support degree during the aggregation process. Several mathematical properties of the proposed AOs are also investigated. In addition, a new integrated 2DLIF-CRITIC-WASPAS methodology is developed in the 2-dimensional linguistic intuitionistic fuzzy environment to deal with complex decision issues. In this method, a “Criteria Importance Through Intercriteria Correlation (CRITIC)” approach is used to obtain the associate weights of the attributes, and the “Weighted Aggregated Sum Product Assessment (WASPAS)” method is executed to establish a valid ranking order among the alternatives. Moreover, a real problem of venture capital investment related to renewable energy projects is considered to demonstrate the applicability of the proposed methodology. The evaluation process considers four aspects: the management team, service or product, finance, and the market, which are related to 14 attributes. A sensitivity analysis is carried out to explain the reliability of the obtained results. Comparing the proposed method with some extant methods establishes the strength and robustness of the formulated method in real-world situations.},
  archive      = {J_ASOC},
  author       = {Rajkumar Verma and Eduardo Álvarez-Miranda},
  doi          = {10.1016/j.asoc.2024.111466},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111466},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multiple-attribute group decision-making approach using power aggregation operators with CRITIC-WASPAS method under 2-dimensional linguistic intuitionistic fuzzy framework},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection algorithm using neighborhood equivalence
tolerance relation for incomplete decision systems. <em>ASOC</em>,
<em>157</em>, 111463. (<a
href="https://doi.org/10.1016/j.asoc.2024.111463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rough set is an important method for dealing with incomplete information systems. In incomplete information systems, the most common way to determine the relation between two samples is the tolerance relation. However, the condition for the tolerance relation to determine those samples may belong to the same category is very lenient, which makes the reduction rate low when using the rough set generated by this relation to select features. In response to the above problems, we design the neighborhood equivalence tolerance relation to solve them. Different from other improved tolerance relations, firstly, the relation designed in this paper does not require additional threshold to accomplish the above goals, which will avoid the trouble caused by the given threshold. Secondly, we notice that most of the current improvements for this kind of problems are computationally cumbersome, and the relation designed in this paper is simple and effective. Based on this, we construct a neighborhood rough set model that handles incomplete information by using this relation, introduce its properties, expound the properties that a reduction set should satisfy, quantify the importance of conditional attributes with attribute dependence degree, which provides the basis for the design of feature selection algorithm. Finally, the greedy strategy is used to design a forward feature selection algorithm. Experimental results show that the model is effective in dealing with incomplete information systems. The feature selection algorithm has the smallest size of the average reduced subset on twelve datasets, and maintains the accuracy of the classifier, which verifies that the feature selection algorithm can effectively deal with incomplete information systems.},
  archive      = {J_ASOC},
  author       = {Shangzhi Wu and Litai Wang and Shuyue Ge and Zheng Xiong and Jie Liu},
  doi          = {10.1016/j.asoc.2024.111463},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111463},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection algorithm using neighborhood equivalence tolerance relation for incomplete decision systems},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective diagnosis of schizophrenia using kernel ridge
regression-based optimized RVFL classifier. <em>ASOC</em>, <em>157</em>,
111457. (<a href="https://doi.org/10.1016/j.asoc.2024.111457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia (SCZ) is a severe mental and debilitating neuropsychiatric disorder that disrupts a person’s thought processes, emotions, and behavior. Due to misdiagnosis, self-denial, and social stigma, many SCZ cases go untreated. Magnetic resonance imaging (MRI) is an excellent noninvasive tool for soft tissue contrast imaging because it provides crucial data on tissue structure size, position, and shape. The Resnet50 network is a deep residual learning framework used for feature extraction. Random-vector functional link network (RVFL) is an example of a single-hidden-layer feedforward network in which input features and hidden layer features are fed to the output layer. In this paper, we introduced a kernel ridge regression-based random vector functional link (KRR-RVFL) classifier which focuses on addressing the linearity issues in RVFL by designating the kernel function in the input layer for the precise diagnosis of SCZ. The genetic algorithm (GA) seeks to minimize the loss function by optimizing the weights and biases of the KRR-RVFL network. The classification performance is investigated on the SCZ and cognitive normal (CN) subjects, collected from the available open neuro platform, including 99 participants. The results of the suggested network show superior performance to the recent state-of-the-art networks in terms of accuracy 93.66%, sensitivity 92.22%, specificity 95.17%, precision 95.33%, F-measure 93.74%, and G-mean 93.68%. The performance metrics demonstrated the applicability of this framework for assisting clinicians in the automatic, precise evaluation of SCZ.},
  archive      = {J_ASOC},
  author       = {S.A. Varaprasad and Tripti Goel and M. Tanveer and R. Murugan},
  doi          = {10.1016/j.asoc.2024.111457},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111457},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective diagnosis of schizophrenia using kernel ridge regression-based optimized RVFL classifier},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy efficient spiking deep residual network and binary
horse herd optimization espoused clustering protocol for wireless sensor
networks. <em>ASOC</em>, <em>157</em>, 111456. (<a
href="https://doi.org/10.1016/j.asoc.2024.111456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Wireless Sensor Networks (WSNs) plays several important application fields, specially monitoring events without any human interference. The sensor nodes (SNs) have lesser life time in wireless sensor network owing to its continual sensing with battery drains very rapidly. As a result of the heavy traffic, sensors nearby quickly expire and energy hole issues start to appear. To overcome these issues, an Energy Efficient Spiking Deep Residual Network and Binary Horse Herd Optimization Espoused Clustering Protocol for Wireless Sensor Networks (EE-SDRN-BHHO-CP-WSN) is proposed in this manuscript for sustaining the energy efficiency in the wireless sensor network. One of the main issues in wireless sensor network assisted applications is making use of the available energy. An optimum path selection is a key process of saving energy from sensor nodes to the sink. Spiking Deep Residual Network (SDRN) and Binary Horse Herd Optimization (BHHO) optimize energy efficiency in WSN. The cluster head (CH) is designated by effectual fitness function generated by multi-objectives. It benefits in low power consuming and decreases a count of idle sensor nodes. After CH selection, Binary horse herd optimization algorithm is considered for optimal route selection and data transmission by sink node. The EE-SDRN-BHHO-CP-WSN method is implemented in MATLAB and the efficiency of the proposed method is analyzed with different metrics, like network lifetime, number of alive nodes, number of dead nodes, throughput, energy consumption, packet delivery ratio (PDR). The proposed EE-SDRN-BHHO-CP-WSN method attains 32.35 %, 42.34 %, 49.27 % higher network lifetime, 26.34 %, 31.94 % and 39.35 % higher number of alive nodes and 46.26 %, 38.64 % and 27.83 % lower number of dead nodes compared with other existing models, such as Multi-objective CH based Energy-aware Optimized Routing Algorithm in WSN (MCH-EOR-WSN), Energy efficient CH selection utilizing improved Sparrow Search Algorithm in WSN (EECH-ISSA-WSN), and New Energy Aware CH Selection Algorithm for WSN (NEA-CHSA-WSN respectively).},
  archive      = {J_ASOC},
  author       = {M. Sudha and D. Chandrakala and S. Sreethar and A. Shrivindhya},
  doi          = {10.1016/j.asoc.2024.111456},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111456},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Energy efficient spiking deep residual network and binary horse herd optimization espoused clustering protocol for wireless sensor networks},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnosis of breast cancer using flexible pinball loss
support vector machine. <em>ASOC</em>, <em>157</em>, 111454. (<a
href="https://doi.org/10.1016/j.asoc.2024.111454">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a common disease that affects feminine health, making it an active area of research. Also, support vector machine with pinball loss (pin-SVM) is an efficient classification algorithm to address noise sensitivity and re-sampling instability. The pinball loss function uses a loss parameter τ ∈ [ 0 , 1 ] τ∈[0,1] which corresponds to the quantile level. However, the non-negativity condition on τ τ is not necessary, and it can be extended to the negative values for an improvement in classification accuracy. Also, instead of a positive loss parameter τ τ , two positive parameters, τ 1 τ1 and τ 2 τ2 are used in literature, which improve the generalization performance of the pin-SVM. Taking motivation from the aforementioned observations, in this paper, we propose an innovative loss function, termed the flexible pinball loss, which extends the parameters τ 1 τ1 and τ 2 τ2 to encompass negative values. This extension enables the function to take τ 1 τ1 and τ 2 τ2 values from − 1 −1 to 1 while preserving convexity. Subsequently, we integrate the proposed flexible pinball loss function into the support vector machine framework and propose a novel model named flexible pinball loss support vector machine (FP-SVM) for the prediction of breast cancer. FP-SVM provides loss to both incorrectly and correctly classified samples, leveraging the parameters τ 1 τ1 and τ 2 τ2 , respectively. Importantly, FP-SVM strategically traverses the maximum solution path, ensuring the preservation of convexity within the optimization problem. The proposed FP-SVM outperforms the baseline models in terms of accuracy, which is empirically supported by numerical experiments on 30 UCI and KEEL benchmark datasets. Furthermore, to show the efficacy of the proposed FP-SVM in real-world application, we performed experiments on publicly available breast cancer dataset (BreakHis), and the results demonstrate that the proposed FP-SVM outperforms the baseline models.},
  archive      = {J_ASOC},
  author       = {Anuradha Kumari and Mushir Akhtar and M. Tanveer and Mohd Arshad},
  doi          = {10.1016/j.asoc.2024.111454},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111454},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diagnosis of breast cancer using flexible pinball loss support vector machine},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated FBWM-FCM-DEMATEL model to assess and manage
the sustainability in the supply chain: A three-stage model based on the
consumers’ point of view. <em>ASOC</em>, <em>157</em>, 111281. (<a
href="https://doi.org/10.1016/j.asoc.2024.111281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While sustainability is recognized as a crucial aspect of supply chain (SC) management, its implementation within a SC is often fraught with challenges. These challenges encompass possible causal relationships and trade-offs among sustainable SC (SSC) practices, spanning all three dimensions of sustainability. Furthermore, the increasing importance of consumers’ point of view in the rapid and widespread communication era highlights the imperative to integrate this view into the SC’s sustainability efforts. This paper proposes a comprehensive model for assessing and managing the implementation of sustainability in the SC while effectively tackling these challenges and requirements. Specifically, across three distinct stages, the model enables (1) identifying the SSC practices from the consumers’ point of view and ranking them based on their contribution to the overall sustainability; (2) determining the causal relationships among the practices and assessing the sustainability status of the SC; and (3) prioritizing the practices and developing improvement scenarios to improve the overall sustainability of the SC. To support its objectives, the proposed model employs social media analysis and scenario planning, along with three key techniques, including the Fuzzy Best-Worst Method (FBWM) for ranking the practices, Fuzzy Cognitive Mapping (FCM) for determining the possible causal relationships among the practices and assessing the sustainability status, and Decision-Making Trial and Evaluation Laboratory (DEMATEL) for segmenting the practices based on their cause-and-effect relationships. Finally, the paper presents an empirical study in which the proposed model is applied to a mobile phone and smartphone (MPSP) SC, representing one of the most commonly consumed products in contemporary human life.},
  archive      = {J_ASOC},
  author       = {Sajad Jahangiri and Sajjad Shokouhyar},
  doi          = {10.1016/j.asoc.2024.111281},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111281},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated FBWM-FCM-DEMATEL model to assess and manage the sustainability in the supply chain: A three-stage model based on the consumers’ point of view},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Assessment of metaverse integration options in the higher
education institution using interval-valued t-spherical
fuzzy-WHMA-CEM-GLDS framework. <em>ASOC</em>, <em>156</em>, 111514. (<a
href="https://doi.org/10.1016/j.asoc.2024.111514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse integration education empowers class teaching activities with invaluable ability in higher education institutions. Currently, different styles of metaverse integration education systems have been developed. In such cases, the selection of suitable systems is a crucial task for promoting the digitalization of higher education, considering their own resources endowment. Therefore, this study develops a hybrid outranking model for evaluating the metaverse integration educational options in higher education institutions with the interval-valued T-spherical fuzzy (IVTSF) setting. This model integrates the weighted Heronian mean aggregation (WHMA) operator, the criteria importance through intercriteria correlation (CRITIC), and the gained and lost dominance score (GLDS) with IVTSF information. The IVTSF-WHMA operator based on CRITIC is established to combine the appraisal data, which elucidates the capacity to encompass the reciprocal dependencies existing between the input data and experts. Then, the cross-entropy model is employed in the IVTSF setting for calculating the criteria weights, which can measure the discrimination information between criteria. Next, the IVTSF-GLDS model is presented to evaluate and rank the alternatives, which can reflect the individual and group attitudes. Finally, a case study for selecting metaverse integration educational options of university courses about supply chain management is organized to demonstrate the application of this presented hybrid outranking model. The result shows that the integration option A o 4 Ao4 (Metaverse adoption in online learning) is the most sustainable method for the humanities, with the largest dominance score (0.176). Further, we confirm the advantages of this framework through sensitivity and comparison investigations. The findings of this work indicate that the developed outranking model provides a more reliable and synthetic tool for evaluating the metaverse integration educational options within higher uncertain and complex circumstances.},
  archive      = {J_ASOC},
  author       = {Weizhong Wang and Yi Wang and Yan Hu and Tinglong Zhang and Qun Wu},
  doi          = {10.1016/j.asoc.2024.111514},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111514},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Assessment of metaverse integration options in the higher education institution using interval-valued T-spherical fuzzy-WHMA-CEM-GLDS framework},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end point supervised object detection with low-level
instance features. <em>ASOC</em>, <em>156</em>, 111513. (<a
href="https://doi.org/10.1016/j.asoc.2024.111513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of point-level annotations in Point Supervision Object Detection (PSOD) has gained popularity due to its low labeling cost and the provision of specific location information. Previous studies have often relied on Multiple Instance Learning (MIL) to select and refine a low-quality pseudo box, which heavily relies on category information. This method concentrates on identifying the most distinctive objects in each category within an image. However, without the boundary constraints provided by Ground-Truth (GT), the detector may suffer from instance ambiguity problems, which can significantly deteriorate the model’s performance. To tackle these concerns, we propose an end-to-end object detection model called low-level instance features object detector (LIFOD) to mime low-level instance features information to refine the pseudo box. LIFOD utilizes a Low-level Feature Extractor (LFE) module to exploit low-level instance feature information to combine low-level feature information and category information, thus solving the challenge of instance ambiguity. To further alleviate the problem of local minima, we introduce bounding box regression in LIFOD and implement an end-to-end training strategy for point-supervised models to mitigate local minima. We evaluate our proposed LIFOD method on the public datasets COCO 2014 and 2017 and show that it outperforms the existing image-level and point-level supervised detection methods, and significantly reduces the gap in performance between PSOD and box-level object detection.},
  archive      = {J_ASOC},
  author       = {Xiangqi Chen and Chengzhuan Yang and Jiashuaizi Mo and Yunliang Jiang and Zhonglong Zheng},
  doi          = {10.1016/j.asoc.2024.111513},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111513},
  shortjournal = {Appl. Soft. Comput.},
  title        = {End-to-end point supervised object detection with low-level instance features},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling and multi-objective optimization for energy-aware
scheduling of distributed hybrid flow-shop. <em>ASOC</em>, <em>156</em>,
111508. (<a href="https://doi.org/10.1016/j.asoc.2024.111508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of economic globalization and sustainable manufacturing, energy-aware scheduling of distributed manufacturing systems has become a research hot topic. However, energy-aware scheduling of distributed hybrid flow-shop is rarely explored. Thus, this paper is the first attempt to study an energy-aware distributed hybrid flow-shop scheduling problem (DHFSP). We formulate a novel mathematical model of the DHFSP with minimizing makespan and total energy consumption ( TEC TEC ) criteria. A hybrid multi-objective iterated greedy (HMOIG) approach is proposed to address this energy-aware DHFSP. In this proposed HMOIG, firstly, a new energy-saving method is presented and introduced into the model for reducing TEC TEC criterion. Secondly, an integration initialization scheme is devised to produce initial solutions with high quality. Thirdly, two properties of DHFSP are used to invent a knowledge-based local search operator. Finally, we validate the effectiveness of each improvement component of HMOIG and compare it with other well-known multi-objective evolutionary algorithms on instances and a real-world case. Experimental results manifest that HMOIG is a promising method to solve this energy-aware DHFSP.},
  archive      = {J_ASOC},
  author       = {Chao Lu and Jiajun Zhou and Liang Gao and Xinyu Li and Junliang Wang},
  doi          = {10.1016/j.asoc.2024.111508},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111508},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modeling and multi-objective optimization for energy-aware scheduling of distributed hybrid flow-shop},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective simulated annealing algorithm for robotic
mixed-model two-sided assembly line balancing with setup times and
multiple constraints. <em>ASOC</em>, <em>156</em>, 111507. (<a
href="https://doi.org/10.1016/j.asoc.2024.111507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic mixed-model two-sided assembly lines (RMTALs) have become increasingly prevalent in manufacturing industries for productivity enhancement. However, only limited attention has been paid to the RMTAL balancing problems that contain both setup times and multiple constraints such as positional constraints, zoning constraints, and synchronism constraints. Given the coexistence of these aspects in real-world scenarios, this study aims to address the RMTAL balancing problem with setup times and the mentioned constraints. To formulate the problem, a mixed integer programming model is proposed to optimize cycle time and total energy consumption. To handle multiple constraints and setup times, a new method based on four-vector encoding is introduced. This method addresses the three constraints during the encoding phase using four intervals and five rules, while setup times are processed during the decoding phase. To solve the problem, a Pareto entropy-based two-mode multi-objective simulated annealing algorithm is developed. The algorithm employs variable neighborhood descent algorithm with switchable objectives to generate the initial Pareto archive. Subsequently, it selects the initial solution with minimal potential in the Pareto archive, alternating between two search modes of exploration and exploitation based on the entropy difference of Pareto solutions. Comparative experiments with four state-of-the-art algorithms on benchmark instances of varying scales demonstrate that the proposed algorithm outperforms the others in over 93% of instances for the hypervolume ratio and inverted generational distance metrics.},
  archive      = {J_ASOC},
  author       = {Yuzhe Huang and Buyun Sheng and Gaocai Fu and Ruiping Luo and Yingkang Lu},
  doi          = {10.1016/j.asoc.2024.111507},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111507},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective simulated annealing algorithm for robotic mixed-model two-sided assembly line balancing with setup times and multiple constraints},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A rolling bearing fault diagnosis technique based on
recurrence quantification analysis and bayesian optimization SVM.
<em>ASOC</em>, <em>156</em>, 111506. (<a
href="https://doi.org/10.1016/j.asoc.2024.111506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A rolling bearing fault diagnosis technique is proposed based on Recurrence Quantification Analysis (abbreviated as RQA) and Bayesian optimized Support Vector Machine (abbreviated as RQA-Bayes-SVM). Firstly, analyzing the vibration signal with recurrence plot and the nonlinear feature parameters are extracted with RQA, constructing a feature matrix describing the fault mode and fault degree comprehensively. Finally, Bayesian optimization algorithm is introduced for searching the best penalty factor C and kernel function parameter g of SVM and establishing an optimal Bayes-SVM model. Bearing datasets from CWRU is imported for diagnosis on fault mode and fault degree. The results show that the technique presents a good performance on fault mode diagnosis as well as fault degree distinction. Compared with common k-Nearest Neighbor (abbreviated as KNN) and Random Forest (abbreviated as RF) diagnosis models, Bayes-SVM has the best accuracy and stability, which indicates a potential value for engineering applications.},
  archive      = {J_ASOC},
  author       = {Bing Wang and Wentao Qiu and Xiong Hu and Wei Wang},
  doi          = {10.1016/j.asoc.2024.111506},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111506},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A rolling bearing fault diagnosis technique based on recurrence quantification analysis and bayesian optimization SVM},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A design of fuzzy rule-based classifier optimized through
softmax function and information entropy. <em>ASOC</em>, <em>156</em>,
111498. (<a href="https://doi.org/10.1016/j.asoc.2024.111498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Takagi–Sugeno–Kang (TSK) classifiers have achieved great success in many applications due to their interpretability and transparent model reliability for users. At present, however, how to evaluate classification results is still an unsolved issue for TSK classifiers. This study designs a fuzzy rule-based classifier based on TSK classifiers, the outputs of which for an instance can be considered as the membership grades that the instance belongs to all classes. Then, an information entropy-based method is proposed to estimate the certainty of the outputs, which facilitates the further evaluation of the classification results of the instance for users. If the confidence level is not high, users can reject the classification results, and use other more advanced classifiers or collect more information about the instance. Moreover, the developed mechanism is suitable for handling large data since the adaptive moment estimation algorithm is used to identify the parameters of it. Experimental results demonstrate that the developed mechanism outperforms several rule-based classifiers.},
  archive      = {J_ASOC},
  author       = {Xiaoyu Han and Xiubin Zhu and Witold Pedrycz and Almetwally M. Mostafa and Zhiwu Li},
  doi          = {10.1016/j.asoc.2024.111498},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111498},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A design of fuzzy rule-based classifier optimized through softmax function and information entropy},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A methodology for using players’ chat content for dynamic
difficulty adjustment in metaverse multiplayer games. <em>ASOC</em>,
<em>156</em>, 111497. (<a
href="https://doi.org/10.1016/j.asoc.2024.111497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalization of game difficulty is a critical task in leveraging artificial intelligence (AI) technologies to enhance player engagement in virtual worlds like metaverse. One of the key challenges in this area is developing methods for assessing a player’s perception of game difficulty. This information can be used to dynamically adjust the game difficulty to match the player’s skill level and preferences, which can improve the player’s experience and engagement. The existing approaches have limitations such as relying on costly external devices, requiring time-consuming feedback or questionnaires, and being specific to certain game genres and narratives. In this paper, we propose a new method called ChatDDA for evaluating a player’s perception of game difficulty by analyzing the content of their chat messages. Our method uses a pre-trained language model to extract semantic features from the chat messages, which are then used to train a feed-forward neural network to predict the player’s level of hopefulness or despair about succeeding in the game. Three pre-trained language models—BERT, RoBERTa, and Twitter-roBERTa—are fine-tuned on a purpose-built dataset of player chat messages of the popular multiplayer online game PlayerUnknown’s Battlegrounds (PUBG) tagged as expressing optimism or pessimism regarding game success. The results showed that our method can accurately predict a player’s perception of game difficulty, with an accuracy of 0.953 on the test dataset of player chat messages. This suggests that our method has the potential to enhance player engagement and immersion within the game, ultimately leading to more satisfying and enjoyable metaverse experiences.},
  archive      = {J_ASOC},
  author       = {Mohammad Mahdi Rezapour and Afsaneh Fatemi and Mohammad Ali Nematbakhsh},
  doi          = {10.1016/j.asoc.2024.111497},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111497},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A methodology for using players’ chat content for dynamic difficulty adjustment in metaverse multiplayer games},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced predictive modeling of rotating machinery remaining
useful life by using separable convolution backbone networks.
<em>ASOC</em>, <em>156</em>, 111493. (<a
href="https://doi.org/10.1016/j.asoc.2024.111493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of Remaining Useful Life (RUL) is a critical aspect in the field of prognostics health management (PHM). Striking a balance between prediction precision and model complexity is a substantial challenge when deploying deep learning (DL) methods in PHM. In response to this challenge, the present study introduces a novel approach called the Separable Convolution Backbone Network (SCBNet), which is designed to address the intricate mapping between degradation patterns and RUL by leveraging a structure based on separable convolutions. This innovative architecture aims to enhance prediction accuracy without unduly increasing the model intricacy. Furthermore, a backbone network is introduced to amplify the mapping features obtained from separable convolutions. To further strengthen the model, a novel strategy is devised to seamlessly integrate adjacent backbones. Through empirical experiments conducted on two bearing lifecycle degradation datasets, the proposed SCBNet demonstrates remarkable superiority over existing mainstream methods in terms of both prediction accuracy and model complexity. This study contributes valuable insights and a practical solution to enhance the effectiveness of DL-based methods in PHM applications.},
  archive      = {J_ASOC},
  author       = {Li Zou and Cong Ma and Jun Hu and Zechuan Yu and Kejia Zhuang},
  doi          = {10.1016/j.asoc.2024.111493},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111493},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced predictive modeling of rotating machinery remaining useful life by using separable convolution backbone networks},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep convolutional neural networks with genetic
algorithm-based synthetic minority over-sampling technique for improved
imbalanced data classification. <em>ASOC</em>, <em>156</em>, 111491. (<a
href="https://doi.org/10.1016/j.asoc.2024.111491">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data classification presents a challenge in machine learning, inducing biased model learning. Moreover, data dimensionality poses another challenge as it highly impacts classifier performance. This paper proposes a new deep-learning method that combines feature selection with oversampling to address these challenges. The proposed approach, GA-SMOTE-DCNN, integrates a genetic algorithm (GA) for feature selection, SMOTE for oversampling, and a deep 1D-convolutional neural network (DCNN) for classification. This study reveals that pre-splitting the data into training and testing sets before applying SMOTE results in higher accuracy, showing an improvement in accuracy ranging between 1.94% and 3.98% compared to post-SMOTE splitting for each dataset. This method achieved accuracy rates of 86.81% for the Balance Scale dataset, 86.15% for the Oil Spill dataset, 89.21% for the Yeast dataset, 91.32% for the Mammography dataset, 88.23% for the Australian credit dataset, and 89.53% for the German Credit dataset when compared with benchmark methods, underscoring its significance in tackling high-dimensional and imbalanced data classification problems. This method demonstrates scalability in effectively addressing challenges associated with high-dimensional and imbalanced data classification across various domains.},
  archive      = {J_ASOC},
  author       = {Suja A. Alex and J. Jesu Vedha Nayahi and Sanaa Kaddoura},
  doi          = {10.1016/j.asoc.2024.111491},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111491},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep convolutional neural networks with genetic algorithm-based synthetic minority over-sampling technique for improved imbalanced data classification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive large neighborhood search for the multi-vehicle
profitable tour problem with flexible compartments and mandatory
customers. <em>ASOC</em>, <em>156</em>, 111482. (<a
href="https://doi.org/10.1016/j.asoc.2024.111482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The home-refill delivery system is a business model that addresses the concerns of plastic waste and its impact on the environment. It allows customers to pick up their household goods at their doorsteps and refill them into their own containers. However, the difficulty in accessing customers’ locations and product consolidations are undeniable challenges. To overcome these issues, we introduce a new variant of the Profitable Tour Problem, named the multi-vehicle profitable tour problem with flexible compartments and mandatory customers (MVPTPFC-MC). The objective is to maximize the difference between the total collected profit and the traveling cost. We model the proposed problem as Mixed Integer Linear Programming and present an Adaptive Large Neighborhood Search (ALNS) algorithm to solve it. Our ALNS outperforms the commercial solver, Gurobi, and Large Neighborhood Search (LNS), as proven by giving better solutions within reasonable computational times. Both ALNS and LNS can obtain optimal solutions for all small instances and three better solutions than Gurobi for medium problems. Furthermore, ALNS is also robust and effective in solving large MVPTPFC-MC, as proven by resulting in better solutions within less CPU time than LNS. Finally, more analyses are conducted to justify the utilization of flexible compartment sizes by comparing it with fixed compartment sizes and to evaluate the robustness of MVPTPFC-MC. The results show that utilizing flexible compartment sizes can yield more benefits than fixed compartment sizes, particularly when the fleet size is limited, and there are fewer mandatory customers to serve.},
  archive      = {J_ASOC},
  author       = {Vincent F. Yu and Nabila Yuraisyah Salsabila and Aldy Gunawan and Anggun Nurfitriani Handoko},
  doi          = {10.1016/j.asoc.2024.111482},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111482},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive large neighborhood search for the multi-vehicle profitable tour problem with flexible compartments and mandatory customers},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pheromone-guided parallel rough hypercuboid attribute
reduction algorithm. <em>ASOC</em>, <em>156</em>, 111479. (<a
href="https://doi.org/10.1016/j.asoc.2024.111479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In knowledge discovery and data mining, removing redundant and irrelevant data attributes is crucial. Traditional algorithms, however, struggle with efficiency in high-dimensional big data contexts. To solve this problem, this paper proposes a novel global search attribute reduction method namely RHC-IGWO (Rough Hypercuboid Structure via Improved Grey Wolf Optimizer) by integrating the rough hypercuboid method and Improved Grey Wolf Optimizer (IGWO) with the pheromone mechanism. The algorithm is embedded into the Apache Spark parallel computing framework (Parallel computing Rough Hypercuboid Structure via Improved Grey Wolf Optimizer, PcRHC-IGWO) to accelerate and simplify the attribute reduction process. The algorithm divides the decision table into several independent blocks, introduces the pheromone mechanism to simulate the wolf pack behavior, and uses the IGWO for global search, which is conducive to efficient local search and global information sharing between individuals. The position of the individual is initialized by calculating the relevance between the attributes, and the pheromone value is dynamically updated according to the reduction quality. This allows automatically giving more search focus to more promising attribute regions. Experiments with public and real datasets demonstrate the RHC-IGWO algorithm&#39;s significant speedup and its efficacy in maintaining or enhancing classification accuracy. Particularly noteworthy is its performance on schizophrenia datasets, where the proposed method achieves outstanding classification accuracies of 86.2%, 88.89%, and 92.86% across various classifiers. These results not only demonstrate its effectiveness but also underline its potential in advanced data analysis scenarios. Additionally, on some large-scale datasets, the time required for processing has been reduced by 85.71%, showcasing the algorithm&#39;s efficiency in handling complex and voluminous data.},
  archive      = {J_ASOC},
  author       = {Weiping Ding and Hongcheng Yao and Hengrong Ju and Jiashuang Huang and Shu Jiang and Yuepeng Chen},
  doi          = {10.1016/j.asoc.2024.111479},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111479},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pheromone-guided parallel rough hypercuboid attribute reduction algorithm},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive threshold optimisation for online feature selection
using dynamic particle swarm optimisation in determining feature
relevancy and redundancy. <em>ASOC</em>, <em>156</em>, 111477. (<a
href="https://doi.org/10.1016/j.asoc.2024.111477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of data-driven decision-making, managing dynamic data streams characterised by evolving data distributions and high dimensionality presents a formidable challenge for online feature selection. This research addresses the challenge by developing innovative solutions in optimising Online Feature Selection (OFS) to manage feature irrelevancy and redundancy and rigorously validating the proposed method in high-dimensional dynamic data streams. The research employs a structured methodology, introducing a novel method: a Dynamic Particle Swarm Optimisation (PSO)-based Threshold Optimisation method. Dynamic PSO, injected on three benchmark methods of OFS (i.e., Online Streaming Feature Selection (OSFS), Fast-OSFS, and Scalable and Accurate Online Feature Selection for Big Data (SAOLA)), enabled the global best position to be dynamically adjusted to exploit the best solution found in streaming data. This research presents two optimisation variants of the proposed method: RedundantPSO, which optimises redundant features, and IrrelevantPSO, which optimises irrelevant features. Unlike the traditional PSO method on feature selection that uses feature encoding representation, the proposed method is underpinned by two contributions: adaptive threshold particle representation of particle swarm optimisation and enhanced fitness function using minimisation of mean absolute deviation of dependency among feature subsets. Adaptive threshold particle representation combines the feature encoding part with a novel aspect that defines a threshold value of significance level ranging from 0.01 to 0.1. This unique contribution sets the research apart in the field where it enables the adaptive adjustment of the threshold value based on incoming features. Next, the adaptation of Mean Absolute Deviation (MAD) was integrated into the fitness evaluation of PSO to gain a more accurate and reliable measure of fitness for feature selection. During the experiment phase, we analysed various benchmark datasets with highly redundant and relevant behaviour. Our analysis concluded that selecting the appropriate threshold values significantly improved model performance for high-redundancy datasets, highlighting the need for careful threshold selection. The experimental evaluations have revealed that integrating RedundantPSO with OSFS (OSFS+RedundantPSO) resulted in a remarkable enhancement of the OSFS method&#39;s accuracy, achieving an impressive average accuracy rate of 76.8%. This substantial improvement includes occasional spikes of up to 3.8% over the baseline OSFS accuracy, showcasing OSFS+RedundantPSO as the top-performing combination. Furthermore, Fast-OSFS + RedundantPSO outperformed Fast-OSFS by a slight margin, reaching an average accuracy of 72.7%, while SAOLA + RedundantPSO exhibited a substantial 3.1% increase in average accuracy over SAOLA, reaching 74.0%. It is noteworthy to highlight that the threshold value searched in the proposed method also significantly impacted the identification of the behaviour of the dataset, either high relevancy or redundancy, even in the absence of prior domain knowledge. A higher threshold signifies the evaluation of a more redundant feature space. In conclusion, the results demonstrated the significant contributions of the method in enhancing model accuracy, adapting to evolving data distributions, and optimising feature subsets with acceptable runtime. The research aims to advance the field of data science, such as cybersecurity, finance, healthcare and more, while empowering end-users to make informed decisions under changing data stream circumstances.},
  archive      = {J_ASOC},
  author       = {Ezzatul Akmal Kamaru Zaman and Azlin Ahmad and Azlinah Mohamed},
  doi          = {10.1016/j.asoc.2024.111477},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111477},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive threshold optimisation for online feature selection using dynamic particle swarm optimisation in determining feature relevancy and redundancy},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust incomplete large-scale group decision-making model
for metaverse metro operations and maintenance. <em>ASOC</em>,
<em>156</em>, 111472. (<a
href="https://doi.org/10.1016/j.asoc.2024.111472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse, constructed through digital technology, serves as a virtual realm intertwining with reality. Within this context, the challenge of evaluating data from diverse sources arises, and the application of large-scale group decision-making (LSGDM) methods emerges as a viable solution. Handling incomplete information and reducing dimensionality for large-scale decision-makers (DMs) is crucial in addressing complex decision-making problems. Moreover, addressing missing data is a fundamental and pivotal concern in tackling real-world decision challenges, given the ubiquitous presence of information gaps that cannot be straightforwardly integrated into decision models. Besides, the intricacies of LSGDM amplify this challenge by introducing a wealth of DMs, thereby augmenting the complexity and diversity of decision-related information. This paper proposes an approach to supplement missing data by double-dimensions. This paper explores various facets of similarity relationships within the data to enhance data completeness. Additionally, this paper categorizes DMs into clusters based on their relevance and establishes a two-stage consensus-reaching process (CRP) that takes into account both group sizes and individual consensus contributions. These CRPs play a crucial role in enhancing the overall consistency and consensus within the decision group. Subsequently, this paper applies a robust decision-making method rooted in MULTIMOORA (Multi-Objective Optimization by Ratio Analysis plus the complete MULTIplicative form) to rank decision objects. Finally, this paper employs this proposed methodology in a practical case study that involves evaluating the operational status of a metaverse’s urban construction metro system. Following these considerations, a comprehensive stability analysis of relevant parameters is conducted to guarantee the robustness and reliability of the decision-making process.},
  archive      = {J_ASOC},
  author       = {Wenhui Bai and Chao Zhang and Yanhui Zhai and Arun Kumar Sangaiah},
  doi          = {10.1016/j.asoc.2024.111472},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111472},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust incomplete large-scale group decision-making model for metaverse metro operations and maintenance},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-factor evolutionary algorithm for solving the
multi-tasking robust optimization problem on networked systems.
<em>ASOC</em>, <em>156</em>, 111470. (<a
href="https://doi.org/10.1016/j.asoc.2024.111470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Realistic networks constantly face complex environments and are vulnerable to external disturbances that can damage their structure and disrupt information flow. These challenges give rise to two critical optimization problems: network robustness optimization and robust influence maximization. These problems hold significant theoretical and practical importance, garnering increased attention in recent research. Models and optimizers have been developed to tackle related typological design and seed determination tasks, which provide rational candidates for practical scenarios. However, existing approaches have limitations. Firstly, while structural disturbances significantly impact network robustness, their effects on information propagation remain understudied. The quest for robust seed determination in the face of topological failures remains unresolved. Secondly, both network robustness optimization and robust influence maximization focus on leveraging structural network information, but the potential synergy between solutions for these problems has been overlooked in current research. There is a pressing need for highly efficient optimizers and versatile candidate solutions capable of withstanding various scenarios comprehensively. In response to these limitations, this study introduces multi-task optimization theory to address the network correlation optimization problem described above. We first systematically analyze the correlation between the two problems, demonstrating a positive relationship through experimental results. Building on this insight, we propose MFEA-Net, a multifactor evolutionary algorithm equipped with problem-specific operators. MFEA-Net concurrently tackles the challenges of network robustness optimization and robust influence maximization, considering multiple optimization scenarios simultaneously. It aims to uncover synergistic insights across different tasks during the optimization process. Our experimental results indicate that MFEA-Net surpasses existing methods in terms of performance. This research bridges a gap in the fields of robust influence maximization and network robust cooperative optimization, offering valuable guidance for addressing real-world network system design and optimization challenges.},
  archive      = {J_ASOC},
  author       = {Minghao Chen and Shuai Wang and Jiazhong Zhang},
  doi          = {10.1016/j.asoc.2024.111470},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111470},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-factor evolutionary algorithm for solving the multi-tasking robust optimization problem on networked systems},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rethinking superpixel segmentation from biologically
inspired mechanisms. <em>ASOC</em>, <em>156</em>, 111467. (<a
href="https://doi.org/10.1016/j.asoc.2024.111467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, advancements in deep learning-based superpixel segmentation methods have brought about improvements in both the efficiency and the performance of segmentation. However, a significant challenge remains in generating superpixels that strictly adhere to object boundaries while conveying rich visual significance, especially with limited training data, leading to the generation of blunt superpixel that encompass different semantics. To address these challenges, we propose a novel bio-inspired superpixel segmentation network (BINet), drawing inspiration from neural structures and visual mechanisms. Specifically, the BINet includes an Enhanced Screening Module (ESM) and a novel Boundary-Aware Label (BAL) for superpixel segmentation. The ESM enhances semantic information by simulating visual cortex interactive projection mechanisms, while the BAL emulates spatial frequency characteristics of visual cortical cells to generate superpixels with strong boundary adherence. Extensive experiments on BSDS500, NYUv2 and KITTI datasets show that our method achieves state-of-the-art performances but maintain satisfactory inference efficiency. Our code is available at https://github.com/zhaotingyu-ss/BINet .},
  archive      = {J_ASOC},
  author       = {TingYu Zhao and Bo Peng and Yuan Sun and DaiPeng Yang and ZhenGuang Zhang and Xi Wu},
  doi          = {10.1016/j.asoc.2024.111467},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111467},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rethinking superpixel segmentation from biologically inspired mechanisms},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based photoplethysmography biometric
authentication for continuous user verification. <em>ASOC</em>,
<em>156</em>, 111461. (<a
href="https://doi.org/10.1016/j.asoc.2024.111461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometric authentication methods have gained prominence as secure and convenient alternatives to traditional passwords and PINs. In this paper, we propose a novel approach for biometric authentication using photoplethysmography (PPG) signals and deep learning techniques. PPG is a non-invasive method that measures variations in blood volume within microvascular tissue beds, and it is typically used for monitoring heart rate and oxygen saturation. Our research leverages the unique characteristics of PPG signals to develop a robust and continuous user verification system. The primary goal of our study is to explore the feasibility and effectiveness of PPG-based biometric authentication, enabling a seamless and secure means of confirming the identity of individuals. We use a diverse dataset of PPG signals from various individuals, ensuring that it encompasses differences in skin tone, age, and other variables that can influence PPG signal characteristics. The collected data undergoes careful preprocessing, including noise removal, baseline correction, and heartbeat segmentation. For the core of our authentication system, we design and train a multiscale feature fusion deep learning (MFFD) model. This model, utilizing a Convolutional Neural Network (CNN) architecture, takes as input the relevant features extracted from PPG signals and learns to differentiate between individuals based on their unique PPG patterns. In this study, the input is constructed by gradually incorporating various features, beginning with a single PPG signal. In this study, the CNN model was trained independently, followed by the implementation of score fusion techniques. Our evaluation demonstrates the effectiveness of the PPG-based biometric authentication system, achieving high accuracy while addressing key security concerns. We consider false acceptance rate (FAR) and false rejection rate (FRR) to assess the system&#39;s performance. The model achieves the Accuracy of 99.5 % on BIDMC, 98.6 % on MIMIC, 99.2 % on CapnoBase dataset.},
  archive      = {J_ASOC},
  author       = {Li Wan and Kechen Liu and Hanan Abdullah Mengash and Nuha Alruwais and Mesfer Al Duhayyim and K. Venkatachalam},
  doi          = {10.1016/j.asoc.2024.111461},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111461},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based photoplethysmography biometric authentication for continuous user verification},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing personalized learning with explainable AI: A
chaotic particle swarm optimization based decision support system.
<em>ASOC</em>, <em>156</em>, 111451. (<a
href="https://doi.org/10.1016/j.asoc.2024.111451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of Educational Technology, personalized learning is pivotal, yet predicting students&#39; learning abilities based on learning styles and ICT remains challenging. We propose a decision support system using Machine Learning (ML), swarm intelligence, and explainable artificial (XAI) techniques to assess students&#39; performance. Our model employs Chaotic Particle Swarm Optimization (C-PSO) with Henon execution, outperforming Genetic Algorithm (GA), Ant Colony Optimization (ACO), Firefly Algorithm (FA), Bee Colony Optimization (BCO), Artificial Fish Swarm Algorithm (AFSA), Mayfly Optimization Algorithm (MFOA), Mother Optimization Algorithm (MOA), Fuzzy Self-Tuning PSO (FST-PSO). Evaluating efficiency, effectiveness, and solution quality reveals C-PSO&#39;s superiority. The study identifies the significant impact of ICT on self-progress and employs Spearman Rank correlation for statistical validation. Findings suggest C-PSO as an effective tool for optimizing educational data analysis and decision-making. Further exploration in real-world educational settings and comparative analyses with alternative optimization techniques are recommended for future research.},
  archive      = {J_ASOC},
  author       = {R. Parkavi and P. Karthikeyan and A. Sheik Abdullah},
  doi          = {10.1016/j.asoc.2024.111451},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111451},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhancing personalized learning with explainable AI: A chaotic particle swarm optimization based decision support system},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision transformer-based overlay processor for edge
computing. <em>ASOC</em>, <em>156</em>, 111421. (<a
href="https://doi.org/10.1016/j.asoc.2024.111421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerating Visual Neural Networks in Edge Computing environments is crucial for processing image and video data. Visual Neural Networks, including Convolutional Neural Networks and Vision Transformers, are central to image recognition, video analysis, and object detection tasks. Deploying these networks on edge devices and accelerating them can significantly enhance data processing speed and efficiency. The large number of parameters, complex computational flows, and various structural variants of Transformer models present both opportunities and challenges. We propose Vis-TOP (Vision Transformer Overlay Processor) , an overlay processor designed for all types of Vision Transformer models. Vis-TOP, distinct from coarse-grained general-purpose accelerators like GPUs and fine-grained custom designs, encapsulates Vision Transformer characteristics into a three-layer, two-level mapping structure, enabling flexible model switching without hardware architecture modifications. Concurrently, we designed a corresponding instruction bundle and hardware architecture within this mapping structure. We implemented the overlay processor design on the ZCU102 after quantizing the Swin Transformer model to 8-bit fixed points (fix_8). Experimentally, our throughput surpasses GPU implementation by 1.5 times. Our throughput per DSP is 2.2 to 11.7 times higher than that of existing Transformer-like accelerators. Overall, our approach satisfies real-time AI requirements in resource consumption and inference speed. Vis-TOP offers a cost-effective image processing solution for Edge Computing on reconfigurable devices, enhancing computational resource utilization, saving data transfer time and costs, and reducing latency.},
  archive      = {J_ASOC},
  author       = {Fang Liu and Zimeng Fan and Wei Hu and Dian Xu and Min Peng and Jing He and Yanxiang He},
  doi          = {10.1016/j.asoc.2024.111421},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111421},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Vision transformer-based overlay processor for edge computing},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An encoder–decoder architecture with fourier attention for
chaotic time series multi-step prediction. <em>ASOC</em>, <em>156</em>,
111409. (<a href="https://doi.org/10.1016/j.asoc.2024.111409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-step prediction of chaotic time series has been a challenging problem. An encoder–decoder architecture based on novel Fourier attention was proposed, called FGNet, applied to chaotic time series multi-step prediction. Two perspectives were carefully re-considered in modeling, one was the integrated use of frequency and temporal domain information, the other was the fusion of channel information and series information. The encoder was a self-attention architecture with Fourier attention module for capturing dynamic information in time series. Correspondingly, the decoder used the Gated Recurrent Unit (GRU) module with identity mapping, and the decoder was used to model the representation based on temporal dependency. For the interpretability deficiency of Fourier attention in FNet, which was proposed by google, a novel Fourier attention module was proposed. Specifically, the frequency domain features were extracted using Fourier transform, series and channel features were integrated capture through channel swapping coupling. Then these features were fused in the frequency domain space to enhance feature representation and their interpretability is achieved through inverse Fourier transform. Simulated data (Mackey–Glass system and Lorenz) and real data (sunspot time series) were used to verify that FGNet proposed achieves pleasantly surprising multi-step prediction performance in practice.},
  archive      = {J_ASOC},
  author       = {Ke Fu and He Li and Xiaotian Shi},
  doi          = {10.1016/j.asoc.2024.111409},
  journal      = {Applied Soft Computing},
  month        = {5},
  pages        = {111409},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An encoder–decoder architecture with fourier attention for chaotic time series multi-step prediction},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Determination of best renewable energy sources in india
using SWARA-ARAS in confidence level based interval-valued fermatean
fuzzy environment. <em>ASOC</em>, <em>155</em>, 111495. (<a
href="https://doi.org/10.1016/j.asoc.2024.111495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The requirement for renewable energy sources arises from the depletion of fossil fuels and the increasing energy demand. A case study in India has been conducted in this context to identify the most promising renewable energy sources. A novel multi-attribute group decision-making (MAGDM) method integrating stepwise weight assessment ratio analysis (SWARA) and additive ratio assessment (ARAS) has been proposed for this purpose. We have used interval-valued Fermatean fuzzy numbers for data representation due to their ability to accommodate a broad range of fuzzy information. By introducing confidence level-based aggregation operators in an interval-valued Fermatean fuzzy environment, we avoid erroneous assessments by decision experts. We have employed objective and subjective weight determination approaches to determine decision experts’ weights. The SWARA method has been used for attribute weight evaluation due to its simplicity and efficiency, and the ARAS method is employed for alternative ranking and optimal choice selection using the utility degree. The findings show the potential to expand solar and wind power, whereas geothermal energy is not yet suitable for widespread deployment because of its early stage of development. We find the technical and environmental aspects to be especially significant during the decision-making process. To demonstrate the proposed method’s applicability, we have compared the results with existing MAGDM methods. To establish its stability, we conducted a sensitivity analysis. The presented MAGDM method can help make informed decisions regarding implementing solar energy as a primary source of energy supply in India, thereby aiding decision-making for policymakers, regulators, energy planners, and stakeholders.},
  archive      = {J_ASOC},
  author       = {Mijanur Rahaman Seikh and Prayosi Chatterjee},
  doi          = {10.1016/j.asoc.2024.111495},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111495},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Determination of best renewable energy sources in india using SWARA-ARAS in confidence level based interval-valued fermatean fuzzy environment},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retraction notice to “on-line system identification of
complex systems using chebyshev neural networks” [appl. Soft comput.
7(2007) 364–372]. <em>ASOC</em>, <em>155</em>, 111484. (<a
href="https://doi.org/10.1016/j.asoc.2024.111484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {S. Purwar and I.N. Kar and A.N. Jha},
  doi          = {10.1016/j.asoc.2024.111484},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111484},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Retraction notice to “On-line system identification of complex systems using chebyshev neural networks” [Appl. soft comput. 7(2007) 364–372]},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Editorial for special issue on “expert decision making for
data analytics with applications.” <em>ASOC</em>, <em>155</em>, 111480.
(<a href="https://doi.org/10.1016/j.asoc.2024.111480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Kevin Kam Fung Yuen and Jenq-Shiou Leu and Alessio Ishizaka and Hissam Tawfik and Frans Coenen},
  doi          = {10.1016/j.asoc.2024.111480},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111480},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Editorial for special issue on “Expert decision making for data analytics with applications”},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task supervised contrastive learning for chest x-ray
diagnosis: A two-stage hierarchical classification framework for
COVID-19 diagnosis. <em>ASOC</em>, <em>155</em>, 111478. (<a
href="https://doi.org/10.1016/j.asoc.2024.111478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Global pandemics have posed great challenges, such as limited samples and the scarcity of carefully curated datasets, in creating reliable models for chest X-ray (CXR) diagnosis. A common approach is to leverage pre-trained deep learning models using large datasets and then fine-tune model parameters on the target CXR dataset. However, in the presence of data bias, it is prone to shortcut learning, where the model learns non-disease-related features. To enable the model to learn robust features from the target dataset, we propose a new training framework called Multi-task Supervised Contrastive Learning (MTSCL). In MTSCL, we concatenate the weighted representations of two supervised tasks and implicitly contrast the representations from the large-scale and target datasets. It helps the model focus on disease-related features in the target dataset. This paper applies the MTSCL model to a two-stage hierarchical classification framework. In the first stage, the MTSCL model detects lung abnormalities, and then tree-based models are applied to classify the abnormalities as pneumonia or COVID-19 in the second stage. The experimental results show that, in the first stage, the MTSCL model outperforms other training methods in terms of generalization with an average AUC improvement of 1.03%-5.33% and has better abnormality localization capability. In the second stage, the model generalization is enhanced by incorporating lung masks and using MTSCL model-generated lesion masks for image preprocessing. Furthermore, the addition of wavelet components manipulation on top of the previous improvements further increases the generalization performance for distinguishing COVID-19 from typical pneumonia with an average AUC improvement of 12.38%. Overall, the two-stage hierarchical classification framework exhibits superior generalization ability for normal, pneumonia, and COVID-19 classification, achieving an average accuracy of 75.19% in two external datasets, outperforming state-of-the-art deep learning models (71.00%-74.08%) and classical machine learning models (67.80%-68.67%).},
  archive      = {J_ASOC},
  author       = {Guan-Ying Chen and Chih-Ting Lin},
  doi          = {10.1016/j.asoc.2024.111478},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111478},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task supervised contrastive learning for chest X-ray diagnosis: A two-stage hierarchical classification framework for COVID-19 diagnosis},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A consensus-based single valued neutrosophic model for
selection of educational vendors under metaverse with extended reality.
<em>ASOC</em>, <em>155</em>, 111476. (<a
href="https://doi.org/10.1016/j.asoc.2024.111476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting the right educational vendors in the Metaverse, particularly those utilizing extended reality (XR), is crucial for creating an engaging and immersive learning environment. Careful vendor selection ensures the delivery of high-quality XR educational content, considering factors such as seamless integration with different XR devices and addressing associated challenges, which can affect the comfort and well-being of learners.This paper proposes a decision-making model to address the complex task of selecting educational vendors within the Metaverse with a focus on extended reality (XR). Recognizing the challenges inherent in such selections, this work presents a consensus-based measurement of alternatives and ranking according to compromise solution (MARCOS) model, incorporating single-valued neutrosophic (SVN) information and SVN Dombi-Archimedean weighted aggregation operators. Attribute weights crucial for decision-making are determined through an optimization tool employing cross-entropy measures with partial weight information. The evaluation encompasses nine criteria, including cost, security, privacy, pedagogical alignment, configuration, maintenance, level of control, output tools, degree of immersion, availability of XR learning contents, and time. Four leading vendors including Victory XR, ARuVR, zSpace, and Varjo Technologies are assessed, with zSpace emerging as the most favorable alternative with the highest overall utility value of 0.6889. The paper further conducts sensitivity analysis and performance comparisons to establish the robustness of the ranking results, providing a comprehensive methodology for educational stakeholders addressing the evolving dynamics of the Metaverse.},
  archive      = {J_ASOC},
  author       = {Abhijit Saha and Renuka Kolandasamy and Prasenjit Chatterjee and Jurgita Antucheviciene},
  doi          = {10.1016/j.asoc.2024.111476},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111476},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A consensus-based single valued neutrosophic model for selection of educational vendors under metaverse with extended reality},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementation of caputo type fractional derivative chain
rule on back propagation algorithm. <em>ASOC</em>, <em>155</em>, 111475.
(<a href="https://doi.org/10.1016/j.asoc.2024.111475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractional gradient computation is a challenging task for neural networks. In this study, the brief history of fractional derivation is abstracted, and the core framework of the Faà di Bruno formula is implemented to the fractional gradient computation problem. As an analytical approach to solve the chain rule problem of fractional derivatives, the use of the Faà di Bruno formula for the Caputo-type fractional derivative is proposed. When the fractional gradient is calculated using the proposed approach, the problem of determining the bounds of the formula for calculating the Caputo derivative is addressed. As a consequence, the fundamental problem with the fractional back-propagation algorithm is solved analytically, paving the way for the use of any differentiable and integrable activation function in case they are stable. The development of the algorithm and its practical implementation on the photovoltaic fault detection data-set is investigated. The reliability of the neural network metrics is also investigated using analysis of variance (ANOVA), the results are then presented to decide which are the best metrics and the best fractional order. Ordinary back-propagation, fractional back-propagation with and without L 2 regularization and momentum are presented in the results to show the advantages of using L 2 regularization and momentum in fractional order gradient. Consequently, the test metrics are reliable but cannot be separated from each other. By changing the batch size from 2 to full batches, the proposed system performs better with bigger batches, but adaptive moment estimation (ADAM) performs better with small batches. The cross validation shows the performance of back propagate ion neural networks have better performance than the ordinary neural networks. It is interesting that the best order for a specific data-set cannot be determined because it depends on the batch size, number of epochs and the cross-validation folds .},
  archive      = {J_ASOC},
  author       = {Mücahid Candan and Mete Çubukçu},
  doi          = {10.1016/j.asoc.2024.111475},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111475},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Implementation of caputo type fractional derivative chain rule on back propagation algorithm},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Reinforcement learning based bilevel real-time pricing
strategy for a smart grid with distributed energy resources.
<em>ASOC</em>, <em>155</em>, 111474. (<a
href="https://doi.org/10.1016/j.asoc.2024.111474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of flexible loads, distributed energy resources, and other technologies is becoming common in advance power and energy systems. However, the integration also presents significant challenges due to the increasing complexity and uncertainty. To effectively manage these resources, an adaptive pricing mechanism is needed that can account for their variable availability. Based on this, we propose a new bilevel real-time pricing model that considers different distributed energy resources, uncertainty of renewable energy generation, carbon trading mechanisms, and grid fluctuations. Specifically, the upper-level optimization problem aims to maximize the total profit of the supplier that applies Q-learning algorithm. The lower-level optimization problem addresses the need for each user to make optimal power consumption decisions by constructing an individual Markov Decision Process framework for each user. The bilevel model achieves an effective balance of interests between the supplier and users by simultaneously considering both the upper-level and lower-level optimization problems. Additionally, our model can be efficiently solved using the distributed algorithm without the need to acquire transition probabilities. Simulation results show that the method is highly effective in balancing power supply and demand between the supplier and users, reducing carbon emissions, and mitigating power fluctuations.},
  archive      = {J_ASOC},
  author       = {Jingqi Wang and Yan Gao and Renjie Li},
  doi          = {10.1016/j.asoc.2024.111474},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111474},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Reinforcement learning based bilevel real-time pricing strategy for a smart grid with distributed energy resources},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to select plan in emergency decision-making? A two-stage
method with case-based reasoning and prospect theory. <em>ASOC</em>,
<em>155</em>, 111473. (<a
href="https://doi.org/10.1016/j.asoc.2024.111473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency events characterized by high uncertainty and complexity bring tremendous pressure and challenges to our society. Emergency decision-making (EDM) is an effective way to mitigate the losses caused by emergency events. The generation of alternatives and the selection of the best emergency plan are crucial to the successful management of an emergency event. To improve the efficiency of EDM, this study proposes a novel two-stage EDM method. In the first stage, to fully represent emergency events and measure their similarities, we adopt heterogeneous multi-attribute information in the dynamic case-based reasoning (CBR) model to generate alternatives. After that we provide an adaptive reaching process model with a dynamic interactive strategy to obtain similar historical emergency events. In the second stage, we apply the prosper theory and the linguistic term information to select the optimal emergency plan. To test the robustness of the two stage EDM methodology, we verify the feasibility and effectiveness of the method with a case study about evaluating emergency plans for public health emergencies. Lastly, we conclude this work with more sensitivity analysis and some discussions about future research.},
  archive      = {J_ASOC},
  author       = {Wenbo Zhang and Xi Chen and Jie Mao and Feng Ke and Haiming Liang},
  doi          = {10.1016/j.asoc.2024.111473},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111473},
  shortjournal = {Appl. Soft. Comput.},
  title        = {How to select plan in emergency decision-making? a two-stage method with case-based reasoning and prospect theory},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimizing the searching time of multiple targets in
uncertain environments with multiple UAVs. <em>ASOC</em>, <em>155</em>,
111471. (<a href="https://doi.org/10.1016/j.asoc.2024.111471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of this paper is the use of Unmanned Aerial Vehicles (UAVs) for searching multiple targets under uncertain conditions in the minimal possible time. The problem, known as Minimum Time Search (MTS), belongs to the Probabilistic Search (PS) field and addresses critical missions, such as search &amp; rescue, and military surveillance. These operations, characterized by complex and uncertain environments, demand efficient UAV trajectory optimization. The multi-target version of PS introduces additional challenges, due to their higher complexity and the need to wisely distribute the UAV’s efforts among multiple targets. In order to tackle the under-explored multi-target aspect of MTS, we optimize the time to find all targets with new Ant Colony Optimization (ACO)-based planner. This novel optimization criterion is formulated using Bayes’ theory, considering probability models of the targets (initial belief and motion model) and the sensor likelihood. Our work contributes significantly by (i) developing an objective function tailored for multi-target MTS, (ii) proposing an ACO-based planner designed to effectively handle the complexities of multiple moving targets, and (iii) introducing a novel constructive heuristic that is used by the ACO-based planner, specifically designed for the multi-target MTS problem. The efficacy of our approach is demonstrated through comprehensive analysis and validation across various scenarios, showing superior performance over existing methods in complex multi-target MTS problems.},
  archive      = {J_ASOC},
  author       = {Sara Pérez-Carabaza and Eva Besada-Portas and José A. López-Orozco},
  doi          = {10.1016/j.asoc.2024.111471},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111471},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Minimizing the searching time of multiple targets in uncertain environments with multiple UAVs},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Association mining based deep learning approach for
financial time-series forecasting. <em>ASOC</em>, <em>155</em>, 111469.
(<a href="https://doi.org/10.1016/j.asoc.2024.111469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stock market plays a vital role in a country’s economy, serving as a platform for companies to raise capital and enabling investors to share in their growth and success. The market is very unpredictable, characterized by non-linear variations and sudden fluctuations driven by a multitude of external factors. In the past, several traditional, deep learning, machine learning-based, and hybrid solutions have been put forth to estimate stock trends accurately. The existing techniques fail to achieve the desired accuracy due to the complex, non-linear, and random behavior of the stock time series. Moreover, the hybrid approaches in this domain have several shortcomings, such as shifting procedure dependency, optimal component count, computationally expensive, and many more. To address these challenges, and improve both accuracy and reliability, the current approach proposes a hybrid approach integrating association rule mining with deep learning models. The association rule mining aims to quantify the impact of a critical external factor (other companies’ stock trends) on a target company stock. The identified associated companies define the data to be fed to the neural models. The current approach implements a multivariate long short-term memory neural architecture for the stock price prediction task. Through performance analysis conducted on the National Stock Exchange (NIFTY50) dataset focusing on four well-known companies, the proposed hybrid approach demonstrates significantly improved prediction accuracy compared to benchmark methods. The proposed approach has demonstrated an average 8%–10% improvement in the prediction results of the benchmark approach. Moreover, the prediction performance of the proposed approach is statistically verified using the T-test.},
  archive      = {J_ASOC},
  author       = {Tanya Srivastava and Ishita Mullick and Jatin Bedi},
  doi          = {10.1016/j.asoc.2024.111469},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111469},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Association mining based deep learning approach for financial time-series forecasting},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Classification assessment tool: A program to measure the
uncertainty of classification models in terms of class-level metrics.
<em>ASOC</em>, <em>155</em>, 111468. (<a
href="https://doi.org/10.1016/j.asoc.2024.111468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy assessments are important steps of classifications and get higher relevance with the soar of machine and deep learning techniques. We provided a method for quick model evaluations with several options: calculate the class level accuracy metrics for as many models and classes as needed; calculate model stability using random subsets of the testing data. The outputs are single calculations, summaries of the repetitions, and/or all accuracy results per repetitions. Using the application, we demonstrated the possibilities of the function and analyzed the accuracies of three experiments. We found that some popular metrics, the binary Overall Accuracy, Sensitivity, Precision, and Specificity, as well as ROC curve, can provide false results when the true negative cases dominate. F1-score, Intersection over Union and the Matthews correlation coefficient were reliable in all experiments. Medians and interquartile ranges (IQR) of the repeated sampling from the testing dataset showed that IQR were small when a model was almost perfect or completely unacceptable; thus, IQR reflected the model stability, reproducibility. We found that there were no general, statistically justified relationship with the median and IQR, furthermore, correlations of accuracy metrics varied by experiments, too. Accordingly, a multi-metric evaluation is suggested instead of a single metric.},
  archive      = {J_ASOC},
  author       = {Szilárd Szabó and Imre J. Holb and Vanda Éva Abriha-Molnár and Gábor Szatmári and Sudhir Kumar Singh and Dávid Abriha},
  doi          = {10.1016/j.asoc.2024.111468},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111468},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Classification assessment tool: A program to measure the uncertainty of classification models in terms of class-level metrics},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Influence measure-based large-scale group decision making
with linear uncertain preference relations. <em>ASOC</em>, <em>155</em>,
111464. (<a href="https://doi.org/10.1016/j.asoc.2024.111464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on the problem of large-scale group decision-making (LSGDM) based on influence measure under linear uncertain preferences. The value of this research is that it improves the performance of the current clustering algorithms, increases the efficiency of consensus reaching for major decision-making events, thus reducing the cost of feedback adjustments, and at the same time reflecting the risk attitude of the decision-makers (DMs) during situations of uncertainty. First, an approach for measuring the influence of the DMs is presented. Based on the influence measure, a clustering method is proposed to classify the experts into subgroups. Then, a method for determining the weights of the subgroups is developed by combining the influence and credibility of the subgroup leader as well as the intra-subgroup consensus level. Subsequently, a feedback adjustment method is provided to reduce the disagreement among the DMs by considering the unit adjustment cost and the corresponding adjustment willingness of the DMs. Further, based on the trust risk and credibility of the DMs, a dual management mechanism of non-cooperative behavior is established to hasten consensus. Finally, a case study is presented to demonstrate the practicality of the suggested approach, while simulation experiments and comparative analysis are conducted to verify the model.},
  archive      = {J_ASOC},
  author       = {Kaixin Gong and Weimin Ma and Wenjing Lei and Mark Goh and Zitong Ren},
  doi          = {10.1016/j.asoc.2024.111464},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111464},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Influence measure-based large-scale group decision making with linear uncertain preference relations},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neutrosophic fusion of multimodal brain images: Integrating
neutrosophic entropy and feature extraction. <em>ASOC</em>,
<em>155</em>, 111462. (<a
href="https://doi.org/10.1016/j.asoc.2024.111462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid growth of imaging modalities in clinical analysis and the indispensable requirement of brain images from various imaging modalities for diagnosing a disease, multi-modal brain image fusion has become an intriguing problem among researchers. Thus, the main motive of this paper is to obtain all the necessary information about the source images in a single fused image of high contrast with clear boundaries and without unnecessary noise. Accordingly, this paper proposes a new approach to eradicate the indeterministic and uncertainty present in brain images with the benefits of the neutrosophic set. Also, a novel neutrosophic entropy is developed to acquire the accurate edge details of the image. In addition, to extract requisite features from the images, Tamura features are implemented. Finally, the fusion is performed by comparing the extracted feature values from the images. Subsequently, the experiment is conducted with three different sets of brain datasets and compared with six other fusion algorithms to prove the efficiency of the proposed method. To support this, qualitative and quantitative assessments for each dataset are executed, and the results are tabulated. The results clearly show that the information is accurately represented, preserving the curves and edges. Moreover, this algorithm consistently produces the highest metric values and remains reasonably efficient in time consumption, thus balancing performance and efficiency.},
  archive      = {J_ASOC},
  author       = {K.G. Lavanya and P. Dhanalakshmi and M. Nandhini},
  doi          = {10.1016/j.asoc.2024.111462},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111462},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic fusion of multimodal brain images: Integrating neutrosophic entropy and feature extraction},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hyperscale data analysis oriented optimization mechanisms
for higher education management systems platforms with evolutionary
intelligence. <em>ASOC</em>, <em>155</em>, 111460. (<a
href="https://doi.org/10.1016/j.asoc.2024.111460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher education institutions face vast amounts of complex data from various sources. Extracting meaningful insights from this data can improve student outcomes and institutional performance. This paper proposes novel evolutionary optimization mechanisms for higher education management systems to enable hyperscale data analysis. An integrated framework is developed, combining educational data mining, learning analytics, and evolutionary computation techniques. The methodology employs a multi-objective evolutionary algorithm with dynamic resource allocation to optimize multiple objectives simultaneously. Adaptive learning control is incorporated to balance exploration and exploitation. Theoretical analyses provide convergence proofs for the proposed algorithms. Comprehensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed mechanisms compared to state-of-the-art approaches. The results show significant performance gains regarding solution quality, scalability, and computational efficiency. The proposed techniques can be a foundation for developing the next generation of intelligent higher education management systems.},
  archive      = {J_ASOC},
  author       = {Ru Zhang and Zihan Meng and Hongli Wang and Tianhe Liu and Guan Wang and Lu Zheng and Cong Wang},
  doi          = {10.1016/j.asoc.2024.111460},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111460},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hyperscale data analysis oriented optimization mechanisms for higher education management systems platforms with evolutionary intelligence},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent framework of upgraded CapsNets with massive
transmissibility data for identifying damage in bridges. <em>ASOC</em>,
<em>155</em>, 111459. (<a
href="https://doi.org/10.1016/j.asoc.2024.111459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural monitoring systems installed on bridges are capable of capturing large-scale dynamic responses online and in real-time. The response data of the bridge under different loading conditions is used for condition assessment of the bridge. There is a chasm between monitoring data and damage assessment due to the difficulty in revealing the relationship between the massive monitoring data and damage. this study proposes an intelligent framework for identifying (localizing and quantifying) damage in a bridge from massive acceleration responses. The framework features (i) physical big-data quantity and (ii) an intelligent identification model. The former specifies a mechanical quantity termed damage pattern spectrum of power-spectrum-density transmissibility (PSDT), the PSDT-damage pattern spectrum for short, constructed to protrude damage information from massive acceleration responses; the latter specifies an upgraded Capsules Networks (CapsNets), Up-CapsNets for short, which intelligently locate and quantify damage via deep learning with inputted PSDT-damage pattern spectrum. The proposed intelligent framework is verified through a numerically truss bridge model and an experimental suspension bridge model. The results demonstrate that this intelligent framework can identify locations and severity of damage with greater accuracy, stronger immunity to noise, and higher generalization than existing convolutional neural networks (CNNs) and CapsNets. Such a peculiarity can be attributed to two points: (i) the sophisticated function of the Up-CapsNets in recognizing damage patterns over conventional deep learning networks; and (ii) the unique qualification of PSDT-damage pattern spectrum in refining damage information over raw acceleration responses. The proposed intelligent framework provides a viable approach for monitoring the condition of a bridge by combining deep-learning deduction and big-data dynamic responses.},
  archive      = {J_ASOC},
  author       = {Shuai Li and Maosen Cao and Mahmoud Bayat and Dragoslav Sumarac and Jie Wang},
  doi          = {10.1016/j.asoc.2024.111459},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111459},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent framework of upgraded CapsNets with massive transmissibility data for identifying damage in bridges},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A data-driven implicit deep adaptive neuro-fuzzy inference
system capable of manifold learning for function approximation.
<em>ASOC</em>, <em>155</em>, 111458. (<a
href="https://doi.org/10.1016/j.asoc.2024.111458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy Neural Networks (FNN) have the ability of decision-making based on constructing semi-ellipsoidal clusters in the input space as the antecedent parts of their fuzzy rules. To determine the output value for each input instance, FNNs consider its membership degree to different sub-regions of the input space. However, forming such meaningful sub-regions is not possible in all applications due to the nonlinear interactions among input variables and their low information gain. Indeed, the samples could be distributed on a manifold in the input space. Therefore, to cover the input space, we need lots of rules, each representing a small region of input space. This issue decreases the generalization ability of the model along with its explainability. Consequently, to efficiently form fuzzy rules, first, it is necessary to unfold the manifold by mapping the samples to an appropriate embedding space. Next, the fuzzy rules in the form of semi-ellipsoidal regions should be constructed in this extracted feature space. Deep Fuzzy Neural Networks address this problem by representation learning through stacking multiple cascade mapping layers. In this paper, we propose a novel approach for nonlinear function approximation and time-series prediction problems, based on using the kernel trick to implicitly learn the mapping function to the new feature space. Moreover, to initialize the fuzzy rules, a KNN-based method using the kernel trick is proposed. A hierarchical Levenberg–Marquardt approach is applied to learn the model’s parameters. The performance and structure of the proposed method are studied and compared with some other relevant methods in synthetic and real-world benchmarks. Based on these experiments, the proposed method has the best performance with the most parsimonious architecture. According to these experiments, the test RMSE of the proposed method is 0.002 for Mc-Glass chaotic time-series prediction, 0.015 for a Nonlinear dynamic system identification, 0.0345 for Box–Jenkins nonlinear system identification, 0.0609 for Fuel consumption prediction of automobiles, 10.24 for Sydney stock price tracking, and 0.595 for California housing.},
  archive      = {J_ASOC},
  author       = {Armin Salimi-Badr},
  doi          = {10.1016/j.asoc.2024.111458},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111458},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A data-driven implicit deep adaptive neuro-fuzzy inference system capable of manifold learning for function approximation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Efficient traffic-based IoT device identification using a
feature selection approach with lévy flight-based sine chaotic sub-swarm
binary honey badger algorithm. <em>ASOC</em>, <em>155</em>, 111455. (<a
href="https://doi.org/10.1016/j.asoc.2024.111455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) refers to the various devices connected to the Internet, enabling them to communicate and transmit data with each other. The rapid development of the IoT also brings security and other problems in cyberspace. In this case, device identification is a crucial tool for IoT security issues, which can detect and prevent cyber-attacks. However, device identification has some challenges on IoT traffic datasets, such as large-scale and high-dimensional sparse datasets, features prone to security vulnerabilities, and identification of IP and non-IP devices, which affects the performance of classifiers. Feature selection can be seen as an effective data preprocessing technique in IoT device identification, which may improve the performance of classification and reduce the computational complexity of IoT device identification. In this paper, we propose a traffic-based IoT device identification model using a novel wrapper feature selection approach based on an improved and efficient method, which we call Lévy flight-based sine chaotic sub-swarm binary honey badger algorithm (LS 2 -BHBA). Specifically, four improved factors are employed in LS 2 -BHBA to expand the search scope, balance the exploration and exploitation phases, and enhance the search capability. In addition, a binary mechanism is implemented to enhance the suitability of the proposed LS 2 -BHBA for feature selection in IoT device identification. The experimental results on several real IoT traffic datasets denote that LS 2 -BHBA can reduce the number of features to 10%, and the classification accuracy can reach 98%, which outperforms some classical and latest comparison algorithms in the feature selection of IoT device identification.},
  archive      = {J_ASOC},
  author       = {Boxiong Wang and Hui Kang and Geng Sun and Jiahui Li},
  doi          = {10.1016/j.asoc.2024.111455},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111455},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient traffic-based IoT device identification using a feature selection approach with lévy flight-based sine chaotic sub-swarm binary honey badger algorithm},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diversity enforced genetic algorithm (GA) for binary
decision diagram (BDD) reordering. <em>ASOC</em>, <em>155</em>, 111453.
(<a href="https://doi.org/10.1016/j.asoc.2024.111453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary Decision Diagrams (BDDs) have become the state-of-art data structures in numerous fields, wherein, small-sized BDDs are required to reduce the companion cost. Since BDD size is sensitive to the order of variables for the Boolean function in use, evolutionary algorithms have been extensively exploited to solve the BDD reordering problem which is provably an NP-hard problem. However, getting trapped in a local minima is more likely as the population gets homogeneous during the evolution of the algorithm. This paper compares various diversity measures correlated to BDDs and studies the impact of different crossover and mutations used in the Genetic Algorithm (GA) on those diversity measures. Thereafter, we propose an enhanced GA-based reordering algorithm for BDD, wherein, the chosen diversity measure is calculated per generation to steer the application of proper variation operators, in such a way that an acceptable level of equilibrium between exploration and exploitation is preserved. Finally, we utilize a new heuristic Cyclic Crossover (HCX) that further improves the performance of the proposed algorithm following the fitness value. Experimental results on public benchmarks show that our proposed algorithm outperforms other algorithms from the literature when switching between HCX and swap mutation operators is made following the measured diversity metric.},
  archive      = {J_ASOC},
  author       = {Baker Abdalhaq and Amjad Hawash and Ahmed Awad},
  doi          = {10.1016/j.asoc.2024.111453},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111453},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diversity enforced genetic algorithm (GA) for binary decision diagram (BDD) reordering},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online semi-supervised active learning ensemble
classification for evolving imbalanced data streams. <em>ASOC</em>,
<em>155</em>, 111452. (<a
href="https://doi.org/10.1016/j.asoc.2024.111452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concept drift is a core challenge in classification tasks of data streams. Although many drift adaptation methods have been presented, most of them assume that labels of all data are available, which is impractical in many real-world applications. Additionally, the absence of label makes the imbalance ratio of an imbalanced data stream difficultly being obtained in time, providing the inaccurate guidance for resampling and causing poor generalization. To tackle the joint challenges, an online semi-supervised active learning method is proposed to classifier imbalanced data streams with concept drift. A newly-arrived data is first added to the sliding window, and then assigned a pseudo label in terms of its nearest cluster. Meanwhile, semi-supervised clustering algorithm offers its predicted label. Based on the above two predictive labels, cluster-based query strategy provides the criteria for the evaluation and selection of representative instances. More especially, the uncertainty and importance of instances are defined to synthetically evaluate its representativeness. After obtaining true labels of typical ones, ensemble classifier is updated by all instances in current sliding window. Experimental results on 13 synthetic and real data streams indicate that the proposed method outperforms six comparative methods on both G-mean and Recall under various labeling budgets.},
  archive      = {J_ASOC},
  author       = {Yinan Guo and Jiayang Pu and Botao Jiao and Yanyan Peng and Dini Wang and Shengxiang Yang},
  doi          = {10.1016/j.asoc.2024.111452},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111452},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online semi-supervised active learning ensemble classification for evolving imbalanced data streams},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A local rough set method for feature selection by variable
precision composite measure. <em>ASOC</em>, <em>155</em>, 111450. (<a
href="https://doi.org/10.1016/j.asoc.2024.111450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection using variable precision neighborhood rough sets (VPNRS) has garnered considerable attention in data mining and knowledge discovery. Nevertheless, the positive region of VPNRS may not be strictly divided due to the introduction of variable parameters, which could reduce the credibility of feature significance. Meanwhile, the calculation of approximate space is also complex and expensive. Hence, how to improve the computation efficiency is also an investigated issue. As for these issues, we propose a variable precision composite measure and design a novel local method for the feature selection of decision data. Firstly, we introduce the variable precision neighborhood rough set model to process uncertain information from global and local viewpoints. Furthermore, the variable precision composite measure is defined to evaluate the model’s accuracy and further used to select the essential features. Finally, a local forward algorithm is provided for feature selection to improve computing efficiency. All experiments on twelve datasets show that the local method is efficient, and the feature selection algorithm based on variable precision composite measure performs well in classification performance. Our work will provide a convenient tool for feature selection methods with uncertainty measures.},
  archive      = {J_ASOC},
  author       = {Kehua Yuan and Weihua Xu and Duoqian Miao},
  doi          = {10.1016/j.asoc.2024.111450},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111450},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local rough set method for feature selection by variable precision composite measure},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised domain adaptation incorporating three-way
decision for multi-view echocardiographic sequence segmentation.
<em>ASOC</em>, <em>155</em>, 111449. (<a
href="https://doi.org/10.1016/j.asoc.2024.111449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view echocardiographic sequence segmentation is essential for the diagnosis of cardiac diseases in clinical practice. However, the variation in cardiac structures in different views and the lack of manual annotations make it challenging to establish a generalized segmentation model. In this paper, we propose a Bidirectional semi-supervised domain adaptation (BSDA) method based on the three-way decision to learn a generalized segmentation model for different views. Specifically, the two-branch structure of BSDA regards echocardiographic sequence data of different views as different domains. The source-pretrained model first roughly predicted the segmentation results for the target domain. Then, BSDA provides the segmentation model with reliable probabilistic supervision and feature-based pseudo-labels to make secondary decisions. Besides, the proposed stage-wise training strategy can better cope with the varied appearance of the cardiac structures in echocardiographic sequence. We evaluate our BSDA on three publicly available datasets, corroborating the superiority of BSDA to segment cardiac structures of multi-view echocardiographic sequences.},
  archive      = {J_ASOC},
  author       = {Shuxin Zhuang and Heye Zhang and Wanli Ding and Zhemin Zhuang and Jinglin Zhang and Zhifan Gao},
  doi          = {10.1016/j.asoc.2024.111449},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111449},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Semi-supervised domain adaptation incorporating three-way decision for multi-view echocardiographic sequence segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-way confirmatory approach to formal concept analysis
in classification. <em>ASOC</em>, <em>155</em>, 111448. (<a
href="https://doi.org/10.1016/j.asoc.2024.111448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Formal concept analysis (FCA) has demonstrated its effectiveness in classification through various studies. A few types of FCA-based classifiers, such as rule-based, concept-cognitive-learning-based, and hypothesis-based models, have been introduced for different purposes and distinct contexts. Nevertheless, these diverse models share fundamental principles that underlie the construction of effective FCA-based classifiers. This study contributes to the field in at least two aspects. Firstly, we present a general framework of FCA-based classification by reviewing, reformulating, and generalizing the existing models. The framework consists of four essential steps: intent learning, intent grouping, rule induction, and rule application. Secondly, following the presented framework, we integrate Bayesian confirmation theory and propose a novel three-way confirmatory approach to FCA-based classification. The proposed approach provides a fresh lens of formulating, analyzing, and interpreting results from FCA-based classifiers. Moreover, this approach can also be used to re-interpret existing hypothesis-based models, potentially leading to new insights and advancements in the field. The integration of Bayesian confirmation theory enriches the theoretical foundation of FCA-based classifiers, fostering the exploration of promising avenues for future research and development.},
  archive      = {J_ASOC},
  author       = {Mengjun Hu and Zhen Wang},
  doi          = {10.1016/j.asoc.2024.111448},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111448},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way confirmatory approach to formal concept analysis in classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An enhanced decision making model for industrial robotic
selection using three factors: Positive, abstained, and negative grades
of membership. <em>ASOC</em>, <em>155</em>, 111447. (<a
href="https://doi.org/10.1016/j.asoc.2024.111447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional packaging industries that lack automation often grapple with a spectrum of challenges that impede operational efficiency, productivity and overall competitiveness. To maintain quality and safety, the food industry must transition from manual to robotic packaging processes. The most suitable robot for executing such task is identified via a new hybrid fuzzy Stratified Multi-Attribute Decision-Making (S-MADM). This model comprises Fuzzy-PIvot Pairwise RElative Criteria Importance Assessment (F-PIPRECIA) and Stratified Network Mapping (SNM). The Hesitant T-Spherical Fuzzy (HT-SF) set effectively tackles the uncertainty associated with the selection process by introducing three factors, namely, positive, abstained, and negative grades of membership. A case study is presented to demonstrate the novel fusion model and determine the optimal food packaging robot. A total of twelve criteria are selected from the literature based on the decision-maker’s judgment. Among the seven industrial robots, the ”Delta robot” gained the highest ranking (71%), followed by the ”Scara robot” (56%), and the ”Multi-Axis Gantry” (49%). Delta robots, with spider-like limbs, can move gently and accurately at fast speeds, with heavy motors fixed on the frame, allowing for lightweight moving parts. The proposed SNM technique helps to enhance the effectiveness of these alternatives by analyzing the possible, inefficient, and highly influential states of the system. The sensitivity study confirms this analysis and the system’s durability while the comparative study verifies the effectiveness and feasibility of the proposed technique than existing MADM models. This paradigm enables company stakeholders to invest in an industrial robot that not only delivers better results but also efficiently overcome barriers associated with manual packaging procedures.},
  archive      = {J_ASOC},
  author       = {Daekook Kang and Michael Sandra and Samayan Narayanamoorthy and Krishnan Suvitha and Dragan Pamucar and Vladimir Simic},
  doi          = {10.1016/j.asoc.2024.111447},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111447},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An enhanced decision making model for industrial robotic selection using three factors: Positive, abstained, and negative grades of membership},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An artificial fish swarm optimization algorithm for the
urban transit routing problem. <em>ASOC</em>, <em>155</em>, 111446. (<a
href="https://doi.org/10.1016/j.asoc.2024.111446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Urban Transit Routing Problem (UTRP) is an NP-hard discrete problem that deals with the design of routes for public transport systems. It is a highly complex, multiply constrained problem, while the evaluation of candidate route sets can prove both challenging and time-consuming, with many potential solutions rejected on the grounds of infeasibility. Due to its difficulty, metaheuristic methods, such us swarm intelligence algorithms, are considered highly suitable for the UTRP. The suitability of these methods heavily relies on the correct adaptation of the chosen method for a discrete-space problem, the initialization procedure, and the solution evaluation method. In this context, this study proposes an artificial fish swarm optimization algorithm for the efficient solution of the UTRP, presenting a novel discrete-space adaptation of the former. The results are subsequently compared to 14 other algorithms, including evolutionary, swarm intelligence and hyper-heuristic implementations, using Mandl’s widely used and accepted benchmark. Comparison of the produced solutions with published results on Mandl’s benchmark network, shows that the developed algorithm yields superior results to the existing techniques, yielding very high shares of direct trip coverage, which is vital for transit systems to attract riders and contribute to urban sustainability. A new indicator for operator cost calculation is also developed and integrated into our analysis, offering insights on the trade-offs between user and operator costs. Differences in generated solutions, influenced by the weighting factor value, can result in variations of up to 13% in direct trip coverage and 1.5 minutes in average travel time.},
  archive      = {J_ASOC},
  author       = {Vasileios Kourepinis and Christina Iliopoulou and Ioannis Tassopoulos and Grigorios Beligiannis},
  doi          = {10.1016/j.asoc.2024.111446},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111446},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An artificial fish swarm optimization algorithm for the urban transit routing problem},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative elastic-net broad learning systems for visual
classification. <em>ASOC</em>, <em>155</em>, 111445. (<a
href="https://doi.org/10.1016/j.asoc.2024.111445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The broad learning system (BLS) has garnered significant attention in the realm of visual classification due to its exceptional balance between accuracy and efficiency. However, the supervision mechanism in BLS typically relies on strict binary labels, limiting the approximation freedom and failing to represent the data distribution adequately. Furthermore, the inadequacy of the guidance mechanism for the output weights hinders the precise approximation of the target spaces by the input features. To tackle these issues, in this paper, we propose two discriminative elastic-net regularized BLS models with label enhancement, achieving the following significant objectives: Firstly, the proposed label enhancement technologies can substantially augment the margins between different classes while enhancing the diversity within label spaces. Secondly, the compactness and effectiveness of the output weights can be further improved via the guidance of elastic-net regularization of singular values. Thirdly, our proposed algorithms can be efficiently optimized with the augmented Lagrangian method, whose convergence and calculation complexity can be guaranteed well with solid theoretical analysis. Extensive experiments are intended on various popular databases to compare our proposed models with numerous other state-of-the-art recognition algorithms. The numerical results indicate that our proposed algorithms can achieve the best face recognition accuracy of 99.67%, and 97.54% for object recognition. Even in challenging recognition tasks, our methods can still yield an average improvement of 0.8 percentage points.},
  archive      = {J_ASOC},
  author       = {Yanting Li and Junwei Jin and Yun Geng and Yang Xiao and Jing Liang and C.L. Philip Chen},
  doi          = {10.1016/j.asoc.2024.111445},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111445},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discriminative elastic-net broad learning systems for visual classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable fuzzy multivariate outliers identification towards
big data applications. <em>ASOC</em>, <em>155</em>, 111444. (<a
href="https://doi.org/10.1016/j.asoc.2024.111444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data outliers is intrinsically a fuzzy concept and should be treated as such. This paper is a continuation of a research on fuzzy outliers. Extending the BACON algorithm, FBACON1 and FBACON2 have been proposed as fuzzy solutions to the crisp decision boundary of BACON. This paper investigates the scalability potentials and drawbacks of FBACON1 and FBACON2 in Big Data. The investigation concluded that the sensitivity of FBACON2 towards Big Data. Therefore, this paper introduces FBACON3 as a more scalable solution than FBACON2. Three performance measures have been introduced to compare the performance of the three solutions. The study shows that FBACON1 provided the best computational performance followed by FBACON3. However, in terms of the other two measures FBACON2 and FBACON3 are tied but they outperformed FBACON1. Considering the sensitivity towards Big Data volumes and the computation time, FBACON3 is a better candidate than FBACON2. Permanent link to reproducible Capsule: .},
  archive      = {J_ASOC},
  author       = {Huda Mohammed Touny and Ahmed Shawky Moussa and Ali S. Hadi},
  doi          = {10.1016/j.asoc.2024.111444},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111444},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Scalable fuzzy multivariate outliers identification towards big data applications},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A frequency-domain approach with learnable filters for image
classification. <em>ASOC</em>, <em>155</em>, 111443. (<a
href="https://doi.org/10.1016/j.asoc.2024.111443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning applied to computer vision and signal processing is achieving results comparable to the human brain due to the great improvements brought by deep neural networks (DNN). The majority of state-of-the-art architectures are DNN-related, but only a few explicitly explore the frequency domain to extract useful information and improve the results. This paper presents a new approach for exploring the Fourier transform of the input images, which is composed of trainable frequency filters that boost discriminative components in the spectrum. Additionally, we propose a cropping procedure to allow the network to learn both global and local spectral features of the image blocks. The proposed method proved to be competitive concerning well-known DNN architectures in the selected experiments, which involved texture classification, cataract detection, and retina image analysis, where there is a noticeable appeal for the frequency domain, with the advantage of being a lightweight model.},
  archive      = {J_ASOC},
  author       = {José Augusto Stuchi and Natalia Gil Canto and Romis Ribeiro de Faissol Attux and Levy Boccato},
  doi          = {10.1016/j.asoc.2024.111443},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111443},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A frequency-domain approach with learnable filters for image classification},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection in time-series data using evolutionary
neural architecture search with non-differentiable functions.
<em>ASOC</em>, <em>155</em>, 111442. (<a
href="https://doi.org/10.1016/j.asoc.2024.111442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have become the benchmark in diverse fields such as energy consumption forecasting, speech recognition, and anomaly detection, owing to their ability to efficiently process and analyse data. However, they face challenges in managing the complexity and variability in time series data, often leading to increased model complexity and prolonged search duration during parameter tuning. This paper proposes a novel anomaly detection approach through evolutionary neural architecture search (AD-ENAS), which is specifically designed for time series data. The proposed approach focuses on the search for the optimal and minimal neural network architecture. The AD-ENAS method consists of two main phases: architecture evolution and weight adjustment. The architecture evolution phase highlights the importance of neural network architecture by evaluating the fitness of each network agent using shared weight values. Subsequently, the convolutional matrix adaptation technique is used in the next phase for optimal weight adjustment of the neural network. The proposed AD-ENAS method operates without relying on differentiable functions, thus expanding the scope of neural network design beyond traditional backpropagation-based approaches. Various non-differentiable loss functions are explored to facilitate effective architecture search and weight adjustment. Comparative experiments are conducted with five baseline anomaly detection methods on three well-known datasets from reputable sources such as NASA SMAP, NASA MSL and Yahoo S5-A1. The results demonstrate that the AD-ENAS approach effectively evolves neural network architectures, outperforming baseline methods with F1 scores across the three datasets (MSL: 0.942, SMAP: 0.961, Yahoo S5-A1: 0.988) with non-differentiable loss functions, showcasing its efficacy in detecting anomalies in time series data.},
  archive      = {J_ASOC},
  author       = {Santiago Gomez-Rosero and Miriam A.M. Capretz},
  doi          = {10.1016/j.asoc.2024.111442},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111442},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Anomaly detection in time-series data using evolutionary neural architecture search with non-differentiable functions},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term global horizontal irradiance forecasting using
weather classified categorical boosting. <em>ASOC</em>, <em>155</em>,
111441. (<a href="https://doi.org/10.1016/j.asoc.2024.111441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate short-term solar irradiance (SI) forecasting is crucial for renewable energy integration to ensure unit commitment and economic load dispatch. However, hourly SI prediction is very challenging due to atmospheric conditions and weather fluctuations. This study proposes a hybrid approach using weather classification and boosting algorithms for short-term global horizontal irradiance (GHI) forecasting. In data pre-processing steps, we employ random forest for feature selection and K-means clustering for weather classification. The weather-based clustered data is used for the model training using categorical boosting (CatBoost). The proposed weather-classified categorical boosting (WC-CB) scheme is compared with benchmarks in literature like adaptive boosting (AdaBoost), bi-directional long short-term memory (BiLSTM) and gated recurrent unit (GRU) using datasets from two distinct geographical locations obtained from the National Solar Radiation Database (NSRDB). The results show that the proposed WC-CB hybrid approach has lower forecast errors compared to conventional CatBoost modelling. The error reduction of 16% and 39% in root mean square error and 6% and 9% in mean absolute error is recorded for the two datasets, respectively. These findings demonstrate the importance of weather classification in improving forecasting accuracy with potential implications for broader renewable energy applications.},
  archive      = {J_ASOC},
  author       = {Ubaid Ahmed and Ahsan Raza Khan and Anzar Mahmood and Iqra Rafiq and Rami Ghannam and Ahmed Zoha},
  doi          = {10.1016/j.asoc.2024.111441},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111441},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term global horizontal irradiance forecasting using weather classified categorical boosting},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Low-cost architecture performance evaluation strategy based
on pixel difference degree contrast measurement. <em>ASOC</em>,
<em>155</em>, 111440. (<a
href="https://doi.org/10.1016/j.asoc.2024.111440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time and effort required to manually design deep neural architectures is extremely high, which has led to the development of neural architecture search technology as an automatic architecture design method. However, the neural architecture search convergence process is slow and expensive, and the process requires training a large number of candidate architectures to get the final result. If the final accuracy of an architecture can be predicted from its initial state, this problem can be greatly alleviated. Therefore, this paper proposes a low-cost architecture performance evaluation strategy based on pixel difference degree contrast measurement, which takes 1) the difference matrix value between the feature map generated in the untrained architecture and the original image, and 2) the predicted accuracy of the neural network as evaluation indices. A new multi-index weight comprehensive measurement strategy was introduced to comprehensively score the multi-index, the real architecture performance can be approximately represented by score, which greatly reduces the cost of architecture evaluation. The experimental show that the proposed scoring strategy is highly correlated with real architecture accuracy. In the practical engineering application research, this strategy can search a high-performance architecture with an accuracy of 96.2% within 343.3 s, which proves that the proposed strategy can significantly improve the search efficiency in practical applications, reduce the subjectivity of artificial architecture design, and promote the application of practical time-consuming projects.},
  archive      = {J_ASOC},
  author       = {Rui Zhang and Peng-Yun Zhang and Mei-Rong Gao and Jian-Zhe Ma and Li-Hu Pan},
  doi          = {10.1016/j.asoc.2024.111440},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111440},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Low-cost architecture performance evaluation strategy based on pixel difference degree contrast measurement},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A two-objective-optimization-driven group decision making
model under the bipolarity of decision information. <em>ASOC</em>,
<em>155</em>, 111439. (<a
href="https://doi.org/10.1016/j.asoc.2024.111439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When building consensus in group decision making (GDM) under uncertainty, an important yet rarely studied issue is to find the Pareto solutions of multi-objective optimization model. This paper reports a two-objective (2Ob) optimization driven consensus model in GDM by describing the bipolarity of judgements through intuitionistic multiplicative preference relations (IMPRs). First, it is realized that the inherent property of IMPRs is the hesitancy degree. A novel inconsistency index of IMPRs is proposed by combining the effects of hesitancy degree and inconsistency of boundary matrices. Second, the compatibility measure between two IMPRs is utilized to quantify the consensus level (CL) of decision makers. The threshold of acceptable group CL is found to decrease with the order of IMPRs for the first time. A 2Ob optimization model is constructed by minimizing group inconsistency degree and group CL, respectively. Third, the method of equipping two flexibility degrees to each expert is proposed for optimizing individual IMPRs. It is interesting to find that the constructed granularity matrix is different from interval-valued IMPRs. A multi-objective particle swarm optimization algorithm is adopted to obtain a set of Pareto solutions to GDM problems. Case studies are carried out to illustrate the proposed consensus reaching model. The results help to identify how to provide flexible decisions in GDM under some complexity and uncertainty of a practical problem.},
  archive      = {J_ASOC},
  author       = {Ziqian Luo and Fang Liu and Qirui You and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2024.111439},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111439},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A two-objective-optimization-driven group decision making model under the bipolarity of decision information},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient approach based on a novel 1D-LBP for the
detection of bearing failures with a hybrid deep learning method.
<em>ASOC</em>, <em>155</em>, 111438. (<a
href="https://doi.org/10.1016/j.asoc.2024.111438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bearings serve as fundamental components in the transmission of motion for rotating machinery. The occurrence of mechanical wear and subsequent bearing failures within these rotating systems can lead to diminished operational efficiency and, if left unaddressed, may result in the complete cessation of the system&#39;s function. Hence, there exists a critical need for effective monitoring methodologies aimed at accurately detecting faults in such systems, preferably in their nascent stages. This study presents a novel approach to fault diagnosis leveraging vibration data obtained from bearings. Initially, a feature extraction technique is devised, which incorporates localized signal variations. Subsequently, these features, extracted via MM-1D-LBP, are utilized in conjunction with a hybrid deep learning network based on Long Short-Term Memory (LSTM) and one-dimensional Convolutional Neural Network (1D-CNN) architectures for diagnostic purposes. To assess the efficacy of the proposed methodology, experiments were conducted on two distinct datasets acquired from real-world bearing assemblies. In the first dataset, the aim was to predict various failure types (Inner Ring, Outer Ring, Ball). In the second dataset, the objective was to estimate defect sizes using bearing vibration signals corresponding to defects of different dimensions (0.15 cm, 0.5 cm, 0.9 cm) under consistent operating conditions. Remarkably high success rates of 99.31 % and 99.65 % were achieved for the two datasets, respectively, thus underscoring the efficacy of the proposed MM-1D-LBP+1D-CNN-LSTM approach. These findings not only demonstrate the feasibility of the proposed method for fault diagnosis in bearing systems but also suggest its potential applicability across diverse signal categories. Ultimately, this research contributes to advancing the state-of-the-art in fault diagnosis methodologies for rotating machinery, offering enhanced accuracy and early detection capabilities.},
  archive      = {J_ASOC},
  author       = {Yılmaz Kaya and Melih Kuncan and Eyyüp Akcan and Kaplan Kaplan},
  doi          = {10.1016/j.asoc.2024.111438},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111438},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient approach based on a novel 1D-LBP for the detection of bearing failures with a hybrid deep learning method},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving spatiotemporal partial differential equations with
physics-informed graph neural network. <em>ASOC</em>, <em>155</em>,
111437. (<a href="https://doi.org/10.1016/j.asoc.2024.111437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-informed neural networks (PINNs) have recently gained considerable attention as a prominent deep learning technique for solving partial differential equations (PDEs). However, traditional fully connected PINNs often encounter slow convergence issues attributed to automatic differentiation in constructing loss functions. In addition, convolutional neural network (CNN)-based PINNs face challenges when dealing with irregular domains and unstructured meshes. To address these issues, we propose a novel framework based on graph neural networks (GNNs) and radial basis function finite difference (RBF-FD). We introduce GNNs into physics-informed learning to better handle irregular domains. RBF-FD is employed to construct a high-precision difference format of the PDE to guide model training. We perform numerical experiments on various PDEs, including heat, wave, and shallow water equations on irregular domains. The results demonstrate that our method is capable of accurate predictions and exhibits strong generalization, allowing for inference with different initial. We evaluate the generalizability, accuracy, and efficiency of our approach by considering different Gaussian noise, PDE parameters, numbers of collection points, and various types of radial basis functions.},
  archive      = {J_ASOC},
  author       = {Zixue Xiang and Wei Peng and Wen Yao and Xu Liu and Xiaoya Zhang},
  doi          = {10.1016/j.asoc.2024.111437},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111437},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving spatiotemporal partial differential equations with physics-informed graph neural network},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data and measurement mechanism integrated imaging method for
electrical capacitance tomography. <em>ASOC</em>, <em>155</em>, 111436.
(<a href="https://doi.org/10.1016/j.asoc.2024.111436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a new imaging paradigm for overcoming the challenges limiting the improvement of the imaging quality in the electrical capacitance technique. The new imaging model enables the integration of measurement physics, sparsity-induced prior and physics-informed multi-fidelity learning prior (PIMFLP), as well as the synergy between data-driven and measurement physics modeling paradigms. The transformed L 1 norm is used to model the PIMFLP, and the maximum correntropy criterion is used as a data misfit term to restrict the adverse impact of noises or outliers. The PIMFLP is learned from data and characterizes the structural details of imaging targets. The half-quadratic optimization method is developed to overcome computational challenges in solving the novel imaging model. A new physics-informed multi-fidelity learning method is developed to predict PIMFLP by synergizing deep convolutional neural network with measurement physics, and a new bilevel optimization model solved by a new nested algorithm that merges the genetic algorithm and the split Bregman method is proposed for training. Comparisons with other popular reconstruction algorithms confirm that the new imaging method leads to the improved reconstruction accuracy and noise immunity, and opens up new possibilities for unlocking the potential of the measurement technology.},
  archive      = {J_ASOC},
  author       = {Jing Lei and Qibin Liu},
  doi          = {10.1016/j.asoc.2024.111436},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111436},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data and measurement mechanism integrated imaging method for electrical capacitance tomography},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective grey wolf–cuckoo search algorithm applied
to spatial truss design optimization. <em>ASOC</em>, <em>155</em>,
111435. (<a href="https://doi.org/10.1016/j.asoc.2024.111435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel hybrid algorithm called Multi-Objective Hybrid Grey Wolf Cuckoo Search (MOGWOCS) is developed for spatial truss designs in this study. A new simple yet efficient mechanism to select the best candidates is proposed. Furthermore, harmonic averaging is employed to be a replacement for conventional arithmetic mean for higher effectiveness. Additionally, the Lévy flight in Cuckoo Search (CS) is utilized to increase efficiency in early searching and also reduce local entrapment possibility. For verification purposes, MOGWOCS is first performed on some mathematical functions and 11 CEC2020 mechanical problems. It is then examined on four large-scale truss design problems, in two of which multi-objective optimization is studied for the first time. To demonstrate the superiority of the proposed approach, five up-to-date algorithms, and various indicators are included for validation. It is found that MOGWOCS is able to produce solutions with higher optimality in terms of diversity and accuracy.},
  archive      = {J_ASOC},
  author       = {Nam Vo and Huy Tang and Jaehong Lee},
  doi          = {10.1016/j.asoc.2024.111435},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111435},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective grey Wolf–Cuckoo search algorithm applied to spatial truss design optimization},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified genetic algorithm and fine-tuned long short-term
memory network for intrusion detection in the internet of things
networks with edge capabilities. <em>ASOC</em>, <em>155</em>, 111434.
(<a href="https://doi.org/10.1016/j.asoc.2024.111434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of smart cities is an example of how new technologies, such as the Internet of Things (IoT), have facilitated the creation of extensive interconnected and intelligent ecosystems. The widespread deployment of IoT devices has enabled the provision of constant environmental feedback, thereby facilitating the automated adaptation of associated systems. This has brought about a fundamental transformation in the way contemporary society functions. The security of emerging technologies such as IoT has become a significant challenge due to the added complexities, misconfigurations, and conflicts between modern and legacy systems. This challenge has a notable impact on the reliability and accessibility of existing infrastructure. Edge computing (EC) is a collaborative computing system that brings data processing and analysis closer to the edge of the network, where the data is generated, rather than in a centralized cloud environment. The utilization of the IoT has become more prevalent in both everyday life and the manufacturing sector, with a particular emphasis on critical infrastructure. The IoT is presently being utilized across diverse domains, including but not limited to industrial, agricultural, healthcare, and logistical sectors. The security of IoT networks has implications for the safety of individuals, the security of the nation, and economic development. Notwithstanding, conventional intrusion detection techniques that rely on centralized cloud-based systems that have been suggested in previous studies for IoT network security are insufficient to meet the requirements for data confidentiality, network capacity, and prompt responsiveness. In addition, the integration of IoT applications into smart devices has been shown to augment their functionalities. However, it is important to note that this integration also brings about potential security vulnerabilities. Furthermore, a significant number of contemporary IoT devices exhibit restricted security capabilities, rendering them vulnerable to intricate attacks and impeding the extensive integration of IoT technologies. Also, a lot of IoT network devices have been put in place that don&#39;t have hardware security measures. This means that traditional intrusion detection systems (IDS) aren&#39;t enough to protect the IoT network ecosystem. To address these issues, this research suggests the IoT-Defender framework, which combines a Modified Genetic Algorithm (MGA) model with a deep Long Short-Term Memory (LSTM) network to find cyberattacks in IoT networks. This research represents a pioneering attempt to employ the MGA for feature selection and the GA for fine-tuning the LSTM parameters within an EC framework. The parameters of the LSTM model were fine-tuned through the manipulation of the number of hidden layers, utilizing the GA fitness function. The customization of the MGA aimed to enhance its performance in selecting relevant features, optimizing the use of limited resources on IoT devices and edge nodes. The fine-tuning process involved optimizing hyperparameters, architecture, and training strategies to maximize the LSTM network&#39;s effectiveness in learning and detecting patterns in IoT network traffic. The synergy between the MGA and LSTM aimed at creating a comprehensive and efficient IDS. The feature selection by the MGA contributes to improving the LSTM’s performance by providing it with more relevant and discriminating features. In order to solve the issue of class imbalance, we utilize the focal loss function, which provides greater weights to minority classes, hence improving the model’s capacity to learn from those particular classes. The performance of the IoT-Defender model was assessed on the BoT-IoT, UNSW-NB15, and N-BaIoT datasets utilizing a Raspberry Pi IoT device. The results of our study show that the IoT-Defender model works better than other methods. This is shown by its accuracy score of 99.41%, detection rate of 99.78%, precision score of 98.50%, false alarm rate of 2.56%, mean intersection over union (mIoU) of 0.68, and training time of 81.3 seconds on BoT-IoT. The proposed IoT model is designed to be lightweight and can be installed on edge servers to detect cyber-attacks in real-time, specifically in the context of IoT security.},
  archive      = {J_ASOC},
  author       = {Yakub Kayode Saheed and Oluwadamilare Harazeem Abdulganiyu and Taha Ait Tchakoucht},
  doi          = {10.1016/j.asoc.2024.111434},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111434},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified genetic algorithm and fine-tuned long short-term memory network for intrusion detection in the internet of things networks with edge capabilities},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Convolution smoothing and non-convex regularization for
support vector machine in high dimensions. <em>ASOC</em>, <em>155</em>,
111433. (<a href="https://doi.org/10.1016/j.asoc.2024.111433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The support vector machine (SVM) is a well-known statistical learning tool for binary classification. One serious drawback of SVM is that it can be adversely affected by redundant variables, and research has shown that variable selection is crucial and necessary for achieving good classification accuracy. Hence some SVM variable selection studies have been devoted, and they have an unified “empirical hinge loss plus sparse penalty” formulation. However, a noteworthy issue is the computational complexity of existing methods is high especially for large-scale problems, due to the non-smoothness of the hinge loss. To solve this issue, we first propose a convolution smoothing approach, which turns the non-smooth hinge loss into a smooth surrogate one, and they are asymptotically equivalent. Moreover, we construct computationally more efficient SVM variable selection procedure by implementing non-convex penalized convolution smooth hinge loss. In theory, we prove that the resulting variable selection possesses the oracle property when the number of predictors is diverging. Numerical experiments also confirm the good performance of the new method.},
  archive      = {J_ASOC},
  author       = {Kangning Wang and Junning Yang and Kemal Polat and Adi Alhudhaif and Xiaofei Sun},
  doi          = {10.1016/j.asoc.2024.111433},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111433},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Convolution smoothing and non-convex regularization for support vector machine in high dimensions},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A spatiotemporal convolution recurrent neural network for
pixel-level peripapillary atrophy prediction using sequential fundus
images. <em>ASOC</em>, <em>155</em>, 111431. (<a
href="https://doi.org/10.1016/j.asoc.2024.111431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progression of peripapillary atrophy (PPA) is closely associated with the development of retinal diseases such as myopia and glaucoma. PPA prediction employing longitudinal images to obtain its progress trend can facilitate personalized treatment. Although existing studies have attempted to predict the persistence of PPA, such studies cannot provide quantitative measurement for personalized treatment. In this paper, we propose a spatiotemporal framework for pixel-level PPA prediction using sequential fundus images, including feature extractor, temporal memory, and spatiotemporal prediction modules. To take advantage of historical information, a temporal memory module is used, integrating current and prior features to build sequential data of features. To further enhance the prediction performance, the recurrent neural network states in a spatiotemporal prediction module transmit between different layers, enabling high-level states to guide the learning of low-level states. To handle missing data in clinical follow-up data, we use the predicted output of the spatiotemporal prediction module to substitute the missing data, and the scheduled-sampling strategy is employed in training. Extensive experiments conducted using a clinical dataset demonstrate that our proposed method achieves a satisfactory performance compared with the start-of-the-art models. The proposed approach can be applied using clinical data to obtain various quantitative indicators for personalized treatment and prevention of retinal disease.},
  archive      = {J_ASOC},
  author       = {Mengxuan Li and Weihang Zhang and He Zhao and Yubin Xu and Jie Xu and Huiqi Li},
  doi          = {10.1016/j.asoc.2024.111431},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111431},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatiotemporal convolution recurrent neural network for pixel-level peripapillary atrophy prediction using sequential fundus images},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust maximum expert consensus model with adjustment path
under uncertain environment. <em>ASOC</em>, <em>155</em>, 111430. (<a
href="https://doi.org/10.1016/j.asoc.2024.111430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum expert consensus model (MECM) is a commonly used consensus model in group decision making (GDM). In traditional MECM, the consensus constraints are not fully considered and the adjustment cost of decision maker (DM) is certain. Moreover, directing the DM’s opinion in a visual path is seldom considered in the consensus reaching process (CRP) of MECM. Inspired by these issues, this paper first proposed two MECMs with different types of consensus constraints. Then, this paper incorporated an adjustment path into MECM by using feedback coefficients that can prevent opinions from being overadjusted. Furthermore, the robust MECM (RMECM) is developed to address the uncertainty of unit adjustment cost under three uncertainty sets. Finally, the feasibility of the proposed models is verified by applying them to the allocation of special funds in the Chinese film industry, which is a large-scale group decision making (LSGDM) problem. The sensitivity analysis and comparative analysis are also conducted to show the efficiency of the proposed models.},
  archive      = {J_ASOC},
  author       = {Yifan Ma and Ying Ji and Chethana Wijekoon},
  doi          = {10.1016/j.asoc.2024.111430},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111430},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Robust maximum expert consensus model with adjustment path under uncertain environment},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated quantum picture fuzzy rough sets with golden
cuts for evaluating carbon footprint-based investment decision policies
of sustainable industries. <em>ASOC</em>, <em>155</em>, 111428. (<a
href="https://doi.org/10.1016/j.asoc.2024.111428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this study is to make evaluation related to the significant determinants of the effectiveness of the carbon footprint-based investments while constructing a novel decision-making model. At the first stage, selected five determinants are evaluated with multi stepwise weight assessment ratio analysis (M-SWARA) methodology based on quantum picture fuzzy rough sets. In the second part, sustainable industry alternatives are ranked by quantum picture fuzzy rough sets extended multi-objective optimization on the basis of ratio analysis (MOORA) technique. Similarly, elimination and choice translating reality (ELECTRE) approach is also taken into consideration to make a comparative evaluation. The main contribution of this study is that a novel methodology is proposed by integrated picture fuzzy row sets and quantum theory. While using the combination of rough sets and picture fuzzy logic, uncertain data in the complex process can be evaluated in a more effective manner. Moreover, due to the criticisms to stepwise weight assessment ratio analysis (SWARA) methodology by not considering causal relationship of the determinants, this methodology is extended with the help of some improvements so that a new approach (M-SWARA) is proposed to overcome this deficiency by creating impact direction map of the items. The ranking results of these two techniques are the same that indicates the coherency of the findings. It is concluded that carbon-free project financing with green bonds is the most important indicator for this situation. On the other side, the ranking results demonstrate that renewable energy investment is the most appropriate sustainable industry alternative. Considering the results obtained in this study, the development of green bonds should be given priority. Establishing an international certification system is important in terms of clearly defining green bonds. Government supports are also of critical importance in the development of green bonds. Tax reductions provided by governments can increase the profitability of green bonds. This may contribute to investors showing more interest in green bonds.},
  archive      = {J_ASOC},
  author       = {Gang Kou and Dragan Pamucar and Hasan Dinçer and Serhat Yüksel and Muhammet Deveci and Muhammad Umar},
  doi          = {10.1016/j.asoc.2024.111428},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111428},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated quantum picture fuzzy rough sets with golden cuts for evaluating carbon footprint-based investment decision policies of sustainable industries},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSFIS-GWO: Metaheuristic-driven takagi-sugeno fuzzy system
for adaptive real-time routing in WBANs. <em>ASOC</em>, <em>155</em>,
111427. (<a href="https://doi.org/10.1016/j.asoc.2024.111427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless body area network (WBAN) is an internet-of-things technology that facilitates remote patient monitoring and enables medical staff to administer timely treatments. One of the main challenges in designing WBANs is the routing problem, which is complicated due to dynamic changes in network topology and the limited resources of nodes. Several heuristic and metaheuristic methods have been presented to solve the routing problem in WBANs. Although metaheuristics outperform heuristics by producing higher-quality solutions, they cannot respond to real-time requests. This paper introduces a reactive routing protocol for WBANs that combines a fuzzy heuristic with a metaheuristic learning model. It utilizes a Takagi-Sugeno Fuzzy Inference System in conjunction with the Grey Wolf Optimizer (named TSFIS-GWO). The objective is to simultaneously benefit from the advantages of both approaches, namely, the effectiveness of metaheuristics for offline hyperparameter tuning and the quickness of fuzzy heuristics for real-time routing. At every round, the tuned fuzzy system takes multiple parameters of the current state of the nodes and links to construct the multi-hop routing tree under IEEE 802.15.6. To optimize the performance of the protocol for each WBAN, the fuzzy rules of the TSFIS model are automatically adjusted through a learning method based on GWO. This is done in accordance with the specific requirements of the application, and the tuning process takes place once before the protocol is applied. Simulation results in three applications demonstrate that the proposed TSFIS-GWO model is capable of providing real-time solutions while outperforming the existing methods in terms of application-specific performance measures.},
  archive      = {J_ASOC},
  author       = {Saeideh Memarian and Navid Behmanesh-Fard and Pouya Aryai and Mohammad Shokouhifar and Seyedali Mirjalili and María del Carmen Romero-Ternero},
  doi          = {10.1016/j.asoc.2024.111427},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111427},
  shortjournal = {Appl. Soft. Comput.},
  title        = {TSFIS-GWO: Metaheuristic-driven takagi-sugeno fuzzy system for adaptive real-time routing in WBANs},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unsupervised feature selection using chronological fitting
with shapley additive explanation (SHAP) for industrial time-series
anomaly detection. <em>ASOC</em>, <em>155</em>, 111426. (<a
href="https://doi.org/10.1016/j.asoc.2024.111426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the industrial Internet of Things (IIOT), an amount of industrial multivariate time series (IMTS) data has been collected by various sensors. IMTS data anomaly detection plays an important role in industrial process monitoring and operation condition identification. Many real-world IMTS datasets usually have a large number of redundant features, which may lead to deviation of the anomaly detection model and misunderstanding of potential feature correlation. Hence, it is essential to select features that have critical and effective effects on anomalies and operation conditions. In this paper, a feature selection that introduces knowledge-based target variables into inner feature selectors (IFSs) is proposed as a wrapper method, referred to as ILSFS. The IFS is constructed by integrating a timing fitter, which is established based on the long and short-term memory (LSTM) network, with the Shapley Additive explanation (SHAP) assisted to eliminate unimportant features. The experiments were conducted on two datasets: the vertical roller mill dataset (VMD) of a slag milling factory in Jiangsu Province and the public server machine dataset (SMD). Compared with None feature selection, SVM-RFE, LGBM -Boruta, Genetic-based methods, and other SHAP-assisted feature selection methods such as XGBSVFIR and ShapHT+, the experiment results highlighted the proposed methodology ILSFS improves the best F1 score of each the above baselines by 0.2799, 0.2208, 0.2647, 0.1882, 0.3775, 0.2765 for the VMD and 0.1475, 0.0586, 0.0567, 0.0638, 0.0519, 0.2250 for the SMD on the downstream anomaly detection. The best precision of the ILSFS for the VMD and the SMD was 0.8224 and 0.9499, respectively, while its best recall was 0.7353 and 0.4902, respectively.},
  archive      = {J_ASOC},
  author       = {Qixuan Li and Yangjian Ji and Mingrui Zhu and Xiaoyang Zhu and Linjin Sun},
  doi          = {10.1016/j.asoc.2024.111426},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111426},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Unsupervised feature selection using chronological fitting with shapley additive explanation (SHAP) for industrial time-series anomaly detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal cluster-based local deep learning or signal
processing-temporal convolutional transformer for daily runoff
prediction? <em>ASOC</em>, <em>155</em>, 111425. (<a
href="https://doi.org/10.1016/j.asoc.2024.111425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Water scarcity poses a major obstacle to sustainable development, and precise discharge prediction plays a vital role in enabling effective water resource management. This study investigated improved prediction techniques for nonstationary time series. The study evaluated the effect of signal processing techniques and blending approaches on the performance of deep learning models for daily discharge prediction. It also compared the performance of cluster-based local modeling with hybrid signal processing-deep learning approaches. Two robust deep learning methods, Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN), along with a powerful signal processing approach called discrete wavelet transform, were utilized for prediction of daily discharge. Three blending approaches were assessed: 1) decomposing both inputs and target, 2) decomposing only the target, and 3) decomposing only the inputs and then blending them with deep learning models. Also, a new hybrid deep learning based model namely discrete wavelet transform-Temporal Convolutional Transformer (DWT-TCT) was developed. The results showed that a single-output wavelet transform-deep learning model (3rd blending approach) outperformed multi-output models, demonstrating a relative enhancement of up to 56% for the LSTM model and 51% for the CNN model. Furthermore, temporal cluster-based local modeling displayed promising performance, resulting in an improvement of up to 18% in NRMSE compared to the wavelet transform-deep learning model, while also requiring less computational cost. The successful results of the temporal cluster-based local modeling approach provide a beneficial alternative to hybrid signal processing-deep learning models. In addition the results showed that the proposed DWT-TCT model outperformed all other models with NRMSE ranges from 6.8% to 16.2% in the study areas. The results have implications for hydrology and water resources management, as they can be used to develop more precise and effective models for predicting discharge in view of nonstationarity.},
  archive      = {J_ASOC},
  author       = {Vahid Moosavi and Sahar Mostafaei and Ronny Berndtsson},
  doi          = {10.1016/j.asoc.2024.111425},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111425},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Temporal cluster-based local deep learning or signal processing-temporal convolutional transformer for daily runoff prediction?},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Interaction matters: Encrypted traffic classification via
status-based interactive behavior graph. <em>ASOC</em>, <em>155</em>,
111423. (<a href="https://doi.org/10.1016/j.asoc.2024.111423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately classifying encrypted traffic is the indispensable cornerstone for network management and Quality of Service (QoS) improvement. Although existing works that learn from non-interaction features of communication behavior have achieved a satisfactory performance, there still remains an unsolved crux before practical application that current works fail to distinguish different encrypted traffic generated by the same application. Such similar but distinct traffic is widely-existed since applications typically employ similar communication settings and encryption technology for diverse data transmission. To address the above issues, this work proposes that interaction features of communication behavior provide substantial information in terms of traffic classification, while directly leveraging such diversiform interaction information is non-trivial. As a solution, we devise a novel graph structure preserving the information of interactive process, referred to interactive behavior graph, to represent communication behaviors. Specifically, the proposed interactive behavior graph respectively stores the transition status and interaction status of the interactive actions during the interactive process in the edges and vertices with attributes. In addition, a classification model is tailored based on sampling subgraphs, which capture communication behavior patterns from the strong interaction correlation among neighboring interactive actions. Comprehensive experimental results demonstrate the superiority of our method over solid comparisons. Particularly, in distinguishing similar encrypted traffic, our method achieves an accuracy rate exceeding 98%, which outperforms the state-of-the-art methods. Furthermore, we validate the generalizability of our proposed method on two well-known encrypted traffic datasets, attaining an accuracy rate of 94%.},
  archive      = {J_ASOC},
  author       = {Yao Li and Xingshu Chen and Wenyi Tang and Yi Zhu and Zhenhui Han and Yawei Yue},
  doi          = {10.1016/j.asoc.2024.111423},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111423},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interaction matters: Encrypted traffic classification via status-based interactive behavior graph},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-density cluster core-based k-means clustering with an
unknown number of clusters. <em>ASOC</em>, <em>155</em>, 111419. (<a
href="https://doi.org/10.1016/j.asoc.2024.111419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k -means algorithm, known for its simplicity and adaptability, faces challenges related to manual cluster number selection and sensitivity to initial centroid placement. This paper introduces an innovative framework aimed at overcoming these challenges. By proposing a data-driven cluster number estimation method and a robust initialization strategy based on high-density cluster cores, our approach revolutionizes k -means, unlocking its full unsupervised potential and ensuring superior performance, even in scenarios involving overlapping clusters. The method employs a novel density-based technique to accurately identify cluster cores, resulting in substantial improvements over existing methods. Rigorous experimentation on synthetic and real-world datasets demonstrates an average performance enhancement of 15% in terms of the Adjusted Rand Index for datasets with overlapping clusters, surpassing the capabilities of state-of-the-art density-based clustering methods and traditional k -means. Moreover, our method autonomously determines the optimal number of clusters, facilitating true unsupervised learning and eliminating the impact of initial centroid placement on clustering outcomes. This leads to stable and consistent results, addressing key limitations of the conventional k -means algorithm. The practical applicability of our approach is exemplified in image segmentation tasks, showcasing its versatility and reliability in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Abhimanyu Kumar and Abhishek Kumar and Rammohan Mallipeddi and Dong-Gyu Lee},
  doi          = {10.1016/j.asoc.2024.111419},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111419},
  shortjournal = {Appl. Soft. Comput.},
  title        = {High-density cluster core-based k-means clustering with an unknown number of clusters},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conceptual clustering method for large-scale group
decision-making with linguistic truth-valued lattice implication
algebra. <em>ASOC</em>, <em>155</em>, 111418. (<a
href="https://doi.org/10.1016/j.asoc.2024.111418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing complexity of decision-making environments has led to a rise in the involvement of decision-makers (DMs) in group decision-making problems. Clustering is widely used in large-scale group decision-making (LSGDM) to categorize DMs into smaller groups. Ensuring reasonable decision-making results requires providing explanations for the generated groups during the clustering process. To address the clustering problem in LSGDM within uncertain linguistic environments, this paper proposes a conceptual clustering method based on the linguistic concept lattice. The method efficiently manages comparable and incomparable linguistic information. To achieve interpretable clustering results for DMs, attribute and expert induction matrices are first introduced. Cluster stability analysis is then employed to automatically determine the optimal number of clusters. Second, linguistic truth-valued aggregation operators are proposed to aggregate the linguistic evaluation information of DMs in each cluster. In addition, a consensus reaching process is conducted within each cluster, and a feedback mechanism is established to iteratively update clusters when consensus cannot be reached. Finally, numerical examples and comparative analyses are presented that verify the effectiveness of the proposed approach in effectively addressing the LSGDM problem within uncertain linguistic environments.},
  archive      = {J_ASOC},
  author       = {Kuo Pang and Yifan Lu and Luis Martínez and Witold Pedrycz and Li Zou and Mingyu Lu},
  doi          = {10.1016/j.asoc.2024.111418},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111418},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A conceptual clustering method for large-scale group decision-making with linguistic truth-valued lattice implication algebra},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel q-rung orthopair fuzzy best-worst method, shannon
entropy and MARCOS method for mobile medical app service quality
evaluation. <em>ASOC</em>, <em>155</em>, 111417. (<a
href="https://doi.org/10.1016/j.asoc.2024.111417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a new hybrid multi-criteria decision-making (MCDM) methodology based on q -rung orthopair fuzzy sets ( q -ROFSs) by integrating best-worst method (BWM), Shannon entropy method (SEM) and measurement of alternatives and ranking according to compromise solution (MARCOS). The methodology consists of two main phases: weight determination and alternative ranking. Firstly, the determination of the weight is mainly from both subjective and objective aspects. To determine the subjective weights, we propose a q -rung orthopair fuzzy best-worst method ( q -ROF-BWM) satisfying additive consistency, which requires fewer comparisons for more reliable results, and define a new consistency ratio (CR) to test the consistency of comparisons. Additionally, objective weights are obtained using SEM extended to q -ROFSs, which are ignored by many MCDM methods. In the second stage, a new integrated alternative ranking approach called q -ROF-MARCOS is proposed, which does not become more complex with the increase in the number of criteria or alternatives. Finally, the proposed method is applied to the evaluation problem of mobile medical app service quality, and a new mobile medical app service quality evaluation index system based on SERVQUAL model is established. Sensitivity and comparative analysis validate the method’s effectiveness, rationality, superiority, and robustness.},
  archive      = {J_ASOC},
  author       = {Zhen Li and Yuping Xing and Peng Dong},
  doi          = {10.1016/j.asoc.2024.111417},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111417},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A novel q-rung orthopair fuzzy best-worst method, shannon entropy and MARCOS method for mobile medical app service quality evaluation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph neural networks based framework to analyze social
media platforms for malicious user detection. <em>ASOC</em>,
<em>155</em>, 111416. (<a
href="https://doi.org/10.1016/j.asoc.2024.111416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online social media (OSM) has emerged as the most pertinent and readily available platform for individuals to effectively express their perspectives. Users connect seamlessly in an unstructured network, allowing information to flow within seconds. This interconnectedness, while enabling rapid information dissemination, also opens the door to significant challenges such as misinformation, disinformation, cyberbullying, privacy concerns, polarized opinions, and digital footprints. Users on social media are active with different intentions, which could include information sharing, social connections, shaping public opinion, or launching campaigns either for or against certain organizations with specific objectives. Depending on the users’ intentions, the content can be either malicious or non-malicious. Malicious content can induce fear, uncertainty, or financial damage, leading to societal polarization or reduced revenue for commercial organizations. Therefore, the detection of users with malicious intentions is crucial to curb the spread of harmful content in society. This paper proposes a deep learning-based framework that explores social media in three different domains: users’ profiles, the content being shared, and the analysis of users’ unstructured ego-networks. The framework is established on an inductive learning-based graph neural network for a 3D analysis of social media platforms. The proposed model can serve as a benchmark and provide a baseline for researchers. The performance of the proposed model is compared with available approaches, such as SVM and LSTM. A series of experiments demonstrates the out-performance of the proposed framework on real-world PHEME dataset. Additionally, the proposed framework may also be used as an OSINT (Open-Source Intelligence) tool, depending on the availability of customized data.},
  archive      = {J_ASOC},
  author       = {Zafran Khan and Zeeshan Khan and Byung-Geun Lee and Hong Kook Kim and Moongu Jeon},
  doi          = {10.1016/j.asoc.2024.111416},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111416},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph neural networks based framework to analyze social media platforms for malicious user detection},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble artificial bee colony algorithm with q-learning for
scheduling bi-objective disassembly line. <em>ASOC</em>, <em>155</em>,
111415. (<a href="https://doi.org/10.1016/j.asoc.2024.111415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses a bi-objective disassembly line scheduling problem (Bi-DLSP), considering interference relationships among tasks. The objectives are to optimize the total disassembly time and the smoothing index simultaneously. First, we propose a mathematical model for the concerned problems. Second, improved artificial bee colony (ABC) algorithms are developed to solve the Bi-DLSP, and seven different local search operators are created to strengthen the performance of the ABC algorithms. Third, to further enhance the improved ABC algorithms, we design two Q-learning-based strategies for selecting high-quality local search operators and integrate them into the ABC algorithm during iterations. Finally, we evaluate the effectiveness of the proposed strategies by comparing the classical ABC algorithm, its variants, and two classical multi-objective algorithms for solving 21 instances. We validate the proposed model using the Gurobi solver and compare its results and time efficiency with the proposed algorithms. The experimental results show that the proposed ABC algorithm based on Q-learning (ABC_QL1) performs the best in solving related problems. This study provides a new approach for solving the Bi-DLSP and demonstrates the effectiveness and competitiveness of our method, providing useful insights for research and applications in related fields.},
  archive      = {J_ASOC},
  author       = {Yaxian Ren and Kaizhou Gao and Yaping Fu and Dachao Li and Ponnuthurai Nagaratnam Suganthan},
  doi          = {10.1016/j.asoc.2024.111415},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111415},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble artificial bee colony algorithm with Q-learning for scheduling bi-objective disassembly line},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning and tree-based models for earth skin
temperature forecasting in malaysian environments. <em>ASOC</em>,
<em>155</em>, 111411. (<a
href="https://doi.org/10.1016/j.asoc.2024.111411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the Earth Skin Temperature (TS) using artificial intelligence (AI) has the potential to offer valuable insights into environmental changes and their impacts. TS has significant nonlinearity due to several meteorological parameters, including average temperature, maximum temperature, minimum temperature, relative humidity, surface pressure, wind speed, and wind direction. This study introduced four machine learning (ML) models, namely solo Long Short-Term Memory (LSTM), Autoencoder-LSTM, Classification and Regression Tree (CART), and a hybrid Convolutional Neural Network-LSTM (CNN-LSTM), to predict daily averaged TS at different locations over the Malaysian region. In the first stage of the modeling development, Pearson Correlation (PC) was adopted to measure the strength and direction of the relationship between input and output variables. The statistical analysis and visual interpretations demonstrated that the Autoencoder-LSTM model outperformed the CNN-LSTM, LSTM, and CART models for each simulated city. The Autoencoder-LSTM model showed outstanding performance at Kuantan in comparison with the seven cities, achieving a coefficient of determination (R 2 = 0.965), Root Mean Square Error (RMSE = 0.183), Mean Absolute Error (MAE = 0.142), and Nash-Sutcliffe Efficiency (NSE = 0.96). The research findings suggested that the coordinates of each station are crucial in determining the level of data randomness, which ultimately affects the learning process of the ML models.},
  archive      = {J_ASOC},
  author       = {Omer A. Alawi and Haslinda Mohamed Kamar and Raad Z. Homod and Zaher Mundher Yaseen},
  doi          = {10.1016/j.asoc.2024.111411},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111411},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning and tree-based models for earth skin temperature forecasting in malaysian environments},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning-based and adaptive region proposal algorithm
for semantic segmentation. <em>ASOC</em>, <em>155</em>, 111410. (<a
href="https://doi.org/10.1016/j.asoc.2024.111410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an adaptive and new region proposal algorithm for generating high-quality regions. The main aim of this algorithm is to investigate different features in the proposal generation process. This algorithm is based on bottom-up image segmentation and deep learning techniques. Each primary region is represented by features derived from a convolutional neural network (CNN). The adjacent and similar regions are merged based on a new proposed searching algorithm and a distance function in a hierarchical way. Additionally, various hand-crafted texture features are examined for representing each region. These texture features are not being utilized previously in region proposal generation. This method also applied both texture-based and deep learning-based features, contemporaneously. Furthermore, the proposed region proposal algorithm was evaluated on two significant challenging datasets, including VOC2012 and COCO2017. The resulting proposals show more high-quality regions in high overlaps in comparison to previous region proposal algorithms. More importantly, the results verified that the deep learning-based features together with handcrafted texture features are complement and this fusion can overcome the shortcoming of other approaches. Additionally, the proposed region proposal algorithm is employed in weakly supervised semantic segmentation. The new generated proposals are used to create some new labeled masks. These masks are very useful and effective in the training phase of deep learning. This approach is evaluated on VOC2012 and compared with previous region-based and free region-based methods. The results show the efficacy of the proposed method.},
  archive      = {J_ASOC},
  author       = {Maryam Taghizadeh and Abdolah Chalechale},
  doi          = {10.1016/j.asoc.2024.111410},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111410},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep learning-based and adaptive region proposal algorithm for semantic segmentation},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A combined interval prediction system based on fuzzy
strategy and neural network for wind speed. <em>ASOC</em>, <em>155</em>,
111408. (<a href="https://doi.org/10.1016/j.asoc.2024.111408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind energy exhibits strong fluctuations and intermittencies. The accurate prediction of wind speed is of considerable significance for the operation and maintenance of wind farms and the safety of the power grid. However, previous studies have often ignored the impact of data noise on trend prediction, and lacked effective data pre-processing methods and adaptive interval prediction schemes, resulting in poor prediction results. To improve the accuracy of prediction, this paper proposes a wind-speed combination interval prediction system based on a fuzzy strategy and neural network. This study used a fuzzy strategy to pre-process the data and proposed a joint optimization algorithm that uses multiple multi-objective metaheuristic algorithms to optimize the neural network jointly. An improved prediction hybrid algorithm was used to reconstruct the predicted results. Finally, based on the fuzzy theory and neural network, interval prediction schemes that can adapt to different situations were proposed. Wind-speed data from the Penglai Wind Farm in China were used for verification. The mean absolute percentage error of the wind-speed point prediction was 3.75%, and the prediction interval coverage probability of the wind-speed interval prediction was 97.92%. The experimental results showed that the proposed model not only outperforms other comparable models but also improves the prediction interval coverage probability by 176.4% compared with the baseline model. This proves that the integrated hybrid model can improve the accuracy and effectiveness of wind-speed prediction. The accurate interval prediction of wind speed can result in the complementary scheduling of renewable energy generation and the sustainable development of the energy field, and support intelligent upgrades.},
  archive      = {J_ASOC},
  author       = {Yunbo Niu and Jianzhou Wang and Ziyuan Zhang and Yannan Yu and Jingjiang Liu},
  doi          = {10.1016/j.asoc.2024.111408},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111408},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A combined interval prediction system based on fuzzy strategy and neural network for wind speed},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving the tourist trip design problem with time windows
and variable profit using incremental local search. <em>ASOC</em>,
<em>155</em>, 111399. (<a
href="https://doi.org/10.1016/j.asoc.2024.111399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Team Orienteering Problem with Time Windows and Variable Profits (TOPTWVP) is a variant of the Orienteering Problem where each node has a predefined time window in which the service must start (in case this node is visited), and the vehicle may spend an amount of time given by a predefined interval so that the profit collected at this node depends on the time spent. Our previous work introduced an incremental local search procedure to solve this problem. Although the results were satisfying in terms of score, this algorithm required more time to obtain solutions due to the added complexity of working with service time intervals. To address this issue, this paper introduces several improvements to our previous implementation aimed at reducing the execution time and maintaining or increasing the quality of the solutions. Specifically, we designed a new procedure for obtaining the initial solution, which is faster and generates higher-quality solutions than the previous approach. Moreover, two new moves have been developed within the local search procedure to account for variable profits. The experiments will show that these new procedures outperform our previous implementations. Finally, we define different types of trips depending on the travel-style preferences of a tourist, and we build some scenarios to show how our TOPTWVP can be adapted to obtain routes that fulfil these preferences.},
  archive      = {J_ASOC},
  author       = {Eliseo Marzal and Laura Sebastia},
  doi          = {10.1016/j.asoc.2024.111399},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111399},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving the tourist trip design problem with time windows and variable profit using incremental local search},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic constrained multi-objective optimization based on
adaptive combinatorial response mechanism. <em>ASOC</em>, <em>155</em>,
111398. (<a href="https://doi.org/10.1016/j.asoc.2024.111398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic multi-objective optimization problems (DMOPs), objective functions, problem parameters, and constraints may change over time. Mainly, DMOPs use response mechanisms to generate the initial population after the environment changes. In this research, we develop an adaptive version of the combinational response mechanism (ACRM). ACRM uses three response mechanisms based on diversity, prediction, and memory to form the initial population. In ACRM, the number of solutions generated by a response mechanism is determined by reinforcement learning according to the severity of environmental changes. The background knowledge is transferred to reinforcement learning using the Q-value initialization method. Thus, in the early stages of optimization, when the experience gained from the environment is low, the proposed algorithm improves its performance using background knowledge. Also, we develop a new combinational constraint handling technique (CCHT). This method uses the dynamic information of the environment (i.e. the ratio of feasible solutions) to choose the appropriate constraint handling technique. The results of the tests on 23 dynamic test functions and seven dynamic constrained test functions indicate that the performance of the proposed algorithm can compete with advanced evolutionary algorithms in terms of the degree of convergence and variety of solutions. Permanent link to reproducible Capsule: .},
  archive      = {J_ASOC},
  author       = {Zahra Aliniya and Seyed Hossein Khasteh},
  doi          = {10.1016/j.asoc.2024.111398},
  journal      = {Applied Soft Computing},
  month        = {4},
  pages        = {111398},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic constrained multi-objective optimization based on adaptive combinatorial response mechanism},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A semi-supervised framework for computational fluid
dynamics prediction. <em>ASOC</em>, <em>154</em>, 111422. (<a
href="https://doi.org/10.1016/j.asoc.2024.111422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven deep learning approach heavily relies on the diversity and quantity of data. Acquiring data in the computational fluid dynamics (CFD) domain is a time and computationally intensive process. This paper proposes a semi-supervised learning method called discriminative regression fitters (DRF) for aerodynamic prediction of airfoils. DRF utilizes neural networks’ memory property to dynamically divide pseudo-labeled data into easy and difficult subsets using a model of Gaussian distribution. The method classifies unlabeled data based on loss and updates the pseudo-labeled data, improving the model’s generalization capability. Experiments on airfoil regression task datasets show that DRF achieves similar or better prediction accuracy than fully supervised approaches. It reduces data acquisition time by 70%. Ablation studies and qualitative results verify the effectiveness of DRF. The surrogate model obtained from DRF is extended to airfoil optimization, demonstrating its practicality. DRF provides a promising direction for improving the regression task while reducing the reliance on large amounts of CFD data.},
  archive      = {J_ASOC},
  author       = {Xiao Wang and Yidao Dong and Shufan Zou and Laiping Zhang and Xiaogang Deng},
  doi          = {10.1016/j.asoc.2024.111422},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111422},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A semi-supervised framework for computational fluid dynamics prediction},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Momentum accelerated unfolding network with spectral–spatial
prior for computational spectral imaging. <em>ASOC</em>, <em>154</em>,
111420. (<a href="https://doi.org/10.1016/j.asoc.2024.111420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is a key technology in computational imaging, where the algorithms determine reconstruction quality and reconstruction speed. Recovering spectral information from ill-conditioned data of coded aperture snapshots is a novel but challenging solution in spectral imaging for dynamic scenes. Further study is necessary to enhance the edge computing power of computational spectral programs. Deep unfolding networks have made considerable headway in this direction, but the existing methods still have two drawbacks: (1) The 3D spectral cube exhibits long-range dependency and non-local self-similarity in both spatial and spectral dimensions, yet learn the global features of spatial and spectral is difficult, which remains to be investigated. (2) The global features degenerate as the number of iterations increases. To solve the above problems, this paper proposes a Momentum Accelerated Unfolding Network with Spectral–Spatial prior (MAUNSS). Specifically, a denoising module based on a spectral–spatial transformer is designed, which uses both sparse spectral features to recover global information and dense spatial features to enhance detail texture. Within such a framework, cross-stage feature transmission channels transmitting features across spectral and spatial transformers at different stages are built to avoid feature degeneration. In order to increase the training speed and save the time and cost associated with the iterative approach, we improve it by means of the momentum acceleration module. Supplemented by a re-projection loss, a technique introduced from 3D measurement, the accuracy is further improved. To the best of our knowledge, the proposed method is an attempt to use momentum to expedite the convergence of unfolding networks. Extensive experiments demonstrate that our proposed method outperforms previous state-of-the-art methods by more than 1.94 dB.},
  archive      = {J_ASOC},
  author       = {Zeyu Cai and Chunlu Li and Yi Yu and Chengqian Jin and Feipeng Da},
  doi          = {10.1016/j.asoc.2024.111420},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111420},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Momentum accelerated unfolding network with spectral–spatial prior for computational spectral imaging},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary multi-objective overlapping community detection
based on fusion of internal and external connectivity and correction of
node intimacy. <em>ASOC</em>, <em>154</em>, 111414. (<a
href="https://doi.org/10.1016/j.asoc.2024.111414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of community detection, node attribute information plays an important role in community division. Existing methods use topology structure and node attribute information to discover non-overlapping communities. However, so far, attribute information has not been fully utilized in overlapping community detection. To address this, we propose a new overlapping community detection method called “evolutionary multi-objective overlapping community detection based on Fusion of internal and external Connectivity and Correction of Node Intimacy” (FCCNI). Firstly, we propose a fusion strategy based on internal and external connectivity, which integrates some communities with sparse intra-connections and dense inter-connections. This automatically determines, reconfirms, and corrects the number of communities. Secondly, a function is designed to calculate the intimacy between nodes, and the node label with the highest intimacy is selected to correct the current wrong node. The correction strategy is used in two stages of initialization and multi-objective evolution to obtain a more accurate node label. Finally, a method that considers not only the connections of the community, but also the node attribute, is designed to obtain the overlapping community indirectly from the non-overlapping community. The experimental results on five real-life networks and four classical synthetic networks show that FCCNI achieves better overlapping community division, compared with six state-of-the-art comparison algorithms from the literature.},
  archive      = {J_ASOC},
  author       = {Ronghua Shang and Sa Wang and Weitong Zhang and Jie Feng and Licheng Jiao and Rustam Stolkin},
  doi          = {10.1016/j.asoc.2024.111414},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111414},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-objective overlapping community detection based on fusion of internal and external connectivity and correction of node intimacy},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep correlation network for synthetic speech detection.
<em>ASOC</em>, <em>154</em>, 111413. (<a
href="https://doi.org/10.1016/j.asoc.2024.111413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetic speech is becoming increasingly rampant, and automatic speaker verification (ASV) systems are vulnerable to its attacks. However, most current synthetic speech detection methods focus on the influence of a single feature in the detection. Since different features can represent the difference between real speech and synthetic speech to a certain extent, there must be common information between different types of features. Effectively finding and fully utilizing this information will facilitate the extraction of better discriminative features and achieve improved performance. Based on the above analysis, we propose a deep correlation network (DCN) to learn the latent common information between different embeddings. It consists of two parts, the bi-parallel network and the correlation learning network. Bi-parallel networks consist of different neural models to learn the middle-level representations from front-end acoustical features. The correlation learning network is the core part of the DCN and is proposed to explore the common information between the above middle-level features. The common information obtained after DCN processing have better discriminative ability for synthetic speech detection. Experimental results show that the proposed DCN can significantly improve the performance of synthetic speech detection system on ASVspoof 2019 and ASVspoof 2021 logical access sub-challenge.},
  archive      = {J_ASOC},
  author       = {Chen Chen and Bohan Dai and Bochao Bai and Deyun Chen},
  doi          = {10.1016/j.asoc.2024.111413},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111413},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep correlation network for synthetic speech detection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effects of diagram plane on neural network based modulation
recognition. <em>ASOC</em>, <em>154</em>, 111412. (<a
href="https://doi.org/10.1016/j.asoc.2024.111412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modulation recognition using deep learning presents challenges in effectively distinguishing high-order modulation schemes while maintaining a balance between complexity and recognition accuracy. In this study, we curate a comprehensive dataset in the r θ rθ plane, encompassing eight distinct modulation schemes. Leveraging hyperparameter optimization and transfer learning, we explore the capabilities of various CNN-based architectures, including MobileNetV2, ResNet50V2, ResNet101V2, InceptionV3, ResNet152V2, Xception, and InceptionResNetV2, for the classification of modulation schemes. The simulation results demonstrate that with signal-to-noise ratio (SNR) values exceeding 5 dB, all models exhibit classification accuracies surpassing 50% and approach near-perfect accuracy at an SNR value of 20 dB. However, under low SNR conditions, such as 5 dB, the recognition accuracies of all models, except for ResNet152V2 and InceptionV3, show minimal variation. As the SNR increases by 5 dB from −5 dB to 20 dB, ResNet152V2 and InceptionV3 demonstrate remarkable classification accuracy improvements, exceeding 40%, 30%, 30%, 10%, and 15%, respectively. In contrast, the other models do not exhibit such robust responsiveness in accuracy enhancements. The remarkable performance improvements are achieved by fine-tuning pre-trained models through these processes.},
  archive      = {J_ASOC},
  author       = {Merih Leblebici and Ali Çalhan and Murtaza Cicioğlu},
  doi          = {10.1016/j.asoc.2024.111412},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111412},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effects of diagram plane on neural network based modulation recognition},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task modeling and multifactorial optimization for path
coverage problem of automated test case generation. <em>ASOC</em>,
<em>154</em>, 111407. (<a
href="https://doi.org/10.1016/j.asoc.2024.111407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research in automated test case generation (ATCG) focuses on multi-objective optimization using functions based on path structure (F-PS) to solve the path coverage (PC) problem. Despite the similarity among F-PSs, the existing multi-objective optimization models fail to consider using the similarity to effectively promote optimization among multiple objectives. Inspired by the similarity and multitask optimization, this paper first establishes a multitasking path coverage (MtPC) model with two different F-PSs as its tasks. A multifactorial optimization framework for solving MtPC model (MfO-PC) is then proposed to optimize the tasks by assortative mating and to cooperatively generate desired test cases by automatic assignment strategy. Three multifactorial optimization algorithms based on the framework are then designed and tested on twelve benchmark programs. Experimental results show that the effectiveness of the proposed model and the designed algorithms based on MfO-PC framework achieve the highest path coverage with fewer test cases and less running time than some compared state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Xupeng Wang and Zhongbo Hu and Lingyi Shi and Gaocheng Cai and Qinghua Su},
  doi          = {10.1016/j.asoc.2024.111407},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111407},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-task modeling and multifactorial optimization for path coverage problem of automated test case generation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-supervised hypergraph neural network for session-based
recommendation supported by user continuous topic intent. <em>ASOC</em>,
<em>154</em>, 111406. (<a
href="https://doi.org/10.1016/j.asoc.2024.111406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation aims to predict the next item an anonymous user is most likely to click based on its limited historical behaviors. However, in session-based recommendation, the anonymity and short sequences often lead to the issue of data sparsity, which can affect the performance of recommendation models. Existing recommendation methods mostly focus on individual items in the session sequence, without considering the implicit user continuous intent underlying multiple continuous items. Moreover, current graph-based recommendation models do not fully take into account the direction of information transfer among shared items when calculating their similarities. This paper proposes, TCAUIS, a topic classification augmented user intend self-supervised hypergraph neural network for session-based recommendation, which contains continuous topic intent of users within a session and the information of co-occurrence items. The TCAUIS models a individual session sequence as a topic intent hypergraph and co-occurrence graph respectively, and employs latent Dirichlet allocation to classify the topic of the dataset and fuses the topic hyperedges into the hypergraph to alleviate the sparsity of the data and enhance the session information. Additionally, the hypergraph attention networks and attention mechanisms are leveraged to capture the complex high-order information between nodes in both the hypergraphs and co-occurrence graphs. The continuous topic intent of users and information of co-occurrence relationship are effectively integrated using a self-supervised mechanism that maximizes the mutual information between the representations of two sessions. Extensive experimental results show that the proposed TCAUIS model outperforms the state-of-the-art models on three real datasets Diginetica, Yoochoose1/64 and Nowplaying.},
  archive      = {J_ASOC},
  author       = {Fan Yang and Dunlu Peng and Shuo Zhang},
  doi          = {10.1016/j.asoc.2024.111406},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111406},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised hypergraph neural network for session-based recommendation supported by user continuous topic intent},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Effectively answering why questions on structural graph
clustering. <em>ASOC</em>, <em>154</em>, 111405. (<a
href="https://doi.org/10.1016/j.asoc.2024.111405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pSCAN algorithm is widely acknowledged for its efficiency in structural graph clustering across diverse graph applications, frequently utilized to detect significant clusters within graphs. The results of pSCAN are constrained by two parameters: (1) the structural similarity constraint ϵ ϵ ; (2) the density constraint μ μ . Nevertheless, determining appropriate values for these parameters proves challenging as users often lack the requisite professional knowledge. Consequently, users may inquire about the inclusion of unexpected vertices in specified clusters, posing w h y why questions. In this paper, we aim to address these inquiries by investigating the challenge of answering why questions on structural graph clustering. The objective is to refine the initialized clustering parameters to exclude unexpected vertices from specified clusters. Initially, we introduce the crucial concept of l a b e l label - v e r t i c e s vertices and delve into a unified explanation framework designed for addressing why questions on structural graph clustering. Subsequently, we propose two effective refining algorithms, namely FReEPS and FReMU , specifically tailored to modify the similarity constraint ϵ ϵ and the density constraint μ μ independently. Moreover, in our approach to enhancing efficiency, we introduce two baseline explanation algorithms, ReParFirstMU and ReParFirstEPS , simultaneously refining the parameters ϵ ϵ and μ μ to exclude unexpected vertices from specified clusters. Additionally, we explore two improved algorithms, FReParFirstMU and FReParFirstEPS , incorporating a process of pruning unnecessary parameter combinations and computations. This enhancement aims to improve the efficiency of simultaneously refining the two parameters for addressing these questions. Finally, we conducted comprehensive experiments on real social network datasets. The results illustrate that the explanation efficiency of FReEPS and FReMU surpasses that of the state-of-the-art explanation algorithms by 1-2 orders of magnitude. Moreover, FReMu exhibits faster performance than FReEPS. Furthermore, simultaneous refinement of the parameters ϵ ϵ and μ μ yields better-refined parameters. Additionally, FReParFirstMU and FReParFirstEPS outperform ReParFirstMU and ReParFirstEPS, respectively.},
  archive      = {J_ASOC},
  author       = {Chuanyu Zong and Chengwei Zhang},
  doi          = {10.1016/j.asoc.2024.111405},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111405},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Effectively answering why questions on structural graph clustering},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy clustering-based neural network based on linear
fitting residual-driven weighted fuzzy clustering and convolutional
regularization strategy. <em>ASOC</em>, <em>154</em>, 111403. (<a
href="https://doi.org/10.1016/j.asoc.2024.111403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a reinforced Fuzzy Clustering-based Neural Network (FCNN) is introduced as an augmented FCNN architecture to address regression issues. It is widely recognized that regardless of the design method and rules employed by a fuzzy model, the determination of fuzzy sets remains a crucial aspect. FCNN and its improved variants utilize conventional fuzzy clustering algorithms to partition the feature space into fuzzy sets. However, this approach tends to disregard the distinctions inherent in data patterns. Although FCNN is a nonlinear model in relation to the input variables, it is a linear model with respect to the parameters that need to be estimated. Inspired by this, our method incorporates a pre-training phase where we utilize sample residuals from a linear regression algorithm to measure differences between data patterns. These differences are subsequently integrated into the fuzzy partition, yielding more refined fuzzy sets. To combat overfitting that can degrade the model’s predictive capability, we introduce a convolutional L 2 L2 regularization strategy that integrates the convolution operator from harmonic analysis into the construction of L 2 L2 regularization. Compared to conventional L 2 L2 regularization, this convolutional regularization strategy is more effective in improving the regularity of the design matrix, thereby reducing the variation between coefficients and enhancing the model’s generalization ability. The efficacy of the presented method is substantiated by experimental studies conducted on both synthetic and real-world datasets.},
  archive      = {J_ASOC},
  author       = {Fan Bu and Congcong Zhang and Eun-Hu Kim and Dachun Yang and Zunwei Fu and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2024.111403},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111403},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy clustering-based neural network based on linear fitting residual-driven weighted fuzzy clustering and convolutional regularization strategy},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implementing the confidence constraint cloud-edge
collaborative computing strategy for ultra-efficient arrhythmia
monitoring. <em>ASOC</em>, <em>154</em>, 111402. (<a
href="https://doi.org/10.1016/j.asoc.2024.111402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram(ECG) monitoring is a critical and intricate task in cardiac healthcare. While large models supported by the remote cloud servers with abundant computational resources offers a feasible solution for this task, which still face challenges related to processing costs, privacy risks, and response time. Efforts have been made to integrate edge computing as a supplementary solution, but the current collaborative computing strategy for ECG monitoring is static and inefficient. To address these shortcomings, this study proposes an innovative confidence constraint cloud–edge collaborative computing(3CE2C) strategy. Firstly, the model implementation processes are illustrated, including an ultra-lightweight model for the edge node and a large model for the cloud server. To enhance classification performance, the personalized strategy is employed, resulting in an accuracy improvement from 0.9849 to 0.9929 in the model-cloud. For edge implementation, the optimal input length and model quantization both are explored to reduce the energy consumption. Based on the given confidence constraint, the models dynamically collaborate, with the low-confidence samples uploaded to the cloud server. This approach can achieve accuracy comparable to cloud computing, transmitting only about 17% low-confidence samples, the accuracy ratio( r A c c rAcc ) is 0.9985. In addition, the method is validated in SVDB, where 3CE2C outperforms state-of-the art framework with the same uploaded sample ratio, resulting in a 1.54% improvement in classification accuracy. In conclusion, the proposed method provides a practical solution in real-time arrhythmia detection applications.},
  archive      = {J_ASOC},
  author       = {Jiarong Chen and Xianbin Zhang and Lin Xu and Victor Hugo C. de Albuquerque and Wanqing Wu},
  doi          = {10.1016/j.asoc.2024.111402},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111402},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Implementing the confidence constraint cloud-edge collaborative computing strategy for ultra-efficient arrhythmia monitoring},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Broad learning: A GPU-free image-based malware
classification. <em>ASOC</em>, <em>154</em>, 111401. (<a
href="https://doi.org/10.1016/j.asoc.2024.111401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s cybersecurity landscape, software security companies encounter a significant challenge in detecting new and unknown malware. Despite the introduction of various machine learning and deep learning tools designed to identify malicious software based on static and dynamic features, achieving the desired level of accuracy remains elusive. This challenge is exacerbated by factors such as encryption, packing, limited distribution, and uneven allocation of malware samples across different families. Moreover, deep learning techniques demand substantial time, computational resources (specifically GPUs), and expertise from data scientists for practical malware analysis. In response to these challenges, we propose a novel GPU-free approach called Image-based Malware Classification using Broad Learning (IMCBL) to address these issues. Our method integrates visualization, feature decomposition, and broad learning architecture to enhance malware detection and classification. We convert raw malware binaries into images, reducing the necessity for extensive feature engineering. These images transform using truncated Singular Value Decomposition (SVD) to reduce the feature vector size, expediting the training process while mitigating model overfitting. The transformed feature vector is then input into our proposed Broad Learning (BL) system, which facilitates malware detection and classification. The BL architecture, structured as a flat network mapping original inputs to feature nodes and expanding the structure in enhancement nodes, ensures efficient and effective classification without the need for retraining. This dynamic and incremental learning capability sets IMCBL apart, making it superior to existing deep learning architectures. To validate our approach, we conducted extensive experiments using five benchmark malware datasets, including the Microsoft Windows malware challenge dataset, the Malimg Windows malware dataset, the IoT-Android mobile malware dataset, the Big Windows malware dataset, and an obfuscated Windows malware dataset. The results demonstrate IMCBL’s remarkable success in classifying most malware samples, even under obfuscation attacks, performing comparably or outperforming current methods using similar benchmarks. Specifically, IMCBL achieved 95.58 % 95.58% accuracy for the Microsoft Windows malware dataset, 97.64 % 97.64% accuracy for the Malimg Windows malware dataset, 96.51 % 96.51% accuracy for IoT Android malware datasets, and 96.19 % 96.19% accuracy for the extensive Windows malware dataset. Additionally, IMCBL demonstrated 93.04 % 93.04% accuracy for an obfuscated Windows malware dataset, which contains both packed and unpacked malware samples. Notably, IMCBL exhibits an exponential advantage in computation overhead, including training and prediction time, when compared to traditional machine learning and advanced state-of-the-art deep learning architectures such as VGG16, ResNet50, and InceptionV3.},
  archive      = {J_ASOC},
  author       = {Danish Vasan and Mohammad Hammoudeh and Mamoun Alazab},
  doi          = {10.1016/j.asoc.2024.111401},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111401},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Broad learning: A GPU-free image-based malware classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-teaching: A three-way decision framework to handle
noisy labels. <em>ASOC</em>, <em>154</em>, 111400. (<a
href="https://doi.org/10.1016/j.asoc.2024.111400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with noisy labels represents a prevalent weakly supervised learning paradigm. Uncertain knowledge resulting from noisy labels poses significant challenges for knowledge analysis. Given the memorization effect observed in deep neural networks, training on instances with minimal loss holds promise for effectively handling noisy labels. “Co-teaching”, which is the state-of-the-art training method in this field, is characterized by the simultaneous training of two deep neural networks using instances with low loss. While this approach has demonstrated promising performance, its effectiveness heavily relies on the predictive capabilities of two neural networks. If these networks fail to provide reliable predictions, the overall learning performance may be unsatisfactory. In order to solve this problem and inspired by three-way decision, we propose a powerful learning paradigm named “Three-teaching”, which employs the “voting mechanism” to guarantee the prediction quality incrementally. In this approach, both neural networks make predictions for all the data. However, only the data that exhibits consistent prediction results and has a low loss is retained to feed into the third neural network for updating its parameters. The learning process will proceed by alternating these three neural networks’ roles. The experimental results obtained from benchmark datasets illustrate that “Three-teaching” surpasses numerous state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Guoqing Chao and Kaiwen Zhang and Xiru Wang and Dianhui Chu},
  doi          = {10.1016/j.asoc.2024.111400},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111400},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-teaching: A three-way decision framework to handle noisy labels},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Perceive, reason, and align: Context-guided cross-modal
correlation learning for image–text retrieval. <em>ASOC</em>,
<em>154</em>, 111395. (<a
href="https://doi.org/10.1016/j.asoc.2024.111395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inconsistency in feature representations between different modalities, namely “Heterogeneous gap”, it remains a persistent challenge to correlate images and texts. Existing studies on image–text retrieval (ITR) mainly emphasize on inter-modal correlation learning through aligning instances or their patches from different modalities. However, it is hard to break through performance bottlenecks of ITR without powerfully supporting from intra-modal correlation. Unfortunately, few studies have sufficiently considered two critical tasks in intra-modal correlation learning: (1) intricate contextual information perceiving, and (2) intrinsic semantic relationships reasoning. Therefore, in this paper, we propose the Context-guided Cross-modal Correlation Learning (CCCL) framework for ITR under a novel paradigm: “Perceive, Reason, and Align”. Firstly, in the stage of “Perceive”, the context-guided mechanism based on the self-attention and gate mechanism is proposed to fully discover contextual information within modalities, eliminating unnecessary interactions between local-level patches. Secondly, in the stage of “Reason”, graph convolutional network with the residual structure is used to uncover relationships among patches within each modality to make reasonable inferences. Thirdly, in the stage of “Align”, to achieve precise inter-modal alignment, the complementarity between different modalities from both global-level and local-level is effectively mined and fused. Finally, to optimize our proposed CCCL framework, the hybrid loss is constructed by combining the cross-modal coherence term with the cross-modal alignment term. Our approach yields highly competitive results on two publicly available ITR datasets, that is, Flickr30K and MS-COCO.},
  archive      = {J_ASOC},
  author       = {Zheng Liu and Xinlei Pei and Shanshan Gao and Changhao Li and Jingyao Wang and Junhao Xu},
  doi          = {10.1016/j.asoc.2024.111395},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111395},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Perceive, reason, and align: Context-guided cross-modal correlation learning for image–text retrieval},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid of jellyfish and particle swarm optimization
algorithm-based support vector machine for stock market trend
prediction. <em>ASOC</em>, <em>154</em>, 111394. (<a
href="https://doi.org/10.1016/j.asoc.2024.111394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Market prediction is a pivotal research domain within the financial market. The continuous evolution of information and communication technology has not only led to an exponential increase in data volume but has also introduced greater diversity in data formats. Thus, this study proposes a novel prediction model employing a hybrid of jellyfish and particle swarm optimization (HJPSO) algorithms. This hybrid model is designed to effectively manage the overwhelming volume of data, including technical indicators and financial news, while simultaneously optimizing the parameters of the support vector machine (SVM). In addition to its predictive capabilities, the study incorporates a rule extraction method, shedding light on the decision rules inherent in the SVM post-prediction. Computational results indicate that the proposed HJPSO-SVM is superior to existing algorithms in terms of accuracy and trading simulation. The incorporation of both stock indicators and news data emerges as a key factor contributing to enhanced predictive performance. This comprehensive approach reveals the significance of integrating diverse data sources for more robust market predictions.},
  archive      = {J_ASOC},
  author       = {R.J. Kuo and Tzu-Hsuan Chiu},
  doi          = {10.1016/j.asoc.2024.111394},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111394},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid of jellyfish and particle swarm optimization algorithm-based support vector machine for stock market trend prediction},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian network learning method for sparse and unbalanced
data with GNN-based multilabel classification application.
<em>ASOC</em>, <em>154</em>, 111393. (<a
href="https://doi.org/10.1016/j.asoc.2024.111393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Creating a well-defined Bayesian network (BN) is helpful for developing effective graph neural networks (GNNs) that exploit annotated labels in multilabel classification (MLC) tasks. Obtaining correct BNs can be challenging when the labels are highly unbalanced and sparse. This study proposes a novel scoring method to address data imbalance and sparsity by introducing an imbalanced weight term. Comparison results show that the BNs learned from the proposed scoring method are simpler and more accurate than the well-known methods. The obtained BNs based on the proposed method are used to construct GNNs for MLC tasks based on three datasets. The findings suggest that the adjacency matrices based on moral graphs of BNs exhibit more excellent stability than those derived from mathematical approaches. By selecting the parameter thoughtfully, GNNs based on the proposed BDsp scores showcase up to 12% classification performance improvement compared to GNNs based on existing Bayesian scores.},
  archive      = {J_ASOC},
  author       = {Ling Chen and Xiangming Jiang and Yuhong Wang},
  doi          = {10.1016/j.asoc.2024.111393},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111393},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bayesian network learning method for sparse and unbalanced data with GNN-based multilabel classification application},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DNNAM: Image inpainting algorithm via deep neural networks
and attention mechanism. <em>ASOC</em>, <em>154</em>, 111392. (<a
href="https://doi.org/10.1016/j.asoc.2024.111392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most image inpainting algorithms have problems such as fuzzy images, texture distortion and semantic inaccuracy, and the image inpainting effect is limited when processing photos with large missing sections and resolution levels. The paper proposes an effective image inpainting algorithm via partial multi-scale channel attention mechanism and deep neural networks to address the above phenomenon that existing image inpainting methods using deep learning modules have insufficient perception and representation capabilities for multi-scale features with high proportion of irregular defects. Initially, we used the Res-U-Net module as a generator. The U-Net-like backbone network topology can achieve the encoding and decoding stages of damaged images. Secondly, the residual network structure was built in the encoder and decoder to improve the ability of the proposed network to extract and display the features of the damaged images. Finally, the partial multi-scale channel attention module was inserted in the skip connection with the decoder to increase the efficiency of using the low-level features of the original images. The experimental results of the research can show that the proposed method outperforms state-of-the-art methods in terms of subjective visual perception and objective evaluation indicators on the CelebA , Places2 and Paris Street View datasets.},
  archive      = {J_ASOC},
  author       = {Yuantao Chen and Runlong Xia and Kai Yang and Ke Zou},
  doi          = {10.1016/j.asoc.2024.111392},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111392},
  shortjournal = {Appl. Soft. Comput.},
  title        = {DNNAM: Image inpainting algorithm via deep neural networks and attention mechanism},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing workload distribution in fog-cloud ecosystem: A
JAYA based meta-heuristic for energy-efficient applications.
<em>ASOC</em>, <em>154</em>, 111391. (<a
href="https://doi.org/10.1016/j.asoc.2024.111391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog-integrated Cloud has emerged as a novel computing paradigm that brings Cloud computing services to the network&#39;s edge in real-time, though with limited capabilities. Despite its advantages, there are several challenges including workload distribution, energy consumption, computational time, and network latency, that require attention. The workload of IoT applications can be distributed over the Fog or Cloud devices based on their priority, deadline, and latency restrictions. In this work, we introduce a novel population-based metaheuristic called MAYA, a modified variant of the JAYA algorithm, to address the Energy-Efficient Workload Distribution of Sensors (EEWDS) in the Fog-Cloud ecosystem. The workload distribution of IoT applications depends on several factors such as request deadlines, the energy consumed during transmission, and needed computation. The performance of the proposed model for the energy consumption, computation time, C O 2 CO2 emission, fairness index, and the convergence rate, is evaluated through simulation experiments. The results are compared in two scenarios: one concerning to methodology, where the performance is compared with JAYA, Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) techniques. The other scenario is based on the environment, where we examine Cloud-only, Fog-only, and Fog-Cloud integrated environments. Compared to JAYA, GA, PSO and ACO, the proposed MAYA technique demonstrates significant improvements, including reduction in energy consumption by 34.76%, 88.92%, 85.36% and 93.84%; decrease in computation time by 37.64%, 85.07%, 87.22%, and 91.08%; decrease in C O 2 CO2 emissions by 23.46%, 76.24%, 97.17%, and 99.02%; and increase in fairness index by 9.62%, 3.72%, 16.90%, and 15.26% respectively.},
  archive      = {J_ASOC},
  author       = {Satveer Singh and Eht E. Sham and Deo Prakash Vidyarthi},
  doi          = {10.1016/j.asoc.2024.111391},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111391},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimizing workload distribution in fog-cloud ecosystem: A JAYA based meta-heuristic for energy-efficient applications},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of supervised random forest paradigms based on
optimization and post-hoc explanation in underground stope stability
prediction. <em>ASOC</em>, <em>154</em>, 111388. (<a
href="https://doi.org/10.1016/j.asoc.2024.111388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study applies a supervised random forest (SRF) paradigm to predict the underground stope stability. To improve the model prediction accuracy, 400 actual stopes with three main features (the optimized stability number (N), the hydraulic radius (HR), and a rock mass quality factor (Q)) are used to train the model, and nine meta-heuristic (MH) optimization algorithms are utilized to select the optimal hyperparameters of the SRF model. The prediction results show that the tiki taka algorithm-based SRF (TTA-SRF) model obtains the most satisfactory classification performance, with the smallest variance in the overfitting evaluation (Precision of 0.0201, Recall of 0.0257, Specificity of 0.0061, Accuracy of 0.0125, and F1-measure of 0.0246). Furthermore, four post-hoc explainable techniques (i.e., model-independent) including the feature importance (FI), the Shapley additive explanations (SHAP), the partial dependence plot (PDP), and the individual conditional expectation (ICE) are adopted to explain the best prediction model. The results of the model interpretation show that all features are essential for predicting the stope stability and they have an opposite tendency on the stability prediction of stable and caved stopes. In particular, the limits at which HR, Q, and N do not contribute to the stope stability prediction are 18 m, 40, and 100, respectively. The results of optimization designs for a specific case stope based on the visualization program showed that the suggestions given by the prediction model are desirable. In conclusion, a high-performance and strongly explanatory prediction model is proposed in this study to facilitate the refinement of stope stability assessment in underground space.},
  archive      = {J_ASOC},
  author       = {Chuanqi Li and Xiancheng Mei and Jiamin Zhang},
  doi          = {10.1016/j.asoc.2024.111388},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111388},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of supervised random forest paradigms based on optimization and post-hoc explanation in underground stope stability prediction},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatial–temporal model for network-wide flight delay
prediction based on federated learning. <em>ASOC</em>, <em>154</em>,
111380. (<a href="https://doi.org/10.1016/j.asoc.2024.111380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network-wide flight delay prediction has emerged as a severe and prominent issue within the realm of intelligent aviation systems, owing to its crucial impact on flight scheduling, airline planning, and airport operations. The methodology employed in flight delay prediction has undergone substantial evolution in recent years, progressing from rudimentary statistical models to more sophisticated and intricate deep learning models. Nevertheless, the majority of current research endeavors primarily concentrate on the utilization of all available local data to collectively construct a prediction model, enduring significant challenges pertaining to data security and privacy issues as well as enormous communication overhead. To tackle these challenges, this investigation centers its attention on developing a hybrid federated deep learning model (HFDL), which only emphasizes updating the model parameters exclusively on the central server without the need for accessing or sharing any private data. Firstly, we employ the Louvain algorithm to partition the global spatial graph of the airport network into several sub-regions. Then, we present a local training model for each sub-region, which uses a diffusion graph convolutional network (DGCN) and residual gated recurrent unit (RGRU) to model the complex local temporal and spatial dependencies of the airport network. The model utilizes a branch structure to predict the flight delay at each airport within a sub-region. Finally, the local models are aggregated through federated learning to form a vigoroso central model that bridges the limits on global data sharing and privacy guarantee. Experiments are carried out on a real flight delay dataset from the U.S. Bureau of Transportation Statistics (BTS) database with the aim of analyzing the performance of the proposed prediction model. The experimental results demonstrate that the HFDL model can attain excellent prediction performance without the interaction of sub-region raw data and improve training efficiency. Moreover, the model has strong generalization ability. The visualization results demonstrate that the proposed model can effectively explore the spatial–temporal dependencies of flight delays, providing guidance for formulating air traffic control strategies.},
  archive      = {J_ASOC},
  author       = {Xiuyu Shen and Jingxu Chen and Ran Yan},
  doi          = {10.1016/j.asoc.2024.111380},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111380},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A spatial–temporal model for network-wide flight delay prediction based on federated learning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrative prognostic modeling for breast cancer: Unveiling
optimal multimodal combinations using graph convolutional networks and
calibrated random forest. <em>ASOC</em>, <em>154</em>, 111379. (<a
href="https://doi.org/10.1016/j.asoc.2024.111379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most crucial step in the clinical decision-making process for patients with breast cancer is the accurate prediction of prognosis and survival length. Correct prognosis prediction aids in making the best treatment decision and may even lessen the effects of cancer. To achieve this, we have developed a novel predictive model using multimodal graph convolutional networks with calibrated random forest (MGCN-CalRF) for forecasting the prognosis of breast cancer by combining multiple sources of information or modalities. We have considered six different modalities, namely, mRNASeq, DNA methylation, Copy number variation, miRSeq, Clinical, and Whole Slide Image data, which have been retrieved from TCGA Database. We have applied a Graph Convolutional Network for the feature extraction from individual modalities to grasp the structural relationships between data. Further, we concatenated all the extracted features concerning various combinations of modalities aiming to find the optimal combination of available modalities. The concatenated features from the Graph Convolutional Network are further fed to the Calibrated Classifier Model using Random Forest for the final prediction. It has been observed that the Graph Convolutional model which has been trained with the combination of three modalities, namely Clinical, miRSeq, and Whole slide Image data outperforms not only the combination of other modalities but also the other state-of-the-art models. This model attained accuracy, F1-score, and AUC of 0.771, 0.867, and 0.729, respectively.},
  archive      = {J_ASOC},
  author       = {Susmita Palmal and Nikhilanand Arya and Sriparna Saha and Somanath Tripathy},
  doi          = {10.1016/j.asoc.2024.111379},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111379},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrative prognostic modeling for breast cancer: Unveiling optimal multimodal combinations using graph convolutional networks and calibrated random forest},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fusion of linear and non-linear dimensionality reduction
techniques for feature reduction in LSTM-based intrusion detection
system. <em>ASOC</em>, <em>154</em>, 111378. (<a
href="https://doi.org/10.1016/j.asoc.2024.111378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Securing networks is becoming increasingly crucial due to the widespread use of information technology. Intrusion Detection System (IDS) plays a crucial role in network security by detecting potential security threats in real-time. In order to create effective IDS, Deep Learning (DL) techniques like Auto Encoder (AE) and Long Short-Term Memory (LSTM) have been widely used. However, the high dimensionality and complexity of network traffic data make it challenging to extract meaningful information. The paper proposes a fusion-based approach that combines AE and Principal Component Analysis (PCA) techniques for dimensionality reduction in IDS. The proposed approach aims to capture both linear and non-linear relationships between features while reducing the dimensionality of the input data. NSL-KDD, UNSW-NB15, CIC-IDS-2017, and MSCAD datasets are used to evaluate this proposed method. The proposed approach has improved accuracy compared to the existing AE+LSTM-based approach by 3% for NSL-KDD and around 1% for UNSW-NB15 and CIC-IDS-2017, while the proposed approach gives comparable results for the MSCAD dataset. The Wilcoxon signed-rank test has been applied to confirm the statistical significance of the result.},
  archive      = {J_ASOC},
  author       = {Ankit Thakkar and Nandish Kikani and Rebakah Geddam},
  doi          = {10.1016/j.asoc.2024.111378},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111378},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fusion of linear and non-linear dimensionality reduction techniques for feature reduction in LSTM-based intrusion detection system},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality assessment of view synthesis based on unsupervised
quality-aware pre-training. <em>ASOC</em>, <em>154</em>, 111377. (<a
href="https://doi.org/10.1016/j.asoc.2024.111377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current view synthesis quality metrics mainly rely on hand-crafted features, which have clear physical meanings but fail to comprehensively describe the complex distortion characteristics in synthesized images. With the rapid advancement of deep models, convolutional neural networks hold great promise for learning complex distortion representations. The exceptional performance of deep learning is closely tied to the availability of abundant labeled training data. However, manual annotation of labels for quality assessment tasks is arduous, which impede the application of deep learning in this area. With this motivation, this paper presents a no-reference quality assessment model for view synthesis based on unsuperviseD quality-Aware Pre-Training (DAPT). Specifically, a two-stream network with spatial destruction sensitivity and adaptive heterogeneous awareness branches is firstly designed, and then the two branch networks are pre-trained unsupervised to fully extract the quality-aware feature representations. Finally, a multi-layer perceptron is utilized to generate quality scores based on the spatial domain destruction and structural damage information. Notably, to better align the quality-aware features learned through unsupervised pre-training in the source domain with those of the target domain, we introduce a domain adaptive module in the adaptive heterogeneous awareness branch. Extensive experiments demonstrate that the proposed DAPT model outperforms the state-of-the-arts.},
  archive      = {J_ASOC},
  author       = {Haozhi Shi and Yipo Huang and Lizhe Wang and Lanmei Wang},
  doi          = {10.1016/j.asoc.2024.111377},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111377},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quality assessment of view synthesis based on unsupervised quality-aware pre-training},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M2GAN: Mimicry fashion generation combined with the two-step
mullerian evolutionary hypothesis. <em>ASOC</em>, <em>154</em>, 111375.
(<a href="https://doi.org/10.1016/j.asoc.2024.111375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mimicry fashion design combines mimicry samples and reference clothing to design novel clothes that can greatly promote the upgrading of clothing styles. The challenge of mimicry fashion generation is that the created clothing needs to balance detailed mimicry features and clothing structures while preserving the semantics of the mimicry samples. Most existing fashion generation methods focus on specified attributes, such as color and texture, but rarely create novel clothing features or generate only simple ones. Therefore, these methods have limitations for mimicry fashion generation. To cope with these challenges, this paper proposes a novel two-stage mimicry fashion generation method called the Müllerian-mimicry generative adversarial network (M2GAN). It combines GAN with the two-step Müllerian-mimicry evolutionary hypothesis. Specifically, it first makes a reasonable deformation of a mimicry sample to approximate the shape of the clothing and then fine-tunes it to generate the mimicry fashion by fusing the reference clothing. Complex constraints during generation often lead to model collapse. Thus, we developed a delayed update policy to improve the stability of the training. To verify the performance of the M2GAN, some advanced fashion generation models were introduced to conduct comparison experiments. The experimental results showed that M2GAN had obvious advantages in preserving clothing structure and mimicry semantics. The extensive experiments demonstrated the effectiveness of M2GAN for mimicry fashion generation.},
  archive      = {J_ASOC},
  author       = {Yangyun Shen and Ruide Meng and Wenkai Huang},
  doi          = {10.1016/j.asoc.2024.111375},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111375},
  shortjournal = {Appl. Soft. Comput.},
  title        = {M2GAN: Mimicry fashion generation combined with the two-step mullerian evolutionary hypothesis},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient k-NN-based rao optimization method for optimal
discrete sizing of truss structures. <em>ASOC</em>, <em>154</em>,
111373. (<a href="https://doi.org/10.1016/j.asoc.2024.111373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new method called k-nearest neighbor comparison (k-NNC) to address the computational cost issue of truss optimization with discrete variables using metaheuristic algorithms. The k-NNC judges a new design candidate is worth evaluating by comparing its k available closest designs ( k -nearest neighbors) with another design in the population. The new design will be eliminated without evaluating it if the majority of the k nearest neighboring designs are inferior to the one being compared. The k-NNC is combined with Rao algorithms along with Deb’s constraint handling rules and the rounding technique to be suitable for constrained optimization problems with discrete variables. The new optimization Rao algorithms based on k-NNC are used in five truss optimization examples, including both planar trusses and spatial trusses, to evaluate their effectiveness. The numerical results demonstrate that the proposed k-NNC-based Rao algorithms outperform the original Rao algorithms in terms of computational costs. Moreover, the overall performance of k-NNC-based Rao algorithms is similar to or better than that of some state-of-the-art metaheuristic algorithms conducted on the same examples.},
  archive      = {J_ASOC},
  author       = {Hoang-Anh Pham and Viet-Hung Dang and Tien-Chuong Vu and Ba-Duan Nguyen},
  doi          = {10.1016/j.asoc.2024.111373},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111373},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient k-NN-based rao optimization method for optimal discrete sizing of truss structures},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-source heterogeneous information fusion fault
diagnosis method based on deep neural networks under limited datasets.
<em>ASOC</em>, <em>154</em>, 111371. (<a
href="https://doi.org/10.1016/j.asoc.2024.111371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent fault diagnosis of critical components of rotating machinery is essential for enhancing production efficiency and reducing maintenance costs. However, the scarce labeled samples and the single monitoring data hinder the engineering application and generalization of diagnostic models to some extent. To this end, a novel multi-source heterogeneous information fusion (MSHIF) network is proposed in this paper to identify the health status of rotating machinery more comprehensively and robustly under limited datasets. Specifically, the data enhanced deep belief network (DEDBN) and data enhanced one-dimension convolutional neural network (DE-1DCNN) are firstly designed by repeatedly appending raw data to the hierarchy of conventional deep belief network (DBN) and one-dimension convolutional neural network (1DCNN). DEDBN and DE-1DCNN improve the diagnostic performance of the model under limited datasets while effectively mitigating the loss of potentially valuable information during layer-by-layer feature extraction and compression of DBN and CNN. Then, the MSHIF is further constructed with the designed DEDBN and DE-1DCNN as structural branches. MSHIF aims to alleviate the limitations of scarce labeled samples and single monitoring data on diagnostic performance within a unified framework by mining the rich and complementary device status information in multi-source heterogeneous monitoring data. Extensive comparative experiments and detailed discussions are constructed on both publicly available datasets and rolling mill experimental dataset to verify the feasibility and effectiveness of MSHIF. The experimental results demonstrate that the proposed MSHIF outperforms other comparative methods in terms of diagnostic accuracy, stability, and robustness against noise, achieving 99.491%, 99.143%, and 99.037% average identification accuracy on three cases, respectively.},
  archive      = {J_ASOC},
  author       = {Dongying Han and Yu Zhang and Yue Yu and Jinghui Tian and Peiming Shi},
  doi          = {10.1016/j.asoc.2024.111371},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111371},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-source heterogeneous information fusion fault diagnosis method based on deep neural networks under limited datasets},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Legendre wavelet method based solution of fractional order
prey–predator model in type-2 fuzzy environment. <em>ASOC</em>,
<em>154</em>, 111370. (<a
href="https://doi.org/10.1016/j.asoc.2024.111370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have recently examined several fractional-order prey–predator population models in a crisp or type-1 fuzzy environment. But incompleteness in the membership may also be present. In this regard, type-2 fuzzy numbers are employed in this inquiry to address the system’s imprecision since they are more realistic than type-1 fuzzy numbers in light of the uncertain membership grades. To numerically estimate the solution, the Legendre wavelet method (LWM) is used in conjunction with the r 2 r2 -cut of the r 1 r1 -plane form of the type-2 fuzzy parameters. To test the applicability of the present method, three cases are examined. The present solutions are quantitatively compared with the RK4 solutions in the crisp case. Approximations of the type-2 fuzzy prey and predator populations are also included. In addition, two- and three-dimensional plots are depicted to examine the behaviour of prey and predator populations in crisp and type-2 fuzzy cases. To assess the LWM’s efficacy, comparison graphs of the acquired solutions and the results of the RK4 approach are also provided. Numerical validation against the RK4 method in crisp cases demonstrates strong agreement, while revealing contrasting population behaviours in first two scenarios, whereas in the third case, both populations reduce with time. Further, the behaviour of the fuzzy solution has also been shown graphically using the footprint of uncertainty (FOU).},
  archive      = {J_ASOC},
  author       = {Dhabaleswar Mohapatra and S. Chakraverty},
  doi          = {10.1016/j.asoc.2024.111370},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111370},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Legendre wavelet method based solution of fractional order prey–predator model in type-2 fuzzy environment},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A many-objective evolutionary algorithm based on reference
vector guided selection and two diversity and convergence enhancement
strategies. <em>ASOC</em>, <em>154</em>, 111369. (<a
href="https://doi.org/10.1016/j.asoc.2024.111369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving the balance between convergence and diversity is a key and challenging issue in many-objective optimization. Reference vector guided selection is an exemplary method for decomposition-based many-objective evolutionary algorithms (MaOEAs). However, there are some problems with it such as insufficient number of obtained solutions and inefficient convergence evaluation metric. Aiming at solving or alleviating these problems, this paper proposes a many-objective evolutionary algorithm based on reference vector guided selection and two diversity and convergence enhancement strategies. The proposed algorithm introduces two new strategies namely adaptive sparse region detection and convergence-only selection. The former is to adaptively detect sparse regions of current elite population, while the latter is to prevent the elimination of solutions with prominent convergence performance. Together with a newly proposed elite retention strategy, these two strategies can achieve diversity and convergence enhancement on the basis on reference vector guided selection. Besides, A new selection criterion for reference vector guided selection is proposed to better measure the convergence of solutions in high dimensionality. Experimental results on widely used test problem suites up to 15 objectives indicate that the proposed algorithm is highly competitive in comparison with seven state-of-the-art MaOEAs.},
  archive      = {J_ASOC},
  author       = {Lei Yang and Yuanye Zhang and Jiale Cao and Kangshun Li and Dongya Wang},
  doi          = {10.1016/j.asoc.2024.111369},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111369},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A many-objective evolutionary algorithm based on reference vector guided selection and two diversity and convergence enhancement strategies},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Imbalanced credit card fraud detection data: A solution
based on hybrid neural network and clustering-based undersampling
technique. <em>ASOC</em>, <em>154</em>, 111368. (<a
href="https://doi.org/10.1016/j.asoc.2024.111368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the economy rapid development, the credit card business enjoys sustained growth, which leads to the frauds happen frequently. Recent years, the intelligence technology has been applied in fraud detection, but they still leave huge potential to improve reliability. Most of the existing researches designed the model only related to transaction information; however, the user’s background information and economy status may be helpful to find abnormal behavior. In view of this, we extract valuable features about individual and transaction information, which can reflect personal background and economic status. Meanwhile, in order to solve the problem of fraud detection and imbalanced class, we innovatively construct a fraud detect framework by learning user features and transaction features, which uses a h ybrid n eural n etwork with a c lustering-based u ndersampling tec h nique on i dentity and t ransaction features (HNN-CUHIT). To test the performance of the HNN-CUHIT in credit card fraud detection, we use a real dataset from a city bank during SARS-CoV2 in 2020 to conduct the experiments. In the imbalanced class problem, the experimental result indicates that the ratio of the number of the normal and fraud classes is 1:1 and then the model performance is optimal, while the F1-score is 0.0572 in HNN-CUHIT and is 0.0454 in CNN by ROS. In the fraud detection experiment, the F1-score is 0.0416 in HNN-CUHIT, getting the best performance, while it is 0.0360, 0.0284 and 0.0396 respectively in LR, RF and CNN. According to experimental results, the HNN-CUHIT performs better than other machine learning models in imbalanced class solutions and fraud detection. Our work provides a new approach to detect credit card fraud in the finance field.},
  archive      = {J_ASOC},
  author       = {Huajie Huang and Bo Liu and Xiaoyu Xue and Jiuxin Cao and Xinyi Chen},
  doi          = {10.1016/j.asoc.2024.111368},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111368},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Imbalanced credit card fraud detection data: A solution based on hybrid neural network and clustering-based undersampling technique},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving dual interactive wasserstein generative
adversarial network for cloud-based road condition monitoring in VANETs.
<em>ASOC</em>, <em>154</em>, 111367. (<a
href="https://doi.org/10.1016/j.asoc.2024.111367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a numerous fatalities have resulted from road damage. Research into road damage detection, particularly the detection and notification of hazardous road damage is essential for enhancing the traffic security. Existing road damage identification schemes often process data in the cloud, but they are unable to warn users on time on account of lengthy latency. Although newer edge computing strategies reduce this issue owing to the limited communication range of edges, the users can only get notifications about potentially harmful road damage within a small radius. Besides, untrusted nodes may misuse the sensitive information of users. Therefore, in this manuscript, a Privacy-preserving Dual Interactive Wasserstein Generative Adversarial Network is proposed for Cloud-Based Road Condition Monitoring in VANETs (PP-DIWGAN-C-RCM-VANET). Here, local models and global models are the two types of cloud detection models are considered. In which, the local models acquire knowledge from cloud-based data, whereas global models support local models&#39; learning by combining local models. The intention of the proposed approach is “to detect the smart hazardous road condition”. The local model recognizes dangerous roads, and also categorized into 3 levels: low, middle, and high, these are based on information gathered about the state of the roads. From the road crack dataset, it is categorized into crack, normal and pothole. DIWGAN secures secrecy from unreliable clouds, but it is still significant danger of privacy leaking from unreliable edges. To ensure privacy, users must erase all data before transmitting it to the edges, here the user indicates the owner of the vehicle. Therefore, a new privacy preserving method called Fractional Discrete Meixner Moments Encryption (FDMME) is taken into account to fulfill this requirement. For real time dataset, the proposed method achieves better accuracy 27.5%, 10.32% and 16.65%, better f-score 30.93%, 11.14% and 15.3% compared with existing methods.},
  archive      = {J_ASOC},
  author       = {K. Lakshmi Narayanan and R. Naresh},
  doi          = {10.1016/j.asoc.2024.111367},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111367},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Privacy-preserving dual interactive wasserstein generative adversarial network for cloud-based road condition monitoring in VANETs},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient crowd density estimation with edge intelligence
via structural reparameterization and knowledge transfer. <em>ASOC</em>,
<em>154</em>, 111366. (<a
href="https://doi.org/10.1016/j.asoc.2024.111366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd stampedes and related incidents pose significant dangers to public safety and have resulted in numerous fatalities over the past few decades. Estimating crowd density in real-time can help avoid stampedes by providing early warning systems to prevent overcrowding and manage the flow of people. With the increasing prevalence of Internet of Things (IoT), the application of edge computing for field pedestrian density estimation can help to enhance security and efficiency of system. To deal with the issue of crowd scale variation, most previous works rely on heavy backbone networks or complex module, which require high runtime consumption and severely limits the deployment scope of their work. To overcome this issue, we propose multi-branch model Repmobilenet. For multi-scale spatial feature extraction. Repmobilenet equips with lightweight multi-branch depthwise separable convolutional block (DSBlock), which can effectively extract multi-scale feature of dense crowd to cope with scale variation. In the inference phase, the multi-branch structure can be transformed into a single-branch feed-forward structure through structural reparameterization. By this way, Repmobilenet can use multi-branch over-parameterized topology to capture more dense spatial features during training stage and decrease inference latency during inference stage. We also added dilated convolutions in the backend to expand the receptive field of the model. Comparison to state-of-the-art methods, proposed Repmobilenet is able to achieve comparable counting performance while maintaining small model size and low inference latency in ShanghaiTech benchmark. At last, we introduce a multi-layer knowledge distillation method to further increase the model’s fitting capability. By imitating the feature of multiple intermediate layers and final output soft ground truth of the teacher model, the student model can learn compact and efficient knowledge without increasing model size and inference latency. The code can be found in https://github.com/lcxxxiii/Repmobilenet .},
  archive      = {J_ASOC},
  author       = {Chenxi Lin and Xiaojian Hu},
  doi          = {10.1016/j.asoc.2024.111366},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111366},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient crowd density estimation with edge intelligence via structural reparameterization and knowledge transfer},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Stock price series forecasting using multi-scale modeling
with boruta feature selection and adaptive denoising. <em>ASOC</em>,
<em>154</em>, 111365. (<a
href="https://doi.org/10.1016/j.asoc.2024.111365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, predicting stock prices has garnered attention from both regulators and academic circles. However, the intricate nature of financial time-series data, with its nonlinearities, discontinuities, and sensitivity to noise, complicates the understanding and forecasting of financial movements. In our approach, we initially deploy an adaptive empirical modal decomposition on the primary data to enhance model precision. Subsequently, we sift the technical indicator data through the Boruta method, enhancing selected functionalities via an adaptive noise reduction technique. We then employ support vector regression (SVR) integrated with brain storm optimization algorithm (BSO) for effective data handling and forecasting target variables. Our results suggest that the composite model outlined in this paper outperforms the other eight comparison models in terms of reducing errors and improving regression scores. Additionally, when juxtaposed against these four models, the outcomes reinforce the efficiency of our proposed multiscale strategy and denoising technique in refining prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Jing Li and Yukun Liu and Hongfang Gong and Xiaofei Huang},
  doi          = {10.1016/j.asoc.2024.111365},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111365},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock price series forecasting using multi-scale modeling with boruta feature selection and adaptive denoising},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A wind turbine damage detection algorithm designed based on
YOLOv8. <em>ASOC</em>, <em>154</em>, 111364. (<a
href="https://doi.org/10.1016/j.asoc.2024.111364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to operational conditions, wind turbines may suffer from various types of damage, including cracks and wear. Traditional methods of wind turbine damage detection face challenges such as low detection accuracy and high computational resource consumption. This study proposes a wind turbine damage detection algorithm designed based on the YOLOv8 to address these issues. Firstly, the C2f-FocalNextBlock module is added to the algorithm’s backbone network, enhancing the feature extraction capability of the main network. Then, the ResNet-EMA module is incorporated into the algorithm’s neck network. This module effectively captures cross-dimensional interactions and establishes dependencies between dimensions, thereby enhancing the algorithm’s feature extraction capability. Finally, a slim-neck structure is introduced into the neck network of the algorithm to better integrate multi-scale features of targets and background information, thus improving the algorithm’s performance. Experimental results demonstrate that the wind turbine damage detection algorithm designed based on YOLOv8 achieves an mean average precision mean (mAP) of 79.9%, accurately detecting wind turbine damage.},
  archive      = {J_ASOC},
  author       = {Lizhao Liu and Pinrui Li and Dahan Wang and Shunzhi Zhu},
  doi          = {10.1016/j.asoc.2024.111364},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111364},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A wind turbine damage detection algorithm designed based on YOLOv8},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Graph-based few-shot incremental learning algorithm for
unknown class detection. <em>ASOC</em>, <em>154</em>, 111363. (<a
href="https://doi.org/10.1016/j.asoc.2024.111363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning, a promising technique for acquiring new concepts from limited data, assumes that testing samples belong to “unknown classes” and are regarded as new knowledge. However, real-world scenarios introduce uncertainty about the class membership of testing samples. To address this uncertainty, we propose a novel challenge of few-shot incremental unknown class detection, aligning more closely with practical situations. Open set recognition can classify known class samples and reject unknown class samples to mitigate the uncertainty, but it struggles to address the critical limitation of having few available samples. To tackle both uncertainty and limitation, we propose a graph-based few-shot incremental learning algorithm for unknown class detection, which includes four components. First, a feature extractor learns from the base dataset during training and is subsequently fixed for embedding node features from the novel dataset during inference. Then, embedded node features, along with their corresponding prototypes, contribute to graph generation and edge construction. Third, a mixed-rejection strategy is proposed to determine the class membership of testing samples. Finally, a novel class is treated as a new known class, engaging the embedded node features in graph update and edge reconstruction. Evaluation on benchmark datasets with varying structures, including USTC-TFC2016 and miniImageNet datasets, demonstrates that our proposed algorithm outperforms classical open set recognition algorithm in few-shot incremental learning for unknown class detection, which offers promising performance and potential for practical applications in real-world scenarios.},
  archive      = {J_ASOC},
  author       = {Zijian Liu and Yaning Wang and Yang Luo and Chunbo Luo},
  doi          = {10.1016/j.asoc.2024.111363},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111363},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Graph-based few-shot incremental learning algorithm for unknown class detection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grid search with a weighted error function: Hyper-parameter
optimization for financial time series forecasting. <em>ASOC</em>,
<em>154</em>, 111362. (<a
href="https://doi.org/10.1016/j.asoc.2024.111362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial time series forecasting is a difficult task due to the complexity and volatility of financial markets. Machine learning models have been applied to tackle this task, but finding their optimal hyper-parameters with less time and ensuring the prediction accuracy of models are still significant challenges. Existing methods such as GridSearch with cross-validation (GridsearchCV) for optimizing the hyper-parameters are time-consuming for complex models or large search spaces, and they do not ensure that the model has excellent predictive accuracy. To address these challenges, we propose a novel method called GridsearchWEF that uses grid search with a weighted error function. This method aims to reduce the time cost of hyper-parameter optimization for machine learning models and guarantee their prediction performance. We conduct an empirical analysis of crude oil return forecasting using four machine learning models: RF, GBDT, SVR, and LASSO. We compare the performance of these models using GridsearchCV, random search with cross-validation (RandomizedSearchCV), Bayes optimization with cross-validation (BayesSearchCV), and GridsearchWEF. The results show that GridsearchWEF outperforms the other methods in terms of hyper-parameter optimization, modeling efficiency, prediction accuracy, and economic values. In particular, the time of all models using GridSearchWEF is less than 30 s, which is much less than other algorithms. GridsearchWEF is a more efficient and superior method for hyper-parameter optimization in financial time series forecasting.},
  archive      = {J_ASOC},
  author       = {Yuan Zhao and Weiguo Zhang and Xiufeng Liu},
  doi          = {10.1016/j.asoc.2024.111362},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111362},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grid search with a weighted error function: Hyper-parameter optimization for financial time series forecasting},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized sparse radial basis function networks for
multi-classification problems. <em>ASOC</em>, <em>154</em>, 111361. (<a
href="https://doi.org/10.1016/j.asoc.2024.111361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, the radial basis function network (RBFN) has attracted extensive attention due to its simple network structure and powerful learning ability. Meanwhile, regularization methods have been intensively applied in RBFNs to enhance the performance of networks. A common regularization method is the ℓ 2 ℓ2 regularization, which improves the stability and generalization ability but leads to dense networks. Another common regularization method is to employ the ℓ 1 ℓ1 regularization that can successfully improve the sparsity of RBFN. The better strategy is to use the elastic-net regularization that combines both ℓ 2 ℓ2 and ℓ 1 ℓ1 regularization to improve stability and sparsity simultaneously. However, in multi-classification tasks, even the elastic-net regularization can only prune the redundant weights of nodes and cannot ensure the sparsity at the node level. In this paper, we propose a generalized sparse RBFN (GS-RBFN) based on the extended elastic-net regularization to handle multi-classification problems. By using the extended elastic-net regularization that integrates the Frobenius norm and L 2 , 1 L2,1 norm, we accomplish the stability and sparsity of RBFN for multi-classification problems, of which the binary classification problem is only a special case. In order to improve the training efficiency under large-scale tasks, we further propose the parallel GS-RBFN (PGS-RBFN) with the matrix inversion lemma to accelerate the intensive computation. The alternating direction method of multipliers (ADMM) and its consensus variant are applied to train our proposed models, and we demonstrate their convergence in solving corresponding optimization problems. Experimental results on multi-classification datasets illustrate the effectiveness and advantages of our algorithms in accuracy, sparsity and convergence.},
  archive      = {J_ASOC},
  author       = {Yunwei Dai and Qingbiao Wu and Yuao Zhang},
  doi          = {10.1016/j.asoc.2024.111361},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111361},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generalized sparse radial basis function networks for multi-classification problems},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A large-scale group decision-making approach based on
hesitancy degrees and non-cooperative behaviors with picture fuzzy
information. <em>ASOC</em>, <em>154</em>, 111360. (<a
href="https://doi.org/10.1016/j.asoc.2024.111360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale group decision-making (LSGDM) problems involve two processes: a clustering process is first implemented to break down a larger group into several smaller subgroups for simplification purposes, after which a consensus reaching process (CRP) is utilized to eliminate information conflicts among large-scale decision-makers (DMs). Opinion conflicts are inevitable in large DM groups due to self-interest biases and information blind spots. To address these limitations, this study proposes a twofold feedback mechanism to address both non-cooperative behaviors and information hesitation. Firstly, based on detailed review of existing similarity measures, we defined a novel picture fuzzy Chi-square similarity measure (NPFCS) to enhance accuracy. To distinguish subgroups, the second procedure was developed to establish a weight assignment model following the implementation of the developed similarity-based fuzzy clustering algorithm. In the third step, decision opinions are examined from two distinct perspectives: the hesitancy degrees and non-cooperative behaviors, to ensure favorable decision outcomes. Alongside the definition of a hierarchical mechanism for detecting DMs with information blind spots, the identified DMs were mandated to resubmit individual matrices. This measure aims to prevent the aggregation of negative impacts on information credibility caused by the major principle. Furthermore, non-cooperative behaviors were identified at the element level of multi-criteria matrices with corresponding adjustments performed automatically. Next, a real-world site selection problem involving thirty DMs was solved using the proposed method to illustrate its application. Moreover, the results of sensitivity analysis demonstrate that our proposal is robust to changes in model parameters, and several comparisons were conducted from two perspectives, thus confirming the superiority of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Juan Juan Peng and Xin Ge Chen},
  doi          = {10.1016/j.asoc.2024.111360},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111360},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A large-scale group decision-making approach based on hesitancy degrees and non-cooperative behaviors with picture fuzzy information},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary optimization of policy responses to COVID-19
pandemic via surrogate models. <em>ASOC</em>, <em>154</em>, 111359. (<a
href="https://doi.org/10.1016/j.asoc.2024.111359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spread of COVID-19 has caused a great series of negative effects on countries around the world. To curb the pandemic, many governments apply a variety of strict measures including the closure of public places, transportation systems, etc. This paper proposes an algorithm that searches through a set of government policies and selects the policies that minimize the growth rate of the pandemic with minimum cost to society. The proposed algorithm first builds a surrogate model of the pandemic and then searches through the set of government policies via an evolutionary algorithm. The surrogate model in this paper is an ensemble of several learning algorithms that uses a meta-learning algorithm that estimates where in the feature space each of the learning algorithms performs better and then gives a higher weight to the learning algorithm in the voting process. Because using the surrogate models as the fitness function induces uncertainty, in this paper we use an uncertainty reduction mechanism to perform a search through the search space better. Data from 124 countries are used in this paper which contains information about the policies each government has taken since the beginning of the pandemic and 75 environmental factors like climate, religion, politics, economy, etc. Several experimental studies are performed on the output of the modeling system and the optimization algorithms and comparisons are performed with state-of-the-art methods.},
  archive      = {J_ASOC},
  author       = {Mohammad-H. Tayarani-N.},
  doi          = {10.1016/j.asoc.2024.111359},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111359},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary optimization of policy responses to COVID-19 pandemic via surrogate models},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A decision-making system based on case-based reasoning for
predicting stroke rehabilitation demands in heterogeneous information
environment. <em>ASOC</em>, <em>154</em>, 111358. (<a
href="https://doi.org/10.1016/j.asoc.2024.111358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the increasing number of stroke survivors has created a greater demand for rehabilitation services. How to predict rehabilitation demands is important for policy makers in various countries. The demand prediction is essentially a multi-attribute decision-making problem. The diverse forms of information or data are often neglected or simplified in previous studies of decision-making problems. In this paper, heterogeneous multi-attributes including crisp numbers, interval numbers, and linguistic variables are defined to represent decision-relevant information concerning the patient&#39;s condition in cases when applied to real-life scenarios. Moreover, a deviation minimization optimization model combining CRITIC method and H2TLWMSM operator is proposed to assign the attribute weights with dual information. The WHMACBR system can be used to predict stroke rehabilitation demands for making the diagnosis and treatment decisions. The proposed decision-making system based on the CBR approach simulates the operations of human memory and reasoning, thus assisting physicians in addressing the ever-increasing demand for rehabilitation services of stroke survivors. Finally, the effectiveness and applicability of the case retrieval process of the presented CBR model are demonstrated through a stroke rehabilitation case study (top 3 cases ranked by similarity to the target case are C 6 C6 , C 2 C2 , and C 3 C3 ) and comparison analysis with relevant existing CBR models. This indicates that the proposed decision-making system, by considering heterogeneous multi-attributes and their weights, is capable of providing more comprehensive and rational decision-related information in the field of rehabilitation medicine.},
  archive      = {J_ASOC},
  author       = {Duojin Wang and Jiawan Liu and Qinglian Lin and Hongliu Yu},
  doi          = {10.1016/j.asoc.2024.111358},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111358},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A decision-making system based on case-based reasoning for predicting stroke rehabilitation demands in heterogeneous information environment},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk prioritization by z-VIKOR method under incomplete
reliable information and its application in CCUS project site selection.
<em>ASOC</em>, <em>154</em>, 111357. (<a
href="https://doi.org/10.1016/j.asoc.2024.111357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carbon Capture, Utilization, and Storage (CCUS) is a promising technology for mitigating climate change and reducing carbon emissions. However, selecting suitable sites for CCUS projects can be challenging due to regional differences in economic strength, ecology, and government policies. This results in a multi-criteria decision problem that is complicated by the fact that the information used for evaluation is often unreliable and uncertain, leading to risks in the decision-making process. Furthermore, decision-makers may not always act rationally, complicating the decision-making process. To address these challenges, we propose an extended Z-VIKOR method that considers the fuzziness and incompleteness of the available information in multiple criteria decision-making problems. Our method also incorporates a risk preference-based prioritized distance measure that reflects the decision-makers&#39; attitudes toward risk. By fully considering Z-information and introducing three priority coefficients to reduce computational complexity, our approach can identify suitable sites for CCUS projects under different risk preferences. Finally, we study alternative cities in Guangdong Province for the CCUS project using the proposed methodology and conclude that Zhanjiang and Shenzhen have certain advantages in developing the project. In summary, our proposed methodology addresses the challenges associated with CCUS site selection by incorporating incomplete reliable information and reflecting decision makers&#39; attitudes towards risk. This approach can help guide decision-makers in selecting suitable sites for CCUS projects and ultimately contribute to the global effort to combat climate change.},
  archive      = {J_ASOC},
  author       = {Ming-run Chen and Kai-wen Shen},
  doi          = {10.1016/j.asoc.2024.111357},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111357},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk prioritization by Z-VIKOR method under incomplete reliable information and its application in CCUS project site selection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model management for low-computational-budget
simulation-based optimization of antenna structures using
nature-inspired algorithms. <em>ASOC</em>, <em>154</em>, 111356. (<a
href="https://doi.org/10.1016/j.asoc.2024.111356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antenna design has been increasingly reliant on computational tools, specifically, full-wave electromagnetic (EM) analysis. EM simulations are capable of rendering a reliable characterization of complex antenna architectures and quantify the effects (mutual coupling, dielectric losses, feed radiation, etc.) that cannot be accounted for using other methods. At the same time, it is CPU-intensive. Repetitive simulations incurred by numerical procedures, especially optimization, constitute a serious bottleneck of EM-driven design. Perhaps the most extreme example thereof is global tuning of antenna parameters, which is typically performed using soft computing methods, in particular, nature-inspired routines. Although these methods are generally recognized for the ability to handle multimodal problems, their computationally efficiency is poor; direct application to EM models is generally prohibitive. A viable alternative is the incorporation of surrogate-assisted frameworks, along the lines of efficient global optimization (EGO) paradigm, in which the surrogate model is refined in an iterative manner using aggregated EM data and serves as a prediction tool which facilitates finding the optimum design. Unfortunately, the scope of applicability of surrogate-assisted methods is encumbered by the curse of dimensionality, and also nonlinearity of antenna responses. The primary objective of this study is investigation of the possibilities of accelerating nature-inspired optimization of antenna structures using multi-fidelity EM simulation models. The primary methodology developed to achieve acceleration is a model management scheme in which the level of EM simulation fidelity is set using two criteria: the convergence status of the optimization algorithm, and relative quality of the individual designs within the solution pool. The search process is initiated using the lowest-fidelity (therefore, the fastest) EM model. The fidelity is step-by-step increased towards the conclusion of the process. At the same time, lower-quality designs are evaluated at lower resolution level as compared to the better ones. Our technique has been extensively validated using several microstrip antennas, and particle swarm optimization (PSO) algorithm as the search engine. The obtained results demonstrate that making the EM model fidelity dependent on just the convergence status of the algorithm allows for relative savings from forty to seventy percent, depending on the algorithm setup. At the same time, managing model fidelity as a function of both convergence status and relative design quality (within the population processed by the algorithm) allows for up to 85% savings, as compared to high-fidelity-based algorithms. Furthermore, the achieved acceleration is not detrimental to the optimization process reliability. Apart from the computational efficiency, the attractive feature of the proposed approach is implementation simplicity and versatility: the presented management scheme can be readily incorporated into most nature-inspired routines.},
  archive      = {J_ASOC},
  author       = {Anna Pietrenko-Dabrowska and Slawomir Koziel},
  doi          = {10.1016/j.asoc.2024.111356},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111356},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Model management for low-computational-budget simulation-based optimization of antenna structures using nature-inspired algorithms},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Self-adaptive two-stage density clustering method with fuzzy
connectivity. <em>ASOC</em>, <em>154</em>, 111355. (<a
href="https://doi.org/10.1016/j.asoc.2024.111355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density Peak Clustering (DPC) was proposed in the journal Science in 2014 and has been widely applied in many fields due to its simplicity and effectiveness. However, there are few studies on the effectiveness of DPC algorithm and its variants on non-clean data sets. Inspired by the idea that DPC algorithm combines density and distance when determining clustering center, this paper creatively designs a two-stage density clustering method with fuzzy connectivity (TS-DCM). It could be used to distinguish different cluster partitions and further identify noise points and sample points. In addition, this paper also introduces a new clustering index: fuzzy connectivity, which could not only adjust the selection of DPC cutoff distance, but also provide a reference for adaptive adjustment of TS-DCM parameter selection, greatly improving the operating efficiency of the clustering algorithm. At the same time, a self-adaptive two-stage density clustering method (STS-DCM) is proposed to adjust the selection of parameters according to the feedback of clustering results. Finally, compared with other traditional and popular clustering algorithms, it is verified that the proposed algorithm has significant advantages in speed and accuracy. Moreover, for non-clean data sets, the algorithm is robust and effective.},
  archive      = {J_ASOC},
  author       = {Kaikai Qiao and Jiawei Chen and Shukai Duan},
  doi          = {10.1016/j.asoc.2024.111355},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111355},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-adaptive two-stage density clustering method with fuzzy connectivity},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated machine-learning-assisted stochastic-fuzzy
multi-criteria decision making tool: Addressing record-to-record
variability in seismic design. <em>ASOC</em>, <em>154</em>, 111354. (<a
href="https://doi.org/10.1016/j.asoc.2024.111354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While uncertainty quantification (UQ) has served a prominent role in ensuring the safety of dynamical engineering systems, the lack of an integrated approach to handle the aleatory nature of ground motion records, a.k.a., record-to-record (RTR) variability, remains a bottleneck in seismic design. This paper presents a novel approach with two key features. First, on the multi-criteria decision-making (MCDM) front, a general-purpose collective decision support system is introduced by integrating Monte Carlo simulation, automated machine learning (AutoML), and a hybrid fuzzy-outranking MCDM technique. This allows for robust uncertainty capture at a group level without compromising computational efficiency—a departure from the traditional trade-off between MCDM competency in uncertainty handling and computational burden. Second, from an Earthquake Engineering perspective, the established AutoML-based community-level MCDM approach is combined with an efficient metamodel-aided reliability-based design optimization technique and a novel probabilistic-fuzzy seismic-design safety index. Conflicting criteria are classified into structural performance, design safety, and construction cost metrics. The results indicate that the developed intelligent seismic design paradigm properly captures uncertainties rooted in the RTR variability, input data, criterion weighting, and decision-makers’ preferences from a community-level standpoint. It also offers acceptable error metrics during the ranking procedure in a user-friendly environment, at a significantly reduced computational expenditure. Moreover, it can be concluded that there is no imperative need for developing complex decision tools for robust-to-uncertainty seismic design. The implementation of the proposed MCDM-based UQ framework has reduced the design cost of an existing dam by 17%.},
  archive      = {J_ASOC},
  author       = {Ali Amini and Azam Abdollahi and Mohammad Amin Hariri-Ardebili},
  doi          = {10.1016/j.asoc.2024.111354},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111354},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An automated machine-learning-assisted stochastic-fuzzy multi-criteria decision making tool: Addressing record-to-record variability in seismic design},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Peak and ultimate stress-strain model of confined
ultra-high-performance concrete (UHPC) using hybrid machine learning
model with conditional tabular generative adversarial network.
<em>ASOC</em>, <em>154</em>, 111353. (<a
href="https://doi.org/10.1016/j.asoc.2024.111353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high-performance concrete (UHPC) has gained prominence owing to its exceptional physical and mechanical properties, along with enhanced sustainability, making it ideal for large-scale structural applications. While numerous analytical studies have focused on predicting the stress-strain response of unconfined UHPC, there remains a lack of a reliable model for predicting the stress-strain response of confined UHPC, which poses challenges to efficient design and broader adoption, particularly in seismically active regions. To bridge this gap, the present study introduces a framework that implements machine learning (ML) models augmented by a state-of-the-art conditional tabular generative adversarial network (CTGAN) and Optuna, a next-generation optimization framework, to accurately predict the peak and ultimate axial stress-strain responses of UHPC confined with either normal-strength steel or high-strength steel. The Optuna-optimized CTGAN is employed to address the issue of limited data by generating synthetic datasets of hypothetical confined UHPC specimens. A comprehensive database of confined UHPC stress-strain responses was compiled from existing literature and used to condition the CTGAN. The augmented database is then used to develop a hybrid ML model that integrates extreme gradient boosting, gradient boosting machine, support vector regression, and K-nearest neighbors for predicting peak and ultimate stress-strain responses of confined UHPC. The predictive accuracy of the proposed hybrid ML model is evaluated and compared with a diverse set of ML models of varying complexity, and the results demonstrate its superior performance in predicting the peak and ultimate stress-strain responses of confined UHPC. Furthermore, a graphical user interface of the proposed model is developed to facilitate its practical implementation and provide a rapid, intelligent, and accurate prediction of the stress-strain response of confined UHPC at both peak and ultimate states.},
  archive      = {J_ASOC},
  author       = {Tadesse G. Wakjira and M. Shahria Alam},
  doi          = {10.1016/j.asoc.2024.111353},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111353},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Peak and ultimate stress-strain model of confined ultra-high-performance concrete (UHPC) using hybrid machine learning model with conditional tabular generative adversarial network},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FPGA-based hardware implementation of chaotic
opposition-based arithmetic optimization algorithm. <em>ASOC</em>,
<em>154</em>, 111352. (<a
href="https://doi.org/10.1016/j.asoc.2024.111352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hardware implementation of optimization algorithms has gained significant attention due to its potential for augmenting performance and efficiency. This paper presents the hardware implementation of a Chaotic Opposition-Based Arithmetic Optimization Algorithm (COAOA). COAOA is inspired by integrating chaotic dynamics and opposition-based learning (OBL) principles in the original AOA algorithm. Inspired by chaotic maps , COAOA amplifies exploration and exploitation capabilities, while OBL enhances the search process by considering both positive and negative solution counterparts. Our proposed hardware implementation of COAOA harnesses Field-Programmable Gate Arrays (FPGAs) to accelerate optimization processes. Leveraging FPGA’s parallel processing capabilities, the COAOA algorithm is efficiently parallelized and mapped onto the hardware architecture, streamlining arithmetic operations and governing algorithmic execution flow. To assess its efficacy, a comparison was made between the performance of the suggested structure and five alternative algorithms: opposition-based learning AOA (OAOA), Chaotic AOA, the original AOA, PSO and GA . Statistical analyses and tests were performed using benchmark functions like Rosenbrock, Rastrigin, Six-Hump Camel-Back, and Zirilli functions. The empirical findings indicate the substantial acceleration achieved through the hardware implementation of COAOA compared to the original AOA algorithm and its variants. This compelling outcome positions COAOA as a viable solution for real-time optimization challenges, signifying its potential to significantly expedite optimization tasks in practical applications.},
  archive      = {J_ASOC},
  author       = {Mohamed Aymen Zermani and Ghaith Manita and Amit Chhabra and Elyes Feki and Abdelkader Mami},
  doi          = {10.1016/j.asoc.2024.111352},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111352},
  shortjournal = {Appl. Soft. Comput.},
  title        = {FPGA-based hardware implementation of chaotic opposition-based arithmetic optimization algorithm},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contextual analysis of solutions in a tourist trip design
problem: A fuzzy logic-based approach. <em>ASOC</em>, <em>154</em>,
111351. (<a href="https://doi.org/10.1016/j.asoc.2024.111351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tourist trip design is a fast-growing area of research. Tourist interest, budget, travel style, safety, and the existence of travelers with special needs (for example, wheelchair accessibility) are some of the elements to consider for obtaining personalized routes. Including these elements in a single mathematical model can considerably complicate the solution process. Also, route decision-making is affected by the context (health, social, political, economic, etc.) in which decisions are made. In this paper, the first aim is to propose a three-step methodology to obtain contextualized solutions to a tourist trip design problem (TTDP) with time-dependent recommendation factors. The methodology consists of (1) providing a basic TTDP model that avoids the complexity of including contextual information, (2) obtaining a set of solutions to the problem using a Modeling to Generate Alternatives (MGA) approach, and (3) using a recently developed a posteriori method to include the contextual information through fuzzy propositions. The second aim of the paper is to evaluate three algorithmic strategies for the MGA step. Lastly, considering a context for people with mobility impairments, an example is solved using real data. The results show the usefulness of the proposed methodology in solving the TTDP with contextual information.},
  archive      = {J_ASOC},
  author       = {Boris Pérez-Cañedo and Pavel Novoa-Hernández and Cynthia Porras and David A. Pelta and José Luis Verdegay},
  doi          = {10.1016/j.asoc.2024.111351},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111351},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contextual analysis of solutions in a tourist trip design problem: A fuzzy logic-based approach},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human activity recognition using a multi-branched
CNN-BiLSTM-BiGRU model. <em>ASOC</em>, <em>154</em>, 111344. (<a
href="https://doi.org/10.1016/j.asoc.2024.111344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human behaviour analysis, human–computer interaction, and pervasive computing are three areas where human activity recognition has recently attracted a lot of attention. Recent advances in deep learning have made it possible to accurately predict a variety of human actions by handling time-series data from wearable sensors and mobile devices quickly. Despite their remarkable achievements in recognising activities, DL-based algorithms continue to face hurdles in successfully processing sequential data. Ongoing difficulties include complex feature extraction, dealing with data imbalances, and other related concerns. Furthermore, manual feature engineering techniques are significantly used in the majority of HAR approaches. In order to recognise human behaviours using wearable sensors, this research introduces a powerful classification technique . The strategy combines a bidirectional long-short-term memory, a convolutional neural network , and a bidirectional gated recurrent unit. This system efficiently and successfully derives critical insights from unprocessed sensor data. The model gains the capacity to recognise both short-term patterns and long-term associations in sequential data by merging CNN, BiGRU, and BiLSTM components. The model can capture a wide range of temporal local connections since the feature extraction process is sped up by the addition of multiple filter sizes. The algorithm’s performance is assessed using common datasets, particularly the WISDM dataset, which shows a remarkable accuracy of 99.33% and an F1-score of 73.20% for the hybrid model CCBB. The proposed model is proven to be superior to competing approaches by experimental findings.},
  archive      = {J_ASOC},
  author       = {Pooja Lalwani and Ganeshan Ramasamy},
  doi          = {10.1016/j.asoc.2024.111344},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111344},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human activity recognition using a multi-branched CNN-BiLSTM-BiGRU model},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the max–min influence spread problem: A multi-objective
optimization approach. <em>ASOC</em>, <em>154</em>, 111343. (<a
href="https://doi.org/10.1016/j.asoc.2024.111343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central problem in network dynamics is understanding how influence spreads through a social network. This problem can be studied from an optimization approach. The aim is to find an initial seed of actors, with certain size restrictions, capable of maximizing or minimizing the activation of other actors in the network through a given influence spread model. The maximization and minimization versions of this problem have been extensively studied. In recent years, the min–max multi-objective version was defined, which involves finding the smallest seed capable of maximizing the influence spread in the network. Searching for exact solutions in these optimization problems is not feasible, even for relatively small networks. Hence, various approximation techniques have been proposed in recent years, with bio-inspired algorithms based on metaheuristics standing out among them. However, the max–min multi-objective version of the problem remains open. This article formally defines the max–min influence spread problem, aiming to find the maximum seed with the minimum spread capacity. We propose a strategy that uses solutions from the min–max version of the problem to reduce the search space , allowing us to avoid trivial solutions . The potential applications of this max–min version are diverse, e.g., finding clusters less susceptible to diseases in a contagion network or the most inefficient coalitions in a voting system. Using swarm intelligence metaheuristics methods as in the min–max version, the results obtained on real social networks show that this approach exhibits rapid convergence, reaching a seed encompassing 51.3% of the actors who could not influence others within the network. Similarly, for a more complex network, the approach is able to generate a seed where 71.8% of the actors showed no influence over others.},
  archive      = {J_ASOC},
  author       = {Fabián Riquelme and Francisco Muñoz and Rodrigo Olivares},
  doi          = {10.1016/j.asoc.2024.111343},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111343},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On the max–min influence spread problem: A multi-objective optimization approach},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive multi-objective multi-task scheduling method by
hierarchical deep reinforcement learning. <em>ASOC</em>, <em>154</em>,
111342. (<a href="https://doi.org/10.1016/j.asoc.2024.111342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Actual manufacturing process scheduling in enterprise alliances are multi-task scheduling problems involving dynamic factors, and the competition and conflict for manufacturing resources also exist between multi-tasks. How to perform adaptive multi-objective scheduling of multi-tasks based on the real-time state of the manufacturing environment becomes critical. Therefore, this paper constructs an adaptive multi-task multi-objective scheduling considering resource competition and conflict among tasks (AMMS-RCCT) model based on the enterprise alliance value net, and adopts a hybrid strategy of “parallel+serial” to resolve conflicts while reducing the waiting time of tasks. With the objective of optimizing the total manufacturing time and total manufacturing cost, an adaptive multi-objective deep Q network (AMDQN) is proposed to solve the AMMS-RCCT model. AMDQN is based on a two-hierarchy deep reinforcement learning architecture containing a front controller deep Q network (C-DQN) and a back actuator deep Q network (A-DQN), which performs hierarchical decision-making on optimization objectives and scheduling rules to achieve compromise between multiple objectives while reducing the complexity for optimal selection scheduling rules. For the two optimization objectives of time and cost, two reward algorithms are proposed by introducing two metrics, the estimated tardiness rate and the estimated overspend rate, which guide the A-DQN to learn and adjust the scheduling rules according to the state changes. Besides, nine composite scheduling rules are designed to adapt to the dynamic manufacturing environment from multiple dimensions such as task urgency and completion rate as well as manufacturing resource utilization and cost. Finally, AMDQN is experimentally compared with the proposed nine composite scheduling rules, scheduling rules in existing research, and other scheduling methods based on reinforcement learning in simulated manufacturing environments with different numbers of tasks, subtasks, and manufacturing cells . The experimental results verify the effectiveness and superiority of AMDQN for multi-objective adaptive scheduling in multi-task scheduling problems.},
  archive      = {J_ASOC},
  author       = {Jianxiong Zhang and Bing Guo and Xuefeng Ding and Dasha Hu and Jun Tang and Ke Du and Chao Tang and Yuming Jiang},
  doi          = {10.1016/j.asoc.2024.111342},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111342},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive multi-objective multi-task scheduling method by hierarchical deep reinforcement learning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clustering-based genetic offspring generation for solving
multi-objective optimization problems with intricate pareto sets.
<em>ASOC</em>, <em>154</em>, 111341. (<a
href="https://doi.org/10.1016/j.asoc.2024.111341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to traditional benchmarks, multiobjective optimization problems (MOPs) encountered in practical applications often exhibit intricate variable interdependencies , giving rise to complex Pareto sets (PSs) characterized by rotated or nonlinear shapes. Simulated binary crossover (SBX), a widely used genetic operator for solving MOPs, experiences significant performance degradation when applied to MOPs with intricate PSs. The rotation-based SBX (RSBX) incorporates the rotational property into SBX to handle MOPs with linear but rotated PSs. Nevertheless, RSBX may encounter difficulties in solving MOPs with nonlinear PSs. In order to tackle this challenge, we propose a clustering-based mating restriction strategy to address MOPs with intricate PSs, and the proposed approach has been integrated with RSBX to formulate an algorithm named CRSBX. The clustering-based mating restriction strategy involves partitioning the parent population into approximately linearly distributed clusters, then RSBX is applied to each cluster for effective offspring generation. We empirically investigate the impact of the clustering algorithm and its associated parameters on CRSBX. Ablation studies are also conducted to examine the efficacy of the clustering-based mating restriction strategy. Additionally, we compare CRSBX with other representative algorithms on benchmark problems and real-world applications with intricate PSs. Comparison results highlight the promising performance of CRSBX in effectively addressing MOPs with intricate PSs.},
  archive      = {J_ASOC},
  author       = {Lianghao Li and Jianqing Lin and Cheng He and Linqiang Pan},
  doi          = {10.1016/j.asoc.2024.111341},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111341},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Clustering-based genetic offspring generation for solving multi-objective optimization problems with intricate pareto sets},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Pruned lightweight neural networks for arrhythmia
classification with clinical 12-lead ECGs. <em>ASOC</em>, <em>154</em>,
111340. (<a href="https://doi.org/10.1016/j.asoc.2024.111340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time electrocardiogram (ECG) monitoring through portable or wearable devices is critical for detecting lethal arrhythmias. Despite the accuracy of 12-lead ECGs in clinical image analysis, their integration into portable devices poses challenges. This paper introduces a novel method for nonmalignant arrhythmia classification, optimized for wearable and portable devices. We utilize ECG records from Shanghai First People’s Hospital, proposing a lightweight neural network strategy involving benchmark network selection, model pruning, and learning rate decay-based finetuning. The proposed Random Horizontal Flip (RHF)-based classification method demonstrated superior performance, achieving a 94.8 % accuracy on a 12-lead clinical ECG test dataset . Utilizing the modified pruning method, the classification accuracy for five-class ECGs improved by 7.2 % over the benchmark network. The model size was reduced by 51.26 %, with parameters and FLOPs decreasing by 47.6 % and 49.1 %, respectively, compared to the benchmark, all under identical hardware conditions. Further experiments were conducted on the Hold-out Test Set (HTS), designed to include ECGs that present slight variations to the original conditions , yielding a slightly lower accuracy of 92.4 %, reflecting the dataset&#39;s complexity and clinical variability. Moreover, benchmarking tests using the Massachusetts Institute of Technology-Beth Israel Hospital (MIT-BIH) arrhythmia database validated the method’s effectiveness, achieving a 99.24 % accuracy and maintaining a lightweight model of 4366KB. Comparative analysis with existing methods confirmed the proposed method’s superiority in accuracy and real-world applicability. This research presents a significant advancement in ECG analysis, offering a viable solution for efficient arrhythmia monitoring in portable healthcare devices.},
  archive      = {J_ASOC},
  author       = {Yunqing Liu and Jinlei Liu and Yuanyuan Tian and Yanrui Jin and Zhiyuan Li and Liqun Zhao and Chengliang Liu},
  doi          = {10.1016/j.asoc.2024.111340},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111340},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pruned lightweight neural networks for arrhythmia classification with clinical 12-lead ECGs},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A negative selection algorithm with hypercube interface
detectors for anomaly detection. <em>ASOC</em>, <em>154</em>, 111339.
(<a href="https://doi.org/10.1016/j.asoc.2024.111339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative selection algorithms play an important role in anomaly detection. Interface detectors are a special negative selection algorithm that completely eliminates outer holes, but there are detection blind areas. This paper proposes a negative selection algorithm with hypercube interface detectors for anomaly detection. It uses self-sample clusters to construct self-space, and boundary self-sample clusters to describe the interface detectors. It eliminates the detection blind area and improves the detection rate. To validate the performance of the proposed method, experiments were conducted using the iris dataset, the skin segmentation dataset, the Breast Cancer of Wisconsin (BCW) dataset, and the Waveform Database Generator (Version 2) dataset. Experimental results show that the proposed method in this paper has a higher detection rate, lower false alarm rate, and fewer detectors than other anomaly detection methods for the same parameters.},
  archive      = {J_ASOC},
  author       = {Ming Gu and Dong Li and Jia Liu and Wangweiyi Shan and Shulin Liu},
  doi          = {10.1016/j.asoc.2024.111339},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111339},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A negative selection algorithm with hypercube interface detectors for anomaly detection},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularizing deep neural networks for medical image analysis
with augmented batch normalization. <em>ASOC</em>, <em>154</em>, 111337.
(<a href="https://doi.org/10.1016/j.asoc.2024.111337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batch Normalization (BN) is a commonly employed regularization technique for deep neural networks . This technique employs normalization and affine transformation to accelerate the training phase. The normalization process forces the distribution of layer inputs into the standard normal distribution in a mini-batch, resolving the internal covariate shift problem. Meanwhile, the affine transformation preserves the network’s ability to perform non-linear feature transformations. However, the effectiveness of BN can be limited when dealing with small mini-batch sizes, since the batch statistics estimated from inadequate samples are inaccurate and unreliable. To address this issue, we present Noise-Assisted Batch Normalization (NABN), which serves as a variant of BN. The proposed method adds random noise into the mean and variance calculated from the mini-batch during the normalization process, enhancing the diversity of mean and variance. We evaluate the effectiveness of our NABN for image classification on CIFAR-10, retinal OCT, and chest X-ray datasets with various convolutional network architectures such as ResNet-20, ResNet-32, ResNet-44, and ResNet-50. Furthermore, experimental results demonstrate the superiority of our proposed approach over the traditional BN for medical image segmentation using U-Net, as evaluated on the MSD liver dataset. Code is available at https://github.com/ROSENty/NABN.git .},
  archive      = {J_ASOC},
  author       = {Shengqian Zhu and Chengrong Yu and Junjie Hu},
  doi          = {10.1016/j.asoc.2024.111337},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111337},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Regularizing deep neural networks for medical image analysis with augmented batch normalization},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint green dynamic order batching and picker routing
problem using PSO with global worst experience. <em>ASOC</em>,
<em>154</em>, 111336. (<a
href="https://doi.org/10.1016/j.asoc.2024.111336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing importance of various countries’ initiatives on reducing carbon emissions (e.g., carbon border adjustment mechanism, CBAM), intelligent order picking systems have assisted electronic retailers in realizing a green supply chain. Previous studies on picker routing operations focused on adopting offline static customer order information to carry out operational decisions. However, in practice, the customer order information is updated dynamically; and high-efficiency warehouse layouts have been increasingly concerned. Therefore, this study creates a mixed-integer programming model for minimizing both the total carbon footprint and the total penalty cost for delayed orders in the joint order batching and picker routing problem with dynamic arriving orders and shipping time constraints in a high-efficiency fishbone warehouse layout. To solve this complex problem, a particle swarm optimization algorithm that integrates the swarm’s previous global and local worst experiences and a migration mechanism is further proposed to increase solution quality and computing efficiency. In addition, a three-dimensional space for the warehouse floorplan and shipping time constraints is designed to divide orders into batches to address the constraints. Theoretical analysis of this algorithm is conducted, and experimental analysis shows that this algorithm finds superior solutions than current practical strategies, to offer a valuable reference for carbon emission reduction in greening warehousing.},
  archive      = {J_ASOC},
  author       = {Chun-Cheng Lin and Yi-Chun Peng and Jia-Rong Kang},
  doi          = {10.1016/j.asoc.2024.111336},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111336},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Joint green dynamic order batching and picker routing problem using PSO with global worst experience},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk assessment of liquefied natural gas storage tank
leakage using failure mode and effects analysis with fermatean fuzzy
sets and CoCoSo method. <em>ASOC</em>, <em>154</em>, 111334. (<a
href="https://doi.org/10.1016/j.asoc.2024.111334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a clean energy source, liquefied natural gas (LNG) has the advantages of being economical, green and reliable. However, due to the flammable, explosive and low temperature characteristics of LNG, a storage tank leakage accident will cause serious consequences to the surrounding environment and people. Therefore, it is important to analyze the causes of LNG storage tank failure and evaluate the risk level to ensure the safe operation. Failure mode and effect analysis (FMEA) is a useful technique for risk assessment. This study suggests a modified FMEA approach named as FFCCS-FMEA method based on Fermatean fuzzy sets (FFSs) and the combined compromise solution (CoCoSo) method to prevent LNG storage tank leaking failure. The CoCoSo approach is used to assess the risk priority of failure modes, and FFSs are used to reduce the ambiguity and uncertainty of expert risk assessment information . In addition, a novel score function of FFS is introduced to improve discrimination ability. Furthermore, the risk factor weights are calculated by integrating the subjective and objective weights. Finally, a case study on the LNG storage tank leakage in Tangshan, China is employed to show the applicability and efficacy of the proposed FFCCS-FMEA model. The results show that failure mode ancillary pipeline rupture holds the highest risk level, while failure mode design defect has the lowest risk level. The flexibility in managing complex and ambiguous expert evaluation opinions and high discriminability in providing effective risk rankings of failure modes are verified by sensitivity and comparative analyses.},
  archive      = {J_ASOC},
  author       = {Jianxing Yu and Hongyu Ding and Yang Yu and Shibo Wu and Qingze Zeng and Ya Xu},
  doi          = {10.1016/j.asoc.2024.111334},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111334},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Risk assessment of liquefied natural gas storage tank leakage using failure mode and effects analysis with fermatean fuzzy sets and CoCoSo method},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Relevance variable selection variational auto-encoder
network for quality-related nonlinear process monitoring. <em>ASOC</em>,
<em>154</em>, 111333. (<a
href="https://doi.org/10.1016/j.asoc.2024.111333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quality-related process monitoring is essential for revealing changes in product quality and ensuring industrial safety. Therefore, it is crucial to distinguish enough quality-related features within the data. To learn the nonlinear characteristics and obtain quality-related features in process data, a novel process monitoring method named relevance variable selection variational auto-encoder (RVS-VAE) is proposed. Firstly, to enhance the strong quality-related variables and learn the time correlation, the normalized original data is weighted by mutual information and augmented into data matrices. Secondly, the RVS strategy is proposed to select the most quality-related variables from the latent features. These quality-related features are further utilized to monitor the process. What is more, when a fault is detected, relevance variable relative contribution (RVRC) method is presented to locate the fault variables. Finally, performance of the proposed RVS-VAE method is evaluated through comparative experiments on tennessee eastman (TE) process.},
  archive      = {J_ASOC},
  author       = {Yao Ma and Hongbo Shi and Shuai Tan and Bing Song and Yang Tao},
  doi          = {10.1016/j.asoc.2024.111333},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111333},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Relevance variable selection variational auto-encoder network for quality-related nonlinear process monitoring},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Teaching-learning-based optimization algorithm with dynamic
neighborhood and crossover search mechanism for numerical optimization.
<em>ASOC</em>, <em>154</em>, 111332. (<a
href="https://doi.org/10.1016/j.asoc.2024.111332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an improving teaching-learning-based optimization algorithm (called DRCMTLBO) combined with the dynamic ring neighborhood topology. Firstly, based on the individuals’ fitness distribution and clustered state within and beyond the ring neighborhood, two evaluations of relative neighborhood quality (RNQ) are developed to separately guide population evolution. On the one hand, a dynamic neighborhood strategy driven by fitness-based evaluation is used to adjust neighborhood radius , maintaining individual variability and neighborhood diversity. On the other hand, to utilize individuals’ information of the entire topology, a novel crossover search mechanism driven by Euclidean distance-based evaluation is used to expand the search space , determining whether individuals should enhance exploitation within the neighborhood or exploration beyond the neighborhood. Finally, the above strategies are embedded into the TLBO algorithm, assisted by improved search approaches that achieve a significant balance between exploitation and exploration. Numerical computation results on functions of CEC2014 and CEC2020 show that our proposed DRCMTLBO algorithm outperforms other ten typical algorithms significantly, and its computational performance can compete with several CEC winner algorithms.},
  archive      = {J_ASOC},
  author       = {Zhibo Zeng and He Dong and Yunlang Xu and Wei Zhang and Hangcheng Yu and Xiaoping Li},
  doi          = {10.1016/j.asoc.2024.111332},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111332},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Teaching-learning-based optimization algorithm with dynamic neighborhood and crossover search mechanism for numerical optimization},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Your trip, your way: An adaptive tourism recommendation
system. <em>ASOC</em>, <em>154</em>, 111330. (<a
href="https://doi.org/10.1016/j.asoc.2024.111330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considerable changes in the behavior of tourists have been observed because the call for their customized experiences prevails in the market. In addition, advancements in artificial intelligence have significantly promoted the development of tourism recommendation systems (TRS), which substantially contributes in enhancing tourists’ personalized experiences. However, existing systems are facing increasing challenges in handling uncertain environments and the changing on-site behavior of tourists. This study intends to address these challenges by proposing an adaptive TRS, which mainly consists of three modules: data collection, itinerary design, and itinerary adjustment. To evaluate the proposed system’s performance, a case study was conducted in Xiamen, China. Results indicated that our system can adapt to uncertain environments and tourists’ changing on-site behavior. Tourism organizations could integrate our system in upgrading their information service platforms as part of their smart tourism construction to enhance the competitiveness of destinations.},
  archive      = {J_ASOC},
  author       = {Yuguo Yuan and Weimin Zheng},
  doi          = {10.1016/j.asoc.2024.111330},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111330},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Your trip, your way: An adaptive tourism recommendation system},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stock trend prediction based on dynamic hypergraph
spatio-temporal network. <em>ASOC</em>, <em>154</em>, 111329. (<a
href="https://doi.org/10.1016/j.asoc.2024.111329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting stock trends is conducive to optimize returns from stock investments, which gains great interest from investors and researchers. Relations between stocks can provide important information for stock trend prediction. However existing stock prediction approaches only consider pairwise linkages, and ignore complex higher-order relations among stocks. To address these limitations, this paper proposes a dynamic hypergraph spatio-temporal network (DHSTN). DHSTN utilizes GRU to learn the sequential embedding of stocks, and a dynamic hypergraph network is proposed to learn the spatio-temporal relations among stocks. In the dynamic hypergraph network, firstly, a novel dynamic hypergraph construction module based on graph attention network is designed to capture stock higher-order spatial relations which are dynamically changing over time. Secondly, an industry relations aggregator based on hypergraph is considered in hypergraph convolution. Finally, a multi-relation fusion module is designed to integrate static and dynamic stock relations. Experiments on CSI300 and NASDAQ100 datasets show that DHSTN outperforms representative stock prediction methods by at least 4.99% in terms of F1-score and at least 47.9% in terms of sharpe ratio.},
  archive      = {J_ASOC},
  author       = {Sihao Liao and Liang Xie and Yuanchuang Du and Shengshuang Chen and Hongyang Wan and Haijiao Xu},
  doi          = {10.1016/j.asoc.2024.111329},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111329},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stock trend prediction based on dynamic hypergraph spatio-temporal network},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective wavelet neural network approach for solving
first and second order ordinary differential equations. <em>ASOC</em>,
<em>154</em>, 111328. (<a
href="https://doi.org/10.1016/j.asoc.2024.111328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of efficient numerical methods for obtaining numerical solutions of first and second order ordinary differential equations (ODEs) is of paramount importance , given the widespread utilization of ODEs as a means of characterizing the behavior in various scientific and engineering disciplines. While various artificial neural networks (ANNs) approaches have recently emerged as potential solutions for approximating ODEs, the limited accuracy of existing models necessitates further advancements. Hence, this study presents a stochastic model utilizing wavelet neural networks (WNNs) to approximate ODEs. Leveraging the compact structure and fast learning speed of WNNs, an improved butterfly optimization algorithm (IBOA) is employed to optimize the adjustable weights, facilitating more effective convergence towards the global optimum. The proposed WNNs approach is then rigorously evaluated by solving first and second order ODEs, including initial value problems , singularly perturbed boundary value problems , and a Lane–Emden type equation. Comparative analyses against alternative training methods, other existing ANNs, and numerical techniques demonstrate the superior performance of the proposed method, affirming its efficiency and accuracy in approximating ODE solutions.},
  archive      = {J_ASOC},
  author       = {Lee Sen Tan and Zarita Zainuddin and Pauline Ong and Farah Aini Abdullah},
  doi          = {10.1016/j.asoc.2024.111328},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111328},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective wavelet neural network approach for solving first and second order ordinary differential equations},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An intelligent digital twinning approach for complex
circuits. <em>ASOC</em>, <em>154</em>, 111327. (<a
href="https://doi.org/10.1016/j.asoc.2024.111327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital twinning process is essential for transferring real-world objects to the Metaverse by creating accurate digital versions, known as digital twins. However, complex systems pose challenges in this process. With the increasing utilization of microwave components and circuits in telecommunication systems such as IoT, 5 G, and 6 G, the need for digital twins of these components arises. Nevertheless, high-frequency components exhibit intricate behavior, requiring advanced modeling techniques. Artificial intelligence (AI) provides a powerful tool for enhancing the reliability and accuracy of estimated models in such cases. In this study, a microstrip lowpass filter (LPF) is designed, fabricated, and measured as the physical twin. An intelligent digital twinning approach is employed using a machine learning method based on an adaptive neuro-fuzzy inference system (ANFIS), trained by a subtractive clustering algorithm. The resulting digital twin of the LPF proves valuable for communication networks and IoT applications. Moreover, this research showcases the applicability and accessibility of machine learning in creating digital twins of electromagnetic components for communication cyber-physical systems (CPSs) and the Metaverse. Furthermore, the proposed method exhibits adaptability to various passive and active electrical or electronic circuits. By harnessing the potential of AI and digital twinning, this study presents a robust and accurate approach for modeling and analyzing complex circuits, specifically in the context of communication systems and their integration into the Metaverse. The findings highlight the advantages of an intelligent digital twinning approach and its potential for advancing various domains involving complex circuitry.},
  archive      = {J_ASOC},
  author       = {Mohammad (Behdad) Jamshidi and Saeedeh Lotfi and Hesam Siahkamari and Tomas Blecha and Jakub Talla and Zdeněk Peroutka},
  doi          = {10.1016/j.asoc.2024.111327},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111327},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An intelligent digital twinning approach for complex circuits},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-strategy active learning for power quality disturbance
identification. <em>ASOC</em>, <em>154</em>, 111326. (<a
href="https://doi.org/10.1016/j.asoc.2024.111326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As clean energy facilities are increasingly integrated into power distribution networks, the incidence of multiple power quality disturbances (MPQDs) is on the rise. Currently, most MPQDs classification models rely on supervised learning, which demands a substantial amount of labeled data. However, obtaining labeled MPQDs data from the real world is often a time-consuming and labor-intensive process, requiring annotation by experienced engineers, particularly for events involving MPQDs. To tackle this challenge, we introduce an innovative active learning (AL) approach known as Adaptive Weighting Multi-Strategy AL (AWMSAL) for MPQDs classification. In our framework, AWMSAL simultaneously considers multiple strategies using rank aggregation to select MPQDs data that is both full-information and highly representative for labeling. In addition, our method employs a unique weight calculation function to dynamically adjust weights based on the differences in data values observed with various strategies during the AL process. To enhance the classifier performance, we incorporate the Laplacian manifold regularizer into the Extreme Learning Machine in both supervised and unsupervised settings. We compared the performance of the AWMSAL method against several state-of-the-art active learning algorithms using synthetic and hardware-generated data sets. The results demonstrated that AWMSAL achieved recognition rates exceeding 99% in a shorter number of iterations. When the same termination condition was met, AWMSAL outperformed other active learning algorithms by 2.45% to 18.33% in terms of accuracy.},
  archive      = {J_ASOC},
  author       = {Haoyi Zhang and Wei Wu and Kaicheng Li and Xinyue Zheng and Xuebin Xu and Xuan Wei and Chen Zhao},
  doi          = {10.1016/j.asoc.2024.111326},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111326},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-strategy active learning for power quality disturbance identification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An uncertainty-based objective function for hyperparameter
optimization in gaussian processes applied to expensive black-box
problems. <em>ASOC</em>, <em>154</em>, 111325. (<a
href="https://doi.org/10.1016/j.asoc.2024.111325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As of today, Gaussian processes (GPs) have been widely and successfully used in the context of design optimization based on expensive-to-evaluate functions. This supervised learning method enables a generation of accurate nonlinear surrogate models based on relatively small datasets. Nonetheless, their most valuable asset is to provide uncertainty in predictions. Despite their excellent stochastic properties , Gaussian processes are unfortunately not immune to threats such as the generation of distorted predictions, especially when the amount of data available is very limited. This shortcoming is caused by a poor choice of the GP hyperparameters and represents a serious threat to the efficiency and effectiveness of the whole surrogate-based optimization. In this paper we present the Hybrid Loss (HL), a novel uncertainty-aware objective function for the hyperparameter tuning of Gaussian processes. This method is intended to exploit information coming from the predictive variance to remedy the typical shortcomings of the log marginal likelihood , i.e. the objective function commonly used to optimize GP hyperparameters. By pairing this methodology with a well-known adaptive sampling strategy, we investigate the performance on a wide range of benchmark functions and a real engineering problem. The observed evidence clearly shows how uncertainty can be successfully exploited to make a wiser choice of the hyperparameters. This translates into more accurate predictions, surrogate models less prone to overfitting, and above all, greatly improved convergence rates.},
  archive      = {J_ASOC},
  author       = {Pietro Lualdi and Ralf Sturm and Andrés Camero and Tjark Siefkes},
  doi          = {10.1016/j.asoc.2024.111325},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111325},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An uncertainty-based objective function for hyperparameter optimization in gaussian processes applied to expensive black-box problems},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An attentive copula-based spatio-temporal graph model for
multivariate time-series forecasting. <em>ASOC</em>, <em>154</em>,
111324. (<a href="https://doi.org/10.1016/j.asoc.2024.111324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series forecasting is widely applied to electricity consumption. However, accurate prediction for tasks is challenging due to intricate spatial dependencies and non-linear temporal dynamics. Existing models have limited capability for considering correlation factors, leading to reduced accuracy. Incorporating geographical information can enhance predictions in multivariate models. Graph neural networks effectively capture variable interdependencies , and including location information between nodes complements these dependencies. Therefore, we propose an attentive spatio-temporal graph neural network framework for accurate time-series forecasting. Our approach incorporates time-series and geographical factors to enhance prediction accuracy. We create a geometrical graph using node locations and a probabilistic graph structure learned from node embedding to capture non-linear temporal dynamics. The attention mechanism facilitates feature crossover, improving spatial-related features. We model representation and correlation information based on joint distributions in the nodes, separating them into edge densities and Copula densities. We link the graph structure and the covariance matrix in the Copula densities. Extensive evaluations of the public electrical consumption dataset demonstrate that our approach outperforms state-of-the-art models, significantly improving accuracy in multi-factor time-series forecasting tasks such as electricity consumption.},
  archive      = {J_ASOC},
  author       = {Xihe Qiu and Jiahui Qian and Haoyu Wang and Xiaoyu Tan and Yaochu Jin},
  doi          = {10.1016/j.asoc.2024.111324},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111324},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An attentive copula-based spatio-temporal graph model for multivariate time-series forecasting},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quaternion tensor completion with sparseness for color video
recovery. <em>ASOC</em>, <em>154</em>, 111322. (<a
href="https://doi.org/10.1016/j.asoc.2024.111322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel low-rank completion algorithm based on the quaternion tensor is proposed in this paper. This approach uses the TQt-rank of quaternion tensor to maintain the structure of RGB channels throughout the entire process. In more detail, the pixels in each frame are encoded on three imaginary parts of a quaternion as an element in a quaternion matrix. Each quaternion matrix is then stacked into a quaternion tensor. A logarithmic function and truncated nuclear norm are employed to characterize the rank of the quaternion tensor in order to promote the low rankness of the tensor. Moreover, by introducing a newly defined quaternion tensor discrete cosine transform-based (QTDCT) regularization to the low-rank approximation framework, the optimized recovery results can be obtained in the local details of color videos. In particular, the sparsity of the quaternion tensor is reasonably characterized by l 1 l1 norm in the QDCT domain. This strategy is optimized via the two-step alternating direction method of multipliers (ADMM) framework with convergence analysis. Numerical experimental results for recovering color videos show the obvious advantage of the proposed method over other potential competing approaches.},
  archive      = {J_ASOC},
  author       = {Liqiao Yang and Kit Ian Kou and Jifei Miao and Yang Liu and Pui Man Hoi},
  doi          = {10.1016/j.asoc.2024.111322},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111322},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Quaternion tensor completion with sparseness for color video recovery},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hybrid decomposed fuzzy multi-criteria decision-making
model for optimizing parcel lockers location in the last-mile delivery
landscape. <em>ASOC</em>, <em>154</em>, 111321. (<a
href="https://doi.org/10.1016/j.asoc.2024.111321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parcel lockers are considered an effective solution to address inefficiencies in last mile delivery. However, choosing the appropriate location is crucial to provide optimal service and improve logistic performance. Therefore, this research aims to assess the most suitable locations for parcel lockers in Dublin, the capital of Ireland, using a novel hybrid decision-making model. This study seeks to extend the Combinative Distance based Assessment method and integrate the Analytical Hierarchy Process method to handle parcel locker location evaluation problems through Decomposed Fuzzy Sets. The proposed model is a pioneering application in real-life scenarios, contributing to the existing knowledge in this field. The integrated model we propose offers a suitable approach for selecting parcel locker locations. The outcomes of this study will provide valuable insights for policymakers, aiding them in making strategic decisions regarding the selection of parcel locker locations within Dublin city.},
  archive      = {J_ASOC},
  author       = {Sarbast Moslem and Fatma Kutlu Gündoğdu and Serhat Saylam and Francesco Pilla},
  doi          = {10.1016/j.asoc.2024.111321},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111321},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hybrid decomposed fuzzy multi-criteria decision-making model for optimizing parcel lockers location in the last-mile delivery landscape},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of an interpretable raman-spectral
classification model based on a transposed convolution and attention
mechanism to tumor-tissue screening. <em>ASOC</em>, <em>154</em>,
111320. (<a href="https://doi.org/10.1016/j.asoc.2024.111320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin and gastric cancers are known for their high mortality rates. In contemporary clinical practice, medical imaging methods are prevalently utilized for the analysis and evaluation of tumor tissues. However, these methods are limited by two major factors. Firstly, they heavily rely on the experience of personnel, which may lead to diagnostic inaccuracies. Secondly, the significant changes in cell morphology that these methods detect typically manifest in the middle or later stages of the disease, rendering them less effective for early cancer screening. Interestingly, these diseases are often marked by alterations in metabolic products , which can be used to identify early-stage cancerous tissues based on compositional changes. This approach not only significantly reduces reliance on experiential judgment but also minimizes manual intervention. Employing Raman spectroscopy, an optical testing method known for its molecular fingerprinting capabilities, this paper introduces the TA-Net classification model . Developed based on Raman spectroscopy data, TA-Net outperforms conventional models in terms of accuracy, specificity, and interpretability . It incorporates a transposed convolution and attention mechanism to address the challenge of varying data lengths collected by different Raman spectrometers, a common issue that has hindered the application of previously trained models. The model features down-sampling, up-sampling, convolutional attention mechanisms, and global-pooling classification modules, and is trained end-to-end. This reduces preprocessing complexity and results in high-precision, interpretable prediction outputs. In comparative experiments using collected skin and gastric cancer Raman spectral data, the TA-Net model demonstrated improvements in sensitivity (0.74% and 6.12%), specificity (5.63% and 5.09%), and accuracy (3.15% and 5.54%) over the suboptimal model. This study offers a promising approach for early cancer screening and detection, holding significant practical value for clinical diagnosis.},
  archive      = {J_ASOC},
  author       = {Dongdong Wan and Zhong Wang and Shasha Liu and Yuee Li and Wenyan Li},
  doi          = {10.1016/j.asoc.2024.111320},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111320},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of an interpretable raman-spectral classification model based on a transposed convolution and attention mechanism to tumor-tissue screening},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binary firefly algorithm based reconfiguration for maximum
power extraction under partial shading and machine learning approach for
fault detection in solar PV arrays. <em>ASOC</em>, <em>154</em>, 111318.
(<a href="https://doi.org/10.1016/j.asoc.2024.111318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functioning of a photovoltaic (PV) array in the shadow presents significant issues owing to power loss, which affects the harvested power. One of the greatest promising methods for reducing the effect of shadow over the array is reconfiguration . Therefore, this work proposes a modern metaheuristic Binary Firefly Algorithm (BFA) technique for solving the reconfiguration mechanism of a partial shade PV array. The proposed BFA is tested on a 9 × 9 panel PV array with four established shadow arrangements: short wide (SW), long wide (LW), short narrow (SN), and long narrow (LN). The proposed BFA approach yields configurations superior when compared to existing reconfigurations. The best GMP boost obtained with the proposed BFA concerning TCT configuration is 36% in the SW shadow pattern, 30 in the LW arrangement, while the least is 7% in the SN and LN patterns. To evaluate the ability of the proposed system, it is compared with other reconfiguration methods tested across various performance metrics such as fill factor, mismatch power loss, percentage of power loss, and power enhancement. The energy estimates and revenue generation demonstrate that the proposed BFA approach generates 15% more power than the TCT setup and other methods. Furthermore, the Nave Bayes based Machine Learning (ML) approach is applied to detect physical degradation in PV panels. To validate the performance proposed ML method and other techniques are undergone with both faulty and non-faulty conditions. The output of the proposed ML technique-based identification of faults approach is compared to existing techniques. The acquired results supported the proposed BFA with ML&#39;s ability and superiority in optimally reconfiguring the shaded array.},
  archive      = {J_ASOC},
  author       = {S. Saravanan and R.Senthil Kumar and P. Balakumar},
  doi          = {10.1016/j.asoc.2024.111318},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111318},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binary firefly algorithm based reconfiguration for maximum power extraction under partial shading and machine learning approach for fault detection in solar PV arrays},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving dynamic multi-objective optimization problems via
quantifying intensity of environment changes and ensemble learning-based
prediction strategies. <em>ASOC</em>, <em>154</em>, 111317. (<a
href="https://doi.org/10.1016/j.asoc.2024.111317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithms designed to solve dynamic multi-objective optimization problems (DMOPs) need to consider all of the multiple conflicting objectives to determine the optimal solutions. However, objective functions, constraints or parameters can change over time, which presents a considerable challenge. Algorithms should be able not only to identify the optimal solution but also to quickly detect and respond to any changes of environment. In order to enhance the capability of detection and response to environmental changes, we propose a dynamic multi-objective optimization (DMOO) algorithm based on the detection of environment change intensity and ensemble learning (DMOO-DECI&amp;EL). First, we propose a method for detecting environmental change intensity, where the change intensity is quantified and used to design response strategies. Second, a series of response strategies under the framework of ensemble learning are given to handle complex environmental changes. Finally, a boundary learning method is introduced to enhance the diversity and uniformity of the solutions. Experimental results on 14 benchmark functions demonstrate that the proposed DMOO-DECI&amp;EL algorithm achieves the best comprehensive performance across three evaluation criteria, which indicates that DMOO-DECI&amp;EL has better robustness and convergence and can generate solutions with better diversity compared to five other state-of-the-art dynamic prediction strategies. In addition, the application of DMOO-DECI&amp;EL to the real-world scenario, namely the economic power dispatch problem, shows that the proposed method can effectively handle real-world DMOPs.},
  archive      = {J_ASOC},
  author       = {Zhenwu Wang and Liang Xue and Yinan Guo and Mengjie Han and Shangchao Liang},
  doi          = {10.1016/j.asoc.2024.111317},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111317},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving dynamic multi-objective optimization problems via quantifying intensity of environment changes and ensemble learning-based prediction strategies},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using entropy maximisation for establishing city-wide
touristic tour patterns. <em>ASOC</em>, <em>154</em>, 111316. (<a
href="https://doi.org/10.1016/j.asoc.2024.111316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comprehending the travel patterns of tourists is the foundation for sustainable urban tourism policies. We establish a tour-based entropy maximisation model to estimate the number of tourists partaking in specific tours. We define “tour” by the starting time, the places tourists visit and the order of the visits over the course of a day. Our main input data are the spatial-temporal distribution of tourists with respect to specific areas of the city. We formulate optimisation problems with and without additional knowledge of specific average tour characteristics such as tour starting times, the number of places visited, and the total tourist number. We find solutions that maximise entropy and minimise a penalty formulated as the sum of squared errors on constraints. Limitations are discussed due to non-convexity and the linearly independent condition not being met in some cases. We verify the efficacy of our method through a survey conducted by the Kyoto city government. It is shown that the errors on constraints are small, and our model successfully estimates the number of tourists partaking in each tour with a high degree of accuracy, provided appropriate constraints. Hence our methodology equips tourism planners with information to devise recommendations aimed at preventing over-tourism, thus fostering a more sustainable and enjoyable travel experience.},
  archive      = {J_ASOC},
  author       = {Tomoki Nishigaki and Jan-Dirk Schmöcker and Tadashi Yamada and Satoshi Nakao},
  doi          = {10.1016/j.asoc.2024.111316},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111316},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using entropy maximisation for establishing city-wide touristic tour patterns},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way multi-attribute decision-making under the double
hierarchy hesitant fuzzy linguistic information system. <em>ASOC</em>,
<em>154</em>, 111315. (<a
href="https://doi.org/10.1016/j.asoc.2024.111315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way multi-attribute decision-making (3MADM) integrated with double hierarchy hesitant fuzzy linguistic term set (DHHFLTS) can not only effectively express the uncertainty of language, but also help reduce the risk of wrong decision-making. However, the existing compared methods for DHHFLTSs dismiss the variances in psychological reference points , resulting in mismatches in form and connotation for some linguistic terms. Furthermore, it is difficult or even impossible to obtain an accurate degree of difference using the existing DHHFLTS distance method for these linguistic terms. This directly affects the accuracy of obtaining conditional probabilities results in the three-way decision model. Therefore, this paper introduces the concept of the superior gradus for double hierarchy linguistic term set (DHLTS) and double hierarchy hesitant fuzzy linguistic element (DHHFLE), respectively. Then, some novel compared methods are defined that allow the identification of differences between linguistic variables . Subsequently, based on the superior gradus, a novel distance measurement is designed with a risk parameters. Through the adjustment of risk parameters, this method can respectively obtain optimistic and pessimistic results. Also, the relative loss functions designed for DHHFLTSs aim at getting more objective decision-making results. Finally, the paper proposes a novel 3MADM method under the double hierarchy hesitant fuzzy linguistic information system (DHHFLIS) and applies it to service assessment. To verify the effectiveness and rationality, the medical diagnosis data set is used, and the results are compared with other classic MAMD methods.},
  archive      = {J_ASOC},
  author       = {Nanfang Luo and Qinghua Zhang and Longjun Yin and Qin Xie and Chengying Wu and Guoyin Wang},
  doi          = {10.1016/j.asoc.2024.111315},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111315},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way multi-attribute decision-making under the double hierarchy hesitant fuzzy linguistic information system},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multimodal dynamic graph convolutional network for
crowdfunding success prediction. <em>ASOC</em>, <em>154</em>, 111313.
(<a href="https://doi.org/10.1016/j.asoc.2024.111313">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdfunding creates opportunities for creative people to raise funds so their ideas can be brought to life. However, the failure of fundraising leads to certain losses for project starters. Crowdfunding success prediction allows them to learn about the probability of fundraising success as early as possible, which can reduce their losses or lead them to modify project content for increasing the probability. Crowdfunding success prediction is a challenging classification task because there is diverse descriptive data but relative little supervisory information. There has been a lot of work done based on post-launch factors, but pre-launch prediction is more valuable for project creators and crowdfunding platforms . Although some efforts on pre-launch prediction were also made, most of them only analyze one or more of metadata, text and image, and the multimodal study about text and video is lacked. Considering this, we propose a multimodal dynamic graph convolutional network for crowdfunding success prediction based on text and video data. Specifically, sentences and frames are viewed as nodes, and edges are constructed based on location relationships and cosine similarity , which reconstructs the text and video into a multimodal graph and allows multimodal features to interact in the form of a graph. Besides, dynamic graph convolution is utilized to fuse the features of nodes, where dynamic means the structure of graph changes during graph convolution. Besides, we make two multimodal crowdfunding datasets containing metadata, text, image, video and project status, and they are utilized to validate the effectiveness of crowdfunding prediction models. Experiments conducted on them show that our model outperforms existing state-of-the-art baselines, and interaction and fusion in the form of a graph can effectively integrate features from multiple modalities.},
  archive      = {J_ASOC},
  author       = {Zihui Cai and Hongwei Ding and Mohan Xu and Xiaohui Cui},
  doi          = {10.1016/j.asoc.2024.111313},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111313},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multimodal dynamic graph convolutional network for crowdfunding success prediction},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Support matrix machine with truncated pinball loss for
classification. <em>ASOC</em>, <em>154</em>, 111311. (<a
href="https://doi.org/10.1016/j.asoc.2024.111311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of vector-based classifiers to matrix-based classifiers, noise insensitivity and sparsity have always been the focal points . Existing SMM and Pin-SMM enjoy the former and the latter separately. To remedy the shortcoming, we propose a support matrix machine based on truncated pinball loss (TPin-SMM) in this paper, which integrates noise insensitivity and sparsity simultaneously. Thanks to the adding of two quantiles , it possesses precious properties including Bayes rule and bounding misclassification error as well. Concerning the non-convexity of TPin-SMM, a targeted CCCP-ADMM algorithm is established, which decomposes the problem into three sub-problems of each sub-iteration. To verify the validity of TPin-SMM, we have conducted numerical experiments on image datasets, EEG signal sets and Daimler Pedestrian Classification Benchmark dataset with different noises, all of them pass the statistical tests.},
  archive      = {J_ASOC},
  author       = {Huiyi Li and Yitian Xu},
  doi          = {10.1016/j.asoc.2024.111311},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111311},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Support matrix machine with truncated pinball loss for classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified new-information-based accumulating generation
operator based on feature decoupling for multi-characteristic time
series forecasting. <em>ASOC</em>, <em>154</em>, 111310. (<a
href="https://doi.org/10.1016/j.asoc.2024.111310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diverse internal system laws and complex external environments often generate multi-characteristic time series, presenting challenges for forecasting methods in terms of model adaptability and practical feasibility. To address this issue, this paper introduces a novel approach by enhancing the Accumulating Generation Operator ( AGO ), commonly employed in grey prediction models, as a sequence pre-process technique for forecasting models. Our innovation lies in the development of a unified new-information-based AGO ( UNAGO ), which distinguishes itself by multiple weight adjustment effects encompassing unconstrained scaling, exponential, and equal-accumulation terms. This technique can significantly enhance the adaptive variability of the accumulating weight structure and mitigate possible incompatibility. To validate our approach, we conduct comprehensive comparisons using UNAGO against seven existing AGO s within grey models . We select five datasets that span different industries and domains, and involve forecasts with various data trend characteristics, data lengths, and prediction horizons. Despite precision comparisons, we conduct further experiments regarding sample-size and trend-reversal analyses and robustness tests on heuristic intelligent algorithms. Empirical results show that the new model with UNAGO exhibits optimal predictive accuracies with improvement rates of over 51% in terms of MAPE values compared to all its rivals, demonstrating its strongest adaptability and robustness with multiple data trend characteristics and short-to-long terms forecasts. Additionally, the robustness tests, including algorithm comparisons, Monte Carlo simulations , and hyper-parameter sensitivity analysis, validate UNAGO &#39;s optimal heuristic intelligent algorithm and confirm its computational stability.},
  archive      = {J_ASOC},
  author       = {Song Ding and Zhijian Cai and Juntao Ye and Bianjing Ma},
  doi          = {10.1016/j.asoc.2024.111310},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111310},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A unified new-information-based accumulating generation operator based on feature decoupling for multi-characteristic time series forecasting},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Canonical triangular interval type-2 fuzzy linguistic
distribution assessment EDAS approach with its application to production
supplier evaluation and ranking. <em>ASOC</em>, <em>154</em>, 111309.
(<a href="https://doi.org/10.1016/j.asoc.2024.111309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation and ranking of suppliers is a critical challenge because it profoundly influences the profitability and competitive positions of manufacturers and producers. This paper presents a canonical triangular interval type-2 fuzzy linguistic distribution assessment based on the distance from average solution technique (CTriIT2F-LDA-EDAS), which is a novel method for evaluating and ranking production suppliers using linguistic distribution assessment (LDA) techniques. The CTriIT2F-LDA uses a canonical triangular interval type-2 fuzzy number (CTriIT2FN) to simplify the trapezium (or trapezoidal) interval type-2 fuzzy number (TrIT2FN) typically used in LDA, thereby making it less intricate and more readily adaptable due to its resemblance to the triangular fuzzy number. This paper presents calculation methods for both the CTriIT2FN and LDA, which involve a de-fuzzification function, distance measurements, and entropy measurements . Moreover, the proposed method was implemented to assess production suppliers involved in the production of electric vehicle DC charging stations for the State Grid Corporation of China. A sensitivity analysis that used both expert and criterion weights was performed. Subsequently, a comparative analysis with existing methods was conducted to validate the versatility and efficacy of the method. The results emphasize the critical factors that influence supplier competitiveness, such as environmental protection training, emissions, safety standards , inventory capacity, and minimum order requirements. Consequently, there is now a pressing need for policies focused on fostering environmental training and expanding both production and inventory management capabilities. The evaluation presented here is used to confirm the adaptability and efficiency of the CTriIT2F-LDA-EDAS method.},
  archive      = {J_ASOC},
  author       = {Yongting Tian and Shouxu Song and Siriguleng Bao and Dan Zhou and Chen Wei},
  doi          = {10.1016/j.asoc.2024.111309},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111309},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Canonical triangular interval type-2 fuzzy linguistic distribution assessment EDAS approach with its application to production supplier evaluation and ranking},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prioritization of drip-irrigation pump alternatives in
agricultural applications: An integrated picture fuzzy BWM&amp;CODAS
methodology. <em>ASOC</em>, <em>154</em>, 111308. (<a
href="https://doi.org/10.1016/j.asoc.2024.111308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the irrigation methods is drip irrigation, for which selecting the right pump has a significant impact. In this study, the process of choosing the appropriate pump for drip irrigation is regarded as a multi-criteria decision-making (MCDM) problem. The objective is to enhance productivity and minimize water consumption in agricultural areas by addressing the drip-irrigation pump-selection problem. Making decisions under uncertainty is a complex task, especially when dealing with intricate problems where complexity raises concerns about finding more dependable solutions. Fuzzy extensions of MCDM methods are designed to tackle such intricate and detailed problems compared to traditional MCDM methods. Therefore, we propose and implement a Picture Fuzzy CODAS (PF-CODAS) method to address the issue of drip-irrigation pump selection under vagueness, utilizing expert opinions. In comparison to other MCDM methods, our suggested approach combines multi-criteria decision analysis with picture fuzzy hesitancy and a negative ideal solution, supported by Euclidean and Taxicab distances. Furthermore, we present an integrated approach that uses the Best Worst Method (BWM) to determine criterion weights and the PF-CODAS method for ranking alternatives. Overall, this study offers valuable support for advancing sustainable agriculture through our proposed MCDM approach.},
  archive      = {J_ASOC},
  author       = {Eren Kamber and Ufuk Aydoğmuş and Hacer Yumurtacı Aydoğmuş and Mehmet Gümüş and Cengiz Kahraman},
  doi          = {10.1016/j.asoc.2024.111308},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111308},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Prioritization of drip-irrigation pump alternatives in agricultural applications: An integrated picture fuzzy BWM&amp;CODAS methodology},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Meta-heuristic algorithms for double roman domination
problem. <em>ASOC</em>, <em>154</em>, 111306. (<a
href="https://doi.org/10.1016/j.asoc.2024.111306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A variety of domination concepts have been defined to provide better routing and defense strategies under different constraints. A double Roman dominating function (DROMDF) on a simple, undirected graph G G is a function g : V → { 0 , 1 , 2 , 3 } g:V→{0,1,2,3} such that every vertex x ∈ V x∈V with g ( x ) = 0 g(x)=0 is adjacent to at least two vertices y 1 , y 2 y1,y2 with g ( y 1 ) = g ( y 2 ) = 2 g(y1)=g(y2)=2 or a vertex z 1 z1 with g ( z 1 ) = 3 g(z1)=3 . Also, a vertex p p with g ( p ) = 1 g(p)=1 is adjacent to at least one vertex q 1 q1 with g ( q 1 ) ≥ 2 g(q1)≥2 . γ d R ( G ) γdR(G) , the double Roman domination number of G G , is the smallest possible weight of all possible DROMDFs of G G . Determining double Roman domination number of a graph is known to be NP-hard. Hence in this paper, we propose a genetic algorithm based approach for solving double Roman domination problem in which three heuristic algorithms have been proposed and problem specific crossover operator and a feasibility function has been developed. Further, we propose an ant colony optimization algorithm to solve double Roman domination problem. This paper provides an in-depth illustration of two algorithms for solving double Roman domination problem. Effectiveness of the proposed meta-heuristic algorithms is tested on the random graphs generated using NetworkX Erdős-Rényi model, a popular model for graph generation and Harwell–Boeing dataset, a well-known dataset for graph related problems. Further, we compare the results of both the meta-heuristic algorithms and the experimental results show that the proposed meta-heuristic algorithms for solving double Roman domination problem give a near optimal solution in reasonable time. Experimental results also show that the proposed ant colony optimization algorithm for solving double Roman domination problem outperforms genetic algorithm based procedure.},
  archive      = {J_ASOC},
  author       = {Himanshu Aggarwal and P. Venkata Subba Reddy},
  doi          = {10.1016/j.asoc.2024.111306},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111306},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Meta-heuristic algorithms for double roman domination problem},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Balancing composite motion optimization using r-ERNN with
plant disease. <em>ASOC</em>, <em>154</em>, 111288. (<a
href="https://doi.org/10.1016/j.asoc.2024.111288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soybean is a crop with a long cultivation history that plays a significant role on agricultural field. Early diagnosis of Soybean Mosaic Virus Disease (SMVD) is must require, because it causes a fast reduction and significant losses in soybean yields. Therefore, this paper proposes a Balancing Composite Motion optimization utilizing Recalling-Enhanced Recurrent Neural Network with Plant Disease for separating its severity into 0, 1 and 2 grades. Initially, hyper spectral image data was taken from Spec View software. Then, the spectral imageries of soybean leaves are processed to remove the redundancy in frequency bands that emerges owing to hyper spectral camera does not present any important information are filtered by Dual Tree Complex Wavelet Transform (DTCWT). In this, hyper spectral image features are extracted using Ternary pattern and discrete wavelet transform (TP-DWT) method. After completing the process of feature extraction, the feature extracted imageries are given to Recalling-Enhanced Recurrent Neural Network (RE-RNN) and it is used to classify the Soybean mosaic disease. Here, Balancing Composite Motion Optimization (BCMO) algorithm is employed for tuning the RE-RNN hyper parameters. The proposed PDI-RE-RNN-BCMO-HSI method provides 27.03%, 28.94% and 39.04% higher precision compared with existing method like grading model of soybean mosaic infection depending upon hyper spectral imaging technology (PDI-CNN-SVM-HSI), recognization of soybean varieties under hyper spectral imaging technology and one-dimensional CNN (PDI-1D-CNN-HSI) and hyper spectral imaging technology combined with multivariate models to recognize soybean disease (PDI-SNV-ELM-HSI) respectively.},
  archive      = {J_ASOC},
  author       = {R. Venkatesan and G.N. Balaji},
  doi          = {10.1016/j.asoc.2024.111288},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111288},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Balancing composite motion optimization using R-ERNN with plant disease},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empowering robust biometric authentication: The fusion of
deep learning and security image analysis. <em>ASOC</em>, <em>154</em>,
111286. (<a href="https://doi.org/10.1016/j.asoc.2024.111286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many societal entities now have more excellent standards for the efficacy and dependability of identification systems due to the ongoing advancement of computer technology. Traditional identification methods, such as keys and smart cards, have been supplanted by biometric systems in highly secure environments. This research presents a smart computational method for automatically authenticating fingerprints for identity (ID) verification and personal identification. Compared to more traditional machine learning algorithms, the results from applying Deep learning (DL) in areas like computer vision, image identification, robotics, and voice processing have generally been positive. Due to their capacity to analyse big data size and deal with fluctuations in biometric data (such as ageing or expression problems), DL has been heavily used by the artificial intelligence research community. Several biometric systems have succeeded with automatic feature extraction employing deep learning approaches like Convolutional Neural Networks (CNNs). In this research, we provide a biometric process that uses convolutional neural networks. This work introduces a deep learning-based biometric identification system that uses Monte Carlo Dropout (MC Dropout). Combining these two systems makes the authentication process more secure and dependable. Fingerprint image enhancement techniques involve the application of Gabor filters and structure-adaptive anisotropic filters, which have proven to be effective in enhancing the clarity and distinctiveness of fingerprint patterns. To improve the efficiency of deep learning models, this work proposes the Inception-Augmentation GAN (IAGAN) model for data augmentation. The study adds to security development by integrating novel biometric identification and authentication approaches with cutting-edge neural network technology. In this research, we provide a new activation function to speed up the convergence of deep neural networks. The results of 99.6% on Gabor filters and 99.8% on the structure-adaptive anisotropic filter with GACNN with MCD show that deep neural networks can excel over competing approaches with enough training data.},
  archive      = {J_ASOC},
  author       = {Zhu Wen and Songtong Han and Yongmin Yu and Xuemin Xiang and Shenzheng Lin and Xiaoling Xu},
  doi          = {10.1016/j.asoc.2024.111286},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111286},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Empowering robust biometric authentication: The fusion of deep learning and security image analysis},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An offbeat bolstered swarm integrated ensemble learning
(BSEL) model for heart disease diagnosis and classification.
<em>ASOC</em>, <em>154</em>, 111273. (<a
href="https://doi.org/10.1016/j.asoc.2024.111273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cardiovascular disease also known as heart disease has emerged as the top cause of mortality throughout the world. Several different cardiac conditions are included. There are many variables that might increase a person&#39;s likelihood of developing heart disease; thus, it is critical to develop reliable, rapid methods of diagnosis and treatment. Therefore, the proposed work aims to develop an effective framework, named as, Bolstered Swarm Integrated Ensemble Learning (BSEL) for heart disease detection. Here, the given medical dataset has been cleaned up, transformed, and normalized using the Linear Interpolation Normalization (LIN) technique. For selecting the best features, the Bolstered-up Beetle Swarm Optimization (BBSO) method is used that removes the irrelevant attributes from the standardized datasets. In addition, the best features are employed in the Weighted Ensemble Classification (WEC) model to establish whether or not a patient has heart disease. The accuracy is improved by using an ensemble classifier with the optimal number of hidden neurons , as determined by the Red Colobuses Monkey Optimization (RCoMO) model. The proposed BSEL prediction model&#39;s results are tested and assessed during experiment using a variety of performance measures .},
  archive      = {J_ASOC},
  author       = {R. Subathra and V. Sumathy},
  doi          = {10.1016/j.asoc.2024.111273},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111273},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An offbeat bolstered swarm integrated ensemble learning (BSEL) model for heart disease diagnosis and classification},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Dual quaternion hand-eye calibration algorithm for
hunter-prey optimization based on twice opposition-learning and random
differential variation. <em>ASOC</em>, <em>154</em>, 111249. (<a
href="https://doi.org/10.1016/j.asoc.2024.111249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem of noise interference in camera calibration and robot forward kinematics solution, this study proposes a dual quaternion hand-eye calibration algorithm based on twice opposition-learning and random differential variation (ODHPO). The hand-eye calibration equation was innovatively rewritten in the form of dual quaternion, and the F-norm minimization model of the rotation and translation error function was constructed for optimization. At the same time, the penalty function method is introduced to effectively transform the constrained optimization problem in hand-eye calibration into an unconstrained problem, which is further solved by the ODHPO algorithm for global optimization. Compared with the singular value decomposition algorithm based on the traditional dual quaternion method, the ODHPO algorithm performs better in global optimization ability and convergence stability. Through numerical simulation and real robot hand-eye calibration experiments , it is proved that the proposed algorithm is superior to the traditional dual quaternion (CDQ) algorithm, the classical algorithms Tsai method and Navy method in terms of solution accuracy, sensitivity to the number of pose transformations and stability, demonstrating its potential for application and practical significance in robot vision system.},
  archive      = {J_ASOC},
  author       = {Yun-tao Zhao and Wen Li and Wei-gang Li},
  doi          = {10.1016/j.asoc.2024.111249},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111249},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dual quaternion hand-eye calibration algorithm for hunter-prey optimization based on twice opposition-learning and random differential variation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Binarized spiking neural network with blockchain based
intrusion detection framework for enhancing privacy and security in
cloud computing environment. <em>ASOC</em>, <em>154</em>, 111218. (<a
href="https://doi.org/10.1016/j.asoc.2023.111218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing (CC) is prone to attacks, which upsurges complication and erudition. By this, its origins provocative implications can enterprise data veracity, concealment, and accessibility. To overwhelm these issues, a Binarized Spiking Neural Network with block chain based deputized proof of stake consensus (DPoS) algorithm fostered intrusion detection scheme is proposed in this manuscript to enhance the privacy and the security on the cloud computing Environment (EP-DPoSBC-ES-BS4NN-IDS-CC). The data is amassed from NSL-KDD benchmark dataset. The first-level privacy process is carried with block chain based deputized proof of stake consensus (DPoS) algorithm. The secondary level privacy process is carried out by utilizing the pre-processing and the feature selection process. For, pre-processing, proposed Basic interlude gradient filtering (BIGF) are utilized to eradicate the unsolicited content and filtering pertinent data. The pre-processing outcome is supplied to the feature selection phase. Here, the ideal features are taken with the help of Weightiness espoused feature assortment approaches (WEFA). The data is classified as normal or abnormal based on Binarized Spiking Neural Network. Subsequently, the proposed EP-DPoSBC-ES-BS4NN-IDS-CC is examined under some performance metrics. The proposed technique attains 12.94 %, 17.68 %, 17.99 % and 13.96 % improved accuracy; 59.9 %, 50 %, 31.45 % and 48.17 % lower Computation Time and 3.19 %, 0.83 %, 2.1 % and 5.43 % higher AUC than the existing methods. Customers and cloud service providers may find this framework useful as a decision-support tool in helping them move their data in a safe, timely, and reliable manner. In future work, a prototype of the approach will develop in real-world scenario, capably inside a tight network of connected computers. It allows evaluate effectively in real-world utility.},
  archive      = {J_ASOC},
  author       = {Velliangiri Sarveshwaran and Shanthini Pandiaraj and Garikapati Bindu and Vithya Ganesan and Iwin Thanakumar Joseph Swamidason},
  doi          = {10.1016/j.asoc.2023.111218},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111218},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Binarized spiking neural network with blockchain based intrusion detection framework for enhancing privacy and security in cloud computing environment},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Practical guidelines for resolving the loss divergence
caused by the root-mean-squared propagation optimizer. <em>ASOC</em>,
<em>153</em>, 111335. (<a
href="https://doi.org/10.1016/j.asoc.2024.111335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Root-mean-squared propagation (RMSProp) and adaptive moment estimation (Adam) are optimizers often adopted for deep learning , owing to their adaptive learning rate and speedy convergence. However, when the gradients decrease after numerous iterations of model training, the learning rates of these optimizers become relatively large, and the loss becomes difficult to manage. Several researchers have proposed methods for improving the convergence of the learning procedures, and have proven RMSProp leads to loss divergence. However, the detection and management of the loss divergence have not been discussed. In this study, we describe how to detect loss divergence and then demonstrate countermeasures for maximizing the advantages of RMSProp, Adam, and Adam variants while avoiding their disadvantages. To verify the proposed method, we conduct a series of experiments using the popular Cifar-10 and Penn Treebank datasets for vision- and language-based tasks. We emphasize how to detect loss divergence in an early stage, and how to take advantage of countermeasures to address it. Accordingly, neural network practitioners can save a large amount of training time when a loss divergence occurs.},
  archive      = {J_ASOC},
  author       = {Yuan-Long Peng and Wei-Po Lee},
  doi          = {10.1016/j.asoc.2024.111335},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111335},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Practical guidelines for resolving the loss divergence caused by the root-mean-squared propagation optimizer},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A picture fuzzy set multi criteria decision-making approach
to customize hospital recommendations based on patient feedback.
<em>ASOC</em>, <em>153</em>, 111331. (<a
href="https://doi.org/10.1016/j.asoc.2024.111331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis techniques have allowed exploiting the information available in millions of opinions conveyed through different Internet services. One example would be the multiple opinions about medical experiences in hospitals available on the website called Careopinion. These opinions usually talk about different medical aspects such as staff, facilities, etc., in a positive, negative, or neutral manner. Nevertheless, there are situations in which the same opinion contains positive, neutral and negative ideas regarding the same aspect. This fact leads to a perception of hesitancy and uncertainty about the opinion. To deal with this issue, this study proposes a picture fuzzy set-based model able to represent this hesitancy in terms of polarity values. To test this model, it has been used to implement a multicriteria decision making-based hospital recommender which considers the patient preferences with respect to the aspects of the hospitals. The proposed approach has been tested using real reviews from 8 hospitals considering diverse patient preferences. The results of all experiments were compared against an ideal ranking computed from the patient reviews using Spearman’s footrule. Furthermore, to assess the effectiveness of the proposal, it has been compared against other state-of-the-art logic-based polarity representation mechanisms. The findings demonstrate that the proposed approach is more effective than the other polarity representation methods by at least 4%, confirming the superiority of the proposed approach to capture and represent sentiments in an accurate manner.},
  archive      = {J_ASOC},
  author       = {Mohammad Bani-Doumi and Jesus Serrano-Guerrero and Francisco Chiclana and Francisco P. Romero and Jose A. Olivas},
  doi          = {10.1016/j.asoc.2024.111331},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111331},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A picture fuzzy set multi criteria decision-making approach to customize hospital recommendations based on patient feedback},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient medical image classification network based on
multi-branch CNN, token grouping transformer and mixer MLP.
<em>ASOC</em>, <em>153</em>, 111323. (<a
href="https://doi.org/10.1016/j.asoc.2024.111323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, medical image classification techniques based on deep learning have made remarkable achievements, but most of the current models sacrifice the efficiency of the model for performance improvement . This poses a great challenge in practical clinical applications. Meanwhile, Convolutional Neural Network (CNN)-based methods, Visual Transformer(ViT)-based and Multi-layer Perceptron(MLP)-based methods have their own advantages and disadvantages in capturing local features and global features of medical images. And there is no good method to combine the three to achieve a better trade-off in model scale and performance. Based on the above problems, we propose Eff-CTM: an hybrid efficient medical image classification network based on multi-branch CNN, token grouping Transformer and mixer MLP. It combines the advantages of all three and takes a small number of parameters to classify pneumonia, colon cancer histopathology and dermatology images quickly and accurately. Eff-CTM uses an efficient CNN module with multi-branch structure to learn local detail information in the shallow CNN stage of the network, an efficient CNN, Transformer (ECT) module and efficient MLP (EM) module in the middle stage of the network to extract local features and global features. An efficient Transformer (ET) module is used in the final stage to fuse the rich feature information. We have conducted extensive experiments on three publicly available medical image classification datasets, and the experimental results show that our proposed Eff-CTM achieves a better trade-off in efficiency and performance than methods based on CNN, Transformer and MLP.},
  archive      = {J_ASOC},
  author       = {Shiwei Liu and Liejun Wang and Wenwen Yue},
  doi          = {10.1016/j.asoc.2024.111323},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111323},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient medical image classification network based on multi-branch CNN, token grouping transformer and mixer MLP},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complemented subspace-based weighted collaborative
representation model for imbalanced learning. <em>ASOC</em>,
<em>153</em>, 111319. (<a
href="https://doi.org/10.1016/j.asoc.2024.111319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative representation-based classifiers (CRCs) have demonstrated remarkable classification performance in various pattern recognition fields. However, their success heavily relies on a balanced class distribution. More radically, significantly skewed class distributions may severely affect the effectiveness of the CRCs. To address this limitation, in this study, a complemented subspace-based regularization technique is integrated into the CRC framework for tackling imbalanced classification tasks . To enhance recognition accuracy for minority classes, a class weight learning algorithm is presented, in which the weight of each class is adaptively obtained according to the prior distribution information of the original data. Furthermore, the proposed model provides an efficient closed-form solution, ensuring computational efficiency comparable to CRCs. Extensive experiments on diverse imbalanced datasets are carried out to substantiate the superiority of our method over numerous advanced imbalanced learning algorithms.},
  archive      = {J_ASOC},
  author       = {Yanting Li and Junwei Jin and Hongwei Tao and Yang Xiao and Jing Liang and C.L. Philip Chen},
  doi          = {10.1016/j.asoc.2024.111319},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111319},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Complemented subspace-based weighted collaborative representation model for imbalanced learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust wide &amp; deep learning framework for log-based
anomaly detection. <em>ASOC</em>, <em>153</em>, 111314. (<a
href="https://doi.org/10.1016/j.asoc.2024.111314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log-based anomaly detections have shown huge commercial potential in system maintenance. However, existing methods encounter two practical challenges. Firstly, they struggle to maintain consistent performance when dealing with evolving logs over time. Secondly, they face difficulties in effectively detecting frequency anomalies, such as abnormal system resource usage and abnormal system operating frequencies. In this paper, we propose a robust log-based anomaly detection framework using Wide &amp; Deep learning called WDLog. Particularly, we enhance the processing of template semantic information by building upon the Drain algorithm, then we introduce invariant features and statistical features and propose a multi-feature anomaly detection method based on the Wide &amp; Deep framework. The experimental results on HDFS and BGL datasets demonstrate the promising performance of WDLog compared to state-of-the-art methods in terms of anomaly detection effectiveness. Furthermore, WDLog exhibits robustness to evolving logs, achieving F1-scores of over 90% under different degrees of log variation.},
  archive      = {J_ASOC},
  author       = {Weina Niu and Xuhan Liao and Shiping Huang and Yudong Li and Xiaosong Zhang and Beibei Li},
  doi          = {10.1016/j.asoc.2024.111314},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111314},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A robust wide &amp; deep learning framework for log-based anomaly detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An explainable AI framework for credit evaluation and
analysis. <em>ASOC</em>, <em>153</em>, 111307. (<a
href="https://doi.org/10.1016/j.asoc.2024.111307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loan Facility is a profitable venture for the banking industry and can render great financial support to the beneficiary. The Global banking systems with secured private cloud are making the service reachable around the world around the clock. Loan acceptance and disbursal are governed by the protocol of the banks with the highest degree of privacy and integrity. As per the report of Experian, the loan acceptance rate of the banks has been reduced to 61%–70%, and it is further reduced to 50% post-pandemic since there is a huge financial setback and a higher rate of defaulters. The banks are not in a position to explain the reasoning behind the rejection since the rejection further diminishes the customers’ credit scores. With the parallel improvements in Industry 5.0 , futuristic banking could evolve around Non Fungible Tokens (NFT) integrated Explainable AI (XAI) framework which can interact with the customer through the human-machine interface in the meta-verse. For such a kind of system, the proposed work could be a driving application which provides explanations for the loan rejection, with the Random Forest integrated XAI framework that provides the reasons for acceptance and rejection of the loan. The proposed Random Forest-based approach rendered the highest accuracy, sensitivity and specificity of 0.998, 0.998, and 0.997, respectively. The LIME and SHAPELY Explainers provide explanations with local and global surrogates of various parameters on the features.},
  archive      = {J_ASOC},
  author       = {M.K. Nallakaruppan and Balamurugan Balusamy and M. Lawanya Shri and V. Malathi and Siddhartha Bhattacharyya},
  doi          = {10.1016/j.asoc.2024.111307},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111307},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An explainable AI framework for credit evaluation and analysis},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predictive reinforcement learning in non-stationary
environments using weighted mixture policy. <em>ASOC</em>, <em>153</em>,
111305. (<a href="https://doi.org/10.1016/j.asoc.2024.111305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) within non-stationary environments presents a formidable challenge. In some applications, anticipating abrupt alterations in the environment model might be possible. The existing literature lacks a framework that proactively harnesses such predictions to enhance reward optimization. This paper introduces an innovative methodology designed to preemptively leverage these predictions, thereby maximizing the overall achieved performance. This is executed by formulating a novel approach that generates a weighted mixture policy from both the optimal policies of the prevailing and forthcoming models. To ensure safe learning, an adaptive learning rate is derived to facilitate training of the weighted mixture policy. This theoretically guarantees monotonic performance improvement at each update during training. Empirical trials focus on a model-free predictive reference tracking scenario involving piecewise constant references. Through the utilization of the cart–pole position control problem, it is demonstrated that the proposed algorithm surpasses prior techniques such as context Q-learning and RL with context detection algorithms in non-stationary environments. Moreover, the algorithm outperforms the application of individual optimal policies derived from each observed environment model (i.e., policies not utilizing predictions).},
  archive      = {J_ASOC},
  author       = {Hossein Pourshamsaei and Amin Nobakhti},
  doi          = {10.1016/j.asoc.2024.111305},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111305},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predictive reinforcement learning in non-stationary environments using weighted mixture policy},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary multi-target neural network architectures for
flow void analysis in optical coherence tomography angiography.
<em>ASOC</em>, <em>153</em>, 111304. (<a
href="https://doi.org/10.1016/j.asoc.2024.111304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography angiography (OCTA) is a non-invasive imaging modality used to evaluate the retinal microvasculature. Recent advances in OCTA allows to visualize the blood flow within the choriocapillaris region, where a granular image is obtained showing a pattern of small dark regions, called flow voids (FVs). Given its relevance, numerous clinical studies have linked the changes in FVs distribution to multiple diseases. The granular structure of these images makes accurate labeling and segmentation difficult, which can be overcome by using a multi-target perspective. However, manually designing a neural architecture that can accurately predict all targets in a balanced way is a major challenge. In this work, we propose a novel methodology based on evolutionary multi-target optimized networks that, through a set of evolutionary operators , traverses a search space of architectures in a deep but efficient way. This methodology allows us to discover efficient and accurate multi-target architectures tailored to our problem, but which are also adaptable to other tasks due to their robustness. To validate and analyze our methodology and the discovered network model, we performed extensive experimentation with cases from a real clinical study, achieving better results than the state of the art and manually designed architectures.},
  archive      = {J_ASOC},
  author       = {Emilio López-Varela and Joaquim de Moura and Jorge Novo and José Ignacio Fernández-Vigo and Francisco Javier Moreno-Morillo and Julián García-Feijóo and Marcos Ortega},
  doi          = {10.1016/j.asoc.2024.111304},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111304},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-target neural network architectures for flow void analysis in optical coherence tomography angiography},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A four-stage branch local search algorithm for minimal test
cost attribute reduction based on the set covering. <em>ASOC</em>,
<em>153</em>, 111303. (<a
href="https://doi.org/10.1016/j.asoc.2024.111303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is a fundamental problem in rough set theory and serves as an effective data reduction technique. The minimal test cost attribute reduction problem poses a more challenging task built upon the classical reduction problem and finds widespread applications in the real world. The development of efficient heuristic algorithms is crucial for effectively addressing this problem. However, research efforts utilizing local search algorithms for solving the minimal test cost attribute reduction problem remain limited. This paper aims to combine the graph-based rough set attribute reduction method with the effectiveness of local search algorithms known for solving the set covering problem . It proposes a method to transform the minimal test cost attribute reduction problem into a set covering problem and a four-stage branch local search algorithm based on the set covering is designed to solve the minimal test cost attribute reduction problem, which includes three core algorithmic techniques to further improve the practical performance of the algorithm. Extensive experiments are conducted to evaluate the effectiveness of the proposed algorithms, comparing them against various existing algorithms. The experimental results demonstrate that the proposed algorithms achieve fewer attributes and lower test costs within an acceptable running time, while maintaining competitive classification accuracy .},
  archive      = {J_ASOC},
  author       = {Haoran Su and Jinkun Chen and Yaojin Lin},
  doi          = {10.1016/j.asoc.2024.111303},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111303},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A four-stage branch local search algorithm for minimal test cost attribute reduction based on the set covering},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fish module ‐ a prognostic tool for modeling the optimal
environmental conditions for fish. <em>ASOC</em>, <em>153</em>, 111302.
(<a href="https://doi.org/10.1016/j.asoc.2024.111302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article discusses the service called the Fish Module for the predictive determination of the optimal environmental conditions for the occurrence of sprat, herring, cod, and flounder on the example of fish found in the southern Baltic Sea (in particular in the Gulf of Gdańsk region). The Fish Module utilizes data from the EcoFish ecohydrodynamic model and then uses fuzzy logic and fish preference data to calculate the Habitat Suitability Index (HSI). Data on fish preference were determined based on 587 expeditions during which physicochemical parameters of the sea were recorded. Our analysis determined threshold HSI values below which successful catches are unlikely for sprat, herring, and cod, confirming the system&#39;s effectiveness in identifying locations with favorable environmental conditions for these species. Fishermen are advised to select routes with specific HSI thresholds to achieve greater efficiency in fishing. Furthermore, the analysis revealed that fishing durations do not necessarily correlate with higher catches, emphasizing the importance of selecting suitable routes based on favorable environmental conditions for fish habitat. We expect the Fish Module to be the most demanded product of the FindFISH project.},
  archive      = {J_ASOC},
  author       = {Maciej Janecki and Lidia Dzierzbicka-Głowacka},
  doi          = {10.1016/j.asoc.2024.111302},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111302},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fish module ‐ a prognostic tool for modeling the optimal environmental conditions for fish},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024f). A generative adversarial networks based methodology for
imbalanced multidimensional time-series augmentation of complex
electromechanical systems. <em>ASOC</em>, <em>153</em>, 111301. (<a
href="https://doi.org/10.1016/j.asoc.2024.111301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multidimensional monitoring time-series of complex electromechanical systems (CESs) plays a foundational role in data-based state management, maintenance, and performance adjustment. However, it is still a challenging work to extract valuable and complete information due to the imbalanced data . To address this issue, a methodology called GAN4MTS (Generative Adversarial Networks for Multidimensional Time-Series) that generates synthetic data closely mimicking the characteristics of real data was proposed, thus directly tackling the problem of data imbalance. First, the uniqueness of multidimensional time series of CESs was analyzed to identify the requirements for data augmentation and to define the problem formulations. Second, the architecture and loss functions of GAN4MTS model were designed based on generative adversarial networks and three specific constraints. Finally, the effectiveness of the proposed work was validated through comparative analysis. Furthermore, the intrinsic mechanisms of data augmentation in enhancing the model capabilities were discussed. The proposed methodology serves as a comprehensive technical solution for data augmentation, enabling the generation of high-quality synthetic data that adheres to the constraints of multidimensional time series in CESs. Additionally, as an open architecture model, this work provides novel methods for time-series data augmentation, addressing the issues of low accuracy and limited model generalization in state classification, identification, and prediction tasks for CESs caused by the presence of highly imbalanced data.},
  archive      = {J_ASOC},
  author       = {Rongxi Wang and Te Li and Zekai Gao and Xuegang Yan and Jin Wang and Zhen Wang and Jianmin Gao},
  doi          = {10.1016/j.asoc.2024.111301},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111301},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A generative adversarial networks based methodology for imbalanced multidimensional time-series augmentation of complex electromechanical systems},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective optimization of electrical discharge
machining parameters using particle swarm optimization. <em>ASOC</em>,
<em>153</em>, 111300. (<a
href="https://doi.org/10.1016/j.asoc.2024.111300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This manuscript presents an efficient multi-objective optimization method based on using particle swarm optimization together with a desirability function that can be applied where the response variables may have an opposite behavior and where the range of variation of the independent variables as well as those of the responses are subjected to constraints, which has a great deal of industrial interest. For example, maintaining roughness and dimensional tolerances within a tolerance range is determined by the design requirements of the manufactured parts (shape errors, microgeometry errors, etc.) and these requirements must be met in the manufacture of parts. It is demonstrated that it is possible to obtain optimal results in the ranges of variation considered for the independent variables, with regard to those obtained by experimentation. Similarly, models based on Adaptive Network-based Fuzzy Inference Systems are used to solve the problem that may arise from the inadequate fitting of the regression models. Thus, thanks to this present study a fast and efficient method is available for the multiple-optimization of response variables, subject to constraints on both response and independent variables, which are obtained from experiments and modelled by means of soft computing techniques . Furthermore, it is also demonstrated that it is possible to obtain technology tables for various manufacturing processes, which is of great interest from a technological point of view so as to obtain the most suitable processing conditions.},
  archive      = {J_ASOC},
  author       = {Carmelo J. Luis-Pérez},
  doi          = {10.1016/j.asoc.2024.111300},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111300},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective optimization of electrical discharge machining parameters using particle swarm optimization},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-neighborhood tabu search for solving multi-budget
maximum coverage problem. <em>ASOC</em>, <em>153</em>, 111299. (<a
href="https://doi.org/10.1016/j.asoc.2024.111299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many practical decision-making problems involve selecting k k distinct subsets of objects from a set of candidate objects such that the chosen objects optimize a given goal while satisfying some constraints. The multi-budget maximum coverage problem (MMCP) is a general model that facilitates formulating such decision-making problems. Given a budget of k k , a set of elements with profits, and a set of items with costs where each item is composed of a subset of the elements, the MMCP aims to select k k disjoint subsets of items that maximize the total profits of the elements covered by these subsets, while their costs do not exceed their respective budgets. In this work, we present a multi-neighborhood tabu search (MTS) for this NP NP -hard problem. MTS comprises three essential components that employ four efficient neighborhoods to collaboratively perform the neighborhood exploration to obtain high-quality local optima, select neighborhood solutions with added perturbations, and generate offspring solutions based on population information exchange. To evaluate the effectiveness of MTS, we performed computations on 30 benchmark instances of the extended BMCP. We compared the results with the approximation algorithm , genetic algorithm , particle swarm optimization algorithm, and CPLEX solver. We also provide experiments to highlight the beneficial effect of the essential components in the MTS algorithm . The experimental results demonstrate that MTS not only performs efficiently but also yields solutions of superior quality.},
  archive      = {J_ASOC},
  author       = {Yawen Liu and Dazhi Pan},
  doi          = {10.1016/j.asoc.2024.111299},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111299},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-neighborhood tabu search for solving multi-budget maximum coverage problem},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neutrosophic data envelopment analysis based on parametric
ranking method. <em>ASOC</em>, <em>153</em>, 111297. (<a
href="https://doi.org/10.1016/j.asoc.2024.111297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of fuzzy numbers and their extensions, especially neutrosophic fuzzy numbers, is inevitable to express the value of the parameters of optimization problems derived from the real world. In this regard, the comparison and ranking of neutrosophic fuzzy numbers as one of the main research topics that can provide a suitable decision to solve optimization problems is of interest to researchers. In this study, we introduce a parametric approach to rank single value trapezoidal neutrosophic numbers which are based on decision–maker aspiration level for degree of the truth membership function and selecting appropriate levels for non-indeterminacy and non-falsity–membership functions. A prominent feature of this method is its high flexibility, which is due to the consideration of acceptable levels for evaluation by the decision maker . Therefore, in most decision issues, it can be used to determine the preferred option . Also, some theorems and reasonable properties are given to demonstrate the effectiveness of our proposed parametric approach. Furthermore, in an example of data envelopment analysis model with neutrosophic information the validity and applicability of the suggested method are examined.},
  archive      = {J_ASOC},
  author       = {Madineh Farnam and Majid Darehmiraki and Zahra Behdani},
  doi          = {10.1016/j.asoc.2024.111297},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111297},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neutrosophic data envelopment analysis based on parametric ranking method},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Preference-based multi-objective evolutionary algorithm with
linear combination scalarizing function and reference point adjustment.
<em>ASOC</em>, <em>153</em>, 111296. (<a
href="https://doi.org/10.1016/j.asoc.2024.111296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practice, the decision-maker (DM) may be only interested in a particular part of Pareto optimal front (PF). For this reason, many preference-based multi-objective evolutionary algorithms (MOEA) have been proposed to find a solution set that approximates the region of the interest (ROI). Most existing preference-based methods focus on selecting solutions in the ROI. Nevertheless, the enhancement of convergence in preference-based MOEAs has been neglected. Most decomposition-based approaches often employ the achievement scalarizing function (ASF) as their scalarizing function. However, it holds a weaker search ability than the weighted sum function (WSF) despite its capability to tackle problems with arbitrary PF geometries. In order to strengthen the selective pressure toward the PF, this paper proposes a new scalarizing function, LSF , which is a linear combination of the WSF and the ASF. Then, a simple adaptive penalty scheme is employed in LSF to balance the search ability and robustness. To focus the search on the ROI, we develop a reference point adjustment method that dynamically adjusts the position of the reference point according to the distance from the approximated target point. We apply the above two innovations to the MOEA/D framework and propose a new preference-based MOEA, namely RAMOEAD. Experimental results show that RAMOEAD is highly competitive when compared with five state-of-the-art preference-based MOEAs . Finally, the proposed algorithm is extended to double reference points for solving the problems in which the ROIs are defined by the reservation and aspiration points.},
  archive      = {J_ASOC},
  author       = {Peipei Zhao and Liping Wang and Zhaolin Fang and Xiaotian Pan and Qicang Qiu},
  doi          = {10.1016/j.asoc.2024.111296},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111296},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Preference-based multi-objective evolutionary algorithm with linear combination scalarizing function and reference point adjustment},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic task allocation in multi autonomous underwater
vehicle confrontational games with multi-objective evaluation model and
particle swarm optimization algorithm. <em>ASOC</em>, <em>153</em>,
111295. (<a href="https://doi.org/10.1016/j.asoc.2024.111295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a non-zero-sum game model based on a multi-objective evaluation model was adopted to solve the dynamic task allocation problem of underwater multiple Autonomous Underwater Vehicle (multi-AUV). By introducing a functional coordination mechanism, the high degree of coordination in practical confrontation scenarios was reflected, and the autonomous decision-making and task allocation process of multiple AUVs were clarified. Secondly, a multi-objective evaluation model including survival value, strike income, and ammunition loss was constructed, and weighted processing was performed on the multiple objectives using the Analytic Hierarchy Process (AHP) method, obtaining the benefits of each non-zero-sum strategy in dynamic games. In addition, a particle swarm optimization (PSO) algorithm that combines the theory of good point sets theory (G) and speed control factors (S) for improvement, called GSPSO, was used to find the optimal strategy in the game and allocate tasks. Finally, simulation analysis showed that the collaborative system with functional coordination mechanisms significantly improved the combat capabilities of underwater multiple AUVs. The multi-objective evaluation model combined with AHP can correctly and comprehensively evaluate the advantages and disadvantages of each strategy and correctly respond to changing confrontation tasks and decision preferences. The proposed algorithm improved the convergence speed and global search ability by enhancing the diversity of the population and the effectiveness of iterative solutions, ensuring real-time decision-making and task allocation in complex high-dimensional dynamic games.},
  archive      = {J_ASOC},
  author       = {Bing Sun and Yuanren Zeng and Daqi Zhu},
  doi          = {10.1016/j.asoc.2024.111295},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111295},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic task allocation in multi autonomous underwater vehicle confrontational games with multi-objective evaluation model and particle swarm optimization algorithm},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grey wolf optimizer based deep learning mechanism for music
composition with data analysis. <em>ASOC</em>, <em>153</em>, 111294. (<a
href="https://doi.org/10.1016/j.asoc.2024.111294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music composition using artificial intelligence has gained increasing research attention recently. However, existing methods often generate music that needs more coherence and authenticity. This paper proposes an evolutionary computation-based deep learning approach for music composition with data analysis. Specifically, we utilize long short-term memory (LSTM) networks for generating melodic sequences and adopt a grey wolf optimizer to optimize LSTM hyperparameters. The training data is first converted to musical instrument digital interface (MIDI) format for data analysis, and melody lines are extracted using a similarity matrix method. The MIDI data is then encoded for input into the LSTM networks. The generated music is evaluated using objective metrics like mean squared error and subjective methods, including surveys of music professionals. Comparisons made to benchmark algorithms like generative adversarial networks demonstrate the advantages of our approach in accurately capturing tone, rhythm, artistic conception, and other attributes of high-quality music. The proposed mechanism provides a practical framework for AI-based music generation while ensuring authenticity.},
  archive      = {J_ASOC},
  author       = {Qian Zhu and Achyut Shankar and Carsten Maple},
  doi          = {10.1016/j.asoc.2024.111294},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111294},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grey wolf optimizer based deep learning mechanism for music composition with data analysis},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Grammar-based evolutionary approach for automated workflow
composition with domain-specific operators and ensemble diversity.
<em>ASOC</em>, <em>153</em>, 111292. (<a
href="https://doi.org/10.1016/j.asoc.2024.111292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of extracting valuable and novel insights from raw data involves a series of complex steps. In the realm of Automated Machine Learning (AutoML), a significant research focus is on automating aspects of this process, specifically tasks like selecting algorithms and optimising their hyper-parameters. A particularly challenging task in AutoML is automatic workflow composition (AWC). AWC aims to identify the most effective sequence of data preprocessing and machine learning algorithms , coupled with their best hyper-parameters, for a specific dataset. However, existing AWC methods are limited in how many and in what ways they can combine algorithms within a workflow. Addressing this gap, this paper introduces EvoFlow , a grammar-based evolutionary approach for AWC. EvoFlow enhances the flexibility in designing workflow structures, empowering practitioners to select algorithms that best fit their specific requirements. EvoFlow stands out by integrating two innovative features. First, it employs a suite of genetic operators, designed specifically for AWC, to optimise both the structure of workflows and their hyper-parameters. Second, it implements a novel updating mechanism that enriches the variety of predictions made by different workflows. Promoting this diversity helps prevent the algorithm from overfitting. With this aim, EvoFlow builds an ensemble whose workflows differ in their misclassified instances. To evaluate EvoFlow ’s effectiveness, we carried out empirical validation using a set of classification benchmarks. We begin with an ablation study to demonstrate the enhanced performance attributable to EvoFlow ’s unique components. Then, we compare EvoFlow with other AWC approaches, encompassing both evolutionary and non-evolutionary techniques. Our findings show that EvoFlow ’s specialised genetic operators and updating mechanism substantially outperform current leading methods in predictive performance . Additionally, EvoFlow is capable of discovering workflow structures that other approaches in the literature have not considered.},
  archive      = {J_ASOC},
  author       = {Rafael Barbudo and Aurora Ramírez and José Raúl Romero},
  doi          = {10.1016/j.asoc.2024.111292},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111292},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Grammar-based evolutionary approach for automated workflow composition with domain-specific operators and ensemble diversity},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). YOLO-based marine organism detection using two-terminal
attention mechanism and difficult-sample resampling. <em>ASOC</em>,
<em>153</em>, 111291. (<a
href="https://doi.org/10.1016/j.asoc.2024.111291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of various types of noise in images of marine-life datasets, as well as the class imbalances in underwater datasets, can exacerbate the difficulty in achieving effective object detection. To address this problem, we proposed you only look once (YOLO)-based marine organism detection using a two-terminal attention mechanism and difficult-sample resampling process. First, a residual building unit (RBU) module with a two-terminal attention mechanism (RBU-TA) was proposed, incorporating a reinforced channel attention mechanism into a shortcut of the residual structure. The proposed method adaptively compressed noisy feature map channels, providing rich shallow image information for high-level deep convolutional features while avoiding shallow noise pollution. To address the imbalance of marine biological image classes, difficult-sample resampling was combined with a focal loss function to suppress excessive background negative samples and retrain targets that could be difficult to distinguish, thus improving their detection accuracy. Finally, the proposed method was validated using the underwater robot professional competition (URPC) and real-world underwater object detection (RUOD) datasets, and the mean average precision (MAP) values of the results improved by 10% and 7%, respectively. The proposed method greatly improved the target detection accuracy of organisms in complex marine environments.},
  archive      = {J_ASOC},
  author       = {Zhiyu Zhou and Yanjun Hu and Xingfan Yang and Junyi Yang},
  doi          = {10.1016/j.asoc.2024.111291},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111291},
  shortjournal = {Appl. Soft. Comput.},
  title        = {YOLO-based marine organism detection using two-terminal attention mechanism and difficult-sample resampling},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel cooperative multiobjective coevolutionary algorithm
for constrained multiobjective optimization problems. <em>ASOC</em>,
<em>153</em>, 111290. (<a
href="https://doi.org/10.1016/j.asoc.2024.111290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing parallel multiobjective evolutionary computation does not perform well for constrained multiobjective optimization problems with discontinuous Pareto fronts or narrow feasible regions . This study parallelizes the state-of-the-art cooperative multiobjective coevolutionary algorithm and proposes an effective parallel evolutionary algorithm for constrained multiobjective optimization problems that are difficult to optimize. Two parallelization methods are compared: a global parallel model in which solution evaluations are performed in parallel, and a hybrid model that treats the cooperative populations in a distributed manner while performing each solution evaluation in parallel. The first model is a straightforward parallelization, while the second one capitalizes on the characteristics of the coevolutionary framework. To investigate the efficacy of the proposed models, experiments are conducted on constrained multiobjective optimization problems, including complex characteristics, while varying the number of parallel cores up to 64. The experiments compare the two proposed methods from the viewpoint of search performance and execution time . The experimental results reveal that the latter hybrid model shows better computational efficiency and scalability against an increasing number of cores without adversely affecting the search performance compared to the former straightforward parallelization.},
  archive      = {J_ASOC},
  author       = {Tomohiro Harada},
  doi          = {10.1016/j.asoc.2024.111290},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111290},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Parallel cooperative multiobjective coevolutionary algorithm for constrained multiobjective optimization problems},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fractional-integer-order echo state network for time series
prediction. <em>ASOC</em>, <em>153</em>, 111289. (<a
href="https://doi.org/10.1016/j.asoc.2024.111289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new echo state network with fractional-order reservoir and integer-order reservoir in series configuration, called fractional-integer-order ESN (FIO-ESN), is proposed for time series prediction. Firstly, considering the infinite memory of fractional-order reservoir, the feature information of input signals will be amplified through the fractional-order reservoir, and then the magnified feature information can be extracted twice by using the integer-order reservoir with very large input weights. Secondly, the magnitude of the fractional-order reservoir state is increased through the integer-order reservoir, and then the output weight can be computed in a reasonable range. Thirdly, in order to realize the stable application of the FIO-ESN, a sufficient stability criterion for the FIO-ESN is given by using an LMI approach. Fourthly, in order to reduce the dependence of the prediction accuracy of the FIO-ESN on the fractional-integer-order reservoir parameters, an optimization algorithm based on gradient descent is given. Finally, two numerical simulation examples and one real-world example are used for demonstrating the feasibility of stability criterion and the learning performance of the FIO-ESN.},
  archive      = {J_ASOC},
  author       = {Xianshuang Yao and Yao Wang and Di Ma and Shengxian Cao and Qingchuan Ma},
  doi          = {10.1016/j.asoc.2024.111289},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111289},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fractional-integer-order echo state network for time series prediction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counter-act against GAN-based attacks: A collaborative
learning approach for anti-forensic detection. <em>ASOC</em>,
<em>153</em>, 111287. (<a
href="https://doi.org/10.1016/j.asoc.2024.111287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive success of deep learning allows us to forge images in more perfect manners for ethical and even unethical purposes. Several forensic methods have been proposed to expose artifacts in fake images. However, the practice of anti-forensics (AF), particularly deep learning-based AF on digital images, has made such forgeries difficult to detect. Therefore, a counter-AF (CAF) algorithm is necessary to reveal AF traces and ensure the authenticity of image content . In this study, we propose a novel data-driven approach to counteract generative adversarial network (GAN)-based AF attacks. We consider different forgery techniques, such as noise addition, filtering, and deepfake generation to generate fake images. Subsequently, GAN-based AF attacks were applied to conceal the forgery fingerprints such that they can deceive forensic methods. We built a new CAF method that allows collaborative learning to detect GAN-based AF attacks. We designed a novel CAF-GAN model by considering the commonly used GAN architectures . The proposed CAF-GAN model generates a new image from the input image, which helps collaborative learning to detect AF images . GAN-based AF attacks can effectively hide forgery fingerprints and significantly reduce the performance of forensic methods. However, the proposed CAF method can effectively detect AF images in match and mismatch scenarios of AF and CAF-GAN models.},
  archive      = {J_ASOC},
  author       = {Kutub Uddin and Tae Hyun Jeong and Byung Tae Oh},
  doi          = {10.1016/j.asoc.2024.111287},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111287},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Counter-act against GAN-based attacks: A collaborative learning approach for anti-forensic detection},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An LSTM-stacked autoencoder multisource response prediction
and constraint optimization for scaled expansion tubes. <em>ASOC</em>,
<em>153</em>, 111285. (<a
href="https://doi.org/10.1016/j.asoc.2024.111285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning is attracting increasing attention due to its excellent predictive power and its ability to be applied in traditional research areas. In this paper, we propose a multisource response prediction network architecture based on a long short-term memory (LSTM)-stacked autoencoder to predict key crashworthiness indicators and complete curve reconstruction. Taking an expansion tube as an example using an equivalent scaling research method, a scaled expandable tubular (SET) finite element model was established and verified using a quasistatic compression test and a full-size coupler and buffer system experiment. A design of experiments (DOE) approach was used to obtain a dataset for training the prediction network. Neural network hyperparameters are critical to network prediction accuracy, and after comparison, the multisource response prediction network architecture showed good computational efficiency and satisfactory prediction accuracy when appropriate hyperparameters were selected. Subsequently, multiobjective constraint optimization was performed using the nondominated sorting genetic algorithm-II (NSGA-II) based on a prediction network architecture, which greatly improved the energy-absorption structure optimization accuracy. The results are expected to provide a research methodology for solving complex engineering problems by establishing a new framework for deep learning algorithms combined with optimization methods.},
  archive      = {J_ASOC},
  author       = {Tuo Xu and Ping Xu and Chengxing Yang and Zhixiang Li and Ao Wang and Weinian Guo},
  doi          = {10.1016/j.asoc.2024.111285},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111285},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An LSTM-stacked autoencoder multisource response prediction and constraint optimization for scaled expansion tubes},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Brain tumor detection with multi-scale fractal feature
network and fractal residual learning. <em>ASOC</em>, <em>153</em>,
111284. (<a href="https://doi.org/10.1016/j.asoc.2024.111284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has enabled the creation of several approaches for segmenting brain tumors using convolutional neural networks . These methods have come about as a direct result of the advancement of the field of machine learning . The proposed pixel-level segmentation is based on fractal residual deep learning; provide an insufficient degree of sensitivity when used for tumor segmentation. This is achieved due to fractal feature extraction and multi-scale approach used for segmentation. If multi-level segmentation is used, it is possible to effectively increase the sensitivity of the segmentation process which is the additional benefit from the proposed method. In this work, the production of tumor region is based on multi-scale pixel segmentation. This approach protects the integrity of tumor information while simultaneously improving the detection accuracy by cutting down on the total number of tumor regions. When compared to the information about the brain found in tumor locations, the proposed strategy has the potential to enhance the percentage of brain tumor information. This work proposes a novel network structure known as the Mutli-scale fractal feature network (MFFN) to increase the accuracy of the network&#39;s classification as well as its sensitivity when it comes to the segmentation of brain tumors. The proposed method with overall feature results in 94.66% accuracy, 94.42% sensitivity and 92.81% specificity using 5-fold cross validation. In this paper the Cancer Imaging Archive (TCIA) dataset has been used in order to evaluate performance evaluation metrics and segmentation results to quantify the superiority of proposed brain tumor detection approach in comparison to existing methods.},
  archive      = {J_ASOC},
  author       = {Shyo Prakash Jakhar and Amita Nandal and Arvind Dhaka and Adi Alhudhaif and Kemal Polat},
  doi          = {10.1016/j.asoc.2024.111284},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111284},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Brain tumor detection with multi-scale fractal feature network and fractal residual learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Different transfer learning approaches for insect pest
classification in cotton. <em>ASOC</em>, <em>153</em>, 111283. (<a
href="https://doi.org/10.1016/j.asoc.2024.111283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boll weevil is an important pest that affects cotton crops worldwide, causing significant economic losses. The classification of the boll-weevil population is crucial for developing effective pest management strategies. However, the low availability of data and features makes classification a challenging task. This study aimed to investigate the use of Transfer Learning (TL) techniques to improve the classification of boll weevil populations. Three types of TL techniques, instance-based, feature-based, and parameter-based, were studied to improve the classification performance of the machine learning algorithms . This work used data from two domains, one with few instances and the other with few features, to test the proposed approaches. Also, climate variables (temperature, humidity, and rainfall) were incorporated as features to predict the level of the boll-weevil attack. The most relevant results of this work are that define (1) How to measure and quantify the similarity or relationship between tasks of different domains; (2) How to select, align, or adapt the relevant features, instances, or models from the source task/domain to the target task/domain; (3) How to reuse parameter settings from the source domain; and (4) How to evaluate and validate the performance and robustness of the TL model on the target task/domain. The proposed approach achieved significant improvements in classification over previous results in the metrics of accuracy and F-measure. For example, in the case with few instances reached an accuracy of 90.79%, while in the case with few features reached an accuracy of 96.28%. Thus, the results demonstrate the effectiveness of TL techniques in improving the classification of boll-weevil populations in cotton crops when few data and/or features are available.},
  archive      = {J_ASOC},
  author       = {Raul Toscano-Miranda and Jose Aguilar and William Hoyos and Manuel Caro and Anibal Trebilcok and Mauricio Toro},
  doi          = {10.1016/j.asoc.2024.111283},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111283},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Different transfer learning approaches for insect pest classification in cotton},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards explainable TOPSIS: Visual insights into the effects
of weights and aggregations on rankings. <em>ASOC</em>, <em>153</em>,
111279. (<a href="https://doi.org/10.1016/j.asoc.2024.111279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-Criteria Decision Analysis (MCDA) is extensively used across diverse industries to assess and rank alternatives. Among numerous MCDA methods developed to solve real-world ranking problems, TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) remains one of the most popular choices in many application areas . TOPSIS calculates distances between the considered alternatives and two predefined ones, namely the ideal and the anti-ideal, and creates a ranking of the alternatives according to a chosen aggregation of these distances. However, interpreting the inner workings of TOPSIS is difficult, especially when the number of criteria is large. To this end, recent research has shown that TOPSIS aggregations can be expressed using the means (M) and standard deviations (SD) of alternatives, creating MSD-space, a tool for visualizing and explaining aggregations. Even though MSD-space is highly useful, it assumes equally weighted criteria, making it less applicable to real-world ranking problems. In this paper, we generalize MSD-space to arbitrary weighted criteria by introducing the concept of WMSD-space defined by what is referred to as weight-scaled means and standard deviations. We demonstrate that TOPSIS and similar distance-based aggregation methods can be successfully illustrated in a plane and interpreted even when the criteria are weighted, regardless of their number. The proposed WMSD-space offers thus a practical method for explaining TOPSIS rankings in real-world decision problems.},
  archive      = {J_ASOC},
  author       = {Robert Susmaga and Izabela Szczęch and Dariusz Brzezinski},
  doi          = {10.1016/j.asoc.2024.111279},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111279},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards explainable TOPSIS: Visual insights into the effects of weights and aggregations on rankings},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incomplete multi-view learning: Review, analysis, and
prospects. <em>ASOC</em>, <em>153</em>, 111278. (<a
href="https://doi.org/10.1016/j.asoc.2024.111278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view data, stemming from diverse information sources , often suffer from incompleteness due to various factors such as equipment failure and data transmission issues. This challenge has given rise to the emerging field of incomplete multi-view learning (IML). To provide guidance for newcomers and researchers in this field, this survey systematically presents an in-depth analysis of IML from generative and discriminative perspectives, focusing on all missing scenarios and various learning tasks. Within these categories, discriminative methods are further classified into matrix learning-based IML and graph learning-based IML, while generative methods encompass generative adversarial networks-based IML, auto-encoder-based IML, and contrastive learning-based IML. Meanwhile, practical applications across various domains are summarized, with extensions of IML to multiple labels as well as unaligned views. To advance this field, we conclude that adapting multi-view learning for incomplete data, addressing complex and arbitrary missing scenarios, tackling high missing ratios, exploring regularization techniques, reducing noise impact, and integrating IML with other learning paradigms are valuable research directions in the future.},
  archive      = {J_ASOC},
  author       = {Jingjing Tang and Qingqing Yi and Saiji Fu and Yingjie Tian},
  doi          = {10.1016/j.asoc.2024.111278},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111278},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incomplete multi-view learning: Review, analysis, and prospects},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Attention-based early warning framework for abnormal
operating conditions in fluid catalytic cracking units. <em>ASOC</em>,
<em>153</em>, 111275. (<a
href="https://doi.org/10.1016/j.asoc.2024.111275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fluid Catalytic Cracking Unit (FCCU) is a critical processing technology in the oil refining industry, playing a vital role in energy efficiency and environmental protection. However, FCCU often encounters various abnormal operating conditions, leading to safety hazards, downtime, and reduced production efficiency. Early warning of these abnormal conditions is crucial but challenging due to high noise, strong hysteresis, and class imbalance problems . To tackle these challenges, a novel and universal attention-based framework called AEW-AOC (Attention-based Early Warning for Abnormal Operating Conditions) is specifically designed for FCCU applications. The proposed AEW-AOC framework incorporates three key components: (1) a Self-Correlation Denoiser (SCD) module is proposed to exploit spatiotemporal data correlation to effectively reduce noise; (2) a Convolutional Long Short-Term Memory (Conv-LSTM) module is employed to address the issue of strong hysteresis by capturing temporal variation features of process parameters; (3) an Anomaly Pattern Attention (APA) module is proposed to enhance the distinguishability of abnormal operating conditions based on clustering results from historical abnormal instances. Extensive experiments demonstrate the effectiveness and superiority of the proposed AEW-AOC framework, particularly in practical applications. Specifically, the AEW-AOC framework obtains an impressive f β fβ score of 91.00% on LIC201, 90.45% on LIC202, and 90.64% on LIC801. The proposed AEW-AOC framework shows great potential in enhancing safety, reducing downtime, optimizing efficiency, promoting sustainability, and expanding its applicability beyond FCCU. Its proactive and versatile nature makes it a valuable tool for improving industrial processes and driving advancements in the field of abnormal operating condition detection and prevention.},
  archive      = {J_ASOC},
  author       = {Chenwei Tang and Jialiang Huang and Mao Xu and Xu Liu and Fan Yang and Wentao Feng and Zhenan He and Jiancheng Lv},
  doi          = {10.1016/j.asoc.2024.111275},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111275},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Attention-based early warning framework for abnormal operating conditions in fluid catalytic cracking units},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discriminative feature selection using signed laplacian
restricted boltzmann machine for speed and generalization improvement of
high dimensional data classification. <em>ASOC</em>, <em>153</em>,
111274. (<a href="https://doi.org/10.1016/j.asoc.2024.111274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep classifiers require lots of computations due to the use of multi-layer structure and processing of bulk input samples. Selecting effective features that have the same accuracy as full features, has recently been considered. Using feature selection, less calculation is needed to update the weights and consequently the speed of the training/testing process is increased. In this research, a new deep feature selection method called Discriminative Deep Feature Selection using Signed Laplacian Restricted Boltzmann Machine (DDFS-SLRBM) is proposed. In the approach, full training samples are fed into a SLRBM model. In the updates, the weight matrix is recomputed using the neighborhood matrix of similar and dissimilar classes which results in the discriminative property of the approach. Then, selected features are identified based on the minimum reconstruction error criterion. The efficiency of the proposed algorithm is demonstrated by performing different experiments on MNIST, GISETTE and Protein datasets. The experiments show that the proposed method is able to improve the classification accuracy and the generalizability of the approach while reducing the processing time. Also, the proposed approach shows scalability on the type of the problem, number of original features, number of samples and the number of final selected features.},
  archive      = {J_ASOC},
  author       = {Yasser Abroshan and Mohammad Hossein Moattar},
  doi          = {10.1016/j.asoc.2024.111274},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111274},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Discriminative feature selection using signed laplacian restricted boltzmann machine for speed and generalization improvement of high dimensional data classification},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining subjective and objective weights considerations to
solve the emergency location selection problems under spherical fuzzy
environments. <em>ASOC</em>, <em>153</em>, 111272. (<a
href="https://doi.org/10.1016/j.asoc.2024.111272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of an emergency will cause difficulties in the selection of emergency locations. The COVID-19 pandemic has caused the establishment of emergency medical facilities to become more important and urgent. However, the selection of emergency locations usually involves unclear assessment information, uncertain information, and unacquirable partial information. Due to incomplete evaluation information, traditional fuzzy set and intuitionistic fuzzy set methods cannot effectively deal with the selection problem of emergency locations. In order to effectively solve the selection problem of emergency locations during an emergency, this paper integrated the spherical fuzzy set and subjective and objective weight considerations to provide correct and reasonable evaluation results for the selection of emergency locations. For the numerical verification, this paper applied the selected location for an emergency temporary hospital during the COVID-19 pandemic location in Istanbul, Turkey as an illustrative example and compared the differences in the calculation results among the proposed flexible emergency location selection method, the spherical weighted arithmetic mean method and the spherical aggregation operator method. The numerical verification assessment results demonstrated that the proposed flexible emergency location selection method not only can deal with unclear, uncertain, and even unacquirable partial information at the same time, but also can consider the subjective and objective weights of assessment data, which could provide effective and correct emergency location selections under spherical fuzzy environments.},
  archive      = {J_ASOC},
  author       = {Kuei-Hu Chang},
  doi          = {10.1016/j.asoc.2024.111272},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111272},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combining subjective and objective weights considerations to solve the emergency location selection problems under spherical fuzzy environments},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Similarity metrics vs human judgment of similarity for
binary data: Which is best to predict typicality? <em>ASOC</em>,
<em>153</em>, 111270. (<a
href="https://doi.org/10.1016/j.asoc.2024.111270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity measures for binary data have been subject to a number of comparative studies. In contrast to these studies, we provide a comparison of similarity measures with human judgment of similarity. For this purpose, we utilize the phenomenon of typicality, whose definition is based on similarity. We observe how well the similarity of objects – either computed by a similarity measure or provided by human judgment – enables the prediction of typicality of these objects in various human categories. In doing so, we examine a large variety of existing similarity measures, and utilize recently available extensive data involving binary data as well as data on human judgment of similarity and typicality.},
  archive      = {J_ASOC},
  author       = {Radim Belohlavek and Tomas Mikula},
  doi          = {10.1016/j.asoc.2024.111270},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111270},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Similarity metrics vs human judgment of similarity for binary data: Which is best to predict typicality?},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interpretable automated feature engineering framework for
improving logistic regression. <em>ASOC</em>, <em>153</em>, 111269. (<a
href="https://doi.org/10.1016/j.asoc.2024.111269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although black-box models such as ensemble learning models often provide better predictive performance than intrinsic interpretable models such as logistic regression , black-box models are not still applicable due to the lack of interpretability . Recently, there has been an explosion of work on explainable machine learning techniques , which utilize external algorithms or models to explain the behavior of black-box models. However, it is problematic to explain the black-box model behavior because the explanation provided might not reveal the real mechanism or decision process of black-box models. In this study, instead of using explainable machine learning techniques , an automated feature engineering task was formulated to help logistic regression achieve predictive performance comparable to or even better than black-box models while maintaining interpretability. In this paper, an INterpretable Automated Feature ENgineering (INAFEN) framework was designed for logistic regression. This framework automatically transforms the nonlinear relationships between numerical features and labels into linear relationships, conducts feature cross through association rule mining , and distills knowledge from black-box models. A case study was performed on gastric survival prediction to present the rationality of the feature transformations through INAFEN and benchmark experiments to show the validity of INAFEN. Experimental results on 10 classification tasks demonstrated that INAFEN achieved an average ranking of 2.60 in area under the ROC curve (AUROC), 3.35 in area under the PR curve (AUROC), 3.70 in F1 score and 3.00 in Brier score (among 13 models), outperforming other interpretable baselines and even black-box models. In addition, the interpretability measurement of INAFEN is significantly better than that of black-box models.},
  archive      = {J_ASOC},
  author       = {Mucan Liu and Chonghui Guo and Liangchen Xu},
  doi          = {10.1016/j.asoc.2024.111269},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111269},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interpretable automated feature engineering framework for improving logistic regression},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-scale cross-attention transformer via graph embeddings
for few-shot molecular property prediction. <em>ASOC</em>, <em>153</em>,
111268. (<a href="https://doi.org/10.1016/j.asoc.2024.111268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Molecular property prediction is a critical step in drug discovery. Deep learning (DL) has accelerated the discovery of compounds with desirable molecular properties for successful drug development. However, molecular property prediction is a low-data problem which makes it hard to solve by regular DL approaches . Graph neural networks (GNNs) operate on graph-structured data using neighborhood aggregation to facilitate the prediction of molecular properties. Nonetheless, GNNs struggle to model the global-semantic structure of graph embeddings for molecular property prediction. Recently, Transformer networks have emerged to model such long-range interactions of molecular embeddings at different scales to predict downstream molecular property tasks. Yet, extending this behavior to molecular embeddings and enabling its training on small biological datasets remains an important challenge in drug discovery. In this work, we study how to learn multi-scale representations from comprehensive graph embeddings for molecular property prediction. To this end, we propose a few-shot GNN-Transformer architecture to combine graph embedding tokens of different sizes and produce stronger features for representation learning . A multi-scale Transformer applies a cross-attention mechanism to exchange information of deep representations fused across two separate branches for small and large embeddings. In addition, a two-module meta-learning framework iteratively updates model parameters across tasks to predict new molecular properties on few-shot data. Extensive experiments on multi-property prediction datasets demonstrate the superior performance of the proposed model when compared with other standard graph-based methods. The code and data underlying this article are available in the repository: https://github.com/ltorres97/FS-CrossTR .},
  archive      = {J_ASOC},
  author       = {Luis H.M. Torres and Bernardete Ribeiro and Joel P. Arrais},
  doi          = {10.1016/j.asoc.2024.111268},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111268},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-scale cross-attention transformer via graph embeddings for few-shot molecular property prediction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Feature selection and its combination with data
over-sampling for multi-class imbalanced datasets. <em>ASOC</em>,
<em>153</em>, 111267. (<a
href="https://doi.org/10.1016/j.asoc.2024.111267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection aims at filtering out some unrepresentative features from a given dataset in order to construct more effective learning models. Furthermore, ensemble feature selection by combining multiple feature selection methods has shown its outperformance over single feature selection. However, the performances of different (ensemble) feature selection methods have not been fully examined over multi-class imbalanced datasets. On the other hand, for class imbalanced datasets, one widely considered solution is to re-balance the datasets by data over-sampling, which generates some synthetic examples for the minority classes. However, the effect of performing (ensemble) feature selection on over-sampling multi-class imbalanced datasets has not been investigated. Therefore, the first research objective is to examine the performances of single and ensemble feature selection methods by fifteen well-known filter, wrapper, and embedded algorithms in terms of classification accuracy . For the second research objective, two orders of combining the feature selection and over-sampling steps are compared in order to find out the best combination procedure as well as the best combined algorithms. The experimental results based on ten different domain datasets containing low to very high feature dimensions show that ensemble feature selection methods slightly perform better than single ones. However, their performance differences are not big. To combine with the Synthetic Minority Oversampling Technique (SMOTE) over-sampling algorithm, performing feature selection first and over-sampling second outperforms the other procedure. Although the best combined algorithms are based on ensemble feature selection, eXtreme Gradient Boosting (XGBoost), as the single best feature selection algorithm , combined with SMOTE provides very similar classification performance to the best combined algorithms. To consider the issues of classification performance and compactional cost, the optimal solution is based on the combined XGBoost and SMOTE.},
  archive      = {J_ASOC},
  author       = {Chih-Fong Tsai and Kuan-Chen Chen and Wei-Chao Lin},
  doi          = {10.1016/j.asoc.2024.111267},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111267},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Feature selection and its combination with data over-sampling for multi-class imbalanced datasets},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An effective memetic algorithm for the close-enough
traveling salesman problem. <em>ASOC</em>, <em>153</em>, 111266. (<a
href="https://doi.org/10.1016/j.asoc.2024.111266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Close-Enough Traveling Salesman Problem (CETSP) is a variant of the well-known Traveling Salesman Problem (TSP). Unlike the TSP, each target in the CETSP has a disk neighborhood, and the target is considered visited when any point in its neighborhood is visited. This feature makes the CETSP a suitable model for many real-world applications. In this work, we propose an effective memetic algorithm that integrates a carefully designed crossover operator and an effective local optimization procedure with original search operators. Experimental results on the 62 well-known benchmark instances show that the algorithm is highly competitive with the state-of-the-art methods, reporting 30 new best upper bounds. We demonstrate the usefulness of the algorithm on a real laser welding robot path planning problem. We provide insights into the understanding of the algorithm design .},
  archive      = {J_ASOC},
  author       = {Zhenyu Lei and Jin-Kao Hao},
  doi          = {10.1016/j.asoc.2024.111266},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111266},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An effective memetic algorithm for the close-enough traveling salesman problem},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simheuristic approach towards supply chain scheduling:
Integrating production, maintenance and distribution. <em>ASOC</em>,
<em>153</em>, 111264. (<a
href="https://doi.org/10.1016/j.asoc.2024.111264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study attempts to integrate production, maintenance, and delivery operations among supply chain members . Despite numerous studies in the field of supply chain management, researchers have often overlooked crucial aspects, such as uncertainties in demand and production. For instance, the significant impact of maintenance activities on production flow has been underrepresented in supply chain management literature. This study investigates these gaps in the context of a fertilizer producer case study , which is characterized by seasonal demand and the functional silos syndrome due to old-fashioned management approaches. This study proposes a mathematical model and two multi-objective simheuristics for the Integrated Production, Maintenance, and Distribution Scheduling Problem (IPMDSP) considering demand variation for multiple products and product delivery time-windows using a heterogeneous fleet of vehicles. The IPMDSP is solved using the ϵ ϵ -constraint method and simheuristics linking the simulation model to customized and tuned versions of Particle Swarm Optimization (MOPSO) and the Non-dominated Sorting Genetic Algorithm (NSGA-II). The optimization objectives include minimizing maintenance duration, distribution costs, and customer dissatisfaction due to delivery tardiness. The results demonstrate the superiority of the simheuristic empowered by NSGA-II over the MOPSO in solving the IPMDSP. The comparison between the performance of deterministic and stochastic approaches in addressing the problem reveals that neglecting uncertainty caused by maintenance activities can lead to an increase in optimization objectives. Furthermore, the proposed simheuristics achieved significant improvements in minimizing objectives in solving the fertilizer producer case study.},
  archive      = {J_ASOC},
  author       = {Rahmat Rabet and Maliheh Ganji and Masood Fathi},
  doi          = {10.1016/j.asoc.2024.111264},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111264},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A simheuristic approach towards supply chain scheduling: Integrating production, maintenance and distribution},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subswarm-guided ant colony optimization with enhanced
pheromone update mechanism and beam search for VNF placement and
routing. <em>ASOC</em>, <em>153</em>, 111263. (<a
href="https://doi.org/10.1016/j.asoc.2024.111263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) is a novel technology that enables flexible and cost-effective networks by replacing the hardware-based middleboxes with software run on virtual machines called virtual network function (VNF). In NFV, each Service Function Chain (SFC) is required to be a set of ordered VNF from that must be optimally located across servers/distributed data centers . This paper studies the multi-objective virtual network function placement and routing problem (MO-VNFPR), which involves placing VNFs optimally in servers nodes and assigning server resources efficiently to these VNFs to satisfy user’s requests in the networks. Most existing methods for this problem only handled one requests or considered VNF placement and SFC routing separately. In this paper, we consider MO-VNFPR with two objectives: (i) minimize the overall cost by deploying servers and installing VNFs; (ii) reduce the network delay. A metaheuristic called subswarm-guided ant colony optimization (named SgACO) is introduced to efficiently solve MO-VNFPR. In SgACO, a subswarm-guided mechanism is adopted to satisfy requests at once by reorganizing the ant colony into subswarms. Moreover, an enhanced pheromone update mechanism and beam search are developed to investigate the search space efficiently. Experimental simulations on COGENT, CONUS, and NSF network topologies demonstrate that our algorithm achieves competitive results and performs better against other existing algorithms.},
  archive      = {J_ASOC},
  author       = {Nguyen Thi Tam and Le Huy Duong and Huynh Thi Thanh Binh and Le Trong Vinh},
  doi          = {10.1016/j.asoc.2024.111263},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111263},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Subswarm-guided ant colony optimization with enhanced pheromone update mechanism and beam search for VNF placement and routing},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved differential evolution algorithm based
convolutional neural network for emotional analysis of music data.
<em>ASOC</em>, <em>153</em>, 111262. (<a
href="https://doi.org/10.1016/j.asoc.2024.111262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary computation is derived from the simulation of natural selection and genetic processes in biological evolution. This approach provides a method for optimizing the structure and parameters of neural networks . When combined with neural networks, forming what&#39;s termed as evolutionary computation based neural networks, it offers a systematic approach to optimize neural network models in diverse applications. In this study, we introduce a method that employs differential evolution algorithms to optimize parameters of convolutional neural network (CNN) for music emotion recognition tasks. This method optimizes the initial weights of the CNN, aiming to achieve near-global optimal solutions and expedite network convergence. Comparative experiments indicate that the proposed approach effectively identifies optimal parameters and structures for CNN, suggesting potential advancements in automated music emotion recognition.},
  archive      = {J_ASOC},
  author       = {Jiajia Li and Samaneh Soradi-Zeid and Amin Yousefpour and Daohua Pan},
  doi          = {10.1016/j.asoc.2024.111262},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111262},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved differential evolution algorithm based convolutional neural network for emotional analysis of music data},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage greedy algorithm based on crowd sensing for tour
route recommendation. <em>ASOC</em>, <em>153</em>, 111260. (<a
href="https://doi.org/10.1016/j.asoc.2024.111260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the demand for tourism is increasing, but traditional tour group routes have been unable to meet individual needs. This paper proposes a novel personalized recommendation method for tour routes based on crowd sensing. First, we utilize ArcMap to model a real road network . We then propose a novel scoring mechanism for points of interest, including an interest label matching score and crowd sensing score, and implement a user-personalized multi-constraint interest model. Based on whether a user has must-see scenic spots, we propose a variable neighborhood greedy tour recommendation algorithm for users with no must-see scenic spots and a single-multiple point of interest two-stage greedy tour route recommendation algorithm for users with must-see scenic spots. We collected real data regarding 200 attractions, 881 restaurants, 570 hotels and 28 mature travel routes in Beijing from Ctrip, Dianping and Tuniu. We perform case analysis on Beijing dataset and comparative experiments on Beijing and public datasets with the existing algorithms. The experimental results demonstrate that our algorithm has superior rationality and efficiency.},
  archive      = {J_ASOC},
  author       = {Xiaoyao Zheng and Hao You and He Huang and Liping Sun and Qingying Yu and Yonglong Luo},
  doi          = {10.1016/j.asoc.2024.111260},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111260},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Two-stage greedy algorithm based on crowd sensing for tour route recommendation},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A deep reinforcement learning-based active suspension
control algorithm considering deterministic experience tracing for
autonomous vehicle. <em>ASOC</em>, <em>153</em>, 111259. (<a
href="https://doi.org/10.1016/j.asoc.2024.111259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the challenges in autonomous driving become more complex and changing, traditional methods are struggling to cope. As a result, artificial intelligence (AI) techniques have gained widespread attention due to their potential in addressing these challenges. To investigate the application and performance of deep reinforcement learning (DRL) techniques in vertical control of autonomous vehicles, we propose an active suspension control algorithm that incorporates deterministic experience tracing (DET). The agent explores and learns deterministic policies by interacting with the environment and continuously exploring and exploiting the generated data. During this process, DET stores state and action data in a separate experience memory buffer over time. Additionally, DET processes this information into auxiliary rewards that decay based on temporal logic. This drives the agent to self-iterate and rapidly improve. DET allows AI techniques to incorporate temporal robustness into data-driven learning, resulting in improved generalization performance and optimized ride comfort in engineering applications . Simulation results demonstrated that DET improved control performance by 74.92%, 64.20%, and 54.64% compared to the deep deterministic policy gradient (DDPG), twin delayed deep deterministic policy gradient (TD3), and model predictive control (MPC) baselines, respectively. Furthermore, it achieved nearly a 90% improvement in ride comfort on random roads in classes A, B, and C across different speeds. Even on class D roads, the optimization remained around 85%, demonstrating its excellent generalization performance.},
  archive      = {J_ASOC},
  author       = {Cheng Wang and Xiaoxian Cui and Shijie Zhao and Xinran Zhou and Yaqi Song and Yang Wang and Konghui Guo},
  doi          = {10.1016/j.asoc.2024.111259},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111259},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep reinforcement learning-based active suspension control algorithm considering deterministic experience tracing for autonomous vehicle},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An echo state network model with the protein structure for
time series prediction. <em>ASOC</em>, <em>153</em>, 111257. (<a
href="https://doi.org/10.1016/j.asoc.2024.111257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the perspective of bionics of biological structure, this paper proposes a new reservoir topology structure with an α α -helix form of the secondary protein, named S-ESN. This network model has some advantages compared with the standard leaky-echo state network (Leaky-ESN) model. Because the neurons in the traditional reservoir are randomly and sparsely connected, the stability of the echo state network (ESN) will be reduced, and the prediction accuracy will also be decreased. The S-ESN model proposed greatly improves the internal stability of the reservoir, the dynamic activity of neurons and the prediction accuracy of the ESN . At the same time, the improved moth-flame optimization algorithm (MFO) with the probability of jump disturbance is used to optimize the three parameters: the leakage rate ( a ) (a) , the spectral radius ( ρ ) (ρ) , and the input scaling factor ( s i n ) (sin) , which can further improve the stability and predictability of the S-ESN. In order to verify the performance of S-ESN, three virtual time series Sin time series with low frequency, Sin time series with high frequency, Mackey-Glass time series (MG) and one practical Sunspot are selected as experimental data. The experimental results show that the S-ESN model has better prediction accuracy.},
  archive      = {J_ASOC},
  author       = {Yuanpeng Gong and Shuxian Lun and Ming Li and Xiaodong Lu},
  doi          = {10.1016/j.asoc.2024.111257},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111257},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An echo state network model with the protein structure for time series prediction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning applied to statistical arbitrage
investment strategy on cryptomarket. <em>ASOC</em>, <em>153</em>,
111255. (<a href="https://doi.org/10.1016/j.asoc.2024.111255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the complex and dynamic nature of financial markets where increasingly sophisticated investment strategies are required, deep reinforcement learning (DRL) has proven successful in generating real-time investment strategies, outperforming classical models. Alongside this, statistical arbitrage strategies exploit temporary market inefficiencies to generate returns. In this regard, a novel DRL-based arbitrage method has been developed. This paper presents a unified framework that combines classical statistical arbitrage theory with DRL framework to generate investment strategies. The framework addresses the challenges of identifying similar asset portfolios, extracting signals indicating temporary price deviations, and determining optimal trading rules given market conditions. The proposed methodology involves constructing arbitrage portfolios based on cointegration relationships, removing signals from price series and portfolios, and using a DRL agent to make optimal decisions within a fixed time horizon. The empirical analysis focuses on the cryptocurrency market, known for its volatility and risk. Results demonstrate that DRL agents can generate strategies with positive returns in out-of-sample periods 79.52% to 112.82% with no transaction cost, outperforming market benchmark Bitcoin 32.51% return, the best performing over the period Litecoin with 57.11% return and the worst performing Solana with a 35.70% loss. Moreover, these strategies effectively reduce risk, achieving higher risk-adjusted returns on individual assets. The strategies maintain positive returns when considering transaction costs, with the DRL agent outperforming the standard arbitrage strategy. The best-performing strategy is based on a Deep Q-Network (DQN) agent with a return of 18.39%, annualized volatility of 12.22%, and an annualized Sharpe ratio of 2.43. At the same time, Bitcoin holds an annualized volatility of 44.13% and a Sharpe ratio of 1.08. Their randomness and coherence are studied to verify the robustness of the agents’ decisions. The actions generated by the agents are not random but based on well-founded policies, which align with the obtained results.},
  archive      = {J_ASOC},
  author       = {Gabriel Vergara and Werner Kristjanpoller},
  doi          = {10.1016/j.asoc.2024.111255},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111255},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning applied to statistical arbitrage investment strategy on cryptomarket},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feedback-based artificial bee colony algorithm for
energy-efficient flexible flow shop scheduling problem with batch
processing machines. <em>ASOC</em>, <em>153</em>, 111254. (<a
href="https://doi.org/10.1016/j.asoc.2024.111254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy-efficient flexible flow shop scheduling problem (EFFSP) and parallel batch processing machines (BPM) scheduling problem have been extensively studied; however, as the integration of EFFSP and parallel BPM scheduling problem, EFFSP with BPM is seldom investigated even though it often exists in many energy-intensive manufacturing processes such as casting and steelmaking. In this study, EFFSP with BPM at a middle stage in hot &amp; cold casting process is considered, and a feedback-based artificial bee colony (FABC) algorithm is proposed to simultaneously minimize makespan, total tardiness, and total energy consumption . A new cooperation between two employed bee swarms and three search cases are implemented, which are employed bee phase and onlooker bee phase or employed bee phase and cooperation or employed bee phase. Feedback mechanism is established by evolution quality of employed bee swarms and adaptive threshold and used to dynamically determine which search case is executed on each generation. New scout phase and recombination operator are also applied. Extensive experiments are conducted and the computational results demonstrated that new strategies such as cooperation are effective and FABC has promising advantages in the considered EFFSP.},
  archive      = {J_ASOC},
  author       = {Jing Wang and Hongtao Tang and Deming Lei},
  doi          = {10.1016/j.asoc.2024.111254},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111254},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feedback-based artificial bee colony algorithm for energy-efficient flexible flow shop scheduling problem with batch processing machines},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards universal and sparse adversarial examples for visual
object tracking. <em>ASOC</em>, <em>153</em>, 111252. (<a
href="https://doi.org/10.1016/j.asoc.2024.111252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attack is aimed to add small perturbations to the model that are imperceptible to humans, resulting in incorrect outputs with high confidence. Currently, adversarial attack mainly focuses on image classification and object detection tasks, but are insufficient in visual tracking. Nevertheless, existing attack methods for object tracking are limited to Siamese networks , with other types of trackers being infrequently targeted. In order to expand the usage of adversarial attacks in object tracking, we propose a model-free black-box framework for learning to generate universal and sparse adversarial examples (USAE) for tracking task. To this end, we first randomly add a noisy patch to any interference image, and then apply standard projected gradient descent to optimize the generation process of adversarial examples which is subjected to a similarity constraint with original images, making its embedding feature closer to the patched interference image in l 2 l2 -norm. Consequently, there is no significant difference between adversarial images and original images for human vision, but leading to tracking failure. Furthermore, our method just attacks 50 original images with adversarial images in each sequence, rather than an entire dataset. Numerous experiments on VOT2018, OTB2013, OTB2015, and GOT-10k datasets verify the effectiveness of USAE attack. Specifically, the number of lost reaches 1180 on VOT2018, the precision of OTB2015 decreased by 42.1%, and the success rate of GOT-10k is reduced to 1.8%, which shows the attack effect is remarkable. Moreover, USAE has a good transferability among various trackers like SiamRPN++, ATOM, DiMP, KYS, and ToMP. Notice that the proposed method is black-box and applicable to most realistic scenarios.},
  archive      = {J_ASOC},
  author       = {Jingjing Sheng and Dawei Zhang and Jianxin Chen and Xin Xiao and Zhonglong Zheng},
  doi          = {10.1016/j.asoc.2024.111252},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111252},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Towards universal and sparse adversarial examples for visual object tracking},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strong (α, k)-cut and computational-based segmentation based
novel hesitant fuzzy time series forecasting model. <em>ASOC</em>,
<em>153</em>, 111251. (<a
href="https://doi.org/10.1016/j.asoc.2024.111251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hesitant fuzzy set (HFS) is an ideal tool to deal with hesitancy situation arises due to collection of membership values for each time series datum . Many researchers developed HFS based fuzzy time series forecasting (FTSF) models in past years. In this article, a novel method is proposed to deal with critical factors affecting the HFS based FTSF and name as strong ( α , k ) (α,k) -cut and computational-based segmentation based hesitant fuzzy time series forecasting (SCS-FTSF). Computational-based segmentation (CBS) approach is developed to determination of number of intervals and generating intervals. Proposed method uses Gaussian and triangular membership functions to construct HFS and uses aggregation operator aggregating the membership values to construct aggregate HFS. A novel fuzzification procedure is proposed by taking all aggregate HFSs with non-zero aggregated membership value relative to data points. Strong ( α , k ) (α,k) -cut is employed to selection of suitable aggregate HFLRs. Further, a defuzzification approach is also proposed to obtaining numerical value . In order to assess the performance of proposed SCS -FTSF method two time series datasets are used. Results of error measures and validation tests confirms that superiority of the proposed SCS-FTSF method.},
  archive      = {J_ASOC},
  author       = {Manish Pant and Nisha Mehra},
  doi          = {10.1016/j.asoc.2024.111251},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {111251},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strong (α, k)-cut and computational-based segmentation based novel hesitant fuzzy time series forecasting model},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-linear channel equalization using modified grasshopper
optimization algorithm. <em>ASOC</em>, <em>153</em>, 110091. (<a
href="https://doi.org/10.1016/j.asoc.2023.110091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a modified grasshopper optimization algorithm is proposed for equalization of non-linear wireless communication channels. Even though grasshopper optimisation algorithm (GOA) is an efficient algorithm, it gets trapped into local optima after some iterations due to loss of swarm diversity. Furthermore, GOA does not involve any provision to retain the elite grasshoppers found so far at each index which weakens the exploitation capability and convergence rate of GOA. These limitations of GOA are alleviated in this paper by incorporating three key modifications into GOA. A threshold parameter is introduced to detect the inefficient search region. Lévy Flight is integrated with the basic GOA to improve the diversity of grasshopper swarm and the greedy selection operator is used to preserve the elite grasshoppers found so far at every index. The superiority of a modified grasshopper optimization algorithm (MGOA) is illustrated over the existing metaheuristic algorithms . The key parameters of MGOA are selected by performing the sensitivity analysis. The simulation results on four non-linear channels demonstrate the equalization capability of the proposed MGOA in terms of MSE and BER performance. A statistical validity of results provided by MGOA is confirmed through Wilcoxon rank-sum test.},
  archive      = {J_ASOC},
  author       = {Kishor Kisan Ingle and Ravi Kumar Jatoth},
  doi          = {10.1016/j.asoc.2023.110091},
  journal      = {Applied Soft Computing},
  month        = {3},
  pages        = {110091},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Non-linear channel equalization using modified grasshopper optimization algorithm},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multilevel pooling scheme in convolutional neural networks
for texture image recognition. <em>ASOC</em>, <em>152</em>, 111282. (<a
href="https://doi.org/10.1016/j.asoc.2024.111282">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have shown successful results in image classification achieving real-time results superior to the human level. However, texture images still pose some challenge to these models due, for example, to the limited availability of data for training in several problems where these images appear, high inter-class similarity, the absence of a global viewpoint of the object represented, and others. In this context, the present study is focused on improving the accuracy of convolutional neural networks in texture classification . This is done by a hierarchical application of deep filter bank modules combined with Fisher vector pooling. Mid-level local features are extracted from earlier convolutional layers of a pre-trained backbone and combined with high level ones from the last convolutional layer. All local features are treated as equally important and aggregated into a single set of features used for pooling by Fisher vectors. No fine tuning is necessary. The rationale behind this approach is obtaining information that is less domain specific. We verify the effectiveness of our method in texture classification of benchmark databases, as well as on a practical task of Brazilian plant species identification. In both scenarios, Fisher vectors calculated on multiple layers show competitive results with state-of-the-art methods, confirming that early convolutional layers provide important information about the texture image for classification.},
  archive      = {J_ASOC},
  author       = {Lucas O. Lyra and Antonio E. Fabris and Joao B. Florindo},
  doi          = {10.1016/j.asoc.2024.111282},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111282},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multilevel pooling scheme in convolutional neural networks for texture image recognition},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A particle swarm optimization algorithm based on modified
crowding distance for multimodal multi-objective problems.
<em>ASOC</em>, <em>152</em>, 111280. (<a
href="https://doi.org/10.1016/j.asoc.2024.111280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal multi-objective optimization problems (MMOPs) are commonly encountered in practice. The difficulty in solving MMOPs is obtaining all of the Pareto optimal sets without degrading the performance of the Pareto optimal front . To address this challenge, this study proposes a particle swarm optimization algorithm based on modified crowding distance (MOPSO_MCD). In MOPSO_MCD, a modified method for calculating the crowding distance (MCD) is devised, which allows for a more comprehensive assessment of the crowding relationship between individuals in the decision space and the objective space . Moreover, a cosine similarity-based elite selection mechanism is designed to identify the neighborhood optimal individuals of the individuals in the population and improve the decision space diversity. Additionally, an offspring competition mechanism is proposed to keep the population from trapping in the local optimum and enhance the global search ability of the MOPSO_MCD algorithm. Experimental results and statistical analysis show that MOPSO_MCD performs better than the other comparison algorithms on sixteen test functions and a map-based practical problem.},
  archive      = {J_ASOC},
  author       = {Da Feng and Yan Li and Jianchang Liu and Yuanchao Liu},
  doi          = {10.1016/j.asoc.2024.111280},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111280},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A particle swarm optimization algorithm based on modified crowding distance for multimodal multi-objective problems},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Optimal scale selection approach for classification based
on generalized multi-scale formal context. <em>ASOC</em>, <em>152</em>,
111277. (<a href="https://doi.org/10.1016/j.asoc.2024.111277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of multi-scale data is an important research topic in granular computing . Its research goal is to determine the most appropriate scale and achieve better classification performance. However, determining the optimal scale is often a difficult problem due to lacking better metrics and optimization methods. In order to solve this problem, this paper proposes the optimal scale selection criteria for generalized multi-scale formal contexts. That is, the optimal scale uses the coarsest conditional attributes and finest decision attributes to optimize the combination of granularities of attributes. We combine these criteria with multi-objective optimization methods for developing an algorithm to fast compute the optimal scale. Experiments show that for the selected 14 data sets and 11 comparative classification methods, there are 9 classification methods with higher classification accuracies on more than 9 data sets. Therefore, the optimal scale selection method proposed in this paper is feasible and can effectively improve the performance of the classification method.},
  archive      = {J_ASOC},
  author       = {Fei Wang and Jinhai Li and Chongchong Yu},
  doi          = {10.1016/j.asoc.2024.111277},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111277},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimal scale selection approach for classification based on generalized multi-scale formal context},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online hierarchical streaming feature selection based on
adaptive neighborhood rough set. <em>ASOC</em>, <em>152</em>, 111276.
(<a href="https://doi.org/10.1016/j.asoc.2024.111276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of open machine learning , a kind of data is accompanied by a hierarchical structure between classes in the label space and the increasing number of features. Therefore, hierarchical classification learning in dynamic changing feature spaces remains an essential research challenge. To address this challenge, we propose an online hierarchical streaming feature selection algorithm based on adaptive neighborhood rough set in this paper, which effectively selects closely interactive features in high-dimensional data with a hierarchical structure. First, a subtree instance set is obtained for each internal node based on the parent–child relationship of the hierarchical structure. Then, an adaptive neighborhood rough set is constructed, and the neighborhood granularity of each instance is auto selected according to the subtree instance set of its parent node . Moreover, an online streaming feature selection framework for hierarchical data is proposed, in which, the streaming feature is evaluated by three steps: online significance analysis, online relevance analysis, and online redundancy analysis . Finally, experiments are conducted on seven hierarchical datasets to evaluate the performance of the proposed algorithm, and extensive results demonstrate that the proposed algorithm outperforms other state-of-the-art online streaming feature selection algorithms.},
  archive      = {J_ASOC},
  author       = {Tongxin Shu and Yaojin Lin and Lei Guo},
  doi          = {10.1016/j.asoc.2024.111276},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111276},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Online hierarchical streaming feature selection based on adaptive neighborhood rough set},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive learning on hierarchical data streams using
window-weighted gaussian probabilities. <em>ASOC</em>, <em>152</em>,
111271. (<a href="https://doi.org/10.1016/j.asoc.2024.111271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hierarchical data stream classification task addresses challenges in both hierarchical and data stream classification primary areas. In these scenarios, machine learning models must simultaneously deal with class hierarchies and adapt to respond to nonstationary data. Given such a challenging set of traits, existing techniques are deficient, as they perform incremental learning and are slow to adapt to newer data, thus not capturing their dynamics in a timely fashion . In this study, we propose two novel adaptive Gaussian Naive Bayes classifiers tailored to classify hierarchical data streams. The models use window-weighted Gaussian probabilities to consider current and historical data and improve the adaptability of the classifiers, especially for nonstationary data streams. As a result of our research, we introduce a unified protocol for evaluating and comparing hierarchical data stream classifiers and establish a benchmark for the hierarchical data stream classification task encompassing the proposed methods and state-of-the-art classifiers. The results demonstrate that our proposed algorithms achieve better prediction correctness than their state-of-the-art counterparts while responding more swiftly to changes in data distribution .},
  archive      = {J_ASOC},
  author       = {Eduardo Tieppo and Júlio Cesar Nievola and Jean Paul Barddal},
  doi          = {10.1016/j.asoc.2024.111271},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111271},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive learning on hierarchical data streams using window-weighted gaussian probabilities},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interval-valued fermatean fuzzy based risk assessment for
self-driving vehicles. <em>ASOC</em>, <em>152</em>, 111265. (<a
href="https://doi.org/10.1016/j.asoc.2024.111265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision-making(DM) processes used by autonomous vehicle driving systems are separate from those of the users, allowing them to oversee and regulate the operations of the cars in expected and unforeseen circumstances. Although there are several advantages to using this technology, such as fewer accidents caused by human error and more efficient energy utilization, it is also evident that there are some risks involved. Hence, developing a risk assessment application for these systems will be advantageous given the hazards associated with autonomous cars and driving systems that must be tested and addressed. In this study, a new integrated FF-based MCDM methodology combining the Analytic Hierarchy Process(AHP), Technique for Order Preference by Similarity to Ideal Solution(TOPSIS), and Multi-Attributive Border Approximation Area Comparison (MABAC) methods is proposed as a new security model that will help decision-makers address the physical design and attack risks of autonomous vehicles, estimate their uncertainty, and control cyber risks Interval-valued Fuzzy Fermatean sets ten possibilities for autonomous vehicle driving systems assessed in the application based on six main criteria and fifteen sub-criteria. Comparative and sensitivity studies have also been used to demonstrate the adaptability, validity, and verification of the suggested approach and the sensitivity of the decisions made. Possible implications from a theoretical, managerial, and policy framework have been examined based on the application findings and studies that have been done.},
  archive      = {J_ASOC},
  author       = {Murat Kirişci},
  doi          = {10.1016/j.asoc.2024.111265},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111265},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval-valued fermatean fuzzy based risk assessment for self-driving vehicles},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An oscillatory particle swarm optimization feature selection
algorithm for hybrid data based on mutual information entropy.
<em>ASOC</em>, <em>152</em>, 111261. (<a
href="https://doi.org/10.1016/j.asoc.2024.111261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid data lead to overfitting in machine learning models, which may reduce the accuracy of classification. Feature selection can not only reduce the computational cost of processing hybrid data but also improve the accuracy of classification. The particle swarm optimization (PSO) algorithm has clear advantages in feature selection. This paper presents an oscillatory particle swarm optimization feature selection algorithm for hybrid data based on mutual information entropy. First, a new distance function on the object set of a hybrid information system (HIS) is built, which yields a tolerance relation on this object set. Then, mutual information entropy is presented to measure the uncertainty of the HIS. On this basis, the maximum-relevance and minimal-redundancy model (MRMR model) for the HIS is proposed. Based on the MRMR model, a feature selection algorithm (denoted as MRMR) for the HIS is naturally designed. As the integration of the MRMR model and PSO can effectively explore all possible feature subsets, an oscillatory particle swarm optimization algorithm based on the MRMR model (denoted as OPSO-MRMR) for the HIS is also designed. Moreover, the MRMR model is utilized to define a fitness function that evaluates the quality of particles. The particle position update process is modified by means of a two-order oscillatory equation. Finally, an experimental analysis is conducted to compare the two designed algorithms with five other algorithms. The statistical analysis of classification accuracy and F1 score shows that OPSO-MRMR improves precision by 5.8% and 10.7% compared to the other six algorithms, respectively.},
  archive      = {J_ASOC},
  author       = {Jiali He and Liangdong Qu and Pei Wang and Zhaowen Li},
  doi          = {10.1016/j.asoc.2024.111261},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111261},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An oscillatory particle swarm optimization feature selection algorithm for hybrid data based on mutual information entropy},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary algorithms for solving single- and
multiple-objective political redistricting problems: The case study of
poland. <em>ASOC</em>, <em>152</em>, 111258. (<a
href="https://doi.org/10.1016/j.asoc.2024.111258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose novel evolutionary algorithms for solving single- and multi-objective political redistricting problems. The objectives include population equality, compactness of districts, deviation from the current districting, and an expected number of mandates attainable by some parties. The former two ensure the constructed solutions are reasonable, while the latter pair is meaningful for the post-analysis on how the alternation of existing districts may affect election outcomes. We operate on data concerning geography, demography, and politics in Poland. The experiments reveal that our algorithms efficiently handle the four-objective variant of the problem. In a single test run, we evaluate around one million solutions in nearly two hours on an average class computer, which is satisfactory given the problem’s complexity. The methods construct high-quality non-dominated solutions, outperforming the current districting and revealing the trade-offs between the objectives. The post-analysis allows us to observe connections between the expected number of mandates and the remaining three objectives. Specifically, attaining a greater number of mandates requires more significant changes in delineating the districts and potential violations of constraints. We also exhibit that the space for possible political manipulations increases when more districts can be determined.},
  archive      = {J_ASOC},
  author       = {Michał K. Tomczyk and Miłosz Kadziński},
  doi          = {10.1016/j.asoc.2024.111258},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111258},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary algorithms for solving single- and multiple-objective political redistricting problems: The case study of poland},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strategic outsourcing in reverse logistics: Neutrosophic
integrated approach with a hierarchical and interactive quality function
deployment. <em>ASOC</em>, <em>152</em>, 111256. (<a
href="https://doi.org/10.1016/j.asoc.2024.111256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of strategic outsourcing in reverse logistics (RL) depends on stakeholder requirements (SRs) directing organization development and experts’ knowledge decomposing and reconstructing strategic decision, which urges the adoption of hierarchical and interactive quality function deployment (QFD). Considering the uncertainty within a QFD, single valued neutrosophic number (SVNN) is introduced to capture both indeterminacy and inconsistency hidden in quantitive votes and qualitative judgments of stakeholders and experts. With unknown weight assignments , two maximum deviation models are constructed to obtain the priority of SRs and Shapley weights of experts. An SVNN-enabled grey relational analysis is extended to identify the interdependence priority of engineer characterizers (ECs). Due to the interactions among components such as experts in a group or ECs in QFD, two information aggregation tools under neutrosophic environments are presented based on fuzzy integral delineating the positive or negative effect. Built on these, we propose an integrated neutrosophic approach composed of a transformation module, weight module, integration module and QFD analysis module to support the outsourcing decision. Finally, an illustration example is used to confirm the practicality of the proposed approach. Sensitivity analysis and comparative analysis with multiple views have been conducted to show the flexibility and superiority of the given decision.},
  archive      = {J_ASOC},
  author       = {Yu Yang and Zhang peng Tian and Jun Lin},
  doi          = {10.1016/j.asoc.2024.111256},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111256},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Strategic outsourcing in reverse logistics: Neutrosophic integrated approach with a hierarchical and interactive quality function deployment},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based fusion networks with high-order
attention mechanism for 3D object detection in autonomous driving
scenarios. <em>ASOC</em>, <em>152</em>, 111253. (<a
href="https://doi.org/10.1016/j.asoc.2024.111253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of autonomous driving , accurately detecting 3D objects is both vital and challenging. Recently, deep convolutional networks have been successfully implemented in the fusion of LiDAR and camera data, delivering impressive results. However, prevailing approaches tend to concentrate on basic architectural designs and the use of fixed 3D bounding boxes , overlooking the exploration of feature interrelations and the varying scales of 3D objects. In this paper, we propose High-order Attention Mechanism Fusion Networks (HAMFNs) for image expression and multi-scale learning, based on a novel high-order attention mechanism with multi-scale detection and scale linear regression. High-order convolution layers are built for tenser filtering with discriminative representations of the holistic image. Multi-scale query module further characterizes the saliency properties of the 3D objects. Our tests on the nuScenes dataset show that HAMFNs outperform the latest top-performing methods, achieving a 0.7% increase in mean Average Precision (mAP). We further integrated high-order convolutional layers into ResNet-50, ResNet-101, and ResNet-152 architectures, enhancing their performance with minimal parameter increase. The Top-1 error rates were reduced by 1.65%, 1.63%, and 1.60% for each network, respectively.},
  archive      = {J_ASOC},
  author       = {Haiyang Jiang and Yuanyao Lu and Duona Zhang and Yuntao Shi and Jingxuan Wang},
  doi          = {10.1016/j.asoc.2024.111253},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111253},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep learning-based fusion networks with high-order attention mechanism for 3D object detection in autonomous driving scenarios},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of self-improving q-learning controller for a
class of dynamical processes: Implementation aspects. <em>ASOC</em>,
<em>152</em>, 111250. (<a
href="https://doi.org/10.1016/j.asoc.2024.111250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with a practical application of the Q-learning algorithm as a general-purpose self-improving controller operating in a class of industrial closed-loop control systems. The proposed approach solves a practical control engineering problem by designing a controller that starts its operation with the predefined closed-loop control performance and then, without unnecessarily disturbing the normally operated closed-loop system, it learns on-line from interactions with the controlled process to ensure the gradual improvement in the closed loop control performance until this performance reaches a desired user-defined level. It is proposed how to ensure its initial performance by an appropriate initialization of the Q-matrix without assuming any knowledge about the process model. The desired target closed-loop performance is defined by the first-order reference trajectory that should be preserved by the closed-loop system, which requires determining the current state of the closed-loop system by the values of the control error and its time derivative. The novelty of the proposed approach results from: (i) preserving the first order reference trajectory with reduced dimensions of the Q-matrix to two, which significantly reduces memory and computational requirements without any loss of generality, (ii) proposing a method for a reduction of the number of states designed directly for the proposed approach, and (iii) proposing a method for a very convenient initialization of the Q-matrix based only on the tunings of the existing PI controller. All these features bring the proposed approach closer towards practical applications. Results of the simulation and experimental validation show that the proposed Q-learning controller can substitute the existing PI controller bumplessly and provides a gradual improvement in the closed loop performance due to its online learning abilities.},
  archive      = {J_ASOC},
  author       = {Jakub Musial and Krzysztof Stebel and Jacek Czeczot and Pawel Nowak and Bogdan Gabrys},
  doi          = {10.1016/j.asoc.2024.111250},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111250},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of self-improving Q-learning controller for a class of dynamical processes: Implementation aspects},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient reinforcement learning scheme for the
confinement escape problem. <em>ASOC</em>, <em>152</em>, 111248. (<a
href="https://doi.org/10.1016/j.asoc.2024.111248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crucial real-world problems in robotics like trajectory planning during convoy missions and autonomous rescue missions can be framed as a confinement escape problem (CEP) (a type of pursuit-evasion game). In a typical CEP, an evader attempts to escape a confinement region by sequentially making decisions to plan an escape while the region is patrolled by multiple smart pursuers. The evader has a limited sensing range and does not know the total number of pursuers and their pursuit strategies making it difficult to model the environment and obtain a generalizable escape strategy. In this paper, the CEP is formulated in a reinforcement learning (RL) framework to overcome the above difficulties. The state function is designed independent of the total number of pursuers and the shape of the confinement region thereby making the RL approach scalable. To handle training consistency issues in deep RL and convergence issues due to sparse rewards, a Scaffolding Reflection based Reinforcement Learning (SR2L) approach is presented in this paper where the SR2L employs an actor–critic method with a motion planner scaffold to accelerate its training speed. Performance evaluation of SR2L shows that it trains twice as fast compared to other existing state-of-the-art actor–critic RL methods. Performance results show that the convergence of SR2L is more consistent than the corresponding conventional actor–critic RL methods and interactive reinforcement learning methods. Monte-Carlo simulation results show that SR2L outperforms other conventional RL methods and the motion planner with at least 28% and 10% faster escape times respectively while having the lowest variance in escape times against different pursuit strategies. Ablation studies done by changing different environmental parameters clearly show the scalability and generalizability of the proposed SR2L approach.},
  archive      = {J_ASOC},
  author       = {Vignesh Gurumurthy and Nishant Mohanty and Suresh Sundaram and Narasimhan Sundararajan},
  doi          = {10.1016/j.asoc.2024.111248},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111248},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient reinforcement learning scheme for the confinement escape problem},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective fitness dependent optimizer for workflow
scheduling. <em>ASOC</em>, <em>152</em>, 111247. (<a
href="https://doi.org/10.1016/j.asoc.2024.111247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Workflow scheduling is a significant challenge due to the large scale of workflows and heterogeneity of cloud resources. The vast size of the cloud makes execution times higher, leading to high computational and communication costs. Workflow scheduling is an N P NP -hard problem, thus, creating meta-heuristic algorithms is one of the best options for finding optimal solutions. This paper models workflow scheduling as a multi-objective optimization problem that considers execution time and communication cost. Optimization efforts are accomplished by proposing a Fitness-Dependent Optimizer (FDO) inspired by bee reproductive behavior. However, it has many drawbacks, including being a single-objective problem. To improve this, we present a Genetic Algorithm-based multi-objective FDO, eliminating many of the previous algorithm’s issues. The proposed algorithm takes advantage of both the Genetic Algorithm and FDO. Moreover, it does not show signs of sticking to a local optimal solution . The proposed algorithm is compared with the Genetic Algorithm (GA), Particle Swarm Optimization (PSO), GA-PSO, and FDO, where it shows its effectiveness by performing better on both parameters.},
  archive      = {J_ASOC},
  author       = {Sugandha Rathi and Renuka Nagpal and Gautam Srivastava and Deepti Mehrotra},
  doi          = {10.1016/j.asoc.2024.111247},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111247},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective fitness dependent optimizer for workflow scheduling},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing the power of transformers and data fusion in
smart irrigation. <em>ASOC</em>, <em>152</em>, 111246. (<a
href="https://doi.org/10.1016/j.asoc.2024.111246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, IoT sensors have enabled smart agriculture to grow rapidly with many compelling real-world applications. One such application is the case of smart irrigation. A particular interest exists in forecasting soil water potential to allow for the establishment of more efficient irrigation systems . Nonetheless, forecasting soil moisture is a complex task and depends on various information sources . Most existing work relies on local approaches, which are less effective at leveraging shared information across different data sources . Therefore, this paper presents a robust global approach for soil water potential forecasting, combining various environmental factors through the cutting-edge Temporal Fusion Transformer. Our proposed approach outperforms established baselines for forecasting soil water potential. As such, this work contributes to the growing body of research on data fusion in real-world applications.},
  archive      = {J_ASOC},
  author       = {Boje Deforce and Bart Baesens and Jan Diels and Estefanía Serral Asensio},
  doi          = {10.1016/j.asoc.2024.111246},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111246},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Harnessing the power of transformers and data fusion in smart irrigation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent strategic bidding in competitive electricity
markets using multi-agent simulation and deep reinforcement learning.
<em>ASOC</em>, <em>152</em>, 111235. (<a
href="https://doi.org/10.1016/j.asoc.2024.111235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the lack of comprehension of agents in Multi-Agent Simulation (MAS) based on classic Reinforcement Learning algorithms of competitive electricity markets, an intelligent strategic bidding method using Deep Reinforcement Learning (DRL) and MAS is proposed in this paper, which not only can provide more intelligent strategies for market participants to maximize their profits, but can enhance the performance of simulation models dealing with high-dimensional continuous data in electricity markets. Firstly, a theoretical framework of intelligent strategic bidding in competitive electricity markets based on MAS and DRL is proposed, and the process of intelligent bidding in electricity markets based on MAS and DRL is described. Then, three MAS models of intelligent strategic bidding are built based on three classic DRL algorithms, including Deep Q-Network (DQN), Double Deep Q-Network (DDQN), and Asynchronous n-step Q-learning (Async n-step QL), and three algorithms’ convergence speed, computational efficiency, and response sensitivity are compared and analyzed. Finally, a novel Improved Async n-step QL (IAsync n-step QL) algorithm is proposed, the MAS model based on the IAsync n-step QL algorithm for intelligent strategic bidding is established. Simulation results show that the model using the novel DRL algorithm is more profitable and responsive than the classic DRL algorithms.},
  archive      = {J_ASOC},
  author       = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
  doi          = {10.1016/j.asoc.2024.111235},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111235},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent strategic bidding in competitive electricity markets using multi-agent simulation and deep reinforcement learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deadlock-free production using dempster–shafer and preset
methods in predictive scheduling for multiagent controlled flexible
manufacturing systems. <em>ASOC</em>, <em>152</em>, 111234. (<a
href="https://doi.org/10.1016/j.asoc.2024.111234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deadlocks can lead to indefinite delays and total production interruption, resulting in industrial losses. The control function is responsible for maintaining deadlock-free production schedules in manufacturing systems that permit product parallelism and resource sharing. Although some predictive scheduling algorithms can set optimal production schedules with parallel tasks for multiple products, they cannot detect or prevent deadlocks. This study proposes two approaches for minimizing the number of deadlocks in predictive production schedules defined by a flexible job-shop scheduling (FJS) algorithm. These approaches were evaluated in a virtual environment operated as a multi-agent controlled flexible manufacturing system (FMS). The first approach adopts the Dempster–Shafer evidence theory with a set acceptability value for the belief function . Agents convert information from different sources in a production system into decision outcomes that indicate the best control case from a case base to drive deadlock-free production. The second approach employs a decision table for product routing in the FMS and three corrective presetting methods to avoid deadlocks in production scheduling. A leaderless consensus is adopted in the proposed approaches to decision processes, ensuring that the shared and parallel activities executed by the multiple agents of the system remain promptly synchronized. In total, 32 FJS schedules containing 10–47 events were tested to evaluate the efficiency of the proposed approach. This paper presents the results of the experiments, an overall evaluation of the approaches, and data on the time complexity of the proposed algorithms. The second approach successfully avoids deadlocks in the FMS; however, both approaches can be adapted to other environments with order-driven production and predictive schedules.},
  archive      = {J_ASOC},
  author       = {Alex Luiz de Sousa and André Schneider de Oliveira},
  doi          = {10.1016/j.asoc.2024.111234},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111234},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deadlock-free production using Dempster–Shafer and preset methods in predictive scheduling for multiagent controlled flexible manufacturing systems},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning and processing framework using fuzzy deep neural
network for trading and portfolio rebalancing. <em>ASOC</em>,
<em>152</em>, 111233. (<a
href="https://doi.org/10.1016/j.asoc.2024.111233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trading strategies are an interesting topic of financial research. Moving Average Convergence Divergence (MACD) indicator is susceptible to performing worse than expected in unstable financial markets. This paper first presents a data-driven Interpretable Fuzzy Deep Neural Network (IFDNN) that provides insight into neural network inferences using fuzzy logic. Fuzzy rules are induced from the inference process of Neural Networks. Next, a learning and processing framework is proposed using IFDNN to detect trend reversals by forecasting look-ahead prices. IFDNN not only learns the drifts and shifts in market patterns, but also provides traders an option to dive into the reasoning behind why Neural Networks predict certain values. Genetic Algorithms are used to optimise trading parameters of the proposed framework. The proposed framework can perform portfolio rebalancing. The effectiveness of the framework is evaluated on three financial market indexes. The whipsaw effects cause frequent entrances and exits from the market. In this paper, a custom percentage oscillator is implemented to avoid this issue. The performances of the proposed framework using f-MACD are compared with those of the vanilla MACD. Two types of Reinforcement Learning models, Advantage Actor Critic and Deep Deterministic Policy Gradient are incorporated into the proposed framework with results compared.},
  archive      = {J_ASOC},
  author       = {Nicole Hui Lin Kan and Qi Cao and Chai Quek},
  doi          = {10.1016/j.asoc.2024.111233},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111233},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Learning and processing framework using fuzzy deep neural network for trading and portfolio rebalancing},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decision variable classification based multi-objective
multifactorial memetic algorithm for multi-objective multi-task
optimization problem. <em>ASOC</em>, <em>152</em>, 111232. (<a
href="https://doi.org/10.1016/j.asoc.2024.111232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task multi-objective optimization problems need to consider the algorithm&#39;s convergence and the population&#39;s diversity. The information transfer of decision variables with different characteristics may harm the effect of knowledge reuse. This paper proposes a novel hybrid multi-objective multifactorial memetic algorithm to address this issue. The proposed variable classification method will classify decision variables into convergence-related and diversity-related decision variables. Only the same type of decision variables in the source and target tasks can transfer information to avoid negative transfer. Different evolutionary operators are adopted according to the characteristics of decision variables during individual recombination. In addition, the proposed algorithm hybridizes the immune algorithm as the global evolutionary operator and the evolutionary gradient search algorithm as the local search operator into the multifactorial framework to enhance the searching ability. Finally, the proposed algorithm is compared with the state-of-the-art multi-objective evolutionary multitasking algorithms. The results of the experiments show that the proposed algorithm can achieve promising performance on the classical and complex multi-task multi-objective benchmark test suites.},
  archive      = {J_ASOC},
  author       = {Zhiwei Xu and Jiafeng Xu and Kai Zhang and Xin Xu and Juanjuan He and Ni Wu},
  doi          = {10.1016/j.asoc.2024.111232},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111232},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decision variable classification based multi-objective multifactorial memetic algorithm for multi-objective multi-task optimization problem},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sentence-level sentiment classification based on
multi-attention bidirectional gated spiking neural p systems.
<em>ASOC</em>, <em>152</em>, 111231. (<a
href="https://doi.org/10.1016/j.asoc.2024.111231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The GSNP model is a new recurrent-like network inspired by nonlinear spiking mechanisms in nonlinear spiking neural P systems. In this study, a novel sentiment classification model MA-BiGSNP is established by using bidirectional GSNP model combined with multi-attention mechanism. BiGSNP, which is created by two GSNP models with opposite directions, is used to capture semantic correlations between word contexts in a sentence. The multi-attention mechanism simulates the variety of relationships between sentences as well as the significance of words in sentences. To evaluate the effectiveness of the proposed MA-BiGSNP model, we perform comparative experiments and ablation experiments on five real datasets and twelve baseline models . The experimental results show that the proposed MA-BiGSNP model is effective for sentiment classification task .},
  archive      = {J_ASOC},
  author       = {Yanping Huang and Xinzhu Bai and Qian Liu and Hong Peng and Qian Yang and Jun Wang},
  doi          = {10.1016/j.asoc.2024.111231},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111231},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Sentence-level sentiment classification based on multi-attention bidirectional gated spiking neural p systems},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An evolving learning style detection approach for online
education using bipartite graph embedding. <em>ASOC</em>, <em>152</em>,
111230. (<a href="https://doi.org/10.1016/j.asoc.2024.111230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly widespread online education recently enabled a new form of teaching, learning, and overall educational outcome. However, the need of personalization is required as learners learn differently and learning follows a one-size-fits-all approach. Learning style is the most used personalization that plays an important role in learning. This learning style is a changeable trait that is influenced by the learner&#39;s behavior based on past experiences . As a result, knowing it over time helps to point learners in the right direction, motivate them, and enhance their learning outcomes . Research done so far does not take into account the changes in the behavior of learners, and the behavioral data is at large volume, making the existing approaches fail to capture and extract the behavior of learners efficiently. Inspired by these constraints, we propose an incremental learning style detection approach for online education with a bipartite graph embedding technique. We first construct a dynamic bipartite graph to represent the incremental interaction between learners and learning resources while learning. Then, introduce a dynamic bipartite graph embedding to learn the low dimensional representation of the constructed graph from the current and previous time. Finally, the low dimensional features are mapped to the selected Felder-Silverman learning style model (FSLSM) dimension each time to identify and group similar learners using the k-means clustering algorithm. The proposed approach can be integrated into different educational systems. Extensive experiments conducted using three datasets from 2015 KDD Cup courses demonstrate the effectiveness of our approach. Average of 93.19%, 95.76%, and 98.48% are achieved across the three datasets in accuracy compared to the existing approaches.},
  archive      = {J_ASOC},
  author       = {Bello Ahmad Muhammad and Chao Qi and Zhenqiang Wu and Hafsa Kabir Ahmad},
  doi          = {10.1016/j.asoc.2024.111230},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111230},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An evolving learning style detection approach for online education using bipartite graph embedding},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent fault identification in sample imbalance
scenarios using robust low-rank matrix classifier with fuzzy weighting
factor. <em>ASOC</em>, <em>152</em>, 111229. (<a
href="https://doi.org/10.1016/j.asoc.2024.111229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank matrix learning techniques, especially support matrix machine (SMM) approach, have significantly altered mechanical fault diagnosis by efficiently uncovering correlations within matrix-form data. However, SMM exhibits limitations to eliminate the influence of redundant features and struggles with imbalanced classification scenarios, hampering the precise creation of SMM-based prediction model. To overcome these challenges, this paper introduces an innovative algorithm termed robust low-rank matrix classifier with fuzzy weighting factor (RLRMC-FWF). The core innovation of RLRMC-FWF lies in the integration of an inventive grouping elastic net within its framework. This novel inclusion empowers the control of essential attributes such as low-rank, sparsity , and grouping effects. Particularly noteworthy is grouping effects can autonomously identify relevant features without limitation of sample number. This greatly improving RLRMC-FWF’s ability to extract information from minority class samples in imbalanced data . Furthermore, RLRMC-FWF introduces the incorporation of a fuzzy weighting factor, enabling the assignment of distinct weights to samples originating from different classes. This weighting mechanism refines the treatment of diverse sample classes, enhancing the adaptability and performance of RLRMC-FWF model. The effectiveness of the proposed method is validated through experimentation on two fault datasets associated with roller bearings. The experimental findings unequivocally highlight the exceptional performance of RLRMC-FWF within the domain of roller bearing fault diagnosis.},
  archive      = {J_ASOC},
  author       = {Haifeng Xu and Haiyang Pan and Jinde Zheng and Jinyu Tong and Feibin Zhang and Fulei Chu},
  doi          = {10.1016/j.asoc.2024.111229},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111229},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Intelligent fault identification in sample imbalance scenarios using robust low-rank matrix classifier with fuzzy weighting factor},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic three-way multi-criteria decision making with basic
uncertain linguistic information: A case study in product ranking.
<em>ASOC</em>, <em>152</em>, 111228. (<a
href="https://doi.org/10.1016/j.asoc.2024.111228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of basic uncertain linguistic information (BULI) is proposed as an extension of basic uncertain information to enhance the measurement of data quality in decision-making processes by allowing for more flexible utilization of degrees of certainty. This paper seeks to investigate a new model of dynamic three-way decisions and develop an integrated framework for BULI-based multi-criteria decision-making using decision-theoretic rough sets (DTRSs). Firstly, we present the BULI-input dynamic ELECTRE-I method, which is capable of establishing an outranking relation. Then, a novel model of BULI decision-theoretic rough sets (BULIDTRSs) is proposed. The BULI similarity measure is employed to accurately estimate the conditional probability . Following this, we propose four potential approaches for combining loss functions by introducing the BULI ordered weighted averaging operator , which incorporates the consideration of decision risk. The derivation of the three-way decision rules is based on the comparison of BULI and the application of operational laws. In addition, the resolution of various issues such as the determination of criteria weights, identification of ideal solutions, and conversion of ratings to BULI is achieved through the development of a multi-criteria keyword frequency statistical method and a user credibility model. Subsequently, the algorithm for the dynamic three-way multi-criteria decision-making method, aimed at resolving product ranking issues, is presented. Eventually, we elaborate on the utilization of the proposed methodology through the illustration of a case study involving the ranking of passenger cars.},
  archive      = {J_ASOC},
  author       = {Yi Yang and Meng-Qi Jie and Zhen-Song Chen},
  doi          = {10.1016/j.asoc.2024.111228},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111228},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic three-way multi-criteria decision making with basic uncertain linguistic information: A case study in product ranking},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Type2 soft biclustering framework for alzheimer microarray.
<em>ASOC</em>, <em>152</em>, 111227. (<a
href="https://doi.org/10.1016/j.asoc.2024.111227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microarray technology is a powerful tool that enables simultaneous analysis of the expression level of a large number of genes for different samples. Reliable information on gene expression level is much needed in the health system as it is widely used to predict, diagnose, and treat human diseases (e.g., Alzheimer&#39;s). For the analysis of the microarray dataset, biclustering is known to be a highly capable approach, however some characteristics of the dataset including high dimensionality , noise, uncertainty, and complex biological processes need to be handled properly. Concerning these characteristics, the current paper proposes a novel two-stage biclustering framework based on soft clustering and a metaheuristic technique. The integration of the two stages ensures a reliable search process to find similar expression patterns concerning gene expression characteristics. The proposed framework employs fuzzy and possibilistic clustering along with Type2-Fuzzy Sets theory to handle high-level uncertainty, noise, and outliers in microarray datasets. Considering the NP-hard nature of the biclustering method, the proposed framework incorporates the Genetic Algorithm with a unique chromosome representation , fitness function, and modification mechanisms. Real microarray datasets of Alzheimer&#39;s Disease have been used to evaluate the proposed framework. The comparative analysis of different versions of our proposed framework and some well-known biclustering methods demonstrates that the proposed framework is superior in terms of some indices including the mean squared residual and variance indices. The final results are further evaluated using the defined fitness function, which indicates the better performance of our possibilistic-based biclustering methods.},
  archive      = {J_ASOC},
  author       = {Zohre Moattar Husseini and Mohammad Hossein Fazel Zarandi and Abbas Ahmadi},
  doi          = {10.1016/j.asoc.2024.111227},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111227},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Type2 soft biclustering framework for alzheimer microarray},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-modal multi-objective evolutionary algorithm based
on scaled niche distance. <em>ASOC</em>, <em>152</em>, 111226. (<a
href="https://doi.org/10.1016/j.asoc.2023.111226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal multi-objective optimization problems (MMOPs) refer to several solutions in the decision space that share the same or similar objective value. Balancing the diversity of the objective space and decision space while maintaining the convergence of the population is a challenging and important problem. To address this issue, a novel multi-modal multi-objective evolutionary algorithm (MMEA) named MMEA-SND is proposed in this study. In the MMEA-SND, to locate Pareto-optimal solutions, and improve the diversity of solutions in the decision space, a diversity fitness is designed by the niche method to calculate the fitness of solutions in the diversity archive. In order to balance the diversity of solutions in the objective space and decision space, a scaled niche distance (SND) method is proposed in environmental selection. In this context, SND are utilized to measure the distances between each solution in the objective space and decision space. Furthermore, a parameter is implemented to avoid disregarding locally optimal solutions. To verify the performance of MMEA-SND, six state-of-the-art MMEAs are adopted to make a comparison on 42 benchmark problems. The experimental results show that the proposed MMEA-SND achieves a competitive performance in solving MMOPs.},
  archive      = {J_ASOC},
  author       = {Jie Cao and Zhi Qi and Zuohan Chen and Jianlin Zhang},
  doi          = {10.1016/j.asoc.2023.111226},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111226},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-modal multi-objective evolutionary algorithm based on scaled niche distance},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced autoencoder-based LiDAR localization in
self-driving vehicles. <em>ASOC</em>, <em>152</em>, 111225. (<a
href="https://doi.org/10.1016/j.asoc.2023.111225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of self-driving vehicles to carry out navigation tasks successfully relies heavily on the implementation of a strong localization system. Global navigation satellite systems have been used to provide such information to the vehicle. However, this faces challenges related to the position computation in certain environments, such as in tunnels, under bridges, or in urban canyons. This paper proposes an autoencoder based method to predict a vehicle’s position based on a LiDAR point cloud (converted into a reference system) without using additional sensors. The proposed model modifies the objective function of the autoencoder to include an additional objective responsible for adapting the output of the encoder layer (the latent space) to predict the position. Moreover, a penalty parameter was used in the objective function to emphasize the learning in the latent space. The convergence of the proposed method was proven by showing that it is a stochastic-local-quasi-convex function, which ensures the effectiveness of the Stochastic Normalized Gradient Descent algorithm in achieving convergence. The NCLT dataset was used to train our model in PyTorch. In comparison to various advanced registration techniques, our method achieves excellent results for the majority of the Kitti dataset sequences as well. The method achieved a mean absolute error of about 0.5, which is desirable for autonomous-driving vehicles.},
  archive      = {J_ASOC},
  author       = {Anas Charroud and Karim El Moutaouakil and Vasile Palade and Ali Yahyaouy},
  doi          = {10.1016/j.asoc.2023.111225},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111225},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Enhanced autoencoder-based LiDAR localization in self-driving vehicles},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systemic risk measurement: A quantile long short-term memory
network approach. <em>ASOC</em>, <em>152</em>, 111224. (<a
href="https://doi.org/10.1016/j.asoc.2023.111224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In finance, systemic risk is the risk that the crisis of an institution could trigger instability or bring down an entire system or market. The Delta Conditional Value-at-Risk is a market-based measure proposed by the recent literature to quantify the systematicity of some financial institutions. Several methods have been proposed to estimate this measure, and the choice of the best method is still an open question. The bivariate constant conditional correlation GARCH model represents one of the most preferred approaches since it allows the computation of the Delta Conditional Value-at-Risk in a closed form . Nevertheless, it requires strong distributional assumptions that are often considered unrealistic. We develop a Quantile Long Short-Term Memory network approach that allows the estimation of the Delta Conditional Value-at-Risk of several financial institutions simultaneously. The model consists of a multi-output neural network able to provide, at the same time, the log-return quantiles of different institutions useful to measure the systemic risk. Furthermore, the proposed model does not need any particular assumption, and it is specifically designed to avoid quantile crossing issues affecting the traditional quantile regression-based approach. Numerical experiments on data of some global systemically important banks reported in the Financial Stability Board validated our approach. We obtain Delta Conditional Value-at-Risk estimates that accurately capture market dynamics and produce a ranking of systemic banks that meets the desired properties of stability and persistence.},
  archive      = {J_ASOC},
  author       = {Imma Lory Aprea and Salvatore Scognamiglio and Paolo Zanetti},
  doi          = {10.1016/j.asoc.2023.111224},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111224},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Systemic risk measurement: A quantile long short-term memory network approach},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An efficient two-stage evolutionary algorithm for
multi-robot task allocation in nuclear accident rescue scenario.
<em>ASOC</em>, <em>152</em>, 111223. (<a
href="https://doi.org/10.1016/j.asoc.2023.111223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing maturity of multi-robot system technology, its applications have expanded across various domains. This paper addresses the critical issue of task allocation in nuclear accident rescue scenario, which plays a pivotal role in the success of such operations. The problem is formulated as a multi-objective optimization problem , taking into account three key indicators: execution time , radiation accumulation, and waiting cost. To effectively tackle this problem, an two-stage evolutionary algorithm is proposed. Firstly, a solution encoding method and a crossover mutation method is devised tailored to the problem’s characteristics. Secondly, a two-stage search strategy is designed. In the first stage, a fixed population size and shift-based density estimation method are used to quickly converge the solution set to the Pareto front . The latter stage uses an infinite size population to find as many Pareto solutions as possible. Finally, a local search strategy is introduced to improve the quality of solution set. In the experimental section, our proposed method is compared with five state-of-the-art algorithms on nine instances of varying scales. Across five evaluation metrics , the proposed algorithm demonstrates competitive performance on all instances. These results underscore the efficacy and competitiveness of our approach in tackling the task allocation problem in multi-robot systems within nuclear accident rescue.},
  archive      = {J_ASOC},
  author       = {Chengxin Wen and Hongbin Ma},
  doi          = {10.1016/j.asoc.2023.111223},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111223},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient two-stage evolutionary algorithm for multi-robot task allocation in nuclear accident rescue scenario},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way evidence theory-based density peak clustering with
the principle of justifiable granularity. <em>ASOC</em>, <em>152</em>,
111217. (<a href="https://doi.org/10.1016/j.asoc.2023.111217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering by fast search and find of density peaks (DPC) is an effective clustering approach that can find all the cluster centers at once with just one parameter and without iterative processing. However, the cutoff distance , a key parameter of density measurement in the DPC approach, affects the quality of the final clustering results . Its selection relies on experimental experience and lacks of a semantic explanation. Furthermore, the allocation strategy of the traditional DPC approach may cause several points to be assigned incorrectly, leading to subsequent points being assigned incorrectly and ultimately forming continuous allocation errors. To overcome the deficiencies, this paper proposes a novel three-way evidence theory-based density peak clustering with the principle of justifiable granularity (3 W-PEDP). First, the computation of the cutoff distance is converted into the search for nearest neighbors. From the perspective of granular computing , 3 W-PEDP transforms the neighbor selection issue into the construction of justifiable granularity . And the optimal neighbors can be achieved with the construction of coverage and specificity criteria. Second, inspired by three-way clustering, we adopt a two-stage method for sample allocation. On the one hand, for core point allocation, a two-layer nearest neighbor is constructed based on the achieved optimal neighbors. On the other hand, we designed a new evidence mass function to guide us in assigning the remaining points. In this novel evidence mass function, not only the labels of the assigned samples are considered, but also the information of the neighborhoods around the unassigned samples is fused. Finally, we assess the effectiveness of 3 W-PEDP on numerous public synthetic datasets and UCI real-world datasets. Then, detail comparing results with several popular clustering methods are presented. In addition, experimental studies verify the effectiveness of constructing justifiable granularity in selecting the optimal neighbors. The experimental results demonstrate 3 W-PEDP has good adaptability and robustness, which can achieve better clustering performance. Our source code is available at https://github.com/Luyangabc/3W-PEDP .},
  archive      = {J_ASOC},
  author       = {Hengrong Ju and Yang Lu and Weiping Ding and Jinxin Cao and Xibei Yang},
  doi          = {10.1016/j.asoc.2023.111217},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111217},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way evidence theory-based density peak clustering with the principle of justifiable granularity},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective trajectory planning for segment assembly
robots using a b-spline interpolation- and infeasible-updating
non-dominated sorting-based method. <em>ASOC</em>, <em>152</em>, 111216.
(<a href="https://doi.org/10.1016/j.asoc.2023.111216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid and smooth functioning of segment assembly robots, which is always conflicting, is critical to improving efficiency and ensuring safety during tunneling construction, particularly for the series-actuated robots employed in non-circular shield machines. However, the trade-off between the aforementioned goals has not been explored for trajectory planning in joint space. Less is known about how to acquire superior trade-off Pareto solutions for this constrained multi-objective optimization problem . To fill this gap, this paper proposes a B-spline interpolation- and infeasible-updating non-dominated sorting-based method for multi-objective trajectory planning of shield machine segment assembly robots. In particular, a multi-objective optimization model is detailed in terms of time, acceleration, and jerk of joint trajectories while accounting for operational efficiency and motion smoothness. The given objective functions can be determined using B-spline interpolation and time information based on the known via-points of hydraulic joints. Meanwhile, the constraints are transformed into a limited number of control point-derived forms. Following that, the infeasible-updating non-dominated sorting-based evolutionary algorithm (INSEA) is introduced to solve this problem and find Pareto-optimal solutions. The main improvement is that the multi-objective function information of infeasible solutions is utilized so as to update the non-dominated sorting process, which adaptively applies population division and individual replacement. The findings indicate that the proposed method is capable of executing multi-objective trajectory planning during different stages of the assembly procedure, and the computed metrics are all greater than serval advanced algorithms. Furthermore, on the basis of the proposed INSEA, multi-degree B-spline interpolation provides lower acceleration peaks and smoother global trajectories than common cubic spline curves throughout the process. Therefore, this method can provide researchers with a wide range of alternatives to achieve the optimum trade-off for multi-objective trajectory planning.},
  archive      = {J_ASOC},
  author       = {Hao Sun and Jianfeng Tao and Chengjin Qin and Chang Dong and Shuang Xu and Qianwei Zhuang and Chengliang Liu},
  doi          = {10.1016/j.asoc.2023.111216},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111216},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective trajectory planning for segment assembly robots using a B-spline interpolation- and infeasible-updating non-dominated sorting-based method},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Tensor product graph diffusion based on nonlinear fusion of
multi-source information to predict circRNA-disease associations.
<em>ASOC</em>, <em>152</em>, 111215. (<a
href="https://doi.org/10.1016/j.asoc.2023.111215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular RNA (circRNA), a non-coding RNA, has been shown in recent years via numerous studies to be more than just a &quot;by-product&quot; of gene transcription. It also participates in the regulation of many diseases and influences various biological functions in the human body. Its unique properties and biological functions indicate that circRNA may become new biomarker and therapeutic target, thus, it is crucial for modern medicine to identify potentially relevant circRNAs. This paper proposed novel computational model SNFTPGd-CDA for circRNA-disease associations prediction with the CircR2Disease v2.0 as dataset, which first constructs multiple similarity networks according to multi-source information related to circRNA and disease, and uses similarity network fusion (SNF) to nonlinearly fuse multi-source similarity networks. Then tensor product graph diffusion (TPGd) is employed to diffuse the similarity in the tensor product graph of the fused similarity network into the context of other nodes, which in turn constructs a more reliable similarity network as the feature matrix. Finally, the cascaded forest model is employed for circRNA-disease associations prediction. An AUC value of 0.9937 and an accuracy of 96.92 % were obtained with five-fold cross-validation. This result indicates that SNFTPGd-CDA achieves more accurate performance than previous methods, and can support validation of circRNA-disease associations.},
  archive      = {J_ASOC},
  author       = {Hao Liu and Chen Chen and Ying Su and Enguang Zuo and Lijun Wu and Min Li and Xuecong Tian and Chenjie Chang and Zhiyuan Cheng and Xiaoyi Lv and Cheng Chen},
  doi          = {10.1016/j.asoc.2023.111215},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111215},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tensor product graph diffusion based on nonlinear fusion of multi-source information to predict circRNA-disease associations},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Echo state network structure optimization algorithm based on
correlation analysis. <em>ASOC</em>, <em>152</em>, 111214. (<a
href="https://doi.org/10.1016/j.asoc.2023.111214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Network (ESN) is an effective variant of Recurrent Neural Network (RNN). However, it is difficult for traditional ESN to determine the reservoir size that matches a given task. In this paper, an ESN structure optimization pruning algorithm based on correlation analysis, called PCESN, is proposed to design the size of the reservoir automatically. First, a characteristic matrix is constructed utilizing probability theory and information theory to measure the correlation between each neuron in the reservoir and the output neuron. On this basis, a pruning criterion is proposed to achieve a sparse reservoir structure by dynamically removing neurons with low correlation . Second, in order to retain the sample information of the neurons in the removed reservoir during the network reduction, the input weights of the retained reservoir are updated by means of the average transverse propagation of the weights. Finally, the performance of PCESN is tested on multiple time series . Simulation results show that the proposed PCESN outperforms some fixed size ESN and other dynamic ESN in terms of prediction accuracy, generalization performance and model complexity.},
  archive      = {J_ASOC},
  author       = {Bowen Wang and Shuxian Lun and Ming Li and Xiaodong Lu},
  doi          = {10.1016/j.asoc.2023.111214},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111214},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Echo state network structure optimization algorithm based on correlation analysis},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A portfolio trading system using a novel pixel graph network
for stock selection and a mean-CDaR optimization for portfolio
rebalancing. <em>ASOC</em>, <em>152</em>, 111213. (<a
href="https://doi.org/10.1016/j.asoc.2023.111213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity and dynamic nature of financial markets, portfolio management tasks require continuous adaptation with market intelligence. Furthermore, to make profitable trading decisions, it is essential to predict the future performance of the market. To that end, this study proposes a dynamic portfolio trading system in two stages: stock trend prediction and portfolio optimization. In the first stage, although many researchers proposed powerful classification models to predict BUY/SELL trading signals, some scholars only focused on converting time series&#39; features into multichannel images without considering any spatial relationships between features. Considering the relational capacity of graphs, this study proposes a novel image-based classification model, called Pixel Graph Network (PGN), to analyze the constructed images with propagating information from all neighboring pixels , build up a representation of the images as special graphs, and get advantages of a graph-based learning algorithm. In the second stage, stocks with predicted BUY trading signals in the following days, are fed into the Mean–Conditional Drawdown at Risk (M-CDaR) optimization model to prevent any significant drawdown of the investment portfolio. Moreover, a hierarchical feature selection algorithm has been proposed, combined with a continuous trend labeling method to improve the efficiency of the training process. To demonstrate the superiority of our proposed PGN model in comparison with benchmarks, 18 stocks from the New York Stock Exchange (NYSE) are used for numerical experiments. The obtained results show the improvement and effectiveness of the proposed model in both stock trend prediction and portfolio optimization. In terms of financial performance, the proposed PGN+M-CDaR portfolio-based trading system achieves an average annual return of 112%, an average annual Sortino ratio of 10.78, and an average annual Sharpe ratio of 2.79 which are promising to achieve substantial profits while mitigating downside risk.},
  archive      = {J_ASOC},
  author       = {Milad Kamali Alamdari and Akbar Esfahanipour and Hossein Dastkhan},
  doi          = {10.1016/j.asoc.2023.111213},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111213},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A portfolio trading system using a novel pixel graph network for stock selection and a mean-CDaR optimization for portfolio rebalancing},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A hierarchical surrogate assisted optimization algorithm
using teaching-learning-based optimization and differential evolution
for high-dimensional expensive problems. <em>ASOC</em>, <em>152</em>,
111212. (<a href="https://doi.org/10.1016/j.asoc.2023.111212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate-assisted evolutionary algorithms (SAEAs) are increasingly used in solving computationally expensive optimization problems . However, when tackling high-dimensional expensive problems, a large number of exact function evaluations (FEs) need to be consumed for existing SAEAs to achieve an acceptable solution . In this paper, a hierarchical surrogate assisted optimization algorithm (HSAOA) using teaching-learning-based optimization and differential evolution is proposed for solving high-dimensional expensive problems with a relatively small number of exact FEs. To keep a balance between global exploration and local exploitation, a hierarchical surrogate framework with hybrid evolutionary algorithms is devised. In the global search phase, a radial basis function surrogate is utilized to assist the teaching-learning-based optimization in locating the promising sub-regions. In the local search phase , a novel dynamic ensemble of surrogates is proposed to assist the differential evolution in speeding up the convergence process . Eight test functions with 10 to 100 dimensions and a spatial truss design problem are employed to compare the proposed method with several state-of-the-art SAEAs. The results show that the proposed HSAOA is superior to the comparison algorithms for solving expensive optimization problems, and needs a much smaller number of exact FEs than other competing SAEAs to produce competitive or even better results for high-dimensional expensive problems.},
  archive      = {J_ASOC},
  author       = {Jian Zhang and Muxi Li and Xinxin Yue and Xiaojuan Wang and Maolin Shi},
  doi          = {10.1016/j.asoc.2023.111212},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111212},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hierarchical surrogate assisted optimization algorithm using teaching-learning-based optimization and differential evolution for high-dimensional expensive problems},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lens imaging opposition-based learning for differential
evolution with cauchy perturbation. <em>ASOC</em>, <em>152</em>, 111211.
(<a href="https://doi.org/10.1016/j.asoc.2023.111211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opposition-based learning (OBL) is an effective optimization strategy that enhances the performance of various global optimization algorithms . Among these algorithms, differential evolution (DE) based on OBL has received the attention of many researchers. However, OBL is a strategy with relative symmetry between the target point and the opposite point, which does not contribute significantly to the population in the later stage of the algorithm, and tends to disregard more meaningful points around the opposite points. Therefore, we propose a novel opposition-based differential evolution based on the principle of convex lens (LensOBLDE), which utilizes the distance relationship between the target point and the focal length of the lens, and dynamically adjusts the search radius by controlling the parameters, thus further improving the search ability. In addition, to avoid the algorithm from falling into a local optimum, a small-scale and low-probability cauchy perturbation strategy is applied to the elite individuals in the population, which ensures the exploration and exploitation of the algorithm. Finally, LensOBLDE evaluates the performance of several advanced algorithms on the CEC2017 benchmark suite and conducts a statistical test to evaluate the difference between algorithms. The results indicate that LensOBLDE outperforms several algorithms with a significant difference between them. This verifies the effectiveness of the strategies newly added in this study.},
  archive      = {J_ASOC},
  author       = {Fei Yu and Jian Guan and Hongrun Wu and Yingpin Chen and Xuewen Xia},
  doi          = {10.1016/j.asoc.2023.111211},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111211},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Lens imaging opposition-based learning for differential evolution with cauchy perturbation},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neuro-evolution-based generic missile guidance law for
many-scenarios. <em>ASOC</em>, <em>152</em>, 111210. (<a
href="https://doi.org/10.1016/j.asoc.2023.111210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing efficient aerial interceptors is an important task. Using Pareto-optimality, this paper presents a novel approach for generating guidance laws that are generic for many aerial pursuit-evasion scenarios. In particular, the pure-proportional navigation law is combined with a neural network to create adaptive guidance laws, which adapt according to the current state of the system. First, a many-objective optimization problem is formulated in which each objective aims at the best performance in one of the scenarios. Next, using simulations with a many-objective evolutionary algorithm , a population of guidance laws is evolved towards the Pareto-optimal ones. The obtained guidance laws from multiple runs are statistically analyzed and compared with a set of Pareto-optimal pure-proportional navigation laws. The results suggest that the proposed approach provides a significant improvement as compared with the pure-proportional navigation law over the entire set of scenarios.},
  archive      = {J_ASOC},
  author       = {Adham Salih and Amiram Moshaiov},
  doi          = {10.1016/j.asoc.2023.111210},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111210},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Neuro-evolution-based generic missile guidance law for many-scenarios},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asynchronous evolution of deep neural network architectures.
<em>ASOC</em>, <em>152</em>, 111209. (<a
href="https://doi.org/10.1016/j.asoc.2023.111209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e., compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks , is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of up to K K individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as M &lt; &lt; K M&amp;lt;&amp;lt;K individuals have been evaluated. A suitable value for M M is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in eight-line sorting network design (a single-population optimization task with limited evaluation-time variability), achieving an over two-fold speedup. Next, it was evaluated in 11-bit multiplexer design (a single-population discovery task with extended variability), where a 14-fold speedup was observed. It was then scaled up to ENAS for image captioning (a multi-population open-ended-optimization task), resulting in an over two-fold speedup. In all problems, a multifold performance improvement was observed, suggesting that AES is a promising method for parallelizing the evolution of complex systems with long and variable evaluation times, such as those in ENAS.},
  archive      = {J_ASOC},
  author       = {Jason Liang and Hormoz Shahrzad and Risto Miikkulainen},
  doi          = {10.1016/j.asoc.2023.111209},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111209},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Asynchronous evolution of deep neural network architectures},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating teletraffic theory with neural networks for
quality-of-service evaluation in mobile networks. <em>ASOC</em>,
<em>152</em>, 111208. (<a
href="https://doi.org/10.1016/j.asoc.2023.111208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile cellular design, one important quality-of-service metric is the blocking probability . Using computer simulation for studying blocking probability is quite time-consuming, whereas existing teletraffic-based methods such as the Information Exchange Surrogate Approximation (IESA) only give a rough estimate of blocking probability . Another common approach, direct blocking probability evaluation using neural networks (NN), performs poorly when extrapolating to network conditions outside of the training set. This paper addresses the shortcomings of existing teletraffic and NN-based approaches by combining both approaches, creating what we call IESA-NN. In IESA-NN, an NN is used to estimate a tuning parameter, which is in turn used to estimate the blocking probability via a modified IESA approach. In other words, the teletraffic approach IESA still forms the core of IESA-NN, with NN techniques used to improve the accuracy of the approach via the tuning parameter. Simulation results show that IESA-NN performs better than previous approaches based on NN or teletraffic theory alone. In particular, even when the NN cannot produce a good value for the tuning parameter, for example when extrapolating to network conditions not experienced in the training set, the final IESA-NN estimate is generally still accurate as the estimate is primarily determined by the underlying teletraffic theory, with the NN determining the tuning parameter playing a supplementary role. The combination of the IESA framework with NN in a secondary role makes IESA-NN quite robust.},
  archive      = {J_ASOC},
  author       = {Yin-Chi Chan and Jingjin Wu and Eric W.M. Wong and Chi Sing Leung},
  doi          = {10.1016/j.asoc.2023.111208},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111208},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Integrating teletraffic theory with neural networks for quality-of-service evaluation in mobile networks},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolving masked low-rank transformer for long text
understanding. <em>ASOC</em>, <em>152</em>, 111207. (<a
href="https://doi.org/10.1016/j.asoc.2023.111207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long sequence text processing is time-consuming owing to the ultra-large-scale self-attention computing. Recent advances demonstrate the attention in transformer can be accelerated by redundancy removal, and there are various sparse variants for attention in large sequences are proposed, which leads to state-of-the-art performance on language and vision task. Low-rank method achieve outstanding success in the field of efficient transformer. The dynamic token sparsification is efficiently time-saving and cost-saving, which can be easily extended to prune redundant spans and to yield semantic features . Evolutionary algorithm is attractive for selecting hyperparameter which is of significant importance in effectiveness. Motivated by these works, we propose an efficient transformers model, termed EMLT, to alleviate time and cost without sacrificing the accuracy. EMLT effectively combines strengths of Low-rank transformers, dynamic token sparsification and evolutionary algorithm to ulteriorly cut redundant token and meanwhile maintains the original precision, which can achieve a linear memory and time complexity. We compress transformer in three stages. Firstly, sliding window is validated as local attention to capture fine-grained dependency semantics . After that, low-rank approximation of attention matrix is applied as global attention to store long-range dependency semantics, and aggregated with local attention. On this basis, we consistently prune redundant token in accordance with importance score to further sparse the attention operation. Finally, Evolutionary algorithm is utilized to optimize the hyper-parameters of every layer. The results of comprehensive experiments and analysis show that our method can rival others on accuracy, and outperforms others on efficiency by a large margin in terms of the computational complexity .},
  archive      = {J_ASOC},
  author       = {Chenjing Liu and Xiangru Chen and Jie Lin and Peng Hu and Junfeng Wang and Xue Geng},
  doi          = {10.1016/j.asoc.2023.111207},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111207},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolving masked low-rank transformer for long text understanding},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Progress, achievements, and challenges in multimodal
sentiment analysis using deep learning: A survey. <em>ASOC</em>,
<em>152</em>, 111206. (<a
href="https://doi.org/10.1016/j.asoc.2023.111206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is a computational technique that analyses the subjective information conveyed within a given expression. This encompasses appraisals, opinions, attitudes or emotions towards a particular subject, individual, or entity. Conventional sentiment analysis solely considers the text modality and derives sentiment by identifying the semantic relationship between words within a sentence. Despite this, certain expressions, such as exaggeration, sarcasm and humor, pose a challenge for automated detection when conveyed only through text. Multimodal sentiment analysis incorporates various forms of data, such as visual and acoustic cues, in addition to text. By utilizing fusion analysis, this approach can more precisely determine the implied sentiment polarity, which includes positive, neutral, and negative sentiments. Thus, the recent advancements in deep learning have boosted the domain of multimodal sentiment analysis to new heights. The research community has also shown significant interest in this topic due to its potential for both practical application and educational research. In light of this fact, this paper aims to present a thorough analysis of recent ground-breaking research studies conducted in multimodal sentiment analysis, which employs deep learning models across various modalities such as text, audio, image, and video. Furthermore, the article dives into a discussion of the multiple categories of multimodal data, diverse domains in which multimodal sentiment analysis can be applied, a range of operations that are integral to multimodal sentiment analysis, deep learning architectures, a variety of fusion methods, challenges associated with multimodal sentiment analysis, and the benchmark datasets in addition to the state-of-the-art approaches. The ultimate goal of this survey is to indicate the success of deep learning architectures in tackling the complexities associated with multimodal sentiment analysis.},
  archive      = {J_ASOC},
  author       = {Ananya Pandey and Dinesh Kumar Vishwakarma},
  doi          = {10.1016/j.asoc.2023.111206},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111206},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Progress, achievements, and challenges in multimodal sentiment analysis using deep learning: A survey},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A software trustworthiness evaluation methodology for cloud
services with picture fuzzy information. <em>ASOC</em>, <em>152</em>,
111205. (<a href="https://doi.org/10.1016/j.asoc.2023.111205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {VlseKriterijumska Optimizacija I Kompromisno Resenje (VIKOR) is one of the most commonly used decision-making technique. This work provides a software trustworthiness evaluation method based on VIKOR technique in a group decision-making (GDM) environment. Entropy is an important measure that refers to the amount of disorder in a decision support system . To quantify the decision quality of decision maker (DM), a new method for determining DM weights is proposed according to the entropy value of decision matrix, which is provided by that DM. A new normalization-projection measure is developed in order to test the closeness between two evaluation matrices in software trustworthiness evaluation process. The group regret measure in VIKOR-based GDM method is based on two negative reference matrices in this model. A new ranking method with classification is also developed in this work. Aiming at the complex and changeable environment, these new measures and methods are expected to enrich VIKOR techniques and to play a constructive role in group decision support systems . The feasibility and practicability of developed method are verified by some experiments. This study finds that the different methods (models) may have different results in situations where the same decision data are provided by DMs; and this study also finds that the different parameters may have different results in situation where the decision model is same. The practical implications of developed GDM method can apply not only to software trustworthiness evaluation, but also to many other product quality evaluations.},
  archive      = {J_ASOC},
  author       = {Chuan Yue},
  doi          = {10.1016/j.asoc.2023.111205},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111205},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A software trustworthiness evaluation methodology for cloud services with picture fuzzy information},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Immune plasma programming: A new evolutionary
computation-based automatic programming method. <em>ASOC</em>,
<em>152</em>, 111204. (<a
href="https://doi.org/10.1016/j.asoc.2023.111204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immune plasma therapy, one of the treatment modalities , has proven effective in combating the now rapidly spreading COVID-19 and many other pandemics. The immune plasma algorithm (IPA), inspired by the application phases of this treatment modality, is a recently proposed metaheuristic algorithm . Since its introduction, it has achieved promising results in engineering applications . In this paper, we propose for the first time immune plasma programming (IPP) based on the structure of IPA as a new evolutionary computation-based automatic programming (AP) method. It is compared with well-known AP methods such as artificial bee colony programming, genetic programming , and cartesian ant programming using symbolic regression test problems. It is also compared with baseline methods , many of which are based on recurrent neural networks and a real-word problem is solved. The control parameters of IPP are also tuned separately. The results of the experiments and statistical tests have shown that the prediction accuracy and convergence speed of the models produced by IPP are high. Therefore, IPP has been proposed as a method that can be used to solve various problems.},
  archive      = {J_ASOC},
  author       = {Sibel Arslan},
  doi          = {10.1016/j.asoc.2023.111204},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111204},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Immune plasma programming: A new evolutionary computation-based automatic programming method},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-precision crown control strategy for hot-rolled
electric steel using theoretical model-guided BO-CNN-BiLSTM framework.
<em>ASOC</em>, <em>152</em>, 111203. (<a
href="https://doi.org/10.1016/j.asoc.2023.111203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction accuracy of strip crown is low under complex industrial data environments to general machine learning models, i.e., lack of reasonable mechanism explanation and spatial dimension dependence, which will directly affect the product quality of hot-rolled electrical steel. Therefore, a high-precision crown prediction model is proposed for electrical steel in hot rolling based on a theoretical model-guided BO-CNN-BiLSTM (Bayesian optimization, Convolution Neural Network, and Bidirectional Long Short-term Memory) framework. The work roll wear model, thermal crown model, and the secondary deformation model of the strip between stands, based on the primary deformation by the loaded gap profile, were constructed. The mechanism parameters and measured parameters are integrated into a dataset as input feature variables. In the TG-BO-CNN-BiLSTM framework, the CNN-BiLSTM model, which can achieve spatial dimension dependence, was used to extract its feature component and sequentially predict the crown using the dataset, simultaneously, the BO module optimizes the hyperparameter of the CNN-BiLSTM model. The advantages of the proposed model are verified by adopting multiple evaluation indicators, which improves running speed and prediction accuracy. The effects of process parameters on the crown with typical upstream and downstream stands were comprehensively analyzed with the proposed model. A high-precision crown control strategy , combining the framework and influence law, for multi-stand and multi-method was proposed to obtain the high-precision crown. The control strategy applied in 1450 mm 4-high hot strip mills shows that the production performance of electrical steel is significantly improved.},
  archive      = {J_ASOC},
  author       = {Chunning Song and Jianguo Cao and Qiufang Zhao and Shuangtao Sun and Wenhui Xia and Lei Sun},
  doi          = {10.1016/j.asoc.2023.111203},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111203},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A high-precision crown control strategy for hot-rolled electric steel using theoretical model-guided BO-CNN-BiLSTM framework},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An ɛ-constrained multiobjective differential evolution with
adaptive gradient-based repair method for real-world constrained
optimization problems. <em>ASOC</em>, <em>152</em>, 111202. (<a
href="https://doi.org/10.1016/j.asoc.2023.111202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, incorporating information from the objective function into the constraint-handling process has garnered considerable attention in evolutionary algorithm research. Stemming from this, multiobjective optimization has emerged as a promising approach that simultaneously optimizes the objective function and constraints. However, the challenges associated with optimizing objective functions and satisfying constraints exhibit significant variability. Some constraints and/or objective functions can be exceptionally challenging, necessitating specific methods to identify the optimal solution within a limited feasible region . This study proposes an adaptive gradient descent-based repair method to enhance the search capability for both objective function optimization and constraint satisfaction . This method leverages objective function information to rectify infeasible solutions using gradient descent , thereby reducing the limitations of a purely constraint-based approach and automating the application of the repair method. Furthermore, an enhanced variant of the ɛ ɛ ɛ -constrained multiobjective differential evolution algorithm is developed for solving constrained optimization problems . The efficacy of the proposed approach is assessed using 57 benchmark test functions derived from real-world applications. Empirical results demonstrate that our approach is capable of locating high-quality solutions, outperforming several selected state-of-the-art algorithms.},
  archive      = {J_ASOC},
  author       = {Jing-Yu Ji and Zusheng Tan and Sanyou Zeng and Man-Leung Wong},
  doi          = {10.1016/j.asoc.2023.111202},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111202},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An ɛ-constrained multiobjective differential evolution with adaptive gradient-based repair method for real-world constrained optimization problems},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bi-objective multi-mode resource-constrained multi-project
scheduling using combined NSGA II and q-learning algorithm.
<em>ASOC</em>, <em>152</em>, 111201. (<a
href="https://doi.org/10.1016/j.asoc.2023.111201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-mode resource-constrained multi-project scheduling problem (MRCMPSP) plays a pivotal role in project management, serving as a critical component in production management for Engineering-to-Order manufacturing companies to enhance productivity, reduce costs, and minimize project completion time. This paper investigates the challenging problem of a bi-objective MRCMPSP, considering resource and ﬁnish time constraints, and develops a mathematical model to reduce project cycles and achieve better resource load balancing. By leveraging on the independent nature of start time selection for each activity, which aligns with the characteristics of a Markov decision process , we propose a two-layer iterative algorithm that combines the Nondominated Sorting Genetic Algorithm II (i.e., NSGA II) and Q-learning algorithm to solve the model eﬀectively. Hence, the NSGA II algorithm generates mode combinations, while its ﬁtness function employs the Q-learning algorithm to search for optimal activity time selections within each mode. We verify the performance superiority of the proposed algorithm by conducting a comparative analysis against classical approaches, encompassing classical NSGA II, Particle Swarm Optimization , and Ant Colony Optimization algorithms. Furthermore, this study’s experimental results therefore unequivocally demonstrate the eﬀectiveness of our algorithm in achieving optimized project scheduling outcomes.},
  archive      = {J_ASOC},
  author       = {Hongbing Yang and Ziyang Wang and Yue Gao and Wei Zhou},
  doi          = {10.1016/j.asoc.2023.111201},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111201},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Bi-objective multi-mode resource-constrained multi-project scheduling using combined NSGA II and Q-learning algorithm},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on personalized itinerary recommendation: From
optimisation to deep learning. <em>ASOC</em>, <em>152</em>, 111200. (<a
href="https://doi.org/10.1016/j.asoc.2023.111200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tourism industry is a significant contributor to the global economy, responsible for generating nearly 10% of the world’s GDP and employing around 9% of the global workforce. A crucial aspect of this industry is personalised itinerary recommendation, where visitors’ preferences and constraints are taken into account to create customised travel plans. This task involves selecting the best points of interests (POIs) for visitors in various cities and then schedule these POIs as an itinerary considering numerous constraints. However, due to the varied ways in which researchers have defined the itinerary recommendations, it can be challenging for new researchers to locate up-to-date literature on the topic. As a result, this paper aims to review existing research in this area and provide a taxonomy of the works based on problem formulations, proposed techniques, constraints, and features used. We divide the study into two directions: user satisfaction and provider satisfaction, where user satisfaction is derived non-personalised and personalised POI/ Itinerary recommendations. We also discuss the data sources , techniques ranging from optimisation approaches to deep learning and evaluation methodologies commonly used in this field. Finally, we highlight the importance of personalised itinerary recommendation and identify areas for future research to address the current challenges.},
  archive      = {J_ASOC},
  author       = {Sajal Halder and Kwan Hui Lim and Jeffrey Chan and Xiuzhen Zhang},
  doi          = {10.1016/j.asoc.2023.111200},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111200},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A survey on personalized itinerary recommendation: From optimisation to deep learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interactively iterative group decision-making method with
interval-valued intuitionistic fuzzy preference relations based on a new
additively consistent concept. <em>ASOC</em>, <em>152</em>, 111199. (<a
href="https://doi.org/10.1016/j.asoc.2023.111199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-valued intuitionistic fuzzy preference relation (IVIFPR) is a powerful instrument for describing uncertain judgement of expert in preference group decision making (PGDM). Nevertheless, the extant additive consistency definitions for IVIFPRs violate the invariance regarding arrangement of objectives. In addition, the modification for comparative judgments in the consensus reaching process (CRP) of existing PGDMs with IVIFPRs do not involve the interaction or communication with experts. Therefore, we propose an PGDM to avoid these situations. First, a new additive consistency definition for IVIFPRs is given satisfying the invariance regarding arrangement of objectives. Then, an interactively iterative algorithm considering minimum information loss is developed to enhance the consistency of IVIFPRs. Next, as to group decision making (GDM) with IVIFPRs, a bi-objective optimization model is constructed to determine the experts’ weights, and an interactively iterative algorithm considering minimum information loss is developed to enhance the consensus level among experts. Furthermore, a fuzzy optimization model is constructed to derive the interval priority weights of objectives from an IVIFPR. Finally, the feasibility and superiority of the proposed GDM method are verified by application examples and analyses. The proposed GDM method has obtained an average of 0.0432 for information deviations and 22/72 for modified judgments on two datasets, respectively, which is one of the best results compared to state-of-art.},
  archive      = {J_ASOC},
  author       = {Xiao-Yun Lu and Jiu-Ying Dong and Shu-Ping Wan and He-Cheng Li},
  doi          = {10.1016/j.asoc.2023.111199},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111199},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interactively iterative group decision-making method with interval-valued intuitionistic fuzzy preference relations based on a new additively consistent concept},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 2-additive choquet integral-based multi-criterion
decision-making method with complex linguistic information in drug value
assessment. <em>ASOC</em>, <em>152</em>, 111198. (<a
href="https://doi.org/10.1016/j.asoc.2023.111198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating incidence and mortality rates of lung cancer, the targeted drugs assessment has emerged a key problem, serving as the foundation for incorporating targeted drugs into the national basic medical insurance. This paper executes drug value assessment within a multi-criterion decision-making framework. Recognizing that experts naturally provide assessments in linguistic representations during drug evaluations, we employ the generalised probabilistic linguistic term set to uniformly model diverse types of linguistic information. Considering the bounded rationality of experts, we propose a method to obtain expert weights through minimizing regret degree of experts. Based on the determined weights of experts, a method to simplify the generalised probabilistic linguistic term set is developed. Afterwards, this study establishes a 2-additional Choquet integral-based aggregation method to fuse the assessments of drugs considering interactive criteria. To examine the effectiveness of the proposed method, we apply the proposed method in drug value assessment for lung cancer. The proposed method provides a feasible attempt to address decision-making problems involving complex linguistic information.},
  archive      = {J_ASOC},
  author       = {Ran Fang and Huchang Liao},
  doi          = {10.1016/j.asoc.2023.111198},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111198},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A 2-additive choquet integral-based multi-criterion decision-making method with complex linguistic information in drug value assessment},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization approach of berth-quay crane-truck allocation
by the tide, environment and uncertainty factors based on chaos quantum
adaptive seagull optimization algorithm. <em>ASOC</em>, <em>152</em>,
111197. (<a href="https://doi.org/10.1016/j.asoc.2023.111197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The post-epidemic era has led to the accumulation of cargo, which has brought greater pressure to container ports. Since traditional methods cannot simultaneously consider the effect of tidal, uncertain, and environmental factors on the allocation plan. To relieve this pressure, firstly, considering tidal factors, formulating time window rules, thinking out uncertain factors, and determining constraints from three perspectives of vessel berthing process, quay crane and container truck operation, a new berth-quay crane-truck joint scheduling model is constructed by minimizing three aspects of vessels turnaround time, the carbon emissions of quay cranes and trucks , namely TEU-BQCT model. Then, aiming at obtaining a relatively high-quality solution, combining chaotic mapping and quantum entanglement , a new chaotic quantum adaptive seagull optimization algorithm is proposed, namely CQASOA, exclusive coding rules suitable for the TEU-BQCT model is formulated, a feasible integer algorithm is developed, the external penalty function is constructed to limit constraints, and a novel joint scheduling solution method of berth-quay crane-truck is proposed, namely TEU-BQCT_CQASOA. Subsequently, two ports of different scales in South China are used to test the constructed solution method feasibility. The simulation results indicate that the constructed TEU-BQCT model can obtain a more suitable scheduling scheme. The proposed CQASOA has better performance than other comparison algorithms selected in this paper, which can obtain a better solution when solving the TEU-BQCT model.},
  archive      = {J_ASOC},
  author       = {Ming-Wei Li and Rui-Zhe Xu and Zhong-Yi Yang and Wei-Chiang Hong and Xiao-Gang An and Yi-Hsuan Yeh},
  doi          = {10.1016/j.asoc.2023.111197},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111197},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Optimization approach of berth-quay crane-truck allocation by the tide, environment and uncertainty factors based on chaos quantum adaptive seagull optimization algorithm},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing seasonal z-number regression for waste-disposal
forecasting in a taiwanese hospital. <em>ASOC</em>, <em>152</em>,
111196. (<a href="https://doi.org/10.1016/j.asoc.2023.111196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops a seasonal z-number regression (SZR) to forecast the daily generated amounts of clinical waste for recycling and related waste. The proposed SZR designs new z-number intervals based on least-squares support-vector regression and is combined with the seasonal-decomposition method. Z-numbers characterize the uncertainty of a variable using a fuzzy number and the associated reliability of this fuzzy number. Biogeography-based optimization is employed to select the parameters of the SZR model. In this study, SZR, long short-term memory, support-vector regression with a genetic algorithm , a generalized regression neural network , the autoregressive integrated moving average, and a recurrent neural network are applied to forecast the daily amounts of generated clinical waste for recycling and related waste. The empirical results indicate the following. First, the SZR model demonstrates better performance and robustness than the other approaches for the prediction of the daily amount of clinical waste for recycling and related waste. Second, the SZR model also exhibits superior performance compared to the other approaches with respect to different testing terms. The proposed SZR method can help experts in hospitals develop reasonable waste-disposal projects that can help waste-management systems effectively achieve operational reliability and economy.},
  archive      = {J_ASOC},
  author       = {Hsing-Chin Chien and Ting-Yu Lin and Kuo-Ping Lin and Trang Cam Hoang and Hoa Anh Tran and Hsiang-Yun Wang and Hui-Mei Huang and Shyue-Yow Chiou},
  doi          = {10.1016/j.asoc.2023.111196},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111196},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Developing seasonal z-number regression for waste-disposal forecasting in a taiwanese hospital},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple and flexible bootstrap-based framework to quantify
epistemic uncertainty of ground motion models by light gradient boosting
machine. <em>ASOC</em>, <em>152</em>, 111195. (<a
href="https://doi.org/10.1016/j.asoc.2023.111195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ground motion models (GMM) incorporating proper epistemic uncertainty quantification are fundamental for seismic hazard analysis, structural seismic design , and earthquake risk management . However, this critical uncertainty has not been systematically addressed in recently prevalent machine learning (ML)-based GMMs. To rectify this gap, a straightforward and flexible framework based on bootstrapping is proposed for generic ML algorithms . Specifically, a group of datasets are generated through bootstrap resampling, each employed to develop an individual GMM. In the inference phase, the epistemic uncertainty is represented by the prediction samples from this ensemble of models. In case study , the Light Gradient Boosting Machine (LGBM), is applied to establish a GMM on the NGA-West2 database. Overfitting is mitigated by early-stopping, and its implications on uncertainty components and random-effect variability are thoroughly examined. Compared to artificial neural network (ANN) in literature, our work demonstrates a decrease in the standard deviation of inter- and intra-event variability by an average of 0.051 and 0.140, respectively. Moreover, our model adeptly captures the heteroscedasticity of epistemic uncertainty, particularly over different regions and within the range of the metadata with limited data. The significance of epistemic uncertainty is highlighted by its potential to diminish the statistical significance of seismic scaling effects at short-period pseudo spectral acceleration (PSA) for great events and at rock sites.},
  archive      = {J_ASOC},
  author       = {Tianxing Wen and Jianguang He and Liqiang Jiang and Yanliang Du and Lizhong Jiang},
  doi          = {10.1016/j.asoc.2023.111195},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111195},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A simple and flexible bootstrap-based framework to quantify epistemic uncertainty of ground motion models by light gradient boosting machine},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inverse distance weighting and radial basis function based
surrogate model for high-dimensional expensive multi-objective
optimization. <em>ASOC</em>, <em>152</em>, 111194. (<a
href="https://doi.org/10.1016/j.asoc.2023.111194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial basis function (RBF) models have attracted a lot of attention in assisting evolutionary algorithms for solving computationally expensive optimization problems . However, most RBFs cannot directly provide the uncertainty information of their predictions, making it difficult to adopt principled infill sampling criteria for model management. To overcome this limitation, an inverse distance weighting (IDW) and RBF based surrogate assisted evolutionary algorithm, named IR-SAEA, is proposed to address high-dimensional expensive multi-objective optimization problems. First, an RBF-IDW model is developed, which can provide both the predicted objective values and the uncertainty of the predictions. Moreover, a modified lower confidence bound infill criterion is proposed based on the RBF-IDW for the balance of exploration and exploitation. Extensive experiments have been conducted on widely used benchmark problems with up to 100 dimensions. The empirical results have validated that the proposed algorithm is able to achieve a competitive performance compared with state-of-the-art SAEAs.},
  archive      = {J_ASOC},
  author       = {Fei Li and Zhengkun Shang and Yuanchao Liu and Hao Shen and Yaochu Jin},
  doi          = {10.1016/j.asoc.2023.111194},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111194},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Inverse distance weighting and radial basis function based surrogate model for high-dimensional expensive multi-objective optimization},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data preprocessing to improve fairness in machine learning
models: An application to the reintegration process of demobilized
members of armed groups in colombia. <em>ASOC</em>, <em>152</em>,
111193. (<a href="https://doi.org/10.1016/j.asoc.2023.111193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of machine learning allows automating decision-making based on data, saving time and resources compared to traditional methods that require human intervention. This automation poses significant challenges in terms of ensuring that the model-building process incorporates ethical considerations and addresses potential biases that may arise. In the context of this research, proposed approaches were explored to ensure fairness through interventions during data preprocessing in the construction of a binary classification model. The use case employed aimed to develop a model capable of determining whether demobilized individuals from armed groups in Colombia, who are in the process of reintegration, were eligible to access the Economic Insertion Benefit. Fairness was evaluated by the difference in false negative rates between men and women. To achieve a balance between model performance and non-discrimination, techniques such as feature engineering, hyperparameter optimization, balancing or resampling, suppression or unawareness, and reweighing were included. These techniques were used both independently and in combination with each other. The results highlighted the need to complement balancing or resampling techniques that do not consider fairness. On the other hand, applying balancing or resampling techniques with a fairness focus reduced the difference in false negative rates but resulted in a higher number of errors. In addition, the application of hyperparameter optimization and reweighing improved fairness without compromising the overall model accuracy.},
  archive      = {J_ASOC},
  author       = {Alveiro Alonso Rosado Gómez and Maritza Liliana Calderón Benavides and Oscar Espinosa},
  doi          = {10.1016/j.asoc.2023.111193},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111193},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data preprocessing to improve fairness in machine learning models: An application to the reintegration process of demobilized members of armed groups in colombia},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Granular fuzzy rule-based model construction under the
collaboration of multiple organizations. <em>ASOC</em>, <em>152</em>,
111191. (<a href="https://doi.org/10.1016/j.asoc.2023.111191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real world, phenomena are often observed and recorded by multiple organizations which results in multiple sources of data. When dealing with such data, the centralized modeling approach aims to achieve collaborative modeling by fusing multiple sources of data into a single data set, which may pose challenges to data privacy. Unlike centralized modeling, the distributed modeling approach can effectively solve the privacy issue. However, modeling approaches based on this idea still suffer from either low prediction accuracy or high communication costs. In this study, we propose a collaborative modeling strategy for multi-source data based on fuzzy rule-based models (FRBMs) to balance the needs of both model prediction accuracy and efficiency. First , we adopt the concept and algorithm of collaborative fuzzy clustering (CFC) to improve prediction accuracy and reduce communication costs by improving the CFC algorithm . Then , we construct a granular FRBM for multi-source data based on the principle of justifiable granularity (PJG) by integrating local models into a more robust and perfect global model. Finally , we improve the performance evaluation index of the existing granular FRBM and propose two model optimization schemes to further improve the performance of the model. We conduct experiments on both synthetic and publicly available data sets to demonstrate the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Bingsheng Liu and Boyang Wang and Yinghua Shen and Witold Pedrycz and Yuan Chen},
  doi          = {10.1016/j.asoc.2023.111191},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111191},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Granular fuzzy rule-based model construction under the collaboration of multiple organizations},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep neural network with modified random forest
incremental interpretation approach for diagnosing diabetes in smart
healthcare. <em>ASOC</em>, <em>152</em>, 111183. (<a
href="https://doi.org/10.1016/j.asoc.2023.111183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) applications based on deep learning for diagnosing type-II diabetes are sometimes difficult to understand and communicate even as patients are eager to understand the rationale behind the diagnostic results. Accordingly, recent studies have used multiple simple rules to adequately explain the diagnostic process and results to patients. However, this can cause patient confusion as the rules vary. Hence, this study proposes a deep neural network (DNN) with random forest (RF) and modified random forest incremental interpretation (MRFII) approach for diagnosing diabetes. This method first entails constructing a DNN to predict the probability of a patient having diabetes. To make the prediction result explainable, an RF is built to explain the process and results in terms of multiple simple decision rules. Additionally, to eliminate patient confusion, the MRFII is proposed to sort and aggregate the decision rules for a specific patient. A certainty mechanism is also established to feed back the explanation results from RF to improve the effectiveness of the DNN. The proposed method was applied to a diabetes dataset from the National Institute of Diabetes and Digestive and Kidney Diseases, and the results showed that this approach provided a more concise and accurate explanation than existing explainable artificial intelligence (XAI) techniques for the same purpose.},
  archive      = {J_ASOC},
  author       = {Tin-Chih Toly Chen and Hsin-Chieh Wu and Min-Chi Chiu},
  doi          = {10.1016/j.asoc.2023.111183},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111183},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A deep neural network with modified random forest incremental interpretation approach for diagnosing diabetes in smart healthcare},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG emotion recognition based on ordinary differential
equation graph convolutional networks and dynamic time wrapping.
<em>ASOC</em>, <em>152</em>, 111181. (<a
href="https://doi.org/10.1016/j.asoc.2023.111181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Network (GCN) has been extensively utilized to extract relations among electroencephalography (EEG) electrode channels for its strong ability to handle non-Euclidean data. However, GCN still has some issues when it comes to extracting features from EEG signals: (1) GCN with more layers may experience over-smoothing, restricting its ability to mine longer dependency relations . (2) At the moment, most GCNs used to process EEG signals construct adjacency matrices by Euclidean distance , only considering the correlations on the feature domain while ignoring changes of signals over the entire time window. To address the issues above, we introduce an Ordinary Differential Equation (ODE) based GCN, which can perfectly eliminate the over-smoothing problem of the traditional GCN. Besides, we also propose a method based on Dynamic Time Wrapping (DTW) algorithm to construct an adjacency matrix in the time domain. To handle adjacency matrices calculated by Euclidean distance and DTW distance respectively, we apply a temporal–spatial model composed of two parallel modules each containing an ODE-based GCN and Long short-term memory neural networks (LSTM) network in turn. We conducted experiments on three public datasets. The results show that our methods have achieved an improvement of 2.19%/2.77%/2.13%/2.01% on Arousal/Valence/Dominance/Liking on DEAP dataset, 1.43% on SEED dataset and 3.06%/3.27% on Arousal/Valence on DREAMER dataset compared with state-of-the-art (SOTA) baseline methods . It demonstrates that our method can effectively approve the performance to handle the relations between EEG channels. The premise of the ODE-based GCN is that signal changes of all EEG channels should be continuous rather than abrupt. We believe that it conforms to the EEG mode, as it is activated by the same emotion stimulation while being collected.},
  archive      = {J_ASOC},
  author       = {Yiyuan Chen and Xiaodong Xu and Xiaoyi Bian and Xiaowei Qin},
  doi          = {10.1016/j.asoc.2023.111181},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111181},
  shortjournal = {Appl. Soft. Comput.},
  title        = {EEG emotion recognition based on ordinary differential equation graph convolutional networks and dynamic time wrapping},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Def-DReL: Towards a sustainable serverless functions
deployment strategy for fog-cloud environments using deep reinforcement
learning. <em>ASOC</em>, <em>152</em>, 111179. (<a
href="https://doi.org/10.1016/j.asoc.2023.111179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern cloud applications are composed of tens of thousands of environment-agnostic serverless functions that can be deployed in either a fog or cloud environment. The key to sustaining fog computing is to offload the maximum amounts of computation to the cloud, and accommodate as many users as possible without compromising quality of service (QoS). However, recent research mainly focuses on assigning maximum resources to serverless applications from the fog node and not taking full advantage of the cloud environment, leading to a lack of sustainability in fog computing. As a way to fill this research gap, we explored what percentage of a user’s request should be handled by fog and cloud. As a result, we proposed Def-DReL, a Systematic Deployment of Serverless Functions in Fog and Cloud environments using Deep Reinforcement Learning , by taking into account several real-life parameters, including distance from a nearby fog node and latency, priority of the user, priority of serverless applications, and resource usage. Def-DReL’s performance is further compared with that of recent related algorithms. Simulation and comparison results clearly demonstrate a lesser number of serverless functions from each user (with approximately 10% improvement) being deployed in the fog node, resulting in accommodating limited fog resources to more number of users. The other simulation results show its superiority over other algorithms as well as its applicability to real-life scenarios.},
  archive      = {J_ASOC},
  author       = {Chinmaya Kumar Dehury and Shivananda Poojara and Satish Narayana Srirama},
  doi          = {10.1016/j.asoc.2023.111179},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111179},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Def-DReL: Towards a sustainable serverless functions deployment strategy for fog-cloud environments using deep reinforcement learning},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tree-shaped multiobjective evolutionary CNN for
hyperspectral image classification. <em>ASOC</em>, <em>152</em>, 111176.
(<a href="https://doi.org/10.1016/j.asoc.2023.111176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have achieved significant performances in hyperspectral image (HSI) classification in recent years. However, designing a high-performance CNN depends on human expertise heavily, which usually takes considerable time and labor. With regard to reducing the burden of designing the networks, neural architecture search (NAS) has attracted increasing attention. A typical NAS approach aims to optimize the network architectures in a predefined search space with a suitable search algorithm automatically. However, the existing NAS work does not fully consider the spatial resolution and the spectral noise interference of HSIs. Furthermore, most NAS approaches use sequential blocks or cells to construct the networks, which are unsuitable for extracting multiscale features of HSIs and result in degraded performance. Considering the above challenges, we propose a tree-shaped multiobjective evolutionary CNN (TMOE-CNN) for HSI classification. An expanded search space is designed, which includes the image patch size and the channel number of the input image patches. A multibranch supernetwork structure is proposed, which resembles a tree as the fundamental architecture for the network block. The image patch size and the denoising strength of the input image patches can be established adaptively throughout the evolutionary search process. The tree-shaped networks can fuse multiscale features to enhance the capacity of the network for feature extraction. Additionally, we consider both the classification accuracy and the floating-point computational complexity in the environmental selection. It is helpful to find the networks with simple structure and low complexity while ensuring classification accuracy . Experiments on different HSI datasets show that TMOE-CNN can search CNNs with high accuracies and simple structures automatically.},
  archive      = {J_ASOC},
  author       = {Mengxuan Zhang and Long Liu and Yaochu Jin and Zhikun Lei and Zhigang Wang and Licheng Jiao},
  doi          = {10.1016/j.asoc.2023.111176},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111176},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Tree-shaped multiobjective evolutionary CNN for hyperspectral image classification},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial neural networks applications in construction and
building engineering (1991–2021): Science mapping and visualization.
<em>ASOC</em>, <em>152</em>, 111174. (<a
href="https://doi.org/10.1016/j.asoc.2023.111174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural network (ANN) has acquired noticeable interest from the research community to handle complex problems in Construction and Building engineering (CB). This interest has led to an enormous amount of scientific publications in diverse CB domains over the last three decades. This study introduces a scientometric review to quantitatively explore and visually map the development pathways and trends of ANN-CB literature. Via the Web of Science (WoS) database, 2406 peer-reviewed journal articles are identified and included for analysis as follows. First, the publication growth over time is inspected and evaluated. Second, the collaboration patterns between key contributors (researchers, countries, and organizations) are explored and mapped using the co-authorship analysis. Third, the key sources’ productivity and influence are explored and mapped using the direct citation analysis. Fourth, the publications clusters and research themes are analyzed and visualized via the keyword co-occurrence analysis and document trend topics mapping. The study outcomes include but are not limited to i) recognizing pioneer ANN-CB researchers for future collaboration opportunities, ii) identifying reliable sources of information or suitable ones for publishing new ANN-CB works, and iii) fostering probable academic partnerships with the leading ANN-CB organizations. These outcomes help researchers to comprehend ANN-CB literature and direct research policy-makers and editorial boards to adopt the promising ANN-CB themes for further research and development.},
  archive      = {J_ASOC},
  author       = {Mohamed Marzouk and Ahmed Elhakeem and Kareem Adel},
  doi          = {10.1016/j.asoc.2023.111174},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111174},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Artificial neural networks applications in construction and building engineering (1991–2021): Science mapping and visualization},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic ε-multilevel hierarchy constraint optimization with
adaptive boundary constraint handling technology. <em>ASOC</em>,
<em>152</em>, 111172. (<a
href="https://doi.org/10.1016/j.asoc.2023.111172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world optimization problems are often difficult to solve because of the complexity of the objective function and the large number of constraints that accompany it. To solve such problems, we propose Adaptive Dynamic ε-Multilevel Hierarchy Constraint Optimization ( ε MHCO). Firstly, we propose the dynamic constraint tolerance factor ε which can change dynamically with the feasible ratio and the number of iterations in the current population. This ensures a reasonable proportion of virtual feasible solutions in the population. Secondly, we propose adaptive boundary constraint handling technology (ABCHT). It can reshape the current individual position adaptively according to the size of constraint violation and increase the diversity of the population. Finally, we propose multi-level hierarchy optimization, whose multiple population structure is beneficial to solve real-world constraint optimization problems (COPs). To validate and analyze the performance of ε MHCO, numerical experiments are conducted on the latest real-world test suite CEC’2020, which contains a set of 57 real-world COPs, and compared with four state-of-the-art algorithms. The results show that ε MHCO is significantly superior to, or at least comparable to the state-of-the-art algorithms in solving real-world COPs. Meanwhile, the effectiveness and feasibility of ε MHCO are verified on the real-world problem of the pipeline inner detector speed control.},
  archive      = {J_ASOC},
  author       = {Jinze Liu and Jian Feng and Shengxiang Yang and Huaguang Zhang and Shaoning Liu},
  doi          = {10.1016/j.asoc.2023.111172},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111172},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic ε-multilevel hierarchy constraint optimization with adaptive boundary constraint handling technology},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight network based on local–global feature fusion
for real-time industrial invisible gas detection with infrared
thermography. <em>ASOC</em>, <em>152</em>, 111138. (<a
href="https://doi.org/10.1016/j.asoc.2023.111138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of industrial invisible gas plays a vital role in preventing environmental pollution and fire accidents. Optical gas imaging (OGI) with infrared thermography is widely used in the field of gas leak monitoring and treatment by visualizing gases to quickly and accurately detect and locate the gas leak sources. However, this method still relies on manual visual inspection . Existing automatic visual gas detection methods suffer from insufficient gas feature extraction and high computational cost due to the indistinct gas features in thermal images . To address these problems, we propose a new lightweight network specialized for thermal gas feature extraction, namely GasViT, sufficiently extracting gas features at very low computational cost by local–global feature fusion . Specifically, two new feature extraction modules M ulti- s cale F usion F eature A ttention (MsFFA) and M ulti- h ead L inear S elf- a ttention (MhLSa) are proposed for GasViT. MsFFA enhances the gas local feature extraction ability by constructing multi-scale channel and spatial feature fusion maps, enabling the network to focus on more valid local information . MhLSa complements the gas global features with very low computational cost by efficiently encoding the global information of the image in terms of the innovative linear self-attention mechanism. Our experimental results on the self-made Industrial Invisible Gas (IIG) Dataset show GasViT achieves 82.7% mAP 50 mAP50 , significantly outperforming the state-of-the-art lightweight networks. Moreover, GasViT achieves 33 FPS real-time detection with a running memory footprint of only 47.2 MB on edge computing devices, making it extremely suitable for portable and embedded detection devices than existing methods to cover gas leakage detection in complex and hazardous industrial scenarios.},
  archive      = {J_ASOC},
  author       = {Huan Yu and Jin Wang and Zhan Wang and Jingru Yang and Kaixiang Huang and Guodong Lu and Fengtao Deng and Yang Zhou},
  doi          = {10.1016/j.asoc.2023.111138},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111138},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight network based on local–global feature fusion for real-time industrial invisible gas detection with infrared thermography},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). A framework based on generational and environmental
response strategies for dynamic multi-objective optimization.
<em>ASOC</em>, <em>152</em>, 111114. (<a
href="https://doi.org/10.1016/j.asoc.2023.111114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the dynamics and uncertainty of the dynamic multi-objective optimization problems ( DMOPs ), it is difficult for algorithms to find a satisfactory solution set before the next environmental change, especially for some complex environments. One reason may be that the information in the environmental static stage cannot be used well in the traditional framework. In this paper, a novel framework based on generational and environmental response strategies ( FGERS ) is proposed, in which response strategies are run both in the environmental change stage and the environmental static stage to obtain population evolution information of those both stages. Unlike in the traditional framework, response strategies are only run in the environmental change stage. For simplicity, the feed-forward center point strategy was chosen to be the response strategy in the novel dynamic framework (FGERS-CPS). FGERS-CPS is not only to predict change trend of the optimum solution set in the environmental change stage, but to predict the evolution trend of the population after several generations in the environmental static stage. Together with the feed-forward center point strategy, a simple memory strategy and adaptive diversity maintenance strategy were used to form the complete FGERS-CPS. On 13 DMOPs with various characteristics, FGERS-CPS was compared with four classical response strategies in the traditional framework. Experimental results show that FGERS-CPS is effective for DMOPs.},
  archive      = {J_ASOC},
  author       = {Qingya Li and Xiangzhi Liu and Fuqiang Wang and Shuai Wang and Peng Zhang and Xiaoming Wu},
  doi          = {10.1016/j.asoc.2023.111114},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {111114},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A framework based on generational and environmental response strategies for dynamic multi-objective optimization},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning with reward shaping for tracking
control and vibration suppression of flexible link manipulator.
<em>ASOC</em>, <em>152</em>, 110756. (<a
href="https://doi.org/10.1016/j.asoc.2023.110756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper puts forward a novel deep reinforcement learning control using deep deterministic policy gradient (DRLC-DDPG) framework to address the reference tracking and vibration suppression problem of rotary flexible link (RFL) manipulator. Specifically, this study attempts to address the continuous action space DRLC problem through DDPG algorithm and presents a Lyapunov function based reward shaping approach for guaranteed deep reinforcement learning (DRL) convergence and enhanced speed of training. The proposed approach synthesizes the hard and soft constraints of the flexible manipulator as a constrained Markov decision problem (MDP) and evaluates the performance of DRLC-DDPG framework through hardware in loop (HIL) testing to realize precise servo tracking and suppressed vibration of the flexible manipulator. For identifying the dynamical model of the RFL, an empirical Auto-Regressive eXogenous (ARX) model using the closed loop identification technique is built. Moreover, to extract the true states (servo angle and deflection angle) from the actual measurements, which typically have the influence of sensor noise, an adaptive Kalman filter (AKF) is augmented with the DRLC scheme. The experimental results of DRLC-DDPG scheme compared with those of the model predictive control (MPC) for several test cases reveal that the proposed scheme is superior to MPC both in terms of trajectory tracking and robustness against the external disturbances and model uncertainty.},
  archive      = {J_ASOC},
  author       = {Joshi Kumar Viswanadhapalli and Vinodh Kumar Elumalai and Shivram S. and Sweta Shah and Dhruv Mahajan},
  doi          = {10.1016/j.asoc.2023.110756},
  journal      = {Applied Soft Computing},
  month        = {2},
  pages        = {110756},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep reinforcement learning with reward shaping for tracking control and vibration suppression of flexible link manipulator},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A genetic operators-based ant lion optimiser for training a
medical multi-layer perceptron. <em>ASOC</em>, <em>151</em>, 111192. (<a
href="https://doi.org/10.1016/j.asoc.2023.111192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense amount of data managed during the diagnosis process overwhelms, by far, the clinicians’ processing capabilities. Artificial intelligence methods like Multi-Layer Perceptrons come to help by providing a second opinion based on powerful and reliable data processing. Unfortunately, these methods often suffer from problems related to their training methods, which can lead to poor performance. Metaheuristics are promising training alternatives because of their stochastic and general-purpose nature. This work introduces a new training method based on metaheuristics, called Genetic Ant Lion Optimiser. It includes new features for dealing with the convergence problems of the original Ant Lion Optimiser and integrates a novel crossover operator for avoiding stagnation. Experiments compare our proposal against 31 state-of-the-art algorithms, over 20 different medical datasets. Classification quality metrics reflect that our approach attains a robust and efficient behaviour with the majority of the datasets, obtaining highlighted results, such as an accuracy of 1.0 with the kidney dataset (bi-class) and 0.943 with the lung-cancer dataset (multi-class). Besides, it reaches adequate convergence rates and reasonable time consumption.},
  archive      = {J_ASOC},
  author       = {Matías Gabriel Rojas and Ana Carolina Olivera and Pablo Javier Vidal},
  doi          = {10.1016/j.asoc.2023.111192},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111192},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A genetic operators-based ant lion optimiser for training a medical multi-layer perceptron},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Behavioural three-way decision making with fermatean fuzzy
mahalanobis distance: Application to the supply chain management
problems. <em>ASOC</em>, <em>151</em>, 111182. (<a
href="https://doi.org/10.1016/j.asoc.2023.111182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Behavioural three-way decision making has superseded conventional two-way decision making as a result of a surge in risk and complexity of decision making. Additionally, uncertainty management is necessary for decision making issues. Thus, the goal of this study is to set up new three-way multi-attribute decision making (MADM) model, particularly targeting the following: (i) to properly deal with issues regarding ambiguity and interrelationship in MADM problems, (ii) to incorporate mindsets of decision maker in all possible aspects, (iii) to depict outcomes in terms of acceptance, suspension and rejection rather than just ranking when there are numerous elements present Firstly, the presence of ambiguity in assessment information is reflected by Fermatean fuzzy set . An advanced distance measure , the Fermatean fuzzy Mahalanobis distance , is devised in order to address the connectivity of attributes. Therefore, regret theory-based gain and loss scores are calculated to define the dominant and dominated relations. The conditional probability is then estimated using a new method based on the two relations discussed before. After that, the value functions associated to each object are derived based on closeness coefficient and prospect theory. Finally, prospect theory-based three-way decision rules are forwarded to classify and rank the objects. The application of the developed model is demonstrated through two case studies of supply chain management. Several experimental analyses are conducted under two substantial datasets taken from the KEEL database. The Spearman correlation coefficient (greater than 0.648) and hypothesis testing ( p p -value is less than 0.01) are employed to establish the validity and rationality of the proposed model. What is more, the error rate and modified error rate under two datasets are 6.71%, 16.85% and 6.35%, 20.65%, respectively, indicate that the proposed model is superior to other available models.},
  archive      = {J_ASOC},
  author       = {Arijit Mondal and Sankar Kumar Roy},
  doi          = {10.1016/j.asoc.2023.111182},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111182},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Behavioural three-way decision making with fermatean fuzzy mahalanobis distance: Application to the supply chain management problems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Image hiding using invertible neural network and similarity
of bits pairs. <em>ASOC</em>, <em>151</em>, 111180. (<a
href="https://doi.org/10.1016/j.asoc.2023.111180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective of image hiding is to conceal a secret image in a cover image inconspicuously. The challenges for image hiding focus on the embedding capacity, image quality and security. To contend with these challenges, this paper presents a novel image hiding method using Invertible Rescaling Net (IRN) and Similarity of Bits Pairs (SBP). Because of the downscaling ability of IRN, our method is able to hide a secret image into a cover image with the same dimensions. Moreover, the superior rescaling capabilities of IRN enable us to produce a recovered image of high-quality. We improve the SBP embedding technique and combine it with differential coding to boost the quality of the stego image . In the experiments, the quality of the recovered secret image and stego image is assessed using various quality evaluation metrics , and for the security, the anti-steganalysis performance is also evaluated in this paper. Extensive experimental results reveal that the quality of both the recovered secret image and the stego-image in our method outperforms several advanced methods and provides a high degree of security.},
  archive      = {J_ASOC},
  author       = {Ping Ping and Qianwen Li and Bobiao Guo and Feng Xu and Festus Sibanda and Yingchi Mao},
  doi          = {10.1016/j.asoc.2023.111180},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111180},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Image hiding using invertible neural network and similarity of bits pairs},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted centroids in adaptive nelder–mead simplex: With
heat source locator and multiple myeloma predictor applications.
<em>ASOC</em>, <em>151</em>, 111178. (<a
href="https://doi.org/10.1016/j.asoc.2023.111178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel approach for enhancing the Nelder–Mead Simplex method by utilizing the weighted mean of simplex vertices to efficiently determine the search direction towards optima. The Nelder–Mead algorithm is commonly used for solving unconstrained optimization problems through iterative direct search. While the Nelder–Mead algorithm excels in low-dimensional scenarios, its challenges in higher dimensions are addressed in our study. We reveal that the Nelder–Mead Simplex algorithm’s operations rely not only on the problem dimension but also on adaptive descending parameters for calculating the weighted centroid of vertices. The adaptive weights are determined based on the concept that generating a descending sequence depends on the fitness values of vertices. Consequently, the search direction tends to prioritize vertices with superior fitness values over others. Through extensive testing on renowned high-dimensional benchmark functions , the proposed algorithm outperforms both the standard and adaptive Nelder–Mead simplex algorithms in terms of performance demonstrating superior consistency and reliability. Furthermore, it is applied to an inverse heat source location problem formulated as a linear elliptic partial differential equation , serving as a large-scale real-world application. Additionally, the algorithm successfully addresses a neural network model solving a classification problem concerning the prediction of multiple myeloma, a type of bone marrow cancer, in patients, achieving an average accuracy of 92.3% with a minimal standard deviation of 0.13%. When the results obtained from eight different methods used in the study are compared in terms of performance in general, the proposed method is among the top three optimization algorithms . These real-world applications showcase the method’s versatility.},
  archive      = {J_ASOC},
  author       = {Korhan Günel},
  doi          = {10.1016/j.asoc.2023.111178},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111178},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted centroids in adaptive Nelder–Mead simplex: With heat source locator and multiple myeloma predictor applications},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive conjugate gradient accelerated evolutionary
algorithm for multi-objective spot optimization in cancer intensity
modulated proton therapy. <em>ASOC</em>, <em>151</em>, 111177. (<a
href="https://doi.org/10.1016/j.asoc.2023.111177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity-modulate proton therapy is one of the most advanced cancer treatment techniques due to the Bragg peak characteristics of proton radiation. The personalized demand of different patients requires treatment optimization methods to quickly provide diverse treatment plans to select the best plan for a patient. However, most existing treatment optimization methods are transformed the multi-objective optimization problem into a single optimization problem. Moreover, the radiation physicists may adjust the objective weights repeatedly to produce a set of high-quality treatment plans. To address this problem, this paper proposes an adaptive conjugate gradient accelerated evolutionary algorithm (ACG-EA) to generate a set of diverse high-quality treatment plans simultaneously. The conjugate gradient method is employed as a directional mutation operator to accelerate the search process in the hybrid mutation operation. In addition, the weight parameters of the conjugate gradient are automatically updated based on the diversity and convergence of the current population. Compared with five representative multi-objective evolutionary algorithms, the experimental results have shown the competitive performance of the proposed ACG-EA on the hypervolume and dose-volume histogram indicators in six clinical cancer cases.},
  archive      = {J_ASOC},
  author       = {Ruifen Cao and Xuesong Li and Wei Chen and Chao Wang and Langchun Si and Xi Pei and Xingyi Zhang},
  doi          = {10.1016/j.asoc.2023.111177},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111177},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive conjugate gradient accelerated evolutionary algorithm for multi-objective spot optimization in cancer intensity modulated proton therapy},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A local correlation estimation surrogate-assisted
bi-objective evolutionary algorithm for heterogeneous objectives.
<em>ASOC</em>, <em>151</em>, 111175. (<a
href="https://doi.org/10.1016/j.asoc.2023.111175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing evolutionary algorithms for solving bi-objective optimization problems are based on the assumption that both objectives can be evaluated once at the same time. However, in real-world optimization problems, there is a significant difference in function evaluation time between objectives, and these problems are defined as the problems with heterogeneous objectives. To utilize the latency of objectives, we propose a local correlation estimation based surrogate-assisted bi-objective evolutionary algorithm for problems with heterogeneous objectives. In the proposed algorithm, surrogate models are employed to approximate the objective functions. The proposed local correlation estimation ( L C E ) (LCE) method is used to analyze the correlation between objectives within a local region, guiding the search direction for one objective while identifying promising solutions for the other objective. Finally, the ablation experiment for the LCE method validates the effectiveness of the proposed strategy, and the comparative results on various expensive bi-objective test problems demonstrate that the proposed algorithm is promising on heterogeneous bi-objective optimization problems.},
  archive      = {J_ASOC},
  author       = {Chenyan Gu and Handing Wang},
  doi          = {10.1016/j.asoc.2023.111175},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111175},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A local correlation estimation surrogate-assisted bi-objective evolutionary algorithm for heterogeneous objectives},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using fuzzy transform for sustainable fake news detection.
<em>ASOC</em>, <em>151</em>, 111173. (<a
href="https://doi.org/10.1016/j.asoc.2023.111173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news has raised concerns regarding its detection, posing a significant challenge. Motivated by the ongoing discussion on the sustainability of machine learning algorithms , this paper discusses the usefulness of data reduction for fake news detection. This is accomplished by using the fuzzy transform (or F-transform for short), which has already been proven effective, in the literature, to reduce the training time. A Long Short Term Memory architecture is then employed for classification to determine the authenticity of the news. From the formal perspective, we discuss in general the role of the F-transform in a learning system . Regarding the numerical experiments, we use five publicly available datasets, trying different compression ratios and different types of F-transform, assessing accuracy, F1 score, training time and energy consumption with and without F-transform. Although the F-transform is a lossy compression technique, the results show a negligible variation in accuracy and F1-score when comparing results with and without F-transform (i.e. 1%–3% in most cases and around 10% in one case). This seems to be congruent with the theoretical achievement. Furthermore, the approach yields substantial training time and energy savings , with over 50% reduction in energy consumption.},
  archive      = {J_ASOC},
  author       = {Tayasan Milinda H. Gedara and Vincenzo Loia and Stefania Tomasiello},
  doi          = {10.1016/j.asoc.2023.111173},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111173},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Using fuzzy transform for sustainable fake news detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A three-way conflict analysis model with decision makers’
varying preferences. <em>ASOC</em>, <em>151</em>, 111171. (<a
href="https://doi.org/10.1016/j.asoc.2023.111171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preferences of decision makers (DMs) over conflicting states have a significant impact on the stability of the conflict system. However, the DM’s preferences over conflicting states are generally unclear and variable, as they are influenced by the unpredictability of future situations of nature and the psychological behavior of DMs. To deal with multi-issue conflict problems, since the interval grey number and three-way decisions can reveal the mechanism of DMs’ grey and varying preferences in uncertain natural situations, we exploit grey system theory, conflict analysis, and three-way decisions to establish a novel conflict analysis model with varying decision makers’ preferences, and a conflict case of industry-university-research innovation alliance verifies the efficacy and rationality of the model.},
  archive      = {J_ASOC},
  author       = {Qin Jiang and Yong Liu and Jin-hong Yi and Jeffrey Yi-Lin Forrest},
  doi          = {10.1016/j.asoc.2023.111171},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111171},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A three-way conflict analysis model with decision makers’ varying preferences},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive learning for unsupervised image-to-image
translation. <em>ASOC</em>, <em>151</em>, 111170. (<a
href="https://doi.org/10.1016/j.asoc.2023.111170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation (I2I) aims to learn a mapping function to transform images into different styles or domains while preserving their key structures. Typically, I2I models require manually defined image domains as a training set to learn the visual differences among the image domains and achieve the ability to translate images across them. However, constructing such multi-domain datasets on a large scale requires expensive data collection and annotation processes. Moreover, if the target domain changes or is expanded, a new dataset should be collected, and the model should be retrained. To address these challenges, this article presents a novel unsupervised I2I method that does not require manually defined image domains. The proposed method automatically learns the visual similarity between individual samples and leverages the learned similarity function to transfer a specific style or appearance across images. Therefore, the developed method does not rely on cost-intensive manual domains or unstable clustering results , leading to improved translation accuracy at minimal cost. For quantitative evaluation , we implemented a state-of-the-art I2I models and performed image transformation on the same input image using the baselines and our method. The image quality was then assessed using two quantitative metrics: Frechet inception distance (FID) and translation accuracy. The proposed method exhibited significant improvements in image quality and translation accuracy compared with the latest unsupervised I2I methods. Specifically, the developed technique achieved a 25% and 19% improvement over the best-performing unsupervised baseline in terms of FID and translation accuracy, respectively. Furthermore, this approach demonstrated performance nearly comparable to those of supervised learning-based methods trained using manually collected and constructed domains.},
  archive      = {J_ASOC},
  author       = {Hanbit Lee and Jinseok Seol and Sang-goo Lee and Jaehui Park and Junho Shim},
  doi          = {10.1016/j.asoc.2023.111170},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111170},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Contrastive learning for unsupervised image-to-image translation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). A locational false data injection attack detection method
in smart grid based on adversarial variational autoencoders.
<em>ASOC</em>, <em>151</em>, 111169. (<a
href="https://doi.org/10.1016/j.asoc.2023.111169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stealthy FDIA (False Data Injection Attack) is a serious cyber threat that can modify state estimation of smart grid through maliciously altering the measurement data , but can’t be detected by traditional bad data detection system in smart grid. There exist two weakpoints for numerous deep neural networks (DNNs) based data-driven schemes against FDIA. First, they mainly focus on detecting the presence of FDIA, but fail to localize the specific bus/nodes affected. Second, their performance is not sufficiently desirable under small attack, i.e., anomalies caused by the attack closely resemble normal data. To address the above issues, this paper proposes an effective locational FDIA detection framework based on data reconstruction, AT-GVAE, which seamlessly integrates the variational autoencoders (VAEs) and generative adversarial network paradigm. Specifically, our contributions are threefold. First, the proposed AT-GVAE framework is novelly composed of two main modules: the generative VAE_G and discriminative VAE_D that both play dual roles: reconstruct data from jointly learning distributions of data and latent feature space , and meanwhile play Minmax game with adversarial way. Second, multiple-layer gated recurrent units (GRUs) are utilized as the basic structure of the encoder and decoders in both VAEs, to characterize the temporal correlations of measurement data sequence. Additionally, the self-attention mechanism is used to enhance the expressive ability of GRU based VAEs. Then, the anomaly score for each busbar in the smart grid is determined by comparing the residual between the observed measurement and the outputs of VAE_G and VAE_D, enabling the localization of FDIA. Finally, thorough experiments on multiple power systems with series data of total 3456 measurements demonstrate that, in terms of multiple typical metrics, including false alarm rate vs. detection probability , AUC-ROC, and AUC-PR, our proposed framework outperforms the state-of-the-art GRU, VAE and adversarial training based FDIA detection schemes.},
  archive      = {J_ASOC},
  author       = {Yufeng Wang and Yangming Zhou and Jianhua Ma and Qun jin},
  doi          = {10.1016/j.asoc.2023.111169},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111169},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A locational false data injection attack detection method in smart grid based on adversarial variational autoencoders},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heuristics for online three-dimensional packing problems and
algorithm selection framework for semi-online with full look-ahead.
<em>ASOC</em>, <em>151</em>, 111168. (<a
href="https://doi.org/10.1016/j.asoc.2023.111168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online three-dimensional packing problems (3D-PPs), unlike offline problems, items arrive sequentially and require immediate packing decisions without any information about the quantities and sizes of the items to come. Heuristic methods are of great importance in solving online problems to find good solutions in a reasonable amount of time. However, the literature on heuristics for online problems is sparse. As our first contribution, we developed a pool of heuristics applicable to online 3D-PPs with complementary performance on different sets of instances. Computational results showed that in terms of the number of used bins, in all problem instances, at least one of our heuristics had a better or equal performance compared to existing heuristics in the literature. The developed heuristics are also fully applicable to an intermediate class between offline and online problems, referred to in this paper as a specific type of “semi-online with full look-ahead”, which has several practical applications. In this class, as in offline problems, complete information about all items is known in advance (i.e., full look-ahead); however, due to time or space constraints, as in online problems, items should be packed immediately in the order of their arrival. As our second contribution, we presented an algorithm selection framework, building on developed heuristics and utilizing prior information about items in this specific class of problems. We used supervised machine learning techniques to find the relationship between the features of problem instances and the performance of heuristics and to build a prediction model. The results indicate an 88% accuracy in predicting (identifying) the most promising heuristic(s) for solving any new instance from this class of problems.},
  archive      = {J_ASOC},
  author       = {Sara Ali and António Galrão Ramos and Maria Antónia Carravilla and José Fernando Oliveira},
  doi          = {10.1016/j.asoc.2023.111168},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111168},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Heuristics for online three-dimensional packing problems and algorithm selection framework for semi-online with full look-ahead},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature-recombinant asynchronous deep reservoir computing
for modeling time series data. <em>ASOC</em>, <em>151</em>, 111167. (<a
href="https://doi.org/10.1016/j.asoc.2023.111167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling time series data is an important issue in many areas. Reservoir computing (RC) is a promising tool to build time series model due to its dynamic characteristic and simple training way. Asynchronous deep reservoir computing (ADRC) is an improvement version of the traditional RC. It solves time-dependent tasks more efficiently than traditional RC because of its rich dynamics and flexible short-term memory (STM). Nevertheless, it has been an open issue to design RC’s or ADRC’s reservoir topology owing to large amounts of random factors. To promote the solution of this problem, the paper proposes a feature-recombinant ADRC (FR-ADRC) for modeling time series data. In the FR-ADRC scheme, the first sub-reservoir is designed as the feature-adaptive layer, and a trainable matrix C is introduced into this layer. By learning C , the singular value (SV) distribution of the first layer could be adjusted. Further, the principal components of the reservoir topology can be extracted by the principal component analysis (PCA). Then a new temporary reservoir is constructed by recombining these extracted components. The subsequent information processing is carried out based on the recombinant reservoir, which can be adaptive to the input signals. The validity of the FR-ADRC is tested by modeling some numerical and real-life time series data. Experimental results show that the proposed approach is better than the traditional ADRC in modeling precision, generalization ability and stability.},
  archive      = {J_ASOC},
  author       = {Ying-Chun Bo and Jun Wang},
  doi          = {10.1016/j.asoc.2023.111167},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111167},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature-recombinant asynchronous deep reservoir computing for modeling time series data},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A light-weight skeleton human action recognition model with
knowledge distillation for edge intelligent surveillance applications.
<em>ASOC</em>, <em>151</em>, 111166. (<a
href="https://doi.org/10.1016/j.asoc.2023.111166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton based human action recognition has evolved as one of the most important applications in multimedia IoT system. However, it requires extensive computation resource including high performance computing unites and large memory to train a deep mode with large number of parameters, which seriously limits it effectiveness and efficiency for edge intelligence multimedia IoT applications . In this paper, a knowledge distillation based light-weight deep model is proposed for skeleton human action recognition to meet the edge multimedia IoT applications. It can get competitive recognition performance in terms of learning accuracy for combination of AI model and edge surveillance equipment. On the one hand, to achieve desirable accuracy, we propose a deep pose-transition image representation method based on two-stream spatial–temporal architecture, which can mine the hidden features of color texture images in spatial and temporal domain, and fuse them for comprehensive discrimination before final classification. On the other hand, to increase the transfer learning ability to the student model on the edge device, we use tucker decomposition to weak the teacher model during knowledge transfer learning process. Finally, in order to validate the effectiveness of our proposal, we conducted extensive experiments to evaluate the proposed approach. The experimental results demonstrate that our proposal can realize deep model miniaturization to meet the requirement of edge multimedia IoT system and achieve the competitive performance.},
  archive      = {J_ASOC},
  author       = {Cheng Dai and Shoupeng Lu and Chuanjie Liu and Bing Guo},
  doi          = {10.1016/j.asoc.2023.111166},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111166},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A light-weight skeleton human action recognition model with knowledge distillation for edge intelligent surveillance applications},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systems engineering issues for industry applications of
large language model. <em>ASOC</em>, <em>151</em>, 111165. (<a
href="https://doi.org/10.1016/j.asoc.2023.111165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language model (LLM) is an important direction in the development of AGI , but its technology is still in rapid change, and its capabilities still have obvious deficiencies and imbalances, with persistent problems such as hallucination, value non-alignment, weak specialization, and black-box effect. In this case, how to apply LLM to different professional fields and develop high-quality AIGC industry applications has become a great challenge for ISVs . Building AIGC industry applications based on LLM is not simply a matter of functional realization. Although researchers and open-source communities have proposed numerous application development frameworks or tool components, there is a lack of overall architecture design for systems engineering and a lack of discussion on theories and methods of LLM application development in large-scale industry domains, such as healthcare, government affairs, finance, and media. This paper analyzes the basic ideas of LLM industry applications development, defines the functional requirements and feature requirements of LLM industry applications, puts forward the concept of Large Language Model Systems Engineering (LLM-SE), and develops an AI assisted clinical risk prediction system for amyloidosis disease based on the architecture of LLM-SE, which adopt knowledge engineering , quality engineering , etc., and verifies the LLM-SE development architecture and methodology.},
  archive      = {J_ASOC},
  author       = {Wang Chen and Liu Yan-yi and Guo Tie-zheng and Li Da-peng and He Tao and Li Zhi and Yang Qing-wen and Wang Hui-han and Wen Ying-you},
  doi          = {10.1016/j.asoc.2023.111165},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111165},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Systems engineering issues for industry applications of large language model},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overcoming graph topology imbalance for inductive and
scalable semi-supervised learning. <em>ASOC</em>, <em>151</em>, 111164.
(<a href="https://doi.org/10.1016/j.asoc.2023.111164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based semi-supervised learning (GSSL) has received much attention recently. Despite some progress made in this area by some recent methods, some limitations still need to be addressed. Namely, there are two main shortcomings. First, the graphs used are very often built in advance and independently of the task at hand, using a heuristic, and generally do not represent the true topology of the data. The second shortcoming is the ability of the model to handle a very large number of unlabeled samples . This can make the GSSL solution impractical from a computational resource perspective. In this paper, we propose the Weighted Simultaneous Graph Construction and Reduced Flexible Manifold Embedding (W-SGRFME) method, which is a scalable and inductive GSSL framework. The main contributions are as follows. First, we extend the concept of graph topology imbalance to large datasets. Second, we integrate the computed weights of the labeled samples into the unified semi-supervised model. The latter jointly estimates the labels of the unlabeled samples, the mapping of the feature space to the label space, and the graph matrix of the anchor graph. Moreover, the fusion of labels and features of anchors is used to adaptively construct the graph. Experimental results on three large datasets from semi-supervised learning show the effectiveness of the proposed scalable method . These datasets are NORB, RCV1, and Covtype. Experimental results on large datasets show the superiority of the proposed method over existing scalable models.},
  archive      = {J_ASOC},
  author       = {F. Dornaika and Z. Ibrahim and A. Bosaghzadeh},
  doi          = {10.1016/j.asoc.2023.111164},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111164},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Overcoming graph topology imbalance for inductive and scalable semi-supervised learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). F-TLBO-ID: Fuzzy fed teaching learning based optimisation
algorithm to predict the number of k-barriers for intrusion detection.
<em>ASOC</em>, <em>151</em>, 111163. (<a
href="https://doi.org/10.1016/j.asoc.2023.111163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring fast and efficient Intrusion Detection and Prevention (IDP) at international borders is crucial for maintaining security and safeguarding nations. In this study, we propose an innovative approach that harnesses the power of machine learning and Wireless Sensor Networks (WSNs) to achieve faster and more accurate IDP. Our novel Fuzzy fed Teaching Learning Based Optimisation regression algorithm (F-TLBO-ID) revolutionises the prediction of the required number of k k -barriers for rapid IDP. To develop and validate our approach, we synthetically generated pertinent features using Monte-Carlo simulations. These features encompass essential parameters such as the concerned region’s area, effective transmission range, effective sensing range, number of sensor nodes , and the fading parameter. Training the F-TLBO-ID algorithm with these features yielded exceptional results, accurately predicting the required number of k k -barriers with an impressive correlation coefficient (R = 0.99), minimal Root Mean Square Error (RMSE = 11.32), and negligible bias (−3.66). To benchmark the performance of our F-TLBO-ID algorithm, we conducted comprehensive comparisons with fine-tuned benchmark algorithms, including AutoML , GPR , GRNN , RF, RNN , SVM , and ANN . Additionally, we evaluated the algorithm against 11 different variants of nature-inspired algorithms. Remarkably, our F-TLBO-ID algorithm outperformed all these methods in terms of accuracy, firmly establishing its superiority. Finally, we validated the performance of the F-TLBO-ID algorithm using publicly available datasets. The results were highly satisfactory, exhibiting a strong correlation coefficient (R = 0.84), acceptable RMSE (36.24), and minimal bias (−7.17). This study offers a robust and reliable algorithm to predict the required barriers for fast IDP, surpassing the accuracy of existing benchmark algorithms. By implementing our proposed algorithm, the efficiency of IDP systems at international borders can be significantly improved, ultimately enhancing security and facilitating smooth border operations.},
  archive      = {J_ASOC},
  author       = {Abhilash Singh and Seyed Muhammad Hossein Mousavi and Jaiprakash Nagar},
  doi          = {10.1016/j.asoc.2023.111163},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111163},
  shortjournal = {Appl. Soft. Comput.},
  title        = {F-TLBO-ID: Fuzzy fed teaching learning based optimisation algorithm to predict the number of k-barriers for intrusion detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Colony-based search algorithm for numerical optimization.
<em>ASOC</em>, <em>151</em>, 111162. (<a
href="https://doi.org/10.1016/j.asoc.2023.111162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficiency of an Evolutionary Algorithm is highly sensitive to the mutation and crossover operators it possesses, as well as to the strategy used for determining the direction of numerical evolution and the values of evolutionary step sizes. There is no analytical method to efficiently define the direction of numerical evolution and the value of evolutionary step size for EAs. The efficiency of EAs&#39; search processes is also influenced by their ability to maintain numerical diversity within the population. This paper introduces the Colony-Based Search Algorithm (CSA). The development of CSA was motivated by the scientific and industrial need for a relatively more efficient EA. CSA possesses relatively more efficient artificial genetic operators and strategies for producing evolutionary direction and step size, and the ability to maintain numerical diversity. CSA generates the Clan Matrix containing the pattern vectors to be evolved in the current iteration by randomly selecting pattern vectors from the Colony Matrix at the beginning of each iteration. This makes it easier for CSA to maintain numerical diversity among pattern vectors for a long time. CSA&#39;s mutation method includes three randomly blended components with different properties. The problem-solving performance of CSA is statistically compared with the problem-solving performance of eight popular evolutionary search methods (i.e., SADE, SHADE, LSHADE, COBIDE, JADE, CK, GWO, and SFS) by using benchmark functions of CEC&#39;2017 and CEC’2022. In the experiments, the 3D viewshed analysis was addressed as a real-world problem, employing the CSA. The statistical analyses conducted on the experimental results indicate that CSA performs relatively better than the compared methods to solve numerical problems.},
  archive      = {J_ASOC},
  author       = {Pinar Civicioglu and Erkan Besdok},
  doi          = {10.1016/j.asoc.2023.111162},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111162},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Colony-based search algorithm for numerical optimization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A memetic based algorithm for simultaneous preventive
maintenance scheduling and spare-parts inventory management for
manufacturing systems. <em>ASOC</em>, <em>151</em>, 111161. (<a
href="https://doi.org/10.1016/j.asoc.2023.111161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To increase machine availability, it may be necessary to halt them to replace components before failures occur. Therefore, optimizing spare parts inventory management is a key factor in improving maintenance activities. Spare part costs are a significant contributor to total maintenance costs, including holding and ordering costs, and are often dependent on the maintenance schedule. Hence, it is important to optimize preventive maintenance scheduling and spare parts inventory management together. The first attempt to introduce a new Mixed-Integer Linear Programming (MILP) model that integrates both decisions was by Mjirda et al. (2016). Our paper is the first to extend their work in order to come up with an efficient aided decision making framework based on a mat-heuristic approach that delivers a good trade-off between computational time and solution quality. Indeed, this paper introduces a novel Hybrid Memetic Algorithm (HMA) that synergistically combines Memetic Algorithm (MA) with MILP to solve this NP-hard problem. The paper aims to provide a unified and efficient mat-heuristic based aided decision making framework that optimizes the sum of maintenance, operating, holding, and ordering costs by synchronizing preventive maintenance schedules and spare parts inventory for a set of independent machines. Its efficiency is illustrated through an extensive computational study. The HMA outperforms standalone MILP solvers, solving 36% of instances to optimality within a computational time of less than 141 s, and achieving a maximum gap of 5% for 46% of instances within less than 200 s. Our findings highlight the computational efficiency and solution quality offered by the HMA, thereby advancing the state-of-the-art in integrated spare parts inventory management and preventive maintenance scheduling.},
  archive      = {J_ASOC},
  author       = {Sohaib Afifi and Mustapha Hrouga and Anis Mjirda and Hamid Allaoui},
  doi          = {10.1016/j.asoc.2023.111161},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111161},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A memetic based algorithm for simultaneous preventive maintenance scheduling and spare-parts inventory management for manufacturing systems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule extraction based on PROMETHEE-assisted multi-objective
genetic algorithm for generating interpretable neural networks.
<em>ASOC</em>, <em>151</em>, 111160. (<a
href="https://doi.org/10.1016/j.asoc.2023.111160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rule extraction from artificial neural networks (ANNs) refers to the process of identifying the underlying decision-making logic used by the network in its predictions. This involves converting the complex, non-linear relationships between inputs and outputs in the network into a set of interpretable rules. The main objective of rule extraction is to enhance the interpretability of decision-making in artificial neural networks (ANNs) and make it more understandable for human experts, especially in critical domains like healthcare, where clear decision-making is essential. Despite the ability of machine learning methods to interpret ANNs, interpreting complex neural networks remains limited, as most studies tend to sacrifice one criterion, related to rule quality, in favor of another. Rule extraction methods applied directly to complex neural networks may produce overly specific rules that are tailored to the training data . Such rules can suffer from overfitting and may not generalize well to unseen data. Also, The use of multiobjective genetic algorithms for rule extraction can result in a large number of rules, making it challenging for decision-makers to interpret them. This study proposes an innovative approach to address challenges in rule extraction from complex artificial neural networks (ANNs) by integrating a multiobjective genetic algorithm with the PROMETHEE method. By integrating these techniques, the study aims to identify meaningful relationships between inputs and outputs of ANNs, generating concise and reliable rules that are ranked based on support, confidence, and lift values. The use of PROMETHEE&#39;s rule ranking mechanism enabled the consideration of rule priority during candidate selection, guiding the genetic algorithm towards discovering high-quality rule sets and efficiently exploring the search space . This approach not only enhances the quality of extracted rules but also facilitates efficient decision-making by striking a balance between rule accuracy, fidelity, and comprehensibility , thus contributing to advancing the understanding of complex neural networks. By equally weighting the criteria (support, confidence, and lift) during the rule ranking mechanism, our approach achieved high coverage rates ranging from 94.88 % to 100 % and generated a manageable number of rules that could be easily interpreted by human experts, ranging from 5 to 10.5. These findings demonstrate the potential of our approach to significantly improve decision-making accuracy and interpretability across various real-world applications, making it a promising tool for applications in healthcare, finance, and other fields.},
  archive      = {J_ASOC},
  author       = {Dounia Yedjour and Hayat Yedjour and Mohammed Bilel Amri and Adlania Senouci},
  doi          = {10.1016/j.asoc.2023.111160},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111160},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Rule extraction based on PROMETHEE-assisted multi-objective genetic algorithm for generating interpretable neural networks},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evolutionary deep learning for computer vision and image
processing. <em>ASOC</em>, <em>151</em>, 111159. (<a
href="https://doi.org/10.1016/j.asoc.2023.111159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Harith Al-Sahaf and Pablo Mesejo and Ying Bi and Mengjie Zhang},
  doi          = {10.1016/j.asoc.2023.111159},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111159},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary deep learning for computer vision and image processing},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A 3D monte carlo tree search method for railway alignment
optimization. <em>ASOC</em>, <em>151</em>, 111158. (<a
href="https://doi.org/10.1016/j.asoc.2023.111158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway alignment design is an important process, which fundamentally affects the construction, operation and maintenance of a railway. However, optimizing an alignment is challenging due to, e.g., the usually huge search space , infinite number of possible alternatives and numerous constraints. To address this problem, we propose a three-dimensional Monte Carlo Tree Search (3D-MCTS) method for alignment optimization. Specifically, a time-varying selection approach is first designed for efficiently exploring the search space. Then, the feasible search space is dynamically delineated with a customized tree expansion operator to accelerate the search process. In addition, a simulation strategy with global reward estimation is proposed to balance global exploration and local exploitation during optimization, which contributes to enhancing the quality of the optimized alignment. Finally, the proposed 3D-MCTS is applied to a complex real-world railway case. It shows that the 3D-MCTS can find better solutions compared to the best alignments that are manually designed by experienced engineers or produced by a previous distance transform algorithm. Two sensitivity analyses also reveal the 3D-MCTS&#39;s performance and robustness with respect to cost optimization and search efficiency.},
  archive      = {J_ASOC},
  author       = {Guanghui Wang and Hao Pu and Taoran Song and Paul Schonfeld and Wei Li and Hong Zhang and Lihui Peng and Jianping Hu and Junfei Qiao},
  doi          = {10.1016/j.asoc.2023.111158},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111158},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A 3D monte carlo tree search method for railway alignment optimization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Uncertain linear programming with cloud set constraints
integrating fuzziness and randomness. <em>ASOC</em>, <em>151</em>,
111157. (<a href="https://doi.org/10.1016/j.asoc.2023.111157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reality, there is a significant need for processing methods of uncertain programming problems with fuzziness and randomness. Although there are many extensions of fuzzy sets to handle uncertain information, it is rare to organically integrate randomness and fuzziness, such as the cloud set which is the set extension of cloud model. To address the limitations of uncertain linear programming, which lacks consideration of fuzziness and randomness, a new approach called uncertain linear programming with cloud set constraints is proposed. This approach integrates both fuzziness and randomness properties. Firstly, the definition of uncertain linear programming model with cloud set constraints integrating fuzzy and randomness properties is given. Secondly, the construction method of cloud set membership function of uncertain linear programming model is studied. Thirdly, the solution method for uncertain linear programming model based on I I operation and P P operation in cloud set is argued. Finally, the effectiveness of the method is verified through a specific application example and a comparison case with other linear programs . This method of uncertain linear programming with cloud set constraints utilizes the normal cloud set to represent the objective and conditional constraints, effectively capturing the fuzziness and randomness inherent in these constraints. The contribution of this study is that it proposes the use of random membership degree to replace the subjective fixed single membership in the constraints of linear programming. The membership degree of normal cloud set is expressed by a normal random number with a stable tendency, which integrates fuzziness and randomness with a regular, discrete, sparse thickness instead of a fixed curve. It can more effectively express and adapt to the inherent uncertainty of the objective world because various variables in natural science and social science typically follow a normal distribution. So, it has good objectivity and universality.},
  archive      = {J_ASOC},
  author       = {Hongli Wang and Yuqiang Feng and Liguo Fei},
  doi          = {10.1016/j.asoc.2023.111157},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111157},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Uncertain linear programming with cloud set constraints integrating fuzziness and randomness},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Virtual special issue on quantum inspired soft computing for
intelligent data processing guest editorial. <em>ASOC</em>,
<em>151</em>, 111156. (<a
href="https://doi.org/10.1016/j.asoc.2023.111156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ASOC},
  author       = {Siddhartha Bhattacharyya and Debashis De and Sergey Gorbachev and Debanjan Konar},
  doi          = {10.1016/j.asoc.2023.111156},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111156},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Virtual special issue on quantum inspired soft computing for intelligent data processing guest editorial},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Credibilistic portfolio optimization with higher-order
moments using coherent triangular fuzzy numbers. <em>ASOC</em>,
<em>151</em>, 111155. (<a
href="https://doi.org/10.1016/j.asoc.2023.111155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial portfolio formation is usually a multi-objective decision-making problem concerning return and risk on the investment. In this study, we make use of an extension of regular triangular fuzzy numbers, known as coherent triangular fuzzy numbers, to describe portfolio returns in a credibility-based framework. The paper’s novelty lies in proposing and deriving a crisp equivalent for computing the coherent triangular fuzzy number-based credibilistic semivariance, credibilistic skewness, and credibilistic semikurtosis. Utilizing these analytical expressions, along with additional formulations found in existing literature, we present three multi-objective portfolio optimization models involving the practical constraints related to investment decisions. All the proposed analytical expressions, when used with the returns of the portfolio as a whole, help to overcome the computationally expensive process of simulating results using the returns of individual assets. The proposed models differ with respect to different risk measures, viz. semivariance, Mean-Absolute-Semi-Deviation (MASD), and Conditional Value-at-Risk (CVaR). These models are solved using an adaptation of an efficient Multi-Objective Genetic Algorithm (MOGA) specifically designed to solve portfolio optimization problems with practical constraints. Data from the National Stock Exchange (NSE) in Mumbai, India and the New York Stock Exchange (NYSE) in New York, USA, are used to demonstrate the effectiveness of the proposed portfolio optimization models and solution methodology. All the proposed models are compared with respect to each other and the benchmarks considered in this study to bring out the performance stability.},
  archive      = {J_ASOC},
  author       = {Pawan Kumar Mandal and Manoj Thakur and Garima Mittal},
  doi          = {10.1016/j.asoc.2023.111155},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111155},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Credibilistic portfolio optimization with higher-order moments using coherent triangular fuzzy numbers},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-objective crowd-aware robot navigation system using
deep reinforcement learning. <em>ASOC</em>, <em>151</em>, 111154. (<a
href="https://doi.org/10.1016/j.asoc.2023.111154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Navigating efficiently and safely through human crowds is essential for mobile robots in diverse applications such as delivery services, home assistance, healthcare, and manufacturing. However, traditional navigation methods are adversely affected by the high randomness of human movements, seriously hindering robot navigation in crowd environments. To tackle these problems, this paper proposes a deep reinforcement learning-based multi-objective crowd-aware robot navigation system called Multi-Objective Dual-Selection Reinforcement Learning (MODSRL). To deal with multiple objectives, including safety, time efficiency, collision avoidance , and path smoothness during navigation, a set of reward functions is used in MODSRL. To address the challenge of hesitation at the beginning when navigating in a crowd environment, a Dual-Selection Attention Module is developed, which enables the robot to make efficient decisions while reducing hesitation. Experimental results demonstrate that the proposed MODSRL outperforms existing approaches in terms of five different metrics. In particular, the average success rate of the proposed MODSRL outperforms ERVO, CADRL, LSTM-RL, and OM-SARL algorithms by 17.1%, 26.4%, 16.6%, and 9.2%, respectively, sufficing to show its robustness in complex crowd environments.},
  archive      = {J_ASOC},
  author       = {Chien-Lun Cheng and Chen-Chien Hsu and Saeed Saeedvand and Jun-Hyung Jo},
  doi          = {10.1016/j.asoc.2023.111154},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111154},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-objective crowd-aware robot navigation system using deep reinforcement learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Actor–critic learning based PID control for robotic
manipulators. <em>ASOC</em>, <em>151</em>, 111153. (<a
href="https://doi.org/10.1016/j.asoc.2023.111153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a reinforcement learning structure for auto-tuning PID gains by solving an optimal tracking control problem for robot manipulators . Capitalizing on the actor–critic framework implemented by neural networks , we achieve optimal tracking performance, estimating unknown system dynamics simultaneously. The critic network is used to approximate the cost function , which serves as an indicator of control performance. With feedback from the critic, the actor network learns time-varying PID gains over time to optimize control input, thereby steering the system toward optimal performance . Furthermore, we utilize Lyapunov’s direct method to demonstrate the stability of the closed-loop system. This approach provides an analytical procedure for a stable robot manipulator system to systematically adjust PID gains, bypassing the ad-hoc and painstaking process. The resultant actor–critic PID-like control exhibits stable adaptive and learning capabilities while maintaining a simple structure and inexpensive online computational demands. Numerical simulations underscore the effectiveness and advantages of the proposed actor–critic neural network PID control .},
  archive      = {J_ASOC},
  author       = {Hamed Rahimi Nohooji and Abolfazl Zaraki and Holger Voos},
  doi          = {10.1016/j.asoc.2023.111153},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111153},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Actor–critic learning based PID control for robotic manipulators},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modified mamdani-fuzzy inference system for predicting the
cost overrun of construction projects. <em>ASOC</em>, <em>151</em>,
111152. (<a href="https://doi.org/10.1016/j.asoc.2023.111152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cost overruns are a common worldwide problem in the construction industry; improved proactive risk management and cost control are much needed. Several models have been proposed, but all have weaknesses, particularly in data demands and the severity of critical risks or uncertainties associated with expert judgment . In response, this study develops a new 3-part model based on the Mamdani-type fuzzy inference system (FIS) to predict the cost overrun of construction projects. The first part assesses the weight of each expert, evaluating the severity of cost overrun factors. The second part contains a list of 40 in-built cost overrun factors and their degree of severity, while the third part establishes the relationships of every factor’s occurrence probability and severity to predict the cost overrun of a specific project . The severity of each factor is assessed based on a survey of 31 randomly selected experts in the Saudi Arabian construction industry. The model is demonstrated on two completed projects in Saudi Arabia. For each project, this involves a group of project-based experts rating the probability of occurrence of each factor on that project and applying this to the factor severity list to obtain a predicted cost overrun (PCO) for the whole project. The model is validated for robustness by sensitivity analysis comparing the predicted and actual whole project cost overrun and shown to be of practical value in assessing critical risks and predicting the likely amount of cost overrun. The model is equally applicable in the early project stages.},
  archive      = {J_ASOC},
  author       = {Yaman Saeid Al-Nahhas and Laith A. Hadidi and Muhammad Saiful Islam and Martin Skitmore and Ziyad Abunada},
  doi          = {10.1016/j.asoc.2023.111152},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111152},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Modified mamdani-fuzzy inference system for predicting the cost overrun of construction projects},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fuzzy self-consistent clustering ensemble. <em>ASOC</em>,
<em>151</em>, 111151. (<a
href="https://doi.org/10.1016/j.asoc.2023.111151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subject of clustering ensemble (CE) has emerged as a pivotal area of exploration within unsupervised learning , primarily due to its remarkable capacity to integrate multiple base clusterings. However, the conventional CE framework exhibits limitations in effectively handling fuzzy relations and ensuring robustness. This study introduces an innovative and robust fuzzy self-consistent clustering ensemble (FSCE) model. Departing from customary approaches to processing base clustering outcomes, the FSCE model considers the scalable dummy variable representation of base clustering results as a novel feature attributes intrinsic to the original dataset. Subsequently, a γ γ - fuzzy operator ε + ε+ is formulated, enabling the adjustment of coupling strength contingent upon the uncertainties inherent in practical problems. Leveraging this operator, the model reappraises the fuzzy relationships among objects, thereby engendering a corresponding relation matrix. This matrix profoundly enhances the model&#39;s capacity to contend with uncertain relationships. The resultant relation matrix assumes a pivotal role as the primary input in the consensus function, culminating in the derivation of the ensemble outcome. Moreover, the FSCE model deviates from the conventional CE paradigm by introducing a reallocation strategy for fuzzy objects within ensemble outcome. Through comprehensive experimentation, we substantiate that the introduction of γ γ - fuzzy operator ε + ε+ offers a viable novel approach to enhancing both the overall performance and the fuzzy interpretability of the CE model. The FSCE model distinctly excels in consensus formation and robustness when juxtaposed against eight archetypal and state-of-the-art clustering models.},
  archive      = {J_ASOC},
  author       = {Yunxiao Shan and Shu Li and Fuxiang Li and Yuxin Cui and Shuai Li and Minghua Chen and Xunjun He},
  doi          = {10.1016/j.asoc.2023.111151},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111151},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Fuzzy self-consistent clustering ensemble},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristic learning algorithms for accurate prediction of
hydraulic performance of porous embankment weirs. <em>ASOC</em>,
<em>151</em>, 111150. (<a
href="https://doi.org/10.1016/j.asoc.2023.111150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A porous weir is an environmentally friendly structure with minimal negative environmental impact. Due to the complex flow mechanism around porous weirs, it is difficult to provide a general empirical relationship to estimate the free and submerged discharge coefficients and threshold submergence or modular limit. Hence, the current study attempts to introduce an efficient alternative approach leveraging on multilayer perceptron (MLP) coupled with generalized normal distribution optimization (GNDO), Runge-Kutta optimization algorithm (RUN), slime mould algorithm (SMA), whale optimization algorithm (WOA), and grey wolf optimizer (GWO). These models&#39; robustness and accuracy are investigated to solve the free discharge coefficient (FDC), modular limit index (MLI), and submerge discharge reduction factor (SDRF) of the porous embankment weirs (PEWs) modeling problem where 6514 datasets are studied experimentally for free, modular limit, and submerged flow conditions . The performance of developed metaheuristic-MLP models is compared with a gradient algorithm-based MLP, namely Levenberg-Marquardt (LM) model. The obtained results from analysis of different performance indicators demonstrate that the GNDO-MLP outperforms other developed MLP models. The results demonstrate that the metaheuristic-based MLP models are more reliable in forecasting FDC, MLI, and SDRF of PEWs than the ML-MLP model. To measure how much the MLP model improved when combined with meta-heuristic algorithms, an aggregated statistical index called objective function was applied. The results of this index showed that the MLP model with GNDO performed better than the MLP models with WOA, SMA, GWO, RUN, and LM by 82%, 66%, 69%, 69%, and 96% for FDC, by 41%, 13%, 9%, 8%, and 48% for MLI, and by 64%, 20%, 8%, 4%, and 69% for SDRF condition, respectively. Furthermore, the results of the uncertainty analysis, using the P-factor and R-factor, demonstrated that GNDO-MLP had lower uncertainty compared to other models. The robustness of the GNDO-MLP model indicates that it can be used for further research on the probabilistic analysis of the PEWs.},
  archive      = {J_ASOC},
  author       = {Mostafa Rahmanshahi and Jafar Jafari-Asl and Manoochehr Fathi-Moghadam and Sima Ohadi and Seyedali Mirjalili},
  doi          = {10.1016/j.asoc.2023.111150},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111150},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic learning algorithms for accurate prediction of hydraulic performance of porous embankment weirs},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Information fusion and attribute reduction for multi-source
incomplete mixed data via conditional information entropy and d-s
evidence theory. <em>ASOC</em>, <em>151</em>, 111149. (<a
href="https://doi.org/10.1016/j.asoc.2023.111149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source incomplete mixed data abound in real life, like medical data, biological data, remote sensing data , military data, etc. However, some of these sources are of less importance than others, and some are essentially worthless. Therefore, their information fusion and attribute reduction face many challenges. This paper studies information fusion for multi-source incomplete mixed data via conditional information entropy (CIE) and considers its application to attribute reduction based on D-S evidence theory. First of all, a new distance function is defined to measure the difference between nominal attribute values with missing information , and the neighborhood rough set model is used to establish the granularity structure of multi-source incomplete mixed data. Then, a source selection method is given via CIE, which is used to fuse multi-source incomplete mixed data into single-source incomplete mixed data. Based on the maximization of CIE, this method allows worthy and reliable information sources to be chosen. Next, the connection between neighborhood rough set model and D-S evidence theory is established. Moreover, two attribute reduction algorithms for the fused incomplete mixed data are proposed based on the belief and plausibility. Finally, experiments are done to verify the effectiveness of the proposed fusion and reduction algorithms. The results of experiment and statistical test on 12 datasets show that the proposed algorithms exceed other advanced algorithms in classification performance.},
  archive      = {J_ASOC},
  author       = {Zhaowen Li and Qinli Zhang and Suping Liu and Yichun Peng and Lulu Li},
  doi          = {10.1016/j.asoc.2023.111149},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111149},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Information fusion and attribute reduction for multi-source incomplete mixed data via conditional information entropy and D-S evidence theory},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early neurological deterioration detection with a
transformer convolutional auto-encoder model. <em>ASOC</em>,
<em>151</em>, 111148. (<a
href="https://doi.org/10.1016/j.asoc.2023.111148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an improved transformer convolutional auto-encoder model combined with the exponentially weighted moving average (EWMA) control chart to detect early neurological deterioration (END) of ischemic stroke patients after endovascular therapy in advance. In the proposed method, the transformer convolutional auto-encoder is used to extract crucial features of multivariate clinical monitoring time series data and obtain the reconfiguration loss while EWMA control chart is utilized to monitor the derived reconfiguration loss and identify anomalies. To verify the feasibility and effectiveness of the proposed END detection approach, multivariate clinical monitoring time series data of ischemic stroke patients in the neurocritical care unit from Beijing Tiantan hospital are collected. Meanwhile, the proposed approach is benchmarked with seven state-of-the-art models. The computation results show that the proposed approach achieves the best performance with the lowest false alarm rate and the highest detection rate. Therefore, the proposed END detection model is practical to guide doctors in conducting clinical interventions in advance to prevent deterioration in patients with ischemic stroke.},
  archive      = {J_ASOC},
  author       = {Jinxu Yang and Ximing Nie and Long Wang and Chao Huang and Liping Liu},
  doi          = {10.1016/j.asoc.2023.111148},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111148},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Early neurological deterioration detection with a transformer convolutional auto-encoder model},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative adversarial network based on poincaré distance
similarity constraint: Focusing on overfitting problem caused by finite
training data. <em>ASOC</em>, <em>151</em>, 111147. (<a
href="https://doi.org/10.1016/j.asoc.2023.111147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks face harsh opposition between training data and model performance. In facing insufficient training data, the training process is extremely unstable, and the modules are difficult to converge, which increases the risk of model collapse, and finally manifests itself as poor image quality generated by the model. The essence of this problem is that a single discriminator is prone to overfitting. However, the traditional methods are to perform data augmentation , optimize the loss function, or improve the training strategy. In this paper, starting from the structural characteristics of the model itself, a dual-ways model with internal interactive learning and synchronous training is designed. The dual-ways discriminator directly maximizes the Poincaré distance similarity, which is used to enhance the effectiveness of the error gradient and eliminate the risk of discriminator overfitting. Experimental results show that the dual-way model produces higher-quality images than the six well-known schemes in four benchmark datasets. The experimental results also show that thanks to the mechanism of internal interactive learning, the training process is more stable and the discriminator module converges in a timelier manner, which is the key to improving the performance of the model. Thus, we obtain a new stronger model that is different from the existing strategy.},
  archive      = {J_ASOC},
  author       = {Jian Wei and Qinzhao Wang and Zixu Zhao},
  doi          = {10.1016/j.asoc.2023.111147},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111147},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Generative adversarial network based on poincaré distance similarity constraint: Focusing on overfitting problem caused by finite training data},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human activity recognition through deep learning: Leveraging
unique and common feature fusion in wearable multi-sensor systems.
<em>ASOC</em>, <em>151</em>, 111146. (<a
href="https://doi.org/10.1016/j.asoc.2023.111146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the progress in IoT and AI technologies, multi-sensor fusion for human activity recognition (HAR) has garnered considerable attention. As a result of integrating diverse information from different sensors, individuals employ sensors to monitor their daily activities. However, identifying crucial features for classification and assigning suitable weights to sensors is a complex task. On the other side, varied data structures pose challenges in establishing a unified format for the fusion of diverse data. To address these challenges, this paper presents UC Fusion, a method focusing on the fusion of unique and common features in wearable multi-sensor systems for HAR. First, UC Fusion merges the unique feature of each sensor with the common features of all sensors. Second, it tackles the challenge of handling heterogeneous data by unifying the data format through segmentation and dimensional transformation. Extensive experiments on the UCI HAR and WISDM datasets were conducted to evaluate UC Fusion’s performance. The results demonstrate that our proposed method secured an average recognition accuracy of 96.84% and 98.85%. Furthermore, ablation studies were performed on each module of UC Fusion to assess their impact on accuracy, confirming the effectiveness of the proposed method.},
  archive      = {J_ASOC},
  author       = {Kang Liu and Chang Gao and Binbin Li and Wenyuan Liu},
  doi          = {10.1016/j.asoc.2023.111146},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111146},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Human activity recognition through deep learning: Leveraging unique and common feature fusion in wearable multi-sensor systems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of generalized hopfield neural network for the
steady state analysis of self-excited induction generators.
<em>ASOC</em>, <em>151</em>, 111145. (<a
href="https://doi.org/10.1016/j.asoc.2023.111145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a generalized Hopfield neural network (GHNN) for solving the non-linear equations involved in the steady state analysis of Self-Excited Induction Generators (SEIGs). Unlike other neural network architectures , the generalized Hopfield neural network can convert a non-linear equation solving problem into an optimization problem . Moreover, the weights and bias of GHNN can easily be obtained from the coefficients of the non-linear equations to be solved without additional training dataset. Motivated by its simplicity, faster convergence, and global stability properties, this study proposes a GHNN for solving the non-linear equations involved in the steady state analysis of Self-Excited Induction Generators (SEIGs). Moreover, unlike the heuristic algorithms , this approach does not require the boundary values for the unknown parameters (per unit frequency , magnetizing reactance X m Xm and core loss resistance R m Rm ) of the SEIG to be given as inputs, for initiating the optimization process and it ensures guaranteed convergence. Symbolic programming technique has been employed to form the non-linear equations required for the analysis, thereby eliminating the lengthy derivations involved. The formulated equations are then used to form an energy function and a set of differential equations describing the dynamic behavior of the Hopfield network is obtained from this function. These equations are then solved until the energy function minimizes to zero to arrive at the unknown parameters of the SEIG. Using the energy function equation, the stability analysis of the proposed network has been carried out according to the Lyapunov’s second method of stability, which assures the convergence of the proposed method leading to the solutions. The proposed method has been found to be more efficient than other conventional optimization techniques, in terms of accuracy, number of iterations required and computation time . Experiments have been conducted on a 3.5 kW, 415 V SEIG and the results are shown to be in close agreement with the values predetermined using GHNN. It is also concluded that wind driven SEIGS, will find increased deployment in hybrid renewable energy sources , being applied in several emerging areas.},
  archive      = {J_ASOC},
  author       = {S. Sundaramoorthy and R. Essaki Raj},
  doi          = {10.1016/j.asoc.2023.111145},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111145},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of generalized hopfield neural network for the steady state analysis of self-excited induction generators},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A categorized information fusion model for reliable services
delivery in smart cities. <em>ASOC</em>, <em>151</em>, 111144. (<a
href="https://doi.org/10.1016/j.asoc.2023.111144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Network serves as the core source of data in the Internet of Things (IoT) environment. The WSN nodes sense, aggregate, and relay sensed data for the different services in the IoT. The reliability of the aggregated and transmitted information is crucial for the effectiveness of IoT services. The data from multiple sensors at different timestamps need to be fused for efficient data transmission, reducing complexity and improving the reliability of the data being transmitted, improving the quality of IoT services. Therefore, data fusion is an essential aspect of WSN, where multiple sensors collect and combine data to produce a single, more comprehensive data for transmission. This data fusion reduces the amount of data transmission and network energy consumption and increases the accuracy of the result. This paper introduces a Categorised Information Fusion Model (CIFM) to minimise the replication in handling the aggregation of multiple instances of sensor data by categorising the information. The proposed CIFM employs federated learning for distributed verification of different aggregation time frames. This learning identifies the replication of sensor data from different sensors based on time frames and aggregation instances. In the multi-timed aggregation level, the sensed information&#39;s interfering distribution is diminished by controlling the relaying instance. The recurrent learning instances diminish the multi-timed information based on the occurrence factor. This recurrent learning improves delivery precision by controlling computation complexity and fusion time. The CIFM model also reduces the communication overhead by reducing the amount of replicated information that needs to be transmitted and processed and providing a consistent delivery ratio. The use of federated learning also allows for privacy-preserving data processing as the raw data is only processed locally, and aggregated results are shared. The performance of the proposed CIFM model is evaluated through simulations and compared with other existing fusion methods. The results show that the proposed model outperforms existing accuracy and communication efficiency methods. This approach helps to simplify the process of aggregating and relaying the data in the WSN to improve the performance and efficiency of IoT services. The goal of CIFM is to provide a more effective way to process and utilise the information from the sensors to deliver accurate and reliable data to the IoT platform. The CIFM improves the fusion rate by 7.77 % and minimize the fusion time up to 13.03 % compared to other methods.},
  archive      = {J_ASOC},
  author       = {Mohd Anjum and Sana Shahab and Muhammad Attique Khan and Shabir Ahmad},
  doi          = {10.1016/j.asoc.2023.111144},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111144},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A categorized information fusion model for reliable services delivery in smart cities},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lightweight odometry network for GNSS/INS integration
during GNSS outages. <em>ASOC</em>, <em>151</em>, 111143. (<a
href="https://doi.org/10.1016/j.asoc.2023.111143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In challenging environments like urban canyons and tunnels, the Global Navigation Satellite System (GNSS) signal can be interrupted. When this happens, the integrated Global Navigation Satellite System/Inertial Navigation System (GNSS/INS) navigation system relies solely on INS, resulting in rapid dispersion of positioning accuracy over time. Incorporating odometer information into a filter algorithm is one potential solution for correcting INS errors and improving navigation accuracy. However, this approach increases the cost and power consumption of the system. To implement an odometer-assisted integrated navigation system without inflating cost and power consumption, we propose a lightweight odometer network, LONet. This network can emulate a wheeled odometer, determine the carrier’s forward velocity using Inertial Measurement Unit (IMU) output data, and utilize non-holonomic constraint (NHC) and zero velocity update (ZUPT) to maintain the system’s positioning accuracy. To evaluate the performance of our network, we conducted velocity estimate experiments across different velocity intervals. The results demonstrated that our network requires fewer parameters and produces lower errors in estimated velocity compared to state-of-the-art networks. Furthermore, we integrated LONet into the navigation system to mitigate the effects of GNSS signal outages. The results show that the LONet-assisted integrated navigation system achieved horizontal errors comparable to, and sometimes lower than, those obtained using a real wheeled odometer.},
  archive      = {J_ASOC},
  author       = {Ziyan Yu and Jinguang Jiang and Peihui Yan and Yuying Li and Jiaji Wu and Dongpeng Xie},
  doi          = {10.1016/j.asoc.2023.111143},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111143},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A lightweight odometry network for GNSS/INS integration during GNSS outages},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bi-objective workflow scheduling in virtualized fog-cloud
computing using NSGA-II with semi-greedy initialization. <em>ASOC</em>,
<em>151</em>, 111142. (<a
href="https://doi.org/10.1016/j.asoc.2023.111142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtualized Fog-Cloud Computing (VFCC) has emerged as a promising computing model in both research and industry. Its inherent characteristics, such as real-time service provisioning, resource heterogeneity, flexibility, and scalable computational resources , present new opportunities and challenges in the scheduling of computational workflows. Efficient workflow scheduling is crucial in optimizing resource utilization and improving service delivery in VFCC environments. In this paper, we first present a bi-objective optimization model for the workflow scheduling problem in VFCC systems with the aim of minimizing both the system’s makespan and energy consumption. We then propose an efficient list-based method to solve the model. The proposed method consists of two distinct phases. In the first phase, we employ the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to determine task priorities, establishing the task execution sequence. To ensure both convergence speed and diversity, we implement an intelligent semi-greedy approach when generating the initial population. In the second phase, we introduce an energy and makespan-aware heuristic algorithm to efficiently allocate virtual machines to the prioritized tasks. To evaluate the effectiveness of our proposed method, we conduct extensive experiments using both synthetic and real datasets, including the Epigenomics, Montage , and LIGO graphs. We compare the proposed method with the Heterogeneous Earliest Finish Time (HEFT) and Green-HEFT algorithms in terms of makespan and energy consumption, respectively. Furthermore, we evaluate our proposed method by comparing it to two other algorithms, Multi-objective HEFT (MOHEFT) and Multi-objective ACO (MOACO), using various quality indicators such as generation distance (GD), inverse generation distance (IGD), goal programming approach (GPA), and Hyper volume (HV). The simulation results demonstrate that our method effectively reduces the makespan by 6.38% to 51.19% and energy consumption by 3.52% to 16.49%. Moreover, on average, our method outperforms the other algorithms with an improvement of 2.3%, 5.7%, 9.8%, and 9.34% for the GD, IGD, GPA, and HV metrics, respectively.},
  archive      = {J_ASOC},
  author       = {Shahriar Karami and Sadoon Azizi and Fardin Ahmadizar},
  doi          = {10.1016/j.asoc.2023.111142},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111142},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A bi-objective workflow scheduling in virtualized fog-cloud computing using NSGA-II with semi-greedy initialization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved binary differential evolution with dimensionality
reduction mechanism and binary stochastic search for feature selection.
<em>ASOC</em>, <em>151</em>, 111141. (<a
href="https://doi.org/10.1016/j.asoc.2023.111141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer systems store massive amounts of data with numerous features, leading to the need to extract the most important features for better classification in a wide variety of applications. Poor performance of various machine learning algorithms may be caused by unimportant features that increase the time and memory required to build a classifier . Feature selection (FS) is one of the efficient approaches to reducing the unimportant features. This paper, therefore, presents a new FS, named BDE-BSS-DR, that utilizes Binary Differential Evolution (BDE), Binary Stochastic Search (BSS) algorithm, and Dimensionality Reduction (DR) mechanism. The BSS algorithm increases the search capability of the BDE by escaping from local optimal points and exploring the search space . The DR mechanism then reduces the dimensions of the search space gradually. As a result of using DR, the local optima of the search space and the problem of wrong removal of important features before starting the search process are reduced. The algorithm&#39;s efficiency is evaluated on 20 different medical datasets. The obtained outcomes indicate that the BDE-BSS-DR outperforms the BDE and BDE-BSS algorithms significantly. Furthermore, the effectiveness of the proposed algorithms in selecting the most important features of the heart disease data, several cancer diseases, and COVID-19 are also compared with several other state-of-the-art methods. Our results show that the BDE-BSS-DR with SVM classifier has a significant advantage over other methods with an average classification accuracy of 95.05% in heart disease and 99.40% in COVID-19 disease. In addition, the comparisons made with KNN and SVM classification prove the efficiency of the DR and BSS in generating a subset of optimal and informative features.},
  archive      = {J_ASOC},
  author       = {Behrouz Ahadzadeh and Moloud Abdar and Fatemeh Safara and Leyla Aghaei and Seyedali Mirjalili and Abbas Khosravi and Salvador García and Fakhri Karray and U.Rajendra Acharya},
  doi          = {10.1016/j.asoc.2023.111141},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111141},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved binary differential evolution with dimensionality reduction mechanism and binary stochastic search for feature selection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic group preference acceptability analysis for
interval-valued multiplicative preference relations based on TODIM
method. <em>ASOC</em>, <em>151</em>, 111140. (<a
href="https://doi.org/10.1016/j.asoc.2023.111140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among many decision problems, interval-valued multiplicative preference relation (IMPR) is widely utilized due to its ability to express uncertain information. A new approach to solve group decision making (GDM) with IMPRs is proposed in this paper, named stochastic group preference acceptability analysis with TODIM (SGPAA-TODIM) method, by combining TODIM method (an acronym in Portuguese of Interactive and multi-criteria Decision Making) with stochastic multi-criteria acceptability analysis (SMAA-2). It effectively circumvents information loss and considers the weight and risk preferences of experts. Firstly, the stochastic multiplicative preference relation is defined through stochastic simulation employing a certain density function, and its priority weight vector is determined using the logarithmic least squares method (LLSM). Then, the priority preference comprehensive matrix is proposed by extracting information of priority vectors . Moreover, the novel SGPAA-TODIM method is developed to analyze the stochastic parameter spaces, with the optimal rank determined based on the analysis of acceptability degrees associated with dominance rank. Finally, to demonstrate the validity and applicability of the proposed method, numerical examples are given.},
  archive      = {J_ASOC},
  author       = {Ke Zhang and Ligang Zhou and Xianchao Dai and Hao Li},
  doi          = {10.1016/j.asoc.2023.111140},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111140},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Stochastic group preference acceptability analysis for interval-valued multiplicative preference relations based on TODIM method},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting steel column stability with uncertain initial
defects using bayesian deep learning. <em>ASOC</em>, <em>151</em>,
111139. (<a href="https://doi.org/10.1016/j.asoc.2023.111139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stability of steel columns is difficult to predict accurately due to uncertain initial defects such as geometric imperfections and residual stress. To address this issue, we propose a probabilistic model that uses variational autoencoder (VAE) and transfer learning to estimate the loading capacities of steel columns. Our model can predict the confidence intervals of buckling loads without knowing the exact distribution of initial defects, providing more comprehensive information for engineering applications than traditional deterministic strength index. We establish a dataset of 1500 load-displacement curves of steel columns using four data augmentation approaches, and analyze the data distribution to validate the model&#39;s assumptions. Various criterions, including the mean squared error (MSE), the prediction interval coverage probability (PICP), and the prediction interval normalized average width (PINAW), are adopted to comprehensively measure the performance of confidence interval prediction. The numerical experiment validates that the trained model accurately predicts the confidence intervals for load-displacement responses, which perfectly cover the true curves with reasonable PINAW. Finally, we conduct a case study with a practical experiment to illustrate the model&#39;s potential application in failure probability calculation and reliability design. Our proposed model provides a promising probabilistic solution for quantifying the impact of uncertain parameters on structural analysis and significantly simplifies probability-based reliability design and optimal design processes.},
  archive      = {J_ASOC},
  author       = {Haoyang Zhao and Chen Wang and Jiansheng Fan},
  doi          = {10.1016/j.asoc.2023.111139},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111139},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Predicting steel column stability with uncertain initial defects using bayesian deep learning},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Golden search optimization based adaptive and diagonal
kernel convolution neural network for disease prediction and securing
IoT data in cloud. <em>ASOC</em>, <em>151</em>, 111137. (<a
href="https://doi.org/10.1016/j.asoc.2023.111137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their open architecture, Internet of Things (IoT) devices have limited user access and are susceptible to various security vulnerabilities . When healthcare data is stored in the cloud, it often becomes vulnerable to side-channel attacks, Denial of Service (DoS) attacks, and eavesdropping. Consequently, there is a need for an efficient access control scheme to enhance the data-sharing processes of IoT devices. Different machine learning and rule-based decision support systems have been proposed to improve the security of cloud-based data storage. However, these models have exhibited increased false alarm rates , complex processing, and longer response times . To address this research gap, we propose a Fuzzy TOPSIS model based on Golden Search Optimization (GSO) to enhance the security of patient information at both IoT and cloud server levels. Additionally, we introduce a revolutionary Diagonal Kernel Convolution Neural Network with an adaptable kernel (DKCNN-AK) to enhance the accuracy of lung disease diagnosis. The GSO algorithm categorizes the security ranks determined by the fuzzy TOPSIS model into the best and worst security principles. We conducted simulations using the COVID-19 X-Ray detection dataset, the lung cancer dataset, and the data collected from IoT devices for analysis.The experiments were conducted to assess various aspects, including computational time , security level analysis, information leakage , privacy level, prediction accuracy, sensitivity, specificity, and confusion matrix . The improved prediction accuracy and security provided by our proposed approach demonstrate its efficiency},
  archive      = {J_ASOC},
  author       = {S. Jerald Nirmal Kumar and M.M. Gowthul Alam and TF Michael Raj and R. Uma Mageswari},
  doi          = {10.1016/j.asoc.2023.111137},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111137},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Golden search optimization based adaptive and diagonal kernel convolution neural network for disease prediction and securing IoT data in cloud},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advantage prioritization of digital carbon footprint
awareness in optimized urban mobility using fuzzy aczel alsina based
decision making. <em>ASOC</em>, <em>151</em>, 111136. (<a
href="https://doi.org/10.1016/j.asoc.2023.111136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {City governments prioritize mobility in urban planning and policy. Greater mobility in a city leads to happier citizens. Although enhanced urban mobility is helpful, it comes with costs, notably in terms of climate change . Transportation systems that enable urban mobility often emit greenhouse gases. Cities must prioritize digital carbon footprint awareness. Cities may reduce the environmental impact of urban mobility while keeping its benefits by close monitoring and reducing the carbon footprint of digital technologies like transportation applications, ride-sharing platforms, and smart traffic control systems . The aim is to advantage prioritize three alternatives, namely doing nothing, upgrading and optimizing data centers and networks, and using renewable energy sources for data centers and networks to minimize the digital carbon footprint using the proposed decision making tool . This study consists of two stages. In the first stage, fuzzy Aczel-Alsina functions (fuzzy Aczel-Alsina weighted assessment — ALWAS method) based Ordinal Priority Approach (OPA) is proposed to find the weights of criteria. Secondly, fuzzy ALWAS Combined Compromise Solution (CoCoSo) model is improved to evaluate and choose the best alternative among the three alternatives. The improved ALWAS-CoCoSo model enables flexible nonlinear processing of uncertain information and simulation of different risk levels. Besides, we proposed the improved fuzzy OPA algorithm for processing uncertain and incomplete information . The case study is provided to the decision-makers to advantage prioritize the alternatives based on twelve criteria organized into four aspects, including digital carbon footprint, externalities, technical capability, and economics. The ranking results reveal that A 3 = 2 . 445 A3=2.445 is the best among the three alternative, while A 1 = 1 . 705 A1=1.705 is the worst alternative. The results show that the best way to reduce the digital carbon footprint is to use renewable energy sources to power data centers and networks ( A 3 A3 ).},
  archive      = {J_ASOC},
  author       = {Muhammet Deveci and Ilgin Gokasar and Dragan Pamucar and Aws Alaa Zaidan and Wei Wei and Witold Pedrycz},
  doi          = {10.1016/j.asoc.2023.111136},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111136},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Advantage prioritization of digital carbon footprint awareness in optimized urban mobility using fuzzy aczel alsina based decision making},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A teaching-learning-based optimization algorithm with
reinforcement learning to address wind farm layout optimization problem.
<em>ASOC</em>, <em>151</em>, 111135. (<a
href="https://doi.org/10.1016/j.asoc.2023.111135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the global demand for renewable energy continues to rise, wind energy has received widespread attention as an eco-friendly energy source. Wind power generation is regarded as one of the key means to reduce carbon emissions and achieve sustainable development. Usually, a mass of turbines works together to produce electricity in a wind farm . However, downstream turbines will inevitably be influenced by the wake generated by upstream turbines, resulting in unused wind energy being lost. To reduce the negative effects of the wake, maximization of wind farm output power, and minimization of wind farm cost, a teaching-learning-based optimization algorithm with reinforcement learning is proposed in this paper. The improvements of the proposed algorithm mainly include the following three points: i) the original serial structure of the algorithm is changed to a parallel structure to accelerate the convergence and improve the efficiency of the algorithm. ii) the parameter F , which is adjusted by RL, is proposed to adjust the selection of the updating phase due to the design of a parallel structure. iii) in the modified learner phase, an individual is added to participate in the update, and a selection probability is proposed to improve the ability of the algorithm to retain the information of superior individuals. To study the performance of the modified algorithm, it was first tested against 10 other advanced algorithms on a benchmark testing suite. They then ran numerical experiments on four hypothetical wind farm cases under two simulated wind conditions. Finally, the superiority of improved algorithm over others and the effectiveness of addressing wind farm layout problem are demonstrated by experimental results.},
  archive      = {J_ASOC},
  author       = {Xiaobing Yu and Wen Zhang},
  doi          = {10.1016/j.asoc.2023.111135},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111135},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A teaching-learning-based optimization algorithm with reinforcement learning to address wind farm layout optimization problem},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating surrogate-based hybrid acquisition processes.
Application to covid-19 contact mitigation. <em>ASOC</em>, <em>151</em>,
111134. (<a href="https://doi.org/10.1016/j.asoc.2023.111134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate models are built to produce computationally efficient versions of time-complex simulation-based objective functions so as to address expensive optimization. In surrogate-assisted evolutionary computations, the surrogate model evaluates and/or filters candidate solutions produced by evolutionary operators . In surrogate-driven optimization, the surrogate is used to define the objective function of an auxiliary optimization problem whose resolution generates new candidates. In this paper, hybridization of these two types of acquisition processes is investigated with a focus on robustness with respect to the computational budget and parallel scalability. A new hybrid method based on the successive use of acquisition processes during the search outperforms competing approaches regarding these two aspects on the Covid-19 contact mitigation problem. To further improve the generalization to larger ranges of search landscapes, another new hybrid method based on the dispersion metric is proposed. The integration of landscape analysis tools in surrogate-based optimization seems promising according to the numerical results reported on the CEC2015 test suite.},
  archive      = {J_ASOC},
  author       = {G. Briffoteaux and N. Melab and M. Mezmaz and D. Tuyttens},
  doi          = {10.1016/j.asoc.2023.111134},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111134},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Investigating surrogate-based hybrid acquisition processes. application to covid-19 contact mitigation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). X-ray image analysis for explosive circuit detection using
deep learning algorithms. <em>ASOC</em>, <em>151</em>, 111133. (<a
href="https://doi.org/10.1016/j.asoc.2023.111133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-ray imaging technologies find applications across various domains, including medical imaging in health institutions or security in military facilities and public institutions. X-ray images acquired from diverse sources necessitate analysis by either trained human experts or automated systems. In cases where concealed electronic cards potentially pose threats, such as in laptops harboring explosive triggering circuits, conventional analysis methods are challenging to detect, even when scrutinized by skilled. The present investigation is centered on the utilization of deep learning algorithms for the analysis of X-ray images of laptop computers, with the aim of identifying concealed hazardous components. To construct the dataset, some control cards such as Arduino, Raspberry Pi and Bluetooth circuits were hidden inside the 60 distinct laptop computers and were subjected to X-ray imaging, yielding a total of 5094 X-ray images. The primary objective of this study is to distinguish laptops based on the presence or absence of concealed electronic cards. To this end, a suite of deep learning models , including EfficientNet, DenseNet , DarkNet19, DarkNet53, Inception, MobileNet, ResNet18, ResNet50 , ResNet101, ShuffleNet and Xception were subjected to training, testing, and comparative evaluation . The performance of these models was assessed utilizing a range of metrics, encompassing accuracy, sensitivity, specificity, precision, f-measure, and g-mean. Among the various models examined, the ShuffleNet model emerged as the top-performing one, yielding superior results in terms of accuracy (0.8355), sensitivity (0.8199), specificity (0.8530), precision (0.8490), f-measure (0.8322), and g-mean (0.8352).},
  archive      = {J_ASOC},
  author       = {Gokhan Seyfi and Merve Yilmaz and Engin Esme and Mustafa Servet Kiran},
  doi          = {10.1016/j.asoc.2023.111133},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111133},
  shortjournal = {Appl. Soft. Comput.},
  title        = {X-ray image analysis for explosive circuit detection using deep learning algorithms},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting cryptocurrencies volatility using statistical
and machine learning methods: A comparative study. <em>ASOC</em>,
<em>151</em>, 111132. (<a
href="https://doi.org/10.1016/j.asoc.2023.111132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting cryptocurrency volatility can help investors make better-informed investment decisions in order to minimize risks and maximize potential profits. Accurate forecasting of cryptocurrency price fluctuations is crucial for effective portfolio management and contributes to the stability of the financial system by identifying potential threats and developing risk management strategies . The objective of this paper is to provide a comprehensive study of statistical and machine learning methods for predicting daily and weekly volatility of the following four cryptocurrencies: Bitcoin , Ethereum , Litecoin , and Monero . Several models and forecasting methods are compared in terms of their forecasting accuracy , i.e ., HAR (heterogeneous autoregressive), ARFIMA (autoregressive fractionally integrated moving average), GARCH (generalized autoregressive conditional heteroscedasticity), LASSO (least absolute shrinkage and selection operator), RR (ridge regression), SVR (support vector regression), MLP (multilayer perceptron), FNM (fuzzy neighbourhood model), RF (random forest), and LSTM (long short-term memory). The realized variance calculated from intraday returns is used as the input variable for the models. In order to assess the predictive power of the models considered, the model confidence set (MCS) procedure is applied. Our experimental results demonstrate that there is no single best method for forecasting volatility of each cryptocurrency, and different models may perform better depending on the specific cryptocurrency, choice of the error metric and forecast horizon. For daily forecasts, the method that is always found in a set of best models is linear SVR, while for weekly forecasts, there are two such methods, namely FNM and RR. Furthermore, we show that simple linear models such as HAR and ridge regression, perform not worse than more complex models like LSTM and RF. The research provides a useful reference point for the development of more sophisticated models.},
  archive      = {J_ASOC},
  author       = {Grzegorz Dudek and Piotr Fiszeder and Paweł Kobus and Witold Orzeszko},
  doi          = {10.1016/j.asoc.2023.111132},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111132},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting cryptocurrencies volatility using statistical and machine learning methods: A comparative study},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024c). Three-way clustering: Foundations, survey and challenges.
<em>ASOC</em>, <em>151</em>, 111131. (<a
href="https://doi.org/10.1016/j.asoc.2023.111131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering, as an unsupervised data mining technique , allows us to classify similar objects into the same cluster according to certain criteria. It helps us identify patterns between objects, reveal the associations between objects, and discover hidden structures. Traditional two-way clustering (2W clustering) algorithms represent one cluster by one set and only two types of relationships are considered between a sample and a cluster, namely, belonging to and not belonging to. Two-way decision is not always feasible especially in situations that are characterized by uncertainty and lack of information. Guided by the principle of three-way decision (3WD) as thinking in threes, three-way clustering (3W clustering) addresses the information uncertainty problem using core and the fringe regions to character a cluster. The universe is split into three sections by these two sets, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging-to. Compared with 2W clustering methods , 3W clustering incorporates the fringe region to describe the uncertain relationship between objects and clusters, which provides more information about the clustering structure . This survey points out the historical developments of three-way clustering and makes an overview of the achievements in the field of three-way clustering. In addition, to reap a clearer grasp of the development and research significance of three-way clustering, we divide the existing three-way clustering approaches into two categories and present the bibliometric analysis of related approaches. Finally, we point out some challenges and future research topics in three-way clustering. It is hoped that this review can serve as a reference and provide convenience for scholars and practitioners in the field of three-way clustering.},
  archive      = {J_ASOC},
  author       = {Pingxin Wang and Xibei Yang and Weiping Ding and Jianming Zhan and Yiyu Yao},
  doi          = {10.1016/j.asoc.2023.111131},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111131},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way clustering: Foundations, survey and challenges},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Particle swarm optimization based leader-follower
cooperative control in multi-agent systems. <em>ASOC</em>, <em>151</em>,
111130. (<a href="https://doi.org/10.1016/j.asoc.2023.111130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent systems (MAS) have attracted significant attention in recent years due to their wide applications in cooperative control, formation control , synchronization of complex networks, and distributed coordination. A fundamental problem in MAS is the leader-follower consensus or cooperative tracking, where the followers are required to track the state trajectory of the leader agent. To solve the leader-follower consensus problem, we propose a novel evolutionary computation approach to design the optimal distributed control protocols for leader-follower MAS. First, we formulate the design of distributed control gains for leader-follower consensus as an optimization problem to minimize tracking errors. Then, we leverage particle swarm optimization as an efficient evolutionary technique for distributed gain optimization in multi-agent networks. Finally, we guarantee stability for the closed-loop dynamics under directed communication topologies based on algebraic graph theory. The simulation results indicate that the proposed method yields a diminished tracking error, expedites the convergence process , and minimizes the requisite control effort while enhancing computational efficiency. Furthermore, these results exemplify the method&#39;s versatility when applied to nonlinear dynamic scenarios, directed network topologies , fluctuating disturbances, and optimization across multiple domains.},
  archive      = {J_ASOC},
  author       = {Xin Wang and Dongsheng Yang and Shuang Chen},
  doi          = {10.1016/j.asoc.2023.111130},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111130},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Particle swarm optimization based leader-follower cooperative control in multi-agent systems},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multiscale feature fusion network based on attention
mechanism for motor imagery EEG decoding. <em>ASOC</em>, <em>151</em>,
111129. (<a href="https://doi.org/10.1016/j.asoc.2023.111129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decoding of motor imagery (MI) electroencephalogram (EEG) is an essential component of the brain–computer interface (BCI), which can help patients with motor impairment communicate directly with the outside world through assistive devices . The key to motor imagery electroencephalogram (MI-EEG) classification is to extract multiple temporal, spatial, and spectral features , to obtain more comprehensive and representative information. However, current deep learning methods must fully consider the depth of temporal features and multi-spectral knowledge in EEG and often ignore the temporal or spectrum dependence in MI-EEG. In addition, the lack of effective feature fusion methods can lead to information redundancy , which affects decoding performance . To solve the above problems, this paper proposes a novel MI-EEG decoding method, named multi-scale feature fusion network based on attention mechanism (MSFF-SENet). Firstly, the multi-scale spatio-temporal module (MS-STM) and the multi-scale temporal module (MSTM) extract spatial and high-dimensional temporal features from the original signal. Then, the power spectral density estimation (PSD) convolution module (PSD-Conv module) acquires the multi-spectral features of the MI-EEG signal. Secondly, the feature fusion module fuses spatio-temporal and multi-spectral features to generate integrated feature mappings and establish the dependencies between different features. Finally, we conducted a visual analysis of the results, explaining the neural activity patterns of various motor imagery tasks in different frequency ranges and revealing the potential relationship between body movement and changes related to brain activity . The experimental results show that the classification accuracy of this model in BCI Competition IV 2a (BCI IV 2a) and High Gamma (HGD) datasets is 85.37% and 96.60%, respectively, which is superior to the most advanced methods.},
  archive      = {J_ASOC},
  author       = {Dongrui Gao and Wen Yang and Pengrui Li and Shihong Liu and Tiejun Liu and Manqing Wang and Yongqing Zhang},
  doi          = {10.1016/j.asoc.2023.111129},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111129},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multiscale feature fusion network based on attention mechanism for motor imagery EEG decoding},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial-temporal traffic data imputation based on dynamic
multi-level generative adversarial networks for urban governance.
<em>ASOC</em>, <em>151</em>, 111128. (<a
href="https://doi.org/10.1016/j.asoc.2023.111128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complete and accurate traffic data lays an essential foundation for urban governance and intelligent transportation system . Data missing is an inevitable issue in road network analysis with the expansion of urban roads and increasing application of data monitors. Data imputation is regarded as a cost-effective approach for solving data missing problem. However, the spatial-temporal features of massive traffic data have not been comprehensively considered in the previous related studies, and non-uniform distribution of missing data often causes poor results. Therefore, we propose a dynamic multi-level generative adversarial networks (MLGAN) model for the imputation of large-scale spatial-temporal traffic data. The model first captures the spatial-temporal features of traffic data through composite features extractor based on the bidirectional gated recurrent unit (BGRU) and the graph convolutional network (GCN) structure. Then, different sampler structures with varied missing rate are adopted to analyze the feature data and present the reconstruction results. Enhanced by the dynamic stratification strategy , the model is less sensitive to the variation of missing rates. Finally, data imputation experiments are conducted to validate the proposed approach using the data collected from the PeMS dataset. The results show that the proposed model has superior data imputation performance on large-scale dataset compared with the traditional models.},
  archive      = {J_ASOC},
  author       = {Bo Zhang and Rui Miao and Zhihua Chen},
  doi          = {10.1016/j.asoc.2023.111128},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111128},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Spatial-temporal traffic data imputation based on dynamic multi-level generative adversarial networks for urban governance},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection of tramcars for sustainable urban transportation
by using the modified WASPAS approach based on heronian operators.
<em>ASOC</em>, <em>151</em>, 111127. (<a
href="https://doi.org/10.1016/j.asoc.2023.111127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The wrong design of rail system vehicle fleets is one of the most critical problems in terms of urban transportation. Light rail system fleets in many large cities, including Istanbul, consist of various types and feature vehicles. It creates significant problems in integrating each rail system vehicle into the system. In addition, while it is necessary to keep an inventory of spare parts for each different type of vehicle, it requires different qualifications for professionals involved in processes such as maintenance and repair, leading to extra costs. In this context, the study&#39;s primary purpose is to determine the most suitable light rail vehicles for urban transportation systems and create vehicle fleets accordingly. In that regard, the study aims to provide a reliable and practical decision-making model that can be used as a roadmap for decision-makers when choosing tramcars to solve these problems. The present work proposes a hybrid procedure integrating the Best and Worst Method (BWM) and Power-Heronian Weighted Aggregated Sum Product Assessment (WASPAS&#39;PH) approaches. The most critical implication of the work indicated that the acquisition cost per tramcar set (0.148) is still the most influential factor. The economic lifespan of tramcars (0.037), the number of seats in a vehicle set (0.041), and energy consumption (0.072) have followed the most significant criterion, respectively. Besides, it highlighted that the A14 Brand CR (0.7819) is the most appropriate option for well-structuring the urban light rail system fleet. An extensive validation test to check the suggested model&#39;s robustness confirmed the procedure&#39;s stability and consistency.},
  archive      = {J_ASOC},
  author       = {Ömer Faruk Görçün and Dragan Pamucar and Hande Küçükönder},
  doi          = {10.1016/j.asoc.2023.111127},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111127},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Selection of tramcars for sustainable urban transportation by using the modified WASPAS approach based on heronian operators},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical parallel search with automatic parameter
configuration for particle swarm optimization. <em>ASOC</em>,
<em>151</em>, 111126. (<a
href="https://doi.org/10.1016/j.asoc.2023.111126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle swarm optimization (PSO) has been widely applied in solving optimization problems . Despite a multitude of PSO variants that have been proposed thus far, they still suffer from the limitations of the original PSO. Thus, this paper proposes the development of an adaptive Hierarchical Parallel Search and Automatic Parameter Configuration algorithm (APCPSO). The algorithm is based on fitness landscape analysis and aims to address the challenges of balancing the exploration and exploitation abilities of Particle Swarm Optimization (PSO) while also overcoming the difficulties in adjusting its parameters. Through fitness landscape analysis, this algorithm hierarchically processes the population into hierarchical layers, wherein each layer harbors a new subswarm of particles. To enable effective inter-swarm communication, guide and communication particles are assimilated into each subswarm. Moreover, a parameter fitness evaluation criterion is established for each particle, periodically assessing the fitness of its current parameters according to the current search stage and state while dynamically adjusting each parameter to different extents. The performance of APCPSO is tested on CEC 2017 benchmark suites, and experimental results show that APCPSO is superior to competing algorithms in both effectiveness and robustness.},
  archive      = {J_ASOC},
  author       = {Fuqing Zhao and Fei Ji and Tianpeng Xu and Ningning Zhu and Jonrinaldi},
  doi          = {10.1016/j.asoc.2023.111126},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111126},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hierarchical parallel search with automatic parameter configuration for particle swarm optimization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost based random forest classifier for intrusion detection
system in internet of things. <em>ASOC</em>, <em>151</em>, 111125. (<a
href="https://doi.org/10.1016/j.asoc.2023.111125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) is the collection of physical and digital devices that are interconnected using Internet for exchange of information and delivery of services. The Internet of Things (IoT) is an extended application of Internet that is used to offer various services for users in the fields of agriculture, healthcare, education, smart homes and so on in the modern world. The significant issues of the intrusion present in IoT are network disconnection, network hacking and data theft from the source. So the challenging task for worldwide utilization of IoT is to address their security issues, because of the feature imbalance in the different types of attacks. The most essential task for addressing security issues is to predict and classify the intrusion in the network. In this paper, the Cost based Random Forest Classifier (CRFC) is proposed for developing an effective Intrusion Detection System (IDS). The CRFC based classification is improvised by incorporating the cost matrix calculated based on feature importance that helps to improve the process of splitting the features even if there is a feature imbalance. Further, three important libraries of Python namely, Spark, Kafka, and Scikit-learn are used in this IDS to improve the classification performances. In that, Spark is used to implement the distributed environment, Kafka is used for streaming the data and Scikit is used to implement CRFC. There are two datasets known as NSL-KDD and UNSW-NB15 that are used to evaluate the performance of the proposed CRFC-IDS method. The CRFC-IDS method is analyzed on the basis of accuracy, precision, recall, F1-Measure, Area Under the Curve (AUC), False Acceptance Rate (FAR) and Matthews Correlation Coefficient (MCC). The existing approaches OCSVM and DBF are used for comparison with the CRFC-IDS method. The accuracy of CRFC-IDS for NSL-KDD dataset is found to be 99.957%, which is highest when compared to OCSVM and DBF .},
  archive      = {J_ASOC},
  author       = {K. Pramilarani and P. Vasanthi Kumari},
  doi          = {10.1016/j.asoc.2023.111125},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111125},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cost based random forest classifier for intrusion detection system in internet of things},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated CNN optimization using multi-objective grammatical
evolution. <em>ASOC</em>, <em>151</em>, 111124. (<a
href="https://doi.org/10.1016/j.asoc.2023.111124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting and optimizing Convolutional Neural Networks (CNNs) has become a very complex task given the number of associated optimizable parameters, as well as the fact that the arrangement of the layers present in a CNN directly influences its performance. Several research areas used automation techniques to construct and optimize these architectures, with Grammatical Evolution (GE) being one of the most promising techniques. Although several works proposed solutions to the problem in question, each adopts its own evaluation strategy (e.g., different datasets, evaluation metrics , hardware infrastructure). This divergence makes it difficult to compare the proposed approaches, and consequently, it is not possible to reach safe conclusions about the performance of the solutions. This work proposes an experimental evaluation of several context-free grammars listed in the literature for constructing and optimizing CNNs architectures . In addition, we included four well-known CNNs as baselines: DenseNet169, EfficientNetB1, InceptionV3 and ResNet50V2. We aim to identify the best practices for elaborating grammars and compare their results with consolidated CNNs for image classification problems in the literature. Besides, we assessed all approaches on the same controlled environment (e.g., datasets, evaluation metrics, software and hardware setup) to guarantee fairness in the evaluation process. The experiments were carried out by investigating the performance of the models generated by different grammars in solving image classification problems in three datasets of variable dimensions: CIFAR-10, EuroSAT, and MNIST. The experiments have validated several key findings: (i) the significance of optimizing Convolutional Neural Networks (CNNs); (ii) the potential of grammar-based methods as a promising alternative for this task, yielding CNN models that outperform state-of-the-art CNN architectures while possessing fewer trainable parameters, resulting in reduced computational complexity ; (iii) grammars incorporating regularization layers (such as dropout and batch normalization) and those that confine the search space (via parameter constraints on CNNs) consistently produce high-performing models with lower complexity, even after a few generations of the evolutionary process; and (iv) the selection of the grammar for optimization can positively or negatively impact the model generation, depending on the specific task requirements.},
  archive      = {J_ASOC},
  author       = {Cleber A.C.F. da Silva and Daniel Carneiro Rosa and Péricles B.C. Miranda and Tapas Si and Ricardo Cerri and Márcio P. Basgalupp},
  doi          = {10.1016/j.asoc.2023.111124},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111124},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Automated CNN optimization using multi-objective grammatical evolution},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A disease diagnosis system for smart healthcare based on
fuzzy clustering and battle royale optimization. <em>ASOC</em>,
<em>151</em>, 111123. (<a
href="https://doi.org/10.1016/j.asoc.2023.111123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ongoing growth of the Internet of Things and machine learning technology have provided increased motivation for the development of smart healthcare . In this study, a disease diagnosis system is proposed for remote identification and early prediction in smart healthcare environments. The originality of this study resides in the innovative implementation of ensuing modules to improve diagnostic accuracy of the system. First, fuzzy clustering based on the forest optimization algorithm is employed to detect outliers and a self-organizing fuzzy logic classifier is applied to supplement missing data in electronic medical records (EMRs). A feature selection technique using the battle royale optimization algorithm is then developed to remove redundant information and identify optimal EMR features. The refined and fused data are further classified using an eigenvalue-based machine learning algorithm to determine whether a patient exhibits a certain disease. Simulation experiments are conducted with widely used heart disease and diabetes datasets to evaluate the performance of the proposed system, using accuracy, precision, recall, and F-measure as evaluation metrics .},
  archive      = {J_ASOC},
  author       = {Fei Yan and Hesheng Huang and Witold Pedrycz and Kaoru Hirota},
  doi          = {10.1016/j.asoc.2023.111123},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111123},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A disease diagnosis system for smart healthcare based on fuzzy clustering and battle royale optimization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy collaborative forecasting approach based on XAI
applications for cycle time range estimation. <em>ASOC</em>,
<em>151</em>, 111122. (<a
href="https://doi.org/10.1016/j.asoc.2023.111122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the cycle time ranges of jobs is a critical task in a factory. this study proposes a fuzzy collaborative intelligence (FCI) approach to improve the precision of cycle time range estimation. In the proposed methodology, a DNN is first built to accurately predict the cycle time of a job. A random forest (RF) is then constructed to explain the DNN. Each decision tree of the RF is fuzzified to estimate the cycle time ranges of the jobs learned by the decision tree. A fuzzy collaboration mechanism is also established between decision trees to narrow the cycle time ranges. The proposed methodology is novel because the RF is not applied to predict job cycle times but used to explain and fuzzify the DNN without solving complex nonlinear programming problems. The proposed methodology has been applied to a real case. According to the experimental results, the FCI approach improved the estimation precision by 18%.},
  archive      = {J_ASOC},
  author       = {Tin-Chih Toly Chen and Chi-Wei Lin and Yu-Cheng Lin},
  doi          = {10.1016/j.asoc.2023.111122},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111122},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy collaborative forecasting approach based on XAI applications for cycle time range estimation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approach to investigate fairness using dominance-based
rough sets analysis—how fair were the COVID-19 restriction decisions in
the UK? <em>ASOC</em>, <em>151</em>, 111121. (<a
href="https://doi.org/10.1016/j.asoc.2023.111121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness is a crucial aspect to consider within decision support systems , to seek to strive for equitable decision outcomes. Therefore, in this work, we introduce an approach to investigate fairness in data-driven decisions. The Dominance-based Rough Sets Approach (DRSA) has been widely used to extract a single set of if-then types of rules from data. Conversely, our approach investigates fairness by extracting multiple separate if-then rule sets for separate groups. The proposed approach facilitates fairness analysis to be performed amongst groups represented by these rule-sets. During the COVID-19 pandemic, several countries have taken the approach of tiered restrictions, which has remained a point of debate due to a lack of transparency. Using our proposed approach, we explore fairness analysis with regards to the UK government’s COVID-19 tiered restrictions allocation system. These insights from the analysis are translated into “if-then” type rules, which can easily be interpreted by policy makers. The differences in the rules extracted from different geographical areas suggest inconsistencies in the allocations of tiers in these areas. We found that the differences delineated an overall north south divide in England, however, this divide was driven mostly by London. Such analysis could provide a more transparent approach to localised public health restrictions, which can help ensure greater conformity to the public safety rules. Our analysis demonstrates the usefulness of our approach, to explore fairness analysis in terms of equal-treatment within data-driven decisions, which could be applied in numerous other domains, for investigating the fairness and explainabilty of decisions.},
  archive      = {J_ASOC},
  author       = {Edward Abel and Sajid Siraj},
  doi          = {10.1016/j.asoc.2023.111121},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111121},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An approach to investigate fairness using dominance-based rough sets Analysis—How fair were the COVID-19 restriction decisions in the UK?},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic trajectory partition optimization method based on
historical trajectory data. <em>ASOC</em>, <em>151</em>, 111120. (<a
href="https://doi.org/10.1016/j.asoc.2023.111120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partitioning dynamic trajectory data can improve the efficiency and accuracy of trajectory data processing, provide a foundation for trajectory data mining and analysis. However, with the continuous growth of trajectory data scales and the urgent demand for trajectory query efficiency and accuracy, partitioning methods have become crucial. The partitioning method of dynamic trajectory data faces significant challenges in terms of spatiotemporal trajectory locality, partition load balancing, and partition time. To address these challenges, we propose a method based on historical trajectory pre-partitioning, which can store data more effectively in distributed systems. We partition similar historical trajectory data to achieve preliminary partitioning of the data. In addition, we also construct a cost model to ensure that the workload of each partition is close to consistency. Extensive experiments have demonstrated the excellent partitioning efficiency and query efficiency achieved by our design compared to other partitioning methods.},
  archive      = {J_ASOC},
  author       = {Xiang Yu and Huawei Zhai and Ruijie Tian and Yao Guan and Kemal Polat and Adi Alhudhaif},
  doi          = {10.1016/j.asoc.2023.111120},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111120},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Dynamic trajectory partition optimization method based on historical trajectory data},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incremental updating fuzzy tolerance rough set approach in
intuitionistic fuzzy information systems with fuzzy decision.
<em>ASOC</em>, <em>151</em>, 111119. (<a
href="https://doi.org/10.1016/j.asoc.2023.111119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As information technology develops rapidly and data is constantly updated, efficient mining knowledge from dynamic intuitionistic fuzzy information systems with fuzzy decision (IFFD) is a significant topic. In the dynamic data environment, the corresponding IFFD is always changing with time when object sets of datasets may evolve in time. Fuzzy tolerance rough set (FTRS), as one of extended rough set models, has a strong capacity for the expression of information and better representation of uncertainty. Therefore, it is very effective and necessary to use FTRS to acquire valuable knowledge from dynamic IFFD. In this paper, we investigate the dynamic fuzzy tolerance rough set approach for IFFD. Firstly, a new fuzzy tolerance similarity is defined to describe fuzzy tolerance relation between objects in the IFFD. Second, we construct a fuzzy tolerance rough set model in the IFFD, discuss some properties, and propose the corresponding static algorithm. Subsequently, incremental approaches which update fuzzy tolerance rough approximations with the insertion and deletion of objects in the IFFD are investigated and the corresponding dynamic algorithms are also designed. Finally, the feasibility of the FTRS model and the effectiveness and efficiency of the dynamic algorithm in dynamic environments are validated through a series of comparative experiments on nine datasets.},
  archive      = {J_ASOC},
  author       = {Lu Wang and Zheng Pei and Keyun Qin and Lei Yang},
  doi          = {10.1016/j.asoc.2023.111119},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111119},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Incremental updating fuzzy tolerance rough set approach in intuitionistic fuzzy information systems with fuzzy decision},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A behavior prediction method for complex system based on
belief rule base with structural adaptive. <em>ASOC</em>, <em>151</em>,
111118. (<a href="https://doi.org/10.1016/j.asoc.2023.111118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the behavior of complex systems and taking appropriate measures for system management is of paramount importance for decision-makers. Belief rule base (BRB) is an effective method for modeling complex systems, and its construction relies on expert knowledge. However, in certain complex system prediction problems, deriving the structure and parameters of BRB from limited expert knowledge or existing models is a challenge, and there may even be a lack of available expert knowledge. Data mining plays a crucial role in obtaining the parameters for model construction in the design of decision support systems (DSSs). Therefore, this paper proposes a method for behavior prediction of complex systems called the structural adaptive BRB (SA-BRB). First, to reduce the randomness of K-means+ +, this paper introduces an error constraint and employs this algorithm to mine historical data for constructing a reference value set. Second, a model ensemble construction process is designed to create different model structures. Subsequently, the evidence reasoning (ER) algorithm is applied to derive the models, and the projection covariance matrix adaptive evolution strategy (P-CMA-ES) algorithm is used for model optimization. Finally, a model evaluation method is established, allowing adaptive adjustment of the model structure according to the needs of engineering practice and the preferences of decision-makers. Furthermore, the effectiveness of the proposed method is validated through two case studies : one focusing on predicting the health status of a flywheel system and the other on forecasting full-load power generation in a combined power plant.},
  archive      = {J_ASOC},
  author       = {QingXi Zhang and BoYing Zhao and Wei He and HaiLong Zhu and GuoHui Zhou},
  doi          = {10.1016/j.asoc.2023.111118},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111118},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A behavior prediction method for complex system based on belief rule base with structural adaptive},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metaheuristic adaptive control based on polynomial
regression and differential evolution for robotic manipulators.
<em>ASOC</em>, <em>151</em>, 111116. (<a
href="https://doi.org/10.1016/j.asoc.2023.111116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adaptive control of systems consists of the online adjustment of their control parameters to deal with parametric uncertainties and disturbances. Indirect Adaptive Control (IAC) of electromechanical systems and robotic manipulators based on metaheuristic optimization has been shown to be competitive compared to other classical and advanced control schemes. In this approach, the identification of system model parameters is stated as an optimization problem and then solved online by metaheuristics. Subsequently, the identified model is used in a second stage of online metaheuristic optimization to predict the system behavior and tune its control parameters. Therefore, this approach can be computationally expensive and unaffordable for systems with more complex dynamics whose models are just as complex. Due to the above, this work proposes an indirect adaptive control method for robotic manipulators based on differential evolution (DE) optimization, which aims to reduce the computational cost of the online identification stage by using Polynomial Regressors (PR) whose adjustment can be performed through a closed-form solution. The proposed method is tested in simulation for a 2 d.o.f. robotic manipulator and the results are compared against the original indirect adaptive control approach based on metaheuristic optimization and also a controller optimized offline. The post-hoc pair-wise Friedman tests confirm that the proposed approach provides competitive results compared to the original approach, with significant savings in computational resources .},
  archive      = {J_ASOC},
  author       = {Alejandro Rodríguez-Molina and Miguel Gabriel Villarreal-Cervantes and Jesús Said Pantoja-García and Alam Gabriel Rojas-López and Eric Hernández-Castillo and Ricardo Mejía-Rodríguez},
  doi          = {10.1016/j.asoc.2023.111116},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111116},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Metaheuristic adaptive control based on polynomial regression and differential evolution for robotic manipulators},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network structure guided multi-objective optimization
approach for key entity identification. <em>ASOC</em>, <em>151</em>,
111115. (<a href="https://doi.org/10.1016/j.asoc.2023.111115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective key entity identification problems have attracted the attention of researchers due to their potential real-world applications, which single-objective approaches simply cannot provide. However, existing multi-objective optimization models seldom consider the heterogeneous propagation costs of key entities. In addition, most multi-objective algorithms are directly used to solve these models, leading to poor performance since they easily fall into local optimal solutions and cause isolated points on the Pareto frontier . This study aims to address the multi-objective key entity identification problem by maximizing the propagation scale while simultaneously minimizing the heterogeneous propagation costs to obtain a more realistic model. A network structure guided approach was designed to solve the formulated model. This consisted of two new strategies, i.e., the best candidate selection strategy and the layered crossover operator strategy, to enhance candidates during the evolution process and to obtain more complete Pareto solutions . Finally, experiments conducted using synthetic and real-world networks show that the proposed approach improves upon five commonly studied multi-objective algorithms and achieves superior performance compared to similar approaches on the Pareto frontier across a number of metrics.},
  archive      = {J_ASOC},
  author       = {Cheng Jiang and Jiaxin Xie and Tanglin Ye},
  doi          = {10.1016/j.asoc.2023.111115},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111115},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Network structure guided multi-objective optimization approach for key entity identification},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A grid-level segmentation model based on encoder-decoder
structure with multi-source features for crop lodging detection.
<em>ASOC</em>, <em>151</em>, 111113. (<a
href="https://doi.org/10.1016/j.asoc.2023.111113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop lodging assessment plays a critical role in acquiring accurate information regarding the location and area of lodging, which is essential for loss assessment and adjustments of harvester parameters. In this paper, we proposed LDVO (Lodging Detection with Visible-image Only), a comprehensive grid-to-grid semantic segmentation method for timely and accurate identification of crop lodging. The LDVO model uses Inception block and dense connection to construct a lightweight feature extraction network , and complements the texture feature and crop vegetation indices as reference features for semantic segmentation . Besides, the model meshes the aerial images according to the operation characteristics of the harvester , and the accuracy of the segmentation task is reduced from the pixel level to the grid level, which minimizes the network scale and computing cost under the premise of meeting the accuracy requirements of lodging detection. Experimental results demonstrate the superiority of the proposed LDVO model over mainstream semantic segmentation networks in terms of processing speed and model parameters. Remarkably, the LDVO model achieves the highest prediction accuracy of 94.86% by leveraging a combination of RGB semantic features , VIs and texture features. Therefore, the proposed LDVO model provides a fast, feasible and low-cost reference for monitoring crop lodging status in complex field environments. It also offers a universal idea for the improvement of semantic segmentation network in special application scenarios.},
  archive      = {J_ASOC},
  author       = {Lihui Wang and Huidi Xiao},
  doi          = {10.1016/j.asoc.2023.111113},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111113},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A grid-level segmentation model based on encoder-decoder structure with multi-source features for crop lodging detection},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BERT-based combination of convolutional and recurrent neural
network for indonesian sentiment analysis. <em>ASOC</em>, <em>151</em>,
111112. (<a href="https://doi.org/10.1016/j.asoc.2023.111112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment analysis is the computational study of opinions and emotions expressed in text. Deep learning is a model that is currently producing state-of-the-art in various application domains, including sentiment analysis. Many researchers are using a hybrid approach that combines different deep learning models and has been shown to improve model performance. In sentiment analysis, input in text data is first converted into a numerical representation. The standard method used to obtain a text representation is the fine-tuned embedding method. However, this method does not pay attention to each word&#39;s context in the sentence. Therefore, the Bidirectional Encoder Representation from Transformer (BERT) model is used to obtain text representations based on the context and position of words in sentences. This research extends the previous hybrid deep learning using BERT representation for Indonesian sentiment analysis. Our simulation shows that the BERT representation improves the accuracies of all hybrid architectures . The BERT-based LSTM-CNN also reaches slightly better accuracies than other BERT-based hybrid architectures.},
  archive      = {J_ASOC},
  author       = {Hendri Murfi and Syamsyuriani and Theresia Gowandi and Gianinna Ardaneswari and Siti Nurrohmah},
  doi          = {10.1016/j.asoc.2023.111112},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111112},
  shortjournal = {Appl. Soft. Comput.},
  title        = {BERT-based combination of convolutional and recurrent neural network for indonesian sentiment analysis},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrated three-way decision methodology for
sustainability of wastewater circularity in thermal power plants.
<em>ASOC</em>, <em>151</em>, 111111. (<a
href="https://doi.org/10.1016/j.asoc.2023.111111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of multiple criteria for evaluating wastewater reuse applications indicates the potential usage of Multi-Criteria Decision-Making (MCDM) methods for this purpose. However, there is currently a scarcity of studies in the domain literature that utilize MCDM approaches in this application topic. This paper therefore advances the domain literature in two distinctive ways. Firstly, it analyzes and advances the reuse agenda of wastewater from thermal power plants, recognized as large-scale users of water, thus promoting greater water circularity . Secondly, it provides a methodological advance by integrating the notion of Three-Way Decision (3WD) into the computational structure of MCDM methods by introducing a middle reference point . Such an initiative results in a novel 3WD extension of the Measurement of Alternatives and Ranking according to COmpromise Solution (MARCOS) method. Additionally, this work provides a proof that the MARCOS method utilizes a compromise solution in identifying priority alternatives, along with the integration of a Weighted Aggregated Sum Product ASsessment (WASPAS) metric. An initial hypothetical example illustrates how the proposed approach augments the canonical MARCOS method, particularly in promoting the “thinking in threes” as a more natural information processing approach and the high degree of distinguishability of priorities between decision alternatives. An actual case study in a thermal power plant then demonstrates the contributions of this work. With the best-worst method used to determine the priorities of the decision attributes , the findings reveal that wastewater reuse applications achieving reduced costs for needed infrastructures, operational simplicity, technological compatibility, consumer safety and household savings are preferred by stakeholders. The 3WD-MARCOS approach identifies industrial and commercial use, municipal use, environmental restoration, and household use as the high-priority alternatives, with cooking and drinking as least preferred. These insights guide stakeholders in their design of initiatives that allocate resources for greater wastewater reuse. A comparative analysis yields high consistency of these findings with similar MCDM methods. In addition, the efficacy of the novel 3WD-MARCOS method highlights its potential in handling MCDM problems, including those promoting water circularity.},
  archive      = {J_ASOC},
  author       = {Lanndon Ocampo and Jenebyb Cabigas and Dylan Jones and Ashraf Labib},
  doi          = {10.1016/j.asoc.2023.111111},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111111},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An integrated three-way decision methodology for sustainability of wastewater circularity in thermal power plants},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The regular language-constrained orienteering problem with
time windows. <em>ASOC</em>, <em>151</em>, 111110. (<a
href="https://doi.org/10.1016/j.asoc.2023.111110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several application domains of the Orienteering Problem (OP) entail the categorization of graph nodes . Specific categories of nodes may be preferred to be included in the solution, while nodes of other categories should be limited to a certain extent or even excluded. Additionally, precedence constraints may apply among nodes from different categories. We contend that regular expressions, which describe patterns of node categories and specify constraints for solution paths, can effectively capture practical tourist tour planning aspects. Hence, we introduce the Regular Language-Constrained OP with Time Windows (RLC-OPTW) as an extension of the OP, where regular expressions are utilized to describe the admissible category patterns in solution paths. Our approach leverages the simplicity, elegance, and expressive power of regular languages, which excel in applications involving pattern recognition and matching. Given that RLC-OPTW is NP-hard, we initially provide an exact solution for small instances of the problem. Then, we present two efficient heuristic approaches: The first heuristic iteratively appends nodes to the solution to generate an initial feasible (i.e., regular expression-constrained) solution and then replaces nodes in search of higher quality solutions. The second heuristic iteratively inserts nodes at any point within the solution and then replaces sets of consecutive nodes upon reaching a local optimum. The efficiency of our proposed algorithms has been assessed using publicly available datasets. We have also showcased the effectiveness of our methods in generating meaningful tourist trips that adhere to practical user constraints using a real dataset with tourist attractions in Athens (Greece) as a case study . Although our algorithmic approaches and experimental evaluation of RLC-OPTW primarily focus on tourist trip planning, the proposed algorithms can be applied to finding paths under constraints in numerous other application domains of the OP.},
  archive      = {J_ASOC},
  author       = {Nikolaos Vathis and Grammati Pantziou and Charalampos Konstantopoulos and Damianos Gavalas},
  doi          = {10.1016/j.asoc.2023.111110},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111110},
  shortjournal = {Appl. Soft. Comput.},
  title        = {The regular language-constrained orienteering problem with time windows},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Soft imitation reinforcement learning with value
decomposition for portfolio management. <em>ASOC</em>, <em>151</em>,
111108. (<a href="https://doi.org/10.1016/j.asoc.2023.111108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imitation learning has been recognized as a method to accelerate the training process of deep reinforcement learning agents in search of optimal strategies . Nevertheless, existing imitation learning algorithms have limitations in effectively leveraging expert demonstrations when confronted with dynamic environments, as the behavior cloning loss weight cannot be adaptively updated. To overcome this challenge, we propose a novel approach called Soft Imitation Reinforcement Learning (SIRL), which combines imitation learning and reinforcement learning to guide the training of reinforcement learning agents in an adaptive manner. Additionally, we addressed the challenge of high-dimensional action spaces for reinforcement learning in portfolio management with value decomposition, and provide theoretical proof of convergence for this method. To validate the effectiveness of the SIRL algorithm, we conduct extensive experiments using stock market data from emerging (China) and developed (the US) countries. Our experiments indicate the versatility of the proposed SIRL across different types of trading data, encompassing both high-frequency (5-minute interval) and low-frequency (daily and weekly) data.},
  archive      = {J_ASOC},
  author       = {Li Dong and Haichao Zheng},
  doi          = {10.1016/j.asoc.2023.111108},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111108},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Soft imitation reinforcement learning with value decomposition for portfolio management},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combined heat and power economic emission dispatch using
dynamic switched crowding based multi-objective symbiotic organism
search algorithm. <em>ASOC</em>, <em>151</em>, 111106. (<a
href="https://doi.org/10.1016/j.asoc.2023.111106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combined heat and power economic emission dispatch (CHPEED) problem is a highly complex, non-linear, non-convex multi-objective optimization problem due to two conflicting objectives and various operational constraints such as valve-point loading effect, power transmission loss, prohibited operating zone, and the feasible operating region of combined heat and power unit. In order to overcome these challenges, it is necessary to design an algorithm that exhibits a search behavior , which is suitable for the characteristics of objective and constraint space of the CHPEED problem. For these reasons, a dynamic switched crowding based multi-objective symbiotic organism search (DSC-MOSOS) algorithm was designed to meet the requirements and geometric space of the CHPEED problem. By applying the DSC method in the MOSOS algorithm, it was aimed to improve the exploration ability, to strengthen exploitation-exploration balance, and to prevent the catching into local solution traps. A comprehensive experimental study was carried out to prove the performance of the proposed algorithm on IEEE CEC 2020 multi-modal multi-objective problems (MMOPs) and CHPEED problem. In the experimental study conducted among eleven versions of MOSOS variations created with DSC-method and the base MOSOS algorithm on IEEE CEC 2020 MMOPs, according to Friedman scores based on the four performance metrics, the base MOSOS algorithm ranked the last. In other experimental study, the best DSC-MOSOS variant was applied to solve the CHPEED problem, where 5-, 7-, 10- and 14-unit test systems and eight case studies were considered. The important points of this study were that 10-unit and 14-unit test systems were presented to the literature, and the prohibited operating zone was considered in CHPEED problem for the first time. According to the results obtained from eight case studies obtained from the DSC-MOSOS and fourteen competitor algorithms, while the improvement in cost was between 0.2% and 16.55%, the reduction of the emission value was between 0.2 kg and 42.97 kg compared to the competitor algorithms. On the other hand, the stability of the DSC-MOSOS and the base MOSOS was evaluated using stability analysis. While the MOSOS algorithms was not able to perform a success in any case study, the DSC-MOSOS was achieved an average success rate with 91.16%. Thus, the performance of the DSC-MOSOS over the MOSOS was verified by the results of experimental studies and analysis.},
  archive      = {J_ASOC},
  author       = {Burcin Ozkaya and Hamdi Tolga Kahraman and Serhat Duman and Ugur Guvenc and Mustafa Akbel},
  doi          = {10.1016/j.asoc.2023.111106},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111106},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combined heat and power economic emission dispatch using dynamic switched crowding based multi-objective symbiotic organism search algorithm},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A green 4-dimensional multi objective transportation system
for disaster relief operations under time-sequential complex fermatean
framework with safety measure. <em>ASOC</em>, <em>151</em>, 111102. (<a
href="https://doi.org/10.1016/j.asoc.2023.111102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a post-disaster response operation of a severe cyclone, like Biparjoy, the transportation of humanitarian aid (HA) is the foremost activity in the aftermath of the disaster. Here, due to the ubiquity of uncertainty in such operations, we have first devised a time-sequential complex Fermatean hesitant fuzzy set (TS-CFHFS) with all-dimensional aspects such as its structure, fundamental operations, score and accuracy function. For the transportation of HA, we have developed a multi objective multi-item green 4-dimensional transportation system under the TS-CFHF configuration. In our proposed framework, the HA from both private and public sectors is transported to distribution centers (DCs). The aid was then distributed to the disaster relief shelters (DRS) established in the affected areas. We have also incorporated several significant factors that enhance its realistic portrayal, including vehicle speed, disturbance rate, and safety while transporting HAs. The first and foremost objective is to minimize transportation time (TT), for which we have incorporated the objective vehicle speed, while other objectives are transportation cost (TC), carbon emission (CE) and job opportunities (JO) in our proposed model. To obtain the optimal solution, three approaches namely fuzzy programming (FP), global criterion method (GCM) and weighted sum technique (WST) have been utilized. The proposed model has been complemented with two specific case studies based on the nature of the safety constraints by using a numerical computation along with a comparative analysis of the results obtained by three distinct approaches that may be useful to the decision-maker to make an empirical decision that would affect emergency response actions.},
  archive      = {J_ASOC},
  author       = {M.K. Sharma and Sadhna Chaudhary and Anil K. Malik and Apu Kumar Saha},
  doi          = {10.1016/j.asoc.2023.111102},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111102},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A green 4-dimensional multi objective transportation system for disaster relief operations under time-sequential complex fermatean framework with safety measure},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pellet image segmentation model of superpixel feature-based
support vector machine in digital twin. <em>ASOC</em>, <em>151</em>,
111083. (<a href="https://doi.org/10.1016/j.asoc.2023.111083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A digital twin model based on superpixel features is established to solve the problem of noise and similar gray values between foreground and background of pellet images. With superpixel as the basic unit of segmentation, the influence of single pixel on segmentation results is reduced, and allows for higher segmentation accuracy . The gray-level co-occurrence matrix is used to represent the superpixel characteristic information, and the color moment and gray level distribution are combined to comprehensively characterize the superpixel. Through principal component analysis and correlation analysis, The feature compression of superpixel is realized, and the computational efficiency is improved. The superpixel binary classification data set is built, and the multi-dimensional feature information of superpixel is extracted as input vector to train the binary classification model of SVM , and the image segmentation problem is transformed into foreground and background classification problem. A multi-scale superpixel segmentation boundary optimization method is proposed to further refine the boundary region of foreground and background. A four-neighborhood search algorithm is proposed to reduce the missegmentation rate of edge superpixels. Experimental results show that the accuracy of the proposed method can reach 95.87%, the precision of image edge segmentation is high, and the foreground and background of granular image are accurately segmented. The digital twin model is established which can provide basis for the subsequent visual analysis and decision.},
  archive      = {J_ASOC},
  author       = {Weining Ma and Jingguo Qu and Lijing Wang and Chun Zhang and Aimin Yang and Yuzhu Zhang},
  doi          = {10.1016/j.asoc.2023.111083},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111083},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pellet image segmentation model of superpixel feature-based support vector machine in digital twin},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hypervolume-based cuckoo search algorithm with enhanced
diversity and adaptive scaling factor. <em>ASOC</em>, <em>151</em>,
111073. (<a href="https://doi.org/10.1016/j.asoc.2023.111073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multi-objective cuckoo search (MOCS) algorithms are based on either Pareto dominance or decomposition. However, when dealing with complex multi-objective problems (MOPs), the Pareto dominance-based algorithms face a decrease in selection pressure, and the decomposition-based algorithms easily gain poor distributions. The objective of this paper is to repurpose an indicator-based MOCS by combining improved diversity enhancement (IDE) and adaptive scaling factor (ASF) for MOPs. In the proposed algorithm, hypervolume is used as the indicator to guarantee better convergence and enough spread of the population. IDE chooses the large hypervolume to rebuild the parent population to compensate for the lack of population diversity. Additionally, ASF makes full use of individuals information to enhance the search ability of Lévy component in cuckoo search. Comprehensive experiments on 31 benchmark functions including two classical suites ZDT, WFG, and one challenged suite proposed in CEC2019, as well as 8 real-world problems were conducted to test the proposed algorithm. Compared with several state-of-the-art multi-objective evolutionary algorithms, the effectiveness and efficiency of our proposed method were demonstrated by the results.},
  archive      = {J_ASOC},
  author       = {Maomao Liang and Liangying Wang and Lijin Wang and Yiwen Zhong},
  doi          = {10.1016/j.asoc.2023.111073},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111073},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A hypervolume-based cuckoo search algorithm with enhanced diversity and adaptive scaling factor},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilayer neurocontrol of servo electromechanical systems
with disturbance compensation. <em>ASOC</em>, <em>151</em>, 111043. (<a
href="https://doi.org/10.1016/j.asoc.2023.111043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For servo electromechanical systems , the existing modeling uncertainties, signal measurement noises and so on always make it difficult to design high-performance closed-loop controllers. In this paper, a novel intelligent controller will be designed to deal with these uncertainties. Specifically, two multilayer neuroadaptive disturbance observers will be proposed to estimate the uncertain nonlinear dynamics and exogenous disturbances simultaneously. And these uncertainties will be compensated feedforwardly. Moreover, in order to reduce the influence of signal measurement noises, the desired-command compensation technique will be incorporated. Additionally, different validation examples will be proposed to demonstrate the advantages of the designed controller.},
  archive      = {J_ASOC},
  author       = {Guichao Yang and Hua Wang and Jianyong Yao and Xiaoqi Zou},
  doi          = {10.1016/j.asoc.2023.111043},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111043},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multilayer neurocontrol of servo electromechanical systems with disturbance compensation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resilient and sustainable global COVID-19 vaccine supply
chain design considering reverse logistics. <em>ASOC</em>, <em>151</em>,
111041. (<a href="https://doi.org/10.1016/j.asoc.2023.111041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a multi-mode, multi-product, and multi-objective optimization model to design a resilient and sustainable global COVID-19 vaccine supply chain network under operational risks . The network includes a forward chain for vaccine supply, production, and distribution and a reverse chain for vaccine waste management. The objective functions comprise minimizing the total network costs and environmental pollution caused by the waste of vaccination centers and the transportation between facilities, as well as considering social impacts (maximizing regional development and the effect of domestic vaccine production on self-sufficiency, and minimizing the impact of vaccine side-effects). Resilience strategies are employed to deal with the disruption risks in different facilities and transportation links of the chain, including multiple sources of raw material supply, vaccine production and distribution, and waste management facilities; import; multiple transportation modes and different vehicles in each mode; inventory holding; capacity expansion; transshipment; and fortification of facilities. Financial parameters, including exchange rate and international transport insurance for importing raw materials and vaccines, are also considered in the presented model. An actual-world case study in Iran is conducted to indicate the model&#39;s applicability. Finally, by analyzing the computational results, important research implications are presented. The analyses investigate the conflict of objectives, examine the impact of vaccine side-effects on production and import, assess the efficiency of resilience strategies in the face of disruptions, demonstrate the effect of disruptions on network components, and determine suppliers&#39; share for the supply of raw materials.},
  archive      = {J_ASOC},
  author       = {Ehsan Torshizi and Ali Bozorgi-Amiri and Fatemeh Sabouhi},
  doi          = {10.1016/j.asoc.2023.111041},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111041},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Resilient and sustainable global COVID-19 vaccine supply chain design considering reverse logistics},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decomposition–based multi-objective differential evolution
for extractive multi-document automatic text summarization.
<em>ASOC</em>, <em>151</em>, 110994. (<a
href="https://doi.org/10.1016/j.asoc.2023.110994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The central challenge in Automatic Text Summarization (ATS) involves efficiently generating machine-generated text summaries through optimization algorithms . An ATS is a critical component for systems dealing with textual information processing. However, the current approach faces a significant hurdle due to the computational intensity of the process, particularly when employing complex optimization techniques like swarm intelligence optimization alongside a costly ATS repair operator. While the current approach yields impressive Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics for the generated summary, it struggles with inefficiencies, mainly attributed to the substantial optimization time consumed by the ATS repair operator scheme. In order to address this, a novel solution called Decomposition-based Multi-Objective Differential Evolution (MODE/D) is proposed. It is built upon the foundation of Differential Evolution for Multi-Objective Optimization (DEMO) and the weighted sum method (WS), coupled with an innovative ATS repair operator scheme. Through experimentation on Document Understanding Conferences (DUC) datasets, the novel approach of MODE/D – WS is validated by evaluating the results using ROUGE metrics. The outcomes are twofold: a remarkable reduction in serial execution time and a noteworthy enhancement over existing techniques in the scholarly domain, as evidenced by improved ROUGE-1, ROUGE-2, and ROUGE-L scores.},
  archive      = {J_ASOC},
  author       = {Muhammad Hafizul Hazmi Wahab and Nor Asilah Wati Abdul Hamid and Shamala Subramaniam and Rohaya Latip and Mohamed Othman},
  doi          = {10.1016/j.asoc.2023.110994},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {110994},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Decomposition–based multi-objective differential evolution for extractive multi-document automatic text summarization},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-state fusion informer integrating transfer learning
for metal tube bending early wrinkling prediction. <em>ASOC</em>,
<em>151</em>, 110991. (<a
href="https://doi.org/10.1016/j.asoc.2023.110991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrinkling is one of the most fatal defects of metal tube bending , which may seriously affect the forming quality and even lead to forming failure. Traditional wrinkling prediction methods fail to provide accurate results due to the complexity of multi-die coupling in the bending process and the neglect of time-varying effects. To this end, a novel early wrinkling prediction method is proposed in this paper, distinct from conventional methods, realizing to forecast future wrinkling trends during the bending process and laying the foundation for real-time wrinkling prediction. It leverages the wrinkling factor (WrF), calculated using the energy method, as temporal data during the bending process to indirectly predict future tube wrinkling trends. Since the wrinkling occurs at the beginning of the bending process, a multi-state informer-based early prediction of tube wrinkling is put forward utilizing the limited WrF collected at the start of the bending process. To meet the demand for high accuracy and efficiency of wrinkling early prediction in a dynamic process, the model pre-trained by the multi-state fusion wrinkling data from the fully bent tube is migrated to the target model through the transfer learning approach. A stainless-steel tube bending case is conducted as the verification experiment, which is simultaneously compared with the finite element analysis (FEA) result. The results show the superior prediction accuracy and higher efficiency of the proposed method mainly compared with the traditional Informer model, Transformer model, and Long Short-Term Memory (LSTM).},
  archive      = {J_ASOC},
  author       = {Zili Wang and Yujun Yuan and Shuyou Zhang and Yaochen Lin and Jianrong Tan},
  doi          = {10.1016/j.asoc.2023.110991},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {110991},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-state fusion informer integrating transfer learning for metal tube bending early wrinkling prediction},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analysis of core attribute and approximate reduct based on
the three-way decision. <em>ASOC</em>, <em>150</em>, 111117. (<a
href="https://doi.org/10.1016/j.asoc.2023.111117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction plays an important role in pattern recognition and machine learning , and the theory of rough sets has become a commonly used model for attribute reduction since its superiority in describing and quantifying vagueness and uncertainty. However, little attention has been paid to analyzing the importance of core attributes in different circumstances, and core attributes are generally all selected into the reduct without considering their differences. In this study, the role of core attributes is analyzed in terms of their impacts on classification ability, and the three-way partition of attributes is proposed to distinguish different core attributes and condition attributes. Then, a unified approximate attribute reduction framework based on the three-way decision is introduced to keep the quality and resulting classification performance of the reduct. Moreover, a general forward-adding and back-deleting heuristic algorithm is developed to effectively select important attributes and also eliminate unimportant core and condition attributes in the boundary region. Comprehensive comparative experiments and statistical significance analysis are conducted on UCI data sets. The experimental results show that our method achieves a better attribute reduction rate and classification performance and also verify that core attributes are not always indispensable.},
  archive      = {J_ASOC},
  author       = {Can Gao and Zhicheng Wang and Jie Zhou and Hang Zeng and Xiaodong Yue},
  doi          = {10.1016/j.asoc.2023.111117},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111117},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Analysis of core attribute and approximate reduct based on the three-way decision},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An archive-based self-adaptive artificial electric field
algorithm with orthogonal initialization for real-parameter optimization
problems. <em>ASOC</em>, <em>150</em>, 111109. (<a
href="https://doi.org/10.1016/j.asoc.2023.111109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a series of learning strategies are proposed to enhance the optimization ability of the artificial electric field algorithm. Orthogonal learning is an important mathematical tool that can greatly influence the adaptability of population-based optimization algorithms . This article proposes, (i) an orthogonal array-based learning strategy to generate a better initial population for the artificial electric field algorithm. Along with the changes in the initialization mechanism, this article also proposes, (ii) an archive-based self-adaptive learning strategy for an artificial electric field algorithm. The proposed learning strategy divides the population into ordinary and extraordinary sub-populations, each with distinct learning mechanisms. The ordinary sub-population utilizes six learning strategies based on three archives, which contain individuals of different quality levels. We incorporate, (iii) a mutation strategy also to update the extraordinary sub-population. Finally, (iv) a self-adaptive strategy is implemented to dynamically adjust the parameters of the proposed algorithm. The effectiveness of these mechanisms is assessed through an extensive analysis of exploration–exploitation dynamics and diversity. Furthermore, an independent structural study is conducted to examine the impact of implemented mechanisms on the algorithm’s behavior and efficiency. The proposed algorithm is evaluated on real parameter CEC 2017 problems across different dimensional search spaces . It is compared to eleven state-of-the-art algorithms, and the results demonstrate superior performance in terms of solution accuracy, convergence rate, search capability, and stability. The overall ranking highlights its exceptional potential for solving challenging optimization problems . Additionally, it outperforms other state-of-the-art algorithms across various dimensions, achieving accuracy rates of 64.48%, 70.05%, 78.73%, and 79.25% for dimensions 10, 30, 50, and 100, respectively. Furthermore, it demonstrates superior performance, outperforming others in 73.13% and 60.61% of the problems concerning average accuracy and statistical significance across all dimensions, respectively.},
  archive      = {J_ASOC},
  author       = {Dikshit Chauhan and Anupam Yadav},
  doi          = {10.1016/j.asoc.2023.111109},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111109},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An archive-based self-adaptive artificial electric field algorithm with orthogonal initialization for real-parameter optimization problems},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Input-parameter optimization using a SVR based ensemble
model to predict landslide displacements in a reservoir area – a
comparative study. <em>ASOC</em>, <em>150</em>, 111107. (<a
href="https://doi.org/10.1016/j.asoc.2023.111107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For reservoir landslides with limited in situ monitoring data , improving the accuracy of an ensemble prediction model through historical displacement-related factor deconstruction and model optimization is challenging. Unfortunately, even though various support vector regression (SVR) based ensemble prediction models have been proposed for this purpose, the optimal combination of new algorithms and their degree of improved prediction accuracy is still unclear. Based on four evaluation indicators, this paper presents a comparative study of a typical landslide displacement prediction model containing four important nodes (landslide displacement decomposition, inducing factor frequency-component extraction, input parameter selection, and optimization algorithm selection) vital to prediction accuracy. The empirical mode decomposition (EMD) series model was adopted to decompose the landslide displacement and extract the frequency component of the factors that affect the landslide movement. Four trajectory similarity judgment models were used to select input variables for the SVR-based prediction model. Nine swarm intelligence (SI) algorithms were used to help optimize the landslide prediction model. A case study shows that the SVR-based ensemble landslide prediction model works well in predicting the displacement of a slow-moving landslide. The combination of EEMD-CEEMDAN-LCSS-PSO yields the best prediction performance, with MAPE, RMSE , MAE , and R 2 values of 0.004, 2.38, 8.64, and 0.96, respectively. The findings of this study can impact further studies of landslide ensemble prediction model optimization with insights into the algorithm selection , parameter setting, evaluation measures, and experimental settings.},
  archive      = {J_ASOC},
  author       = {Junrong Zhang and Chengyuan Lin and Huiming Tang and Tao Wen and Dwayne D. Tannant and Bocheng Zhang},
  doi          = {10.1016/j.asoc.2023.111107},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111107},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Input-parameter optimization using a SVR based ensemble model to predict landslide displacements in a reservoir area – a comparative study},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Solving multimodal optimization problems by a
knowledge-driven brain storm optimization algorithm. <em>ASOC</em>,
<em>150</em>, 111105. (<a
href="https://doi.org/10.1016/j.asoc.2023.111105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal optimization problem (MMOP) refers to the problem having more than one optima or satisfied solution in the decision space. The accuracy and diversity of solutions should be considered when solving MMOPs. In the brain storm optimization (BSO) algorithm, the information on current solutions is analyzed, but the information on previous solutions needs to be more effectively used to guide the search. A knowledge-driven BSO in objective space (KBSOOS) algorithm is proposed to enhance the search performance and to maintain the diversity of the solutions for solving MMOPs. In addition, a diversity indicator is proposed as a quantitative measurement to measure the performance of various algorithms for solving MMOPs. The 30 nonlinear equation system (NES) problems are modeled as MMOPs and solved by six swarm intelligence algorithms to validate the proposed algorithm’s performance. Based on the experimental results, the diversity indicator could give a good indication of the performance of algorithms, and the KBSOOS algorithm could enhance the performance of various BSO algorithms.},
  archive      = {J_ASOC},
  author       = {Shi Cheng and Xueping Wang and Mingming Zhang and Xiujuan Lei and Hui Lu and Yuhui Shi},
  doi          = {10.1016/j.asoc.2023.111105},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111105},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Solving multimodal optimization problems by a knowledge-driven brain storm optimization algorithm},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing relocation rules with genetic programming for the
container relocation problem with multiple bays and container groups.
<em>ASOC</em>, <em>150</em>, 111104. (<a
href="https://doi.org/10.1016/j.asoc.2023.111104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container relocation problem (CRP) is an NP-hard combinatorial optimisation problem that arises in yard management. The problem is concerned with loading all containers from the storage yard to the ship in a certain order. The yard layout consists of bays where containers are placed in stacks on top of each other, and each container has a due date that determines their retrieval order. Due to its complexity, heuristic methods are used to solve CRP, ranging from relocation rules to metaheuristics . Relocation rules (RRs) are used when the goal is to obtain a solution of acceptable quality in short time. Manually designing RRs is difficult and time-consuming, which motivates the use of different methods to automatically design RRs. In this study, we investigate the application of genetic programming (GP) to design RRs for CRP with multiple bays and container groups. The GP algorithm was adapted for generating RRs by proposing a new set of terminals and several solution construction methods. The proposed method was evaluated on an extensive benchmark of existing problems. The results obtained with automatically developed RRs were compared with the results of manually designed RRs and it was found that the automatically designed RRs performed significantly better in all cases.},
  archive      = {J_ASOC},
  author       = {Marko Đurasević and Mateja Đumić},
  doi          = {10.1016/j.asoc.2023.111104},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111104},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Designing relocation rules with genetic programming for the container relocation problem with multiple bays and container groups},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian distribution resampling via chebyshev distance for
food computing. <em>ASOC</em>, <em>150</em>, 111103. (<a
href="https://doi.org/10.1016/j.asoc.2023.111103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of data imbalance often occurs in the real-world food domain. Traditional classification algorithms are prone to overfitting on imbalanced datasets, and the decision surface will be biased toward majority-class samples, making it difficult to identify minority-class samples. Although previous resampling techniques can deal with the imbalance problem by balancing the dataset, they may produce class overlap because the anchor samples are not appropriately selected and the generated dataset does not conform to the original distribution. This paper proposes an adaptive resampling technique based on Gaussian distribution oversampling combined with random undersampling (GDRS) to address the abovementioned problems. The technique is based on the Chebyshev distance combining the weight information of the minority-class samples to select a suitable anchor sample. A new dataset conforming to the original distribution is generated in the form of a Gaussian distribution around the anchor sample. Then the random undersampling technique is combined to reduce the possibility of overfitting. The technique is applied to five UCI datasets and compared with seven imbalanced learning methods. The experimental results demonstrate that our method GDRS yields optimal performance . We also validate the effectiveness of our method in dealing with real dairy datasets with different imbalance ratios , which is promising for application in the food field.},
  archive      = {J_ASOC},
  author       = {Tianle Li and Enguang Zuo and Chen Chen and Cheng Chen and Jie Zhong and Junyi Yan and Xiaoyi Lv},
  doi          = {10.1016/j.asoc.2023.111103},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111103},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Gaussian distribution resampling via chebyshev distance for food computing},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Evolutionary multi-objective attribute community detection
based on similarity fusion strategy with central nodes. <em>ASOC</em>,
<em>150</em>, 111101. (<a
href="https://doi.org/10.1016/j.asoc.2023.111101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, evolutionary multi-objective based community detection algorithms are widely used in attribute networks, but such algorithms usually ignore the attributes between nodes and may lead to incorrect node division. Therefore, this paper proposes an evolutionary multi-objective attribute community detection based on similarity fusion strategy with central nodes. First, this paper proposes a pre-processing of similarity fusion to completely utilize node and topology information, the topological similarity matrix of the network is effectively combined with the attribute similarity matrix to obtain the fusion similarity matrix, and finds central nodes based on node assignment of the fusion similarity matrix. Then, the pre-division set of the attribute network is selected by central nodes and the label update equation is designed. In the scheme of evolutionary algorithm , using the community results initialized for the network after the label update can speed up the iterative process of the algorithm. Finally, a modularity-based community integration strategy is proposed to correct community detection results of attribute network based on modularity of neighbor nodes. Comparing the effectiveness of the proposed algorithm with four excellent community detection algorithms for attribute networks on fifteen real networks and six synthetic networks, the proposed algorithm can achieve high division accuracy in most networks.},
  archive      = {J_ASOC},
  author       = {Weitong Zhang and Kejia Zhao and Ronghua Shang},
  doi          = {10.1016/j.asoc.2023.111101},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111101},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Evolutionary multi-objective attribute community detection based on similarity fusion strategy with central nodes},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-view feature fusion and density-based minority
over-sampling technique for amyloid protein prediction under imbalanced
data. <em>ASOC</em>, <em>150</em>, 111100. (<a
href="https://doi.org/10.1016/j.asoc.2023.111100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amyloid fibers formed by the aggregation of amyloid proteins can lead to various neurological diseases. Accurate prediction of amyloid proteins can provide scientific basis for exploring the pathogenesis of related neurological diseases, and developing targeted treatment plans. In view of the limitations of the synthetic minority over-sampling technique (SMOTE), an improved SMOTE method based on density clustering is proposed in this paper, and combined with multi-view features to construct an amyloid protein prediction model. The benefits of this enhanced SMOTE method include eliminating problematic samples, accomplishing an evenly distributed minority sample pool, and utilizing all available boundary sample information. Compared with other over-sampling methods, this method can effectively improve the authenticity and representativeness of the synthesized samples, and has a strong ability to process imbalanced data . After the dataset is over-sampling, different prediction algorithms are trained respectively to construct the baseline models . To fully characterize proteins, the probability and category features are extracted from the baseline models and combined with the sequence features selected by CFS-GSSS (Correlation-based Feature Selection Combined with Greedy Stepwise Search Strategy) to generate the multi-view features. Experimental results demonstrate the complementarity and feasibility of the multi-view features. On the independent test set, compared with the existing best model (ECAmyloid), the proposed amyloid protein prediction model improved the sensitivity and geometric mean by 0.0706 and 0.0129, respectively, far superior to other existing methods.},
  archive      = {J_ASOC},
  author       = {Runtao Yang and Jiaming Liu and Qian Zhang and Lina Zhang},
  doi          = {10.1016/j.asoc.2023.111100},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111100},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-view feature fusion and density-based minority over-sampling technique for amyloid protein prediction under imbalanced data},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven multi-step energy consumption forecasting with
complex seasonality patterns and exogenous variables: Model accuracy
assessment in change point neighborhoods. <em>ASOC</em>, <em>150</em>,
111099. (<a href="https://doi.org/10.1016/j.asoc.2023.111099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy forecasting tools became a significant scope of application for time series modeling due to the specific challenges in energy trading — the forecast of consumption for the whole next trading day based on the limited data availability at the forecast origin. The research article addresses the scope of high-frequency time series data , multiple seasonal patterns , exogenous variables , and nonstationary properties in a multi-step forecast horizon tasks. The contribution of the research is the introduction of a machine- and deep-learning-based data-driven approach for multi-output time series forecasting and mainly an introduction of the new evaluation metric called the Change Point Neighborhood Error (CPNE). The purpose of the metric is to provide a distinctive measure of forecasting accuracy of the proposed models in parts of the time series where a change point or a data drift emerges. The experimental findings indicate a notable improvement in accuracy achieved by machine and deep learning models , resulting in a substantial reduction of the mean absolute percentage error (MAPE) by approximately 45 % compared to the optimal statistical model across both datasets used, and also in terms of change point neighborhood error in comparison to statistical models due to their requirement for stationary data input. Deep learning models may be a viable alternative to machine learning approaches; however, deep learning models require long input sequences for accurate forecasting, whereas machine learning methods require shorter input sequences and can benefit more from feature engineering.},
  archive      = {J_ASOC},
  author       = {Radek Svoboda and Vojtech Kotik and Jan Platos},
  doi          = {10.1016/j.asoc.2023.111099},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111099},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven multi-step energy consumption forecasting with complex seasonality patterns and exogenous variables: Model accuracy assessment in change point neighborhoods},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of the soft computing methods shaping
the future of the metaverse. <em>ASOC</em>, <em>150</em>, 111098. (<a
href="https://doi.org/10.1016/j.asoc.2023.111098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse is an emerging technology with the potential to revolutionize our interactions with digital environments. Soft computing presents exciting opportunities in shaping this immersive virtual world. This paper provides a systematic review of the research on soft computing methods in the metaverse, highlighting the interdisciplinary nature of the field and the need for coordination to shape its future. The systematic literature review conducted in this article identifies the contributors and domains in soft computing, emphasizing the need for new developments and joint applications in soft computing and the metaverse. The study categorizes soft computing techniques into five classes - machine learning , fuzzy systems, evolutionary computing, probability analysis, and mixed/hybrid methods - contributing to the emerging metaverse-related research and development. We propose a decision framework for selecting the most suitable soft computing method to assist researchers and developers in methodically assessing the alternative methods. The findings provide a roadmap and opportunities for soft computing models and applications shaping the future of the metaverse. This article can serve as a useful reference for researchers, practitioners, and policymakers working in soft computing and the metaverse.},
  archive      = {J_ASOC},
  author       = {Madjid Tavana and Shahryar Sorooshian},
  doi          = {10.1016/j.asoc.2023.111098},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111098},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic review of the soft computing methods shaping the future of the metaverse},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cyclic style generative adversarial network for near
infrared and visible light face recognition. <em>ASOC</em>,
<em>150</em>, 111096. (<a
href="https://doi.org/10.1016/j.asoc.2023.111096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near Infrared and Visible Light (NIR-VIS) face recognition attracts attention from researchers because of its potential for safety, illumination invariance, and stability. Nevertheless, the difference between NIR and VIS domains, the domain gap, remains a huge problem for matching NIR and VIS images. Specifically, for the same identity, the appearance in the NIR domain is different from the VIS domain. Thus, traditional face recognition methods cannot be effective. To address this problem, this paper proposes a novel model, called Cyclic-Style GAN (CS-GAN). First, the pre-trained Style-GAN 3 network is embedded into the Cycle-GAN structure for NIR-VIS cross-domain learning. Second, there is a cyclic subspace learning method consisting of latent loss and style loss, through which both style (domain feature) and facial characteristic features are learned to improve the quality of synthesized images. The model synthesizes realistic VIS images from NIR images and does the face recognition task in the VIS domain. The proposed method achieves 99.6% Rank-1 accuracy on the CASIA NIR-VIS 2.0 database which is a state-of-the-art result. The visualization results show that the proposed model synthesizes VIS images with a clear texture of faces and in close-to-reality color.},
  archive      = {J_ASOC},
  author       = {Fangzheng Huang and Xikai Tang and Chao Li and Dayan Ban},
  doi          = {10.1016/j.asoc.2023.111096},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111096},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Cyclic style generative adversarial network for near infrared and visible light face recognition},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble clustering with low-rank optimal laplacian matrix
learning. <em>ASOC</em>, <em>150</em>, 111095. (<a
href="https://doi.org/10.1016/j.asoc.2023.111095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The co-association (CA) matrix that describes connection relationship between instances is of importance for ensemble clustering. Existing ensemble clustering methods demonstrate that Laplacian matrix can help to improve the quality of CA matrix and finally produce better clustering results . However, they usually consider the first-order information only and ignore the higher-order information that exists among base clustering results. To address this issue, this paper introduces the theory of higher-order connectivity in ensemble clustering for the first time and further proposes a low-rank optimal Laplacian matrix learning (LROLML) approach for ensemble clustering. Specifically, the proposed LROLML first constructs a set of multi-order Laplacian matrices and then learns the optimal Laplacian matrix from the Laplacian matrix set with different orders. The optimal Laplacian matrix mines more higher-order link information, thereby improving the ensemble performance. We use alternating direction method of multipliers (ADMM) to solve the optimization problem and then apply the average-link hierarchical agglomerative clustering to get the final result. We conducted extensive experiments on 20 popular datasets to validate the proposed LROLML, which is compared with two types of results of basic k-means clustering and seven state-of-the-art ensemble clustering models. The results demonstrate the LROLML is superior to the compared models, showing that the LROLML is promising for ensemble clustering.},
  archive      = {J_ASOC},
  author       = {Jiaxuan Xu and Taiyong Li},
  doi          = {10.1016/j.asoc.2023.111095},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111095},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ensemble clustering with low-rank optimal laplacian matrix learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Layer factor analysis in convolutional neural networks for
explainability. <em>ASOC</em>, <em>150</em>, 111094. (<a
href="https://doi.org/10.1016/j.asoc.2023.111094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanatory methods that focus on the analysis of the features encoded by Convolutional Neural Networks (CNNs) are of great interest, since they help to understand the underlying process hidden behind the black-box nature of these models. However, to explain the knowledge gathered in a given layer, they must decide which of the numerous filters to study, further assuming that each of them corresponds to a single feature . This, coupled with the redundancy of information, makes it difficult to ensure that the relevant characteristics are being analyzed. The above represents an important challenge and defines the aim and scope of our proposal. In this paper we present a novel method, named Explainable Layer Factor Analysis for CNNs (ELFA-CNNs), which models and describes with quality convolutional layers relying on factor analysis. Regarding contributions, ELFA obtains the essential underlying features, together with their correlation with the original filters, providing an accurate and well-founded summary. Through the factorial parameters we gain insights about the information learned, the connections between channels, and the redundancy of the layer, among others. To provide visual explanations in a similarly way to other methods, two additional proposals are made: a) Essential Feature Attribution Maps (EFAM) and b) intrinsic features inversion. The results prove the effectiveness of the developed general methods. They are evaluated in different CNNs (VGG-16, ResNet-50, and DeepLabv3+) on generic datasets (CIFAR-10, imagenette, and CamVid). We demonstrate that convolutional layers adequately fit a factorial model thanks to the new metrics presented for factor and fitting residuals ( D 1 D1 , D &gt; , D&amp;gt;, and Res, derive from covariance matrices). Moreover, knowledge about the deep image representations and the learning process is acquired, as well as reliable heat maps highlighting regions where essential features are located. This study effectively provides an explainable approach that can be applied to different CNNs and over different datasets.},
  archive      = {J_ASOC},
  author       = {Clara I. López-González and María J. Gómez-Silva and Eva Besada-Portas and Gonzalo Pajares},
  doi          = {10.1016/j.asoc.2023.111094},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111094},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Layer factor analysis in convolutional neural networks for explainability},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus-based generalized TODIM approach for occupational
health and safety risk analysis with opinion interactions.
<em>ASOC</em>, <em>150</em>, 111093. (<a
href="https://doi.org/10.1016/j.asoc.2023.111093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupational health and safety (OHS) risk analysis serves as a foundation for identifying, preventing, and controlling OHS hazards to reduce occupational accidents . As a representative risk analysis approach, Fine-Kinney has been commonly applied to control hazards . However, current Fine-Kinney studies ranked hazards without considering the consensus reaching process (CRP) with incomplete information , insufficient to tackle decision makers’ (DMs’) dissatisfaction. Besides, risk analysis mainly relies on DMs’ subjective assessments, where opinion interactions inevitably exist because of DMs’ communication during the assessment process. This paper aims to develop a hybrid generalized TODIM (an acronym in Portuguese for Interactive Multi-criteria Decision Making) approach in the Fine-Kinney framework, integrating CRP with dynamic social influence network (SIN), and probabilistic linguistic terms (PLTSs). The PLTSs are used to cope with the complex and incomplete DMs’ opinions. The dynamic SIN is proposed to calculate the weights of DMs and describe the opinion interactions considering the psychological behaviors of DMs. Then, a new CRP is developed including a two-fold personalized feedback mechanism based on the dynamic SIN. The generalized TODIM method is introduced to rank all identified potential occupational hazards based on the collective opinions after CRP. Finally, a numerical example is conducted to verify the efficiency of the proposed approach. Comparison and sensitivity studies are also carried out to test the rationality and efficiency of the proposed approach.},
  archive      = {J_ASOC},
  author       = {Jing Tang and Xinwang Liu and Weizhong Wang},
  doi          = {10.1016/j.asoc.2023.111093},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111093},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Consensus-based generalized TODIM approach for occupational health and safety risk analysis with opinion interactions},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnosis of photovoltaic faults using digital twin and
PSO-optimized shifted window transformer. <em>ASOC</em>, <em>150</em>,
111092. (<a href="https://doi.org/10.1016/j.asoc.2023.111092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a new method for the detection, localization, and classification of grid-connected photovoltaic (PV) array faults. Line-to-line, open-module, shorted-module, open-string, and shorted-string faults as well as partial shading conditions are studied. The proposed method has two stages, which are (1) detection and localization of faults and (2) classification of faults. In the first stage, detection and localization are performed using a digital twin (DT) by analyzing the current ratio of each PV array. The measured DC (direct current) power after the operation of the DC/DC boost converter in the physical object is firstly converted into a 2-dimensional image using a recurrence plot (RP) and is then inputted to the second stage. A deep learning-based shifted windows (swin) transformer optimized by particle swarm optimization (PSO) is used in the classification stage, eliminating the need for model tuning by trial-and-error. A PV system of ten arrays with 49 kW is studied. The coefficients of determination (R 2 ) between the results of the digital object (digital twin) and the physical object for different scenarios demonstrate the accuracy and success of the digital twin. R 2 values of 0.99988 for varying irradiation with constant temperature and 0.97923 for constant irradiation with varying temperature indicate strong correlations between the digital and physical objects, further confirming the applicability of the digital twin in PV fault detection. The comparative evaluation of the PSO-optimized swin transformer against classical machine learning algorithms and state-of-the-art convolutional neural networks reveals the superior performance of the proposed method. It achieves an outstanding classification accuracy of 98.55%, demonstrating its ability to effectively classify various types of PV faults. The results of the area under the curve (AUC) of the receiver operating characteristic (ROC) curve, which measures the trade-off between true positive rate (TPR) and false positive rate (FPR), further illustrate the effectiveness of the proposed method for PV fault classification.},
  archive      = {J_ASOC},
  author       = {Ying-Yi Hong and Rolando A. Pula},
  doi          = {10.1016/j.asoc.2023.111092},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111092},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Diagnosis of photovoltaic faults using digital twin and PSO-optimized shifted window transformer},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved energy management of chiller system with AI-based
regression. <em>ASOC</em>, <em>150</em>, 111091. (<a
href="https://doi.org/10.1016/j.asoc.2023.111091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research aims to enhance energy management in commercial building air-conditioning systems, specifically focusing on chillers, which are significant energy consumers. This study evaluates various regularized regression models using comprehensive time series operating data from a system comprising five chillers of two distinct capacities. Compared with lasso and elastic net regression, ridge regression exhibits superior performance metrics when optimized with the appropriate hyperparameter. This makes it the most suitable method for modeling the system coefficient of performance (SCOP), thereby facilitating the development of an effective energy management plan. Key variables that strongly influence SCOP include the part load ratios of operating chillers, the operating numbers of chillers and pumps, and the temperatures of chilled water and condenser water. This study further identifies July as the month with the highest potential for performance improvement based on the predicted benchmark regions. This study introduces a novel approach that balances feature selection, model accuracy, and optimal tuning of hyperparameters. It highlights the significance of a generic and simplified chiller system model in evaluating energy management opportunities for sustainable operation, taking into account the distinct performance characteristics and time series features of individual system components. The findings from this research can guide future efforts towards more energy-efficient and sustainable operations in commercial buildings.},
  archive      = {J_ASOC},
  author       = {Fu-Wing Yu and Wai-Tung Ho and Chak-Fung Jeff Wong},
  doi          = {10.1016/j.asoc.2023.111091},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111091},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Improved energy management of chiller system with AI-based regression},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Interval forecasting for wind speed using a combination
model based on multiobjective artificial hummingbird algorithm.
<em>ASOC</em>, <em>150</em>, 111090. (<a
href="https://doi.org/10.1016/j.asoc.2023.111090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-term wind speed prediction is critical for enhancing the efficiency of wind power systems and assuring the stability and continuity of power generation and the host electricity markets. Various methods are available to improve the performance of wind speed prediction. However, these methods use traditional point forecasting and neglect the limitations of individual models, which cannot handle uncertainty in system operation. We propose a combined interval forecasting method that combines multiobjective artificial hummingbird algorithm, interval forecasting, and individual forecasting methods. As our proposal integrates various forecasting models including autoregressive integrated moving average, bidirectional long short-term memory, long short-term memory, and gated recurrent unit, it overcomes the limitations of single models and enhances the prediction accuracy. Experimental results show that the forecasting performance of the proposed combined interval forecasting model is considerably higher than that of similar models.},
  archive      = {J_ASOC},
  author       = {Peiqi Sun and Zhenkun Liu and Jianzhou Wang and Weigang Zhao},
  doi          = {10.1016/j.asoc.2023.111090},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111090},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Interval forecasting for wind speed using a combination model based on multiobjective artificial hummingbird algorithm},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mode standardization: A practical countermeasure against
mode collapse of GAN-based signal synthesis. <em>ASOC</em>,
<em>150</em>, 111089. (<a
href="https://doi.org/10.1016/j.asoc.2023.111089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent fault diagnosis , particularly via data-driven deep learning , is vital for industrial health management. However, it often faces challenges due to unbalanced datasets, with either insufficient fault data or redundant data representing the same condition. For signal-based diagnosis, generative adversarial networks (GANs) provide a countermeasure by generating new signals for data augmentation but can suffer from mode collapse, causing monotonous new signals and further imbalance. Inspired by the conditional mechanism, we propose mode standardization to mitigate mode collapse in GAN-based signal synthesis . This method utilizes part of the original signals as reference input to the generator. As a result, although mode collapse still occurs, and the new signals may still be monotonous, this monotony is confined to their references. In other words, as long as the references are diverse, the generated signals will maintain an acceptable diversity, similar to the original. The negative effects of mode collapse are diminished, decreasing harm to the generation process. Comparative experiments demonstrate mode standardization&#39;s effectiveness in diversity and quality on various datasets, affirming it as a practical countermeasure against mode collapse and a suitable strategy for industrial practice.},
  archive      = {J_ASOC},
  author       = {Zhenglin Dai and Liping Zhao and Ke Wang and Yanlin Zhou},
  doi          = {10.1016/j.asoc.2023.111089},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111089},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Mode standardization: A practical countermeasure against mode collapse of GAN-based signal synthesis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Frequency-based methods for improving the imperceptibility
and transferability of adversarial examples. <em>ASOC</em>,
<em>150</em>, 111088. (<a
href="https://doi.org/10.1016/j.asoc.2023.111088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adversarial attack is a popular technology to evaluate the robustness of deep learning models . However, adversarial examples crafted by current methods often have poor imperceptibility and low transferability, hindering the utility of attacks in practice. In this paper, we creatively leverage the frequency information to promote the imperceptibility and adversarial transferability in the white-box scenario and black-box scenario, respectively. Specifically, in the white-box scenario, we adopt the low-frequency constraint and normal projection to improve the imperceptibility of the adversarial example without reducing the attack performance. In the black-box scenario, we propose an effective Frequency Spectrum Diversity Transformation (FSDT) to address the issue of overfitting to the substitute model. FSDT enriches the input with a diverse set of unfamiliar information, significantly improving the transferability of adversarial attacks. Towards those defended target models in the black-box scenario, we also design a gradient refinement technology named Frequency Dropout (FD) to discard some useless components of gradients in the frequency domain, which can further mitigate the protective effect of defense mechanisms. Plentiful experiments forcefully validate the superiority of our proposed methods. Furthermore, we apply the proposed method to evaluate the robustness of real-world online models and discover their vulnerability. Finally, we analyze why imperceptibility and adversarial transferability are hard to improve concurrently from the view of frequency. Our codes are available at https://github.com/RYC-98/FSD-MIM-and-NPGA .},
  archive      = {J_ASOC},
  author       = {Hegui Zhu and Yuchen Ren and Chong Liu and Xiaoyan Sui and Libo Zhang},
  doi          = {10.1016/j.asoc.2023.111088},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111088},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Frequency-based methods for improving the imperceptibility and transferability of adversarial examples},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Active distribution network operational optimization
problem: A multi-objective tuna swarm optimization model. <em>ASOC</em>,
<em>150</em>, 111087. (<a
href="https://doi.org/10.1016/j.asoc.2023.111087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes multi-objective tuna swarm optimization through multi-objective transformation, initialization improvement and population variation for active distribution network (ADN). The ADN energy optimization model with dynamic reconfiguration , reactive power compensation, on-load tap changer and controllable load coordinated control are established with the minimum economic and environmental cost. Several controllable resources increase and increase the complexity of energy optimization problem in realized the effective clean energy consumption and the power grid stable operation. The minimum control cost and the minimum node voltage deviation are proposed. This study also proposes a decision-making method based on pareto front and an index to evaluate the maximum extensible dimension of intelligent algorithms in the process of analyzing energy optimization problems with intelligent algorithms. The proposed ADN energy optimization method based on multi-objective tuna swarm optimization shows excellent results after testing with the improved IEEE33 system. The voltage deviation and network loss are reduced by 51.62% and 22.16% on average compared with the single means control. The proposed model provides the support for the new energy consumption and the power grid stable operation, has strong engineering significance in the intelligent upgrading and power grid active control transformation.},
  archive      = {J_ASOC},
  author       = {Ling-Ling Li and Bing-Xiang Ji and Ming K. Lim and Ming-Lang Tseng},
  doi          = {10.1016/j.asoc.2023.111087},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111087},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Active distribution network operational optimization problem: A multi-objective tuna swarm optimization model},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Siamese learning based on graph differential equation for
next-POI recommendation. <em>ASOC</em>, <em>150</em>, 111086. (<a
href="https://doi.org/10.1016/j.asoc.2023.111086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next Point-of-Interest (POI) recommendation is highly challenging in its ill-posedness of data sparsity and elusive motives. Many models, including sequence- and graph-based, have been proposed to alleviate these problems. However, they still contain drawbacks: they either need a more accurate depiction of users’ complex trajectories or novel perspectives on temporal-variant features in the discretized sequences of visits. To better cope with the two challenges, we proposed a novel time-continuous model, namely POIGDE . Our model explicitly exploits continuous variation of users’ interests by solving a graph differential equation (GDE) on users’ interaction behaviors. To maintain an invariant distribution while solving GDEs, we utilize a time-serial graph along with an interval-aware attention mechanism to learn the dynamics of interest transference. This novel update mechanism helps to track the original data distribution by confining the update process in a linear combination manner. Under this restriction, our model can obtain item representations for prediction, which share the same distribution as the actual POIs with which users interact. This practice also applies Siamese Learning in the POI recommendation to directly learn from the representation of ground-truth labels. By comparison with positive samples, the inferred probability of POIs being visited in the future can be obtained based on similarity (i.e., mean square error) between high-dimensional representations of the corresponding POIs. Our model outperforms state-of-the-art models on three real-world datasets by 0.16-23.44%, showing its potential and prospects in the POI recommendation domain.},
  archive      = {J_ASOC},
  author       = {Yuxuan Yang and Siyuan Zhou and He Weng and Dongjing Wang and Xin Zhang and Dongjin Yu and Shuiguang Deng},
  doi          = {10.1016/j.asoc.2023.111086},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111086},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Siamese learning based on graph differential equation for next-POI recommendation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A multi-objective cooperation search algorithm for cascade
reservoirs operation optimization considering power generation and
ecological flows. <em>ASOC</em>, <em>150</em>, 111085. (<a
href="https://doi.org/10.1016/j.asoc.2023.111085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing attention focused on energy generation and environmental conservation, the operation of cascade reservoirs that considers power generation benefits and ecological flow requirements plays an increasingly important role in water resource and power systems . However, the traditional optimization methods may fail to solve complex cascade reservoirs operation models with strong spatial-temporal coupling physical constraints. To effectively address this problem, this study proposes an efficient multi-objective cooperation search algorithm (MOCSA) that utilizes the modified team communication, reflective learning operator and internal competition operators to enhance global exploration and local exploitation. MOCSA is tested on a group of multi-objective benchmark functions and real-world constrained engineering problems. The results demonstrate that MOCSA can provide better results for most benchmark test functions in comparison with the competitive algorithms. Besides, MOCSA exhibits excellent search ability to find feasible solutions of constrained engineering problems. The proposed method is then applied to resolve a real-world reservoir system under different operation scenarios. Simulations indicate that MOCSA provides diversified decision options for the operation of cascade reservoir system and find a more diverse set of non-dominated solutions within the feasible solution space . Overall, this research presents MOCSA as an effective multi-objective optimization tool for complex cascade reservoir operation problems.},
  archive      = {J_ASOC},
  author       = {Zhong-kai Feng and Li Zhang and Li Mo and Yong-qiang Wang and Wen-jing Niu},
  doi          = {10.1016/j.asoc.2023.111085},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111085},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A multi-objective cooperation search algorithm for cascade reservoirs operation optimization considering power generation and ecological flows},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recommendation rules to personalize itineraries for tourists
in an unfamiliar city. <em>ASOC</em>, <em>150</em>, 111084. (<a
href="https://doi.org/10.1016/j.asoc.2023.111084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, tourism has become increasingly related to new technologies. In this context, itinerary planning and trip recommendation are challenging tasks for tourists with different profiles. Tourists are generally not familiar with different Points-of-Interest (POIs) in a new city. To this end, they need to select and organize POIs that align with their interest preferences and trip constraints such as departure location, available trip duration, etc. as an itinerary. In this paper, we propose to extract recommendation rules using Multi-Objective Evolutionary Algorithms (MOEAs), to find a trade-off between two objectives: (1) maximizing the POIs popularity, and (2) maximizing the time user interest. We conducted a comparative study of several Multi-Objective Evolutionary Algorithms (MOEAs), such as NSGAII, SPEA2, and IBEA , based on the Flickr dataset of different cities. Our findings confirm the efficiency of NSGA-II in generating recommendation rules to personalize itineraries for tourists in unfamiliar cities.},
  archive      = {J_ASOC},
  author       = {Ines Gasmi and Makram Soui and Khaoula Barhoumi and Mourad Abed},
  doi          = {10.1016/j.asoc.2023.111084},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111084},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Recommendation rules to personalize itineraries for tourists in an unfamiliar city},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Three-dimension object detection and forward-looking
control strategy for non-destructive grasp of thin-skinned fruits.
<em>ASOC</em>, <em>150</em>, 111082. (<a
href="https://doi.org/10.1016/j.asoc.2023.111082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dynamic non-destructive grasp of thin-skinned fruits using flexible robotic hands , which requires obtaining three-dimension(3D) spatial structure information along with adaptive planning and motion control toward the target object, is a challenging topic in agricultural intelligence. To tackle the issue of 3D detection, we utilize RGB images and LiDAR point clouds for feature extraction and construct a multi-modal depth fusion convolution neural network (MDF-CNN) to obtain the classification information and perform image segmentation . Incorporating the advantages of a variable palm structure, we establish an evaluation mechanism of the optimal grasping stability (EM-OGS) using a hybrid method of the best configuration and force closure to build a new comprehensive performance optimal configuration planning (CPO-CP) method that is based on the multiple grasping performance indexes. We also create three cross-related nonlinear prediction models P-MGF, P-ODAP, and P-OBA along with a forward-looking non-destructive grasp control algorithm (FL-NGCA) using minimum grasping force to grasp thin-skinned fruits. The control algorithm carries out online, self-directed learning in the actual grasping process of the flexible hand and constantly optimizes the accuracy of the prediction model. Experimental results show that our proposed approach greatly improves the flexible hand comprehensive performance of grasping, outperforming state-of-the-art methods for non-destructive grasping of delicate fruits in most areas.},
  archive      = {J_ASOC},
  author       = {Xiaoyan Chen and Yilin Sun and Qiuju Zhang and Xuesong Dai and Shen Tian and Yongxin Guo},
  doi          = {10.1016/j.asoc.2023.111082},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111082},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-dimension object detection and forward-looking control strategy for non-destructive grasp of thin-skinned fruits},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A problem transformation-based and decomposition-based
evolutionary algorithm for large-scale multiobjective optimization.
<em>ASOC</em>, <em>150</em>, 111081. (<a
href="https://doi.org/10.1016/j.asoc.2023.111081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For large-scale multi-objective optimization problems , the search space becomes exceptionally vast, resulting in increased complexity in the search process. The search space usually contains several local optimal individuals, and the difficulty of finding the global optimal individuals increases greatly. A problem transformation-based and decomposition-based large-scale multi-objective evolutionary algorithm is proposed to solve the problem of reducing the dimensionality of the search space and the optimization of populations. Reducing the number of optimization problems in the decision space through problem transformation has lowered the complexity of multi-objective optimization problems and enhanced computational efficiency. In the decision space, employing two direction vectors adaptively guides the generation of promising individuals, thereby preventing the population from falling into local optima. Due to the dimensionality reduction of the decision space and the optimization of the individuals in the objective space , a set of optimal solutions are effectively obtained and uniformly distributed on the approximate Pareto optimal front . Experimental results show that the algorithm is highly competitive with five large-scale multi-objective evolutionary algorithms for large-scale multi-objective optimization test problems with up to 2000 decision variables.},
  archive      = {J_ASOC},
  author       = {Zhijian Xiong and Xiaojing Wang and Yu Li and Wei Feng and Yashuang Liu},
  doi          = {10.1016/j.asoc.2023.111081},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111081},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A problem transformation-based and decomposition-based evolutionary algorithm for large-scale multiobjective optimization},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep q-network-based heuristic intrusion detection against
edge-based SIoT zero-day attacks. <em>ASOC</em>, <em>150</em>, 111080.
(<a href="https://doi.org/10.1016/j.asoc.2023.111080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to process and classify zero-day attacks due to their huge damage to social Internet of Things (SIoT) systems has become a hot research issue. To solve this issue, we propose a heuristic learning intrusion detection system with Deep Q-Networks (DQN) for edge-based SIoT networks under the scenario of insufficient training samples , which is named DQN-HIDS. It is composed of an SIoT network traffic processing module and a DQN-based heuristic learning network. The SIoT network traffic processing module generates SIoT traffic samples, selects samples entering a classifier and a cybersecurity examiner center, and outputs similarity. We integrate DQN into a heuristic learning network to gradually improve its ability to identify malicious traffic. Specially, reward functions are designed according to the selected actions of the network, in order to punish the behavior of incorrectly labeling malicious samples and make variable reward functions adapt to different execution actions. The LSTM-based DQN then maximizes the cumulative expected reward to find the optimal strategy for the heuristic learning network. Consequently, DQN-HIDS gradually improves the behavior frequency of its labeling, reduces resource workloads, and increases the ability to label SIoT network traffic. Experiments show the performance of DQN-HIDS in terms of the workload of the examiner center and the queue workload of delayed samples, the rewards obtained by the DQN-based heuristic learning network, and the accuracy of the classifier. Comparisons with a state-of-the-art deep learning model and typical machine learning methods are also made, demonstrating the advantages of DQN-HIDS with fewer SIoT network traffic samples.},
  archive      = {J_ASOC},
  author       = {Shigen Shen and Chenpeng Cai and Zhenwei Li and Yizhou Shen and Guowen Wu and Shui Yu},
  doi          = {10.1016/j.asoc.2023.111080},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111080},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Deep Q-network-based heuristic intrusion detection against edge-based SIoT zero-day attacks},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A feature selection and ensemble learning based methodology
for transformer fault diagnosis. <em>ASOC</em>, <em>150</em>, 111072.
(<a href="https://doi.org/10.1016/j.asoc.2023.111072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dissolved gas analysis (DGA) data are generally used to diagnose a transformer fault. However, the measurement errors in DGA data are inevitable and will affect the accuracy and reliability of the diagnosis results. Nevertheless, so far, only a few efforts have been devoted to addressing this issue. To provide an accurate and stable transformer fault diagnosis system , a feature selection and ensemble learning based methodology is proposed. Firstly, an Overlapping Information Feature Selection (OIFS) method is proposed to select efficient features from the given feature set for classifiers . Secondly, an Intelligent Voting Ensemble Learning (IVEL) method is proposed to generate a diagnosis model based on the feature combination from the OIFS method. The reported test results show that the OIFS method outperforms existing methods in 9 out of 10 tested classifiers. Additionally, the IVEL outperforms three popular ensemble learning methods, including random forest (RF), gradient boosting decision tree (GBDT), and LightGBM, in terms of both accuracy and robustness performances. Finally, the proposed methodology (OIFS-IVEL) is applied to diagnose the transformer faults in the IEC TC 10 database, achieving a 100% accuracy in recognizing fault types and a 92.6 % accuracy in evaluating fault severity.},
  archive      = {J_ASOC},
  author       = {Shaowei Rao and Guoping Zou and Shiyou Yang and Sami Barmada},
  doi          = {10.1016/j.asoc.2023.111072},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111072},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature selection and ensemble learning based methodology for transformer fault diagnosis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stacked ensemble method based on TCN and convolutional
bi-directional GRU with multiple time windows for remaining useful life
estimation. <em>ASOC</em>, <em>150</em>, 111071. (<a
href="https://doi.org/10.1016/j.asoc.2023.111071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread popularity of sensors, time-series data of engine degradation processes have been widely applied for remaining useful life (RUL) prediction. As a result, the large-dimensional, large-scale, and multi-state data characteristics of degraded data make accurate prediction very challenging. To overcome this issue, this paper proposes a stacked integration method based on temporal convolutional network (TCN) and convolutional bi-directional gate recurrent unit (CNN-Bi-GRU) with multiple time windows for RUL prediction, which has smaller ensemble dimensions and stronger reliability and adaptability. In the proposed model, the TCN model can well overcome the limitations of large amounts of data which leads to the problem of difficulty in learning temporal relationships; CNN-Bi-GRU model is used to extract important features to solve the problem of high-dimensional data. In addition, the multi-time window method is used to enhance the adaptability of the method and increase the information obtained by the model. Compared with the popular prediction methods, the prediction results of the proposed ensemble model have been improved by about 20 % on average on data sets. In addition, on the N-CMAPSS data set with larger data volume and more diverse states, the proposed model also achieves an average improvement of about 10% over comparison methods. It shows that the proposed method enhances the reliability and applicability of prediction.},
  archive      = {J_ASOC},
  author       = {Jun Guo and Dapeng Li and Baigang Du},
  doi          = {10.1016/j.asoc.2023.111071},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111071},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A stacked ensemble method based on TCN and convolutional bi-directional GRU with multiple time windows for remaining useful life estimation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combinatorial t-way test suite generation using an improved
asexual reproduction optimization algorithm. <em>ASOC</em>,
<em>150</em>, 111070. (<a
href="https://doi.org/10.1016/j.asoc.2023.111070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the correctness and quality of a software system, it is desirable to test all possible combinations of the input parameters under various configurations. But the exhaustive testing of software systems with a large number of input parameters is practically impossible due to the combinatorial explosion problem. In order to address and mitigate this problem, combinatorial t-way strategy can be used to generate an array of test cases covering all combinations of only t input parameters. Since the minimum covering array generation (MCAG) is an NP-hard optimization problem , recently some strategies based on meta-heuristics have been used to solve this problem. Despite the usefulness of these strategies, they cannot completely solve the MCAG problem in systems with a large number of input parameters. This paper solves the mentioned MCAG problem by adapting the Asexual Reproduction Optimization (ARO) algorithm. Additionally, the ARO algorithm is improved (named ImpARO) by changing the population size, the mutation and crossover functions. The statistical analysis of the experimental results on several different configurations shows that ImpARO outperforms ARO, GALP, GS, BAPSO, DPSO, WOA , and GSTG as meta-heuristics, TConfig as a mathematical strategy, and PICT and IPOG as greedy strategies by 89%, 46%, 68%, 65%, 69%, 62%, 65%, 64%, 65%, and 59%, respectively. Moreover, various experiments show that ImpARO has faster convergence speed compared to ARO.},
  archive      = {J_ASOC},
  author       = {Einollah Pira and Mohammad Khodizadeh-Nahari},
  doi          = {10.1016/j.asoc.2023.111070},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111070},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Combinatorial t-way test suite generation using an improved asexual reproduction optimization algorithm},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On fuzzy fractional quadratic programming problems with an
application in the tourism sector. <em>ASOC</em>, <em>150</em>, 111069.
(<a href="https://doi.org/10.1016/j.asoc.2023.111069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the corporations working with logistics, many management issues involving optimization require fractional quadratic programming . Further, crisp parameters are unable to model the real life problems, where impreciseness is involved. In the present study, fuzzy fractional quadratic programming problem is studied using a hybrid method that combines analytic and numerical approaches . The original problem is converted to a crisp multiobjective problem , wherein the constraint coefficients are handled by variation of parameter technique through an interactive manner. The proposed method attempts to alleviate existing problems of straightforward conversion to a deterministic model by making constraint parameters vary as per α r ∈ ( 0 , 1 ] αr∈(0,1] . The flexibility of choices in constraint parameters with α r = 0 αr=0 leading to best solution and α r = 1 αr=1 resulting in worst solution, indicates the robust nature of proposed algorithm. The theorems have been constructed and proved to show the equivalence between the original and transformed problem. A numerical example and a transportation problem in tourism sector switching between both balanced and unbalanced cases is modelled and later solved using the proposed methodology.},
  archive      = {J_ASOC},
  author       = {Sumati Mahajan and S.K. Gupta and Izhar Ahmad},
  doi          = {10.1016/j.asoc.2023.111069},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111069},
  shortjournal = {Appl. Soft. Comput.},
  title        = {On fuzzy fractional quadratic programming problems with an application in the tourism sector},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An extensible complex spherical fuzzy decision making model
based selection framework for the food waste treatment method.
<em>ASOC</em>, <em>150</em>, 111068. (<a
href="https://doi.org/10.1016/j.asoc.2023.111068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Food waste has been recognized as one of the most severe environmental problems globally. Food waste management is an effective procedure for achieving global food security, which requires rational and efficacious waste treatment and disposal techniques. Thus, selecting proper and suitable food waste treatment methods is one of the most significant food waste management issues. However, existing selection frameworks based on decision-making models for food waste treatment methods can seldom handle this issue with complicated uncertainty and periodic information. Accordingly, this paper aims to generate a complex spherical fuzzy information-based selection framework for evaluating the food waste treatment method with mutual support and periodicity decision information. First, the complex spherical fuzzy sets express subjective and uncertain selection information. Then, the power weighted average (PWA) operator for complex spherical fuzzy numbers is introduced to fuse the decision information of mutual support from experts that can reflect the impact of information with supportive relationships. Next, an extensible ARAS (Additive Ratio Assessment) approach-based selection framework is constructed to determine the most sustainable food waste treatment method in which the distance-measure-based weighting method is utilized to calculate the importance degrees of criteria. Finally, a case study of evaluating food waste treatment methods is expounded to display the application of the proposed framework. The result indicates that the alternative a 2 a2 (Anaerobic digestion) is the most sustainable method for food waste treatment with the largest utility degree (0.920). After that, the comparison studies are also performed to illustrate further the reasonability and effectiveness of the proposed selection framework. The result shows that stakeholders can adopt the proposed methodology to improve the performance of the evaluation of food waste treatment.},
  archive      = {J_ASOC},
  author       = {Weizhong Wang and Yushuo Cao and Muhammet Deveci and Qun Wu},
  doi          = {10.1016/j.asoc.2023.111068},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111068},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An extensible complex spherical fuzzy decision making model based selection framework for the food waste treatment method},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A chinese power text classification algorithm based on deep
active learning. <em>ASOC</em>, <em>150</em>, 111067. (<a
href="https://doi.org/10.1016/j.asoc.2023.111067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of knowledge graph is beneficial for grid production, electrical safety protection, fault diagnosis and traceability in an observable and controllable way. Highly-precision text classification algorithm is crucial to build a professional knowledge graph in power system . Unfortunately, there are a large number of poorly described and specialized texts in the power business system, and the amount of data containing valid labels in these texts is low. This will bring great challenges to improve the precision of text classification models. To offset the gap, we propose a classification algorithm for Chinese text in the power system based on deep active learning (CCTP-DAL). Our core idea is to apply a hierarchical confidence strategy to a deep active learning model, to balance the trade-offs between the amount of training data and the accuracy of text classification. Our CCTP-DAL (1) trains the Bert model using a small amount of labeled data to calculate the confidence level of each short text, (2) selects high-confidence text data with optimal model generalization capability based on the hierarchical confidence level, and (3) fuses deep learning models and active learning strategies to ensure high text classification accuracy with less labeled training data. We benchmark our model on a real crawler data on the web with extensive experiments. The experimental results demonstrate that our proposed model can achieve higher text classification accuracy with less labeled training data compared with other deep learning models.},
  archive      = {J_ASOC},
  author       = {Song Deng and Qianliang Li and Renjie Dai and Siming Wei and Di Wu and Yi He and Xindong Wu},
  doi          = {10.1016/j.asoc.2023.111067},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111067},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A chinese power text classification algorithm based on deep active learning},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way imbalanced learning based on fuzzy twin SVM.
<em>ASOC</em>, <em>150</em>, 111066. (<a
href="https://doi.org/10.1016/j.asoc.2023.111066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-way decision (3WD) is a powerful tool for granular computing to deal with uncertain data, commonly used in information systems , decision-making, and medical care. Three-way decision gets much research in traditional rough set models. However, three-way decision is rarely combined with the currently popular field of machine learning to expand its research. In this paper, three-way decision is connected with SVM, a standard binary classification model in machine learning, for solving imbalanced classification problems that SVM needs to improve. A new three-way fuzzy membership function and a new fuzzy twin support vector machine with three-way membership (TWFTSVM) are proposed. The new three-way fuzzy membership function is defined to increase the certainty of uncertain data in both input space and feature space , which assigns higher fuzzy membership to minority samples compared with majority samples. To evaluate the effectiveness of the proposed model, comparative experiments are designed for forty-seven different datasets with varying imbalance ratios . In addition, datasets with different imbalance ratios are derived from the same dataset to further assess the proposed model’s performance. The results show that the proposed model significantly outperforms other traditional SVM-based methods.},
  archive      = {J_ASOC},
  author       = {Wanting Cai and Mingjie Cai and Qingguo Li and Qiong Liu},
  doi          = {10.1016/j.asoc.2023.111066},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111066},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Three-way imbalanced learning based on fuzzy twin SVM},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive surrogate assisted multi-objective optimization
approach for highly nonlinear and complex engineering design problems.
<em>ASOC</em>, <em>150</em>, 111065. (<a
href="https://doi.org/10.1016/j.asoc.2023.111065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite enormous advances in computer power, computationally costly models impede the use of traditional optimization approaches that must be invoked repeatedly during the optimization process in practical engineering applications . Surrogate models have been found to be a promising endeavor in multi-objective optimization problems involving expensive analysis and simulation processes such as multi-physics modeling and simulation , finite element analysis (FEA), and computational fluid dynamics (CFD. Developing an optimization algorithm that can easily identify the Pareto frontier of highly nonlinear multi-objective optimization problems with less computation cost is the aim of this work. In this paper, an Adaptive Multi-Objective Optimization approach based Surrogate models (AMOS) is developed to reduce computation cost of fitness evaluations and discover the Pareto optima for multi-objective optimization problems with comparable high accuracy. AMOS explores the design space by sampling using LHD to identify promising regions. Then, AMOS exploits the identified promising region by adaptively constructing the most suitable surrogate model, which could be response surface, radial basis , or Kriging surrogates, in the feasible design space based on root mean square error values (RMSE). AMOS stops iterating when a termination criterion is met, and a Pareto frontier is identified based on developed guidance and fitness functions. AMOS has successfully identified the pareto frontier of practical engineering optimization problems with expensive black box functions and significantly reduced the computation cost. The novel method was put to the test utilizing real-world challenges and engineering design examples such vehicle magnetorheological design and wind turbine airfoil geometry .},
  archive      = {J_ASOC},
  author       = {Adel Younis and Zuomin Dong},
  doi          = {10.1016/j.asoc.2023.111065},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111065},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive surrogate assisted multi-objective optimization approach for highly nonlinear and complex engineering design problems},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Short-term wind power forecasting based on
multivariate/multi-step LSTM with temporal feature attention mechanism.
<em>ASOC</em>, <em>150</em>, 111050. (<a
href="https://doi.org/10.1016/j.asoc.2023.111050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision enhancement for short-term wind power forecasting can alleviate negative impact of the forecasting results on wind power generation . Due to complexities and nonlinearities among factors and facets in wind power, it is essential to achieve reliable and stable power generation via the long short-term memory (LSTM) forecasting. To this purpose, multi-task temporal feature attention (MTTFA) based LSTM, namely MTTFA-LSTM, is proposed for multivariate/multi-step wind power forecasting with historical power and meteorological data , in which task-sharing and task-specifying layers are designed for task co-features extracting and task specifics discriminating, respectively. More specifically, in the task-sharing layer, multi-dimensional inputs are fed into LSTM to extract long-term trends, while in the task-specifying layer, one-dimensional convolution operations extract temporal features hidden in each and all time steps. Furthermore, an attention mechanism is adopted to adaptively tune weights for temporal features. Finally, the proposed model is leveraged to cope with different short-term wind power forecasting (SWPF) problems based on the national renewable energy laboratory’s (NREL) wind power data. Simulation results show that the proposed MTTFA-LSTM achieves persistent excellent forecasting accuracy , comparing its backbone STL model, TFA-LSTM as well as the benchmark MTL models in the same setting, which indicate that the complex and non-linear interdependencies among multi-dimensional data can be well depicted by the proposed model.},
  archive      = {J_ASOC},
  author       = {Xin Liu and Jun Zhou},
  doi          = {10.1016/j.asoc.2023.111050},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111050},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Short-term wind power forecasting based on multivariate/multi-step LSTM with temporal feature attention mechanism},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Software bug severity and priority prediction using SMOTE
and intuitionistic fuzzy similarity measure. <em>ASOC</em>,
<em>150</em>, 111048. (<a
href="https://doi.org/10.1016/j.asoc.2023.111048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A software bug tracking system receives several bug reports in a rapid manner during the maintenance of software. In order to fix the important and urgent bugs, the triager has to assign severity and priority to individual bugs on time. However, there are a lot of uncertainties in the bug reports due to bias, noise, and abnormal data. At the same time, the presence of common terms in multiple severity and priority classes creates confusion in the mind of the triager. Furthermore, machine learning and deep learning approaches generally belong to discriminative learning with a clear-cut outcome. Instances of software bug reports are textual in nature. As a result, these are fuzzy and cannot be classified with a clear-cut outcome. To overcome the above problems, in this paper, an Intuitionistic Fuzzy Similarity Measure (IFSM) based severity prediction technique (IFSMSP) and priority prediction technique (IFSMPP) are proposed for predicting the severity and priority of a new bug by using already labeled bugs. Initially, the Synthetic Minority Oversampling Technique (SMOTE) is used to balance the severity and priority label of software bugs. Then the severity-term dictionary or priority-term dictionary is created by extracting the most frequent terms from the bug summary using text mining and Natural Language Processing (NLP). Then the data is represented using an intuitionistic fuzzy set (IFS) by calculating the membership, non-membership, and hesitancy degrees. Then 15 different IFSM techniques are investigated for predicting the severity and priority of software bugs. Experiments are carried out on large software bug repositories (Eclipse, Mozilla, Apache, and NetBeans) with a 10-fold cross-validation technique. IFSMSP outperformed other state-of-the-art priority models by obtaining an accuracy of 92.3%, 90.6%, 91.9%, and 91.2%, and IFSMPP outperformed other state-of-the-art models by obtaining an accuracy of 93.2%, 91.9%, 92.7%, and 92.3% on the Eclipse, Mozilla, Apache, and NetBeans software bug repositories, respectively.},
  archive      = {J_ASOC},
  author       = {Rama Ranjan Panda and Naresh Kumar Nagwani},
  doi          = {10.1016/j.asoc.2023.111048},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111048},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Software bug severity and priority prediction using SMOTE and intuitionistic fuzzy similarity measure},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of metaheuristic algorithms in electric
power systems optimization. <em>ASOC</em>, <em>150</em>, 111047. (<a
href="https://doi.org/10.1016/j.asoc.2023.111047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electric power system applications are intricate optimization problems . Most literature reviews focus on studying an electrical paradigm through different optimization techniques. Since no review targets Metaheuristics (MHs) in electric power system applications, our work provides a general panorama of the paradigms that underlay such applications: Renewable Energies, Load Forecasting, Power Flow, Microgrids and Smart grids, and Power Quality. Our analysis revealed that the most employed MHs are Particle Swarm Optimization , Gray Wolf Optimizer, Genetic Algorithms , Cuckoo Search , and Differential Evolution. Historically, MHs have been classified into metaphor-based and non-metaphor-based. However, in some cases, this categorization does not correspond to pure MH procedures. Therefore, we also analyze MHs from a more formal perspective: their search operators. Moreover, we detected that the Renewable Energies paradigm presents a strong synergy with the remaining ones. Plus, there is a significant interest in topics related to Load-Forecasting optimization problems. Based on these data, we provide helpful recommendations for the current challenges and potential research paths. In doing so, our insights can support researchers and practitioners interested in furthering this field.},
  archive      = {J_ASOC},
  author       = {Gerardo Humberto Valencia-Rivera and Maria Torcoroma Benavides-Robles and Alonso Vela Morales and Ivan Amaya and Jorge M. Cruz-Duarte and José Carlos Ortiz-Bayliss and Juan Gabriel Avina-Cervantes},
  doi          = {10.1016/j.asoc.2023.111047},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111047},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A systematic review of metaheuristic algorithms in electric power systems optimization},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interval-valued intuitionistic fuzzy group
decision-making method for evaluating online knowledge payment products.
<em>ASOC</em>, <em>150</em>, 111046. (<a
href="https://doi.org/10.1016/j.asoc.2023.111046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increased demand for knowledge, evolving consumer attitudes, and the convenient online transaction opportunities provided by Internet technology have led to the rapid rise and growth of online knowledge payment. At present, studies on the online knowledge payment industry mainly focus on exploring the influencing factors of users&#39; willingness to pay for online knowledge payment products (OKPPs), and there are fewer studies on the evaluation and rating methods of OKPPs. To address this problem, this paper proposes an improved group decision-making method based on consensus adjustment and prospect theory to realize the evaluation of OKPPs from the perspective of consumer experience value. The method uses interval-valued intuitionistic fuzzy numbers to process evaluation information, selects leading users (LUs) as decision makers , and identifies four evaluation criteria based on consumer experience value, which are functional value, self-fulfillment value, hedonic value, and emotional value. The proposed group decision-making method in this paper takes into account the consensus problem of LUs’ opinions, proposes a consensus adjustment method, and uses prospect theory to incorporate the psychological factors of LUs into the group decision-making process. Finally, the effectiveness and advantages of the method proposed in this paper are verified using an example and a comparison with existing methods. This research will provide methodological reference for knowledge payment platforms (KPPs) to select high quality OKPPs, and will also urge knowledge producers (KPs) to create OKPPs that can bring higher experience value to knowledge consumers (KCs). At the same time, this research makes possible the co-creation of knowledge between KPs and KCs, and enriches and develops the relevant research on the online knowledge payment industry.},
  archive      = {J_ASOC},
  author       = {Su Jiafu and Dan Wang and Baojian Xu and Fengting Zhang and Na Zhang},
  doi          = {10.1016/j.asoc.2023.111046},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111046},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An interval-valued intuitionistic fuzzy group decision-making method for evaluating online knowledge payment products},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient evolution of decision trees via fully matrix-based
fitness evaluation. <em>ASOC</em>, <em>150</em>, 111045. (<a
href="https://doi.org/10.1016/j.asoc.2023.111045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision Trees (DTs) are a class of supervised learning models that are widely used for both classification and regression applications. They are well-known for their interpretability and robustness, which have led them to remain popular even 60 years after they were first proposed. However, because traditional tree algorithms use greedy methods that are prone to suboptimality, several works have explored the usage of evolutionary algorithms instead. Although these algorithms are often reported to outperform the traditional greedy approach , their computational cost is much higher, since the evolutionary component requires a large number (millions or billions) of function evaluations in order to produce a single tree. Aiming to reduce this computational cost, in this work we propose an encoding that allows the training and evaluation of DTs using only matrix operations . The proposed procedure is shown to be much faster than the traditional tree implementation for complete trees with depths ranging from 2 to 6, and for datasets ranging in size from 100 to 100,000 observations. In particular, the results show speedups of nearly up to 20 times, especially when the dataset is large and the desired tree is small enough to be interpretable. The proposed procedure also benefits from GPU parallelization , although it is still highly performing without it. Furthermore, we propose an evolutionary algorithm, called Coral Reef Optimization for Decision Trees (CRO-DT), that integrates this encoding with a pre-existing ensemble algorithm to evolve better univariate trees. The results obtained show that the proposed CRO-DT is competitive with traditional and modern tree algorithms, consistently producing models of good quality across 14 tested UCI Datasets. We conclude that for most relevant situations, the proposed matrix encoding provides significant speedups over the traditional implementation, and also may serve as a basis for high quality evolutionary DT algorithms .},
  archive      = {J_ASOC},
  author       = {Vinícius G. Costa and Sancho Salcedo-Sanz and Carlos E. Pedreira},
  doi          = {10.1016/j.asoc.2023.111045},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111045},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Efficient evolution of decision trees via fully matrix-based fitness evaluation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opposition-based differential evolution for synchronized
control of multi-agent systems with uncertain nonlinear dynamics.
<em>ASOC</em>, <em>150</em>, 111044. (<a
href="https://doi.org/10.1016/j.asoc.2023.111044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent systems (MAS) have gained increasing research attention in recent years due to their wide applications in cooperative control of autonomous vehicles, sensor networks, robotics, etc. However, designing effective distributed control strategies for MAS with complex dynamics remains challenging. This paper proposes an improved differential evolution (DE) based cooperative control strategy to stabilize and synchronize MAS with uncertain nonlinear dynamics . Specifically, we develop a new mutation strategy inspired by opposition-based learning to enhance the exploration ability and adopt an adaptive parameter mechanism to balance exploration and exploitation. Moreover, a distributed sliding mode term is incorporated in the control input to counteract uncertainties and external disturbances . Rigorous stability analysis shows that the closed-loop MAS can achieve synchronization under the proposed control strategy. Extensive simulations demonstrate that compared to other state-of-the-art DE algorithms and cooperative control methods , the proposed strategy achieves lower synchronization error, control cost, higher convergence speed, and higher accuracy for MAS with different scales, dynamics, and uncertainties.},
  archive      = {J_ASOC},
  author       = {Xin Wang and Dongsheng Yang and D Raveena Judie Dolly and Shuang Chen and Madini O. Alassafi and Fawaz E. Alsaadi},
  doi          = {10.1016/j.asoc.2023.111044},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111044},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Opposition-based differential evolution for synchronized control of multi-agent systems with uncertain nonlinear dynamics},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). An efficient manta ray foraging optimization algorithm with
individual information interaction and fractional derivative mutation
for solving complex function extremum and engineering design problems.
<em>ASOC</em>, <em>150</em>, 111042. (<a
href="https://doi.org/10.1016/j.asoc.2023.111042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The manta ray foraging optimization algorithm (MRFO) is a recently proposed meta-heuristic algorithm that mimics the foraging process of manta rays. It has yielded good outcomes in solving some optimization problems because its mechanism is clear, no additional parameters need to be set, and the balance between global and local search is good. Nonetheless, while dealing with high-dimensional global optimization and complex engineering optimization problems, there are also issues such as premature convergence, low optimization-seeking accuracy, or unstable solutions. To this end, this article proposes an efficient manta ray foraging optimization algorithm (NIFMRFO) by incorporating individual information interaction and fractional derivative mutation. First, to prevent premature convergence of the algorithm, a nonlinear cosine adjustment parameter is presented, which is intended to make the demand relationship between global exploration and local development more reasonable. Then, an information interaction strategy among random individuals is employed to expedite the rate at which the algorithm converges. Finally, a fractional derivative mutation strategy is utilized to continually enhance individuals’ quality in each iteration, which not only increases the population diversity but also helps to improve the precision and stability of the search results. Theoretical analysis indicates that the improved NIFMRFO algorithm and basic MRFO algorithm have the same time complexity. In simulation experiments, the CEC2017 suite is used to conduct comparison tests with six superior-performance representative comparison algorithms in several dimensions. In terms of the optimization-seeking accuracy, convergence curve, violin plot, and Friedman average ranking, the analysis of these graphs and data shows that the NIFMRFO algorithm’s ameliorated strategy improves superiority-seeking power, convergence speed, and steadiness. Meanwhile, the Wilcoxon rank-sum test result illustrates significant differences between NIFMRFO and other compared algorithms. Finally, these algorithms are utilized to tackle seven realistic engineering design optimization problems. The result makes it clear that NIFMRFO is distinctly superior to the other six algorithms, showing that its solving ability is superior and has broad application prospects.},
  archive      = {J_ASOC},
  author       = {Jingsen Liu and Yang Chen and Xiaoyu Liu and Fang Zuo and Huan Zhou},
  doi          = {10.1016/j.asoc.2023.111042},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111042},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An efficient manta ray foraging optimization algorithm with individual information interaction and fractional derivative mutation for solving complex function extremum and engineering design problems},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting the hydropower unit vibration based on adaptive
variational mode decomposition and neural network. <em>ASOC</em>,
<em>150</em>, 111040. (<a
href="https://doi.org/10.1016/j.asoc.2023.111040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The feature extraction, diagnosis and prediction of vibration and fault are important to maintain the stable operation of units with the large-scale popularization of virtual power plants in hydropower plants . Therefore, more accurate and suitable state detection methods and forecasting models for complex conditions are needed. VMD is limited by the presetting of mode number, penalty factor and initial center frequency, which greatly reduces the accuracy of the algorithm. In this study, IAVMD algorithm is proposed. The initial center frequencies and the mode number can be determined by the scale space plane and the energy loss coefficient. The numerical model of penalty factor is constructed based on center frequency differences. Finally, the mode numbers are combined and adjusted by autocorrelation and other methods. Through comparative analysis of simulation signal and experimental signal, IAVMD shows great improvements in complex signals. At the same time, it is proved that the forecasting model combining IAVMD with BO and LSTM has significantly improved the prediction accuracy. Through comparative analysis, the results prove that the prediction model constructed by this method has higher accuracy.},
  archive      = {J_ASOC},
  author       = {Zhaoheng Lu and Ran Tao and Ruofu Xiao and Puxi Li},
  doi          = {10.1016/j.asoc.2023.111040},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111040},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Forecasting the hydropower unit vibration based on adaptive variational mode decomposition and neural network},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dimensionality reduction method for large-scale group
decision-making using TF-IDF feature similarity and information loss
entropy. <em>ASOC</em>, <em>150</em>, 111039. (<a
href="https://doi.org/10.1016/j.asoc.2023.111039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an innovative approach for dimensionality reduction targeting linguistic preferences in large-scale group decision-making scenarios. This method combines TF-IDF feature similarity and information loss entropy to address the challenges of decision-making over large-scale decision makers . Firstly, text vectorization is performed to capture the semantics of the text as a TF-IDF feature matrix, which facilitates subsequent calculations. Secondly, a cluster process integrating the TF-IDF feature similarity is operated to divide the large-scale decision-maker group into several clusters. Thirdly, the selection process is activated to select representatives from among the large-scale decision-makers based on information loss entropy. Finally, a case study was conducted to test the practical feasibility of the proposed method, along with a comparative analysis to discuss the scenarios in which it is applicable.},
  archive      = {J_ASOC},
  author       = {Qifeng Wan and Xuanhua Xu and Jing Han},
  doi          = {10.1016/j.asoc.2023.111039},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111039},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A dimensionality reduction method for large-scale group decision-making using TF-IDF feature similarity and information loss entropy},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ranking intuitionistic fuzzy sets with hypervolume-based
approach: An application for multi-criteria assessment of energy
alternatives. <em>ASOC</em>, <em>150</em>, 111038. (<a
href="https://doi.org/10.1016/j.asoc.2023.111038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranking Intuitionistic Fuzzy Sets (IFS) using distance-based methods involves calculating the distance between an IFS and a reference point which represents either maximum (positive ideal solution) or minimum (negative ideal solution) value. These methods assume that similarity of an IFSs to the reference point increases as its distance from it decreases. While it is a common practice to use nonlinear distance functions for ranking IFSs, this paper proves that no nonlinear IF distance function can be robust. In this study, the shortcomings of the conventional procedure are demonstrated by providing a mathematical proof and an alternative ranking method based on the hypervolume metric is proposed. In addition, the suggested ranking approach is extended as a new multi-criteria decision making method called Hypervolume-based Evaluation and Ranking Technique (HEART). HEART is applied for multi-criteria assessment of Turkey’s energy alternatives. Results are compared with three distance based multi-criteria decision making methods: TOPSIS , VIKOR , and CODAS.},
  archive      = {J_ASOC},
  author       = {Kaan Deveci and Önder Güler},
  doi          = {10.1016/j.asoc.2023.111038},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111038},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ranking intuitionistic fuzzy sets with hypervolume-based approach: An application for multi-criteria assessment of energy alternatives},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A modified convolutional neural network with rectangular
filters for frequency-hopping spread spectrum signals. <em>ASOC</em>,
<em>150</em>, 111036. (<a
href="https://doi.org/10.1016/j.asoc.2023.111036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of frequency-hopping spread spectrum (FHSS) signal in a complex electromagnetic environment is essential due to security concerns associated with its applications, such as drones. Traditional convolutional neural networks (CNNs) include square-designed filters and pooling operators at several layers, which are appropriate for two-dimensional images. Nevertheless, the information encoded in time-frequency representation (TFR) produced by the spectrogram method is different. The time and frequency of the FHSS signal are represented along the x- and y-axis, respectively, whereas the intensity at a specific spot indicates the amplitude, thereby producing it in a rectangular form . Therefore, in this study, a modified convolutional neural network (MCNN) with rectangular filters is proposed to classify FHSS signals, with a background signal and additive white Gaussian noise being present as interference. Rectangular filters of varying shapes and sizes with max-pooling in their regions are used to extract differentiative features from the TFR using MCNN. A problem of an unbalanced dataset occurred because of the unequal observations amongst the classes, which is solved by employing the random erasing method. The developed method efficaciously acquires distinguished features from the TFR and performs better than traditional CNN with 80.44% accuracy at −2 dB of signal-to-noise ratio (SNR). The code to verify the proposed work is available at: https://github.com/DrTuryalai/MCNN .},
  archive      = {J_ASOC},
  author       = {Muhammad Turyalai Khan},
  doi          = {10.1016/j.asoc.2023.111036},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111036},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A modified convolutional neural network with rectangular filters for frequency-hopping spread spectrum signals},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HilbertSCNet: Self-attention networks for small target
segmentation of aerial drone images. <em>ASOC</em>, <em>150</em>,
111035. (<a href="https://doi.org/10.1016/j.asoc.2023.111035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many advanced semantic segmentation algorithms have made great progress in common datasets. However, in the drones aerial photography dataset, since some of the segmentation targets are very small in such high-resolution images, these current algorithms are not designed specifically for them resulting in their mediocre performance in the drones aerial photography dataset. To address the problem of difficult segmentation of these small targets, this paper proposes a new semantic segmentation network model-HilbertSCNet: (1) Combine the image dimensionality reduction algorithm of Hilbert curve traversal and the idea of dual pathway to design a new spatial computation module to solve the problem of small target information is easily lost in the downsampling; (2) Add a windowing algorithm in the image dimensionality reduction algorithm of Hilbert curve traversal to solve the problem that the computational complexity of the module is too high, which makes the module cannot be applied to high-resolution feature maps. Experiments show that the proposed network is very effective for segmentation of small targets under high-resolution maps such as drone aerial photography, with certain superiority and generalization. The overall segmentation performance is improved compared to the current new network, including 1.76% better than OCNet in MIoU and 4.55% better than the dedicated drone algorithm RCCT-ASPPNet.},
  archive      = {J_ASOC},
  author       = {Qiumei Zheng and Linkang Xu and Fenghua Wang and Yongqi Xu and Chao Lin and Guoqiang Zhang},
  doi          = {10.1016/j.asoc.2023.111035},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111035},
  shortjournal = {Appl. Soft. Comput.},
  title        = {HilbertSCNet: Self-attention networks for small target segmentation of aerial drone images},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-mechanism swarm optimization for multi-UAV task
assignment and path planning in transmission line inspection under
multi-wind field. <em>ASOC</em>, <em>150</em>, 111033. (<a
href="https://doi.org/10.1016/j.asoc.2023.111033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the growing size of the power system , the inspection and maintenance of power lines and facilities in complicated environments will be put to a greater test. Currently, more and more research are being done on UAV-based transmission line inspection techniques, of which multi-UAV task assignment and path planning are two important aspects. This paper first generates a three-dimensional inspection environment model while considering and measuring the influence of numerous wind fields in the mountain environment. Next, a multi-indicator hybrid cost function that considers wind field influence is defined for the multi-UAV task assignment problem. An improved Bidirectional Ant Colony and discrete Honey Badger Algorithm (BACOHBA) is proposed to solve it. Then, the multi-UAV path planning problem based on the multi-wind field condition is defined, and an improved Honey Badger-Fruit Fly Algorithm (HBAFOA) is proposed to solve it. Finally, simulation experiments are conducted under different algorithms, which prove that the proposed algorithm has better performance in terms of fast-solving ability, solution accuracy, and optimization stability.},
  archive      = {J_ASOC},
  author       = {Kun Li and Xinxin Yan and Ying Han},
  doi          = {10.1016/j.asoc.2023.111033},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111033},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Multi-mechanism swarm optimization for multi-UAV task assignment and path planning in transmission line inspection under multi-wind field},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of machine learning-based surrogate models for
urban flood depth modeling in ho chi minh city, vietnam. <em>ASOC</em>,
<em>150</em>, 111031. (<a
href="https://doi.org/10.1016/j.asoc.2023.111031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapid flood prediction in coastal urban areas is an important but challenging task. However, multi-driver floods in coastal areas and their non-linearity in physical processes are hard to represent in physics-based numerical models (PBNMs). In this study, we investigated the performance of surrogate machine learning (ML) models and their flood prediction capability. Initially, we utilize the MIKE+ coupled 1D–2D model to simulate coastal urban flooding in one of the severely flood-affected areas of Ho Chi Minh City (HCMC), Vietnam. Then, nine ML models, including AdaBoost (AB), Decision Tree (DT), Gaussian Process (GP), k-Nearest Neighbors (KNN), Linear Discriminant Analysis (LDA), Naive Bayes (NB), Neural Network (NN), Random Forest (RF), and Support Vector Machine (SVM) are employed to surrogate the PBNM flood prediction performance and engaged to predict flood depths of the study area domain. 806 simulation scenarios of MIKE+ modeling having a spatial grid of 1107 ×1513, grid size = 2 m, extracting 270,000 inundation points to generate input data for nine ML models are used to simulate surface flood depths for the study area. Results show three ML models, GP, RF, and NN, outperform the remaining models, with R 2 value of 0.997, 0.996, and 0.995, respectively. Thus, applying ML models can significantly reduce the simulation time by a PBNM, improve accuracy, and potentially be adopted for real-time forecasting and emergency management.},
  archive      = {J_ASOC},
  author       = {Thanh Quang Dang and Ba Hoang Tran and Quyen Ngoc Le and Thanh Duc Dang and Ahad Hasan Tanim and Quoc Bao Pham and Van Hieu Bui and Son T. Mai and Phong Nguyen Thanh and Duong Tran Anh},
  doi          = {10.1016/j.asoc.2023.111031},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111031},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Application of machine learning-based surrogate models for urban flood depth modeling in ho chi minh city, vietnam},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pairwise causal discovery with support measure machines.
<em>ASOC</em>, <em>150</em>, 111030. (<a
href="https://doi.org/10.1016/j.asoc.2023.111030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bivariate causal discovery amounts to inferring the causal association between two random variables , usually from observational data . This task is the simplest and most fundamental causal discovery problem from which more complex discovery methods can be envisioned and developed. Classical bivariate causal discovery methods exploit a combination of specific sets of assumptions and data to obtain identifiability of the causal direction. Data-driven supervised approaches train machine learning models over large sets of causally-labeled bivariate datasets to learn the task of inferring the causal relationship from data. In this work, an ensemble algorithm based on support measure machines is proposed with the aim of combining the strength of different classical approaches (base methods) with data-driven decisions. In particular, support measure machine classifiers are trained to estimate the performance of each base method. Their decision functions are then used as data-dependent weights of a weighted voting scheme to estimate the causal direction in a bivariate causal discovery problem. This work demonstrates that the proposed algorithm, denoted as Causal Ensemble Measure Machine, performs equal to or better than state-of-the-art methods on a wide range of synthetic and real-world bivariate problems. Perhaps more importantly, this method enables a closer examination of the assumption dependence of existing algorithms on observational data.},
  archive      = {J_ASOC},
  author       = {Gherardo Varando and Salvador Catsis and Emiliano Diaz and Gustau Camps-Valls},
  doi          = {10.1016/j.asoc.2023.111030},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111030},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Pairwise causal discovery with support measure machines},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven prediction of possible quality deterioration in
injection molding processes. <em>ASOC</em>, <em>150</em>, 111029. (<a
href="https://doi.org/10.1016/j.asoc.2023.111029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach for the automated prediction of possible quality deteriorations at injection molding machines using data-driven models. This approach relies on data solely recorded during the regular production phase without the need to (i.) collect data from anomalous phases, which may be cost-intensive due to production waste, (ii.) perform time-intensive (manual) annotation cycles for assigning class labels, and (iii.) know specific fault modes in advance. Our approach embeds two main concepts: the first is the establishment of causal relations between process variables , which serve as online residual generators. The multi-variate residual signals are analyzed using an advanced independent component analysis , whose reconstruction characteristics serve as a control signal indicating a potential anomaly of any type/mode, upon violation of the internal dependence structure . No parameters are needed for the online analysis, which in turn avoids the use of (pre-collected) data from anomalous phases, which is typically required to tune the parameters properly. The second concept performs a direct prediction of quality criteria, which are permanently by-measured through the usage of an optical inspection system of the produced/molded parts. Time series-based trends of the process data are used to establish quality prediction models based on a specific time series transformation technique. It preserves the local structure of the data to ensure it can be effectively combined with predictive fuzzy systems training. To account for system dynamics , the prior trained prediction models can be automatically updated on the fly with new measurements. To further increase flexibility in the case of higher dynamics, as was the case in our application, a dynamic lazy learning (dLL) approach was developed in combination with partial least squares . This technique is able to permanently update the reference data base and establish a local model per new query sample with reduced input dimensionality. Our approach was successfully evaluated during the real injection molding processes of bottle caps: several real-occurring anomalies and significant changes in material and production parameters were successfully found over a horizon of 7162 production cycles. Furthermore, three essential quality criteria were predicted with a correlation of 0.7, 0.89 and 0.98 between predicted and observed trend lines, with mean absolute error rates below 5%. This makes the detection of quality deteriorations possible at an early stage.},
  archive      = {J_ASOC},
  author       = {Edwin Lughofer and Kurt Pichler},
  doi          = {10.1016/j.asoc.2023.111029},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111029},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Data-driven prediction of possible quality deterioration in injection molding processes},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An adaptive data compression technique based on optimal
thresholding using multi-objective PSO algorithm for power system data.
<em>ASOC</em>, <em>150</em>, 111028. (<a
href="https://doi.org/10.1016/j.asoc.2023.111028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread development of monitoring devices in power system has indeed led to the generation of large amounts of power consumption data. Storing and transmitting this enormous volume of data has become a significant challenge for power system operators and researchers. Although high data rates are available for transmission, data compression is still necessary for power system applications to lessen the load of data transmission and storage. The wavelet transform plays an important role in data compression for power system applications. This paper presents an adaptive data compression algorithm based on the discrete wavelet transform (DWT) for power system applications. In general, some wavelet coefficients are neglected to produce DWT-based data compression by using hard or soft thresholds. But the volume of compressed data differs for various threshold values. In this context, selecting an optimal threshold is a challenging task in power system data compression. This paper proposes a multi-objective particle swarm optimisation (MO-PSO) for optimal threshold selection. The proposed approach is tested using IEEE power quality wave data sets and Dutch Residential Energy Datasets (DRED). The proposed MO-PSO algorithm finds the global optimum threshold, and it outperforms the other existing algorithms. The proposed algorithm efficiently compresses the test dataset with a maximum compression ratio (CR) of 2.07.},
  archive      = {J_ASOC},
  author       = {S. Karthika and P. Rathika},
  doi          = {10.1016/j.asoc.2023.111028},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111028},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An adaptive data compression technique based on optimal thresholding using multi-objective PSO algorithm for power system data},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Self-supervised global graph neural networks with
enhance-attention for session-based recommendation. <em>ASOC</em>,
<em>150</em>, 111026. (<a
href="https://doi.org/10.1016/j.asoc.2023.111026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Session-based recommendation is a challenging task which predicts the next click based on the short-term behavior of anonymous users. Compared to other recommendation models, session-based recommendations are more difficult due to the limited amount of available data, which is also data sparsity . To solve the problem, we induce self-supervised learning, which can be incorporated into network training by constructing real samples from raw data. It generates self-supervised signals and maximizes the mutual information of session expressions learned. In addition, we propose an enhanced attention module called Enhance-attention. It combines knowledge from global-level graphs and session-level graphs and enhances the intent representation of sessions using Transformer. In this paper, we propose a new approach, called EAT-SGNN, that is able to predict the next click in a more granular way using all items in the session. The model is augmented by self-supervised learning that generates supervised signals. The model is evaluated on three public datasets: Tmall, Nowplaying, and Diginetica. According to the experimental results, the proposed method achieves state-of-the-art performance. All the code and datasets are publicly available on https://github.com/ch30git798/EAT-SGNN.git .},
  archive      = {J_ASOC},
  author       = {Qi Wang and Hao Cui and Jiapeng Zhang and Yan Du and Xiaojun Lu},
  doi          = {10.1016/j.asoc.2023.111026},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111026},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Self-supervised global graph neural networks with enhance-attention for session-based recommendation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). A feature selection method based on multiple feature
subsets extraction and result fusion for improving classification
performance. <em>ASOC</em>, <em>150</em>, 111018. (<a
href="https://doi.org/10.1016/j.asoc.2023.111018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directly applying high-dimensional data to machine learning leads to dimensionality disasters and may induce model overfitting. Feature selection can effectively reduce feature size. However, a single feature selection algorithm has instability and poor generalization ability problems. The ensemble feature selection algorithm is complex to find a suitable feature subset aggregation strategy. To solve these two problems, we propose a feature selection method based on multiple feature subsets extraction and result fusion (FSM). Generate multiple feature subsets to improve stability. This method uses mutual information to mine the relationship between features and categories. Fast non-dominated sorting combines this correlation to distribute similar features in the same layer. A layer optimization algorithm is proposed to combine the layered features to generate multiple different feature subsets. To evaluate the excellence of feature subsets, FSM uses precision, recall, and F-Score comprehensively to assess and remove ineffective feature subsets. The idea of fusion is put on the output of the results. Multiple superior feature subsets train various classifiers . The results of numerous classifiers are fused as the final output according to the voting method, simplifying the ensemble feature selection method’s aggregation process. Experiments on 20 well-known datasets show that FSM can effectively reduce the data dimension and improve the classification performance compared with the original datasets. FSM performs well in classification performance and efficiency compared with other dimensionality reduction algorithms .},
  archive      = {J_ASOC},
  author       = {Jia Liu and Dong Li and Wangweiyi Shan and Shulin Liu},
  doi          = {10.1016/j.asoc.2023.111018},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111018},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A feature selection method based on multiple feature subsets extraction and result fusion for improving classification performance},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An in-depth metaheuristic approach to design a sustainable
closed-loop agri-food supply chain network. <em>ASOC</em>, <em>150</em>,
111017. (<a href="https://doi.org/10.1016/j.asoc.2023.111017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an essential component of human life, agricultural products play a very important role in guaranteeing that individuals get all the essential nutrition. Governments and industries spend great financial resources, define short- and long-term goals, and organize their policies to develop a steady agri-food supply chain and provide fresh, healthy products to their societies. This work proposes a new mixed-integer linear programming model to propose an agri-food supply chain network design for the coconut industry under sustainable terms. This study mainly aims to solve a multi-objective closed-loop supply chain, considering both forward and reverse product movements. The model attempts to manage the net present value of total cost for specific planning horizons while monitoring environmental pollution and job opportunities within the network. Given the NP -hard nature of the network, the solution approach embraces a set of recently developed metaheuristics to overcome its complexity effectively. To this end, six multi-objective optimizers and three hybrid algorithms are utilized, among which the multi-objective artificial rabbit optimizer is first developed and applied in this study. Hence, the model&#39;s compatibility with real conditions is investigated using fifteen practical tests. The results of interval plots and the Friedman statistical test emphasize that optimizers can solve all sizes of problems. However, the Non-Dominated Sorting Genetic Algorithm-II (NSGA-II) outperforms solving practical tests according to both the results of statistical tests and the novel hybrid Multi-Criteria Decision Making (MCDM) framework.},
  archive      = {J_ASOC},
  author       = {Fatemeh Gholian-Jouybari and Mostafa Hajiaghaei-Keshteli and Neale R. Smith and Ericka Zulema Rodríguez Calvo and Christopher Mejía-Argueta and Behzad Mosallanezhad},
  doi          = {10.1016/j.asoc.2023.111017},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111017},
  shortjournal = {Appl. Soft. Comput.},
  title        = {An in-depth metaheuristic approach to design a sustainable closed-loop agri-food supply chain network},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ordered weighted utility distance operators and their
applications in group decision-making. <em>ASOC</em>, <em>150</em>,
111016. (<a href="https://doi.org/10.1016/j.asoc.2023.111016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple attribute group decision-making (MAGDM), a consensus exists that the psychological factors of decision makers (DMs) generally influence their choice of optimal alternatives. To quantify well the influence of DMs’ risk attitudes on the group decision-making process, this paper develops a new approach to MAGDM by introducing DMs’ utility functions into the aggregation process. First, the ordered weighted utility distance (OWUD) operator is constructed by introducing DMs’ utility functions into the distance operator. As a novel operator, the OWUD operator not only satisfies both the basic features of distance measures and the desirable properties of aggregation operators, but also has some unique characteristics. Then, the linear risk tolerance (LRT) utility function is further used in the OWUD operator as the basic utility function, and a specific form of the OWUD operator, which is named as the ordered weighted-linear risk tolerance utility distance (OW-LRTUD) operator, is proposed. The OW-LRTUD operator can quantify well the influence of DMs’ risk attitudes on the group decision-making process. It also has an analytically tractable form and includes many well-known distance measures and aggregation operators, and thus can be applied to many different kinds of decision-making problems. Subsequently, to determine the weights of the OW-LRTUD operator, an extension of the maximum Bayesian entropy method is proposed by incorporating the prior known weight information and different risk attitudes of all individual DMs. An approach to MAGDM based on the OW-LRTUD operator and the proposed weight determination method is developed. An application for determining the optimal alternative in the investment selection is provided to show the feasibility, effectiveness and robustness of our approach in practice, and the corresponding sensitive analysis illustrates the influence of DMs’ risk attitudes on the group decision-making process.},
  archive      = {J_ASOC},
  author       = {Jiehua Xie and Biyao Wu and Wei Zou},
  doi          = {10.1016/j.asoc.2023.111016},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111016},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Ordered weighted utility distance operators and their applications in group decision-making},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted ensemble clustering with multivariate randomness
and random walk strategy. <em>ASOC</em>, <em>150</em>, 111015. (<a
href="https://doi.org/10.1016/j.asoc.2023.111015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble clustering algorithms have made significant progress in recent years due to their excellent performance. However, most of these algorithms face two challenges: one is to focus on the selection of subspaces since there is limited discussion on how to construct a potential metric space, the other is to treat basic clustering equally without fully considering the local connection between clusters when constructing the cooperative association matrix . To solve these issues, we propose a weighted ensemble clustering algorithm with multiple randomness and random walk strategy. We define the free exponential similarity kernel to create a diverse set of random metric spaces coupled with random subspaces and use spectral clustering to generate base clustering. Moreover, we use random walk strategy to discover the local connection between clusters and weight the collaborative association matrix. Finally, the collaborative association matrix uses consensus functions based on hierarchical clustering and meta clustering to obtain clustering results . On this basis, two specific ensemble clustering algorithms WECMR-HC and WECMR-MC are proposed. Theoretical analysis and experimental results demonstrate that our proposed algorithm outperforms existing ensemble algorithms.},
  archive      = {J_ASOC},
  author       = {Shibing Zhou and Rao Duan and Zhewei Chen and Wei Song},
  doi          = {10.1016/j.asoc.2023.111015},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111015},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Weighted ensemble clustering with multivariate randomness and random walk strategy},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive state-of-the-art survey on the recent
modified and hybrid analytic hierarchy process approaches.
<em>ASOC</em>, <em>150</em>, 111014. (<a
href="https://doi.org/10.1016/j.asoc.2023.111014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analytic Hierarchy Process (AHP) is a widely applied technique in multi-criteria decision-making (MCDM) problems. Over time, numerous hybridizations, improvements, and modifications have been proposed to address the shortcomings of traditional AHP. Considering the sheer number of the AHP-based methods, scholars/practitioners are faced with certain challenges when selecting a suitable method due to: (i) lack of adequate knowledge on pros and cons of different AHP approaches, (ii) difficulties and limitations in the application and analysis, and (iii) uncertainties about the suitability of the method. As a result, there is a need for a comprehensive review functioning as a guidance when choosing the best-suited approach considering the specific features of the problem at hand. This paper, therefore, reviews articles published between 2010 and 2023 that have proposed a hybrid, improved, or modified AHP and classifies them based on three main categories of contributions: (A) consistency improvements, (B) reducing the difficulties or limitations, and (C) increasing the accuracy of the results. These categories are further discussed based on the nature of variation (hybridizing with fuzzy sets , metaheuristic algorithms , modification of AHP structure, and hybridization with other approaches). A comprehensive summary table is provided to showcase the strengths and weaknesses of each method, and a roadmap is put forward for scholars and industry experts assisting them in the selection of the appropriate method considering various aspects of problems. Finally, directions for future research are discussed.},
  archive      = {J_ASOC},
  author       = {Mojtaba Ashour and Amir Mahdiyar},
  doi          = {10.1016/j.asoc.2023.111014},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111014},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A comprehensive state-of-the-art survey on the recent modified and hybrid analytic hierarchy process approaches},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fuzzy multi-objective optimization model for sustainable
healthcare supply chain network design. <em>ASOC</em>, <em>150</em>,
111012. (<a href="https://doi.org/10.1016/j.asoc.2023.111012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare supply chains play a crucial role, which enables the implementation of optimization strategies that have rapidly emerged as highly effective means for improving the overall structure of pharmaceutical and healthcare supply chains.. In the healthcare industry , parameters such as increasing the quality of service , as well as optimizing costs, environmental, and social factors play a unique role in supply chain management. To improve the healthcare supply chain network, this study proposed a novel optimization model to optimize multiple objectives, including minimizing the total costs and environmental impacts, while maximizing the social factors by creating jobs simultaneously. To address the effects of uncertain parameters, a fuzzy optimization method alongside the multi-objective gray wolf optimizer (MOGWO), non-dominated sorting genetic algorithm II (NSGA-II), multi-objective differential evolution algorithm (MODEA), and ε-constraint are applied to optimize the model. Also, a case study of the pharmaceutical industry demonstrates the model&#39;s efficacy in a real-life context. The numerical results show the MOGWO manages to create high-quality Pareto solutions with a good spread at the Pareto boundary within a short time compared to the ε-constraint approach. Further, it has shown a more robust performance compared to MODEA and NSGA-II, indicating the efficiency of MOGWO, among other solution methods and other objective indicators.},
  archive      = {J_ASOC},
  author       = {Ali Ala and Alireza Goli and Seyedali Mirjalili and Vladimir Simic},
  doi          = {10.1016/j.asoc.2023.111012},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111012},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A fuzzy multi-objective optimization model for sustainable healthcare supply chain network design},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Style linear k-nearest neighbor classification method.
<em>ASOC</em>, <em>150</em>, 111011. (<a
href="https://doi.org/10.1016/j.asoc.2023.111011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most studies on linear k -nearest neighbor methods assume independent and identically distributed training and testing samples. However, this is not the case in many practical applications, particularly when each class shares a homogeneous style. We propose a novel style linear k -nearest neighbor method to achieve two goals: (a) the linear k -nearest neighbor method should mine the stylistic data for explicit or implicit stylistic features while obtaining linear expressions and effectively transfer them to the predictor through matrix expressions; (b) the similarity of stylistic features between testing and training samples should be numerically quantified to enhance the generalizability of the predictor. To this end, firstly we introduce style matrices to express the style information of each class. In order to prevent the style matrices from degenerating into identity matrices , we introduce enhanced nodes to separate the manifold structure of the original data. The dual representation of matrix and enhanced nodes makes it easy to extract stylistic features from data. Furthermore, we introduce the style membership vector for the first time in the linear k -nearest neighbor method to calculate the style similarity of testing samples to each class, and determine the labels of testing samples easily and accurately through the style membership vector. We also propose an implementable alternating optimization strategy for the proposed method, which decomposes the complex optimization problem into independent subproblems for easy implementation. The experimental results demonstrate that the proposed method keeps comparable generalization capability on the total 9 ordinary datasets. And it outperforms the comparative methods , and even achieving 9.76 % improvement of average testing accuracy over the state-of-the-art weighted locally linear k -nearest neighbor method on the total 6 stylistic datasets.},
  archive      = {J_ASOC},
  author       = {Jin Zhang and Zekang Bian and Shitong Wang},
  doi          = {10.1016/j.asoc.2023.111011},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111011},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Style linear k-nearest neighbor classification method},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid deep learning model for wave height prediction in
australia’s wave energy region. <em>ASOC</em>, <em>150</em>, 111003. (<a
href="https://doi.org/10.1016/j.asoc.2023.111003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Waves are emerging as a renewable energy resource , but the harnessing of such energy remains among the least developed in terms of renewable energy technologies on a regional or a global basis. To generate usable energy, wave heights must be predicted in near-real-time, which is the driving force for wave energy converters. This study develops a hybrid Convolutional Neural Network-Long Short-Term Memory-Bidirectional Gated Recurrent Unit forecast system (CLSTM-BiGRU) trained to accurately predict significant wave height ( H sig ) at multiple forecasting horizons (30 min, 0.5 H ; 2 h, 02 H ; 3 h, 03 H and 6 h, 06 H . In this model, convolutional neural networks (CNNs), long-short-term memories (LSTMs), and bidirectional gated recurrent units (BiGRUs) are employed to predict H sig . To construct the proposed CLSTM-BiGRU model, historical wave properties, including maximum wave height, zero-up crossing wave period, peak energy wave period, sea surface temperature, and significant wave heights are analysed. Several wave energy generation sites in Queensland, Australia were tested using the hybrid deep learning CLSTM-BiGRU model. Based on statistical score metrics, scatterplots , and error evaluations, the hybrid CLSTM-BiGRU model generates more accurate forecasts than the benchmark models . This study established the practical utility of the hybrid CLSTM-BiGRU model for modelling H sig and therefore shows the model could have significant implications for wave and ocean energy generation systems, tidal or wave height monitoring as well as sustainable wave energy resource evaluation where a prediction of wave heights is required.},
  archive      = {J_ASOC},
  author       = {Abul Abrar Masrur Ahmed and S.Janifer Jabin Jui and Mohanad S. AL-Musaylh and Nawin Raj and Reepa Saha and Ravinesh C. Deo and Sanjoy Kumar Saha},
  doi          = {10.1016/j.asoc.2023.111003},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {111003},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Hybrid deep learning model for wave height prediction in australia&#39;s wave energy region},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A real‐time fuzzy motion planning system for unmanned aerial
vehicles in dynamic 3D environments. <em>ASOC</em>, <em>150</em>,
110995. (<a href="https://doi.org/10.1016/j.asoc.2023.110995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new fuzzy potential system to plan the motion of Unmanned Aerial Vehicles (UAVs) in dynamic 3D Space. The system consists of two fuzzy subsystems representing the attractive model and the repulsive model of virtual forces in 3D. The attractive model will generate the attractive force required to pull the UAV in a smooth and optimized trajectory to land softly on a moving or stationary target . The repulsive model will generate the required repulsive force to avoid stationary or moving obstacles in 3D Space. The attractive fuzzy inference system takes the relative position and relative velocity between UAV and the target in the x, y, and z directions as inputs. It generates the required attractive force in the x, y, and z directions. The repulsive fuzzy inference system takes the relative position between UAV and obstacle in the xyz directions as input. Fuzzy associative memory (FAM) models the inputs and generates the required repulsive force in the x, y, and z directions. As a result, the UAV is considered to be moving under the influence of fuzzy virtual attractive and repulsive forces simultaneously. Accordingly, it will be able to change both its altitude and projected planner position concurrently and resolves the local minima problem if occurred. On the other hand, many classical models in dynamic environments require several additional inputs, such as the relative position and relative velocity, which increase the requirement on the measurement system to localize the moving objects in the 3D Space. Several experiments were performed and discussed to verify the robustness and effectiveness of the proposed motion planner with real-time implementations. The system performance was validated using three robotics platforms , two quadcopter drones, and one ground robot. The position and orientation of each robot were defined using a motion capture system with 6 opti-track cameras. The motion planning system produces a quadcopter drone&#39;s efficient and accurate low-frequency trajectory. The generated trajectory allows the drone to track the ground robot and avoid collision effectively with the second drone in the vicinity.},
  archive      = {J_ASOC},
  author       = {Mohammad Hamdan Garibeh and Mohammad A. Jaradat and Ahmad M. Alshorman and M. Hayajneh and A. Bani Younes},
  doi          = {10.1016/j.asoc.2023.110995},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {110995},
  shortjournal = {Appl. Soft. Comput.},
  title        = {A real‐time fuzzy motion planning system for unmanned aerial vehicles in dynamic 3D environments},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive SV-borderline SMOTE-SVM algorithm for imbalanced
data classification. <em>ASOC</em>, <em>150</em>, 110986. (<a
href="https://doi.org/10.1016/j.asoc.2023.110986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, imbalanced data classification has emerged as a challenging task. To address this issue, we propose an adaptive SV-Borderline SMOTE-SVM (Synthetic Minority Oversampling Technique-Support Vector Machine) algorithm, specifically designed to overcome the challenges associated with imbalanced data classification . The algorithm begins by mapping the dataset into the kernel space using SVM to identify the class boundary samples, known as support vectors (SVs). Subsequently, the neighbors of positive sample’s support vector (SV+) are calculated based on the kernel distance. Based on the class distribution of these neighbors, the SV+ samples are labeled as either “concave” or “convex”. Based on these labels, new samples are adaptively generated using two distinct calculation approaches for different labeled SV+ samples. To construct the SVM decision function without requiring the explicit expression of new samples in the kernel space, a Gram matrix is designed. Notably, all the processes ensure the credibility and reliability of the new samples. Additionally, the adaptive interpolation approach helps to ensure the security and diversity of new samples. Extensive experiments were conducted on a set of 50 KEEL datasets to evaluate the performance of our proposed method for imbalanced data classification. In experiments, our method achieved the highest G -mean score in 33 datasets and the highest F -values in 32 datasets. These results highlight the effectiveness and superiority of our proposed method compared to other approaches in addressing the challenges of imbalanced data classification.},
  archive      = {J_ASOC},
  author       = {Jiaqi Guo and Haiyan Wu and Xiaolei Chen and Weiguo Lin},
  doi          = {10.1016/j.asoc.2023.110986},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {110986},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Adaptive SV-borderline SMOTE-SVM algorithm for imbalanced data classification},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Economic dispatch using metaheuristics: Algorithms,
problems, and solutions. <em>ASOC</em>, <em>150</em>, 110891. (<a
href="https://doi.org/10.1016/j.asoc.2023.110891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Economic dispatch (ED) has received considerable interest in the field of energy management and optimization. The problem aims to determine the most cost-effective power allocation strategy that satisfies the power demand and all physical constraints of the power system . To solve this problem, we propose an algorithm based on differential evolution and adopt a hybrid mutation strategy, a linear population size reduction mechanism, and an improved single-unit repair mechanism. Experimental results confirmed that these mechanisms are useful for performance improvement . The proposed algorithm ( L -HMDE) showed good performance when compared with more than 90 algorithms in solving 22 test cases. It could provide high-quality solutions stably and efficiently. In addition to designing a good algorithm, we present a review of over 100 papers and highlight their algorithm features. We also provide a comprehensive collection of test cases in the literature. Through careful examination and verification, data coefficients of these test cases and solutions to them are included in this paper as a useful reference for researchers who are interested in this problem.},
  archive      = {J_ASOC},
  author       = {Thammarsat Visutarrom and Tsung-Che Chiang},
  doi          = {10.1016/j.asoc.2023.110891},
  journal      = {Applied Soft Computing},
  month        = {1},
  pages        = {110891},
  shortjournal = {Appl. Soft. Comput.},
  title        = {Economic dispatch using metaheuristics: Algorithms, problems, and solutions},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
