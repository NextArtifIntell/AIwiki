<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda---111">CSDA - 111</h2>
<ul>
<li><details>
<summary>
(2024). Hierarchical bayesian spectral regression with shape
constraints for multi-group data. <em>CSDA</em>, <em>200</em>, 108036.
(<a href="https://doi.org/10.1016/j.csda.2024.108036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hierarchical Bayesian (HB) model for multi-group analysis with group–specific, flexible regression functions. The lower–level (within group) and upper–level (between groups) regression functions have hierarchical Gaussian process priors. HB smoothing priors are developed for the spectral coefficients. The HB priors smooth the estimated functions within and between groups. The HB model is particularly useful when data within groups are sparse because it shares information across groups, and provides more accurate estimates than fitting separate nonparametric models to each group. The proposed model also allows shape constraints, such as monotone, U and S–shaped, and multi-modal constraints. When appropriate, shape constraints improve estimation by recognizing violations of the shape constraints as noise. The model is illustrated by two examples: monotone growth curves for children, and happiness as a convex, U-shaped function of age in multiple countries. Various basis functions could also be used, and the paper also implements versions with B-splines and orthogonal polynomials.},
  archive      = {J_CSDA},
  author       = {Peter Lenk and Jangwon Lee and Dongu Han and Jichan Park and Taeryon Choi},
  doi          = {10.1016/j.csda.2024.108036},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {12},
  pages        = {108036},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hierarchical bayesian spectral regression with shape constraints for multi-group data},
  volume       = {200},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pinball boosting of regression quantiles. <em>CSDA</em>,
<em>200</em>, 108027. (<a
href="https://doi.org/10.1016/j.csda.2024.108027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm for boosting regression quantiles using asymmetric least absolute deviations, better known as pinball loss, is proposed. Existing approaches for boosting regression quantiles are essentially equal to least squares boosting of regression means with the single difference that their working residuals are based on pinball loss. All steps of our boosting algorithm are embedded in the well-established framework of quantile regression, and its main components – sequential base learning, fitting, and updating – are based on consistent scoring rules for regression quantiles. The Monte Carlo simulations performed indicate that the pinball boosting algorithm is competitive with existing approaches for boosting regression quantiles in terms of estimation accuracy and variable selection, and that its application to the study of regression quantiles of hedonic price functions allows the estimation of previously infeasible high-dimensional specifications.},
  archive      = {J_CSDA},
  author       = {Ida Bauer and Harry Haupt and Stefan Linner},
  doi          = {10.1016/j.csda.2024.108027},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {12},
  pages        = {108027},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Pinball boosting of regression quantiles},
  volume       = {200},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tests for high-dimensional generalized linear models under
general covariance structure. <em>CSDA</em>, <em>199</em>, 108026. (<a
href="https://doi.org/10.1016/j.csda.2024.108026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the testing of regression coefficients within high-dimensional generalized linear models featuring general covariance structures. The derived asymptotic properties reveal that distinct covariance structures can lead to varying limiting null distributions, including the normal distribution, for a widely employed quadratic-norm based test statistic. This circumstance renders it infeasible to determine critical values through a limiting null distribution. In response to this challenge, we propose a multiplier bootstrap test procedure for practical implementation. Additionally, we introduce a modified version of this procedure, incorporating projection when dealing with nuisance parameters. We then proceed to examine the asymptotic level and power of the proposed tests and assess their finite-sample performance through simulations. Finally, we present a real data analysis to illustrate the practical application of the proposed tests.},
  archive      = {J_CSDA},
  author       = {Weichao Yang and Xu Guo and Lixing Zhu},
  doi          = {10.1016/j.csda.2024.108026},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108026},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tests for high-dimensional generalized linear models under general covariance structure},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling non-stationarity in asymptotically independent
extremes. <em>CSDA</em>, <em>199</em>, 108025. (<a
href="https://doi.org/10.1016/j.csda.2024.108025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications, evaluating the joint impact of combinations of environmental variables is important for risk management and structural design analysis. When such variables are considered simultaneously, non-stationarity can exist within both the marginal distributions and dependence structure, resulting in complex data structures. In the context of extremes, few methods have been proposed for modelling trends in extremal dependence, even though capturing this feature is important for quantifying joint impact. Moreover, most proposed techniques are only applicable to data structures exhibiting asymptotic dependence. Motivated by observed dependence trends of data from the UK Climate Projections, a novel semi-parametric modelling framework for bivariate extremal dependence structures is proposed. This framework can capture a wide variety of dependence trends for data exhibiting asymptotic independence. When applied to the climate projection dataset, the model detects significant dependence trends in observations and, in combination with models for marginal non-stationarity, can be used to produce estimates of bivariate risk measures at future time points.},
  archive      = {J_CSDA},
  author       = {C.J.R. Murphy-Barltrop and J.L. Wadsworth},
  doi          = {10.1016/j.csda.2024.108025},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108025},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Modelling non-stationarity in asymptotically independent extremes},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-way data clustering based on the mean-mixture of
matrix-variate normal distributions. <em>CSDA</em>, <em>199</em>,
108016. (<a href="https://doi.org/10.1016/j.csda.2024.108016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the steady growth of computer technologies, the application of statistical techniques to analyze extensive datasets has garnered substantial attention. The analysis of three-way (matrix-variate) data has emerged as a burgeoning field that has inspired statisticians in recent years to develop novel analytical methods. This paper introduces a unified finite mixture model that relies on the mean-mixture of matrix-variate normal distributions. The strength of our proposed model lies in its capability to capture and cluster a wide range of three-way data that exhibit heterogeneous, asymmetric and leptokurtic features. A computationally feasible ECME algorithm is developed to compute the maximum likelihood (ML) estimates. Numerous simulation studies are conducted to investigate the asymptotic properties of the ML estimators, validate the effectiveness of the Bayesian information criterion in selecting the appropriate model, and assess the classification ability in presence of contaminated noise. The utility of the proposed methodology is demonstrated by analyzing a real-life data example.},
  archive      = {J_CSDA},
  author       = {Mehrdad Naderi and Mostafa Tamandi and Elham Mirfarah and Wan-Lun Wang and Tsung-I Lin},
  doi          = {10.1016/j.csda.2024.108016},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108016},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Three-way data clustering based on the mean-mixture of matrix-variate normal distributions},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimizing designs in clinical trials with an application in
treatment of epidermolysis bullosa simplex, a rare genetic skin disease.
<em>CSDA</em>, <em>199</em>, 108015. (<a
href="https://doi.org/10.1016/j.csda.2024.108015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidermolysis bullosa simplex (EBS) skin disease is a rare disease, which renders the use of optimal design techniques especially important to maximize the potential information in a future study, that is, to make efficient use of the limited number of available subjects and observations. A generalized linear mixed effects model (GLMM), built on an EBS trial was used to optimize the design. The model assumed a full treatment effect in the follow-up period. In addition to this model, two models with either no assumed treatment effect or a linearly declining treatment effect in the follow-up were assumed. The information gain and loss when changing the number of EBS blisters counts, altering the duration of the treatment as well as changing the study period was assessed. In addition, optimization of the EBS blister assessment times was performed. The optimization was utilizing the derived Fisher information matrix for the GLMM with EBS blister counts and the information gain and loss was quantified by D-optimal efficiency. The optimization results indicated that using optimal assessment times increases the information of about 110-120%, varying slightly between the assumed treatment models. In addition, the result showed that the assessment times were also sensitive to be moved ± one week, but assessment times within ± two days were not decreasing the information as long as three assessments (out of four assessments in the trial period) were within the treatment period and not in the follow-up period. Increasing the number of assessments to six or five per trial period increased the information to 130% and 115%, respectively, while decreasing the number of assessments to two or three, decreased the information to 50% and 80%, respectively. Increasing the length of the trial period had a minor impact on the information, while increasing the treatment period by two and four weeks had a larger impact, 120% and 130%, respectively. To conclude, general applications of optimal design methodology, derivation of the Fisher information matrix for GLMM with count data and examples on how optimal design could be used when designing trials for treatment of the EBS disease is presented. The methodology is also of interest for study designs where maximizing the information is essential. Therefore, a general applied research guidance for using optimal design is also provided.},
  archive      = {J_CSDA},
  author       = {Joakim Nyberg and Andrew C. Hooker and Georg Zimmermann and Johan Verbeeck and Martin Geroldinger and Konstantin Emil Thiel and Geert Molenberghs and Martin Laimer and Verena Wally},
  doi          = {10.1016/j.csda.2024.108015},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108015},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Optimizing designs in clinical trials with an application in treatment of epidermolysis bullosa simplex, a rare genetic skin disease},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap-based statistical inference for linear mixed
effects under misspecifications. <em>CSDA</em>, <em>199</em>, 108014.
(<a href="https://doi.org/10.1016/j.csda.2024.108014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed effects are considered excellent predictors of cluster-level parameters in various domains. However, previous research has demonstrated that their performance is affected by departures from model assumptions. Given the common occurrence of these departures in empirical studies, there is a need for inferential methods that are robust to misspecifications while remaining accessible and appealing to practitioners. Statistical tools have been developed for cluster-wise and simultaneous inference for mixed effects under distributional misspecifications, employing a user-friendly semiparametric random effect bootstrap. The merits and limitations of this approach are discussed in the general context of model misspecification. Theoretical analysis demonstrates the asymptotic consistency of the methods under general regularity conditions. Simulations show that the proposed intervals are robust to departures from modelling assumptions, including asymmetry and long tails in the distributions of errors and random effects, outperforming competitors in terms of empirical coverage probability. Finally, the methodology is applied to construct confidence intervals for household income across counties in the Spanish region of Galicia.},
  archive      = {J_CSDA},
  author       = {Katarzyna Reluga and María-José Lombardía and Stefan Sperlich},
  doi          = {10.1016/j.csda.2024.108014},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108014},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bootstrap-based statistical inference for linear mixed effects under misspecifications},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate ordinal regression for multiple repeated
measurements. <em>CSDA</em>, <em>199</em>, 108013. (<a
href="https://doi.org/10.1016/j.csda.2024.108013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multivariate ordinal regression model which allows the joint modeling of three-dimensional panel data containing both repeated and multiple measurements for a collection of subjects is proposed. This is achieved by a multivariate autoregressive structure on the errors of the latent variables underlying the ordinal responses, which accounts for the correlations at a single point in time and the persistence over time. The error distribution is assumed to be normal or Student- t distributed. The estimation is performed using composite likelihood methods. Through several simulation exercises, the quality of the estimates in different settings as well as in comparison with a Bayesian approach is investigated. The simulation study confirms that the estimation procedure is able to recover the model parameters well and is competitive in terms of computation time. Finally, the framework is illustrated using a data set containing bankruptcy and credit rating information for US exchange-listed companies.},
  archive      = {J_CSDA},
  author       = {Laura Vana-Gür},
  doi          = {10.1016/j.csda.2024.108013},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108013},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multivariate ordinal regression for multiple repeated measurements},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian modal regression based on mixture distributions.
<em>CSDA</em>, <em>199</em>, 108012. (<a
href="https://doi.org/10.1016/j.csda.2024.108012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to mean regression and quantile regression, the literature on modal regression is very sparse. A unifying framework for Bayesian modal regression is proposed, based on a family of unimodal distributions indexed by the mode, along with other parameters that allow for flexible shapes and tail behaviors. Sufficient conditions for posterior propriety under an improper prior on the mode parameter are derived. Following prior elicitation, regression analysis of simulated data and datasets from several real-life applications are conducted. Besides drawing inference for covariate effects that are easy to interpret, prediction and model selection under the proposed Bayesian modal regression framework are also considered. Evidence from these analyses suggest that the proposed inference procedures are very robust to outliers, enabling one to discover interesting covariate effects missed by mean or median regression, and to construct much tighter prediction intervals than those from mean or median regression. Computer programs for implementing the proposed Bayesian modal regression are available at https://github.com/rh8liuqy/Bayesian_modal_regression .},
  archive      = {J_CSDA},
  author       = {Qingyang Liu and Xianzheng Huang and Ray Bai},
  doi          = {10.1016/j.csda.2024.108012},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108012},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian modal regression based on mixture distributions},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An embedded diachronic sense change model with a case study
from ancient greek. <em>CSDA</em>, <em>199</em>, 108011. (<a
href="https://doi.org/10.1016/j.csda.2024.108011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as “kosmos” (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.},
  archive      = {J_CSDA},
  author       = {Schyan Zafar and Geoff K. Nicholls},
  doi          = {10.1016/j.csda.2024.108011},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108011},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An embedded diachronic sense change model with a case study from ancient greek},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametrically corrected likelihood for bayesian
spectral analysis of multivariate time series. <em>CSDA</em>,
<em>199</em>, 108010. (<a
href="https://doi.org/10.1016/j.csda.2024.108010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to Bayesian nonparametric spectral analysis of stationary multivariate time series is presented. Starting with a parametric vector-autoregressive model, the parametric likelihood is nonparametrically adjusted in the frequency domain to account for potential deviations from parametric assumptions. A proof of mutual contiguity of the nonparametrically corrected likelihood, the multivariate Whittle likelihood approximation and the exact likelihood for Gaussian time series is given. A multivariate extension of the nonparametric Bernstein-Dirichlet process prior for univariate spectral densities to the space of Hermitian positive definite spectral density matrices is specified directly on the correction matrices. An infinite series representation of this prior is then used to develop a Markov chain Monte Carlo algorithm to sample from the posterior distribution. The code is made publicly available for ease of use and reproducibility. With this novel approach, a generalisation of the multivariate Whittle-likelihood-based method of Meier et al. (2020) as well as an extension of the nonparametrically corrected likelihood for univariate stationary time series of Kirch et al. (2019) to the multivariate case is presented. It is demonstrated that the nonparametrically corrected likelihood combines the efficiencies of a parametric with the robustness of a nonparametric model. Its numerical accuracy is illustrated in a comprehensive simulation study. Its practical advantages are illustrated by a spectral analysis of two environmental time series data sets: a bivariate time series of the Southern Oscillation Index and fish recruitment and a multivariate time series of windspeed data at six locations in California.},
  archive      = {J_CSDA},
  author       = {Yixuan Liu and Claudia Kirch and Jeong Eun Lee and Renate Meyer},
  doi          = {10.1016/j.csda.2024.108010},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108010},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A nonparametrically corrected likelihood for bayesian spectral analysis of multivariate time series},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A double pólya-gamma data augmentation scheme for a
hierarchical negative binomial - binomial data model. <em>CSDA</em>,
<em>199</em>, 108009. (<a
href="https://doi.org/10.1016/j.csda.2024.108009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A double Pólya-Gamma data augmentation scheme is developed for posterior sampling from a Bayesian hierarchical model of total and categorical count data. The scheme applies to a Negative Binomial - Binomial (NBB) hierarchical regression model with logit links and normal priors on regression coefficients. The approach is shown to be very efficient and in most cases out-performs the Stan program. The hierarchical modeling framework and the Pólya-Gamma data augmentation scheme are applied to human mitochondrial DNA data.},
  archive      = {J_CSDA},
  author       = {Xuan Ma and Jenný Brynjarsdóttir and Thomas LaFramboise},
  doi          = {10.1016/j.csda.2024.108009},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {108009},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A double pólya-gamma data augmentation scheme for a hierarchical negative binomial - binomial data model},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional mean dimension reduction for tensor time series.
<em>CSDA</em>, <em>199</em>, 107998. (<a
href="https://doi.org/10.1016/j.csda.2024.107998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dimension reduction problem for a stationary tensor time series is addressed. The goal is to remove linear combinations of the tensor time series that are mean independent of the past, without imposing any parametric models or distributional assumptions. To achieve this goal, a new metric called cumulative tensor martingale difference divergence is introduced and its theoretical properties are studied. Unlike existing methods, the proposed approach achieves dimension reduction by estimating a distinctive subspace that can fully retain the conditional mean information. By focusing on the conditional mean, the proposed dimension reduction method is potentially more accurate in prediction. The method can be viewed as a factor model-based approach that extends the existing techniques for estimating central subspace or central mean subspace in vector time series. The effectiveness of the proposed method is illustrated by extensive simulations and two real-world data applications.},
  archive      = {J_CSDA},
  author       = {Chung Eun Lee and Xin Zhang},
  doi          = {10.1016/j.csda.2024.107998},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {11},
  pages        = {107998},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional mean dimension reduction for tensor time series},
  volume       = {199},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonnegative GARCH-type models with conditional gamma
distributions and their applications. <em>CSDA</em>, <em>198</em>,
108006. (<a href="https://doi.org/10.1016/j.csda.2024.108006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of real data are characterized by positive, asymmetric and skewed distributions of various shapes. Modelling and forecasting of such data are addressed by proposing nonnegative conditional heteroscedastic time series models with Gamma distributions. Three types of time-varying parameters of Gamma distributions are adopted to construct the nonnegative GARCH models. A condition for the existence of a stationary Gamma-GARCH model is given. Parameter estimates are discussed via maximum likelihood estimation (MLE) method. A Monte-Carlo study is conducted to illustrate sample paths of the proposed models and to see finite-sample validity of the MLEs, as well as to evaluate model diagnostics using standardized Pearson residuals. Furthermore, out-of-sample forecasting analysis is performed to compute forecasting accuracy measures. Applications to oil price and Bitcoin data are given, respectively.},
  archive      = {J_CSDA},
  author       = {Eunju Hwang and ChanHyeok Jeon},
  doi          = {10.1016/j.csda.2024.108006},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {108006},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonnegative GARCH-type models with conditional gamma distributions and their applications},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Strong orthogonal latin hypercubes for computer experiments.
<em>CSDA</em>, <em>198</em>, 107999. (<a
href="https://doi.org/10.1016/j.csda.2024.107999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Latin hypercubes are widely used for computer experiments. They achieve both orthogonality and the maximum one-dimensional stratification property. When two-factor (and higher-order) interactions are active, two- and three-dimensional stratifications are also important. Unfortunately, little is known about orthogonal Latin hypercubes with good two (and higher)–dimensional stratification properties. A method is proposed for constructing a new class of orthogonal Latin hypercubes whose columns can be partitioned into groups, such that the columns from different groups maintain two- and three-dimensional stratification properties. The proposed designs perform well under almost all popular criteria (e.g., the orthogonality, stratification, and maximin distance criterion). They are the most ideal designs for computer experiments. The construction method can be straightforward to implement, and the relevant theoretical supports are well established. The proposed strong orthogonal Latin hypercubes are tabulated for practical needs.},
  archive      = {J_CSDA},
  author       = {Chunyan Wang and Dennis K.J. Lin},
  doi          = {10.1016/j.csda.2024.107999},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107999},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Strong orthogonal latin hypercubes for computer experiments},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference for high-dimensional linear expectile regression
with de-biasing method. <em>CSDA</em>, <em>198</em>, 107997. (<a
href="https://doi.org/10.1016/j.csda.2024.107997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The methodology for the inference problem in high-dimensional linear expectile regression is developed. By transforming the expectile loss into a weighted-least-squares form and applying a de-biasing strategy, Wald-type tests for multiple constraints within a regularized framework are established. An estimator for the pseudo-inverse of the generalized Hessian matrix in high dimension is constructed using general amenable regularizers, including Lasso and SCAD, with its consistency demonstrated through a novel proof technique. Simulation studies and real data applications demonstrate the efficacy of the proposed test statistic in both homoscedastic and heteroscedastic scenarios.},
  archive      = {J_CSDA},
  author       = {Xiang Li and Yu-Ning Li and Li-Xin Zhang and Jun Zhao},
  doi          = {10.1016/j.csda.2024.107997},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107997},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inference for high-dimensional linear expectile regression with de-biasing method},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent event history models for quasi-reaction systems.
<em>CSDA</em>, <em>198</em>, 107996. (<a
href="https://doi.org/10.1016/j.csda.2024.107996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various processes, such as cell differentiation and disease spreading, can be modelled as quasi-reaction systems of particles using stochastic differential equations. The existing Local Linear Approximation (LLA) method infers the parameters driving these systems from measurements of particle abundances over time. While dense observations of the process in time should in theory improve parameter estimation, LLA fails in these situations due to numerical instability. Defining a latent event history model of the underlying quasi-reaction system resolves this problem. A computationally efficient Expectation-Maximization algorithm is proposed for parameter estimation, incorporating an extended Kalman filter for evaluating the latent reactions. A simulation study demonstrates the method&#39;s performance and highlights the settings where it is particularly advantageous compared to the existing LLA approaches. An illustration of the method applied to the diffusion of COVID-19 in Italy is presented.},
  archive      = {J_CSDA},
  author       = {Matteo Framba and Veronica Vinciotti and Ernst C. Wit},
  doi          = {10.1016/j.csda.2024.107996},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107996},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Latent event history models for quasi-reaction systems},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medoid splits for efficient random forests in metric spaces.
<em>CSDA</em>, <em>198</em>, 107995. (<a
href="https://doi.org/10.1016/j.csda.2024.107995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptation of the random forest algorithm for Fréchet regression is revisited, addressing the challenge of regression with random objects in metric spaces. To overcome the limitations of previous approaches, a new splitting rule is introduced, substituting the computationally expensive Fréchet means with a medoid-based approach. The asymptotic equivalence of this method to Fréchet mean-based procedures is demonstrated, along with the consistency of the associated regression estimator. This approach provides a sound theoretical framework and a more efficient computational solution to Fréchet regression, broadening its application to non-standard data types and complex use cases.},
  archive      = {J_CSDA},
  author       = {Matthieu Bulté and Helle Sørensen},
  doi          = {10.1016/j.csda.2024.107995},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107995},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Medoid splits for efficient random forests in metric spaces},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Study of imputation procedures for nonparametric density
estimation based on missing censored lifetimes. <em>CSDA</em>,
<em>198</em>, 107994. (<a
href="https://doi.org/10.1016/j.csda.2024.107994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imputation is a standard procedure in dealing with missing data and there are many competing imputation methods. It is proposed to analyze imputation procedures via comparison with a benchmark developed by the asymptotic theory. Considered model is nonparametric density estimation of the missing right censored lifetime of interest. This model is of a special interest for understanding imputation because each underlying observation is the pair of censored lifetime and indicator of censoring. The latter creates a number of interesting scenarios and challenges for imputation when best methods may or may not be applicable. Further, the theory sheds light on why the effect of imputation depends on an underlying density. The methodology is tested on real life datasets and via intensive simulations. Data and R code are provided.},
  archive      = {J_CSDA},
  author       = {Sam Efromovich and Lirit Fuksman},
  doi          = {10.1016/j.csda.2024.107994},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107994},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Study of imputation procedures for nonparametric density estimation based on missing censored lifetimes},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent skinny gibbs in probit regression. <em>CSDA</em>,
<em>198</em>, 107993. (<a
href="https://doi.org/10.1016/j.csda.2024.107993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spike and slab priors have emerged as effective and computationally scalable tools for Bayesian variable selection in high-dimensional linear regression. However, the crucial model selection consistency and efficient computational strategies using spike and slab priors in probit regression have rarely been investigated. A hierarchical probit model with continuous spike and slab priors over regression coefficients is considered, and a highly scalable Gibbs sampler with a computational complexity that grows only linearly in the dimension of predictors is proposed. Specifically, the “Skinny Gibbs” algorithm is adapted to the setting of probit and negative binomial regression and model selection consistency for the proposed method under probit model is established, when the number of covariates is allowed to grow much larger than the sample size. Through simulation studies, the method is shown to achieve superior empirical performance compared with other state-of-the art methods. Gene expression data from 51 asthmatic and 44 non-asthmatic samples are analyzed and the performance for predicting asthma using the proposed approach is compared with existing approaches.},
  archive      = {J_CSDA},
  author       = {Jiarong Ouyang and Xuan Cao},
  doi          = {10.1016/j.csda.2024.107993},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107993},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Consistent skinny gibbs in probit regression},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conditional approach for regression analysis of case k
interval-censored failure time data with informative censoring.
<em>CSDA</em>, <em>198</em>, 107991. (<a
href="https://doi.org/10.1016/j.csda.2024.107991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses regression analysis of case K interval-censored failure time data, a general type of failure time data, in the presence of informative censoring with the focus on simultaneous variable selection and estimation. Although many authors have considered the challenging variable selection problem for interval-censored data, most of the existing methods assume independent or non-informative censoring. More importantly, the existing methods that allow for informative censoring are frailty model-based approaches and cannot directly assess the degree of informative censoring among other shortcomings. To address these, we propose a conditional approach and develop a penalized sieve maximum likelihood procedure for the simultaneous variable selection and estimation of covariate effects. Furthermore, we establish the oracle property of the proposed method and illustrate the appropriateness and usefulness of the approach using a simulation study. Finally we apply the proposed method to a set of real data on Alzheimer&#39;s disease and provide some new insights.},
  archive      = {J_CSDA},
  author       = {Mingyue Du and Xingqiu Zhao},
  doi          = {10.1016/j.csda.2024.107991},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107991},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A conditional approach for regression analysis of case k interval-censored failure time data with informative censoring},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Principal component analysis for zero-inflated compositional
data. <em>CSDA</em>, <em>198</em>, 107989. (<a
href="https://doi.org/10.1016/j.csda.2024.107989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in DNA sequencing technology have led to a growing interest in microbiome data. Since the data are often high-dimensional, there is a clear need for dimensionality reduction. However, the compositional nature and zero-inflation of microbiome data present many challenges in developing new methodologies. New PCA methods for zero-inflated compositional data are presented, based on a novel framework called principal compositional subspace. These methods aim to identify both the principal compositional subspace and the corresponding principal scores that best approximate the given data, ensuring that their reconstruction remains within the compositional simplex. To this end, the constrained optimization problems are established and alternating minimization algorithms are provided to solve the problems. The theoretical properties of the principal compositional subspace, particularly focusing on its existence and consistency, are further investigated. Simulation studies have demonstrated that the methods achieve lower reconstruction errors than the existing log-ratio PCA in the presence of a linear pattern and have shown comparable performance in a curved pattern. The methods have been applied to four microbiome compositional datasets with excessive zeros, successfully recovering the underlying low-rank structure.},
  archive      = {J_CSDA},
  author       = {Kipoong Kim and Jaesung Park and Sungkyu Jung},
  doi          = {10.1016/j.csda.2024.107989},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107989},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Principal component analysis for zero-inflated compositional data},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral co-clustering in multi-layer directed networks.
<em>CSDA</em>, <em>198</em>, 107987. (<a
href="https://doi.org/10.1016/j.csda.2024.107987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern network analysis often involves multi-layer network data in which the nodes are aligned, and the edges on each layer represent one of the multiple relations among the nodes. Current literature on multi-layer network data is mostly limited to undirected relations. However, direct relations are more common and may introduce extra information. This study focuses on community detection (or clustering) in multi-layer directed networks. To take into account the asymmetry, a novel spectral-co-clustering-based algorithm is developed to detect co-clusters , which capture the sending patterns and receiving patterns of nodes, respectively. Specifically, the eigendecomposition of the debiased sum of Gram matrices over the layer-wise adjacency matrices is computed, followed by the k -means, where the sum of Gram matrices is used to avoid possible cancellation of clusters caused by direct summation. Theoretical analysis of the algorithm under the multi-layer stochastic co-block model is provided, where the common assumption that the cluster number is coupled with the rank of the model is relaxed. After a systematic analysis of the eigenvectors of the population version algorithm, the misclassification rates are derived, which show that multi-layers would bring benefits to the clustering performance. The experimental results of simulated data corroborate the theoretical predictions, and the analysis of a real-world trade network dataset provides interpretable results.},
  archive      = {J_CSDA},
  author       = {Wenqing Su and Xiao Guo and Xiangyu Chang and Ying Yang},
  doi          = {10.1016/j.csda.2024.107987},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {10},
  pages        = {107987},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Spectral co-clustering in multi-layer directed networks},
  volume       = {198},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online bootstrap inference for the geometric median.
<em>CSDA</em>, <em>197</em>, 107992. (<a
href="https://doi.org/10.1016/j.csda.2024.107992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world applications, the geometric median is a natural quantity to consider for robust inference of location or central tendency, particularly when dealing with non-standard or irregular data distributions. An innovative online bootstrap inference algorithm, using the averaged nonlinear stochastic gradient algorithm, is proposed to make statistical inference about the geometric median from massive datasets. The method is computationally fast and memory-friendly, and it is easy to update as new data is received sequentially. The validity of the proposed online bootstrap inference is theoretically justified. Simulation studies under a variety of scenarios are conducted to demonstrate its effectiveness and efficiency in terms of computation speed and memory usage. Additionally, the online inference procedure is applied to a large publicly available dataset for skin segmentation.},
  archive      = {J_CSDA},
  author       = {Guanghui Cheng and Qiang Xiong and Ruitao Lin},
  doi          = {10.1016/j.csda.2024.107992},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107992},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online bootstrap inference for the geometric median},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gibbs sampler approach for objective bayesian inference in
elliptical multivariate meta-analysis random effects model.
<em>CSDA</em>, <em>197</em>, 107990. (<a
href="https://doi.org/10.1016/j.csda.2024.107990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference procedures for the parameters of the multivariate random effects model are derived under the assumption of an elliptically contoured distribution when the Berger and Bernardo reference and the Jeffreys priors are assigned to the model parameters. A new numerical algorithm for drawing samples from the posterior distribution is developed, which is based on the hybrid Gibbs sampler. The new approach is compared to the two Metropolis-Hastings algorithms previously derived in the literature via an extensive simulation study. The findings are applied to a Bayesian multivariate meta-analysis, conducted using the results of ten studies on the effectiveness of a treatment for hypertension. The analysis investigates the treatment effects on systolic and diastolic blood pressure. The second empirical illustration deals with measurement data from the CCAUV.V-K1 key comparison, aiming to compare measurement results of sinusoidal linear accelerometers at four frequencies.},
  archive      = {J_CSDA},
  author       = {Olha Bodnar and Taras Bodnar},
  doi          = {10.1016/j.csda.2024.107990},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107990},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Gibbs sampler approach for objective bayesian inference in elliptical multivariate meta-analysis random effects model},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goodness–of–fit tests based on the min–characteristic
function. <em>CSDA</em>, <em>197</em>, 107988. (<a
href="https://doi.org/10.1016/j.csda.2024.107988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tests of fit for classes of distributions that include the Weibull, the Pareto and the Fréchet families are proposed. The new tests employ the novel tool of the min–characteristic function and are based on an L 2 L2 –type weighted distance between this function and its empirical counterpart applied on suitably standardized data. If data–standardization is performed using the MLE of the distributional parameters then the method reduces to testing for the standard member of the family, with parameter values known and set equal to one. Asymptotic properties of the tests are investigated. A Monte Carlo study is presented that includes the new procedure as well as competitors for the purpose of specification testing with three extreme value distributions. The new tests are also applied on a few real–data sets.},
  archive      = {J_CSDA},
  author       = {S.G. Meintanis and B. Milošević and M.D. Jiménez–Gamero},
  doi          = {10.1016/j.csda.2024.107988},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107988},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Goodness–of–fit tests based on the min–characteristic function},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rank-based sequential feature selection for high-dimensional
accelerated failure time models with main and interaction effects.
<em>CSDA</em>, <em>197</em>, 107978. (<a
href="https://doi.org/10.1016/j.csda.2024.107978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional accelerated failure time (AFT) models are commonly used regression models in survival analysis. Feature selection problem in high-dimensional AFT models is addressed, considering scenarios involving solely main effects or encompassing both main and interaction effects. A rank-based sequential feature selection (RankSFS) method is proposed, the selection consistency is established and illustrated by comparing it with existing methods through extensive numerical simulations. The results show that RankSFS achieves a higher Positive Discovery Rate (PDR) and lower False Discovery Rate (FDR). Additionally, RankSFS is applied to analyze the data on Breast Cancer Relapse. With a remarkable short computational time, RankSFS successfully identifies two crucial genes.},
  archive      = {J_CSDA},
  author       = {Ke Yu and Shan Luo},
  doi          = {10.1016/j.csda.2024.107978},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107978},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rank-based sequential feature selection for high-dimensional accelerated failure time models with main and interaction effects},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A switching state-space transmission model for tracking
epidemics and assessing interventions. <em>CSDA</em>, <em>197</em>,
107977. (<a href="https://doi.org/10.1016/j.csda.2024.107977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective control of infectious diseases relies on accurate assessment of the impact of interventions, which is often hindered by the complex dynamics of the spread of disease. A Beta-Dirichlet switching state-space transmission model is proposed to track underlying dynamics of disease and evaluate the effectiveness of interventions simultaneously. As time evolves, the switching mechanism introduced in the susceptible-exposed-infected-recovered (SEIR) model is able to capture the timing and magnitude of changes in the transmission rate due to the effectiveness of control measures. The implementation of this model is based on a particle Markov Chain Monte Carlo algorithm, which can estimate the time evolution of SEIR states, switching states, and high-dimensional parameters efficiently. The efficacy of the proposed model and estimation procedure are demonstrated through simulation studies. With a real-world application to British Columbia&#39;s COVID-19 outbreak, the proposed switching state-space transmission model quantifies the reduction of transmission rate following interventions. The proposed model provides a promising tool to inform public health policies aimed at studying the underlying dynamics and evaluating the effectiveness of interventions during the spread of the disease.},
  archive      = {J_CSDA},
  author       = {Jingxue Feng and Liangliang Wang},
  doi          = {10.1016/j.csda.2024.107977},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107977},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A switching state-space transmission model for tracking epidemics and assessing interventions},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical bayes poisson matrix completion. <em>CSDA</em>,
<em>197</em>, 107976. (<a
href="https://doi.org/10.1016/j.csda.2024.107976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An empirical Bayes method for the Poisson matrix denoising and completion problems is proposed, and a corresponding algorithm called EBPM (Empirical Bayes Poisson Matrix) is developed. This approach is motivated by the non-central singular value shrinkage prior, which was used for the estimation of the mean matrix parameter of a matrix-variate normal distribution. Numerical experiments show that the EBPM algorithm outperforms the common nuclear norm penalized method in both matrix denoising and completion. The EBPM algorithm is highly efficient and does not require heuristic parameter tuning, as opposed to the nuclear norm penalized method, in which the regularization parameter should be selected. The EBPM algorithm also performs better than others in real-data applications.},
  archive      = {J_CSDA},
  author       = {Xiao Li and Takeru Matsuda and Fumiyasu Komaki},
  doi          = {10.1016/j.csda.2024.107976},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107976},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical bayes poisson matrix completion},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning via random forests: A one-shot federated
approach. <em>CSDA</em>, <em>197</em>, 107975. (<a
href="https://doi.org/10.1016/j.csda.2024.107975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A one-shot f ederated t ransfer learning method using r andom f orests (FTRF) is developed to improve the prediction accuracy at a target data site by leveraging information from auxiliary sites. Both theoretical and numerical results show that the proposed federated transfer learning approach is at least as accurate as the model trained on the target data alone regardless of possible data heterogeneity, which includes imbalanced and non-IID data distributions across sites and model mis-specification. FTRF has the ability to evaluate the similarity between the target and auxiliary sites, enabling the target site to autonomously select more similar site information to enhance its predictive performance. To ensure communication efficiency, FTRF adopts the model averaging idea that requires a single round of communication between the target and the auxiliary sites. Only fitted models from auxiliary sites are sent to the target site. Unlike traditional model averaging, FTRF incorporates predicted outcomes from other sites and the original variables when estimating model averaging weights, resulting in a variable-dependent weighting to better utilize models from auxiliary sites to improve prediction. Five real-world data examples show that FTRF reduces the prediction error by 2-40% compared to methods not utilizing auxiliary information.},
  archive      = {J_CSDA},
  author       = {Pengcheng Xiang and Ling Zhou and Lu Tang},
  doi          = {10.1016/j.csda.2024.107975},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107975},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Transfer learning via random forests: A one-shot federated approach},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian simultaneous factorization and prediction using
multi-omic data. <em>CSDA</em>, <em>197</em>, 107974. (<a
href="https://doi.org/10.1016/j.csda.2024.107974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative factorization methods for multi-omic data estimate factors explaining biological variation. Factors can be treated as covariates to predict an outcome and the factorization can be used to impute missing values. However, no available methods provide a comprehensive framework for statistical inference and uncertainty quantification for these tasks. A novel framework, Bayesian Simultaneous Factorization (BSF), is proposed to decompose multi-omics variation into joint and individual structures simultaneously within a probabilistic framework. BSF uses conjugate normal priors and the posterior mode of this model can be estimated by solving a structured nuclear norm-penalized objective that also achieves rank selection and motivates the choice of hyperparameters. BSF is then extended to simultaneously predict a continuous or binary phenotype while estimating latent factors, termed Bayesian Simultaneous Factorization and Prediction (BSFP). BSF and BSFP accommodate concurrent imputation, i.e., imputation during the model-fitting process, and full posterior inference for missing data, including “blockwise” missingness. It is shown via simulation that BSFP is competitive in recovering latent variation structure, and demonstrate the importance of accounting for uncertainty in the estimated factorization within the predictive model. The imputation performance of BSF is examined via simulation under missing-at-random and missing-not-at-random assumptions. Finally, BSFP is used to predict lung function based on the bronchoalveolar lavage metabolome and proteome from a study of HIV-associated obstructive lung disease, revealing multi-omic patterns related to lung function decline and a cluster of patients with obstructive lung disease driven by shared metabolomic and proteomic abundance patterns.},
  archive      = {J_CSDA},
  author       = {Sarah Samorodnitsky and Chris H. Wendt and Eric F. Lock},
  doi          = {10.1016/j.csda.2024.107974},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107974},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian simultaneous factorization and prediction using multi-omic data},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FDR control for linear log-contrast models with
high-dimensional compositional covariates. <em>CSDA</em>, <em>197</em>,
107973. (<a href="https://doi.org/10.1016/j.csda.2024.107973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear log-contrast models have been widely used to describe the relationship between the response of interest and the compositional covariates, in which one central task is to identify the significant compositional covariates while controlling the false discovery rate (FDR) at a nominal level. To achieve this goal, a new FDR control method is proposed for linear log-contrast models with high-dimensional compositional covariates. An appealing feature of the proposed method is that it completely bypasses the traditional p-values and utilizes only the symmetry property of the test statistic for the unimportant compositional covariates to give an upper bound of the FDR. Under some regularity conditions, the FDR can be asymptotically controlled at the nominal level for the proposed method in theory, and the theoretical power is also proven to approach one as the sample size tends to infinity. The finite-sample performance of the proposed method is evaluated through extensive simulation studies, and applications to microbiome compositional datasets are also provided.},
  archive      = {J_CSDA},
  author       = {Panxu Yuan and Changhan Jin and Gaorong Li},
  doi          = {10.1016/j.csda.2024.107973},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107973},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {FDR control for linear log-contrast models with high-dimensional compositional covariates},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CR-lasso: Robust cellwise regularized sparse regression.
<em>CSDA</em>, <em>197</em>, 107971. (<a
href="https://doi.org/10.1016/j.csda.2024.107971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellwise contamination remains a challenging problem for data scientists, particularly in research fields that require the selection of sparse features. Traditional robust methods may not be feasible nor efficient in dealing with such contaminated datasets. A robust Lasso-type cellwise regularization procedure is proposed which is coined CR-Lasso, that performs feature selection in the presence of cellwise outliers by minimising a regression loss and cell deviation measure simultaneously. The evaluation of this approach involves simulation studies that compare its selection and prediction performance with several sparse regression methods. The results demonstrate that CR-Lasso is competitive within the considered settings. The effectiveness of the proposed method is further illustrated through an analysis of a bone mineral density dataset.},
  archive      = {J_CSDA},
  author       = {Peng Su and Garth Tarr and Samuel Muller and Suojin Wang},
  doi          = {10.1016/j.csda.2024.107971},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {9},
  pages        = {107971},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {CR-lasso: Robust cellwise regularized sparse regression},
  volume       = {197},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian estimation of large-scale simulation models with
gaussian process regression surrogates. <em>CSDA</em>, <em>196</em>,
107972. (<a href="https://doi.org/10.1016/j.csda.2024.107972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large scale, computationally expensive simulation models pose a particular challenge when it comes to estimating their parameters from empirical data. Most simulation models do not possess closed-form expressions for their likelihood function, requiring the use of simulation-based inference, such as simulated method of moments, indirect inference, likelihood-free inference or approximate Bayesian computation. However, given the high computational requirements of large-scale models, it is often difficult to run these estimation methods, as they require more simulated runs that can feasibly be carried out. The aim is to address the problem by providing a full Bayesian estimation framework where the true but intractable likelihood function of the simulation model is replaced by one generated by a surrogate model trained on the limited simulated data. This is provided by a Linear Model of Coregionalization, where each latent variable is a sparse variational Gaussian process, chosen for its desirable convergence and consistency properties. The effectiveness of the approach is tested using both a simulated Bayesian computing analysis on a known data generating process, and an empirical application in which the free parameters of a computationally demanding agent-based model are estimated on US macroeconomic data.},
  archive      = {J_CSDA},
  author       = {Sylvain Barde},
  doi          = {10.1016/j.csda.2024.107972},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107972},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian estimation of large-scale simulation models with gaussian process regression surrogates},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous treatment effect-based random forest: HTERF.
<em>CSDA</em>, <em>196</em>, 107970. (<a
href="https://doi.org/10.1016/j.csda.2024.107970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimates of causal effects are needed to answer what-if questions about shifts in policy, such as new treatments in pharmacology or new pricing strategies for business owners. A new non-parametric approach is proposed to estimate the heterogeneous treatment effect based on random forests (HTERF). The potential outcome framework with unconfoundedness shows that the HTERF is pointwise almost surely consistent with the true treatment effect. Interpretability results are also presented.},
  archive      = {J_CSDA},
  author       = {Bérénice-Alexia Jocteur and Véronique Maume-Deschamps and Pierre Ribereau},
  doi          = {10.1016/j.csda.2024.107970},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107970},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Heterogeneous treatment effect-based random forest: HTERF},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian taut splines for estimating the number of modes.
<em>CSDA</em>, <em>196</em>, 107961. (<a
href="https://doi.org/10.1016/j.csda.2024.107961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of modes in a probability density function is representative of the complexity of a model and can also be viewed as the number of subpopulations. Despite its relevance, there has been limited research in this area. A novel approach to estimating the number of modes in the univariate setting is presented, focusing on prediction accuracy and inspired by some overlooked aspects of the problem: the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view that blends local and global density properties. The technique combines flexible kernel estimators and parsimonious compositional splines in the Bayesian inference paradigm, providing soft solutions and incorporating expert judgment. The procedure includes feature exploration, model selection, and mode testing, illustrated in a sports analytics case study showcasing multiple companion visualisation tools. A thorough simulation study also demonstrates that traditional modality-driven approaches paradoxically struggle to provide accurate results. In this context, the new method emerges as a top-tier alternative, offering innovative solutions for analysts.},
  archive      = {J_CSDA},
  author       = {José E. Chacón and Javier Fernández Serrano},
  doi          = {10.1016/j.csda.2024.107961},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107961},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian taut splines for estimating the number of modes},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection using data splitting and projection for
principal fitted component models in high dimension. <em>CSDA</em>,
<em>196</em>, 107960. (<a
href="https://doi.org/10.1016/j.csda.2024.107960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction (SDR) is such an effective way to detect nonlinear relationship between response variable and covariates by reducing the dimensionality of covariates without information loss. The principal fitted component (PFC) model is a way to implement SDR using some class of basis functions, however the PFC model is not efficient when there are many irrelevant or noisy covariates. There have been a few studies on the selection of variables in the PFC model via penalized regression or sequential likelihood ratio test. A novel variable selection technique in the PFC model has been proposed by incorporating a recent development in multiple testing such as mirror statistics and random data splitting. It is highlighted how we construct a mirror statistic in the PFC model using the idea of projection of coefficients to the other space generated from data splitting. The proposed method is superior to some existing methods in terms of false discovery rate (FDR) control and applicability to high-dimensional cases. In particular, the proposed method outperforms other methods as the number of covariates tends to be getting larger, which would be appealing in high dimensional data analysis. Simulation studies and analyses of real data sets have been conducted to show the finite sample performance and the gain that it yields compared to existing methods.},
  archive      = {J_CSDA},
  author       = {Seungchul Baek and Hoyoung Park and Junyong Park},
  doi          = {10.1016/j.csda.2024.107960},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107960},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection using data splitting and projection for principal fitted component models in high dimension},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sufficient dimension reduction for a novel class of
zero-inflated graphical models. <em>CSDA</em>, <em>196</em>, 107959. (<a
href="https://doi.org/10.1016/j.csda.2024.107959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models allow modeling of complex dependencies among components of a random vector. In many applications of graphical models, however, for example microbiome data, the data have an excess number of zero values. New pairwise graphical models with distributions in an exponential family are presented, that accommodate excess numbers of zeros in the random vector components. First these multivariate distributions are characterized in terms of univariate conditional distributions. Then predictors that arise from such a pairwise graphical model with excess zeros are modeled as functions of an outcome, and the corresponding first order sufficient dimension reduction (SDR) is derived. That is, linear combinations of the predictors that contain all the information for the regression of the outcome as a function of the predictors are obtained. To incorporate variable selection, the SDR is estimated using a pseudo-likelihood with a hierarchical penalty that prioritizes sparse interactions only for variables associated with the outcome. These methods yield consistent estimators of the reduction and can be applied to continuous or categorical outcomes. The new methods are then illustrated by studying normal, Poisson and truncated Poisson graphical models with excess zeros in simulations and by analyzing microbiome data from the American Gut Project. The models provided robust variable selection and the predictive performance of the Poisson zero-inflated pairwise graphical model was equal or better than that of other available methods for the analysis of microbiome data.},
  archive      = {J_CSDA},
  author       = {Eric Koplin and Liliana Forzani and Diego Tomassi and Ruth M. Pfeiffer},
  doi          = {10.1016/j.csda.2024.107959},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107959},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sufficient dimension reduction for a novel class of zero-inflated graphical models},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection in bayesian multiple instance regression
using shotgun stochastic search. <em>CSDA</em>, <em>196</em>, 107954.
(<a href="https://doi.org/10.1016/j.csda.2024.107954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple instance learning (MIL), a bag represents a sample that has a set of instances, each of which is described by a vector of explanatory variables, but the entire bag only has one label/response. Though many methods for MIL have been developed to date, few have paid attention to interpretability of models and results. The proposed Bayesian regression model stands on two levels of hierarchy, which transparently show how explanatory variables explain and instances contribute to bag responses. Moreover, two selection problems are simultaneously addressed; the instance selection to find out the instances in each bag responsible for the bag response, and the variable selection to search for the important covariates. To explore a joint discrete space of indicator variables created for selection of both explanatory variables and instances, the shotgun stochastic search algorithm is modified to fit in the MIL context. Also, the proposed model offers a natural and rigorous way to quantify uncertainty in coefficient estimation and outcome prediction, which many modern MIL applications call for. The simulation study shows the proposed regression model can select variables and instances with high performance (AUC greater than 0.86), thus predicting responses well. The proposed method is applied to the musk data for prediction of binding strengths (labels) between molecules (bags) with different conformations (instances) and target receptors. It outperforms all existing methods, and can identify variables relevant in modeling responses.},
  archive      = {J_CSDA},
  author       = {Seongoh Park and Joungyoun Kim and Xinlei Wang and Johan Lim},
  doi          = {10.1016/j.csda.2024.107954},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107954},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection in bayesian multiple instance regression using shotgun stochastic search},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mixture of logistic skew-normal multinomial models.
<em>CSDA</em>, <em>196</em>, 107946. (<a
href="https://doi.org/10.1016/j.csda.2024.107946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The logistic normal multinomial distribution is gaining interest in modelling microbiome data. It utilizes a hierarchical structure such that the observed counts conditional on the compositions are assumed to be multinomial random variables and the log-ratio transformed compositions are assumed to be from a Gaussian distribution. While multinomial distribution accounts for the compositional nature of the data, and a Gaussian prior offers flexibility in the structure of covariance matrices, the log-ratio transformed compositions of the microbiome data can be highly skewed, especially at a lower taxonomic level. Thus, a Gaussian distribution may not be an ideal prior for the log-ratio transformed compositions. A novel mixture of logistic skew-normal multinomial (LSNM) distribution is proposed in which a multivariate skew-normal distribution is utilized as a prior for the log-ratio transformed compositions. A variational Gaussian approximation in conjunction with the EM algorithm is utilized for parameter estimation.},
  archive      = {J_CSDA},
  author       = {Wangshu Tu and Ryan Browne and Sanjeena Subedi},
  doi          = {10.1016/j.csda.2024.107946},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107946},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A mixture of logistic skew-normal multinomial models},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian imaging inverse problem with SA-roundtrip prior via
HMC-pCN sampler. <em>CSDA</em>, <em>196</em>, 107930. (<a
href="https://doi.org/10.1016/j.csda.2024.107930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference with deep generative prior has received considerable interest for solving imaging inverse problems in many scientific and engineering fields. The selection of the prior distribution is learned from, and therefore an important representation learning of, available prior measurements. The SA-Roundtrip, a novel deep generative prior, is introduced to enable controlled sampling generation and identify the data&#39;s intrinsic dimension. This prior incorporates a self-attention structure within a bidirectional generative adversarial network. Subsequently, Bayesian inference is applied to the posterior distribution in the low-dimensional latent space using the Hamiltonian Monte Carlo with preconditioned Crank-Nicolson (HMC-pCN) algorithm, which is proven to be ergodic under specific conditions. Experiments conducted on computed tomography (CT) reconstruction with the MNIST and TomoPhantom datasets reveal that the proposed method outperforms state-of-the-art comparisons, consistently yielding a robust and superior point estimator along with precise uncertainty quantification.},
  archive      = {J_CSDA},
  author       = {Jiayu Qian and Yuanyuan Liu and Jingya Yang and Qingping Zhou},
  doi          = {10.1016/j.csda.2024.107930},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {8},
  pages        = {107930},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian imaging inverse problem with SA-roundtrip prior via HMC-pCN sampler},
  volume       = {196},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric accelerated failure time models under
unspecified random effect distributions. <em>CSDA</em>, <em>195</em>,
107958. (<a href="https://doi.org/10.1016/j.csda.2024.107958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated failure time (AFT) models with random effects, a useful alternative to frailty models, have been widely used for analyzing clustered (or correlated) time-to-event data. In the AFT model, the distribution of the unobserved random effect is conventionally assumed to be parametric, often modeled as a normal distribution. Although it has been known that a misspecfied random-effect distribution has little effect on regression parameter estimates, in some cases, the impact caused by such misspecification is not negligible. Particularly when our focus extends to quantities associated with random effects, the problem could become worse. In this paper, we propose a semi-parametric maximum likelihood approach in which the random-effect distribution under the AFT models is left unspecified. We provide a feasible algorithm to estimate the random-effect distribution as well as model parameters. Through comprehensive simulation studies, our results demonstrate the effectiveness of this proposed method across a range of random-effect distribution types (discrete or continuous) and under conditions of heavy censoring. The efficacy of the approach is further illustrated through simulation studies and real-world data examples.},
  archive      = {J_CSDA},
  author       = {Byungtae Seo and Il Do Ha},
  doi          = {10.1016/j.csda.2024.107958},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107958},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric accelerated failure time models under unspecified random effect distributions},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational bayesian approach for analyzing
interval-censored data under the proportional hazards model.
<em>CSDA</em>, <em>195</em>, 107957. (<a
href="https://doi.org/10.1016/j.csda.2024.107957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored failure time data frequently occur in medical follow-up studies among others and include right-censored data as a special case. Their analysis is much difficult than the analysis of the right-censored data due to their much more complicated structures and no partial likelihood. This article presents a variational Bayesian (VB) approach for analyzing such data under a proportional hazards model. The VB approach obtains a direct approximation of the posterior density. Compared to the Markov chain Monte Carlo (MCMC)-based sampling approaches, the VB approach achieves enhanced computational efficiency without sacrificing estimation accuracy. An extensive simulation study is conducted to compare the performance of the proposed methods with two main Bayesian methods currently available in the literature and the classic proportional hazards model and indicates that they work well in practical situations.},
  archive      = {J_CSDA},
  author       = {Wenting Liu and Huiqiong Li and Niansheng Tang and Jun Lyu},
  doi          = {10.1016/j.csda.2024.107957},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107957},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variational bayesian approach for analyzing interval-censored data under the proportional hazards model},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-task learning regression via convex clustering.
<em>CSDA</em>, <em>195</em>, 107956. (<a
href="https://doi.org/10.1016/j.csda.2024.107956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) is a methodology that aims to improve the general performance of estimation and prediction by sharing common information among related tasks. In the MTL, there are several assumptions for the relationships and methods to incorporate them. One of the natural assumptions in the practical situation is that tasks are classified into some clusters with their characteristics. For this assumption, the group fused regularization approach performs clustering of the tasks by shrinking the difference among tasks. This enables the transfer of common information within the same cluster. However, this approach also transfers the information between different clusters, which worsens the estimation and prediction. To overcome this problem, an MTL method is proposed with a centroid parameter representing a cluster center of the task. Because this model separates parameters into the parameters for regression and the parameters for clustering, estimation and prediction accuracy for regression coefficient vectors are improved. The effectiveness of the proposed method is shown through Monte Carlo simulations and applications to real data.},
  archive      = {J_CSDA},
  author       = {Akira Okazaki and Shuichi Kawano},
  doi          = {10.1016/j.csda.2024.107956},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107956},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multi-task learning regression via convex clustering},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new algorithm for inference in HMM’s with lower span
complexity. <em>CSDA</em>, <em>195</em>, 107955. (<a
href="https://doi.org/10.1016/j.csda.2024.107955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum likelihood problem for Hidden Markov Models is usually numerically solved by the Baum-Welch algorithm, which uses the Expectation-Maximization algorithm to find the estimates of the parameters. This algorithm has a recursion depth equal to the data sample size and cannot be computed in parallel, which limits the use of modern GPUs to speed up computation time. A new algorithm is proposed that provides the same estimates as the Baum-Welch algorithm, requiring about the same number of iterations, but is designed in such a way that it can be parallelized. As a consequence, it leads to a significant reduction in the computation time. This reduction is illustrated by means of numerical examples, where we consider simulated data as well as real datasets.},
  archive      = {J_CSDA},
  author       = {Diogo Pereira and Cláudia Nunes and Rui Rodrigues},
  doi          = {10.1016/j.csda.2024.107955},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107955},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new algorithm for inference in HMM&#39;s with lower span complexity},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semiparametric model for the cause-specific hazard under
risk proportionality. <em>CSDA</em>, <em>195</em>, 107953. (<a
href="https://doi.org/10.1016/j.csda.2024.107953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric Cox proportional hazards models enjoy great popularity in empirical survival analysis. A semiparametric model for cause-specific hazards under a proportionality restriction across risks is considered, which has desired practical properties such as estimation by partial likelihood and an analytical solution for the copula-graphic estimator. The cause-specific and marginal hazards are shown to share functional form restrictions in this case. The model for the cause-specific hazard can be used for inference about parametric restrictions on the marginal hazard without the risk of misspecifying the latter and without knowing the risk dependence. After the class of parametric marginal hazards has been determined, it can be estimated in conjunction with the degree of risk dependence. Finite sample properties are investigated with simulations. An application to employment duration demonstrates the practicality of the approach.},
  archive      = {J_CSDA},
  author       = {Simon M.S. Lo and Ralf A. Wilke and Takeshi Emura},
  doi          = {10.1016/j.csda.2024.107953},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107953},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A semiparametric model for the cause-specific hazard under risk proportionality},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pairwise share ratio interpretations of compositional
regression models. <em>CSDA</em>, <em>195</em>, 107945. (<a
href="https://doi.org/10.1016/j.csda.2024.107945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interpretation of regression models with compositional vectors as response and/or explanatory variables has been approached from different perspectives. The initial approaches are performed in coordinate space subsequent to applying a log-ratio transformation to the compositional vectors. Given that these models exhibit non-linearity concerning classical operations within real space, an alternative approach has been proposed. This approach relies on infinitesimal increments or derivatives, interpreted within a simplex framework. Consequently, it offers interpretations of elasticities or semi-elasticities in the original space of shares which are independent of any log-ratio transformations. Some functions of these elasticities or semi-elasticities turn out to be constant throughout the sample observations, making them natural parameters for interpreting CoDa models. These parameters are linked to relative variations of pairwise share ratios of the response and/or of the explanatory variables. Approximations of share ratio variations are derived and linked to these natural parameters. A real dataset on the French presidential election is utilized to illustrate each type of interpretation in detail.},
  archive      = {J_CSDA},
  author       = {Lukas Dargel and Christine Thomas-Agnan},
  doi          = {10.1016/j.csda.2024.107945},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107945},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Pairwise share ratio interpretations of compositional regression models},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection of kolmogorov-smirnov maximization with a
penalized surrogate loss. <em>CSDA</em>, <em>195</em>, 107944. (<a
href="https://doi.org/10.1016/j.csda.2024.107944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kolmogorov-Smirnov (KS) statistic is quite popular in many areas as the major performance evaluation criterion for binary classification due to its explicit business intension. Fang and Chen (2019) proposed a novel DMKS method that directly maximizes the KS statistic and compares favorably with the popular existing methods. However, DMKS did not consider the critical problem of variable selection since the special form of KS brings great challenge to establish the DMKS estimator&#39;s asymptotic distribution which is most likely to be nonstandard. This intractable issue is handled by introducing a surrogate loss function which leads to a n n -consistent estimator for the true parameter up to a multiplicative scalar. Then a nonconcave penalty function is combined to achieve the variable selection consistency and asymptotical normality with the oracle property. Results of empirical studies confirm the theoretical results and show advantages of the proposed SKS (Surrogated Kolmogorov-Smirnov) method compared to the original DMKS method without variable selection.},
  archive      = {J_CSDA},
  author       = {Xiefang Lin and Fang Fang},
  doi          = {10.1016/j.csda.2024.107944},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {7},
  pages        = {107944},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection of kolmogorov-smirnov maximization with a penalized surrogate loss},
  volume       = {195},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference on order restricted means of inverse gaussian
populations under heteroscedasticity. <em>CSDA</em>, <em>194</em>,
107943. (<a href="https://doi.org/10.1016/j.csda.2024.107943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hypothesis testing problem of homogeneity of k ( ≥ 2 ) (≥2) inverse Gaussian means against ordered alternatives is studied when nuisance or scale-like parameters are unknown and unequal. The maximum likelihood estimators (MLEs) of means and scale-like parameters are obtained when means satisfy some simple order restrictions and scale-like parameters are unknown and unequal. An iterative algorithm is proposed for finding these estimators. It has been proved that under a specific condition, the proposed algorithm converges to the true MLEs uniquely. A likelihood ratio test and two simultaneous tests are proposed. Further, an algorithm for finding the MLEs of parameters is given when means are equal but unknown. Using the estimators, the likelihood ratio test is developed for testing against ordered alternative means. Using the asymptotic distribution, the asymptotic likelihood ratio test is proposed. However, for small samples, it does not perform well. Hence, a parametric bootstrap likelihood ratio test (PB LRT) is proposed. Therefore, the asymptotic validity of the bootstrap procedure has been shown. Using the Box-type approximation method, test statistics are developed for the two-sample problem of equality of means when scale-like parameters are heterogeneous. Using these, two PB-based heuristic tests are proposed. Asymptotic null distributions are derived and PB accuracy is also developed. Two asymptotic tests are also proposed using the asymptotic null distributions. To get the critical points and test statistics of the three PB tests and two asymptotic tests, an ‘R’ package is developed and shared on GitHub. Applications of the tests are illustrated using real data.},
  archive      = {J_CSDA},
  author       = {Anjana Mondal and Somesh Kumar},
  doi          = {10.1016/j.csda.2024.107943},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107943},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inference on order restricted means of inverse gaussian populations under heteroscedasticity},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential estimation for mixture of regression models for
heterogeneous population. <em>CSDA</em>, <em>194</em>, 107942. (<a
href="https://doi.org/10.1016/j.csda.2024.107942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneity among patients commonly exists in clinical studies and leads to challenges in medical research. It is widely accepted that there exist various sub-types in the population and they are distinct from each other. The approach of identifying the sub-types and thus tailoring disease prevention and treatment is known as precision medicine. The mixture model is a classical statistical model to cluster the heterogeneous population into homogeneous sub-populations. However, for the highly heterogeneous population with multiple components, its parameter estimation and clustering results may be ambiguous due to the dependence of the EM algorithm on the initial values. For sub-typing purposes, the finite mixture of regression models with concomitant variables is considered and a novel statistical method is proposed to identify the main components with large proportions in the mixture sequentially. Compared to existing typical statistical inferences, the new method not only requires no pre-specification on the number of components for model fitting, but also provides more reliable parameter estimation and clustering results. Simulation studies demonstrated the superiority of the proposed method. Real data analysis on the drug response prediction illustrated its reliability in the parameter estimation and capability to identify the important subgroup.},
  archive      = {J_CSDA},
  author       = {Na You and Hongsheng Dai and Xueqin Wang and Qingyun Yu},
  doi          = {10.1016/j.csda.2024.107942},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107942},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sequential estimation for mixture of regression models for heterogeneous population},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic process representation for time warping
functions. <em>CSDA</em>, <em>194</em>, 107941. (<a
href="https://doi.org/10.1016/j.csda.2024.107941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time warping function provides a mathematical representation to measure phase variability in functional data. Recent studies have developed various approaches to estimate optimal warping between functions. However, a principled, linear, generative representation on time warping functions is still under-explored. This is highly challenging because the warping functions are non-linear in the conventional L 2 L2 space. To address this problem, a new linear warping space is defined and a stochastic process representation is proposed to characterize time warping functions. The key is to define an inner-product structure on the time warping space, followed by a transformation which maps the warping functions into a sub-space of the L 2 L2 space. With certain constraints on the warping functions, this transformation is an isometric isomorphism. In the transformed space, the L 2 L2 basis in the Hilbert space is adopted for representation, which can be easily utilized to generate time warping functions by using different types of stochastic process. The effectiveness of this representation is demonstrated through its use as a new penalty in the penalized function registration, accompanied by an efficient gradient method to minimize the cost function. The new penalized method is illustrated through simulations that properly characterize nonuniform and correlated constraints in the time domain. Furthermore, this representation is utilized to develop a boxplot for warping functions, which can estimate templates and identify warping outliers. Finally, this representation is applied to a Covid-19 dataset to construct boxplots and identify states with outlying growth patterns.},
  archive      = {J_CSDA},
  author       = {Yijia Ma and Xinyu Zhou and Wei Wu},
  doi          = {10.1016/j.csda.2024.107941},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107941},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A stochastic process representation for time warping functions},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Factor selection in screening experiments by aggregation
over random models. <em>CSDA</em>, <em>194</em>, 107940. (<a
href="https://doi.org/10.1016/j.csda.2024.107940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Screening experiments are useful for identifying a small number of truly important factors from a large number of potentially important factors. The Gauss-Dantzig Selector (GDS) is often the preferred analysis method for screening experiments. Just considering main-effects models can result in erroneous conclusions, but including interaction terms, even if restricted to two-factor interactions, increases the number of model terms dramatically and challenges the GDS analysis. A new analysis method, called Gauss-Dantzig Selector Aggregation over Random Models (GDS-ARM), which performs a GDS analysis on multiple models that include only some randomly selected interactions, is proposed. Results from these different analyses are then aggregated to identify the important factors. The proposed method is discussed, the appropriate choices for the tuning parameters are suggested, and the performance of the method is studied on real and simulated data.},
  archive      = {J_CSDA},
  author       = {Rakhi Singh and John Stufken},
  doi          = {10.1016/j.csda.2024.107940},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107940},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Factor selection in screening experiments by aggregation over random models},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter estimation and random number generation for
student lévy processes. <em>CSDA</em>, <em>194</em>, 107933. (<a
href="https://doi.org/10.1016/j.csda.2024.107933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges in estimating parameters of the widely applied Student-Lévy process, the study introduces two distinct methods: a likelihood-based approach and a data-driven approach. A two-step quasi-likelihood-based method is initially proposed, countering the non-closed nature of the Student-Lévy process&#39;s distribution function under convolution. This method utilizes the limiting properties observed in high-frequency data, offering estimations via a quasi-likelihood function characterized by asymptotic normality. Additionally, a novel neural-network-based parameter estimation technique is advanced, independent of high-frequency observation assumptions. Utilizing a CNN-LSTM framework, this method effectively processes sparse, local jump-related data, extracts deep features, and maps these to the parameter space using a fully connected neural network. This innovative approach ensures minimal assumption reliance, end-to-end processing, and high scalability, marking a significant advancement in parameter estimation techniques. The efficacy of both methods is substantiated through comprehensive numerical experiments, demonstrating their robust performance in diverse scenarios.},
  archive      = {J_CSDA},
  author       = {Shuaiyu Li and Yunpei Wu and Yuzhong Cheng},
  doi          = {10.1016/j.csda.2024.107933},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107933},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Parameter estimation and random number generation for student lévy processes},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Block-wise primal-dual algorithms for large-scale doubly
penalized ANOVA modeling. <em>CSDA</em>, <em>194</em>, 107932. (<a
href="https://doi.org/10.1016/j.csda.2024.107932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For multivariate nonparametric regression, doubly penalized ANOVA modeling (DPAM) has recently been proposed, using hierarchical total variations (HTVs) and empirical norms as penalties on the component functions such as main effects and multi-way interactions in a functional ANOVA decomposition of the underlying regression function. The two penalties play complementary roles: the HTV penalty promotes sparsity in the selection of basis functions within each component function, whereas the empirical-norm penalty promotes sparsity in the selection of component functions. To facilitate large-scale training of DPAM using backfitting or block minimization, two suitable primal-dual algorithms are developed, including both batch and stochastic versions, for updating each component function in single-block optimization. Existing applications of primal-dual algorithms are intractable for DPAM with both HTV and empirical-norm penalties. The validity and advantage of the stochastic primal-dual algorithms are demonstrated through extensive numerical experiments, compared with their batch versions and a previous active-set algorithm, in large-scale scenarios.},
  archive      = {J_CSDA},
  author       = {Penghui Fu and Zhiqiang Tan},
  doi          = {10.1016/j.csda.2024.107932},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107932},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Block-wise primal-dual algorithms for large-scale doubly penalized ANOVA modeling},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible regularized estimation in high-dimensional mixed
membership models. <em>CSDA</em>, <em>194</em>, 107931. (<a
href="https://doi.org/10.1016/j.csda.2024.107931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed membership models are an extension of finite mixture models, where each observation can partially belong to more than one mixture component. A probabilistic framework for mixed membership models of high-dimensional continuous data is proposed with a focus on scalability and interpretability. The novel probabilistic representation of mixed membership is based on convex combinations of dependent multivariate Gaussian random vectors. In this setting, scalability is ensured through approximations of a tensor covariance structure through multivariate eigen-approximations with adaptive regularization imposed through shrinkage priors. Conditional weak posterior consistency is established on an unconstrained model, allowing for a simple posterior sampling scheme while keeping many of the desired theoretical properties of our model. The model is motivated by two biomedical case studies: a case study on functional brain imaging of children with autism spectrum disorder (ASD) and a case study on gene expression data from breast cancer tissue. These applications highlight how the typical assumption made in cluster analysis, that each observation comes from one homogeneous subgroup, may often be restrictive in several applications, leading to unnatural interpretations of data features.},
  archive      = {J_CSDA},
  author       = {Nicholas Marco and Damla Şentürk and Shafali Jeste and Charlotte C. DiStefano and Abigail Dickinson and Donatello Telesca},
  doi          = {10.1016/j.csda.2024.107931},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107931},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Flexible regularized estimation in high-dimensional mixed membership models},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Goodness-of-fit test for point processes first-order
intensity. <em>CSDA</em>, <em>194</em>, 107929. (<a
href="https://doi.org/10.1016/j.csda.2024.107929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling the first-order intensity function is one of the main aims in point process theory. An appropriate model describes the first-order intensity as a nonparametric function of spatial covariates . A formal testing procedure is presented to assess the goodness-of-fit of this model, assuming an inhomogeneous Poisson point process . The test is based on a quadratic distance between two kernel intensity estimators. The asymptotic normality of the test statistic is proved and a bootstrap procedure to approximate its distribution is suggested. The proposal is illustrated with two applications to real data sets, and an extensive simulation study to evaluate its finite-sample performance.},
  archive      = {J_CSDA},
  author       = {M.I. Borrajo and W. González-Manteiga and M.D. Martínez-Miranda},
  doi          = {10.1016/j.csda.2024.107929},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107929},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Goodness-of-fit test for point processes first-order intensity},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heterogeneous quantile regression for longitudinal data with
subgroup structures. <em>CSDA</em>, <em>194</em>, 107928. (<a
href="https://doi.org/10.1016/j.csda.2024.107928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgroup analysis for modeling longitudinal data with heterogeneity across all individuals has drawn attention in the modern statistical learning. In this paper, we focus on heterogeneous quantile regression model and propose to achieve variable selection, heterogeneous subgrouping and parameter estimation simultaneously, by using the smoothed generalized estimating equations in conjunction with the multi-directional separation penalty. The proposed method allows individuals to be divided into multiple subgroups for different heterogeneous covariates such that estimation efficiency can be gained through incorporating individual correlation structure and sharing information within subgroups. A data-driven procedure based on a modified BIC is applied to estimate the number of subgroups. Theoretical properties of the oracle estimator given the underlying true subpopulation information are firstly provided and then it is shown that the proposed estimator is equivalent to the oracle estimator under some conditions. The finite-sample performance of the proposed estimators is studied through simulations and an application to an AIDS dataset is also presented.},
  archive      = {J_CSDA},
  author       = {Zhaohan Hou and Lei Wang},
  doi          = {10.1016/j.csda.2024.107928},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107928},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Heterogeneous quantile regression for longitudinal data with subgroup structures},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust heavy-tailed versions of generalized linear models
with applications in actuarial science. <em>CSDA</em>, <em>194</em>,
107920. (<a href="https://doi.org/10.1016/j.csda.2024.107920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear models (GLMs) form one of the most popular classes of models in statistics . The gamma variant is used, for instance, in actuarial science for the modelling of claim amounts in insurance. A flaw of GLMs is that they are not robust against outliers (i.e., against erroneous or extreme data points). A difference in trends in the bulk of the data and the outliers thus yields skewed inference and predictions. To address this problem, robust methods have been introduced. The most commonly applied robust method is frequentist and consists in an estimator which is derived from a modification of the derivative of the log-likelihood. The objective is to propose an alternative approach which is modelling-based and thus fundamentally different. Such an approach allows for an understanding and interpretation of the modelling, and it can be applied for both frequentist and Bayesian statistical analyses. The proposed approach possesses appealing theoretical and empirical properties.},
  archive      = {J_CSDA},
  author       = {Philippe Gagnon and Yuxi Wang},
  doi          = {10.1016/j.csda.2024.107920},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107920},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust heavy-tailed versions of generalized linear models with applications in actuarial science},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unified framework of analyzing missing data and variable
selection using regularized likelihood. <em>CSDA</em>, <em>194</em>,
107919. (<a href="https://doi.org/10.1016/j.csda.2024.107919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data arise commonly in applications, and research on this topic has received extensive attention in the past few decades. Various inference methods have been developed under different missing data mechanisms , including missing at random and missing not at random. The assessment of a feasible missing data mechanism is, however, difficult due to the lack of validation data. The problem is further complicated by the presence of spurious variables in covariates . Focusing on missingness in the response variable, a unified modeling scheme is proposed by utilizing the parametric generalized additive model to characterize various types of missing data processes. Taking the generalized linear model to facilitate the dependence of the response on the associated covariates, the concurrent estimation and variable selection procedures are developed using regularized likelihood, and the asymptotic properties for the resultant estimators are rigorously established. The proposed methods are appealing in their flexibility and generality; they circumvent the need of assuming a particular missing data mechanism that is required by most available methods. Empirical studies demonstrate that the proposed methods result in satisfactory performance in finite sample settings. Extensions to accommodating missingness in both the response and covariates are also discussed.},
  archive      = {J_CSDA},
  author       = {Yuan Bian and Grace Y. Yi and Wenqing He},
  doi          = {10.1016/j.csda.2024.107919},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107919},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A unified framework of analyzing missing data and variable selection using regularized likelihood},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple approach for local and global variable importance
in nonlinear regression models. <em>CSDA</em>, <em>194</em>, 107914. (<a
href="https://doi.org/10.1016/j.csda.2023.107914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to interpret machine learning models has become increasingly important as their usage in data science continues to rise. Most current interpretability methods are optimized to work on either ( i ) a global scale, where the goal is to rank features based on their contributions to overall variation in an observed population, or ( ii ) the local level, which aims to detail on how important a feature is to a particular individual in the data set. In this work, a new operator is proposed called the “GlObal And Local Score” (GOALS): a simple post hoc approach to simultaneously assess local and global feature variable importance in nonlinear models . Motivated by problems in biomedicine, the approach is demonstrated using Gaussian process regression where the task of understanding how genetic markers are associated with disease progression both within individuals and across populations is of high interest. Detailed simulations and real data analyses illustrate the flexible and efficient utility of GOALS over state-of-the-art variable importance strategies.},
  archive      = {J_CSDA},
  author       = {Emily T. Winn-Nuñez and Maryclare Griffin and Lorin Crawford},
  doi          = {10.1016/j.csda.2023.107914},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {6},
  pages        = {107914},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A simple approach for local and global variable importance in nonlinear regression models},
  volume       = {194},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated subgroup identification from multi-source data.
<em>CSDA</em>, <em>193</em>, 107918. (<a
href="https://doi.org/10.1016/j.csda.2024.107918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subgroup identification is crucial in dealing with the heterogeneous population and has wide applications in various areas, such as clinical trials and market segmentation. With the prevalence of multi-source data, there is a practical need to identify subgroups based on multi-source data. This paper proposes a working-independence pseudo-loglikelihood and integrates the parameters of each source into a pairwise fusion penalty for simultaneous parameter estimation and subgroup identification. To implement the proposed method, an alternating direction method of multipliers (ADMM) algorithm is derived. Furthermore, the weak oracle properties of parameter estimation are established, illustrating the latent subgroups can be consistently identified. Finally, numerical simulations and an analysis of a randomized trial on reduced nicotine standards for cigarettes are conducted to evaluate the performance of the proposed method.},
  archive      = {J_CSDA},
  author       = {Lihui Shao and Jiaqi Wu and Weiping Zhang and Yu Chen},
  doi          = {10.1016/j.csda.2024.107918},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107918},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Integrated subgroup identification from multi-source data},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oracle-efficient estimation and trend inference in
non-stationary time series with trend and heteroscedastic ARMA error.
<em>CSDA</em>, <em>193</em>, 107917. (<a
href="https://doi.org/10.1016/j.csda.2024.107917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The non-stationary time series often contain an unknown trend and unobserved error terms . The error terms in the proposed model consist of a smooth variance function and the latent stationary ARMA series, which allows heteroscedasticity at different time points . The theoretically justified two-step B-spline estimation method is proposed for the trend and variance function in the model, and then residuals are obtained by removing the trend and variance function estimators from the data. The maximum likelihood estimator (MLE) for the latent ARMA error coefficients based on the residuals is shown to be oracally efficient in the sense that it has the same asymptotic distribution as the infeasible MLE if the trend and variance function were known. In addition to the oracle efficiency, a kernel estimator is obtained for the trend function and shown to converge to the Gumbel distribution . It yields an asymptotically correct simultaneous confidence band (SCB) for the trend function, which can be used to test the specific form of trend. A simulation-based procedure is proposed to implement the SCB, and simulation and real data analysis illustrate the finite sample performance.},
  archive      = {J_CSDA},
  author       = {Chen Zhong},
  doi          = {10.1016/j.csda.2024.107917},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107917},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Oracle-efficient estimation and trend inference in non-stationary time series with trend and heteroscedastic ARMA error},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Post-clustering difference testing: Valid inference and
practical considerations with applications to ecological and biological
data. <em>CSDA</em>, <em>193</em>, 107916. (<a
href="https://doi.org/10.1016/j.csda.2023.107916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is part of unsupervised analysis methods that group samples into homogeneous and separate subgroups of observations also called clusters. To interpret the clusters, statistical hypothesis testing is often used to infer the variables that significantly separate the estimated clusters from each other. However, data-driven hypotheses are thus used for the inference process because the hypotheses are derived from the clustering results . This double use of the data leads traditional hypothesis test to fail to control the Type I error rate particularly because of uncertainty in the clustering process and the potential artificial differences it could create. Three novel statistical hypothesis tests are introduced, each designed to account for the clustering process. These tests efficiently control the Type I error rate by identifying only variables that contain a true signal separating groups of observations. The proposed tests were applied in two distinct contexts: animal ecology and immunology, demonstrating the relevance of the results with real datasets.},
  archive      = {J_CSDA},
  author       = {Benjamin Hivert and Denis Agniel and Rodolphe Thiébaut and Boris P. Hejblum},
  doi          = {10.1016/j.csda.2023.107916},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107916},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Post-clustering difference testing: Valid inference and practical considerations with applications to ecological and biological data},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generalized latent space model for one-mode networks with
awareness of two-mode networks. <em>CSDA</em>, <em>193</em>, 107915. (<a
href="https://doi.org/10.1016/j.csda.2023.107915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent space models have been widely studied for one-mode networks, in which the same type of nodes connect with each other. In many applications, one-mode networks are often observed along with two-mode networks, which reflect connections between different types of nodes and provide important information for understanding the one-mode network structure. However, the classical one-mode latent space models have several limitations in incorporating two-mode networks. To address this gap, a generalized latent space model is proposed to capture common structures and heterogeneous connecting patterns across one-mode and two-mode networks. Specifically, each node is embedded with a latent vector and network-specific degree parameters that determine the connection probabilities between nodes. A projected gradient descent algorithm is developed to estimate the latent vectors and degree parameters. Moreover, the theoretical properties of the estimators are established and it has been proven that the estimation accuracy of the shared latent vectors can be improved through incorporating two-mode networks. Finally, simulation studies and applications on two real-world datasets demonstrate the usefulness of the proposed model.},
  archive      = {J_CSDA},
  author       = {Xinyan Fan and Kuangnan Fang and Dan Pu and Ruixuan Qin},
  doi          = {10.1016/j.csda.2023.107915},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107915},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized latent space model for one-mode networks with awareness of two-mode networks},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change point detection via feedforward neural networks with
theoretical guarantees. <em>CSDA</em>, <em>193</em>, 107913. (<a
href="https://doi.org/10.1016/j.csda.2023.107913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article mainly studies change point detection for mean shift change point model. An estimation method is proposed to estimate the change point via feedforward neural networks . The complete f -moment consistency of the proposed estimator is obtained. Numerical simulation results show that the performance of the proposed estimator is better than that of cumulative sum type estimator which is widely used in the change point detection, especially when the mean shift signal size is small. Finally, we demonstrate the proposed method by empirically analyzing a stock data set.},
  archive      = {J_CSDA},
  author       = {Houlin Zhou and Hanbing Zhu and Xuejun Wang},
  doi          = {10.1016/j.csda.2023.107913},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107913},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Change point detection via feedforward neural networks with theoretical guarantees},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical likelihood in a partially linear single-index
model with censored response data. <em>CSDA</em>, <em>193</em>, 107912.
(<a href="https://doi.org/10.1016/j.csda.2023.107912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An empirical likelihood (EL) approach for a partial linear single-index model with censored response data is studied. A bias-corrected EL ratio is proposed, and the asymptotic chi-squared distribution of this ratio is obtained. The result can be directly used to construct the confidence regions of the regression parameters . The estimators of regression parameters and link function are constructed, and their asymptotic distributions are obtained. Also, a confidence band of the link function is constructed. The proposed method has two main features: The first feature is that the EL ratio is calibrated directly from within, instead of multiplying an adjustment factor by an EL ratio, which reflects the nature of EL. The second feature is avoiding undersmoothing of nonparametric functions, thus ensuring that the n n -consistency of the parameter estimator. As a byproduct, the EL and estimation of a single-index model with censored response data are studied. The performance of the bias-corrected EL is evaluated by the simulation studies. The proposed method is illustrated with an example of a real data analysis.},
  archive      = {J_CSDA},
  author       = {Liugen Xue},
  doi          = {10.1016/j.csda.2023.107912},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {5},
  pages        = {107912},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical likelihood in a partially linear single-index model with censored response data},
  volume       = {193},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Group variable selection via group sparse neural network.
<em>CSDA</em>, <em>192</em>, 107911. (<a
href="https://doi.org/10.1016/j.csda.2023.107911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group variable selection is an important issue in high-dimensional data modeling and most of existing methods consider only the linear model. Therefore, a new method based on the deep neural network (DNN), an increasingly popular nonlinear method in both statistics and deep learning communities, is proposed. The method is applicable to general nonlinear models , including the linear model as a special case. Specifically, a group sparse neural network (GSNN) is designed, where the definition of nonlinear group high-level features (NGHFs) is generalized to the network structure. A two-stage group sparse (TGS) algorithm is employed to induce group variables selection by performing group structure selection on the network. GSNN is promising for complex nonlinear systems with interactions and correlated predictors , overcoming the shortcomings of linear or marginal variable selection methods. Theoretical results on convergence and group-level selection consistency are also given. Simulations results and real data analysis demonstrate the superiority of our method.},
  archive      = {J_CSDA},
  author       = {Xin Zhang and Junlong Zhao},
  doi          = {10.1016/j.csda.2023.107911},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107911},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Group variable selection via group sparse neural network},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subgroup detection based on partially linear additive
individualized model with missing data in response. <em>CSDA</em>,
<em>192</em>, 107910. (<a
href="https://doi.org/10.1016/j.csda.2023.107910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on partially linear additive individualized model, a fusion-penalized inverse probability weighted least squares method is proposed to detect the subgroup for missing data in response. Firstly, the B-spline technique is used to approximate the unknown additive individualized functions and then an inverse probability weighted quadratic loss function is established with fusion penalty on the difference of subject-wise B-spline coefficients. Secondly, minimization of such quadratic loss function leads to the estimation of linear regression parameters and individualized B spline coefficients . With a proper tuning parameter, some differences in penalty term are shrunk into zero and thus the corresponding subjects will be clustered into the same subgroup. Thirdly, a clustering method is developed to automatically determine the subgroup membership for the subjects with missing data. Fourthly, large sample properties of resulting estimates are given under some regular conditions. Finally, numerical studies are presented to illustrate the performance of the proposed subgroup detection method.},
  archive      = {J_CSDA},
  author       = {Tingting Cai and Jianbo Li and Qin Zhou and Songlou Yin and Riquan Zhang},
  doi          = {10.1016/j.csda.2023.107910},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107910},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Subgroup detection based on partially linear additive individualized model with missing data in response},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A laplace-based model with flexible tail behavior.
<em>CSDA</em>, <em>192</em>, 107909. (<a
href="https://doi.org/10.1016/j.csda.2023.107909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proposed multiple scaled contaminated asymmetric Laplace (MSCAL) distribution is an extension of the multivariate asymmetric Laplace distribution to allow for a different excess kurtosis on each dimension and for more flexible shapes of the hyper-contours. These peculiarities are obtained by working on the principal component (PC) space. The structure of the MSCAL distribution has the further advantage of allowing for automatic PC-wise outlier detection – i.e., detection of outliers separately on each PC – when convenient constraints on the parameters are imposed. The MSCAL is fitted using a Monte Carlo expectation-maximization (MCEM) algorithm that uses a Monte Carlo method to estimate the orthogonal matrix of eigenvectors . A simulation study is used to assess the proposed MCEM in terms of computational efficiency and parameter recovery. In a real data application, the MSCAL is fitted to a real data set containing the anthropometric measurements of monozygotic/dizygotic twins. Both a skewed bivariate subset of the full data, perturbed by some outlying points, and the full data are considered.},
  archive      = {J_CSDA},
  author       = {Cristina Tortora and Brian C. Franczak and Luca Bagnato and Antonio Punzo},
  doi          = {10.1016/j.csda.2023.107909},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107909},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A laplace-based model with flexible tail behavior},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Graph-based spatial segmentation of areal data.
<em>CSDA</em>, <em>192</em>, 107908. (<a
href="https://doi.org/10.1016/j.csda.2023.107908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smoothing is often used to improve the readability and interpretability of noisy areal data. However, there are many instances where the underlying quantity is discontinuous. For such cases, specific methods are needed to estimate the piecewise constant spatial process. A well-known approach in this setting is to perform segmentation of the signal using the adjacency graph, such as the graph-based fused lasso. However, this method does not scale well to large graphs. A new method is introduced for piecewise constant spatial estimation that (i) is faster to compute on large graphs and (ii) yields sparser models than the fused lasso (for the same amount of regularization), resulting in estimates that are easier to interpret. The method is illustrated on simulated data and applied to real data on overweight prevalence in the Netherlands. Healthy and unhealthy zones are identified, which cannot be explained by demographic or socio-economic characteristics alone. The method is found capable of identifying such zones and can assist policymakers with their health improving strategies.},
  archive      = {J_CSDA},
  author       = {Vivien Goepp and Jan van de Kassteele},
  doi          = {10.1016/j.csda.2023.107908},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107908},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Graph-based spatial segmentation of areal data},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Discrepancy between structured matrices in the power
analysis of a separability test. <em>CSDA</em>, <em>192</em>, 107907.
(<a href="https://doi.org/10.1016/j.csda.2023.107907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in the analysis of multivariate data is testing of the covariance matrix structure. In particular, for assessing separability , various tests have been proposed. However, the development of a method of measuring discrepancy between two covariance matrix structures, in relation to the study of the power of the test, remains an open problem. Therefore, a discrepancy measure is proposed such that for two arbitrary alternative hypotheses with the same value of discrepancy, the power of tests remains stable, while for increasing discrepancy the power increases. The basic hypothesis is related to the separable structure of the observation matrix under a doubly multivariate normal model, as assessed by the likelihood ratio and Rao score tests. It is shown that the particular one-parameter method and the Frobenius norm fail in the power analysis of tests, while the entropy and quadratic loss functions can be efficiently used to measure the discrepancy between separable and non-separable covariance structures for a multivariate normal distribution .},
  archive      = {J_CSDA},
  author       = {Katarzyna Filipiak and Daniel Klein and Monika Mokrzycka},
  doi          = {10.1016/j.csda.2023.107907},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107907},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Discrepancy between structured matrices in the power analysis of a separability test},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical false discovery rate control for
high-dimensional survival analysis with interactions. <em>CSDA</em>,
<em>192</em>, 107906. (<a
href="https://doi.org/10.1016/j.csda.2023.107906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of data collection techniques, analysis with a survival response and high-dimensional covariates has become routine. Here we consider an interaction model, which includes a set of low-dimensional covariates, a set of high-dimensional covariates, and their interactions. This model has been motivated by gene-environment (G-E) interaction analysis, where the E variables have a low dimension, and the G variables have a high dimension . For such a model, there has been extensive research on estimation and variable selection. Comparatively, inference studies with a valid false discovery rate (FDR) control have been very limited. The existing high-dimensional inference tools cannot be directly applied to interaction models, as interactions and main effects are not “equal”. In this article, for high-dimensional survival analysis with interactions, we model survival using the Accelerated Failure Time (AFT) model and adopt a “weighted least squares + debiased Lasso” approach for estimation and selection. A hierarchical FDR control approach is developed for inference and respect of the “main effects, interactions” hierarchy. The asymptotic distribution properties of the debiased Lasso estimators are rigorously established. Simulation demonstrates the satisfactory performance of the proposed approach, and the analysis of a breast cancer dataset further establishes its practical utility.},
  archive      = {J_CSDA},
  author       = {Weijuan Liang and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1016/j.csda.2023.107906},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107906},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hierarchical false discovery rate control for high-dimensional survival analysis with interactions},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and adaptive functional logistic regression.
<em>CSDA</em>, <em>192</em>, 107905. (<a
href="https://doi.org/10.1016/j.csda.2023.107905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel family of robust estimators for the functional logistic regression model is introduced and studied. The estimators are based on the concept of density power divergence between densities and may be formed with any combination of lower rank approximations and penalties, as the need arises. Uniform convergence and high rates of convergence with respect to the commonly used prediction error are established under general assumptions. Importantly, these assumptions permit random tuning parameters thereby allowing for the robustness of the estimators to adapt to the data. This leads to estimators with high efficiency in clean data and a high degree of resistance towards atypical observations. The highly competitive practical performance of the proposed family of estimators is illustrated on a simulation study and a real data example involving gait analysis and which includes atypical observations.},
  archive      = {J_CSDA},
  author       = {Ioannis Kalogridis},
  doi          = {10.1016/j.csda.2023.107905},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107905},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust and adaptive functional logistic regression},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HiQR: An efficient algorithm for high-dimensional quadratic
regression with penalties. <em>CSDA</em>, <em>192</em>, 107904. (<a
href="https://doi.org/10.1016/j.csda.2023.107904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the efficient solution of penalized quadratic regressions in high-dimensional settings. A novel and efficient algorithm for ridge-penalized quadratic regression is proposed, leveraging the matrix structures of the regression with interactions. Additionally, an alternating direction method of multipliers (ADMM) framework is developed for penalized quadratic regression with general penalties, including both single and hybrid penalty functions. The approach simplifies the calculations to basic matrix-based operations, making it appealing in terms of both memory storage and computational complexity for solving penalized quadratic regressions in high-dimensional settings.},
  archive      = {J_CSDA},
  author       = {Cheng Wang and Haozhe Chen and Binyan Jiang},
  doi          = {10.1016/j.csda.2023.107904},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107904},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {HiQR: An efficient algorithm for high-dimensional quadratic regression with penalties},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-sample test of stochastic block models. <em>CSDA</em>,
<em>192</em>, 107903. (<a
href="https://doi.org/10.1016/j.csda.2023.107903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of two-sample test of large networks with community structures. A test statistic is proposed based on the maximum entry of the difference between the two adjacency matrices . Asymptotic null distribution is derived, and the asymptotic power guarantee against the alternative hypothesis is provided. The simulations and real data examples show that the proposed test statistic performs well.},
  archive      = {J_CSDA},
  author       = {Qianyong Wu and Jiang Hu},
  doi          = {10.1016/j.csda.2023.107903},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107903},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two-sample test of stochastic block models},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of l0 norm penalized models: A statistical
treatment. <em>CSDA</em>, <em>192</em>, 107902. (<a
href="https://doi.org/10.1016/j.csda.2023.107902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fitting penalized models for the purpose of merging the estimation and model selection problem has become commonplace in statistical practice. Of the various regularization strategies that can be leveraged to this end, the use of the l 0 l0 norm to penalize parameter estimation poses the most daunting model fitting task. In fact, this particular strategy requires an end user to solve a non-convex NP-hard optimization problem irregardless of the underlying data model. For this reason, the use of the l 0 l0 norm as a regularization strategy has been woefully under utilized. To obviate this difficulty, a strategy to solve such problems that is generally accessible by the statistical community is developed. The approach can be adopted to solve l 0 l0 norm penalized problems across a very broad class of models, can be implemented using existing software, and is computationally efficient. The performance of the method is demonstrated through in-depth numerical experiments and through using it to analyze several prototypical data sets.},
  archive      = {J_CSDA},
  author       = {Yuan Yang and Christopher S. McMahan and Yu-Bo Wang and Yuyuan Ouyang},
  doi          = {10.1016/j.csda.2023.107902},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107902},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of l0 norm penalized models: A statistical treatment},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-block alternating direction method of multipliers for
ultrahigh dimensional quantile fused regression. <em>CSDA</em>,
<em>192</em>, 107901. (<a
href="https://doi.org/10.1016/j.csda.2023.107901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a quantile fused LASSO regression model that combines quantile regression loss with the fused LASSO penalty. Intuitively, this model offers robustness to outliers, thanks to the quantile regression , while also effectively recovering sparse and block coefficients through the fused LASSO penalty. To adapt our proposed method for ultrahigh dimensional datasets, we introduce an iterative algorithm based on the multi-block alternating direction method of multipliers (ADMM). Moreover, we demonstrate the global convergence of the algorithm and derive comparable convergence rates. Importantly, our ADMM algorithm can be easily applied to solve various existing fused LASSO models. In terms of theoretical analysis, we establish that the quantile fused LASSO can achieve near oracle properties with a practical penalty parameter, and additionally, it possesses a sure screening property under a wide class of error distributions. The numerical experimental results support our claims, showing that the quantile fused LASSO outperforms existing fused regression models in robustness, particularly under heavy-tailed distributions.},
  archive      = {J_CSDA},
  author       = {Xiaofei Wu and Hao Ming and Zhimin Zhang and Zhenyu Cui},
  doi          = {10.1016/j.csda.2023.107901},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107901},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multi-block alternating direction method of multipliers for ultrahigh dimensional quantile fused regression},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous inference and uniform test for eigensystems of
functional data. <em>CSDA</em>, <em>192</em>, 107900. (<a
href="https://doi.org/10.1016/j.csda.2023.107900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The asymptotically correct confidence interval (CI) and simultaneous confidence band (SCB) of any individual eigenvalue and eigenfunction are constructed under dense functional data through B-spline smoothing. Besides, uniform inference procedures for eigensystems with a diverging number of components are novelly developed. The proposed estimators for functional eigensystems employ “oracle” efficiency up to order n n , which means they are asymptotically indistinguishable from the estimators conducted by completely observed trajectories , and enjoy computational efficiency with much more convenient spectrum decomposition forms. Furthermore, an extension to two-sample problems is also investigated. Numerical simulation results strongly corroborate the asymptotic theory . Real data analysis for ElectroEncephalogram (EEG) data illustrates applicability of the developed methods.},
  archive      = {J_CSDA},
  author       = {Leheng Cai and Qirui Hu},
  doi          = {10.1016/j.csda.2023.107900},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107900},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Simultaneous inference and uniform test for eigensystems of functional data},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A fast trans-lasso algorithm with penalized weighted score
function. <em>CSDA</em>, <em>192</em>, 107899. (<a
href="https://doi.org/10.1016/j.csda.2023.107899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient transfer learning algorithm for high-dimensional sparse logistic regression models is proposed using penalized weighted score function based on square root Lasso, which intends to prespecify the tuning parameter. Three different choices of the tuning parameter are considered in the case of fixed design matrix . With a novel weight construction, the estimator of the regression vector is showed to be consistent when the inequality with respect to the Karush-Kuhn-Tucker (KKT) optimality conditions holds with high probability and the sparsity assumption for the regression vectors is required. There is information from source data to fit target data such that with high probability tending to 1- α , which is sharper than the corresponding probability bound without using the auxiliary samples, the KKT optimality conditions hold for the asymptotic choice. To detect which sources are transferable, an efficient data-driven method is proposed, which helps avoid negative transfer in practice. Simulation studies are carried out to demonstrate the numerical performance of the proposed procedure and their superiority over some existing methods. The procedures are also illustrated by analyzing the China Migrants Dynamic Survey dataset with binary outcomes concerning the associations among different provinces.},
  archive      = {J_CSDA},
  author       = {Xianqiu Fan and Jun Cheng and Hailing Wang and Bin Zhang and Zhenzhen Chen},
  doi          = {10.1016/j.csda.2023.107899},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107899},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A fast trans-lasso algorithm with penalized weighted score function},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and robust optimal design for quantile regression
based on linear programming. <em>CSDA</em>, <em>192</em>, 107892. (<a
href="https://doi.org/10.1016/j.csda.2023.107892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When informing decisions with experimental data , it is often necessary to quantify the distribution tails of uncertain system responses using limited data. To maximize the information content of the data, one is naturally led to use experimental design . However, common design techniques minimize global statistics such as the average estimation or prediction variance. Novel methods for optimal experimental design that target distribution tails are developed. To achieve this, pre-asymptotic estimates of the data uncertainty are produced via an upper bound on a prescribed quantile , computed using quantile regression. Two optimal design problems are formulated: (i) Minimize the variance of the upper bound; and (ii) Minimize the Conditional Value-at-Risk of the upper bound. Additionally, each design problem is augmented with an added cardinality constraint to bound the number of experiments. These optimal design problems are reduced to continuous and mixed-integer linear programming problems . Consequently, the proposed methods are extremely efficient, even when applied to large datasets. The application of the proposed design formulation is demonstrated through a sensor placement problem in direct field acoustic testing.},
  archive      = {J_CSDA},
  author       = {Cheng Peng and Drew P. Kouri and Stan Uryasev},
  doi          = {10.1016/j.csda.2023.107892},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107892},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient and robust optimal design for quantile regression based on linear programming},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous confidence region of an embedded
one-dimensional curve in multi-dimensional space. <em>CSDA</em>,
<em>192</em>, 107891. (<a
href="https://doi.org/10.1016/j.csda.2023.107891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the simultaneous confidence region of a one-dimensional curve embedded in multi-dimensional space. Local linear regression is applied component-wise to each variable in multi-dimensional data, which yields an estimator of the one-dimensional curve. A simultaneous confidence region of the curve is proposed based on this estimator and theoretical results for the estimator and the region are developed under some reasonable assumptions. Practically efficient algorithms to determine the thickness of the region are also addressed. The effectiveness of the region is investigated through simulation studies and applications to artificial and real datasets, which reveal that the proposed simultaneous confidence region works well.},
  archive      = {J_CSDA},
  author       = {Hiroya Yamazoe and Kanta Naito},
  doi          = {10.1016/j.csda.2023.107891},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107891},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Simultaneous confidence region of an embedded one-dimensional curve in multi-dimensional space},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Burn-in selection in simulating stationary time series.
<em>CSDA</em>, <em>192</em>, 107886. (<a
href="https://doi.org/10.1016/j.csda.2023.107886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many time series models are defined in a recursive manner, which prohibits exact simulations. In practice, one appeals to simulating a long time series and discarding a large number of initial simulated observations, known as the burn-in. For autoregressive models where the dependence decays exponentially fast, the choice of the burn-in is not critical. However, for long-memory time series where the dependence from the remote past is strong, it is not clear how to select the burn-in number. By combining several samplers with randomized burn-in numbers, a method for exactly simulating the expectation of a statistic computed from a time series is developed. Moreover, with some suitably chosen statistics, the exact simulation method can be applied to quantify the effect of burn-in numbers on the simulated sample. Simulation studies are conducted to provide some practical guidances for burn-in selections.},
  archive      = {J_CSDA},
  author       = {Yuanbo Li and Chu Kin Chan and Chun Yip Yau and Wai Leong Ng and Henry Lam},
  doi          = {10.1016/j.csda.2023.107886},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107886},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Burn-in selection in simulating stationary time series},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variable selection for high-dimensional incomplete data.
<em>CSDA</em>, <em>192</em>, 107877. (<a
href="https://doi.org/10.1016/j.csda.2023.107877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression analysis is often affected by high dimensionality, severe multicollinearity, and a large proportion of missing data. These problems may mask important relationships and even lead to biased conclusions. This paper proposes a novel computationally efficient method that integrates data imputation and variable selection to address these issues. More specifically, the proposed method incorporates a new multiple imputation algorithm based on matrix completion (Multiple Accelerated Inexact Soft-Impute), a more stable and accurate new randomized lasso method (Hybrid Random Lasso), and a consistent method to integrate a variable selection method with multiple imputation. Compared to existing methodologies, the proposed approach offers greater accuracy and consistency through mechanisms that enhances robustness against different missing data patterns and sampling variations. The method is applied to analyze the Asian American minority subgroup in the 2017 National Youth Risk Behavior Survey, where key risk factors related to the intention for suicide among Asian Americans are studied. Through simulations and real data analyses on various regression and classification settings, the proposed method demonstrates enhanced accuracy, consistency, and efficiency in both variable selection and prediction.},
  archive      = {J_CSDA},
  author       = {Lixing Liang and Yipeng Zhuang and Philip L.H. Yu},
  doi          = {10.1016/j.csda.2023.107877},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {4},
  pages        = {107877},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variable selection for high-dimensional incomplete data},
  volume       = {192},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One point per cluster spatially balanced sampling.
<em>CSDA</em>, <em>191</em>, 107888. (<a
href="https://doi.org/10.1016/j.csda.2023.107888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spatial sampling design determines where sample locations are placed in a study area so that population parameters can be estimated with relatively high precision. Spatially balanced designs have good spatial spread and give precise results for commonly used estimators when surveying natural resources . A new design is proposed which draws spatially balanced samples from stratified and unstratified populations. The method is two-fold. First, the population is partitioned into n compact geographic clusters. Then, a one point per cluster sample is drawn using a linear assignment strategy that optimises the spatial spread of the sample. Numerical results on several simulated populations show that the method generates well-spread samples and compares favourably with existing designs. An example application is also provided, where soil organic matter concentrations are estimated over a study area in Voorst, Netherlands.},
  archive      = {J_CSDA},
  author       = {Blair Robertson and Chris Price},
  doi          = {10.1016/j.csda.2023.107888},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107888},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {One point per cluster spatially balanced sampling},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional-mean multiplicative operator models for count
time series. <em>CSDA</em>, <em>191</em>, 107885. (<a
href="https://doi.org/10.1016/j.csda.2023.107885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplicative error models (MEMs) are commonly used for real-valued time series, but they cannot be applied to discrete-valued count time series as the involved multiplication would not preserve the integer nature of the data. Thus, the concept of a multiplicative operator for counts is proposed (as well as several specific instances thereof), which are then used to develop a kind of MEMs for count time series (CMEMs). If equipped with a linear conditional mean, the resulting CMEMs are closely related to the class of so-called integer-valued generalized autoregressive conditional heteroscedasticity (INGARCH) models and might be used as a semi-parametric extension thereof. Important stochastic properties of different types of INGARCH-CMEM as well as relevant estimation approaches are derived, namely types of quasi-maximum likelihood and weighted least squares estimation. The performance and application are demonstrated with simulations as well as with two real-world data examples.},
  archive      = {J_CSDA},
  author       = {Christian H. Weiß and Fukang Zhu},
  doi          = {10.1016/j.csda.2023.107885},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107885},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional-mean multiplicative operator models for count time series},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating machine learning and bayesian nonparametrics for
flexible modeling of point pattern data. <em>CSDA</em>, <em>191</em>,
107875. (<a href="https://doi.org/10.1016/j.csda.2023.107875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two common approaches to analyze point pattern (location-only) data are mixture models and log-Gaussian Cox processes. The former provides a flexible model for the intensity surface at the expense of no covariate effect estimates while the latter estimates covariate effects at the expense of computation. A bridge is built between these two methods that leverages the strengths of both approaches. Namely, Bayesian nonparametrics are first used to flexibly model the intensity surface. The posterior draws of the fitted intensity surface are then transformed into the equivalent representation under the log-Gaussian Cox process approach. Using principles of machine learning , estimates of covariate effects are obtained. The proposed two-step approach results in accurate estimates of parameters, with proper uncertainty quantification , which is illustrated with real and simulated examples.},
  archive      = {J_CSDA},
  author       = {Matthew J. Heaton and Benjamin K. Dahl and Caleb Dayley and Richard L. Warr and Philip White},
  doi          = {10.1016/j.csda.2023.107875},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107875},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Integrating machine learning and bayesian nonparametrics for flexible modeling of point pattern data},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian nonparametric erlang mixture modeling for survival
analysis. <em>CSDA</em>, <em>191</em>, 107874. (<a
href="https://doi.org/10.1016/j.csda.2023.107874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of a flexible Erlang mixture model for survival analysis is introduced. The model for the survival density is built from a structured mixture of Erlang densities, mixing on the integer shape parameter with a common scale parameter . The mixture weights are constructed through increments of a distribution function on the positive real line , which is assigned a Dirichlet process prior. The model has a relatively simple structure, balancing flexibility with efficient posterior computation. Moreover, it implies a mixture representation for the hazard function that involves time-dependent mixture weights, thus offering a general approach to hazard estimation. Extension of the model is made to accommodate survival responses corresponding to multiple experimental groups, using a dependent Dirichlet process prior for the group-specific distributions that define the mixture weights. Model properties, prior specification, and posterior simulation are discussed, and the methodology is illustrated with synthetic and real data examples.},
  archive      = {J_CSDA},
  author       = {Yunzhe Li and Juhee Lee and Athanasios Kottas},
  doi          = {10.1016/j.csda.2023.107874},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107874},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian nonparametric erlang mixture modeling for survival analysis},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric quantile scalar-on-image regression.
<em>CSDA</em>, <em>191</em>, 107873. (<a
href="https://doi.org/10.1016/j.csda.2023.107873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quantile scalar-on-image regression model is developed to comprehensively study the relationship between cognitive decline and various clinical covariates and imaging factors. As a motivating example, the high-dimensional brain imaging data from the research on Alzheimer&#39;s disease are considered predictors of patients&#39; cognitive decline. A Bayesian nonparametric model is proposed to handle the complex spatially distributed imaging data, where the coefficient function is assumed to be a latent Gaussian process . A soft-thresholding operator is introduced to capture the sparse structure of the regression coefficients . Utilizing kernel basis functions to approximate the latent Gaussian process facilitates easy-to-implement computation and consistent estimation. Inference is performed within the Bayesian framework , using an efficient Markov chain Monte Carlo algorithm. The proposed method is compared with the functional principal component analysis method in simulations and applied to a study of Alzheimer&#39;s disease.},
  archive      = {J_CSDA},
  author       = {Chuchu Wang and Xinyuan Song},
  doi          = {10.1016/j.csda.2023.107873},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107873},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric quantile scalar-on-image regression},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed debiased estimation of high-dimensional
partially linear models with jumps. <em>CSDA</em>, <em>191</em>, 107857.
(<a href="https://doi.org/10.1016/j.csda.2023.107857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the estimations of both parameter vector and nonparametric component in a high-dimensional partially linear model with jumps within the framework of divide and conquer strategy. We find that a three-stage estimation procedure works well in this setting. Applying the lasso penalty and projected spline approximation , first a profiled estimator for the linear part and a projected spline estimator for the nonparametric part are obtained on each local machine. In the second stage, an efficient jump detection algorithm is developed to obtain the new knot sequence, and then based on this, the estimate of the nonparametric function is obtained and averaged after plugging in the linear part estimate on each local machine. The aggregated estimate of the nonparametric function is then computed by pooling these local estimates. In the third stage, a debiased lasso estimator is averaged to obtain a distributed debiased estimator of the linear part after plugging in the aggregated estimate of nonparametric function. Asymptotic properties of resultant estimators are established under some mild assumptions. Some simulations are conducted to illustrate the empirical performances of our proposed method.},
  archive      = {J_CSDA},
  author       = {Yan-Yong Zhao and Yuchun Zhang and Yuan Liu and Noriszura Ismail},
  doi          = {10.1016/j.csda.2023.107857},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {3},
  pages        = {107857},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Distributed debiased estimation of high-dimensional partially linear models with jumps},
  volume       = {191},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the efficacy of higher-order spectral clustering under
weighted stochastic block models. <em>CSDA</em>, <em>190</em>, 107872.
(<a href="https://doi.org/10.1016/j.csda.2023.107872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order structures of networks, namely, small subgraphs of networks (also called network motifs), are widely known to be crucial and essential to the organization of networks. Several works have studied the community detection problem–a fundamental problem in network analysis at the level of motifs. In particular, the higher-order spectral clustering has been developed, where the notion of motif adjacency matrix is introduced as the algorithm&#39;s input. However, how the higher-order spectral clustering works and when it performs better than its edge-based counterpart remain largely unknown. To elucidate these problems, the higher-order spectral clustering is investigated from a statistical perspective. The clustering performance of the higher-order spectral clustering is theoretically studied under a weighted stochastic block model, and the resulting bounds are compared with the corresponding results of the edge-based spectral clustering. The upper bounds and simulations show that when the network is dense and the edge weights have a weak signal, higher-order spectral clustering can lead to a performance gain in clustering. Real data experiments also corroborate the merits of higher-order spectral clustering.},
  archive      = {J_CSDA},
  author       = {Xiao Guo and Hai Zhang and Xiangyu Chang},
  doi          = {10.1016/j.csda.2023.107872},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107872},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On the efficacy of higher-order spectral clustering under weighted stochastic block models},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calibrated regression estimation using empirical likelihood
under data fusion. <em>CSDA</em>, <em>190</em>, 107871. (<a
href="https://doi.org/10.1016/j.csda.2023.107871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analysis based on information from different sources, typically known as the data fusion problem, is common in economic and biomedical studies . An interesting question concerns the regression of an outcome variable on certain covariates when combining two distinct datasets. These datasets consist of a primary sample containing the outcome and a subset of the covariates, and a supplemental sample comprising information only on the full set of covariates. Previous methods have proposed doubly robust estimation procedures that employ a single propensity score model for the data fusion process and a single imputation model for the covariates available only in the supplemental dataset. However, it may be questionable to assume that either model is correctly specified due to an unknown data generating process. To address this issue, an empirical likelihood based approach that calibrates multiple propensity scores and imputation models is introduced. The resulting estimator is consistent when any one of the models is correctly specified and is robust against extreme values of the fitted propensity scores. The asymptotic normality property and the estimation efficiency are also discussed. Simulation studies show that the proposed estimator has substantial advantages over existing estimators, and an assembled U.S. household expenditure data example is used for illustration.},
  archive      = {J_CSDA},
  author       = {Wei Li and Shanshan Luo and Wangli Xu},
  doi          = {10.1016/j.csda.2023.107871},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107871},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Calibrated regression estimation using empirical likelihood under data fusion},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Additive partially linear model for pooled biomonitoring
data. <em>CSDA</em>, <em>190</em>, 107862. (<a
href="https://doi.org/10.1016/j.csda.2023.107862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human biomonitoring involves monitoring human health by measuring the accumulation of harmful chemicals, typically in specimens like blood samples. The high cost of chemical analysis has led researchers to adopt a cost-effective approach. This approach physically combines specimens and subsequently analyzes the concentration of toxic substances within the merged pools. Consequently, there arises a need for innovative regression techniques to effectively interpret these aggregated measurements. To address this need, a new regression framework is proposed by extending the additive partially linear model (APLM) to accommodate the pooling context. The APLM is well-known for its versatility in capturing the complex association between outcomes and covariates , which is particularly valuable in assessing the complex interplay between chemical bioaccumulation and potential risk factors. Consistent estimators of the APLM are obtained through an iterative process that disaggregates information from the pooled observations. The performance is evaluated through simulations and an environmental health study focused on brominated flame retardants using data from the National Health and Nutrition Examination Survey.},
  archive      = {J_CSDA},
  author       = {Xichen Mou and Dewei Wang},
  doi          = {10.1016/j.csda.2023.107862},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107862},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Additive partially linear model for pooled biomonitoring data},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast same-step forecast in SUTSE model and its theoretical
properties. <em>CSDA</em>, <em>190</em>, 107861. (<a
href="https://doi.org/10.1016/j.csda.2023.107861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of forecasting multivariate time series by a Seemingly Unrelated Time Series Equations (SUTSE) model is considered. The SUTSE model usually assumes that error variables are correlated. A crucial issue is that the model estimation requires heavy computational loads because of a large matrix computation , especially for high-dimensional data. To alleviate the computational issue, a two-stage procedure for forecasting is constructed. First, Kalman filtering is performed as if the error variables are uncorrelated; that is, univariate time-series analyses are conducted separately to avoid a large matrix computation . Next, the forecast value is computed by using a distribution of forecast error. The proposed algorithm is much faster than the ordinary SUTSE model because a large matrix computation is not required. Some theoretical properties of the proposed estimator are presented, and Monte Carlo simulation is performed to investigate the effectiveness of the proposed method. The usefulness of the proposed procedure is illustrated through a bus congestion data application.},
  archive      = {J_CSDA},
  author       = {Wataru Yoshida and Kei Hirose},
  doi          = {10.1016/j.csda.2023.107861},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107861},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fast same-step forecast in SUTSE model and its theoretical properties},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variables selection using l0 penalty. <em>CSDA</em>,
<em>190</em>, 107860. (<a
href="https://doi.org/10.1016/j.csda.2023.107860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of a tuning parameter by the generalized information criterion (GIC) is considered an important issue in variable selection. It is shown that the GIC and the L 0 L0 penalized objective functions are equivalent, leading to a new L 0 L0 penalized maximum likelihood method for high-dimensional generalized linear models in this article. Based on the technique of the well-known discrete optimization problem in theoretical computer science , a two-step algorithm for local solutions is proposed. The first step optimizes the L 0 L0 penalized objective function under a given model size, where only a maximum likelihood algorithm is needed. The second step optimizes the L 0 L0 penalized objective function under a candidate set of model sizes, where only the GIC is needed. As the tuning parameter can be fixed, the selection of the tuning parameter can be ignored in the proposed method. The theoretical study shows that the algorithm is polynomial and any resulting local solution is consistent. Thus, it is not necessary to use the global solution in practice. The numerical studies show that the proposed method outperforms its competitors in general.},
  archive      = {J_CSDA},
  author       = {Tonglin Zhang},
  doi          = {10.1016/j.csda.2023.107860},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107860},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variables selection using l0 penalty},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Laplace approximated quasi-likelihood method for
heteroscedastic survival data. <em>CSDA</em>, <em>190</em>, 107859. (<a
href="https://doi.org/10.1016/j.csda.2023.107859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classical accelerated failure time model is the major linear model for right censored survival data. It requires the survival data to exhibit homoscedasticity of variance and excludes heteroscedastic survival data that are often seen in practical applications. The least squares method for the classical accelerated failure time model has been extended to accommodate the heteroscedasticity in survival data. However, the estimating equations are discrete and hence they are time consuming and may not be feasible for large datasets. This paper proposes a Laplace approximated quasi-likelihood method with a continuous estimating equation. It utilizes the Laplace approximation to approximate the survival function in the quasi-likelihood, in which the variance function is approximated by a spline function. Then it shows the asymptotic distribution of the Laplace approximated estimator, its estimation bias and the formula for confidence interval estimation for the parameter of interest. The finite sample performance of the proposed approach is evaluated through simulation studies and follows real data examples for illustration.},
  archive      = {J_CSDA},
  author       = {Lili Yu and Yichuan Zhao},
  doi          = {10.1016/j.csda.2023.107859},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107859},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Laplace approximated quasi-likelihood method for heteroscedastic survival data},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GP-BART: A novel bayesian additive regression trees approach
using gaussian processes. <em>CSDA</em>, <em>190</em>, 107858. (<a
href="https://doi.org/10.1016/j.csda.2023.107858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Bayesian additive regression trees (BART) model is an ensemble method extensively and successfully used in regression tasks due to its consistently strong predictive performance and its ability to quantify uncertainty. BART combines “weak” tree models through a set of shrinkage priors, whereby each tree explains a small portion of the variability in the data. However, the lack of smoothness and the absence of an explicit covariance structure over the observations in standard BART can yield poor performance in cases where such assumptions would be necessary. The Gaussian processes Bayesian additive regression trees (GP-BART) model is an extension of BART which addresses this limitation by assuming Gaussian process (GP) priors for the predictions of each terminal node among all trees. The model&#39;s effectiveness is demonstrated through applications to simulated and real-world data, surpassing the performance of traditional modelling approaches in various scenarios.},
  archive      = {J_CSDA},
  author       = {Mateus Maia and Keefe Murphy and Andrew C. Parnell},
  doi          = {10.1016/j.csda.2023.107858},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107858},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {GP-BART: A novel bayesian additive regression trees approach using gaussian processes},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting change structures of nonparametric regressions.
<em>CSDA</em>, <em>190</em>, 107856. (<a
href="https://doi.org/10.1016/j.csda.2023.107856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates detecting change points of general nonparametric regression functions by introducing a novel criterion. It is based on the moving sums of conditional expectation to avoid both computationally expensive algorithms, exhaustive search methods need, and false positives hypothesis testing-based approaches encounter. This new criterion can simultaneously and consistently, in a certain sense, detect multiple change points and their locations even when, as the sample size goes to infinity, the number of changes grows up to infinity, and some changes tend to zero. Further, because of its visualization nature, in practice, the locations can be relatively more easily identified, by plotting its signal statistic , than existing methods in the literature. Numerical studies are conducted to examine its performance in finite sample scenarios, and a real data example is analyzed for illustration.},
  archive      = {J_CSDA},
  author       = {Wenbiao Zhao and Lixing Zhu},
  doi          = {10.1016/j.csda.2023.107856},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107856},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Detecting change structures of nonparametric regressions},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An RIHT statistic for testing the equality of several
high-dimensional mean vectors under homoskedasticity. <em>CSDA</em>,
<em>190</em>, 107855. (<a
href="https://doi.org/10.1016/j.csda.2023.107855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the problem of testing the equality of several mean vectors is considered under the homoskedasticity in a high-dimensional setting. A ridgelized Hotelling&#39;s T 2 T2 test (RIHT) is developed and the asymptotic distributions are derived. By requiring only the conditions on the first four moments of the underlying distribution, the RIHT test can be used to test the mean vector free of population distributions under both p ≥ n p≥n and p &lt; n p&amp;lt;n and improve the power of the classic Hotelling&#39;s T 2 T2 test. The innovations of the proposed statistic include the following: (1) the RIHT statistic is derived in accordance with a penalized likelihood ratio test ; (2) the exact four-moment theorem of the RIHT test makes it possible to test data with an arbitrary distribution; and (3) the proposed statistic is less sensitive to highly correlated data from simulations due to the penalty imposed on concentration matrix. Simulations and real data applications show that the RIHT test performs well and is more powerful than alternatives.},
  archive      = {J_CSDA},
  author       = {Qiuyan Zhang and Chen Wang and Baoxue Zhang and Hu Yang},
  doi          = {10.1016/j.csda.2023.107855},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107855},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An RIHT statistic for testing the equality of several high-dimensional mean vectors under homoskedasticity},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recursive ridge regression using second-order stochastic
algorithms. <em>CSDA</em>, <em>190</em>, 107854. (<a
href="https://doi.org/10.1016/j.csda.2023.107854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recursive second-order stochastic algorithms are presented for solving ridge regression problems in the linear and binary logistic case. The proposed algorithms allow to update the estimates of ridge solution when the data arrive in continuous flow. Some guarantees on the almost sure behavior of the proposed algorithms are established. Numerical experiments on simulated and real-world data show the advantages of our algorithms compared to alternative methods.},
  archive      = {J_CSDA},
  author       = {Antoine Godichon-Baggioni and Wei Lu and Bruno Portier},
  doi          = {10.1016/j.csda.2023.107854},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107854},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Recursive ridge regression using second-order stochastic algorithms},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hybrid exact-approximate design approach for sparse
functional data. <em>CSDA</em>, <em>190</em>, 107850. (<a
href="https://doi.org/10.1016/j.csda.2023.107850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal designs for sparse functional data under the functional empirical component (FEC) settings are studied. This design issue has some unique features, making it different from classical design problems. To efficiently obtain optimal exact and approximate designs, new computational methods and useful theoretical results are developed, and a hybrid exact-approximate design approach is proposed. The proposed methods are demonstrated to be efficient via simulation studies and a real example.},
  archive      = {J_CSDA},
  author       = {Ming-Hung Kao and Ping-Han Huang},
  doi          = {10.1016/j.csda.2023.107850},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {2},
  pages        = {107850},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Hybrid exact-approximate design approach for sparse functional data},
  volume       = {190},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probability of default estimation in credit risk using
mixture cure models. <em>CSDA</em>, <em>189</em>, 107853. (<a
href="https://doi.org/10.1016/j.csda.2023.107853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An estimator of the probability of default (PD) in credit risk is proposed. It is derived from a nonparametric conditional survival function estimator based on cure models. Asymptotic expressions for the bias and the variance, as well as the asymptotic normality of the proposed estimator are presented. A simulation study shows the performance of the nonparametric estimator compared with Beran&#39;s PD estimator and other semiparametric methods. Finally, an empirical study based on modified real data illustrates the practical behaviour.},
  archive      = {J_CSDA},
  author       = {Rebeca Peláez and Ingrid Van Keilegom and Ricardo Cao and Juan M. Vilar},
  doi          = {10.1016/j.csda.2023.107853},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107853},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Probability of default estimation in credit risk using mixture cure models},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Standard error estimates in hierarchical generalized linear
models. <em>CSDA</em>, <em>189</em>, 107852. (<a
href="https://doi.org/10.1016/j.csda.2023.107852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical generalized linear models are often used to fit random effects models. However, attention is mostly paid to the estimation of fixed unknown parameters and inference for latent random effects. In contrast, standard error estimators receive less attention than they should be. Currently, the standard error estimators are based on various approximations , even when the mean parameters may be estimated from a higher-order approximation of the likelihood and the dispersion parameters are estimated by restricted maximum likelihood . Existing standard error estimation procedures are reviewed. A numerical illustration shows that the current standard errors are not necessarily accurate. Alternative standard errors are also proposed. In particular, a sandwich estimator that accounts for the dependence between the mean parameters and the dispersion parameters greatly improve the current standard errors.},
  archive      = {J_CSDA},
  author       = {Shaobo Jin and Youngjo Lee},
  doi          = {10.1016/j.csda.2023.107852},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107852},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Standard error estimates in hierarchical generalized linear models},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing the parametric form of the conditional variance in
regressions based on distance covariance. <em>CSDA</em>, <em>189</em>,
107851. (<a href="https://doi.org/10.1016/j.csda.2023.107851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new test based on distance covariance is proposed for testing the parametric form of the conditional variance in parametric and nonparametric regression models. Inherit from the nice properties of distance covariance, the new test is very easy to implement in practice and less effected by the dimensionality of covariates . The asymptotic properties of the test statistic are investigated under the null and alternative hypotheses . The proposed test is consistent against any alternative hypothesis and can detect some classes of local alternative hypotheses converging to the null hypothesis at the parametric rate in both the parametric and nonparametric settings. As the limiting null distribution of the test statistic is intractable, a smooth residual bootstrap is proposed to approximate the limiting null distribution. Simulation studies are conducted to assess the finite sample performance of the proposed test. A real data set is also analyzed for illustration.},
  archive      = {J_CSDA},
  author       = {Yue Hu and Haiqi Li and Falong Tan},
  doi          = {10.1016/j.csda.2023.107851},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107851},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Testing the parametric form of the conditional variance in regressions based on distance covariance},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of banded time-varying precision matrix based on
SCAD and group lasso. <em>CSDA</em>, <em>189</em>, 107849. (<a
href="https://doi.org/10.1016/j.csda.2023.107849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new banded time-varying precision matrix estimator is proposed for high-dimensional time series. The estimator utilizes the modified Cholesky decomposition , and the two factors in the decomposition are dynamically estimated by applying the GARCH model to the innovation variance and the Kalman filter on the Cholesky factor . The SCAD penalty and group lasso penalty are imposed on the Cholesky factor to estimate the banded structure. An efficient algorithm based on the alternating direction method of multipliers (ADMM), local linear approximation (LLA), and blockwise coordinate descent (BCD) algorithms is developed. The convergence of the algorithm is proven theoretically, and the estimator is guaranteed to be banded. Simulation and real-data analysis demonstrate the favorable performance of the proposed algorithm compared to other methods.},
  archive      = {J_CSDA},
  author       = {Xiaonan Zhu and Yu Chen and Jie Hu},
  doi          = {10.1016/j.csda.2023.107849},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107849},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of banded time-varying precision matrix based on SCAD and group lasso},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modelling of the body and tail of bivariate data.
<em>CSDA</em>, <em>189</em>, 107841. (<a
href="https://doi.org/10.1016/j.csda.2023.107841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In situations where both extreme and non-extreme data are of interest, modelling the whole data set accurately is important. In a univariate framework, modelling the bulk and tail of a distribution has been extensively studied before. However, when more than one variable is of concern, models that aim specifically at capturing both regions correctly are scarce in the literature. A dependence model that blends two copulas with different characteristics over the whole range of the data support is proposed. One copula is tailored to the bulk and the other to the tail, with a dynamic weighting function employed to transition smoothly between them. Tail dependence properties are investigated numerically and simulation is used to confirm that the blended model is sufficiently flexible to capture a wide variety of structures. The model is applied to study the dependence between temperature and ozone concentration at two sites in the UK and compared with a single copula fit. The proposed model provides a better, more flexible, fit to the data, and is also capable of capturing complex dependence structures .},
  archive      = {J_CSDA},
  author       = {L.M. André and J.L. Wadsworth and A. O&#39;Hagan},
  doi          = {10.1016/j.csda.2023.107841},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107841},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint modelling of the body and tail of bivariate data},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Likelihood-type confidence regions for optimal sensitivity
and specificity of a diagnostic test. <em>CSDA</em>, <em>189</em>,
107840. (<a href="https://doi.org/10.1016/j.csda.2023.107840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New methods are proposed that provide approximate joint confidence regions for the optimal sensitivity and specificity of a diagnostic test, i.e., sensitivity and specificity corresponding to the optimal cutpoint as defined by the Youden index criterion. Such methods are semi-parametric or non-parametric and attempt to overcome the limitations of alternative approaches. The proposed methods are based on empirical likelihood pivots, giving rise to likelihood-type regions with no predetermined constraints on the shape and automatically range-respecting. The proposal covers three situations: the binormal model, the binormal model after the use of Box-Cox transformations and the fully non-parametric model. In the second case, it is also shown how to use two different transformations, for the healthy and the diseased subjects. The finite sample behaviour of our methods is investigated using simulation experiments. The simulation results also show the advantages offered by our methods when compared with existing competitors. Illustrative examples, involving three real datasets, are also provided.},
  archive      = {J_CSDA},
  author       = {Gianfranco Adimari and Duc-Khanh To and Monica Chiogna and Francesca Scatozza and Antonio Facchiano},
  doi          = {10.1016/j.csda.2023.107840},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107840},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Likelihood-type confidence regions for optimal sensitivity and specificity of a diagnostic test},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Significance test for semiparametric conditional average
treatment effects and other structural functions. <em>CSDA</em>,
<em>189</em>, 107839. (<a
href="https://doi.org/10.1016/j.csda.2023.107839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper investigates a hypothesis testing problem concerning the potential additional contributions of other covariates to the structural function, given the known covariates. The structural function is the conditional expectation given covariates in which the response may depend on unknown nuisance functions. It includes classic regression functions and the conditional average treatment effects as illustrative instances. Based on Neyman&#39;s orthogonality condition , the proposed distance-based test exhibits the quasi-oracle property in the sense that the nuisance function asymptotically does not influence on the limiting distributions of the test statistic under both the null and alternatives. This novel test can effectively detect the local alternatives distinct from the null at the fastest possible rate in hypothesis testing . This is particularly noteworthy given the involvement of nonparametric estimation of the conditional expectation . Numerical studies are conducted to examine the performance of the test. In the real data analysis section, the proposed tests are applied to identify significantly explanatory covariates that are associated with AIDS treatment effects, yielding noteworthy insights.},
  archive      = {J_CSDA},
  author       = {Niwen Zhou and Xu Guo and Lixing Zhu},
  doi          = {10.1016/j.csda.2023.107839},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107839},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Significance test for semiparametric conditional average treatment effects and other structural functions},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Full uncertainty analysis for bayesian nonparametric mixture
models. <em>CSDA</em>, <em>189</em>, 107838. (<a
href="https://doi.org/10.1016/j.csda.2023.107838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A full posterior analysis for nonparametric mixture models using Gibbs-type prior distributions is presented. This includes the well known Dirichlet process mixture (DPM) model. The random mixing distribution is removed enabling a simple-to-implement Markov chain Monte Carlo (MCMC) algorithm. The removal procedure takes away some of the posterior uncertainty and how it is replaced forms a novel aspect to the work. The removal, MCMC algorithm and replacement of the uncertainty only require the probabilities of a new or an old value associated with the corresponding Gibbs-type exchangeable sequence. Consequently, no explicit representations of the prior or posterior are required and instead only knowledge of the exchangeable sequence is needed. This allows the implementation of mixture models with full posterior uncertainty, not previously possible, including one introduced by Gnedin. Numerous illustrations are presented, as is an R-package called CopRe which implements the methodology, and other supplemental material.},
  archive      = {J_CSDA},
  author       = {Blake Moya and Stephen G. Walker},
  doi          = {10.1016/j.csda.2023.107838},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107838},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Full uncertainty analysis for bayesian nonparametric mixture models},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic risk score modeling for multiple longitudinal risk
factors and survival. <em>CSDA</em>, <em>189</em>, 107837. (<a
href="https://doi.org/10.1016/j.csda.2023.107837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling disease risk and survival using longitudinal risk factor trajectories is of interest in various clinical scenarios. The capacity to build a prognostic model using the trajectories of multiple longitudinal risk factors, in the presence of potential dependent censoring, would enable more informed, personalized decision making. A dynamic risk score modeling framework is proposed for multiple longitudinal risk factors and survival in the presence of dependent censoring, where both events depend on participants&#39; post-baseline clinical progression and form a competing risks structure. The model requires relatively few random effects regardless of the number of longitudinal risk factors and can therefore accommodate multiple longitudinal risk factors in a parsimonious manner. The proposed method performed satisfactorily in extensive simulation studies. It is further applied to the motivating registry study on pediatric acute liver failure to model death using the trajectories of multiple clinical and biochemical markers. Once established, the model yields an easily calculable longitudinal risk score that can be used for disease monitoring among future patients.},
  archive      = {J_CSDA},
  author       = {Cuihong Zhang and Jing Ning and Jianwen Cai and James E. Squires and Steven H. Belle and Ruosha Li},
  doi          = {10.1016/j.csda.2023.107837},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107837},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dynamic risk score modeling for multiple longitudinal risk factors and survival},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variational bayesian inference for bipartite
mixed-membership stochastic block model with applications to
collaborative filtering. <em>CSDA</em>, <em>189</em>, 107836. (<a
href="https://doi.org/10.1016/j.csda.2023.107836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A network-based method applied to collaborative filtering in recommender systems is introduced in this paper. Specifically, a novel mixed-membership stochastic block model with a conjugate prior from the exponential family is proposed for bipartite networks. The analytical expression of the model is derived, and a variational Bayesian algorithm that is computationally feasible for approximating the untractable posterior distributions is presented. Extensive simulations show that the proposed model provides more accurate inference than competing methods with the presence of outliers. The proposed model is also applied to a MovieLens dataset for a real data application.},
  archive      = {J_CSDA},
  author       = {Jie Liu and Zifeng Ye and Kun Chen and Panpan Zhang},
  doi          = {10.1016/j.csda.2023.107836},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107836},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variational bayesian inference for bipartite mixed-membership stochastic block model with applications to collaborative filtering},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subsampling spectral clustering for stochastic block models
in large-scale networks. <em>CSDA</em>, <em>189</em>, 107835. (<a
href="https://doi.org/10.1016/j.csda.2023.107835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of science and technology has generated large amounts of network data, leading to significant computational challenges for network community detection. A novel subsampling spectral clustering algorithm is proposed to address this issue, which aims to identify community structures in large-scale networks with limited computing resources. The algorithm constructs a subnetwork by simple random subsampling from the entire network, and then extends the existing spectral clustering to the subnetwork to estimate the community labels for entire network nodes. As a result, for large-scale datasets, the method can be realized even using a personal computer. Moreover, the proposed method can be generalized in a parallel way. Theoretically, under the stochastic block model and its extension, the degree-corrected stochastic block model, the theoretical properties of the subsampling spectral clustering method are correspondingly established. Finally, to illustrate and evaluate the proposed method, a number of simulation studies and two real data analyses are conducted.},
  archive      = {J_CSDA},
  author       = {Jiayi Deng and Danyang Huang and Yi Ding and Yingqiu Zhu and Bingyi Jing and Bo Zhang},
  doi          = {10.1016/j.csda.2023.107835},
  journal      = {Computational Statistics &amp; Data Analysis},
  month        = {1},
  pages        = {107835},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Subsampling spectral clustering for stochastic block models in large-scale networks},
  volume       = {189},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
