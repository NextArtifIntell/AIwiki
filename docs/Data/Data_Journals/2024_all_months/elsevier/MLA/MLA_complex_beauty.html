<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mla---91">MLA - 91</h2>
<ul>
<li><details>
<summary>
(2024). Sharing is CAIRing: Characterizing principles and assessing
properties of universal privacy evaluation for synthetic tabular data.
<em>MLA</em>, <em>18</em>, 100608. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing is a necessity for innovative progress in many domains, especially in healthcare. However, the ability to share data is hindered by regulations protecting the privacy of natural persons. Synthetic tabular data provide a promising solution to address data sharing difficulties but does not inherently guarantee privacy. Still, there is a lack of agreement on appropriate methods for assessing the privacy-preserving capabilities of synthetic data, making it difficult to compare results across studies. To the best of our knowledge, this is the first work to identify properties that constitute good universal privacy evaluation metrics for synthetic tabular data. The goal of universally applicable metrics is to enable comparability across studies and to allow non-technical stakeholders to understand how privacy is protected. We identify four principles for the assessment of metrics: Comparability, Applicability, Interpretability, and Representativeness (CAIR). To quantify and rank the degree to which evaluation metrics conform to the CAIR principles, we design a rubric using a scale of 1–4. Each of the four properties is scored on four parameters, yielding 16 total dimensions. We study the applicability and usefulness of the CAIR principles and rubric by assessing a selection of metrics popular in other studies. The results provide granular insights into the strengths and weaknesses of existing metrics that not only rank the metrics but highlight areas of potential improvements. We expect that the CAIR principles will foster agreement among researchers and organizations on which universal privacy evaluation metrics are appropriate for synthetic tabular data.},
  archive      = {J_MLA},
  author       = {Tobias Hyrup and Anton Danholt Lautrup and Arthur Zimek and Peter Schneider-Kamp},
  doi          = {10.1016/j.mlwa.2024.100608},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100608},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Sharing is CAIRing: Characterizing principles and assessing properties of universal privacy evaluation for synthetic tabular data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised machine learning for microbiomics: Bridging the
gap between current and best practices. <em>MLA</em>, <em>18</em>,
100607. (<a href="https://doi.org/10.1016/j.mlwa.2024.100607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) is poised to drive innovations in clinical microbiomics, such as in disease diagnostics and prognostics. However, the successful implementation of ML in these domains necessitates the development of reproducible, interpretable models that meet the rigorous performance standards set by regulatory agencies. This study aims to identify key areas in need of improvement in current ML practices within microbiomics, with a focus on bridging the gap between existing methodologies and the requirements for clinical application. To do so, we analyze 100 peer-reviewed articles from 2021 to 2022. Within this corpus, datasets have a median size of 161.5 samples, with over one-third containing fewer than 100 samples, signaling a high potential for overfitting. Limited demographic data further raises concerns about generalizability and fairness, with 24% of studies omitting participants&#39; country of residence, and attributes like race/ethnicity, education, and income rarely reported (11%, 2%, and 0%, respectively). Methodological issues are also common; for instance, for 86% of studies we could not confidently rule out test set omission and data leakage, suggesting a strong potential for inflated performance estimates across the literature. Reproducibility is a concern, with 78% of studies abstaining from sharing their ML code publicly. Based on this analysis, we provide guidance to avoid common pitfalls that can hinder model performance, generalizability, and trustworthiness. An interactive tutorial on applying ML to microbiomics data accompanies the discussion, to help establish and reinforce best practices within the community.},
  archive      = {J_MLA},
  author       = {Natasha Katherine Dudek and Mariami Chakhvadze and Saba Kobakhidze and Omar Kantidze and Yuriy Gankin},
  doi          = {10.1016/j.mlwa.2024.100607},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100607},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Supervised machine learning for microbiomics: Bridging the gap between current and best practices},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Document layout error rate (DLER) metric to evaluate image
segmentation methods. <em>MLA</em>, <em>18</em>, 100606. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scholarly editions play a crucial role in humanities research, particularly in the study of literature and historical documents. The primary objective of these editions is to reconstruct the original text or provide insights into the author’s intentions. Traditionally, crafting a critical edition required a lifetime of dedication. However, thanks to recent advancements in deep learning and computer vision, modern text recognition tools can now be used to expedite this process. A key part of these tools is document layout analysis (DLA), where image segmentation methods are used to detect different text elements. Most existing DLA solutions have focused on evaluating the accuracy of these methods, often neglecting to study the practical consequences of method selection. In this study, we have developed a new metric, the Document Layout Error Rate (DLER), which evaluates the performance of fine-grained DLA methods within the overall pipeline. This metric helps identify the method with the lowest error rate, thereby minimizing the manual effort required for corrections. We applied this evaluation method to assess four different methods and their efficacy for the DLA task in the context of David Hume’s History of England .},
  archive      = {J_MLA},
  author       = {Ari Vesalainen and Mikko Tolonen and Laura Ruotsalainen},
  doi          = {10.1016/j.mlwa.2024.100606},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100606},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Document layout error rate (DLER) metric to evaluate image segmentation methods},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey on knowledge distillation: Recent advancements.
<em>MLA</em>, <em>18</em>, 100605. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has achieved notable success across academia, medicine, and industry. Its ability to identify complex patterns in large-scale data and to manage millions of parameters has made it highly advantageous. However, deploying deep learning models presents a significant challenge due to their high computational demands. Knowledge distillation (KD) has emerged as a key technique for model compression and efficient knowledge transfer, enabling the deployment of deep learning models on resource-limited devices without compromising performance. This survey examines recent advancements in KD, highlighting key innovations in architectures, training paradigms, and application domains. We categorize contemporary KD methods into traditional approaches, such as response-based, feature-based, and relation-based knowledge distillation, and novel advanced paradigms, including self-distillation, cross-modal distillation, and adversarial distillation strategies. Additionally, we discuss emerging challenges, particularly in the context of distillation under limited data scenarios, privacy-preserving KD, and the interplay with other model compression techniques like quantization. Our survey also explores applications across computer vision, natural language processing, and multimodal tasks, where KD has driven performance improvements and enhanced model compression. This review aims to provide researchers and practitioners with a comprehensive understanding of the state-of-the-art in knowledge distillation, bridging foundational concepts with the latest methodologies and practical implications.},
  archive      = {J_MLA},
  author       = {Amir Moslemi and Anna Briskina and Zubeka Dang and Jason Li},
  doi          = {10.1016/j.mlwa.2024.100605},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100605},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A survey on knowledge distillation: Recent advancements},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Texas rural land market integration: A causal analysis using
machine learning applications. <em>MLA</em>, <em>18</em>, 100604. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Texas rural land markets have several special features that makes it unique from other rural land markets in the United States. In 2021, Texas agricultural land, including buildings, is valued at $299.88 billion, which is almost 10% of the nation&#39;s total agricultural real estate value and 83% of the state&#39;s land is categorized as rural. In addition, due to its size and geologic features, Texas’ diverse landscape creates complex and widely divergent conditions affecting ownership and marketing of the land. Despite this complexity, lack of granular level and reliable transactional data on land sales has prevented thorough investigation into Texas land markets to uncover various interdependencies. Using quarterly transactional land value data from 1966 to 2017, this study uses cutting-edge machine learning algorithms and probabilistic graphical models to uncover causal interaction patterns of different land markets in Texas. The results reveal that Texas rural land markets are interdependent. Current and potential landholders and lenders can use the results from this work to aid strategic decision making. Financial institutions and investment groups could be made aware of the trend of one land market relative to other markets and adjust their holdings accordingly. Landowners may better understand changes in net wealth, which affect their ability to borrow capital and operate efficiently. Moreover, lenders may also benefit from the information to manage collateral and thus maintain the stability of their operation.},
  archive      = {J_MLA},
  author       = {Tian Su and Senarath Dharmasena and David Leatham and Charles Gilliland},
  doi          = {10.1016/j.mlwa.2024.100604},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100604},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Texas rural land market integration: A causal analysis using machine learning applications},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Playing with words: Comparing the vocabulary and lexical
diversity of ChatGPT and humans. <em>MLA</em>, <em>18</em>, 100602. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and conversational tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text, would this affect the language capabilities of readers and also the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical diversity? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical diversity of ChatGPT and humans when performing the same tasks. In more detail, two datasets containing the answers to different types of questions answered by ChatGPT and humans, and a third dataset in which ChatGPT paraphrases sentences and questions are used. The analysis shows that ChatGPT-3.5 tends to use fewer distinct words and lower diversity than humans while ChatGPT-4 has a similar lexical diversity as humans and in some cases even larger. These results are very preliminary and additional datasets and ChatGPT configurations have to be evaluated to extract more general conclusions. Therefore, further research is needed to understand how the use of ChatGPT and more broadly generative AI tools will affect the vocabulary and lexical diversity in different types of text and languages.},
  archive      = {J_MLA},
  author       = {Pedro Reviriego and Javier Conde and Elena Merino-Gómez and Gonzalo Martínez and José Alberto Hernández},
  doi          = {10.1016/j.mlwa.2024.100602},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100602},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Playing with words: Comparing the vocabulary and lexical diversity of ChatGPT and humans},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applications of cluster-based transfer learning in image and
localization tasks. <em>MLA</em>, <em>18</em>, 100601. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning can address the issue of insufficient labels in machine learning. Using knowledge in a labeled domain (source domain) can assist in acquiring and learning knowledge in a domain (target domain) that lacks some or all labels. In this paper, we propose a new cluster-based semi-supervised transfer learning (CBSSTL) under a new assumption that samples in the target domain are unlabeled but contain cluster information. Furthermore, we propose a new transfer learning framework and a method for fine-tuning parameters. We tested and compared the proposed method with other unsupervised and semi-supervised transfer learning methods on well-known image datasets. The experimental results demonstrate the effectiveness of the proposed method. Additionally, we created a localization dataset for transfer learning. Finally, we tested and analyzed the proposed method on this dataset. Its particularly challenging nature makes it difficult for our method to work effectively.},
  archive      = {J_MLA},
  author       = {Liuyi Yang and Patrick Finnerty and Chikara Ohta},
  doi          = {10.1016/j.mlwa.2024.100601},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100601},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Applications of cluster-based transfer learning in image and localization tasks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Review of machine learning applications for defect detection
in composite materials. <em>MLA</em>, <em>18</em>, 100600. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) techniques have shown promising applications in a broad range of topics in engineering, composite materials behavior analysis, and manufacturing. This paper reviews successful ML implementations for defect and damage identification and progression in composites. The focus is on predicting composites&#39; responses under specific loads and environments and optimizing setting and imperfection sensitivity. Discussions and recommendations toward promising ML implementation practices for fruitful interpretable results in the composites’ analysis are provided.},
  archive      = {J_MLA},
  author       = {Vahid Daghigh and Hamid Daghigh and Thomas E. Lacy Jr. and Mohammad Naraghi},
  doi          = {10.1016/j.mlwa.2024.100600},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100600},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Review of machine learning applications for defect detection in composite materials},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Moral decision making: Explainable insights into the role of
working memory in autonomous driving. <em>MLA</em>, <em>18</em>, 100599.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intersection of Artificial Intelligence (AI) and moral philosophy presents unique challenges in the development of autonomous vehicles, particularly in scenarios requiring split-second ethical decisions. This study examines the relationship between working memory (WM) and moral judgments in simulated AV scenarios, quantifying the effects of varying cognitive load on utilitarian decision-making under different time constraints. We experimented with 336 participants, each completing 16 simulated driving trials presenting unique ethical dilemmas. Results reveal a complex interplay between cognitive load and ethical choices. Under high temporal pressure (1-second response window), utilitarian decisions decreased significantly from 92.77 % to 70.08 %. Extended time constraints led to increased utilitarian choices. Statistical analyses validated these findings across diverse ethical contexts. Chi-square tests revealed significant associations between WM load and utilitarian decisions in 1-second conditions, particularly for high-stakes scenarios. Logistic regression showed that WM significantly decreased the likelihood of utilitarian decisions in these scenarios. Six supervised machine learning models were employed, with Gaussian Naive Bayes achieving the highest predictive accuracy (82.2 % to 97.0 %) in distinguishing utilitarian decisions. Partial Dependence analysis revealed a strong negative correlation between WM and utilitarian decisions, especially in the 1-second interval. The 2-second interval emerged as potentially optimal for balancing time constraints and cognitive load. These findings contribute to the theoretical understanding of ethical decision-making under cognitive load and provide practical insights for developing ethically aligned autonomous systems, with implications for improving safety, optimizing takeover protocols, and enhancing the ethical reasoning capabilities of autonomous driving systems.},
  archive      = {J_MLA},
  author       = {Amandeep Singh and Yovela Murzello and Hyowon Lee and Shene Abdalla and Siby Samuel},
  doi          = {10.1016/j.mlwa.2024.100599},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100599},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Moral decision making: Explainable insights into the role of working memory in autonomous driving},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vulnerability detection using BERT based LLM model with
transparency obligation practice towards trustworthy AI. <em>MLA</em>,
<em>18</em>, 100598. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vulnerabilities in the source code are one of the main causes of potential threats in software-intensive systems. There are a large number of vulnerabilities published each day, and effective vulnerability detection is critical to identifying and mitigating these vulnerabilities. AI has emerged as a promising solution to enhance vulnerability detection, offering the ability to analyse vast amounts of data and identify patterns indicative of potential threats. However, AI-based methods often face several challenges, specifically when dealing with large datasets and understanding the specific context of the problem. Large Language Model (LLM) is now widely considered to tackle more complex tasks and handle large datasets, which also exhibits limitations in terms of explaining the model outcome and existing works focus on providing overview of explainability and transparency. This research introduces a novel transparency obligation practice for vulnerability detection using BERT based LLMs. We address the black-box nature of LLMs by employing XAI techniques, unique combination of SHAP, LIME, heat map. We propose an architecture that combines the BERT model with transparency obligation practices, which ensures the assurance of transparency throughout the entire LLM life cycle. An experiment is performed with a large source code dataset to demonstrate the applicability of the proposed approach. The result shows higher accuracy of 91.8 % for the vulnerability detection and model explainability outcome is highly influenced by “vulnerable”, “function”, &quot;mysql_tmpdir_list&quot;, “strmov” tokens using both SHAP and LIME framework. Heatmap of attention weights, highlights the local token interactions that aid in understanding the model&#39;s decision points.},
  archive      = {J_MLA},
  author       = {Jean Haurogné and Nihala Basheer and Shareeful Islam},
  doi          = {10.1016/j.mlwa.2024.100598},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100598},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Vulnerability detection using BERT based LLM model with transparency obligation practice towards trustworthy AI},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing SMOTE for imbalanced data with abnormal minority
instances. <em>MLA</em>, <em>18</em>, 100597. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced datasets are frequent in machine learning, where certain classes are markedly underrepresented compared to others. This imbalance often results in sub-optimal model performance, as classifiers tend to favour the majority class. A significant challenge arises when abnormal instances, such as outliers, exist within the minority class, diminishing the effectiveness of traditional re-sampling methods like the Synthetic Minority Over-sampling Technique (SMOTE). This manuscript addresses this critical issue by introducing four SMOTE extensions: Distance ExtSMOTE, Dirichlet ExtSMOTE, FCRP SMOTE, and BGMM SMOTE. These methods leverage a weighted average of neighbouring instances to enhance the quality of synthetic samples and mitigate the impact of outliers. Comprehensive experiments conducted on diverse simulated and real-world imbalanced datasets demonstrate that the proposed methods improve classification performance compared to the original SMOTE and its most competitive variants. Notably, we demonstrate that Dirichlet ExtSMOTE outperforms most other proposed and existing SMOTE variants in terms of achieving better F1 score, MCC, and PR-AUC. Our results underscore the effectiveness of these advanced SMOTE extensions in tackling class imbalance, particularly in the presence of abnormal instances, offering robust solutions for real-world applications.},
  archive      = {J_MLA},
  author       = {Surani Matharaarachchi and Mike Domaratzki and Saman Muthukumarana},
  doi          = {10.1016/j.mlwa.2024.100597},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100597},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing SMOTE for imbalanced data with abnormal minority instances},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning applications for real-time and early detection
of fall armyworm, african armyworm, and maize stem borer. <em>MLA</em>,
<em>18</em>, 100596. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of artificial intelligence for identifying Fall armyworm ( Spodoptera frugiperda ), African armyworm ( Spodoptera exempta ), and Maize stem borer ( Busseola fusca ) is critical due to the threats they pose to global food production. This study aims to evaluate and identify the most accurate and robust DL models in detecting and classifying these three significant agricultural pests. Seven traditional DL models: Convolutional Neural Network, Visual Geometry Group (VGG16), Residual Networks (ResNet50), MobileNetV2, InceptionV3, Deep Neural Network (DNN), and InceptionResNetV2 and the advanced You Look Only Once (YOLOv8) model were trained and tested using pest image datasets. The results showed that all traditional models except DNN had high accuracies ranging from 93.17% (InceptionResNetV2) to 99.43% (MobileNet) in training and testing, with losses ranging from 1.71% (MobileNetV2) to 24.99% (InceptionResNetV2). DNN had a slightly lower accuracy range of 55.27% to 56.39% and a loss range of 85.02% to 89.96% in training and testing. YOLOv8 emerged as the best and most robust model in the pest detection and classification tasks, achieving Precision and Recall scores ranging from 98.4% to 100% on single-class and multi-class classifications, making it highly suitable for real-world pest management applications. This research pioneers the use of DL for the classification and detection of maize stem borer, African armyworm and Fall armyworm, unique and separately addressing a critical gap in agricultural pest management in corn. With early and accurate pest identification, crop protection measures can be implemented efficiently. The findings lead to reduced crop damage and enhanced food security.},
  archive      = {J_MLA},
  author       = {Ivan Oyege and Harriet Sibitenda and Maruthi Sridhar Balaji Bhaskar},
  doi          = {10.1016/j.mlwa.2024.100596},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100596},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning applications for real-time and early detection of fall armyworm, african armyworm, and maize stem borer},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning framework to measure water drop
penetration time (WDPT) for soil water repellency analysis.
<em>MLA</em>, <em>18</em>, 100595. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heat from wildfires volatilizes soil’s organic compounds which form a waxy layer when condensed on cooler soil particles causing soil to repel water. Timely assessment of soil water repellency (SWR) is critical for prediction and prevention of detrimental impacts of hydrophobic soils such as soil erosion, reduced availability of water to plants, and water runoff after rainfalls leading to floods. The Water Drop Penetration Time (WDPT), i.e., the time elapsed from a drop landing on the soil surface to its complete absorption is commonly used to assess the SWR level. Its manual measurements have variability based on the used instruments and subjective observations. The goal of this work is to design an automated system to perform standardized WDPT tests and assess the SWR levels. It consists of an electronically controlled mechanism to release a water drop, and a video camera to record the water penetration process. The latter is modeled as an “action” in video and Temporal Action Localization (TAL) analytics is used for predicting the WDPT and assessing the SWR level.},
  archive      = {J_MLA},
  author       = {Danxu Wang and Emma Regentova and Venkatesan Muthukumar and Markus Berli and Frederick C. Harris Jr.},
  doi          = {10.1016/j.mlwa.2024.100595},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100595},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning framework to measure water drop penetration time (WDPT) for soil water repellency analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Geographical origin identification of dendrobium officinale
based on NNRW-stacking ensembles. <em>MLA</em>, <em>18</em>, 100594. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dendrobium officinale is a well-recognized functional food material. Considering its therapeutic effect and price vary among different geographical origins, this paper proposed an origin identification method based on Raman spectroscopy and NNRW (neural network with random weights)-stacking ensemble model. In a case study of dendrobium officinale samples from three different geographical origins, we compare both single estimators, i.e., KNN (k-nearest neighbors), MLP (multi-layer perceptron), DTC (decision tree classifier), and NNRW, and their stacking ensemble counterparts. The results showed that the NNRW-stacking ensemble has the best test accuracy (96.3%) and an impressive fitting speed (the fastest among all ensembles). In conclusion, the NNRW-stacking ensemble model combined with Raman spectroscopy can be a promising method for herb geographical original identification. The proposed model has demonstrated the speed advantage of NNRW (no need for gradient-based iterations) and the generalization power of stacking ensembles (reduce single-estimator bias).},
  archive      = {J_MLA},
  author       = {Yinsheng Zhang and Chen Chen and Fangjie Guo and Haiyan Wang},
  doi          = {10.1016/j.mlwa.2024.100594},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100594},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Geographical origin identification of dendrobium officinale based on NNRW-stacking ensembles},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trends in audio scene source counting and analysis.
<em>MLA</em>, <em>18</em>, 100593. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Audio scene analysis involves a variety of tasks to obtain information from an audio environment. Audio source counting is one such task that has implications to many other aspects of audio analysis, yet it is relatively unexplored. This work presents the first review of the audio source counting literature and aims to convey the significance of this task to the wider domain of audio analysis. We identify and discuss connections between audio source counting and other more commonly studied audio analysis tasks. In addition, a review of the publicly available audio datasets is presented, highlighting the lack of datasets geared towards audio source counting. Our goal of this review paper is to promote future research of audio source counting.},
  archive      = {J_MLA},
  author       = {Michael Nigro and Sridhar Krishnan},
  doi          = {10.1016/j.mlwa.2024.100593},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100593},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Trends in audio scene source counting and analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Tumor detection in breast cancer pathology patches using a
multi-scale multi-head self-attention ensemble network on whole slide
images. <em>MLA</em>, <em>18</em>, 100592. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is the most common type of cancer among women globally and is one of the leading causes of cancer-related deaths among women. In the diagnosis of BC, histopathological assessment is the gold standard, where automated tumor detection technologies play a pivotal role. Utilizing Convolutional Neural Networks (CNNs) for automated analysis of image patches from Whole Slide Images (WSIs) enhances detection accuracy and alleviates the workload of pathologists. However, CNNs often face limitations in handling pathological patches due to a lack of sufficient contextual information and limited feature generation capabilities. To address this, we propose a novel Multi-scale Multi-head Self-attention Ensemble Network (MMSEN), which integrates a multi-scale feature generation module, a convolutional self-attention module, and an adaptive feature integration with an output module, effectively optimizing the performance of classical CNNs. The design of MMSEN optimizes the capture of key information and the comprehensive integration of features in WSIs pathological patches, significantly enhancing the precision of tumor detection. Validation results from a five-fold cross-validation experiment on the PatchCamelyon (PCam) dataset demonstrate that MMSEN achieves a ROC-AUC of 99.01% ± 0.02%, an F1-score of 98.00% ± 0.08%, a Balanced Accuracy (B-Acc) of 98.00% ± 0.08%, and a Matthews Correlation Coefficient (MCC) of 96.00% ± 0.16% ( p &lt; 0 . 05 ). These results demonstrate the effectiveness and potential of MMSEN in detecting tumors from pathological patches in WSIs for BC.},
  archive      = {J_MLA},
  author       = {Ruigang Ge and Guoyue Chen and Kazuki Saruta and Yuki Terata},
  doi          = {10.1016/j.mlwa.2024.100592},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100592},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Tumor detection in breast cancer pathology patches using a multi-scale multi-head self-attention ensemble network on whole slide images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Artificial intelligence and sustainable development in
africa: A comprehensive review. <em>MLA</em>, <em>18</em>, 100591. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) techniques are transforming various sectors and hold significant potential to advance sustainable development in Africa. However, their effective integration is constrained by region-specific challenges, limiting widespread deployment. This study reviews the current state of sustainable development in Africa, highlighting the role AI can play in driving progress across key sectors, including healthcare, agriculture, education, environmental protection, and infrastructure. The paper outlines the challenges hindering AI adoption and presents strategic approaches to address these obstacles, specifically targeting Africa’s socio-economic and environmental needs. In addition, the study proposes a comprehensive framework for integrating AI into Africa’s sustainable development efforts, offering tailored AI-driven strategies that align with the continent’s unique context. This framework provides a valuable resource for AI researchers, policymakers, and practitioners working towards sustainable development in Africa.},
  archive      = {J_MLA},
  author       = {Ibomoiye Domor Mienye and Yanxia Sun and Emmanuel Ileberi},
  doi          = {10.1016/j.mlwa.2024.100591},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100591},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Artificial intelligence and sustainable development in africa: A comprehensive review},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting drug transfers via the drop-off method: A
supervised model approach using AIS data. <em>MLA</em>, <em>18</em>,
100590. (<a href="https://doi.org/10.1016/j.mlwa.2024.100590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maritime security is of tremendous importance in countering drug trafficking, particularly through sea-based routes. In this paper, we address the pressing need for effective detection methods by introducing a novel approach utilizing Automatic Identification System (AIS) data. Our focus lies on detecting the ‘drop-off’ method, a prevalent technique for contraband smuggling at sea. Unlike existing research, primarily employing unsupervised methods, we propose a supervised model specifically tailored to this illicit activity, with a particular emphasis on its application to fishing vessels. Our model significantly reduces the number of data points requiring classification by the observer by 70% , thereby enhancing the efficiency of the drop-off detection process. By employing a Long Short-Term Memory (LSTM) model, our approach demonstrates a change from traditional methods and offers advantages in capturing complex temporal patterns inherent in ‘drop-off’ activities. The rationale behind choosing LSTM lies in its ability to effectively model sequential data, which is essential for detecting drug traffic activities at sea where patterns are subtle and dynamic. Moreover, this model holds the potential for integration into real-time surveillance systems, thereby enhancing operational capabilities in detecting and preventing drug traffic. The generalizability of our model makes for considerable potential in enhancing maritime security efforts and providing assistance in countering drug traffic on a global scale. Importantly, our model outperforms both baseline models, underscoring its effectiveness and superiority in addressing the specific challenges posed by ‘drop-off’ detection. For more information and access to the code repository, please visit this link .},
  archive      = {J_MLA},
  author       = {Britt van Leeuwen and Maike Nutzel},
  doi          = {10.1016/j.mlwa.2024.100590},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100590},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Detecting drug transfers via the drop-off method: A supervised model approach using AIS data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noninvasive pressure monitoring using acoustic resonance
spectroscopy and machine learning. <em>MLA</em>, <em>18</em>, 100589.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring pressure inside hermetically sealed vessels typically relies on devices that have direct contact with the fluid inside. Gaining this access requires a hole through the wall of the vessel, which creates potential for leaks, ruptures, and complete failures. To solve this, noninvasive solutions utilize external sensors that relate vessel-wall behavior to internal pressure. However, existing noninvasive techniques require permanently attaching sensors to a unique vessel and then monitoring for changes in the vessel. We present a noninvasive pressure monitoring technique based on acoustic resonance spectroscopy (ARS) and machine learning (ML) that enables estimating pressure in a vessel similar to those it was trained on and does not require sensors to be permanently attached. We train k-nearest neighbor (KNN) regressor models using experimentally gathered acoustic resonance spectra to estimate the pressure in six stainless-steel vessels. We demonstrate accurate estimation of the pressure inside the vessels when training and testing using spectra taken exclusively from an individual vessel, and when performing cross-validation between vessels. The acoustic technique presented in this paper finds broad applications across industry to monitor pressure in systems where having permanent sensors is undesirable, such as complicated pneumatic systems, vacuum sealed foods, and more.},
  archive      = {J_MLA},
  author       = {M. Prisbrey and D. Pereira and J. Greenhall and E. Davis and P. Vakhlamov and C. Chavez and C. Pantea},
  doi          = {10.1016/j.mlwa.2024.100589},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100589},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Noninvasive pressure monitoring using acoustic resonance spectroscopy and machine learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VLFSE: Enhancing visual tracking through visual language
fusion and state update evaluator. <em>MLA</em>, <em>18</em>, 100588.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, visual tracking algorithms have achieved impressive results by combining dynamic templates. However, the instability of visual images and the incorrect timing of template updates lead to decreased tracking accuracy and stability in intricate scenarios. To address these issues, we propose a visual tracking algorithm through visual language fusion and a state update evaluator (VLFSE). Specifically, our approach introduces a multimodal attention mechanism that uses self-attention to mine and integrate information from diverse sources effectively. This mechanism ensures a richer, context-aware representation of the target, enabling more accurate tracking even in complex scenes. Moreover, we recognize the critical need for precise template updates to maintain tracking accuracy over time. To this end, we develop a state update evaluator, a component trained online to assess the necessity and timing of template updates accurately. This evaluator acts as a safeguard, preventing erroneous updates and ensuring the tracker adapts optimally to changes in the target’s appearance. The experimental results on challenging visual language tracking datasets demonstrate our tracker’s superior performance, showcasing its adaptability and accuracy in complex tracking scenarios.},
  archive      = {J_MLA},
  author       = {Fuchao Yang and Mingkai Jiang and Qiaohong Hao and Xiaolei Zhao and Qinghe Feng},
  doi          = {10.1016/j.mlwa.2024.100588},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100588},
  shortjournal = {Mach. Learn. Appl.},
  title        = {VLFSE: Enhancing visual tracking through visual language fusion and state update evaluator},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). InvarNet: Molecular property prediction via rotation
invariant graph neural networks. <em>MLA</em>, <em>18</em>, 100587. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting molecular properties is crucial in drug synthesis and screening, but traditional molecular dynamics methods are time-consuming and costly. Recently, deep learning methods, particularly Graph Neural Networks (GNNs), have significantly improved efficiency by capturing molecular structures’ invariance under translation, rotation, and permutation. However, current GNN methods require complex data processing, increasing algorithmic complexity. This high complexity leads to several challenges, including increased computation time, higher computational resource demands, increased memory consumption. This paper introduces InvarNet, a GNN-based model trained with a composite loss function that bypasses intricate data processing while maintaining molecular property invariance. By pre-storing atomic feature attributes, InvarNet avoids repeated feature extraction during forward propagation. Experiments on three public datasets (Electronic Materials, QM9, and MD17) demonstrate that InvarNet achieves superior prediction accuracy, excellent stability, and convergence speed. It reaches state-of-the-art performance on the Electronic Materials dataset and outperforms existing models on the R 2 and a l p h a properties of the QM9 dataset. On the MD17 dataset, InvarNet excels in energy prediction of benzene without atomic force. Additionally, InvarNet accelerates training time per epoch by 2.24 times compared to SphereNet on the QM9 dataset, simplifying data processing while maintaining acceptable accuracy.},
  archive      = {J_MLA},
  author       = {Danyan Chen and Gaoxiang Duan and Dengbao Miao and Xiaoying Zheng and Yongxin Zhu},
  doi          = {10.1016/j.mlwa.2024.100587},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100587},
  shortjournal = {Mach. Learn. Appl.},
  title        = {InvarNet: Molecular property prediction via rotation invariant graph neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recurrent neural networks for anomaly detection in magnet
power supplies of particle accelerators. <em>MLA</em>, <em>18</em>,
100585. (<a href="https://doi.org/10.1016/j.mlwa.2024.100585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research illustrates how time-series forecasting employing recurrent neural networks (RNNs) can be used for anomaly detection in particle accelerators—complex machines that accelerate elementary particles to high speeds for various scientific and industrial applications. Our approach utilizes an RNN to predict temperatures of key components of magnet power supplies (PSs), which can number up to thousands in an accelerator. An anomaly is declared when the predicted temperature deviates significantly from observation. Our method can help identify a PS requiring maintenance before it fails and leads to costly downtime of an entire billion-dollar accelerator facility. We demonstrate that the RNN outperforms a reasonably complex physics-based model at predicting the PS temperatures and at anomaly detection. We conclude that for practical applications it can be beneficial to use RNNs instead of increasing the complexity of the physics-based model. We chose the long short-term memory (LSTM) as opposed to other RNN cell structures due to its widespread use in time-series forecasting and its relative simplicity. However, we demonstrate that the LSTM’s precision of predicting PS temperatures is nearly on par with measurement precision, making more complex or custom architectures unnecessary. Lastly, we dedicate a section of this paper to presenting a proof-of-concept for using infrared cameras for spatially-resolved anomaly detection inside power supplies, which will be a subject of future research.},
  archive      = {J_MLA},
  author       = {Ihar Lobach and Michael Borland},
  doi          = {10.1016/j.mlwa.2024.100585},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100585},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Recurrent neural networks for anomaly detection in magnet power supplies of particle accelerators},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time monitoring and optimization of machine learning
intelligent control system in power data modeling technology.
<em>MLA</em>, <em>18</em>, 100584. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the problems of insufficient model accuracy and poor real-time performance in real-time monitoring and optimization of data models in traditional power systems, this paper explored them based on machine learning. It used methods such as box plots and Kalman filters to preprocess power data and used Fourier transform and long short-term memory (LSTM) network to extract data features. Based on long-term and short-term memory networks, this paper constructed a deep neural network model to process data. Recursive feature elimination method can be used for feature selection in the model, and multi-model integration method can be used for feature fusion. The experiment trained and tested the model on two datasets, IEEE 39-Bus and Pecan Street. The experimental results show that the accuracy of the paper&#39;s model in both the training and testing sets is higher than that of the convolutional neural network, decision tree, and support vector machine models in the comparative experiments, reaching 93 % and 94 %, respectively. The average response times in three different testing methods were 139.8 ms, 151 ms, and 140.6 ms, respectively. The lowest daily failure rate for 30 consecutive days of work was 0 %, which reflects the stability of the model; in the satisfaction survey of practitioners, the proportion of high recognition answers to each question was higher than 80 %. The experimental results show that the machine learning intelligent control system has achieved good application in power data modeling technology, providing a reference for similar research in the future.},
  archive      = {J_MLA},
  author       = {Qiong Wang and Zuohu Chen and Yongbo Zhou and Zhiyuan Liu and Zhenguo Peng},
  doi          = {10.1016/j.mlwa.2024.100584},
  journal      = {Machine Learning with Applications},
  month        = {12},
  pages        = {100584},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Real-time monitoring and optimization of machine learning intelligent control system in power data modeling technology},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust image classification system via cloud computing,
aligned multimodal embeddings, centroids and neighbours. <em>MLA</em>,
<em>17</em>, 100583. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for a cloud-based application of an image classification system that is highly accessible, maintains data confidentiality, and robust to incorrect training labels. The end-to-end system is implemented using Amazon Web Services (AWS), with a detailed guide provided for replication, enhancing the ways which researchers can collaborate with a community of users for mutual benefits. A front-end web application allows users across the world to securely log in, contribute labelled training images conveniently via a drag-and-drop approach, and use that same application to query an up-to-date model that has knowledge of images from the community of users. This resulting system demonstrates that theory can be effectively interlaced with practice, with various considerations addressed by our architecture. Users will have access to an image classification model that can be updated and automatically deployed within minutes, gaining benefits from and at the same time providing benefits to the community of users. At the same time, researchers, who will act as administrators, will be able to conveniently and securely engage a large number of users with their respective machine learning models and build up a labelled database over time, paying only variable costs that is proportional to utilization.},
  archive      = {J_MLA},
  author       = {Wei Lun Koh and James Boon Yong Koh and Bing Tian Dai},
  doi          = {10.1016/j.mlwa.2024.100583},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100583},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Robust image classification system via cloud computing, aligned multimodal embeddings, centroids and neighbours},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of bike-sharing station demand using explainable
artificial intelligence. <em>MLA</em>, <em>17</em>, 100582. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bike-sharing systems have grown in popularity in metropolitan areas, providing a handy and environmentally friendly transportation choice for commuters and visitors alike. As demand for bike-sharing programs grows, efficient capacity planning becomes critical to ensuring good user experience and system sustainability in terms of demand. The random forest model was used in this study to predict bike-sharing station demand and is considered a strong ensemble learning approach that can successfully capture complicated nonlinear correlations and interactions between input variables. This study employed data from the Smart Location Database (SLD) to test the model accuracy in estimating station demand and used a form of explainable artificial intelligence (XAI) function to further understand machine learning (ML) prediction outcomes owing to the blackbox tendencies of ML models. Vehicle Miles of Travel (VMT) and Greenhouse Gas (GHG) emissions were the most important features in predicting docking station demand individually but not holistically based on the datasets. The percentage of zero-car households, gross residential density, road network density, aggregate frequency of transit service, and gross activity density were found to have a moderate influence on the prediction model. Further, there may be a better prediction model generating sensible results for every type of explanatory variable, but their contributions are minimum to the prediction outcome. By measuring each feature&#39;s contribution to demand prediction in feature engineering, bike-sharing operators can acquire a better understanding of the bike-sharing station capacity and forecast future demands during planning. At the same time, ML models will need further assessment before a holistic conclusion.},
  archive      = {J_MLA},
  author       = {Frank Ngeni and Boniphace Kutela and Tumlumbe Juliana Chengula and Cuthbert Ruseruka and Hannah Musau and Norris Novat and Debbie Aisiana Indah and Sarah Kasomi},
  doi          = {10.1016/j.mlwa.2024.100582},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100582},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Prediction of bike-sharing station demand using explainable artificial intelligence},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Navigating interpretability and alpha control in GF-KCSD
testing with measurement error: A kernel approach. <em>MLA</em>,
<em>17</em>, 100581. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gradient-Free Kernel Conditional Stein Discrepancy (GF-KCSD), presented in our prior work, represents a significant advancement in goodness-of-fit testing for conditional distributions. This method offers a robust alternative to previous gradient-based techniques, specially when the gradient calculation is intractable or computationally expensive. In this study, we explore previously unexamined aspects of GF-KCSD, with a particular focus on critical values and test power—essential components for effective hypothesis testing. We also present novel investigation on the impact of measurement errors on the performance of GF-KCSD in comparison to established benchmarks, enhancing our understanding of its resilience to these errors. Through controlled experiments using synthetic data, we demonstrate GF-KCSD’s superior ability to control type-I error rates and maintain high statistical power, even in the presence of measurement inaccuracies. Our empirical evaluation extends to real-world datasets, including brain MRI data. The findings confirm that GF-KCSD performs comparably to KCSD in hypothesis testing effectiveness while requiring significantly less computational time. This demonstrates GF-KCSD’s capability as an efficient tool for analyzing complex data, enhancing its value for scenarios that demand rapid and robust statistical analysis.},
  archive      = {J_MLA},
  author       = {Elham Afzali and Saman Muthukumarana and Liqun Wang},
  doi          = {10.1016/j.mlwa.2024.100581},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100581},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Navigating interpretability and alpha control in GF-KCSD testing with measurement error: A kernel approach},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing advanced driver assistance systems through
explainable artificial intelligence for driver anomaly detection.
<em>MLA</em>, <em>17</em>, 100580. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent advancements in Advanced Driver Assistance Systems (ADAS) have significantly contributed to road safety and driving comfort. An integral aspect of these systems is the detection of driver anomalies such as drowsiness, distraction, and impairment, which are crucial for preventing accidents. Building upon previous studies that utilized ensemble model learning (XGBoost) with deep learning models (ResNet50, DenseNet201, and InceptionV3) for anomaly detection, this study introduces a comprehensive feature importance analysis using the SHAP (SHapley Additive exPlanations) technique. The technique is implemented through explainable artificial intelligence (XAI). The primary objective is to unravel the complex decision-making process of the ensemble model, which has previously demonstrated near-perfect performance metrics in classifying driver behaviors using in-vehicle cameras. By applying SHAP, the study aims to identify and quantify the contribution of each feature – such as facial expressions, head position, yawning, and sleeping – in predicting driver states. This analysis offers insights into the model’s inner workings and guides the enhancement of feature engineering for more precise and reliable anomaly detection. The findings of this study are expected to impact the development of future ADAS technologies significantly. By pinpointing the most influential features and understanding their dynamics, a model can be optimized for various driving scenarios, ensuring that ADAS systems are robust, accurate, and tailored to real-world conditions. Ultimately, this study contributes to the overarching goal of enhancing road safety through technologically advanced, data-driven approaches.},
  archive      = {J_MLA},
  author       = {Tumlumbe Juliana Chengula and Judith Mwakalonge and Gurcan Comert and Methusela Sulle and Saidi Siuhi and Eric Osei},
  doi          = {10.1016/j.mlwa.2024.100580},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100580},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing advanced driver assistance systems through explainable artificial intelligence for driver anomaly detection},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Detection of presence or absence of metastasis in WSI
patches of breast cancer using the dual-enhanced convolutional ensemble
neural network. <em>MLA</em>, <em>17</em>, 100579. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer (BC) is a prevalent malignancy worldwide, posing a significant public health burden due to its high incidence rate. Accurate detection is crucial for improving survival rates, and pathological diagnosis through biopsy is essential for detailed BC detection. Convolutional Neural Network (CNN)-based methods have been proposed to support this detection, utilizing patches from Whole Slide Imaging (WSI) combined with sophisticated CNNs. In this research, we introduced DECENN, a novel deep learning architecture designed to overcome the limitations of single CNN models under fixed pre-trained parameter transfer learning settings. DECENN employs an ensemble of VGG16 and DenseNet121, integrated with innovative modules such as Multi-Scale Feature Extraction, Heterogeneous Convolution Enhancement, Feature Harmonization and Fusion, and Feature Integration Output. Through progressive stages – from baseline models, intermediate DCNN and DCNN+ models, to the fully integrated DECENN model – significant performance improvements were observed in experiments using 5-fold cross-validation on the Patch Camelyon(PCam) dataset. DECENN achieved an AUC of 99.70% ± 0.12%, an F-score of 98.93% ± 0.06%, and an Accuracy of 98.92% ± 0.06%, ( p &lt; 0 . 001 p&amp;lt;0.001 ). These results highlight DECENN’s potential to significantly enhance the automated detection and diagnostic accuracy of BC metastasis in biopsy specimens.},
  archive      = {J_MLA},
  author       = {Ruigang Ge and Guoyue Chen and Kazuki Saruta and Yuki Terata},
  doi          = {10.1016/j.mlwa.2024.100579},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100579},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Detection of presence or absence of metastasis in WSI patches of breast cancer using the dual-enhanced convolutional ensemble neural network},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for modelling customer invoice payment
predictions. <em>MLA</em>, <em>17</em>, 100578. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By offering clients attractive credit terms on sales, a company may increase its turnover, but granting credit also incurs the cost of money tied up in accounts receivable (AR), increased administration and a heightened probability of incurring bad debt. The management of credit sales, although eminently important to any business, is often performed manually, which may be time-consuming, expensive and inaccurate. Such an administrative workload becomes increasingly cumbersome as the number of credit sales increases. As a result, a new approach towards proactively identifying invoices from AR accounts that are likely to be paid late, or not at all, has recently been proposed in the literature, with the aim of employing intervention strategies more effectively. Several computational techniques from the credit scoring literature and particularly techniques from the realms of survival analysis or machine learning have been embedded in the aforementioned approach. This body of work is, however, lacking due to the limited guidance provided during the data preparation phase of the model development process and because survival analytic and machine learning techniques have not yet been ensembled. In this paper, we propose a generic framework for modelling invoice payment predictions with the aim of facilitating the process of preparing transaction data for analysis, generating relevant features from past customer behaviours, and selecting and ensembling suitable models for predicting the time to payment associated with invoices. We also introduce a new sequential ensembling approach, called the Survival Boost algorithm. The rationale behind this method is that features generated by a survival analytic model can enhance the efficacy of a machine learning classification algorithm.},
  archive      = {J_MLA},
  author       = {Willem Roux Moore and Jan H. van Vuuren},
  doi          = {10.1016/j.mlwa.2024.100578},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100578},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A framework for modelling customer invoice payment predictions},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data efficient contrastive learning in histopathology using
active sampling. <em>MLA</em>, <em>17</em>, 100577. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) based diagnostics systems can provide accurate and robust quantitative analysis in digital pathology. These algorithms require large amounts of annotated training data which is impractical in pathology due to the high resolution of histopathological images. Hence, self-supervised methods have been proposed to learn features using ad-hoc pretext tasks. The self-supervised training process uses a large unlabeled dataset which makes the learning process time consuming. In this work, we propose a new method for actively sampling informative members from the training set using a small proxy network, decreasing sample requirement by 93% and training time by 62% while maintaining the same performance of the traditional self-supervised learning method. The code is available on github .},
  archive      = {J_MLA},
  author       = {Tahsin Reasat and Asif Sushmit and David S. Smith},
  doi          = {10.1016/j.mlwa.2024.100577},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100577},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Data efficient contrastive learning in histopathology using active sampling},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised machine learning in drug discovery and
development: Algorithms, applications, challenges, and prospects.
<em>MLA</em>, <em>17</em>, 100576. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug discovery and development is a time-consuming process that involves identifying, designing, and testing new drugs to address critical medical needs. In recent years, machine learning (ML) has played a vital role in technological advancements and has shown promising results in various drug discovery and development stages. ML can be categorized into supervised, unsupervised, semi-supervised, and reinforcement learning. Supervised learning is the most used category, helping organizations solve several real-world problems. This study presents a comprehensive survey of supervised learning algorithms in drug design and development, focusing on their learning process and succinct mathematical formulations, which are lacking in the literature. Additionally, the study discusses widely encountered challenges in applying supervised learning for drug discovery and potential solutions. This study will be beneficial to researchers and practitioners in the pharmaceutical industry as it provides a simplified yet comprehensive review of the main concepts, algorithms, challenges, and prospects in supervised learning.},
  archive      = {J_MLA},
  author       = {George Obaido and Ibomoiye Domor Mienye and Oluwaseun F. Egbelowo and Ikiomoye Douglas Emmanuel and Adeola Ogunleye and Blessing Ogbuokiri and Pere Mienye and Kehinde Aruleba},
  doi          = {10.1016/j.mlwa.2024.100576},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100576},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Supervised machine learning in drug discovery and development: Algorithms, applications, challenges, and prospects},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DeepCKID: A multi-head attention-based deep neural network
model leveraging classwise knowledge to handle imbalanced textual data.
<em>MLA</em>, <em>17</em>, 100575. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents DeepCKID, a Multi-Head Attention (MHA)-based deep learning model that exploits statistical and semantic knowledge corresponding to documents across different classes in the datasets to improve the model’s ability to detect minority class instances in imbalanced text classification. In this process, corresponding to each document, DeepCKID extracts — (i) word-level statistical and semantic knowledge, namely, class correlation and class similarity corresponding to each word, based on its association with different classes in the dataset, and (ii) class-level knowledge from the document using n n -grams and relation triplets corresponding to classwise keywords present, identified using cosine similarity utilizing Transformers-based Pre-trained Language Models (PLMs). DeepCKID encodes the word-level and class-level features using deep convolutional networks, which can learn meaningful patterns from them. At first, DeepCKID combines the semantically meaningful Sentence-BERT document embeddings and word-level feature matrix to give the final document representation, which it further fuses to the different classwise encoded representations to strengthen feature propagation. DeepCKID then passes the encoded document representation and its different classwise representations through an MHA layer to identify the important features at different positions of the feature subspaces, resulting in a latent dense vector accentuating its association with a particular class. Finally, DeepCKID passes the latent vector to the softmax layer to learn the corresponding class label. We evaluate DeepCKID over six publicly available Amazon reviews datasets using four Transformers-based PLMs. We compare DeepCKID with three approaches and four ablation-like baselines. Our study suggests that in most cases, DeepCKID outperforms all the comparison approaches, including baselines.},
  archive      = {J_MLA},
  author       = {Amit Kumar Sah and Muhammad Abulaish},
  doi          = {10.1016/j.mlwa.2024.100575},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100575},
  shortjournal = {Mach. Learn. Appl.},
  title        = {DeepCKID: A multi-head attention-based deep neural network model leveraging classwise knowledge to handle imbalanced textual data},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial instability of crash prediction models: A case of
scooter crashes. <em>MLA</em>, <em>17</em>, 100574. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scooters have gained widespread popularity in recent years due to their accessibility and affordability, but safety concerns persist due to the vulnerability of riders. Researchers are actively investigating the safety implications associated with scooters, given their relatively new status as transportation options. However, analyzing scooter safety presents a unique challenge due to the complexity of determining safe riding environments. This study presents a comprehensive analysis of scooter crash risk within various buffer zones, utilizing the Extreme Gradient Boosting (XGBoost) machine learning algorithm. The core objective was to unravel the multifaceted factors influencing scooter crashes and assess the predictive model’s performance across different buffers or spatial proximity to crash sites. After evaluating the model’s accuracy, sensitivity, and specificity across buffer distances ranging from 5 ft to 250 ft with the scooter crash as a reference point, a discernible trend emerged: as the buffer distance decreases, the model’s sensitivity increases, although at the expense of accuracy and specificity, which exhibit a gradual decline. Notably, at the widest buffer of 250 ft, the model achieved a high accuracy of 97% and specificity of 99%, but with a lower sensitivity of 31%. Contrastingly, at the closest buffer of 5 ft, sensitivity peaked at 95%, albeit with slightly reduced accuracy and specificity. Feature importance analysis highlighted the most significant predictor across all buffer distances, emphasizing the impact of vehicle interactions on scooter crash likelihood. Explainable Artificial Intelligence through SHAP value analysis provided deeper insights into each feature’s contribution to the predictive model, revealing passenger vehicle types of significantly escalated crash risks. Intriguingly, specific vehicular maneuvers, notably stopping in traffic lanes, alongside the absence of Traffic Control Devices (TCDs), were identified as the major contributors to increased crash occurrences. Road conditions, particularly wet and dry, also emerged as substantial risk factors. Furthermore, the study highlights the significance of road design, where elements like junction types and horizontal alignments – specifically 4 and 5-legged intersections and curves – are closely associated with heightened crash risks. These findings articulate a complex and spatially detailed framework of factors impacting scooter crashes, offering vital insights for urban planning and policymaking.},
  archive      = {J_MLA},
  author       = {Tumlumbe Juliana Chengula and Boniphace Kutela and Norris Novat and Hellen Shita and Abdallah Kinero and Reuben Tamakloe and Sarah Kasomi},
  doi          = {10.1016/j.mlwa.2024.100574},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100574},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Spatial instability of crash prediction models: A case of scooter crashes},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CLRiuS: Contrastive learning for intrinsically unordered
steel scrap. <em>MLA</em>, <em>17</em>, 100573. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been remarkable progress in the field of Deep Learning and Computer Vision, but there is a lack of freely available labeled data, especially when it comes to data for specific industrial applications. However, large volumes of structured, semi-structured and unstructured data are generated in industrial environments, from which meaningful representations can be learned. The effort required for manual labeling is extremely high and can often only be carried out by domain experts. Self-supervised methods have proven their effectiveness in recent years in a wide variety of areas such as natural language processing or computer vision. In contrast to supervised methods, self-supervised techniques are rarely used in real industrial applications. In this paper, we present a self-supervised contrastive learning approach that outperforms existing supervised approaches on the used scrap dataset. We use different types of augmentations to extract the fine-grained structures that are typical for this type of images of intrinsically unordered items. This extracts a wider range of features and encodes more aspects of the input image. This approach makes it possible to learn characteristics from images that are common for applications in the industry, such as quality control. In addition, we show that this self-supervised learning approach can be successfully applied to scene-like images for classification.},
  archive      = {J_MLA},
  author       = {Michael Schäfer and Ulrike Faltings and Björn Glaser},
  doi          = {10.1016/j.mlwa.2024.100573},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100573},
  shortjournal = {Mach. Learn. Appl.},
  title        = {CLRiuS: Contrastive learning for intrinsically unordered steel scrap},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive study of auto-encoders for anomaly
detection: Efficiency and trade-offs. <em>MLA</em>, <em>17</em>, 100572.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised anomaly detection (UAD) is a diverse research area explored across various application domains. Over time, numerous anomaly detection techniques, including clustering, generative, and variational inference-based methods, are developed to address specific drawbacks and advance state-of-the-art techniques. Deep learning and generative models recently played a significant role in identifying unique challenges and devising advanced approaches. Auto-encoders (AEs) represent one such powerful technique that combines generative and probabilistic variational modeling with deep architecture. Auto-Encoder aims to learn the underlying data distribution to generate consequential sample data. This concept of data generation and the adoption of generative modeling have emerged in extensive research and variations in Auto-Encoder design, particularly in unsupervised representation learning. This study systematically reviews 11 Auto-Encoder architectures categorized into three groups, aiming to differentiate their reconstruction ability, sample generation, latent space visualization, and accuracy in classifying anomalous data using the Fashion-MNIST (FMNIST) and MNIST datasets. Additionally, we closely observed the reproducibility scope under different training parameters. We conducted reproducibility experiments utilizing similar model setups and hyperparameters and attempted to generate comparative results to address the scope of improvements for each Auto-Encoder. We conclude this study by analyzing the experimental results, which guide us in identifying the efficiency and trade-offs among auto-encoders, providing valuable insights into their performance and applicability in unsupervised anomaly detection techniques.},
  archive      = {J_MLA},
  author       = {Asif Ahmed Neloy and Maxime Turgeon},
  doi          = {10.1016/j.mlwa.2024.100572},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100572},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comprehensive study of auto-encoders for anomaly detection: Efficiency and trade-offs},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An algorithm for two-dimensional pattern detection by
combining echo state network-based weak classifiers. <em>MLA</em>,
<em>17</em>, 100571. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern detection is one of the essential technologies in computer vision. To solve pattern detection problems, the system needs a vast amount of computational resources. To train a multilayer perceptron or convolutional neural network, the gradient descent method is commonly used. The method consumes computational resources. To reduce the amount of computation, we propose a two-dimensional pattern detection algorithm based on Echo State Network (ESN). The training rule of ESN is based on one-shot ridge regression, which enables us to avoid the gradient descent. ESN is a kind of recurrent neural network (RNN), which is often used to embed temporal signals inside the network, rarely used for the embedding of static patterns. In our prior work (Kage, 2023), we found that static patterns can be embedded in an ESN network by associating the training patterns with its stable states, or attractors. By using the same training procedure as our prior work, we made sure that we can associate each training patch image with the desired output vector. The resulting performance of a single ESN classifier is, however, relatively poor. To overcome this poor performance, we introduced an ensemble learning framework by combining multiple ESN weak classifiers. To evaluate the performance, we used CMU-MIT frontal face images (CMU DB). We trained eleven ESN-based classifiers by using six CMU DB training images and evaluated the performance by using a CMU DB test image. We succeeded in reducing false positives in the CMU DB test image down to 0.0515 %.},
  archive      = {J_MLA},
  author       = {Hiroshi Kage},
  doi          = {10.1016/j.mlwa.2024.100571},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100571},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An algorithm for two-dimensional pattern detection by combining echo state network-based weak classifiers},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Managing linux servers with LLM-based AI agents: An
empirical evaluation with GPT4. <em>MLA</em>, <em>17</em>, 100570. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an empirical study on the application of Large Language Model (LLM)-based AI agents for automating server management tasks in Linux environments. We aim to evaluate the effectiveness, efficiency, and adaptability of LLM-based AI agents in handling a wide range of server management tasks, and to identify the potential benefits and challenges of employing such agents in real-world scenarios. We present an empirical study where a GPT-based AI agent autonomously executes 150 unique tasks across 9 categories, ranging from file management to editing to program compilations. The agent operates in a Dockerized Linux sandbox, interpreting task descriptions and generating appropriate commands or scripts. Our findings reveal the agent’s proficiency in executing tasks autonomously and adapting to feedback, demonstrating the potential of LLMs in simplifying complex server management for users with varying technical expertise. This study contributes to the understanding of LLM applications in server management scenarios, and paves the foundation for future research in this domain.},
  archive      = {J_MLA},
  author       = {Charles Cao and Feiyi Wang and Lisa Lindley and Zejiang Wang},
  doi          = {10.1016/j.mlwa.2024.100570},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100570},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Managing linux servers with LLM-based AI agents: An empirical evaluation with GPT4},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-class AUC maximization for imbalanced ordinal
multi-stage tropical cyclone intensity change forecast. <em>MLA</em>,
<em>17</em>, 100569. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intense tropical cyclones (TCs) cause significant damage to human societies. Forecasting the multiple stages of TC intensity changes is considerably crucial yet challenging. This difficulty arises due to imbalanced data distribution and the need for ordinal multi-class classification. While existing classification methods, such as linear discriminant analysis, have been utilized to predict rare rapidly intensifying (RI) stages based on features related TC intensity changes, they are limited to binary classification distinguishing between RI and non-RI stages. In this paper, we introduce a novel methodology to tackle the challenges of imbalanced ordinal multi-class classification. We extend the Area Under the Curve maximization technique with inter-instance/class cross-hinge losses and inter-class distance-based slack variables. The proposed loss function, implemented within a deep learning framework, demonstrates its effectiveness using real sequence data of multi-stage TC intensity changes, including satellite infrared images and environmental variables observed in the western North Pacific.},
  archive      = {J_MLA},
  author       = {Hirotaka Hachiya and Hiroki Yoshida and Udai Shimada and Naonori Ueda},
  doi          = {10.1016/j.mlwa.2024.100569},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100569},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Multi-class AUC maximization for imbalanced ordinal multi-stage tropical cyclone intensity change forecast},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). VashaNet: An automated system for recognizing handwritten
bangla basic characters using deep convolutional neural network.
<em>MLA</em>, <em>17</em>, 100568. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated character recognition is currently highly popular due to its wide range of applications. Bengali handwritten character recognition (BHCR) is an extremely difficult issue because of the nature of the script. Very few handwritten character recognition (HCR) models are capable of accurately classifying all different sorts of Bangla characters. Recently, image recognition, video analytics, and natural language processing have all found great success using convolutional neural network (CNN) due to its ability to extract and classify features in novel ways. In this paper, we introduce a VashaNet model for recognizing Bangla handwritten basic characters. The suggested VashaNet model employs a 26-layer deep convolutional neural network (DCNN) architecture consisting of nine convolutional layers, six max pooling layers, two dropout layers, five batch normalization layers, one flattening layer, two dense layers, and one output layer. The experiment was performed over 2 datasets consisting of a primary dataset of 5750 images, CMATERdb 3.1.2 for the purpose of training and evaluating the model. The suggested character recognition model worked very well, with test accuracy rates of 94.60% for the primary dataset, 94.43% for CMATERdb 3.1.2 dataset. These remarkable outcomes demonstrate that the proposed VashaNet outperforms other existing methods and offers improved suitability in different character recognition tasks. The proposed approach is a viable candidate for the high efficient practical automatic BHCR system. The proposed approach is a more powerful candidate for the development of an automatic BHCR system for use in practical settings.},
  archive      = {J_MLA},
  author       = {Mirza Raquib and Mohammad Amzad Hossain and Md Khairul Islam and Md Sipon Miah},
  doi          = {10.1016/j.mlwa.2024.100568},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100568},
  shortjournal = {Mach. Learn. Appl.},
  title        = {VashaNet: An automated system for recognizing handwritten bangla basic characters using deep convolutional neural network},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining customer churn prediction in telecom industry
using tabular machine learning models. <em>MLA</em>, <em>17</em>,
100567. (<a href="https://doi.org/10.1016/j.mlwa.2024.100567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study addresses customer churn, a major issue in service-oriented sectors like telecommunications, where it refers to the discontinuation of subscriptions. The research emphasizes the importance of recognizing customer satisfaction for retaining clients, focusing specifically on early churn prediction as a key strategy. Previous approaches mainly used generalized classification techniques for churn prediction but often neglected the aspect of interpretability, vital for decision-making. This study introduces explainer models to address this gap, providing both local and global explanations of churn predictions. Various classification models, including the standout Gradient Boosting Machine (GBM), were used alongside visualization techniques like Shapley Additive Explanations plots and scatter plots for enhanced interpretability. The GBM model demonstrated superior performance with an 81% accuracy rate. A Wilcoxon signed rank test confirmed GBM’s effectiveness over other models, with the p p -value indicating significant performance differences. The study concludes that GBM is notably better for churn prediction, and the employed visualization techniques effectively elucidate key churn factors in the telecommunications sector.},
  archive      = {J_MLA},
  author       = {Sumana Sharma Poudel and Suresh Pokharel and Mohan Timilsina},
  doi          = {10.1016/j.mlwa.2024.100567},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100567},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Explaining customer churn prediction in telecom industry using tabular machine learning models},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Word embedding and classification methods and their effects
on fake news detection. <em>MLA</em>, <em>17</em>, 100566. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing contains multiple methods of translating written text or spoken words into numerical information called word embeddings. Some of these embedding methods, such as Bag of Words, assume words are independent of one another. Other embedding methods, such as Bidirectional Encoder Representations from Transformers and Word2Vec, capture the relationship between words in various ways. In this paper, we are interested in comparing methods treating words as independent and methods capturing the relationship between words by looking at the effect these methods have on the classification of fake news. Using various classification methods, we compare the word embedding processes based on their effects on accuracy, precision, sensitivity, and specificity.},
  archive      = {J_MLA},
  author       = {Jessica Hauschild and Kent Eskridge},
  doi          = {10.1016/j.mlwa.2024.100566},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100566},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Word embedding and classification methods and their effects on fake news detection},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ensemble prediction of RRC session duration in real-world
NR/LTE networks. <em>MLA</em>, <em>17</em>, 100564. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the rapidly evolving realm of telecommunications, Machine Learning (ML) stands as a key driver for intelligent 6 G networks, leveraging diverse datasets to optimize real-time network parameters. This transition seamlessly extends from 4 G LTE and 5 G NR to 6 G, with ML insights from existing networks, specifically in predicting RRC session durations. This work introduces a novel use of weighted ensemble approach using AutoGluon library, employing multiple base models for accurate prediction of user session durations in real-world LTE and NR networks. Comparative analysis reveals superior accuracy in LTE, with &#39;Data Volume&#39; as a crucial feature due to its direct impact on network load and user experience. Notably, NR sessions, marked by extended durations, reflect unique patterns attributed to Fixed Wireless Access (FWA) devices. An ablation study underscores the weighted ensemble&#39;s superior performance. This study highlights the need for techniques like data categorization to enhance prediction accuracies for evolving technologies, providing insights for enhanced adaptability in ML-based prediction models for the next network generation.},
  archive      = {J_MLA},
  author       = {Roopesh Kumar Polaganga and Qilian Liang},
  doi          = {10.1016/j.mlwa.2024.100564},
  journal      = {Machine Learning with Applications},
  month        = {9},
  pages        = {100564},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Ensemble prediction of RRC session duration in real-world NR/LTE networks},
  volume       = {17},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Empirical loss weight optimization for PINN modeling laser
bio-effects on human skin for the 1D heat equation. <em>MLA</em>,
<em>16</em>, 100563. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of deep neural networks towards solving problems in science and engineering has demonstrated encouraging results with the recent formulation of physics-informed neural networks (PINNs). Through the development of refined machine learning techniques, the high computational cost of obtaining numerical solutions for partial differential equations governing complicated physical systems can be mitigated. However, solutions are not guaranteed to be unique, and are subject to uncertainty caused by the choice of network model parameters. For critical systems with significant consequences for errors, assessing and quantifying this model uncertainty is essential. In this paper, an application of PINN for laser bio-effects with limited training data is provided for uncertainty quantification analysis. Additionally, an efficacy study is performed to investigate the impact of the relative weights of the loss components of the PINN and how the uncertainty in the predictions depends on these weights. Network ensembles are constructed to empirically investigate the diversity of solutions across an extensive sweep of hyper-parameters to determine the model that consistently reproduces a high-fidelity numerical simulation.},
  archive      = {J_MLA},
  author       = {Jenny Farmer and Chad A. Oian and Brett A. Bowman and Taufiquar Khan},
  doi          = {10.1016/j.mlwa.2024.100563},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100563},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Empirical loss weight optimization for PINN modeling laser bio-effects on human skin for the 1D heat equation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying losers: Automatic identification of
growth-stunted salmon in aquaculture using computer vision.
<em>MLA</em>, <em>16</em>, 100562. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the production of salmonids in aquaculture, it is common to observe growth-stunted individuals. The cause for the so-called “loser fish syndrome” is unclear, which needs further investigation. Here, we present and compare computer vision systems for the automatic detection and classification of loser fish in Atlantic salmon images taken in sea cages. We evaluated two end-to-end approaches (combined detection and classification) based on YoloV5 and YoloV7, and a two-stage approach based on transfer learning for detection and an ensemble of classifiers (e.g., linear perception, Adaline, C-support vector, K-nearest neighbours, and multi-layer perceptron) for classification. To our knowledge, the use of an ensemble of classifiers, considering consolidated classifiers proposed in the literature, has not been applied to this problem before. Classification entailed the assigning of every fish to a healthy and a loser class. The results of the automatic classification were compared to the reliability of human classification. The best-performing computer vision approach was based on YoloV7, which reached a precision score of 86.30%, a recall score of 71.75%, and an F1 score of 78.35%. YoloV5 presented a precision of 79.7%, while the two-stage approach reached a precision of 66.05%. Human classification had a substantial agreement strength (Fleiss’ Kappa score of 0.68), highlighting that evaluation by a human is subjective. Our proposed automatic detection and classification system will enable farmers and researchers to follow the abundance of losers throughout the production period. We provide our dataset of annotated salmon images for further research.},
  archive      = {J_MLA},
  author       = {Kana Banno and Filipe Marcel Fernandes Gonçalves and Clara Sauphar and Marianna Anichini and Aline Hazelaar and Linda Helen Sperre and Christian Stolz and Grete Hansen Aas and Lars Christian Gansel and Ricardo da Silva Torres},
  doi          = {10.1016/j.mlwa.2024.100562},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100562},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Identifying losers: Automatic identification of growth-stunted salmon in aquaculture using computer vision},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning feature importance selection for predicting
aboveground biomass in african savannah with landsat 8 and ALOS PALSAR
data. <em>MLA</em>, <em>16</em>, 100561. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In remote sensing, multiple input bands are derived from various sensors covering different regions of the electromagnetic spectrum. Each spectral band plays a unique role in land use/land cover characterization. For example, while integrating multiple sensors for predicting aboveground biomass (AGB) is important for achieving high accuracy, reducing the dataset size by eliminating redundant and irrelevant spectral features is essential for enhancing the performance of machine learning algorithms. This accelerates the learning process, thereby developing simpler and more efficient models. Our results indicate that compared individual sensor datasets, the random forest (RF) classification approach using recursive feature elimination (RFE) increased the accuracy based on F score by 82.86 % and 26.19 respectively. The mutual information regression (MIR) method shows a slight increase in accuracy when considering individual sensor datasets, but its accuracy decreases when all features are taken into account for all models. Overall, the combination of features from the Landsat 8, ALOS PALSAR backscatter, and elevation data selected based on RFE provided the best AGB estimation for the RF and XGBoost models. In contrast to the k-nearest neighbors (KNN) and support vector machines (SVM), no significant improvement in AGB estimation was detected even when RFE and MIR were used. The effect of parameter optimization was found to be more significant for RF than for all the other methods. The AGB maps show patterns of AGB estimates consistent with those of the reference dataset. This study shows how prediction errors can be minimized based on feature selection using different ML classifiers.},
  archive      = {J_MLA},
  author       = {Sa&#39;ad Ibrahim and Heiko Balzter and Kevin Tansey},
  doi          = {10.1016/j.mlwa.2024.100561},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100561},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning feature importance selection for predicting aboveground biomass in african savannah with landsat 8 and ALOS PALSAR data},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applications of machine learning in surge prediction for
vehicle turbochargers. <em>MLA</em>, <em>16</em>, 100560. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surging in vehicle turbochargers is an important phenomenon that can damage the compressor and its peripheral equipment due to pressure fluctuations and vibration, so it is essential to understand the operating points where surging occurs. In this paper, we constructed a Neural Network (NN) that can predict these operating points, using as explanatory variables the geometry parameters of the vehicle turbocharger and one-dimensional predictions of the flow rates at surge. Our contribution is the use of machine learning to enable fast and low-cost prediction of surge points, which is usually only available through experiments or calculation-intensive Computational Fluid Dynamics (CFD). Evaluations conducted on the test data revealed that prediction accuracy was poor for some turbocharger geometries and operating conditions, and that this was associated with the relatively small data quantity included in the training data. Expanding the appropriate data offers some prospect of improving prediction accuracy.},
  archive      = {J_MLA},
  author       = {Hiroki Saito and Dai Kanzaki and Kazuo Yonekura},
  doi          = {10.1016/j.mlwa.2024.100560},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100560},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Applications of machine learning in surge prediction for vehicle turbochargers},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explaining vulnerabilities of heart rate biometric models
securing IoT wearables. <em>MLA</em>, <em>16</em>, 100559. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of health informatics, extensive research has been conducted to predict diseases and extract valuable insights from patient data. However, a significant gap exists in addressing privacy concerns associated with data collection. Therefore, there is an urgent need to develop a machine-learning authentication model to secure the patients’ data seamlessly and continuously, as well as to find potential explanations when the model may fail. To address this challenge, we propose a unique approach to secure patients’ data using novel eigenheart features calculated from coarse-grained heart rate data. Various statistical and visualization techniques are utilized to explain the potential vulnerabilities of the model. Though it is feasible to develop continuous user authentication models from readily available heart rate data with reasonable performance, they are affected by factors such as age and Body Mass Index (BMI). These factors will be crucial for developing a more robust authentication model in the future.},
  archive      = {J_MLA},
  author       = {Chi-Wei Lien and Sudip Vhaduri and Sayanton V. Dibbo and Maliha Shaheed},
  doi          = {10.1016/j.mlwa.2024.100559},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100559},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Explaining vulnerabilities of heart rate biometric models securing IoT wearables},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TeenyTinyLlama: Open-source tiny language models trained in
brazilian portuguese. <em>MLA</em>, <em>16</em>, 100558. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development.},
  archive      = {J_MLA},
  author       = {Nicholas Kluge Corrêa and Sophia Falk and Shiza Fatimah and Aniket Sen and Nythamar De Oliveira},
  doi          = {10.1016/j.mlwa.2024.100558},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100558},
  shortjournal = {Mach. Learn. Appl.},
  title        = {TeenyTinyLlama: Open-source tiny language models trained in brazilian portuguese},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using ChatGPT to annotate a dataset: A case study in
intelligent tutoring systems. <em>MLA</em>, <em>16</em>, 100557. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models like ChatGPT can learn in-context (ICL) from examples. Studies showed that, due to ICL, ChatGPT achieves impressive performance in various natural language processing tasks. However, to the best of our knowledge, this is the first study that assesses ChatGPT&#39;s effectiveness in annotating a dataset for training instructor models in intelligent tutoring systems (ITSs). The task of an ITS instructor model is to automatically provide effective tutoring instruction given a student&#39;s state, mimicking human instructors. These models are typically implemented as hardcoded rules, requiring expertise, and limiting their ability to generalize and personalize instructions. These problems could be mitigated by utilizing machine learning (ML). However, developing ML models requires a large dataset of student states annotated by corresponding tutoring instructions. Using human experts to annotate such a dataset is expensive, time-consuming, and requires pedagogical expertise. Thus, this study explores ChatGPT&#39;s potential to act as a pedagogy expert annotator. Using prompt engineering, we created a list of instructions a tutor could recommend to a student. We manually filtered this list and instructed ChatGPT to select the appropriate instruction from the list for the given student&#39;s state. We manually analyzed ChatGPT&#39;s responses that could be considered incorrectly annotated. Our results indicate that using ChatGPT as an annotator is an effective alternative to human experts. The contributions of our work are (1) a novel dataset annotation methodology for the ITS, (2) a publicly available dataset of student states annotated with tutoring instructions, and (3) a list of possible tutoring instructions.},
  archive      = {J_MLA},
  author       = {Aleksandar Vujinović and Nikola Luburić and Jelena Slivka and Aleksandar Kovačević},
  doi          = {10.1016/j.mlwa.2024.100557},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100557},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Using ChatGPT to annotate a dataset: A case study in intelligent tutoring systems},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep learning approach for maize lethal necrosis and maize
streak virus disease detection. <em>MLA</em>, <em>16</em>, 100556. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maize is an important crop cultivated in Sub-Saharan Africa, essential for food security. However, its cultivation faces significant challenges due to debilitating diseases such as Maize Lethal Necrosis (MLN) and Maize Streak Virus (MSV), which can lead to severe yield losses. Traditional plant disease diagnosis methods are often time-consuming and prone to errors, necessitating more efficient approaches. This study explores the application of deep learning, specifically Convolutional Neural Networks (CNNs), in the automatic detection and classification of maize diseases. We investigate six architectures: Basic CNN, EfficientNet V2 B0 and B1, LeNet-5, VGG-16, and ResNet50, using a dataset of 15344 images comprising MSV, MLN, and healthy maize leaves. Additionally, We performed hyperparameter tuning to improve the performance of the models and Gradient-weighted Class Activation Mapping (Grad-CAM) for model interpretability. Our results show that the EfficientNet V2 B0 model demonstrated an accuracy of 99.99% in distinguishing between healthy and disease-infected plants. The results of this study contribute to the advancement of AI applications in agriculture, particularly in diagnosing maize diseases within Sub-Saharan Africa.},
  archive      = {J_MLA},
  author       = {Tony O’Halloran and George Obaido and Bunmi Otegbade and Ibomoiye Domor Mienye},
  doi          = {10.1016/j.mlwa.2024.100556},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100556},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A deep learning approach for maize lethal necrosis and maize streak virus disease detection},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing breast cancer segmentation and classification: An
ensemble deep convolutional neural network and u-net approach on
ultrasound images. <em>MLA</em>, <em>16</em>, 100555. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is a condition where the irregular growth of breast cells occurs uncontrollably, leading to the formation of tumors. It poses a significant threat to women’s lives globally, emphasizing the need for enhanced methods of detecting and categorizing the disease. In this work, we propose an Ensemble Deep Convolutional Neural Network (EDCNN) model that exhibits superior accuracy compared to several transfer learning models and the Vision Transformer model. Our EDCNN model integrates the strengths of the MobileNet and Xception models to improve its performance in breast cancer detection and classification. We employ various preprocessing techniques, including image resizing, data normalization, and data augmentation, to prepare the data for analysis. By following these measures, the formatting is optimized, and the model’s capacity to make generalizations is improved. We trained and evaluated our proposed EDCNN model using ultrasound images, a widely available modality for breast cancer imaging. The outcomes of our experiments illustrate that the EDCNN model attains an exceptional accuracy of 87.82% on Dataset 1 and 85.69% on Dataset 2, surpassing the performance of several well-known transfer learning models and the Vision Transformer model. Furthermore, an AUC value of 0.91 on Dataset 1 highlights the robustness and effectiveness of our proposed model. Moreover, we highlight the incorporation of the Grad-CAM Explainable Artificial Intelligence (XAI) technique to improve the interpretability and transparency of our proposed model. Additionally, we performed image segmentation using the U-Net segmentation technique on the input ultrasound images. This segmentation process allowed for the identification and isolation of specific regions of interest, facilitating a more comprehensive analysis of breast cancer characteristics. In conclusion, the study presents a creative approach to detecting and categorizing breast cancer, demonstrating the superior performance of the EDCNN model compared to well-established transfer learning models. Through advanced deep learning techniques and image segmentation, this study contributes to improving diagnosis and treatment outcomes in breast cancer.},
  archive      = {J_MLA},
  author       = {Md Rakibul Islam and Md Mahbubur Rahman and Md Shahin Ali and Abdullah Al Nomaan Nafi and Md Shahariar Alam and Tapan Kumar Godder and Md Sipon Miah and Md Khairul Islam},
  doi          = {10.1016/j.mlwa.2024.100555},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100555},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing breast cancer segmentation and classification: An ensemble deep convolutional neural network and U-net approach on ultrasound images},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Anomaly detection in log-event sequences: A federated deep
learning approach and open challenges. <em>MLA</em>, <em>16</em>,
100554. (<a href="https://doi.org/10.1016/j.mlwa.2024.100554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly Detection (AD) is an important area to reliably detect malicious behavior and attacks on computer systems. Log data is a rich source of information about systems and thus provides a suitable input for AD. With the sheer amount of log data available today, for years Machine Learning (ML) and more recently Deep Learning (DL) have been applied to create models for AD. Especially when processing complex log data, DL has shown some promising results in recent research to spot anomalies. It is necessary to group these log lines into log-event sequences, to detect anomalous patterns that span over multiple log lines. This work uses a centralized approach using a Long Short-Term Memory (LSTM) model for AD as its basis which is one of the most important approaches to represent long-range temporal dependencies in log-event sequences of arbitrary length. Therefore, we use past information to predict whether future events are normal or anomalous. For the LSTM model we adapt a state of the art open source implementation called LogDeep. For the evaluation, we use a Hadoop Distributed File System (HDFS) data set, which is well studied in current research. In this paper we show that without padding, which is a commonly used preprocessing step that strongly influences the AD process and artificially improves detection results and thus accuracy in lab testing, it is not possible to achieve the same high quality of results shown in literature. With the large quantity of log data, issues arise with the transfer of log data to a central entity where model computation can be done. Federated Learning (FL) tries to overcome this problem, by learning local models simultaneously on edge devices and overcome biases due to a lack of heterogeneity in training data through exchange of model parameters and finally arrive at a converging global model. Processing log data locally takes privacy and legal concerns into account, which could improve coordination and collaboration between researchers, cyber security companies, etc., in the future. Currently, there are only few scientific publications on log-based AD which use FL. Implementing FL gives the advantage of converging models even if the log data are heterogeneously distributed among participants as our results show. Furthermore, by varying individual LSTM model parameters, the results can be greatly improved. Further scientific research will be necessary to optimize FL approaches.},
  archive      = {J_MLA},
  author       = {Patrick Himler and Max Landauer and Florian Skopik and Markus Wurzenberger},
  doi          = {10.1016/j.mlwa.2024.100554},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100554},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Anomaly detection in log-event sequences: A federated deep learning approach and open challenges},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel variational mode decomposition based convolutional
neural network for the identification of freezing of gait intervals for
patients with parkinson’s disease. <em>MLA</em>, <em>16</em>, 100553.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of gait (FoG) is a debilitating and serious motor system complication of Parkinson&#39;s disease (PD) that may expose patients to frequent falls and life-threating injuries. Several artificial and machine learning methods have been proposed for the prediction of FoG based upon a limited time-duration of sensory data, However, most of the related work has been insufficiently trained and tested on smaller datasets compromising the generalizability of the models. Further, the proposed models provided a prediction at a lower rate (e.g., every 7.8 s). In response to the above shortcomings, we propose a novel variational mode decomposition (VMD) based deep learning that is capable of efficiently inferring the occurrence of FoG at a higher time-resolution (i.e., every sampling period of 7.8 ms) and with a subject-independent accuracy up to 98.8 % outperforming the state-of-the-art architectures and the standard LSTM models. The proposed model will enable the prompt detection of FoG episodes and support PD sufferers reducing the likelihood of falls.},
  archive      = {J_MLA},
  author       = {Mohamed Shaban},
  doi          = {10.1016/j.mlwa.2024.100553},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100553},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A novel variational mode decomposition based convolutional neural network for the identification of freezing of gait intervals for patients with parkinson&#39;s disease},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning-based spatial-temporal graph neural networks
for price movement classification in crude oil and precious metal
markets. <em>MLA</em>, <em>16</em>, 100552. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we adapt three spatial-temporal graph neural network models to the unique characteristics of crude oil, gold, and silver markets for forecasting purposes. It aims to be the first to ( i ) explore the potential of spatial-temporal graph neural networks family for price forecasting of these markets, ( ii ) examine the role of attention mechanism in improving forecasting accuracy, and ( iii ) integrate various sources of predictors for better performance. Specifically, we present three distinct models: Multivariate Time Series Graph Neural Networks with Temporal Attention and Learnable Adjacency matrix (MTGNN-TAttLA), Spatial Attention Graph with Temporal Convolutional Networks (SAG-TCN), and Attention-based Spatial-Temporal Graph Convolutional Networks (ASTGCN), to capture the intricate interplay of spatial and temporal dependencies within crude oil and precious metals markets. Moreover, the effectiveness of the attention mechanism in improving models&#39; accuracies is shown. Our empirical results reveal remarkable prediction accuracy, with all three models outperforming conventional deep learning methods such as Temporal Convolutional Networks (TCN), long short-term memory networks (LSTM) and convolutional neural networks (CNN). The MTGNN-TAttLA model, enriched with a temporal attention mechanism, exhibits exceptional performance in predicting the direction of price movement in the WTI, Brent, and silver markets, while ASTGCN is the best-performing model for the gold market. Additionally, we observed that incorporating technical indicators from the crude oil and precious metal markets into the graph structure has improved the classification accuracy of spatial-temporal graph neural networks.},
  archive      = {J_MLA},
  author       = {Parisa Foroutan and Salim Lahmiri},
  doi          = {10.1016/j.mlwa.2024.100552},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100552},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Deep learning-based spatial-temporal graph neural networks for price movement classification in crude oil and precious metal markets},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of streamflow predictions from LSTM models in
water- and energy-limited regions in the united states. <em>MLA</em>,
<em>16</em>, 100551. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100551">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of Long Short-Term Memory (LSTM) models for streamflow predictions has been an area of rapid development, supported by advancements in computing technology, increasing availability of spatiotemporal data, and availability of historical data that allows for training data-driven LSTM models. Several studies have focused on improving the performance of LSTM models; however, few studies have assessed the applicability of these LSTM models across different hydroclimate regions. This study investigated the single-basin trained local (one model for each basin), multi-basin trained regional (one model for one region), and grand (one model for several regions) models for predicting daily streamflow in water-limited Great Basin (18 basins) and energy-limited New England (27 basins) regions in the United States using the CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) data set. The results show a general pattern of higher accuracy in daily streamflow predictions from the regional model when compared to local or grand models for most basins in the New England region. For the Great Basin region, local models provided smaller errors for most basins and substantially lower for those basins with relatively larger errors from the regional and grand models. The evaluation of one-layer and three-layer LSTM network architectures trained with 1-day lag information indicates that the addition of model complexity by increasing the number of layers may not necessarily increase the model skill for improving streamflow predictions. Findings from our study highlight the strengths and limitations of LSTM models across contrasting hydroclimate regions in the United States, which could be useful for local and regional scale decisions using standalone or potential integration of data-driven LSTM models with physics-based hydrological models.},
  archive      = {J_MLA},
  author       = {Kul Khand and Gabriel B. Senay},
  doi          = {10.1016/j.mlwa.2024.100551},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100551},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Evaluation of streamflow predictions from LSTM models in water- and energy-limited regions in the united states},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spam detection for youtube video comments using machine
learning approaches. <em>MLA</em>, <em>16</em>, 100550. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning models have the ability to streamline the process by which Youtube video comments are filtered between legitimate comments (ham) and spam. In order to integrate machine learning models into regular usage on media-sharing platforms, recent approaches have aimed to develop models trained on Youtube comments, which have emerged as valuable tools for the classification and have enabled the identification of spam content and enhancing user experience. In this paper, eight machine learning approaches are applied to spam detection for YouTube comments. The eight machine learning models include Gaussian Naive Bayes, logistic regression, K-nearest neighbors (KNN) classifier, multi-layer perceptron (MLP), support vector machine (SVM) classifier, random forest classifier, decision tree classifier, and voting classifier. All eight models perform very well, specifically random forest approach can achieve almost perfect performance with average precision of 100% and AUC-ROC of 0.9841. The computational complexity of the eight machine learning approaches are compared.},
  archive      = {J_MLA},
  author       = {Andrew S. Xiao and Qilian Liang},
  doi          = {10.1016/j.mlwa.2024.100550},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100550},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Spam detection for youtube video comments using machine learning approaches},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). INSTRAS: INfrared spectroscopic imaging-based TRAnsformers
for medical image segmentation. <em>MLA</em>, <em>16</em>, 100549. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100549">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared (IR) spectroscopic imaging is of potentially wide use in medical imaging applications due to its ability to capture both chemical and spatial information. This complexity of the data both necessitates using machine intelligence as well as presents an opportunity to harness a high-dimensionality data set that offers far more information than today’s manually-interpreted images. While convolutional neural networks (CNNs), including the well-known U-Net model, have demonstrated impressive performance in image segmentation, the inherent locality of convolution limits the effectiveness of these models for encoding IR data, resulting in suboptimal performance. In this work, we propose an INfrared Spectroscopic imaging-based TRAnsformers for medical image Segmentation (INSTRAS). This novel model leverages the strength of the transformer encoders to segment IR breast images effectively. Incorporating skip-connection and transformer encoders, INSTRAS overcomes the issue of pure convolution models, such as the difficulty of capturing long-range dependencies. To evaluate the performance of our model and existing convolutional models, we conducted training on various encoder–decoder models using a breast dataset of IR images. INSTRAS, utilizing 9 spectral bands for segmentation, achieved a remarkable AUC score of 0.9788, underscoring its superior capabilities compared to purely convolutional models. These experimental results attest to INSTRAS’s advanced and improved segmentation abilities for IR imaging.},
  archive      = {J_MLA},
  author       = {Hangzheng Lin and Kianoush Falahkheirkhah and Volodymyr Kindratenko and Rohit Bhargava},
  doi          = {10.1016/j.mlwa.2024.100549},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100549},
  shortjournal = {Mach. Learn. Appl.},
  title        = {INSTRAS: INfrared spectroscopic imaging-based TRAnsformers for medical image segmentation},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A machine learning approach feature to forecast the future
performance of the universities in canada. <em>MLA</em>, <em>16</em>,
100548. (<a href="https://doi.org/10.1016/j.mlwa.2024.100548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {University ranking is a technique of measuring the performance of Higher Education Institutions (HEIs) by evaluating them on various criteria like student satisfaction, expenditure, research and teaching quality, citation count, grants, and enrolment. Ranking has been determined as a vital factor that helps students decide which institution to attend. Hence, universities seek to increase their overall rank and use these measures of success in their marketing communications and prominently place their ranked status on their institution&#39;s websites. Despite decades of research on ranking methods, a limited number of studies have leveraged predictive analytics and machine learning to rank universities. In this article, we collected 49 Canadian universities’ data for 2017–2021 and divided them based on Maclean&#39;s categories into Primarily Undergraduate, Comprehensive, and Medical/Doctoral Universities. After identifying the input and output components, we leveraged various feature engineering and machine learning techniques to predict the universities’ ranks. We used Pearson Correlation, Feature Importance, and Chi-Square as the feature engineering methods, and the results show that “student to faculty ratio,” “total number of citations”, and “total number of Grants” are the most important factors in ranking Canadian universities. Also, the Random Forest machine learning model for the “primarily undergraduate category,” the Voting classifier model for the “comprehensive category” and the Gradient Boosting model for the “medical/doctoral category” performed the best. The selected machine learning models were evaluated based on accuracy, precision, F1 score, and recall.},
  archive      = {J_MLA},
  author       = {Leslie J. Wardley and Enayat Rajabi and Saman Hassanzadeh Amin and Monisha Ramesh},
  doi          = {10.1016/j.mlwa.2024.100548},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100548},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A machine learning approach feature to forecast the future performance of the universities in canada},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Augmenting roadway safety with machine learning and deep
learning: Pothole detection and dimension estimation using in-vehicle
technologies. <em>MLA</em>, <em>16</em>, 100547. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100547">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and estimation of pothole dimensions is an essential step in road maintenance. Aging, heavy rainfall, traffic, and weak underlying layers may cause pavement potholes. Potholes can cause accidents when drivers lose control after hitting or swerving to avoid them, which may lead to injuries or fatal crashes. Also, potholes may result in property damages, such as flat tires, scrapes, dents, and leaks. Additionally, potholes are costly; for example, in the United States, potholes cost drivers about $3 Billion annually. Traditional ways of attending to potholes involve field surveys carried out by skilled personnel to determine their sizes for quantity and cost estimates. This process is expensive, prone to errors, subjectivity, unsafe, and time-consuming. Some authorities use sensor vehicles to carry out the surveys, a method that is accurate, safer, and faster than the traditional approach but much more expensive; therefore, not all authorities can afford them. To avoid these challenges, a modern, real-time, cost-effective approach is proposed to ensure the efficient and fast process of pothole maintenance. This paper presents a Deep Learning model trained using the You Only Look Once (YOLO) algorithm to capture potholes and estimate their dimensions and locations using only built-in vehicle technologies. The model attained 93.0 % precision, 91.6 % recall, 87.0 % F1-score, and 96.3 % mAP. A statistical analysis of the on-site test results indicates that the results are significant at a 5 % level, with a p-value of 0.037. This approach provides an economical and faster way of monitoring road surface conditions.},
  archive      = {J_MLA},
  author       = {Cuthbert Ruseruka and Judith Mwakalonge and Gurcan Comert and Saidi Siuhi and Frank Ngeni and Quincy Anderson},
  doi          = {10.1016/j.mlwa.2024.100547},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100547},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Augmenting roadway safety with machine learning and deep learning: Pothole detection and dimension estimation using in-vehicle technologies},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A survey of malware detection using deep learning.
<em>MLA</em>, <em>16</em>, 100546. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100546">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of malicious software (malware) detection and classification is a complex task, and there is no perfect approach. There is still a lot of work to be done. Unlike most other research areas, standard benchmarks are difficult to find for malware detection. This paper aims to investigate recent advances in malware detection on MacOS, Windows, iOS, Android, and Linux using deep learning (DL) by investigating DL in text and image classification, the use of pre-trained and multi-task learning models for malware detection approaches to obtain high accuracy and which the best approach if we have a standard benchmark dataset. We discuss the issues and the challenges in malware detection using DL classifiers by reviewing the effectiveness of these DL classifiers and their inability to explain their decisions and actions to DL developers presenting the need to use Explainable Machine Learning (XAI) or Interpretable Machine Learning (IML) programs. Additionally, we discuss the impact of adversarial attacks on deep learning models, negatively affecting their generalization capabilities and resulting in poor performance on unseen data. We believe there is a need to train and test the effectiveness and efficiency of the current state-of-the-art deep learning models on different malware datasets. We examine eight popular DL approaches on various datasets. This survey will help researchers develop a general understanding of malware recognition using deep learning.},
  archive      = {J_MLA},
  author       = {Ahmed Bensaoud and Jugal Kalita and Mahmoud Bensaoud},
  doi          = {10.1016/j.mlwa.2024.100546},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100546},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A survey of malware detection using deep learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The dark side of language models: Exploring the potential of
LLMs in multimedia disinformation generation and dissemination.
<em>MLA</em>, <em>16</em>, 100545. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disinformation - the deliberate spread of false or misleading information poses a significant threat to our society by undermining trust, exacerbating polarization, and manipulating public opinion. With the rapid advancement of artificial intelligence and the growing prominence of large language models (LLMs) such as ChatGPT, new avenues for the dissemination of disinformation are emerging. This review paper explores the potential of LLMs to initiate the generation of multi-media disinformation, encompassing text, images, audio, and video. We begin by examining the capabilities of LLMs, highlighting their potential to create compelling, context-aware content that can be weaponized for malicious purposes. Subsequently, we examine the nature of disinformation and the various mechanisms through which it spreads in the digital landscape. Utilizing these advanced models, malicious actors can automate and scale up disinformation effectively. We describe a theoretical pipeline for creating and disseminating disinformation on social media. Existing interventions to combat disinformation are also reviewed. While these efforts have shown success, we argue that they need to be strengthened to effectively counter the escalating threat posed by LLMs. Digital platforms have, unfortunately, enabled malicious actors to extend the reach of disinformation. The advent of LLMs poses an additional concern as they can be harnessed to significantly amplify the velocity, variety, and volume of disinformation. Thus, this review proposes augmenting current interventions with AI tools like LLMs, capable of assessing information more swiftly and comprehensively than human fact-checkers. This paper illuminates the dark side of LLMs and highlights their potential to be exploited as disinformation dissemination tools.},
  archive      = {J_MLA},
  author       = {Dipto Barman and Ziyi Guo and Owen Conlan},
  doi          = {10.1016/j.mlwa.2024.100545},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100545},
  shortjournal = {Mach. Learn. Appl.},
  title        = {The dark side of language models: Exploring the potential of LLMs in multimedia disinformation generation and dissemination},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient surrogate models for materials science
simulations: Machine learning-based prediction of microstructure
properties. <em>MLA</em>, <em>16</em>, 100544. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining, understanding, and predicting the so-called structure–property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures from the Cahn–Hilliard model. We analyze the accuracy and robustness of all models and elucidate the reasons for the differences in their performances. The impact of including domain knowledge through tailored features is studied, and general recommendations based on the availability and quality of training data are derived from this.},
  archive      = {J_MLA},
  author       = {Binh Duong Nguyen and Pavlo Potapenko and Aytekin Demirci and Kishan Govind and Sébastien Bompas and Stefan Sandfeld},
  doi          = {10.1016/j.mlwa.2024.100544},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100544},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Efficient surrogate models for materials science simulations: Machine learning-based prediction of microstructure properties},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An automated machine learning approach for detecting
anomalous peak patterns in time series data from a research watershed in
the northeastern united states critical zone. <em>MLA</em>, <em>16</em>,
100543. (<a href="https://doi.org/10.1016/j.mlwa.2024.100543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an automated machine learning framework designed to assist hydrologists in detecting anomalies in time series data generated by sensors in a research watershed in the northeastern United States critical zone. The framework specifically focuses on identifying peak-pattern anomalies, which may arise from sensor malfunctions or natural phenomena. However, the use of classification methods for anomaly detection poses challenges, such as the requirement for labeled data as ground truth and the selection of the most suitable deep learning model for the given task and dataset. To address these challenges, our framework generates labeled datasets by injecting synthetic peak patterns into synthetically generated time series data and incorporates an automated hyperparameter optimization mechanism. This mechanism generates an optimized model instance with the best architectural and training parameters from a pool of five selected models, namely Temporal Convolutional Network (TCN), InceptionTime, MiniRocket, Residual Networks (ResNet), and Long Short-Term Memory (LSTM). The selection is based on the user’s preferences regarding anomaly detection accuracy and computational cost. The framework employs Time-series Generative Adversarial Networks (TimeGAN) as the synthetic dataset generator. The generated model instances are evaluated using a combination of accuracy and computational cost metrics, including training time and memory, during the anomaly detection process. Performance evaluation of the framework was conducted using a dataset from a watershed, demonstrating consistent selection of the most fitting model instance that satisfies the user’s preferences.},
  archive      = {J_MLA},
  author       = {Ijaz Ul Haq and Byung Suk Lee and Donna M. Rizzo and Julia N. Perdrial},
  doi          = {10.1016/j.mlwa.2024.100543},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100543},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An automated machine learning approach for detecting anomalous peak patterns in time series data from a research watershed in the northeastern united states critical zone},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Best performance with fewest resources: Unveiling the most
resource-efficient convolutional neural network for p300 detection with
the aid of explainable AI. <em>MLA</em>, <em>16</em>, 100542. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have shown remarkable prowess in detecting P300, an Event-Related Potential (ERP) crucial in Brain–Computer Interfaces (BCIs). Researchers persistently seek simple and efficient CNNs for P300 detection, exemplified by models like DeepConvNet, EEGNet, and SepConv1D. Noteworthy progress has been made, manifesting in reducing parameters from millions to hundreds while sustaining state-of-the-art performance. However, achieving further simplification or performance improvement beyond SepConv1D appears challenging due to inherent oversimplification. This study explores landmark CNNs and P300 data with the aid of Explainable AI, proposing a simpler yet superior-performing CNN architecture which incorporates (1) precise separable convolution for feature extraction of P300 data, (2) adaptive activation function tailored for P300 data, and (3) customized large learning rate schedules for training P300 data. Termed the Minimalist CNN for P300 detection (P300MCNN), this novel model is characterized by its requirement of the fewest filters and epochs to date, concurrently achieving best performance in cross-subject P300 detection. P300MCNN not only introduces groundbreaking concepts for CNN architectures in P300 detection but also showcases the importance of Explainable AI in demystifying the “black box” design of CNNs.},
  archive      = {J_MLA},
  author       = {Maohua Liu and Wenchong Shi and Liqiang Zhao and Fred R. Beyette Jr.},
  doi          = {10.1016/j.mlwa.2024.100542},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100542},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Best performance with fewest resources: Unveiling the most resource-efficient convolutional neural network for p300 detection with the aid of explainable AI},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChatGPT: A meta-analysis after 2.5 months. <em>MLA</em>,
<em>16</em>, 100541. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and media attention since its release in November 2022. However, little hard evidence is available regarding its perception in various sources. In this paper, we analyze over 300,000 tweets and more than 150 scientific papers to investigate how ChatGPT is perceived and discussed. Our findings show that ChatGPT is generally viewed as of high quality, with positive sentiment and emotions of joy dominating social media. Its perception has slightly decreased since its debut, however, with joy decreasing and (negative) surprise on the rise, and it is perceived more negatively in languages other than English. In recent scientific papers, ChatGPT is characterized as a great opportunity across various fields including the medical domain, but also as a threat concerning ethics and receives mixed assessments for education. Our comprehensive meta-analysis of ChatGPT’s perception after 2.5 months since its release can contribute to shaping the public debate and informing its future development. We make our data available. 1},
  archive      = {J_MLA},
  author       = {Christoph Leiter and Ran Zhang and Yanran Chen and Jonas Belouadi and Daniil Larionov and Vivian Fresen and Steffen Eger},
  doi          = {10.1016/j.mlwa.2024.100541},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100541},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ChatGPT: A meta-analysis after 2.5 months},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “new interpretation of GNRVⓇ knee arthrometer
results for ACL injury diagnosis support using machine learning”
[machine learning with applications 13 (2023) 100480]. <em>MLA</em>,
<em>16</em>, 100540. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLA},
  author       = {Jean Mouchotte and Matthieu LeBerre and Théo Cojean and Henri Robert},
  doi          = {10.1016/j.mlwa.2024.100540},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100540},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Erratum to “New interpretation of GNRVⓇ knee arthrometer results for ACL injury diagnosis support using machine learning” [Machine learning with applications 13 (2023) 100480]},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for sports betting: Should model selection
be based on accuracy or calibration? <em>MLA</em>, <em>16</em>, 100539.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100539">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports betting’s recent federal legalisation in the USA coincides with the golden age of machine learning. If bettors can leverage data to reliably predict the probability of an outcome, they can recognise when the bookmaker’s odds are in their favour. As sports betting is a multi-billion dollar industry in the USA alone, identifying such opportunities could be extremely lucrative. Many researchers have applied machine learning to the sports outcome prediction problem, generally using accuracy to evaluate the performance of predictive models. We hypothesise that for the sports betting problem, model calibration is more important than accuracy. To test this hypothesis, we train models on NBA data over several seasons and run betting experiments on a single season, using published odds. We show that using calibration, rather than accuracy, as the basis for model selection leads to greater returns, on average (return on investment of +34.69% versus -35.17%) and in the best case (+36.93% versus +5.56%). These findings suggest that for sports betting (or any probabilistic decision-making problem), calibration is a more important metric than accuracy. Sports bettors who wish to increase profits should therefore select their predictive model based on calibration, rather than accuracy.},
  archive      = {J_MLA},
  author       = {Conor Walsh and Alok Joshi},
  doi          = {10.1016/j.mlwa.2024.100539},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100539},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning for sports betting: Should model selection be based on accuracy or calibration?},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate detection of cell deformability tracking in
hydrodynamic flow by coupling unsupervised and supervised learning.
<em>MLA</em>, <em>16</em>, 100538. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The using of deep learning methods in medical images has been successfully used for various applications, including cell segmentation and deformability detection, thereby contributing significantly to advancements in medical analysis. Cell deformability is a fundamental criterion, which must be measured easily and accurately. One common approach for measuring cell deformability is to use microscopy techniques. Recent works have been efforts to develop more advanced and automated methods for measuring cell deformability based on microscopic images, but cell membrane segmentation techniques are still difficult to achieve with precision because of the quality of images. In this paper, we introduce a novel algorithm for cell segmentation that addresses the challenge of microscopic images. AD-MSC cells were controlled by a microfluidic-based system and cell images were acquired by an ultra-fast camera with variable frequency connected to a powerful computer to collect data. The proposed algorithm has a combination of two main components: denoising images using unsupervised learning and cell segmentation and deformability detection using supervised learning which aim to enhance image quality without the need for expensive materials and expert intervention and segment cell deformability with more precision. The contribution of this paper is the combination of two neural networks that treat the database more easily and without the presence of experts. This approach is used to have faster results with high performance according to low datasets from microscopy even with noisy microscopic images. The precision increases to 81 % when we combine DAE with U-Net, compared to 78 % when adding VAE to U-Net and 59 % when using only U-Net.},
  archive      = {J_MLA},
  author       = {Imen Halima and Mehdi Maleki and Gabriel Frossard and Celine Thomann and Edwin-Joffrey Courtial},
  doi          = {10.1016/j.mlwa.2024.100538},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100538},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Accurate detection of cell deformability tracking in hydrodynamic flow by coupling unsupervised and supervised learning},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient machine learning-assisted failure analysis method
for circuit-level defect prediction. <em>MLA</em>, <em>16</em>, 100537.
(<a href="https://doi.org/10.1016/j.mlwa.2024.100537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integral to the success of transistor advancements is the accurate use of failure analysis (FA) which benefits in fine-tuning and optimization of the fabrication processes. However, the chip makers face several FA challenges as device sizes, structure, and material complexities scale dramatically. To sustain manufacturability, one can accelerate defect identification at all steps of the chip processing and design. On the other hand, as technologies scale below the nanometer nodes, devices are more sensitive to unavoidable process-induced variability. Therefore, metallic defects and process-induced variability need to be treated concurrently in the context of chip scaling, while failure diagnostic methods to decouple the effects should be developed. Indeed, the locating a defective component from thousands of circuits in a microchip in the presence of variability is a tedious task. This work shows how the SPICE circuit simulations coupled with machine learning based-physical modeling should be effectively used to tackle such a problem for a 6T-SRAM bit cell. An automatic bridge defect recognition system for such a circuit is devised by training a predictive model on simulation data. For feature descriptors of the model, the symmetry of the circuit and a fundamental material property are leveraged: metals (semiconductors) have a positive (negative) temperature coefficient of resistance up to a certain voltage range. Then, this work successfully demonstrates that how a defective circuit is identified along with its defective component&#39;s position with approximately 99.5 % accuracy. This proposed solution should greatly help to accelerate the production process of the integrated circuits.},
  archive      = {J_MLA},
  author       = {Joydeep Ghosh},
  doi          = {10.1016/j.mlwa.2024.100537},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100537},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Efficient machine learning-assisted failure analysis method for circuit-level defect prediction},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Case-base neural network: Survival analysis with
time-varying, higher-order interactions. <em>MLA</em>, <em>16</em>,
100535. (<a href="https://doi.org/10.1016/j.mlwa.2024.100535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of survival analysis, data-driven neural network-based methods have been developed to model complex covariate effects. While these methods may provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that includes time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the full hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN outperforming competitors. Then, we apply all methods to three real data applications, with CBNNs outperforming the competing models in two studies and showing similar performance in the third. Our results highlight the benefit of combining case-base sampling with deep learning to provide a simple and flexible framework for data-driven modeling of single event survival outcomes that estimates time-varying effects and a complex baseline hazard by design. An R package is available at https://github.com/Jesse-Islam/cbnn .},
  archive      = {J_MLA},
  author       = {Jesse Islam and Maxime Turgeon and Robert Sladek and Sahir Bhatnagar},
  doi          = {10.1016/j.mlwa.2024.100535},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100535},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Case-base neural network: Survival analysis with time-varying, higher-order interactions},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GPT classifications, with application to credit lending.
<em>MLA</em>, <em>16</em>, 100534. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Pre-trained Transformers (GPT) and Large language models (LLMs) have made significant advancements in natural language processing in recent years. The practical applications of LLMs are undeniable, rendering moot any debate about their impending influence. The power of LLMs has made them similar to machine learning models for decision-making problems. In this paper, we focus on binary classification which is a common use of ML models, particularly in credit lending applications. We show how a GPT model can perform almost as accurately as a classical logistic machine learning model but with a much lower number of sample observations. In particular, we show how, in the context of credit lending, LLMs can be improved and reach performances similar to classical logistic regression models using only a small set of examples.},
  archive      = {J_MLA},
  author       = {Golnoosh Babaei and Paolo Giudici},
  doi          = {10.1016/j.mlwa.2024.100534},
  journal      = {Machine Learning with Applications},
  month        = {6},
  pages        = {100534},
  shortjournal = {Mach. Learn. Appl.},
  title        = {GPT classifications, with application to credit lending},
  volume       = {16},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “automated recognition of individual performers
from de-identified video sequences” [machine learning with applications
11 (2023) 100450]. <em>MLA</em>, <em>15</em>, 100533. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLA},
  author       = {Zizui Chen and Stephen Czarnuch and Erica Dove and Arlene Astell},
  doi          = {10.1016/j.mlwa.2024.100533},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100533},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Erratum to “Automated recognition of individual performers from de-identified video sequences” [Machine learning with applications 11 (2023) 100450]},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “a BCI system for imagined bengali speech
recognition” [machine learning with applications 13 (2023) 100486].
<em>MLA</em>, <em>15</em>, 100532. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLA},
  author       = {Arman Hossain and Kathak Das and Protima Khan and Md. Fazlul Kader},
  doi          = {10.1016/j.mlwa.2024.100532},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100532},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Erratum to “A BCI system for imagined bengali speech recognition” [Machine learning with applications 13 (2023) 100486]},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural network prediction of the effect of thermomechanical
controlled processing on mechanical properties. <em>MLA</em>,
<em>15</em>, 100531. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The as-rolled mechanical properties of microalloyed steels result from their chemical composition and thermomechanical processing history. Accurate predictions of the mechanical properties would reduce the need for expensive and time-consuming testing. At the same time, understanding the interplay between process variables and alloy composition will help reduce product variability and facilitate future alloy design. This paper provides an artificial neural network methodology to predict lower yield strength (LYS) and ultimate tensile strength (UTS). The proposed method uses feature engineering to transform raw data into features typically used in physical metallurgy to better utilize the artificial neural network model in understanding the process. SHAP values are used to reveal the effect of thermomechanical controlled processing, which can be rationalized by physical metallurgy theory.},
  archive      = {J_MLA},
  author       = {Sushant Sinha and Denzel Guye and Xiaoping Ma and Kashif Rehman and Stephen Yue and Narges Armanfard},
  doi          = {10.1016/j.mlwa.2024.100531},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100531},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Neural network prediction of the effect of thermomechanical controlled processing on mechanical properties},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing data efficiency for autonomous vehicles: Using
data sketches for detecting driving anomalies. <em>MLA</em>,
<em>15</em>, 100530. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models for near collision detection in autonomous vehicles promise enhanced predictive power . However, training on these large datasets presents storage and computational challenges, particularly when operated on conventional computing systems. This paper addresses the problem of training anomaly detection models from large-scale vehicle trajectory datasets and adopts a reservoir sampling-based data sketching technique. Predetermined subset sizes ranging from 0.4% to 100% of the original data are utilized, A single-pass reservoir sampling algorithm is then applied to construct these data subsets efficiently. Subsequently, a Support Vector Machine (SVM) model is trained on these subsets, and its performance is assessed by various metrics, including accuracy, precision, recall, and F1-score. Experimental outcomes on the HighD dataset, a comprehensive real-world collection of vehicle trajectories, confirm that our approach can achieve robust near-collision detection. With a full dataset, our model achieved an F1-score of 0.9998 for class 0 and 0.9984 for class 1. When the data was reduced to as low as 0.4% of the original size, the F1-score for class 0 remained at 0.9998 and 0.7143 for class 1. This demonstrates a capability to maintain a relatively high performance even with a 99.6% reduction in data size. Moreover, precision and recall values ranged from 71.3% to 0.999 across varying sketch sizes.},
  archive      = {J_MLA},
  author       = {Debbie Aisiana Indah and Judith Mwakalonge and Gurcan Comert and Saidi Siuhi},
  doi          = {10.1016/j.mlwa.2024.100530},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100530},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Enhancing data efficiency for autonomous vehicles: Using data sketches for detecting driving anomalies},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wearables to detect independent variables, objective task
performance, and metacognitive states. <em>MLA</em>, <em>15</em>,
100529. (<a href="https://doi.org/10.1016/j.mlwa.2024.100529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable biometric tracking devices are becoming increasingly common, providing users with physiological metrics such as heart rate variability (HRV) and skin conductance . We hypothesize that these metrics can be used as inputs for machine learning models to detect independent variables, such as target prevalence or hours awake, objective task performance, and metacognitive states. Over the course of 1–25 h awake, 40 participants completed four sessions of a simulated mine hunting task while non-invasive wearables collected physiological and behavioral data. The collected data were used to generate multiple machine learning models to detect the independent variables of the experiment (e.g., time awake and target prevalence), objective task performance, or metacognitive states. The strongest generated model was the time awake detection model (area under the curve = 0.92). All other models performed much closer to chance (area under the curve = 0.57–0.66), suggesting the model architecture used in this paper can detect time awake but falls short in other domains.},
  archive      = {J_MLA},
  author       = {Matthew S. Daley and Jeffrey B. Bolkhovsky and Rachel Markwald and Timothy Dunn},
  doi          = {10.1016/j.mlwa.2024.100529},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100529},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Wearables to detect independent variables, objective task performance, and metacognitive states},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parsimonious bayesian model-based clustering with
dissimilarities. <em>MLA</em>, <em>15</em>, 100528. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering techniques are used to group observations and discover interesting patterns within data. Model-based clustering is one such method that is often an attractive choice due to the specification of a generative model for the given data and the ability to calculate model-selection criteria, which is in turn used to select the number of clusters. However, when only distances between observations are available, model-based clustering can no longer be used, and heuristic algorithms without the aforementioned advantages are usually used instead. As a solution, Oh and Raftery (2007) suggest a Bayesian model-based clustering method (named BMCD) that only requires a dissimilarity matrix as input, while also accounting for the measurement error that may be present within the observed data. In this paper, we extend the BMCD framework by proposing several additional models, alternative model selection criteria, and strategies for reducing computing time of the algorithm. These extensions ensure that the algorithm is effective even in high-dimensional spaces and provides a wide range of choices to the practitioner that can be used with a variety of data. Additionally, a publicly available software implementation of the algorithm is provided as a package in the R programming language.},
  archive      = {J_MLA},
  author       = {Samuel Morrissette and Saman Muthukumarana and Maxime Turgeon},
  doi          = {10.1016/j.mlwa.2024.100528},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100528},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Parsimonious bayesian model-based clustering with dissimilarities},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey, classification and critical analysis of the
literature on corporate bankruptcy and financial distress prediction.
<em>MLA</em>, <em>15</em>, 100527. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corporate bankruptcy and financial distress prediction is a topic of interest for a variety of stakeholders, including businesses, financial institutions, investors, regulatory bodies , auditors, and academics. Various statistical and artificial intelligence methodologies have been devised to produce more accurate predictions. As more researchers are now focusing on this growing field of interest, this paper provides an up-to-date comprehensive survey, classification, and critical analysis of the literature on corporate bankruptcy and financial distress predictions, including definitions of bankruptcy and financial distress, prediction methodologies and models, data pre-processing, feature selection, model implementation, performance criteria and their measures for assessing the performance of classifiers or prediction models, and methodologies for the performance evaluation of prediction models. Finally, a critical analysis of the surveyed literature is provided to inspire possible future research directions.},
  archive      = {J_MLA},
  author       = {Jinxian Zhao and Jamal Ouenniche and Johannes De Smedt},
  doi          = {10.1016/j.mlwa.2024.100527},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100527},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Survey, classification and critical analysis of the literature on corporate bankruptcy and financial distress prediction},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Programming with ChatGPT: How far can we go? <em>MLA</em>,
<em>15</em>, 100526. (<a
href="https://doi.org/10.1016/j.mlwa.2024.100526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) has made remarkable strides, giving rise to the development of large language models such as ChatGPT . The chatbot has garnered significant attention from academia, industry, and the general public, marking the beginning of a new era in AI applications . This work explores how well ChatGPT can write source code. To this end, we performed a series of experiments to assess the extent to which ChatGPT is capable of solving general programming problems. Our objective is to assess ChatGPT’s capabilities in two different programming languages, namely C++ and Java, by providing it with a set of programming problem, encompassing various types and difficulty levels . We focus on evaluating ChatGPT’s performance in terms of code correctness, run-time efficiency, and memory usage. The experimental results show that, while ChatGPT is good at solving easy and medium programming problems written in C++ and Java, it encounters some difficulties with more complicated tasks in the two languages. Compared to code written by humans, the one generated by ChatGPT is of lower quality, with respect to runtime and memory usage.},
  archive      = {J_MLA},
  author       = {Alessio Bucaioni and Hampus Ekedahl and Vilma Helander and Phuong T. Nguyen},
  doi          = {10.1016/j.mlwa.2024.100526},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100526},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Programming with ChatGPT: How far can we go?},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The butterfly effect in artificial intelligence systems:
Implications for AI bias and fairness. <em>MLA</em>, <em>15</em>,
100525. (<a href="https://doi.org/10.1016/j.mlwa.2024.100525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of the Butterfly Effect, derived from chaos theory, highlights how seemingly minor changes can lead to significant, unpredictable outcomes in complex systems. This phenomenon is particularly pertinent in the realm of AI fairness and bias. Factors such as subtle biases in initial data, deviations during algorithm training , or shifts in data distribution from training to testing can inadvertently lead to pronounced unfair results. These results often disproportionately impact marginalized groups , reinforcing existing societal inequities. Furthermore, the Butterfly Effect can magnify biases in data or algorithms, intensify feedback loops, and heighten susceptibility to adversarial attacks . Recognizing the complex interplay within AI systems and their societal ramifications, it is imperative to rigorously scrutinize any modifications in algorithms or data inputs for possible unintended effects . This paper proposes a combination of algorithmic and empirical methods to identify, measure, and counteract the Butterfly Effect in AI systems. Our approach underscores the necessity of confronting these challenges to foster equitable outcomes and ensure responsible AI evolution.},
  archive      = {J_MLA},
  author       = {Emilio Ferrara},
  doi          = {10.1016/j.mlwa.2024.100525},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100525},
  shortjournal = {Mach. Learn. Appl.},
  title        = {The butterfly effect in artificial intelligence systems: Implications for AI bias and fairness},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning nonlinear integral operators via recurrent neural
networks and its application in solving integro-differential equations.
<em>MLA</em>, <em>15</em>, 100524. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose using LSTM-RNNs (Long Short-Term Memory-Recurrent Neural Networks) to learn and represent nonlinear integral operators that appear in nonlinear integro-differential equations (IDEs). The LSTM-RNN representation of the nonlinear integral operator allows us to turn a system of nonlinear integro-differential equations into a system of ordinary differential equations for which many efficient solvers are available. Furthermore, because the use of LSTM-RNN representation of the nonlinear integral operator in an IDE eliminates the need to perform a numerical integration in each numerical time evolution step, the overall temporal cost of the LSTM-RNN-based IDE solver can be reduced to O ( n T ) O(nT) from O ( n T 2 ) O(nT2) if a n T nT -step trajectory is to be computed. We illustrate the efficiency and robustness of this LSTM-RNN-based numerical IDE solver with a model problem. Additionally, we highlight the generalizability of the learned integral operator by applying it to IDEs driven by different external forces. As a practical application, we show how this methodology can effectively solve the Dyson’s equation for quantum many-body systems.},
  archive      = {J_MLA},
  author       = {Hardeep Bassi and Yuanran Zhu and Senwei Liang and Jia Yin and Cian C. Reeves and Vojtěch Vlček and Chao Yang},
  doi          = {10.1016/j.mlwa.2023.100524},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100524},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Learning nonlinear integral operators via recurrent neural networks and its application in solving integro-differential equations},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SODRet: Instance retrieval using salient object detection
for self-service shopping. <em>MLA</em>, <em>15</em>, 100523. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-service shopping technologies have become commonplace in modern society. Although various innovative solutions have been adopted, there is still a gap in providing efficient services to consumers. Recent developments in mobile application technologies and internet-of-things devices promote information and knowledge dissemination by integrating innovative services to meet users’ needs. We argue that object retrieval applications can be used to provide effective online or self-service shopping. Therefore, to fill this technological void, this study aims to propose an object retrieval system using a fusion-based salient object detection (SOD) method. The SOD has attracted significant attention, and recently many heuristic computational models have been developed for object detection. It has been widely used in object detection and retrieval applications. This work proposes an instance retrieval system based on the SOD to find the objects from the commodity datasets. A prediction about the object’s position is made using the saliency detection system through a saliency model, and the proposed SOD-based retrieval (SODRet) framework uses saliency maps for retrieving the searched items. The method proposed in this work is evaluated on INSTRE and Flickr32 datasets. Our proposed work outperforms state-of-the-art object retrieval methods and can further be employed for large-scale self-service shopping-based points of sales.},
  archive      = {J_MLA},
  author       = {Muhammad Umair Hassan and Xiuyang Zhao and Raheem Sarwar and Naif R. Aljohani and Ibrahim A. Hameed},
  doi          = {10.1016/j.mlwa.2023.100523},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100523},
  shortjournal = {Mach. Learn. Appl.},
  title        = {SODRet: Instance retrieval using salient object detection for self-service shopping},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ChatReview: A ChatGPT-enabled natural language processing
framework to study domain-specific user reviews. <em>MLA</em>,
<em>15</em>, 100522. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent search engines including pre-trained generative transformers (GPT) have revolutionized the user search experience. Several fields including e-commerce, education, and hospitality are increasingly exploring GPT tools to study user reviews and gain critical insights to improve their service quality. However, massive user-review data and imprecise prompt engineering lead to biased, irrelevant, and impersonal search results. In addition, exposing user data to these search engines may pose privacy issues. Motivated by these factors, we present ChatReview, a ChatGPT-enabled natural language processing (NLP) framework that effectively studies domain-specific user reviews to offer relevant and personalized search results at multiple levels of granularity . The framework accomplishes this task using four phases including data collection, tokenization, query construction, and response generation . The data collection phase involves gathering domain-specific user reviews from public and private repositories. In the tokenization phase, ChatReview applies sentiment analysis to extract keywords and categorize them into various sentiment classes. This process creates a token repository that best describes the user sentiments for a given user-review data. In the query construction phase, the framework uses the token repository and domain knowledge to construct three types of ChatGPT prompts including explicit, implicit, and creative. In the response generation phase, ChatReview pipelines these prompts into ChatGPT to generate search results at varying levels of granularity. We analyze our framework using three real-world domains including education, local restaurants, and hospitality. We assert that our framework simplifies prompt engineering for general users to produce effective results while minimizing the exposure of sensitive user data to search engines. We also present a one-of-a-kind Large Language Model (LLM) peer assessment of the ChatReview framework. Specifically, we employ Google’s Bard to objectively and qualitatively analyze the various ChatReview outputs. Our Bard-based analyses yield over 90% satisfaction, establishing ChatReview as a viable survey analysis tool.},
  archive      = {J_MLA},
  author       = {Brittany Ho and Ta’Rhonda Mayberry and Khanh Linh Nguyen and Manohar Dhulipala and Vivek Krishnamani Pallipuram},
  doi          = {10.1016/j.mlwa.2023.100522},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100522},
  shortjournal = {Mach. Learn. Appl.},
  title        = {ChatReview: A ChatGPT-enabled natural language processing framework to study domain-specific user reviews},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatiotemporal integration of GCN and e-LSTM networks for
PM2.5 forecasting. <em>MLA</em>, <em>15</em>, 100521. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PM 2.5 , inhalable particles, with a size of 2.5 micrometers or less, negatively impact the environment as well as our health. Monitoring PM 2.5 is critical to guard against extreme events by alerting people and initiating actions to alleviate PM 2.5′ s impacts. Developing PM 2.5 forecasting frameworks empowers the authorities to predict extremely polluted events in advance and gives them time to implement necessary strategies in advance (e.g., Action! Days). Understanding the spatiotemporal behavior of PM 2.5 and meteorological factors is of significance for having accurate predictions. This study utilizes EPA sensor data to quantify the PM 2.5 air quality index (AQI) and meteorological factors such as temperature over 2015–2019 across Michigan, USA. Here, a spatiotemporal deep neural structure is proposed through integrating graph convolutional neural (GCN) and exogenous long short-term memory (E-LSTM) networks to incorporate spatial and temporal patterns within PM 2.5 AQI and meteorological factors for predicting PM 2.5 AQI. Results illustrate that not only does our proposed framework outperform the traditional approaches such as LSTM and E-LSTM, but also it is robust against the network structure of EPA stations. The study&#39;s findings demonstrate that the integration of GCN with E-LSTM significantly enhances the accuracy of PM 2.5 AQI predictions compared to traditional models. This advancement indicates a promising direction for environmental monitoring, offering improved forecasting tools that can aid in timely and effective decision-making for air quality management and public health protection.},
  archive      = {J_MLA},
  author       = {Ali Kamali Mohammadzadeh and Halima Salah and Roohollah Jahanmahin and Abd E Ali Hussain and Sara Masoud and Yaoxian Huang},
  doi          = {10.1016/j.mlwa.2023.100521},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100521},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Spatiotemporal integration of GCN and E-LSTM networks for PM2.5 forecasting},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving top-n recommendations using batch approximation
for weighted pair-wise loss. <em>MLA</em>, <em>15</em>, 100520. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In collaborative filtering, matrix factorization and collaborative metric learning are challenged by situations where non-preferred items may appear so close to a user in the feature embedding space that they lead to degrading the recommendation performance. We call such items ‘potential impostor’ risks. Addressing the issues with ‘potential impostor’ is important because it can result in inefficient learning and poor feature extraction. To achieve this, we propose a novel loss function formulation designed to enhance learning efficiency by actively identifying and addressing impostors, leveraging item associations and learning the distribution of negative items. This approach is crucial for models to differentiate between positive and negative items effectively, even when they are closely aligned in the feature space. Here, a loss function is generally an objective optimization function that is defined based on user–item interaction data, through either implicit or explicit feedback. The loss function essentially decides how well a recommendation algorithm performs. In this paper, we introduce and define the concept of ‘potential impostor’, highlighting its impact on learned representation quality and algorithmic efficiency . We tackle the limitations of non-metric methods, like the Weighted Approximate Rank Pairwise Loss (WARP) method, which struggles to capture item–item similarities, by using a ‘similarity propagation’ strategy with a new loss term. Similarly, we address fixed margin inefficiencies in Weighted Collaborative Metric Learning (WCML), through density distribution approximation . This moves potential impostors away from the margin for more robust learning. Additionally, we propose a large-scale batch approximation algorithm for increased detection of impostors, coupled with an active learning strategy for improved top- N N recommendation performance. Our extensive empirical analysis across five major and diverse datasets demonstrates the effectiveness and feasibility of our methods, compared to existing techniques with respect to improving AUC, reducing impostor rate, and increasing the average distance metrics. More specifically, our evaluation shows that our two proposed methods outperform the existing state-of-the-art techniques, with an improvement of AUC by 3.5% and 3.7%, NDCG by 1.0% and 9.1% and HR by 1.3% and 3.6%, respectively. Similarly, the impostor rate is decreased by 35% and 18%, and their average distance is increased by 33% and 37%, respectively.},
  archive      = {J_MLA},
  author       = {Sofia Aftab and Heri Ramampiaro},
  doi          = {10.1016/j.mlwa.2023.100520},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100520},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Improving top-N recommendations using batch approximation for weighted pair-wise loss},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An inpatient fall risk assessment tool: Application of
machine learning models on intrinsic and extrinsic risk factors.
<em>MLA</em>, <em>15</em>, 100519. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aimed to identify the most impactful set of intrinsic and extrinsic fall risk factors and develop a data-driven inpatient fall risk assessment tool (FRAT). The dataset used for the study comprised in-hospital fall records from 2012 to 2017. Four machine learning (ML) algorithms, Support Vector Machine (SVM), Random Forest (RF), Gradient Boosting (Gboost), and Deep Neural Network (DNN) were utilized to predict the inpatient fall risk level. To enhance the performance of the prediction models, two approaches were implemented, including (1) feature selection to identify the optimal feature set and (2) the development of three distinct shift-wise models. Furthermore, the optimal feature sets in the shift-wise models were extracted. According to the results, DNN outperformed other methods by reaching an accuracy, sensitivity, specificity, and AUC of 0.71, 0.8, 0.6, and 0.7, respectively, considering the full set of features. The performance of the models was further improved (by 3-5 %) by conducting a feature selection process for all models. Specifically, the DNN model achieved an accuracy of 0.74 while considering the optimal set of predictors. Moreover, the shift-wise RF models demonstrated higher accuracies (by 4-10 %) compared to the same model using a full feature set. This study&#39;s outcome confirms ML models&#39; compelling capability in developing an inpatient FRAT while considering intrinsic and extrinsic factors. The insight from such models could form a foundation to (1) monitor the inpatients’ fall risk, (2) identify the major factors involved in inpatient falls, and (3) create subject-specific self-care plans.},
  archive      = {J_MLA},
  author       = {Sonia Jahangiri and Masoud Abdollahi and Rasika Patil and Ehsan Rashedi and Nasibeh Azadeh-Fard},
  doi          = {10.1016/j.mlwa.2023.100519},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100519},
  shortjournal = {Mach. Learn. Appl.},
  title        = {An inpatient fall risk assessment tool: Application of machine learning models on intrinsic and extrinsic risk factors},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comparison of machine learning surrogate models of
street-scale flooding in norfolk, virginia. <em>MLA</em>, <em>15</em>,
100518. (<a href="https://doi.org/10.1016/j.mlwa.2023.100518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to personal and property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models : Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The comparison of deep learning to the random forest algorithm is motivated by the desire to utilize a machine learning architecture that allows for the future inclusion of common uncertainty quantification techniques and the effective integration of relevant, multi-modal features.},
  archive      = {J_MLA},
  author       = {Diana McSpadden and Steven Goldenberg and Binata Roy and Malachi Schram and Jonathan L. Goodall and Heather Richter},
  doi          = {10.1016/j.mlwa.2023.100518},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100518},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comparison of machine learning surrogate models of street-scale flooding in norfolk, virginia},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on machine learning applications for
drilling and blasting in surface mining. <em>MLA</em>, <em>15</em>,
100517. (<a href="https://doi.org/10.1016/j.mlwa.2023.100517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drilling and blasting operations are pivotal for productivity and safety in hard rock surface mining. These operations are restricted due to complexities such as site-specific uncertainties, safety risks, and environmental and economic constraints. Machine Learning (ML) is a transformative approach to tackle these complexities resulting in significant cost reductions. ML applications can reduce overall blasting costs by up to 23% and decrease the amount of explosives by as much as 89% compared to traditional methods. This survey presents a comprehensive review of how ML can be applied to optimize drill and blast designs while accounting for its operational challenges. Our research highlights the difficulties in collecting quality site-specific data, the complexity of interpreting this data into insightful information, the selection of ML models relating to mining objectives, and the need for established methods to assess blast efficiency quantitatively. We provide a synthesis of ML model development practices in drilling and blasting and demonstrate the value of ML methodologies. Based on our survey, we present actionable recommendations for developing ML methodologies to improve safety, reduce costs, and enhance efficiency in drilling and blasting processes. This includes establishing standardized data schematics, multiobjective model optimization, and comprehensive evaluation metrics . These benefits can guide mine management and engineers to adopt ML techniques and improve on-ground operational practices. This survey aims to serve as a resource for both practitioners and researchers shaping the future research direction in ML applications for drilling and blasting practices.},
  archive      = {J_MLA},
  author       = {Venkat Munagala and Srikanth Thudumu and Irini Logothetis and Sushil Bhandari and Rajesh Vasa and Kon Mouzakis},
  doi          = {10.1016/j.mlwa.2023.100517},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100517},
  shortjournal = {Mach. Learn. Appl.},
  title        = {A comprehensive survey on machine learning applications for drilling and blasting in surface mining},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for an explainable cost prediction of
medical insurance. <em>MLA</em>, <em>15</em>, 100516. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive modeling in healthcare continues to be an active actuarial research topic as more insurance companies aim to maximize the potential of Machine Learning (ML) approaches to increase their productivity and efficiency. In this paper, the authors deployed three regression-based ensemble ML models that combine variations of decision trees through Extreme Gradient Boosting (XGBoost), Gradient-boosting Machine (GBM), and Random Forest (RF) methods in predicting medical insurance costs. Explainable Artificial Intelligence (XAi) methods SHapley Additive exPlanations (SHAP) and Individual Conditional Expectation (ICE) plots were deployed to discover and explain the key determinant factors that influence medical insurance premium prices in the dataset. The dataset used comprised 986 records and is publicly available in the KAGGLE repository. The models were evaluated using four performance evaluation metrics, including R-squared (R 2 ), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Mean Absolute Percentage Error (MAPE). The results show that all models produced impressive outcomes; however, the XGBoost model achieved a better overall performance although it also expanded more computational resources, while the RF model recorded a lesser prediction error and consumed far fewer computing resources than the XGBoost model. Furthermore, we compared the outcome of both XAi methods in identifying the key determinant features that influenced the PremiumPrices for each model and whereas both XAi methods produced similar outcomes, we found that the ICE plots showed in more detail the interactions between each variable than the SHAP analysis which seemed to be more high-level. It is the aim of the authors that the contributions of this study will help policymakers, insurers, and potential medical insurance buyers in their decision-making process for selecting the right policies that meet their specific needs.},
  archive      = {J_MLA},
  author       = {Ugochukwu Orji and Elochukwu Ukwandu},
  doi          = {10.1016/j.mlwa.2023.100516},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100516},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Machine learning for an explainable cost prediction of medical insurance},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Considerations for adapting higher education technology
courses for AI large language models: A critical review of the impact of
ChatGPT. <em>MLA</em>, <em>15</em>, 100513. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Following the very recent launch of the ChatGPT chatbot , numerous comments and speculations were posted concerning the potential aspects of society that are expected to benefit from this AI revolution. In particular, the education sector is considered as one of the primary domains affected by this application, the impact of which remains yet to be fully understood. Furthermore, many Higher Education institutions are required to get to terms with its impact on teaching and learning , and to clarify their stances on the use of ChatGPT software. This study was developed to investigate some critical case studies considered as relevant to the inevitable re-evaluation of educational aspects needed, ranging from academic missions to student and course learning outcomes and its ethical uses. Following a review of some of the pros and cons of ChatGPT in the higher educational sector, this paper shall demonstrate several case studies of early trials in teaching and learning assessments related to various specializations. Next, the ability of some well-known AI detector software and analyzed in terms of their capacity to successfully detect AI-generated content. Analysis shall be made of the foreseen impact on important aspects including challenges and benefits related to its use in course assessments as well as academic integrity and ethical use. The study concludes with a set of recommendations made from our findings and benchmarks obtained from top universities in order to assist faculty members and decision makers at Higher Education institutions concerning their response strategy and use of ChatGPT.},
  archive      = {J_MLA},
  author       = {Omar Tayan and Ali Hassan and Khaled Khankan and Sanaa Askool},
  doi          = {10.1016/j.mlwa.2023.100513},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100513},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Considerations for adapting higher education technology courses for AI large language models: A critical review of the impact of ChatGPT},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using machine learning for detecting liquidity risk in
banks. <em>MLA</em>, <em>15</em>, 100511. (<a
href="https://doi.org/10.1016/j.mlwa.2023.100511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate classification of banks’ Liquidity Risk (LR) for regulatory supervision is hindered by limitations in the measures, such as Minimum Liquid Assets (MLA), Net-Stable Funding Ratio (NSFR), and Liquidity Coverage Ratio (LCR). This study addressed two limitations on data integrity vulnerabilities and the narrow composition of LR factors excluding practical LR determinants such as credit portfolio quality, market conditions, strategies of assets and funding. Theoretical gaps included the eight new LR factors in this study, benchmarking study results with measures to interpret the studies’ contributions and the selection of suitable prediction methods for non-linear, imbalanced, scaling, and near real-time data. We used data from 38 Tanzanian banks (2010-2021) from the Bank of Tanzania (BOT). Extensive factors experimentation using Random Forest (RF) and Multi-Layer Perceptron (MLP) models identified ten features for Machine Learning (ML) analysis and LR rating as output. A hybrid RF-MLP model with a 199-tree RF and 10-512-250-120-80-60-6 MLP was developed. It increased LR sensitivity and reduced RF and MLP model limitations through generalisation, and demonstrated statistical and practical performance. It minimised classification errors with Type I and II errors, and Negative Likelihood of 0.8%, 9.1%, and 1%; Discriminant Power of 2.61; and 90% to 96% Accuracy, Balanced Accuracy, Precision, Recall, F1 Score, G-mean, Cohen’s Kappa, Youden Index, and Area Under the Curve. Past LR scenarios confirmed RF-MLP performance improvement over MLA. The unavailability of LCR and NSFR data hindered a comprehensive evaluation. This study extended LR factors and proposed a model to complement LR classification.},
  archive      = {J_MLA},
  author       = {Rweyemamu Ignatius Barongo and Jimmy Tibangayuka Mbelwa},
  doi          = {10.1016/j.mlwa.2023.100511},
  journal      = {Machine Learning with Applications},
  month        = {3},
  pages        = {100511},
  shortjournal = {Mach. Learn. Appl.},
  title        = {Using machine learning for detecting liquidity risk in banks},
  volume       = {15},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
