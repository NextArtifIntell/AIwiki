<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>ARTMED_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="artmed---210">ARTMED - 210</h2>
<ul>
<li><details>
<summary>
(2024). Acoustical features as knee health biomarkers: A critical
analysis. <em>ARTMED</em>, <em>158</em>, 103013. (<a
href="https://doi.org/10.1016/j.artmed.2024.103013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustical knee health assessment has long promised an alternative to clinically available medical imaging tools, but this modality has yet to be adopted in medical practice. The field is currently led by machine learning models processing acoustical features, which have presented promising diagnostic performances. However, these methods overlook the intricate multi-source nature of audio signals and the underlying mechanisms at play. By addressing this critical gap, the present paper introduces a novel causal framework for validating knee acoustical features. We argue that current machine learning methodologies for acoustical knee diagnosis lack the required assurances and thus cannot be used to classify acoustic features as biomarkers. Our framework establishes a set of essential theoretical guarantees necessary to validate this claim. We apply our methodology to three real-world experiments investigating the effect of researchers’ expectations, the experimental protocol, and the wearable employed sensor. We reveal latent issues such as underlying shortcut learning and performance inflation. This study is the first independent result reproduction study in acoustical knee health evaluation. We conclude by offering actionable insights that address key limitations, providing valuable guidance for future research in knee health acoustics.},
  archive      = {J_ARTMED},
  author       = {Christodoulos Kechris and Jerome Thevenot and Tomas Teijeiro and Vincent A. Stadelmann and Nicola A. Maffiuletti and David Atienza},
  doi          = {10.1016/j.artmed.2024.103013},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103013},
  shortjournal = {Artif. Intell. Med.},
  title        = {Acoustical features as knee health biomarkers: A critical analysis},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). LSSF-net: Lightweight segmentation with self-awareness,
spatial attention, and focal modulation. <em>ARTMED</em>, <em>158</em>,
103012. (<a href="https://doi.org/10.1016/j.artmed.2024.103012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of skin lesions within dermoscopic images plays a crucial role in the timely identification of skin cancer for computer-aided diagnosis on mobile platforms. However, varying shapes of the lesions, lack of defined edges, and the presence of obstructions such as hair strands and marker colours make this challenge more complex. Additionally, skin lesions often exhibit subtle variations in texture and colour that are difficult to differentiate from surrounding healthy skin, necessitating models that can capture both fine-grained details and broader contextual information. Currently, melanoma segmentation models are commonly based on fully connected networks and U-Nets. However, these models often struggle with capturing the complex and varied characteristics of skin lesions, such as the presence of indistinct boundaries and diverse lesion appearances, which can lead to suboptimal segmentation performance. To address these challenges, we propose a novel lightweight network specifically designed for skin lesion segmentation utilising mobile devices, featuring a minimal number of learnable parameters (only 0.8 million). This network comprises an encoder–decoder architecture that incorporates conformer-based focal modulation attention, self-aware local and global spatial attention, and split channel-shuffle. The efficacy of our model has been evaluated on four well-established benchmark datasets for skin lesion segmentation: ISIC 2016, ISIC 2017, ISIC 2018, and PH2. Empirical findings substantiate its state-of-the-art performance, notably reflected in a high Jaccard index.},
  archive      = {J_ARTMED},
  author       = {Hamza Farooq and Zuhair Zafar and Ahsan Saadat and Tariq M. Khan and Shahzaib Iqbal and Imran Razzak},
  doi          = {10.1016/j.artmed.2024.103012},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103012},
  shortjournal = {Artif. Intell. Med.},
  title        = {LSSF-net: Lightweight segmentation with self-awareness, spatial attention, and focal modulation},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Domain generalization for enhanced predictions of hospital
readmission on unseen domains among patients with diabetes.
<em>ARTMED</em>, <em>158</em>, 103010. (<a
href="https://doi.org/10.1016/j.artmed.2024.103010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prediction model to assess the risk of hospital readmission can be valuable to identify patients who may benefit from extra care. Developing hospital-specific readmission risk prediction models using local data is not feasible for many institutions. Models developed on data from one hospital may not generalize well to another hospital. There is a lack of an end-to-end adaptable readmission model that can generalize to unseen test domains. We propose an early readmission risk domain generalization network, ERR-DGN, for cross-domain knowledge transfer. ERR-DGN internalizes the shared patterns and characteristics that are consistent across source domains, enabling it to adapt to a new domain. It transforms source datasets to a common embedding space while capturing relevant temporal long-term dependencies of sequential data. Domain generalization is then applied on domain-specific fully connected linear layers. The model is optimized by a loss function that integrates distribution discrepancy loss to match the mean embeddings of multiple source distributions with the task-specific loss. A model was developed using electronic health record (EHR) data of 201,688 patients with diabetes across urban, suburban, rural, and mixed hospital systems to enhance 30-day readmission predictions among patients with diabetes on 67,066 unseen patients at a rural hospital. We also explored how model performance varied by the number of sites and over time. The proposed method outperformed the baseline models, yielding a 6 % increase in F1-score (0.79 ± 0.006 vs. 0.73 ± 0.007). Model performance peaked with the inclusion of three sites. Performance of the model was relatively stable for 3 years then declined at 4 years. ERR-DGN may be a proficient tool for learning data from multiple sites and subsequently applying a hospitalization readmission prediction model to a new site. Including a relatively small number of varied sites may be sufficient to achieve peak performance. Periodic retraining at least every 3 years may mitigate model degradation over time.},
  archive      = {J_ARTMED},
  author       = {Ameen Abdel Hai and Mark G. Weiner and Alice Livshits and Jeremiah R. Brown and Anuradha Paranjape and Wenke Hwang and Lester H. Kirchner and Nestoras Mathioudakis and Esra Karslioglu French and Zoran Obradovic and Daniel J. Rubin},
  doi          = {10.1016/j.artmed.2024.103010},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103010},
  shortjournal = {Artif. Intell. Med.},
  title        = {Domain generalization for enhanced predictions of hospital readmission on unseen domains among patients with diabetes},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Developing healthcare language model embedding spaces.
<em>ARTMED</em>, <em>158</em>, 103009. (<a
href="https://doi.org/10.1016/j.artmed.2024.103009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-trained Large Language Models (LLMs) have revolutionised Natural Language Processing (NLP) tasks, but often struggle when applied to specialised domains such as healthcare. The traditional approach of pre-training on large datasets followed by task-specific fine-tuning is resource-intensive and poorly aligned with the constraints of many healthcare settings. This presents a significant challenge for deploying LLM-based NLP solutions in medical contexts, where data privacy, computational resources, and domain-specific language pose unique obstacles. This study aims to develop and evaluate efficient methods for adapting smaller LLMs to healthcare-specific datasets and tasks. We seek to identify pre-training approaches that can effectively instil healthcare competency in compact LLMs under tight computational budgets, a crucial capability for responsible and sustainable deployment in local healthcare settings. We explore three specialised pre-training methods to adapt smaller LLMs to different healthcare datasets: traditional Masked Language modelling (MLM), Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel approach utilising metadata categories from healthcare settings. These methods are assessed across multiple healthcare datasets, with a focus on downstream document classification tasks. We evaluate the performance of the resulting LLMs through classification accuracy and analysis of the derived embedding spaces. Contrastively trained models consistently outperform other approaches on classification tasks, delivering strong performance with limited labelled data and fewer model parameter updates. While our novel metadata-based pre-training does not further improve classifications across datasets, it yields interesting embedding cluster separability. Importantly, all domain-adapted LLMs outperform their publicly available, general-purpose base models, validating the importance of domain specialisation. This research demonstrates the efficacy of specialised pre-training methods in adapting compact LLMs to healthcare tasks, even under resource constraints. We provide guidelines for pre-training specialised healthcare LLMs and motivate continued inquiry into contrastive objectives. Our findings underscore the potential of these approaches for aligning small LLMs with privacy-sensitive medical tasks, offering a path toward more efficient and responsible NLP deployment in healthcare settings. This work contributes to the broader goal of making advanced NLP techniques accessible and effective in specialised domains, particularly where resource limitations and data sensitivity are significant concerns.},
  archive      = {J_ARTMED},
  author       = {Niall Taylor and Dan Schofield and Andrey Kormilitzin and Dan W. Joyce and Alejo Nevado-Holgado},
  doi          = {10.1016/j.artmed.2024.103009},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103009},
  shortjournal = {Artif. Intell. Med.},
  title        = {Developing healthcare language model embedding spaces},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploring the effectiveness of instruction tuning in
biomedical language processing. <em>ARTMED</em>, <em>158</em>, 103007.
(<a href="https://doi.org/10.1016/j.artmed.2024.103007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), particularly those similar to ChatGPT, have significantly influenced the field of Natural Language Processing (NLP). While these models excel in general language tasks, their performance in domain-specific downstream tasks such as biomedical and clinical Named Entity Recognition (NER), Relation Extraction (RE), and Medical Natural Language Inference (NLI) is still evolving. In this context, our study investigates the potential of instruction tuning for biomedical language processing, applying this technique to two general LLMs of substantial scale. We present a comprehensive, instruction-based model trained on a dataset that consists of approximately 200,000 instruction-focused samples. This dataset represents a carefully curated compilation of existing data, meticulously adapted and reformatted to align with the specific requirements of our instruction-based tasks. This initiative represents an important step in utilising such models to achieve results on par with specialised encoder-only models like BioBERT and BioClinicalBERT for various classical biomedical NLP tasks. Our work includes an analysis of the dataset’s composition and its impact on model performance, providing insights into the intricacies of instruction tuning. By sharing our codes, models, and the distinctively assembled instruction-based dataset, we seek to encourage ongoing research and development in this area. 2},
  archive      = {J_ARTMED},
  author       = {Omid Rohanian and Mohammadmahdi Nouriborji and Samaneh Kouchaki and Farhad Nooralahzadeh and Lei Clifton and David A. Clifton},
  doi          = {10.1016/j.artmed.2024.103007},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103007},
  shortjournal = {Artif. Intell. Med.},
  title        = {Exploring the effectiveness of instruction tuning in biomedical language processing},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fully automatic deep convolutional approaches for the
screening of neurodegeneratives diseases using multi-view OCT images.
<em>ARTMED</em>, <em>158</em>, 103006. (<a
href="https://doi.org/10.1016/j.artmed.2024.103006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of neurodegenerative diseases (NDDs) such as Alzheimer’s (AD), Parkinson’s (PD), Essential tremor (ET), and Multiple Sclerosis (MS) is increasing alongside the aging population. Recent studies suggest that these disorders can be identified through retinal imaging, allowing for early detection and monitoring via Optical Coherence Tomography (OCT) scans. This study is at the forefront of research, pioneering the application of multi-view OCT and 3D information to the neurological diseases domain. Our methodology consists of two main steps. In the first one, we focus on the segmentation of the retinal nerve fiber layer (RNFL) and a class layer grouping between the ganglion cell layer and Bruch’s membrane (GCL-BM) in both macular and optic disc OCT scans. These are the areas where changes in thickness serve as a potential indicator of NDDs. The second phase is to select patients based on information about the retinal layers. We explore how the integration of both views (macula and optic disc) improves each screening scenario: Healthy Controls (HC) vs. NDD, AD vs. NDD, ET vs. NDD, MS vs. NDD, PD vs. NDD, and a final multi-class approach considering all four NDDs. For the segmentation task, we obtained satisfactory results for both 2D and 3D approaches in macular segmentation, in which 3D performed better due to the inclusion of depth and cross-sectional information. As for the optic disc view, transfer learning did not improve the metrics over training from scratch, but it did provide a faster training. As for screening, 3D computational biomarkers provided better results than 2D ones, and multi-view methods were usually better than the single-view ones. Regarding separability among diseases, MS and PD were the ones that provided better results in their screening approaches, being also the most represented classes. In conclusion, our methodology has been successfully validated with an extensive experimentation of configurations, techniques and OCT views, becoming the first multi-view analysis that merges data from both macula-centered and optic disc-centered perspectives. Besides, it is also the first effort to examine key retinal layers across four major NDDs within the framework of pathological screening.},
  archive      = {J_ARTMED},
  author       = {Lorena Álvarez-Rodríguez and Ana Pueyo and Joaquim de Moura and Elisa Vilades and Elena Garcia-Martin and Clara I. Sánchez and Jorge Novo and Marcos Ortega},
  doi          = {10.1016/j.artmed.2024.103006},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {103006},
  shortjournal = {Artif. Intell. Med.},
  title        = {Fully automatic deep convolutional approaches for the screening of neurodegeneratives diseases using multi-view OCT images},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of networks for prognostic prediction of
health outcomes and diagnostic prediction of health conditions within
electronic health records. <em>ARTMED</em>, <em>158</em>, 102999. (<a
href="https://doi.org/10.1016/j.artmed.2024.102999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background and objective Using graph theory, Electronic Health Records (EHRs) can be represented graphically to exploit the relational dependencies of the multiple information formats to improve Machine Learning (ML) prediction models. In this systematic qualitative review, we explore the question: How are graphs used on EHRs, to predict diagnosis and health outcomes? Methodology The search strategy identified studies that used patient-level graph representations of EHRs to utilise ML to predict health outcomes and diagnoses. We conducted our search on MEDLINE, Web of Science and Scopus. Results 832 studies were identified by the search strategy, of which 27 studies were selected for data extraction. Following data extraction, 18 studies used ML with patient-level graph-based representations of EHRs to predict health outcomes and diagnoses. Models ranged from traditional ML to neural network-based models. MIMIC-III was the most used dataset (n = 6, where n is the number of occurrences), followed by National Health Insurance Research Database (NHIRD) (n = 4) and eICU Collaborative Research Database (eICU) (n = 4). The most predicted health outcomes were mortality (n = 9; 21%), hospital readmission (n = 9; 21%), and treatment success (n = 4; 9%). Model performances ranged across outcomes, mortality prediction (Area Under the Receiver Operating Characteristic (AUROC): 72.1 - 91.6; Area Under Precision-Recall Curve (AUPRC): 34.8 - 81.3) and readmission prediction (AUROC: 63.7 - 85.8; AUPRC 39.86 - 84.7). Only one paper had a low Risk of Bias (RoB) that applied to our research question (4%). Conclusion Graph-based representations using EHRs, for individual health outcomes and diagnoses requires further research before we can see the results applied clinically. The use of graph representations appears to improve EHR representation and predictive performance compared to baseline ML methods in multiple fields of medicine.},
  archive      = {J_ARTMED},
  author       = {Zoe Hancox and Allan Pang and Philip G. Conaghan and Sarah R. Kingsbury and Andrew Clegg and Samuel D. Relton},
  doi          = {10.1016/j.artmed.2024.102999},
  journal      = {Artificial Intelligence in Medicine},
  month        = {12},
  pages        = {102999},
  shortjournal = {Artif. Intell. Med.},
  title        = {A systematic review of networks for prognostic prediction of health outcomes and diagnostic prediction of health conditions within electronic health records},
  volume       = {158},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individualised recovery trajectories of patients with
impeded mobility, using distance between probability distributions of
learnt graphs. <em>ARTMED</em>, <em>157</em>, 103005. (<a
href="https://doi.org/10.1016/j.artmed.2024.103005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients who are undergoing physical rehabilitation, benefit from feedback that follows from reliable assessment of their cumulative performance attained at a given time. In this paper, we provide a method for the learning of the recovery trajectory of an individual patient, as they undertake exercises as part of their physical therapy towards recovery of their loss of movement ability, following a critical illness. The difference between the Movement Recovery Scores (MRSs) attained by a patient, when undertaking a given exercise routine on successive instances, is given by a statistical distance/divergence between the (posterior) probabilities of random graphs that are Bayesianly learnt using time series data on locations of 20 of the patient’s joints, recorded on an e-platform as the patient exercises. This allows for the computation of the MRS on every occasion the patient undertakes this exercise, using which, the recovery trajectory is drawn. We learn each graph as a Random Geometric Graph drawn in a probabilistic metric space, and identify the closed-form marginal posterior of any edge of the graph, given the correlation structure of the multivariate time series data on joint locations. On the basis of our recovery learning, we offer recommendations on the optimal exercise routines for patients with given level of mobility impairment.},
  archive      = {J_ARTMED},
  author       = {Chuqiao Zhang and Crina Grosan and Dalia Chakrabarty},
  doi          = {10.1016/j.artmed.2024.103005},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103005},
  shortjournal = {Artif. Intell. Med.},
  title        = {Individualised recovery trajectories of patients with impeded mobility, using distance between probability distributions of learnt graphs},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advancing health coaching: A comparative study of large
language model and health coaches. <em>ARTMED</em>, <em>157</em>,
103004. (<a href="https://doi.org/10.1016/j.artmed.2024.103004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective Recent advances in large language models (LLM) offer opportunities to automate health coaching. With zero-shot learning ability, LLMs could revolutionize health coaching by providing better accessibility, scalability, and customization. The aim of this study is to compare the quality of responses to clients&#39; sleep-related questions provided by health coaches and an LLM. Design, setting, and participants From a de-identified dataset of coaching conversations from a pilot randomized controlled trial, we extracted 100 question-answer pairs comprising client questions and corresponding health coach responses. These questions were entered into a retrieval-augmented generation (RAG)-enabled open-source LLM (LLaMa-2-7b-chat) to generate LLM responses. Out of 100 question-answer pairs, 90 were taken out and assigned to three groups of evaluators: experts, lay-users, and GPT-4. Each group conducted two evaluation tasks: (Task 1) a single-response quality assessment spanning five criteria—accuracy, readability, helpfulness, empathy, and likelihood of harm—rated on a five-point Likert scale, and (Task 2) a pairwise comparison to choose the superior response between pairs. A suite of inferential statistical methods, including the paired and independent sample t -tests, Pearson correlation, and chi-square tests, were utilized to answer the study objective. Recognizing potential biases in human judgment, the remaining 10 question-answer pairs were used to assess inter-evaluator reliability among the human evaluators, quantified using the interclass correlation coefficient and percentage agreement metrics. Results Upon exclusion of incomplete data, the analysis included 178 single-response evaluations (Task 1) and 83 pairwise comparisons (Task 2). Expert and GPT-4 assessments revealed no discernible disparities in health coach and LLM responses across the five metrics. Contrarily, lay-users deemed LLM responses significantly more helpful than that of human coaches ( p &lt; 0.05). LLM responses were preferred in the majority (62.25 %, n = 155) of the aggregate 249 assessments, with all three evaluator groups favoring LLM over health coach inputs. While GPT-4 rated both health coach and LLM responses significantly higher than experts in terms of readability, helpfulness, and empathy, its ratings on accuracy and likelihood of harm aligned with those of experts. Response length positively correlated with accuracy and empathy scores, but negatively affected readability across all evaluator groups. Expert and lay-user evaluators demonstrated moderate to high inter-evaluator reliability. Conclusion Our study showed encouraging findings by demonstrating that RAG-enabled LLM has comparable performance to health coaches in the domain tested. Serving as an initial step towards the creation of more sophisticated, adaptive, round-the-clock automated health coaching systems, our findings call for more extensive evaluation which could assist in the development of the model that could in the future lead to potential clinical implementation.},
  archive      = {J_ARTMED},
  author       = {Qi Chwen Ong and Chin-Siang Ang and Davidson Zun Yin Chee and Ashwini Lawate and Frederick Sundram and Mayank Dalakoti and Leonardo Pasalic and Daniel To and Tatiana Erlikh Fox and Iva Bojic and Josip Car},
  doi          = {10.1016/j.artmed.2024.103004},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103004},
  shortjournal = {Artif. Intell. Med.},
  title        = {Advancing health coaching: A comparative study of large language model and health coaches},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From pre-training to fine-tuning: An in-depth analysis of
large language models in the biomedical domain. <em>ARTMED</em>,
<em>157</em>, 103003. (<a
href="https://doi.org/10.1016/j.artmed.2024.103003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we delve into the adaptation and effectiveness of Transformer-based, pre-trained Large Language Models (LLMs) within the biomedical domain, a field that poses unique challenges due to its complexity and the specialized nature of its data. Building on the foundation laid by the transformative architecture of Transformers, we investigate the nuanced dynamics of LLMs through a multifaceted lens, focusing on two domain-specific tasks, i.e., Natural Language Inference (NLI) and Named Entity Recognition (NER). Our objective is to bridge the knowledge gap regarding how these models’ downstream performances correlate with their capacity to encapsulate task-relevant information. To achieve this goal, we probed and analyzed the inner encoding and attention mechanisms in LLMs, both encoder- and decoder-based, tailored for either general or biomedical-specific applications. This examination occurs before and after the models are fine-tuned across various data volumes. Our findings reveal that the models’ downstream effectiveness is intricately linked to specific patterns within their internal mechanisms, shedding light on the nuanced ways in which LLMs process and apply knowledge in the biomedical context. The source code for this paper is available at https://github.com/agnesebonfigli99/LLMs-in-the-Biomedical-Domain .},
  archive      = {J_ARTMED},
  author       = {Agnese Bonfigli and Luca Bacco and Mario Merone and Felice Dell’Orletta},
  doi          = {10.1016/j.artmed.2024.103003},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103003},
  shortjournal = {Artif. Intell. Med.},
  title        = {From pre-training to fine-tuning: An in-depth analysis of large language models in the biomedical domain},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiency at scale: Investigating the performance of
diminutive language models in clinical tasks. <em>ARTMED</em>,
<em>157</em>, 103002. (<a
href="https://doi.org/10.1016/j.artmed.2024.103002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability. This was followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as 25 million parameters. Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure. The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from large foundation LLMs. Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, finding the domain pre-training to be particularly important in smaller models and discuss how these factors interplay to provide the best efficiency-performance trade-off. Full code available at: https://github.com/nlpie-research/efficient-ml .},
  archive      = {J_ARTMED},
  author       = {Niall Taylor and Upamanyu Ghose and Omid Rohanian and Mohammadmahdi Nouriborji and Andrey Kormilitzin and David A. Clifton and Alejo Nevado-Holgado},
  doi          = {10.1016/j.artmed.2024.103002},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103002},
  shortjournal = {Artif. Intell. Med.},
  title        = {Efficiency at scale: Investigating the performance of diminutive language models in clinical tasks},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). OphGLM: An ophthalmology large language-and-vision
assistant. <em>ARTMED</em>, <em>157</em>, 103001. (<a
href="https://doi.org/10.1016/j.artmed.2024.103001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision computer-aided diagnostic methods have been used in early ophthalmic disease screening and diagnosis. However, the limited output formats of these methods lead to poor human–computer interaction and low clinical applicability value. Thus, ophthalmic visual question answering is worth studying. Unfortunately, no practical solutions exist before Large Language Models(LLMs). In this paper, we investigate the ophthalmic visual diagnostic interaction problem. We construct an ophthalmology large language-and-vision assistant, OphGLM, consisting of an image encoder, a text encoder, a fusion module, and an LLM module. We establish a new Chinese ophthalmic fine-tuning dataset, FundusTuning-CN, including the fundus instruction and conversation sets. Based on FundusTuning-CN, we establish a novel LLM-tuning strategy to introduce visual model understanding and ophthalmic knowledge into LLMs at a low cost and high efficiency. Leveraging the pre-training of the image encoder, OphGLM demonstrates strong visual understanding and surpasses open-source visual language models in common fundus disease classification tasks. The FundusTuning-CN enables OphGLM to surpass open-source medical LLMs in both ophthalmic knowledge and interactive capabilities. Our proposed OphGLM has the potential to revolutionize clinical applications in ophthalmology. The dataset, code, and models will be publicly available at https://github.com/ML-AILab/OphGLM .},
  archive      = {J_ARTMED},
  author       = {Zhuo Deng and Weihao Gao and Chucheng Chen and Zhiyuan Niu and Zheng Gong and Ruiheng Zhang and Zhenjie Cao and Fang Li and Zhaoyi Ma and Wenbin Wei and Lan Ma},
  doi          = {10.1016/j.artmed.2024.103001},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103001},
  shortjournal = {Artif. Intell. Med.},
  title        = {OphGLM: An ophthalmology large language-and-vision assistant},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent wearable-assisted digital healthcare industry
5.0. <em>ARTMED</em>, <em>157</em>, 103000. (<a
href="https://doi.org/10.1016/j.artmed.2024.103000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The latest evolution of the healthcare industry from Industry 1.0 to 5.0, incorporating smart wearable devices and digital technologies, has revolutionized healthcare delivery and improved patient treatment. Integrating smart wearables such as fitness trackers, smartwatches, and biosensors has endowed healthcare Industry 5.0 with numerous advantages, including remote patient monitoring, personalized healthcare, patient empowerment and engagement, telemedicine, and virtual care. This digital healthcare paradigm embraces promising technologies like Machine Learning (ML) and the Internet of Medical Things (IoMT) to enhance patient care. The key contribution of digital healthcare Industry 5.0 lies in its ability to revolutionize patient care by leveraging smart wearables and digital technologies to provide personalized, proactive, and patient-centric healthcare solutions. Despite the remarkable growth of smart wearables, the exploration of ML-based applications still needs to be expanded. Motivated by this gap, our paper conducts a comprehensive examination and evaluation of advanced ML techniques pertinent to the digital healthcare Industry 5.0 and wearable technology. We propose a detailed taxonomy for digital healthcare Industry 5.0, transforming it into an innovative process model highlighting key research challenges such as wearable modes for data collection, health tracking, security, and privacy issues. The proposed ML-based process comprises data collection from wearables like smartwatches and performs data pre-processing. Several ML models are applied, such as Support Vector Machine (SVM), Decision Tree (DT), and Random Forest(RF), to predict and classify the activity of the person. ML algorithms are capable of analyzing extensive healthcare data encompassing electronic health records (EHR) from sensors to offer valuable insights to improve decision-making processes. A comparative study of the existing work is discussed in detail. Lastly, a case study is presented to render the process model, where the RF-based model shows its efficacy by obtaining the lowest RMSE of 0.94, MSE of 0.88, and MAE of 0.27 for the prediction of activity.},
  archive      = {J_ARTMED},
  author       = {Vrutti Tandel and Aparna Kumari and Sudeep Tanwar and Anupam Singh and Ravi Sharma and Nagendar Yamsani},
  doi          = {10.1016/j.artmed.2024.103000},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {103000},
  shortjournal = {Artif. Intell. Med.},
  title        = {Intelligent wearable-assisted digital healthcare industry 5.0},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing autism spectrum disorder identification in
multi-site MRI imaging: A multi-head cross-attention and multi-context
approach for addressing variability in un-harmonized data.
<em>ARTMED</em>, <em>157</em>, 102998. (<a
href="https://doi.org/10.1016/j.artmed.2024.102998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-site MRI imaging poses a significant challenge due to the potential variations in images across different scanners at different sites. This variability can introduce ambiguity in further image analysis. Consequently, the image analysis techniques become site-dependent and scanner-dependent, implying that adjustments in the analysis methodologies are necessary for each scanner configuration. Further, implementing real-time modifications becomes intricate, particularly when incorporating a new type of scanner, as it requires adapting the analysis methods accordingly. Taking into account the aforementioned challenge, we have considered its implications for an Autism spectrum disorder (ASD) application. Our objective is to minimize the impact of site and scanner variability in the analysis, aiming to develop a model that remains effective across different scanners and sites. This entails devising a methodology that allows the same model to function seamlessly across multiple scanner configurations and sites. ASD, a behavioral disorder affecting child development, requires early detection. Clinical observation is time-consuming, prompting the use of fMRI with machine/deep learning for expedited diagnosis. Previous methods leverage fMRI’s functional connectivity but often rely on less generalized feature extractors and classifiers. Hence, there is significant room for improvement in the generalizability of detection methods across multi-site data, which is acquired from multiple scanners with different settings. In this study, we propose a Cross-Combination Multi-Scale Multi-Context Framework (CCMSMCF) capable of performing neuroimaging-based diagnostic classification of mental disorders for a multi-site dataset. Thus, this framework attains a degree of internal data harmonization, rendering it to some extent site and scanner-agnostic. Our proposed network, CCMSMCF, is constructed by integrating two sub-modules: the Multi-Head Attention Cross-Scale Module (MHACSM) and the Residual Multi-Context Module (RMCN). We also employ multiple loss functions in a novel manner for training the model, which includes Binary Cross Entropy, Dice loss, and Embedding Coupling loss. The model is validated on the Autism Brain Imaging Data Exchange I (ABIDE-I) dataset, which includes data from multiple scanners across different sites, and achieves promising results.},
  archive      = {J_ARTMED},
  author       = {Ranjeet Ranjan Jha and Arvind Muralie and Munish Daroch and Arnav Bhavsar and Aditya Nigam},
  doi          = {10.1016/j.artmed.2024.102998},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102998},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing autism spectrum disorder identification in multi-site MRI imaging: A multi-head cross-attention and multi-context approach for addressing variability in un-harmonized data},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the role of artificial intelligence in analysing oocytes
during in vitro fertilisation procedures. <em>ARTMED</em>, <em>157</em>,
102997. (<a href="https://doi.org/10.1016/j.artmed.2024.102997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, the most adopted technique to address infertility problems is in vitro fertilisation (IVF). However, its success rate is limited, and the associated procedures, known as assisted reproduction technology (ART), suffer from a lack of objectivity at the laboratory level and in clinical practice. This paper deals with applications of Artificial Intelligence (AI) techniques to IVF procedures. Artificial intelligence is considered a promising tool for ascertaining the quality of embryos, a critical step in IVF. Since the oocyte quality influences the final embryo quality, we present a systematic review of the literature on AI-based techniques used to assess oocyte quality; we analyse its results and discuss several promising research directions. In particular, we highlight how AI-based techniques can support the IVF process and examine their current applications as presented in the literature. Then, we discuss the challenges research must face in fully deploying AI-based solutions in current medical practice. Among them, the availability of high-quality data sets as well as standardised imaging protocols and data formats, the use of physics-informed simulation and machine learning techniques, the study of informative, descriptive yet observable features, and, above all, studies of the quality of oocytes and embryos, specifically about their live birth potential. An improved understanding of determinants for oocyte quality can improve success rates while reducing costs, risks for long-term embryo cultures, and bioethical concerns.},
  archive      = {J_ARTMED},
  author       = {Antonio Iannone and Alessandro Carfì and Fulvio Mastrogiovanni and Renato Zaccaria and Claudio Manna},
  doi          = {10.1016/j.artmed.2024.102997},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102997},
  shortjournal = {Artif. Intell. Med.},
  title        = {On the role of artificial intelligence in analysing oocytes during in vitro fertilisation procedures},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EEG spatial inter-channel connectivity analysis: A GCN-based
dual stream approach to distinguish mental fatigue status.
<em>ARTMED</em>, <em>157</em>, 102996. (<a
href="https://doi.org/10.1016/j.artmed.2024.102996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental fatigue is defined as a decline in the ability and efficiency of mental activities. A lot of research suggests that the transition from alertness to fatigue is accompanied by alterations in correlation patterns among various brain regions. However, conventional methods for detecting mental fatigue seldom emphases inter-channel connectivity in the spatial domain. To fill this gap, this paper explores the spatial inter-channel connectivity in alertness and fatigue, employing spectral graph convolutional networks (GCN) for mental fatigue detection. We utilized Pearson correlation coefficients (PCC) to establish temporal connections and magnitude-squared coherence (MSC) for spectral connections. Topological features of the brain network were then analysed. To enhance the learning of spatial inter-channel connectivity, a dual-graph strategy transforms edge features into node features, serving as inputs to the spectral GCN. By simultaneously learning PCC and MSC features, the model results indicate significant differences in some brain network characteristics between alert and fatigue states. It confirms that the synchronicity of brain operations differs in the alert state compared to mental fatigue, and indicates that fatigue states can influence correlation patterns among different brain regions. Our approach is evaluated on a self-designed experimental dataset containing 7 subjects, demonstrating a classification accuracy of 89.59 % in group-level experiments and 95.24 % at the subject level. Additionally, on the public dataset SEED-VIG containing 23 subjects, our method achieves an accuracy of 86.58 %. In summary, this paper proposes a neural network approach based on a dynamic functional connectivity network. The network integrates both temporal and spectral connections with the goal of simultaneously learning spatial inter-channel connectivity in time and frequency domains. This effectively accomplishes fatigue state detection, highlighting that fatigue significantly influences correlations among different brain regions.},
  archive      = {J_ARTMED},
  author       = {Kun Chen and Shulong Chai and Tianli Xie and Quan Liu and Li Ma},
  doi          = {10.1016/j.artmed.2024.102996},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102996},
  shortjournal = {Artif. Intell. Med.},
  title        = {EEG spatial inter-channel connectivity analysis: A GCN-based dual stream approach to distinguish mental fatigue status},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rapid estimation of left ventricular contractility with a
physics-informed neural network inverse modeling approach.
<em>ARTMED</em>, <em>157</em>, 102995. (<a
href="https://doi.org/10.1016/j.artmed.2024.102995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physics-based computer models based on numerical solutions of the governing equations generally cannot make rapid predictions, which in turn limits their applications in the clinic. To address this issue, we developed a physics-informed neural network (PINN) model that encodes the physics of a closed-loop blood circulation system embedding a left ventricle (LV). The PINN model is trained to satisfy a system of ordinary differential equations (ODEs) associated with a lumped parameter description of the circulatory system. The model predictions have a maximum error of less than 5% when compared to those obtained by solving the ODEs numerically. An inverse modeling approach using the PINN model is also developed to rapidly estimate model parameters (in ∼ 3 min) from single-beat LV pressure and volume waveforms. Using synthetic LV pressure and volume waveforms generated by the PINN model with different model parameter values, we show that the inverse modeling approach can recover the corresponding ground truth values for LV contractility indexed by the end-systolic elastance E e s with a 1% error, which suggests that this parameter is unique. The estimated E e s is about 58% to 284% higher for the data associated with dobutamine compared to those without, which implies that this approach can be used to estimate LV contractility using single-beat measurements. The PINN inverse modeling can potentially be used in the clinic to simultaneously estimate LV contractility and other physiological parameters from single-beat measurements.},
  archive      = {J_ARTMED},
  author       = {Ehsan Naghavi and Haifeng Wang and Lei Fan and Jenny S. Choy and Ghassan Kassab and Seungik Baek and Lik-Chuan Lee},
  doi          = {10.1016/j.artmed.2024.102995},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102995},
  shortjournal = {Artif. Intell. Med.},
  title        = {Rapid estimation of left ventricular contractility with a physics-informed neural network inverse modeling approach},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep reinforcement learning for personalized diagnostic
decision pathways using electronic health records: A comparative study
on anemia and systemic lupus erythematosus. <em>ARTMED</em>,
<em>157</em>, 102994. (<a
href="https://doi.org/10.1016/j.artmed.2024.102994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background: Clinical diagnoses are typically made by following a series of steps recommended by guidelines that are authored by colleges of experts. Accordingly, guidelines play a crucial role in rationalizing clinical decisions. However, they suffer from limitations, as they are designed to cover the majority of the population and often fail to account for patients with uncommon conditions. Moreover, their updates are long and expensive, making them unsuitable for emerging diseases and new medical practices. Methods: Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms to learn the optimal sequence of actions to perform in order to obtain a correct diagnosis from Electronic Health Records (EHRs), which we name a diagnostic decision pathway. We apply DRL to synthetic yet realistic EHRs and develop two clinical use cases: Anemia diagnosis, where the decision pathways follow a decision tree schema, and Systemic Lupus Erythematosus (SLE) diagnosis, which follows a weighted criteria score. We particularly evaluate the robustness of our approaches to noise and missing data, as these frequently occur in EHRs. Results: In both use cases, even with imperfect data, our best DRL algorithms exhibit competitive performance compared to traditional classifiers, with the added advantage of progressively generating a pathway to the suggested diagnosis, which can both guide and explain the decision-making process. Conclusion: DRL offers the opportunity to learn personalized decision pathways for diagnosis. Our two use cases illustrate the advantages of this approach: they generate step-by-step pathways that are explainable, and their performance is competitive when compared to state-of-the-art methods.},
  archive      = {J_ARTMED},
  author       = {Lillian Muyama and Antoine Neuraz and Adrien Coulet},
  doi          = {10.1016/j.artmed.2024.102994},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102994},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep reinforcement learning for personalized diagnostic decision pathways using electronic health records: A comparative study on anemia and systemic lupus erythematosus},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint self-supervised and supervised contrastive learning
for multimodal MRI data: Towards predicting abnormal neurodevelopment.
<em>ARTMED</em>, <em>157</em>, 102993. (<a
href="https://doi.org/10.1016/j.artmed.2024.102993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of different imaging modalities, such as structural, diffusion tensor, and functional magnetic resonance imaging, with deep learning models has yielded promising outcomes in discerning phenotypic characteristics and enhancing disease diagnosis. The development of such a technique hinges on the efficient fusion of heterogeneous multimodal features, which initially reside within distinct representation spaces. Naively fusing the multimodal features does not adequately capture the complementary information and could even produce redundancy. In this work, we present a novel joint self-supervised and supervised contrastive learning method to learn the robust latent feature representation from multimodal MRI data, allowing the projection of heterogeneous features into a shared common space, and thereby amalgamating both complementary and analogous information across various modalities and among similar subjects. We performed a comparative analysis between our proposed method and alternative deep multimodal learning approaches. Through extensive experiments on two independent datasets, the results demonstrated that our method is significantly superior to several other deep multimodal learning methods in predicting abnormal neurodevelopment. Our method has the capability to facilitate computer-aided diagnosis within clinical practice, harnessing the power of multimodal data. The source code of the proposed model is publicly accessible on GitHub: https://github.com/leonzyzy/Contrastive-Network .},
  archive      = {J_ARTMED},
  author       = {Zhiyuan Li and Hailong Li and Anca L. Ralescu and Jonathan R. Dillman and Mekibib Altaye and Kim M. Cecil and Nehal A. Parikh and Lili He},
  doi          = {10.1016/j.artmed.2024.102993},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102993},
  shortjournal = {Artif. Intell. Med.},
  title        = {Joint self-supervised and supervised contrastive learning for multimodal MRI data: Towards predicting abnormal neurodevelopment},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Abnormal recognition-assisted and onset-offset aware network
for pathological wearable ECG delineation. <em>ARTMED</em>,
<em>157</em>, 102992. (<a
href="https://doi.org/10.1016/j.artmed.2024.102992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) delineation is essential to the identification of abnormal cardiac status, especially when ECG signals are remotely monitored with wearable devices. The complexity and diversity of cardiac conditions generate numerous pathological ECG patterns, not only requiring the recognition of normal ECG but also addressing an extensive range of abnormal ECG patterns, posing a challenging task. Therefore, we propose an abnormal recognition-assisted network to integrate supplementary information on diverse ECG patterns. Simultaneously, we design an onset-offset aware loss to enhance precise waveform localization. Specifically, we establish a two-branch framework where ECG delineation serves as the target task, producing the final segmentation results. Additionally, the abnormal recognition-assisted network serves as an auxiliary task, extracting multi-label pathological information from ECGs. This joint learning approach establishes crucial correlations between ECG delineation and associated ECG abnormalities. The correlations enable the model to demonstrate sufficient generalization in the presence of diverse abnormal ECG patterns. Besides, onset-offset aware loss focuses intensively on wave onsets and offsets by applying biased weights to various waveform positions. This approach ensures a focus on precise localization, facilitating seamless integration into cross-entropy loss function. A large-scale wearable 12-lead dataset containing 4,913 signals is collected, offering an extensive range of ECG data for model training. Results demonstrate that our method achieves outstanding performance on two test datasets, attaining sensitivity of 94.97% and 94.27% and an error tolerance lower than 20 ms. Furthermore, our method is effective for various aberrant ECG signals, including ST-segment changes, atrial premature beats, and right and left bundle branch blocks.},
  archive      = {J_ARTMED},
  author       = {Yue Zhang and Jiewei Lai and Chenyu Zhao and Jinliang Wang and Yong Yan and Mingyang Chen and Lei Ji and Jun Guo and Baoshi Han and Yajun Shi and Yundai Chen and Wei Yang and Qianjin Feng},
  doi          = {10.1016/j.artmed.2024.102992},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102992},
  shortjournal = {Artif. Intell. Med.},
  title        = {Abnormal recognition-assisted and onset-offset aware network for pathological wearable ECG delineation},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning for anxiety and depression profiling and
risk assessment in the aftermath of an emergency. <em>ARTMED</em>,
<em>157</em>, 102991. (<a
href="https://doi.org/10.1016/j.artmed.2024.102991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background &amp; objectives Mental health disorders pose an increasing public health challenge worsened by the COVID-19 pandemic. The pandemic highlighted gaps in preparedness, emphasizing the need for early identification of at-risk groups and targeted interventions. This study aims to develop a risk assessment tool for anxiety, depression, and self-perceived stress using machine learning (ML) and explainable AI to identify key risk factors and stratify the population into meaningful risk profiles. Methods We utilized a cohort of 9291 individuals from Northern Spain, with extensive post-COVID-19 mental health surveys. ML classification algorithms predicted depression, anxiety, and self-reported stress in three classes: healthy, mild, and severe outcomes. A novel combination of SHAP (SHapley Additive exPlanations) and UMAP (Uniform Manifold Approximation and Projection) was employed to interpret model predictions and facilitate the identification of high-risk phenotypic clusters. Results The mean macro-averaged one-vs-one AUROC was 0.77 (± 0.01) for depression, 0.72 (± 0.01) for anxiety, and 0.73 (± 0.02) for self-perceived stress. Key risk factors included poor self-reported health, chronic mental health conditions, and poor social support. High-risk profiles, such as women with reduced sleep hours, were identified for self-perceived stress. Binary classification of healthy vs. at-risk classes yielded F1-Scores over 0.70. Conclusions Combining SHAP with UMAP for risk profile stratification offers valuable insights for developing effective interventions and shaping public health policies. This data-driven approach to mental health preparedness, when validated in real-world scenarios, can significantly address the mental health impact of public health crises like COVID-19.},
  archive      = {J_ARTMED},
  author       = {Guillermo Villanueva Benito and Ximena Goldberg and Nicolai Brachowicz and Gemma Castaño-Vinyals and Natalia Blay and Ana Espinosa and Flavia Davidhi and Diego Torres and Manolis Kogevinas and Rafael de Cid and Paula Petrone},
  doi          = {10.1016/j.artmed.2024.102991},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102991},
  shortjournal = {Artif. Intell. Med.},
  title        = {Machine learning for anxiety and depression profiling and risk assessment in the aftermath of an emergency},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MMF-NNs: Multi-modal multi-granularity fusion neural
networks for brain networks and its application to epilepsy
identification. <em>ARTMED</em>, <em>157</em>, 102990. (<a
href="https://doi.org/10.1016/j.artmed.2024.102990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural and functional brain networks are generated from two scan sequences of magnetic resonance imaging data, which can provide different perspectives for describing pathological changes caused by brain diseases. Recent studies found that fusing these two types of brain networks improves performance in brain disease identification. However, traditional fusion models combine these brain networks at a single granularity, ignoring the natural multi-granularity structure of brain networks that can be divided into the edge, node, and graph levels. To this end, this paper proposes a Multi-modal Multi-granularity Fusion Neural Networks (MMF-NNs) framework for brain networks, which integrates the features of the multi-modal brain network from global (i.e., graph-level) and local (i.e., edge-level and node-level) granularities to take full advantage of the topological information. Specifically, we design an interactive feature learning module at the local granularity to learn feature maps of structural and functional brain networks at the edge-level and the node-level, respectively. In that way, these two types of brain networks are fused during the feature learning process. At the global granularity, a multi-modal decomposition bilinear pooling module is designed to learn the graph-level joint representation of these brain networks. Experiments on real epilepsy datasets demonstrate that MMF-NNs are superior to several state-of-the-art methods in epilepsy identification.},
  archive      = {J_ARTMED},
  author       = {Jiashuang Huang and Xiaoyu Qi and Xueyun Cheng and Mingliang Wang and Hengrong Ju and Weiping Ding and Daoqiang Zhang},
  doi          = {10.1016/j.artmed.2024.102990},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102990},
  shortjournal = {Artif. Intell. Med.},
  title        = {MMF-NNs: Multi-modal multi-granularity fusion neural networks for brain networks and its application to epilepsy identification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing systematic reviews: An in-depth analysis on the
impact of active learning parameter combinations for biomedical abstract
screening. <em>ARTMED</em>, <em>157</em>, 102989. (<a
href="https://doi.org/10.1016/j.artmed.2024.102989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic Review (SR) are foundational to influencing policies and decision-making in healthcare and beyond. SRs thoroughly synthesise primary research on a specific topic while maintaining reproducibility and transparency. However, the rigorous nature of SRs introduces two main challenges: significant time involved and the continuously growing literature, resulting in potential data omission, making most SRs become outmoded even before they are published. As a solution, AI techniques have been leveraged to simplify the SR process, especially the abstract screening phase. Active learning (AL) has emerged as a preferred method among these AI techniques, allowing interactive learning through human input. Several AL software have been proposed for abstract screening. Despite its prowess, how the various parameters involved in AL influence the software’s efficacy is still unclear. This research seeks to demystify this by exploring how different AL strategies, such as initial training set, query strategies etc. impact SR automation. Experimental evaluations were conducted on five complex medical SR datasets, and the GLM model was used to interpret the findings statistically. Some AL variables, such as the feature extractor, initial training size, and classifiers, showed notable observations and practical conclusions were drawn within the context of SR and beyond where AL is deployed.},
  archive      = {J_ARTMED},
  author       = {Regina Ofori-Boateng and Tamy Goretty Trujillo-Escobar and Magaly Aceves-Martins and Nirmalie Wiratunga and Carlos Francisco Moreno-Garcia},
  doi          = {10.1016/j.artmed.2024.102989},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102989},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing systematic reviews: An in-depth analysis on the impact of active learning parameter combinations for biomedical abstract screening},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model development for bespoke large language models for
digital triage assistance in mental health care. <em>ARTMED</em>,
<em>157</em>, 102988. (<a
href="https://doi.org/10.1016/j.artmed.2024.102988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary large language models (LLMs) may have utility for processing unstructured, narrative free-text clinical data contained in electronic health records (EHRs) — a particularly important use-case for mental health where a majority of routinely-collected patient data lacks structured, machine-readable content. A significant problem for the United Kingdom’s National Health Service (NHS) are the long waiting lists for specialist mental healthcare. According to NHS data (NHS Digital, 2024), in each month of 2023, there were between 370,000 and 470,000 individual new referrals into secondary mental healthcare services. Referrals must be triaged by clinicians, using clinical information contained in the patient’s EHR to arrive at a decision about the most appropriate mental healthcare team to assess and potentially treat these patients. The ability to efficiently recommend a relevant team by ingesting potentially voluminous clinical notes could help services both reduce referral waiting times and with the right technology, improve the evidence available to justify triage decisions. We present and evaluate three different approaches for LLM-based, end-to-end ingestion of variable-length clinical EHR data to assist clinicians when triaging referrals. Our model is able to deliver triage recommendations consistent with existing clinical practices and its architecture was implemented on a single GPU, making it practical for implementation in resource-limited NHS environments where private implementations of LLM technology will be necessary to ensure confidential clinical data are appropriately controlled and governed. Code available at: https://github.com/NtaylorOX/BespokeLLM_Triage .},
  archive      = {J_ARTMED},
  author       = {Niall Taylor and Andrey Kormilitzin and Isabelle Lorge and Alejo Nevado-Holgado and Andrea Cipriani and Dan W. Joyce},
  doi          = {10.1016/j.artmed.2024.102988},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102988},
  shortjournal = {Artif. Intell. Med.},
  title        = {Model development for bespoke large language models for digital triage assistance in mental health care},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A self-supervised deep riemannian representation to classify
parkinsonian fixational patterns. <em>ARTMED</em>, <em>157</em>, 102987.
(<a href="https://doi.org/10.1016/j.artmed.2024.102987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s disease (PD) is the second most prevalent neurodegenerative disorder, and it remains incurable. Currently there is no definitive biomarker for detecting PD, measuring its severity, or monitoring of treatments. Recently, oculomotor fixation abnormalities have emerged as a sensitive biomarker to discriminate Parkinsonian patterns from a control population, even at early stages. For oculomotor analysis, current experimental setups use invasive and restrictive capture protocols that limit the transfer in clinical routine. Alternatively, computational approaches to support the PD diagnosis are strictly based on supervised strategies, depending of large labeled data, and introducing an inherent expert-bias. This work proposes a self-supervised architecture based on Riemannian deep representation to learn oculomotor fixation patterns from compact descriptors. Firstly, deep convolutional features are recovered from oculomotor fixation video slices, and then encoded in compact symmetric positive matrices (SPD) to summarize second-order relationships. Each SPD input matrix is projected onto a Riemannian encoder until obtain a SPD embedding. Then, a Riemannian decoder reconstructs SPD matrices while preserving the geometrical manifold structure. The proposed architecture successfully recovers geometric patterns in the embeddings without any label diagnosis supervision, and demonstrates the capability to be discriminative regarding PD patterns. In a retrospective study involving 13 healthy adults and 13 patients diagnosed with PD, the proposed Riemannian representation achieved an average accuracy of 95.6% and an AUC of 99% during a binary classification task using a Support Vector Machine.},
  archive      = {J_ARTMED},
  author       = {Edward Sandoval and Juan Olmos and Fabio Martínez},
  doi          = {10.1016/j.artmed.2024.102987},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102987},
  shortjournal = {Artif. Intell. Med.},
  title        = {A self-supervised deep riemannian representation to classify parkinsonian fixational patterns},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey of drug–target interaction analysis
in allopathy and siddha medicine. <em>ARTMED</em>, <em>157</em>, 102986.
(<a href="https://doi.org/10.1016/j.artmed.2024.102986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective drug delivery is the cornerstone of modern healthcare, ensuring therapeutic compounds reach their intended targets efficiently. This paper explores the potential of personalized and holistic healthcare, driven by the synergy between traditional and allopathic medicine systems, with a specific focus on the vast reservoir of medicinal compounds found in plants rooted in the historical legacy of traditional medicine. Motivated by the desire to unlock the therapeutic potential of medicinal plants and bridge the gap between traditional and allopathic medicine, this survey delves into in-silico computational approaches for studying Drug–Target Interactions (DTI) within the contexts of allopathy and siddha medicine. The contributions of this survey are multifaceted: it offers a comprehensive overview of in-silico methods for DTI analysis in both systems, identifies common challenges in DTI studies, provides insights into future directions to advance DTI analysis, and includes a comparative analysis of DTI in allopathy and siddha medicine. The findings of this survey highlight the pivotal role of in-silico computational approaches in advancing drug research and development in both allopathy and siddha medicine, emphasizing the importance of integrating these methods to drive the future of personalized healthcare.},
  archive      = {J_ARTMED},
  author       = {Uma E. and Mala T. and Geetha A.V. and Priyanka D.},
  doi          = {10.1016/j.artmed.2024.102986},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102986},
  shortjournal = {Artif. Intell. Med.},
  title        = {A comprehensive survey of drug–target interaction analysis in allopathy and siddha medicine},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Explanatory argument extraction of correct answers in
resident medical exams. <em>ARTMED</em>, <em>157</em>, 102985. (<a
href="https://doi.org/10.1016/j.artmed.2024.102985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing technology to assist medical experts in their everyday decision-making is currently a hot topic in the field of Artificial Intelligence (AI). This is specially true within the framework of Evidence-Based Medicine (EBM), where the aim is to facilitate the extraction of relevant information using natural language as a tool for mediating in human–AI interaction. In this context, AI techniques can be beneficial in finding arguments for past decisions in evolution notes or patient journeys, especially when different doctors are involved in a patient’s care. In those documents the decision-making process towards treating the patient is reported. Thus, applying Natural Language Processing (NLP) techniques has the potential to assist doctors in extracting arguments for a more comprehensive understanding of the decisions made. This work focuses on the explanatory argument identification step by setting up the task in a Question Answering (QA) scenario in which clinicians ask questions to the AI model to assist them in identifying those arguments. In order to explore the capabilities of current AI-based language models, we present a new dataset which, unlike previous work: (i) includes not only explanatory arguments for the correct hypothesis, but also arguments to reason on the incorrectness of other hypotheses; (ii) the explanations are written originally in Spanish by doctors to reason over cases from the Spanish Residency Medical Exams. Furthermore, this new benchmark allows us to set up a novel extractive task by identifying the explanation written by medical doctors that supports the correct answer within an argumentative text. An additional benefit of our approach lies in its ability to evaluate the extractive performance of language models using automatic metrics, which in the Antidote CasiMedicos dataset corresponds to a 74.47 F1 score. Comprehensive experimentation shows that our novel dataset and approach is an effective technique to help practitioners in identifying relevant evidence-based explanations for medical questions.},
  archive      = {J_ARTMED},
  author       = {Iakes Goenaga and Aitziber Atutxa and Koldo Gojenola and Maite Oronoz and Rodrigo Agerri},
  doi          = {10.1016/j.artmed.2024.102985},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102985},
  shortjournal = {Artif. Intell. Med.},
  title        = {Explanatory argument extraction of correct answers in resident medical exams},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic functional connections analysis with spectral
learning for brain disorder detection. <em>ARTMED</em>, <em>157</em>,
102984. (<a href="https://doi.org/10.1016/j.artmed.2024.102984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Yanfang Xue and Hui Xue and Pengfei Fang and Shipeng Zhu and Lishan Qiao and Yuexuan An},
  doi          = {10.1016/j.artmed.2024.102984},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102984},
  shortjournal = {Artif. Intell. Med.},
  title        = {Dynamic functional connections analysis with spectral learning for brain disorder detection},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSR-DTA: Substructure-aware multi-layer graph neural
networks for drug–target binding affinity prediction. <em>ARTMED</em>,
<em>157</em>, 102983. (<a
href="https://doi.org/10.1016/j.artmed.2024.102983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of drug–target binding affinity (DTA) is essential in the field of drug discovery. Recently, scientists have been attempting to utilize artificial intelligence prediction to screen out a significant number of ineffective compounds, thereby mitigating labor and financial losses. While graph neural networks (GNNs) have been applied to DTA, existing GNNs have limitations in effectively extracting substructural features across various sizes. Functional groups play a crucial role in modulating molecular properties, but existing GNNs struggle with feature extraction from certain motifs due to scale mismatches. Additionally, sequence-based models for target proteins lack the integration of structural information. To address these limitations, we present SSR-DTA, a multi-layer graph network capable of adapting to diverse structural sizes, which can extract richer biological features, thereby improving the robustness and accuracy of predictions. Multi-layer GNNs enable the capture of molecular motifs across different scales, ranging from atomic to macrocyclic motifs. Furthermore, we introduce BiGNN to simultaneously learn sequence and structural information. Sequence information corresponds to the primary structure of proteins, while graph information represents the tertiary structure. BiGNN assimilates richer information compared to sequence-based methods while mitigating the impact of errors from predicted structures, resulting in more accurate predictions. Through rigorous experimental evaluations conducted on four benchmark datasets, we demonstrate the superiority of SSR-DTA over state-of-the-art models. Particularly, in comparison to state-of-the-art models, SSR-DTA demonstrates an impressive 20% reduction in mean squared error on the Davis dataset and a 5% reduction on the KIBA dataset, underscoring its potential as a valuable tool for advancing DTA prediction.},
  archive      = {J_ARTMED},
  author       = {Yuansheng Liu and Xinyan Xia and Yongshun Gong and Bosheng Song and Xiangxiang Zeng},
  doi          = {10.1016/j.artmed.2024.102983},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102983},
  shortjournal = {Artif. Intell. Med.},
  title        = {SSR-DTA: Substructure-aware multi-layer graph neural networks for drug–target binding affinity prediction},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating federated learning for improved counterfactual
explanations in clinical decision support systems for sepsis therapy.
<em>ARTMED</em>, <em>157</em>, 102982. (<a
href="https://doi.org/10.1016/j.artmed.2024.102982">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have witnessed both artificial intelligence obtaining remarkable results in clinical decision support systems (CDSSs) and explainable artificial intelligence (XAI) improving the interpretability of these models. In turn, this fosters the adoption by medical personnel and improves trustworthiness of CDSSs. Among others, counterfactual explanations prove to be one such XAI technique particularly suitable for the healthcare domain due to its ease of interpretation, even for less technically proficient staff. However, the generation of high-quality counterfactuals relies on generative models for guidance. Unfortunately, training such models requires a huge amount of data that is beyond the means of ordinary hospitals. In this paper, we therefore propose to use federated learning to allow multiple hospitals to jointly train such generative models while maintaining full data privacy. We demonstrate the superiority of our approach compared to locally generated counterfactuals. Moreover, we prove that generative models for counterfactual generation that are trained using federated learning in a suitable environment perform only marginally worse compared to centrally trained ones while offering the benefit of data privacy preservation. Finally, we integrate our method into a prototypical CDSS for treatment recommendation for sepsis patients, thus providing a proof of concept for real-world application as well as insights and sanity checks from clinical application.},
  archive      = {J_ARTMED},
  author       = {Christoph Düsing and Philipp Cimiano and Sebastian Rehberg and Christiane Scherer and Olaf Kaup and Christiane Köster and Stefan Hellmich and Daniel Herrmann and Kirsten Laura Meier and Simon Claßen and Rainer Borgstedt},
  doi          = {10.1016/j.artmed.2024.102982},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102982},
  shortjournal = {Artif. Intell. Med.},
  title        = {Integrating federated learning for improved counterfactual explanations in clinical decision support systems for sepsis therapy},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RECOMED: A comprehensive pharmaceutical recommendation
system. <em>ARTMED</em>, <em>157</em>, 102981. (<a
href="https://doi.org/10.1016/j.artmed.2024.102981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Mariam Zomorodi and Ismail Ghodsollahee and Jennifer H Martin and Nicholas J Talley and Vahid Salari and Paweł Pławiak and Kazem Rahimi and U.R. Acharya},
  doi          = {10.1016/j.artmed.2024.102981},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102981},
  shortjournal = {Artif. Intell. Med.},
  title        = {RECOMED: A comprehensive pharmaceutical recommendation system},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive analytics of COVID-19 vaccine research: From
topic modeling to topic classification. <em>ARTMED</em>, <em>157</em>,
102980. (<a href="https://doi.org/10.1016/j.artmed.2024.102980">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {COVID-19 vaccine research has played a vital role in successfully controlling the pandemic, and the research surrounding the coronavirus vaccine is ever-evolving and accruing. These enormous efforts in knowledge production necessitate a structured analysis as secondary research to extract useful insights. In this study, comprehensive analytics was performed to extract these insights, which has moved the boundaries of data analytics in secondary research in the vaccine field by utilizing topic modeling, sentiment analysis, and topic classification based on the abstracts of related publications indexed in Scopus and PubMed. By applying topic modeling to 4803 abstracts filtered by this study criterion, 8 research arenas were identified by merging related topics. The extracted research areas were entitled “Reporting,” “Acceptance,” “Reaction,” “Surveyed Opinions,” “Pregnancy,” “Titer of Variants,” “Categorized Surveys,” and “International Approaches.” Moreover, the investigation of topics sentiments variations over time led to identifying researchers&#39; attitudes and focus in various years from 2020 to 2022. Finally, a CNN-LSTM classification model was developed to predict the dominant topics and sentiments of new documents based on the 25 pre-determined topics with 75 % accuracy. The findings of this study can be utilized for future research design in this area by quickly grasping the structure of the current research on the COVID-19 vaccine. Through the findings of current research, a classification model was developed to classify the topic of a new article as one of the identified topics. Also, vaccine manufacturing firms will achieve a niche market by having a schema to invest in the gap of fields that have yet to be concentrated in extracted topics.},
  archive      = {J_ARTMED},
  author       = {Saeed Rouhani and Fatemeh Mozaffari},
  doi          = {10.1016/j.artmed.2024.102980},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102980},
  shortjournal = {Artif. Intell. Med.},
  title        = {Comprehensive analytics of COVID-19 vaccine research: From topic modeling to topic classification},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SG-fusion: A swin-transformer and graph convolution-based
multi-modal deep neural network for glioma prognosis. <em>ARTMED</em>,
<em>157</em>, 102972. (<a
href="https://doi.org/10.1016/j.artmed.2024.102972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of morphological attributes extracted from histopathological images and genomic data holds significant importance in advancing tumor diagnosis, prognosis, and grading. Histopathological images are acquired through microscopic examination of tissue slices, providing valuable insights into cellular structures and pathological features. On the other hand, genomic data provides information about tumor gene expression and functionality. The fusion of these two distinct data types is crucial for gaining a more comprehensive understanding of tumor characteristics and progression. In the past, many studies relied on single-modal approaches for tumor diagnosis. However, these approaches had limitations as they were unable to fully harness the information from multiple data sources. To address these limitations, researchers have turned to multi-modal methods that concurrently leverage both histopathological images and genomic data. These methods better capture the multifaceted nature of tumors and enhance diagnostic accuracy. Nonetheless, existing multi-modal methods have, to some extent, oversimplified the extraction processes for both modalities and the fusion process. In this study, we presented a dual-branch neural network, namely SG-Fusion. Specifically, for the histopathological modality, we utilize the Swin-Transformer structure to capture both local and global features and incorporate contrastive learning to encourage the model to discern commonalities and differences in the representation space. For the genomic modality, we developed a graph convolutional network based on gene functional and expression level similarities. Additionally, our model integrates a cross-attention module to enhance information interaction and employs divergence-based regularization to enhance the model’s generalization performance. Validation conducted on glioma datasets from the Cancer Genome Atlas unequivocally demonstrates that our SG-Fusion model outperforms both single-modal methods and existing multi-modal approaches in both survival analysis and tumor grading.},
  archive      = {J_ARTMED},
  author       = {Minghan Fu and Ming Fang and Rayyan Azam Khan and Bo Liao and Zhanli Hu and Fang-Xiang Wu},
  doi          = {10.1016/j.artmed.2024.102972},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102972},
  shortjournal = {Artif. Intell. Med.},
  title        = {SG-fusion: A swin-transformer and graph convolution-based multi-modal deep neural network for glioma prognosis},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MAPRS: An intelligent approach for post-prescription review
based on multi-label learning. <em>ARTMED</em>, <em>157</em>, 102971.
(<a href="https://doi.org/10.1016/j.artmed.2024.102971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Antimicrobial resistance (AMR) is a major threat to public health worldwide. It is a promising way to improve appropriate prescription by the review and stewardship of antimicrobials, and Post-Prescription Review (PPR) is currently the main tool used in hospitals. Existing methods of PPR typically focus on the dichotomy of antimicrobial prescription based on binary classification which, however, is usually a multi-label classification problem. Moreover, previous research did not explain the causes beneath the inappropriate antimicrobial used in the clinical setting, which could be practically important for problem location and decision improvement. In this paper, we collected antimicrobial prescriptions and related data from clean surgery in a hospital in northeastern China, and proposed a Multi-label Antimicrobial Post-Prescription Review System (MAPRS). MAPRS first uses NLP techniques to process unstructured data in prescriptions and explores the value of clinical record text for solving medical problems. Then, Classifier Chains are used to deal with multi-label problems and fused with machine learning algorithms to construct a classifier. At last, a SHAP explanation module is introduced to explain the inappropriate prescriptions. The experimental results show that MAPRS could achieve great performance in a challenging six-category multi-label task, with a subset accuracy of 90.7 % and an average AUROC of 94.3 %. Our results can help hospitals to perform intelligent prescription review and improve the antimicrobial stewardship.},
  archive      = {J_ARTMED},
  author       = {Guangfei Yang and Ziyao Zhou and Aili Ding and Yuanfeng Cai and Fanli Kong and Yalin Xi and Nannan Liu},
  doi          = {10.1016/j.artmed.2024.102971},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102971},
  shortjournal = {Artif. Intell. Med.},
  title        = {MAPRS: An intelligent approach for post-prescription review based on multi-label learning},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CMCN: Chinese medical concept normalization using continual
learning and knowledge-enhanced. <em>ARTMED</em>, <em>157</em>, 102965.
(<a href="https://doi.org/10.1016/j.artmed.2024.102965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical Concept Normalization (MCN) is a crucial process for deep information extraction and natural language processing tasks, which plays a vital role in biomedical research. Although MCN in English has achieved significant research achievements, Chinese medical concept normalization (CMCN) remains insufficiently explored due to its complex syntactic structure and the paucity of Chinese medical semantic and ontology resources. In recent years, deep learning has been extensively applied across numerous natural language processing tasks, owing to its robust learning capabilities, adaptability, and transferability. It has proven to be well suited for intricate and specialized knowledge discovery research in the biomedical field. In this study, we conduct research on CMCN through the lens of deep learning. Specifically, our research introduces a model that leverages polymorphic semantic information and knowledge enhanced through multi-task learning and retain more important medical features through continual learning. As the cornerstone of CMCN, disease names are the main focus of this research. We evaluated various methodologies on Chinese disease dataset built by ourselves, finally achieving 76.12 % on Accuracy@1, 87.20 % on Accuracy@5 and 90.02 % on Accuracy@10 with our best-performing model GCBM-BSCL. This research not only advances the fields of knowledge mining and medical concept normalization but also enhances the integration and application of artificial intelligence in the medical and health field. We have published the source code and results on https://github.com/BearLiX/CMCN .},
  archive      = {J_ARTMED},
  author       = {Pu Han and Xiong Li and Zhanpeng Zhang and Yule Zhong and Liang Gu and Yingying Hua and Xiaoyan Li},
  doi          = {10.1016/j.artmed.2024.102965},
  journal      = {Artificial Intelligence in Medicine},
  month        = {11},
  pages        = {102965},
  shortjournal = {Artif. Intell. Med.},
  title        = {CMCN: Chinese medical concept normalization using continual learning and knowledge-enhanced},
  volume       = {157},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). From zero to hero: Harnessing transformers for biomedical
named entity recognition in zero- and few-shot contexts.
<em>ARTMED</em>, <em>156</em>, 102970. (<a
href="https://doi.org/10.1016/j.artmed.2024.102970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised named entity recognition (NER) in the biomedical domain depends on large sets of annotated texts with the given named entities. The creation of such datasets can be time-consuming and expensive, while extraction of new entities requires additional annotation tasks and retraining the model. This paper proposes a method for zero- and few-shot NER in the biomedical domain to address these challenges. The method is based on transforming the task of multi-class token classification into binary token classification and pre-training on a large number of datasets and biomedical entities, which allows the model to learn semantic relations between the given and potentially novel named entity labels. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based model. The results demonstrate the effectiveness of the proposed method for recognizing new biomedical entities with no or limited number of examples, outperforming previous transformer-based methods, and being comparable to GPT3-based models using models with over 1000 times fewer parameters. We make models and developed code publicly available.},
  archive      = {J_ARTMED},
  author       = {Miloš Košprdić and Nikola Prodanović and Adela Ljajić and Bojana Bašaragin and Nikola Milošević},
  doi          = {10.1016/j.artmed.2024.102970},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102970},
  shortjournal = {Artif. Intell. Med.},
  title        = {From zero to hero: Harnessing transformers for biomedical named entity recognition in zero- and few-shot contexts},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust blind color deconvolution and blood detection on
histological images using bayesian k-SVD. <em>ARTMED</em>, <em>156</em>,
102969. (<a href="https://doi.org/10.1016/j.artmed.2024.102969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hematoxylin and Eosin (H&amp;E) color variation among histological images from different laboratories can significantly degrade the performance of Computer-Aided Diagnosis systems. The staining procedure is the primary factor responsible for color variation, and consequently, the methods designed to reduce such variations are designed in concordance with this procedure. In particular, Blind Color Deconvolution (BCD) methods aim to identify the true underlying colors in the image and to separate the tissue structure from the color information. Unfortunately, BCD methods often assume that images are stained solely with pure staining colors (e.g., blue and pink for H&amp;E). This assumption does not hold true when common artifacts such as blood are present, requiring an additional color component to represent them. This is a challenge for color standardization algorithms, which are unable to correctly identify the stains in the image, leading to unexpected results. In this work, we propose a Blood-Robust Bayesian K-Singular Value Decomposition model designed to simultaneously detect blood and extract color from histological images while preserving structural details. We evaluate our method using both synthetic and real images, which contain varying amounts of blood pixels.},
  archive      = {J_ARTMED},
  author       = {Fernando Pérez-Bueno and Kjersti Engan and Rafael Molina},
  doi          = {10.1016/j.artmed.2024.102969},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102969},
  shortjournal = {Artif. Intell. Med.},
  title        = {Robust blind color deconvolution and blood detection on histological images using bayesian K-SVD},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Remote assessment of eczema severity via AI-powered skin
image analytics: A systematic review. <em>ARTMED</em>, <em>156</em>,
102968. (<a href="https://doi.org/10.1016/j.artmed.2024.102968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various studies have been published on the remote assessment of eczema severity from digital camera images. Successful deployment of an accurate and robust AI-powered tool for such purposes can aid the formulation of eczema treatment plans and assist in patient monitoring. This review aims to provide an overview of the quality of published studies on this topic and to identify challenges and suggestions to improve the robustness and reliability of existing tools. We identified 25 articles from the Scopus database that aimed to assess eczema severity automatically from digital camera images by eczema area detection ( n =13), which is important for prior delineation of the most relevant clinical features, and/or severity prediction ( n =12). Deep learning methods ( n =14) were more commonly used in recent years over conventional machine learning ( n =11). A set of 20 pre-defined criteria were used for critical appraisal in this study. Study quality was hindered in many cases due to dataset challenges, with only 28% of studies reporting patient age range and 16% reporting skin phototype range. Furthermore, 52% of studies utilised solely non-public datasets and only 17% provided open-source access to code repositories, making validation of experimental results a significant challenge. In terms of algorithm design, attempts to improve model accuracy and process automation are widely reported. However, there remains limited implementation of methods for explicitly improving model trustworthiness and robustness. There is a need for a high-quality dataset with a sufficient number of bias-free images and consistent labels, as well as improved image analytics methods, to enhance the state of remote eczema severity assessment algorithms. Improving the interpretability and explainability of developed tools will further improve long-term reliability and trustworthiness.},
  archive      = {J_ARTMED},
  author       = {Leo Huang and Wai Hoh Tang and Rahman Attar and Claudia Gore and Hywel C. Williams and Adnan Custovic and Reiko J. Tanaka},
  doi          = {10.1016/j.artmed.2024.102968},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102968},
  shortjournal = {Artif. Intell. Med.},
  title        = {Remote assessment of eczema severity via AI-powered skin image analytics: A systematic review},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced ICD-10 code assignment of clinical texts: A
summarization-based approach. <em>ARTMED</em>, <em>156</em>, 102967. (<a
href="https://doi.org/10.1016/j.artmed.2024.102967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assigning International Classification of Diseases (ICD) codes to clinical texts is a common and crucial practice in patient classification, hospital management, and further statistics analysis. Current auto-coding methods mainly transfer this task to a multi-label classification problem. Such solutions are suffering from high-dimensional mapping space and excessive redundant information in long clinical texts. To alleviate such a situation, we introduce text summarization methods to the ICD coding regime and apply text matching to select ICD codes. We focus on the tenth revision of the ICD (ICD-10) coding and design a novel summarization-based approach (SuM) with an end-to-end strategy to efficiently assign ICD-10 code to clinical texts. In this approach, a knowledge-guided pointer network is purposed to distill and summarize key information in clinical texts precisely. Then a matching model with matching-aggregation architecture follows to align the summary result with code, tuning the one-vs-all scenario to one-vs-one matching so that the large-label-space obstacle laid in classification approaches would be avoided. The 12,788 ICD-10 coded discharge summaries from a Chinese hospital were collected to evaluate the proposed approach. Compared with existing methods, the purposed model achieves the greatest coding results with Micro AUC of 0.9548, MRR@10 of 0.7977, Precision@10 of 0.0944, and Recall@10 of 0.9439 for the TOP-50 Dataset. Results on the FULL-Dataset remain consistent. Also, the proposed knowledge encoder and applied end-to-end strategy are proven to facilitate the whole model to gain efficacy in selecting the most suitable code. The proposed automatic ICD-10 code assignment approach via text summarization can effectively capture critical messages in long clinical texts and improve the performance of ICD-10 coding of clinical texts.},
  archive      = {J_ARTMED},
  author       = {Yaoqian Sun and Lei Sang and Dan Wu and Shilin He and Yani Chen and Huilong Duan and Han Chen and Xudong Lu},
  doi          = {10.1016/j.artmed.2024.102967},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102967},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhanced ICD-10 code assignment of clinical texts: A summarization-based approach},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated transtibial prosthesis alignment: A systematic
review. <em>ARTMED</em>, <em>156</em>, 102966. (<a
href="https://doi.org/10.1016/j.artmed.2024.102966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This comprehensive systematic review critically analyzes the current progress and challenges in automating transtibial prosthesis alignment. The manual identification of alignment changes in prostheses has been found to lack reliability, necessitating the development of automated processes. Through a rigorous systematic search across major electronic databases, this review includes the highly relevant studies out of an initial pool of 2111 records. The findings highlight the urgent need for automated alignment systems in individuals with transtibial amputation. The selected studies represent cutting-edge research, employing diverse approaches such as advanced machine learning algorithms and innovative alignment tools, to automate the detection and adjustment of prosthesis alignment. Collectively, this review emphasizes the immense potential of automated transtibial prosthesis alignment systems to enhance alignment accuracy and significantly reduce human error. Furthermore, it identifies important limitations in the reviewed studies, serving as a catalyst for future research to address these gaps and explore alternative machine learning algorithms. The insights derived from this systematic review provide valuable guidance for researchers, clinicians, and developers aiming to propel the field of automated transtibial prosthesis alignment forward.},
  archive      = {J_ARTMED},
  author       = {Taha Khamis and Abd Alghani Khamis and Mouaz Al Kouzbary and Hamza Al Kouzbary and Hamam Mokayed and Nasrul Anuar AbdRazak and Noor Azuan AbuOsman},
  doi          = {10.1016/j.artmed.2024.102966},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102966},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automated transtibial prosthesis alignment: A systematic review},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiomics and eXplainable artificial intelligence for
decision support in insulin resistance early diagnosis: A pediatric
population-based longitudinal study. <em>ARTMED</em>, <em>156</em>,
102962. (<a href="https://doi.org/10.1016/j.artmed.2024.102962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pediatric obesity can drastically heighten the risk of cardiometabolic alterations later in life, with insulin resistance standing as the cornerstone linking adiposity to the increased cardiovascular risk. Puberty has been pointed out as a critical stage after which obesity-associated insulin resistance is more difficult to revert. Timely prediction of insulin resistance in pediatric obesity is therefore vital for mitigating the risk of its associated comorbidities. The construction of effective and robust predictive systems for a complex health outcome like insulin resistance during the early stages of life demands the adoption of longitudinal designs for more causal inferences, and the integration of factors of varying nature involved in its onset. In this work, we propose an eXplainable Artificial Intelligence-based decision support pipeline for early diagnosis of insulin resistance in a longitudinal cohort of 90 children. For that, we leverage multi-omics (genomics and epigenomics) and clinical data from the pre-pubertal stage. Different data layers combinations, pre-processing techniques (missing values, feature selection, class imbalance, etc.), algorithms, training procedures were considered following good practices for Machine Learning. SHapley Additive exPlanations were provided for specialists to understand both the decision-making mechanisms of the system and the impact of the features on each automatic decision, an essential issue in high-risk areas such as this one where system decisions may affect people’s lives. The system showed a relevant predictive ability (AUC and G-mean of 0.92). A deep exploration, both at the global and the local level, revealed promising biomarkers of insulin resistance in our population, highlighting classical markers, such as Body Mass Index z-score or leptin/adiponectin ratio, and novel ones such as methylation patterns of relevant genes, such as HDAC4 , PTPRN2 , MATN2 , RASGRF1 and EBF1 . Our findings highlight the importance of integrating multi-omics data and following eXplainable Artificial Intelligence trends when building decision support systems.},
  archive      = {J_ARTMED},
  author       = {Álvaro Torres-Martos and Augusto Anguita-Ruiz and Mireia Bustos-Aibar and Alberto Ramírez-Mena and María Arteaga and Gloria Bueno and Rosaura Leis and Concepción M. Aguilera and Rafael Alcalá and Jesús Alcalá-Fdez},
  doi          = {10.1016/j.artmed.2024.102962},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102962},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multiomics and eXplainable artificial intelligence for decision support in insulin resistance early diagnosis: A pediatric population-based longitudinal study},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FA-net: A hierarchical feature fusion and interactive
attention-based network for dose prediction in liver cancer patients.
<em>ARTMED</em>, <em>156</em>, 102961. (<a
href="https://doi.org/10.1016/j.artmed.2024.102961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dose prediction is a crucial step in automated radiotherapy planning for liver cancer. Several deep learning-based approaches for dose prediction have been proposed to enhance the design efficiency and quality of radiotherapy plan. However, these approaches usually take CT images and contours of organs at risk (OARs) and planning target volume (PTV) as a multi-channel input and is thus difficult to extract sufficient feature information from each input, which results in unsatisfactory dose distribution. In this paper, we propose a novel dose prediction network for liver cancer based on hierarchical feature fusion and interactive attention. A feature extraction module is first constructed to extract multi-scale features from different inputs, and a hierarchical feature fusion module is then built to fuse these multi-scale features hierarchically. A decoder based on attention mechanism is designed to gradually reconstruct the fused features into dose distribution. Additionally, we design an autoencoder network to generate a perceptual loss during training stage, which is used to improve the accuracy of dose prediction. The proposed method is tested on private clinical dataset and obtains HI and CI of 0.31 and 0.87, respectively. The experimental results are better than those by several existing methods, indicating that the dose distribution generated by the proposed method is close to that approved in clinics. The codes are available at https://github.com/hired-ld/FA-Net .},
  archive      = {J_ARTMED},
  author       = {Miao Liao and Shuanhu Di and Yuqian Zhao and Wei Liang and Zhen Yang},
  doi          = {10.1016/j.artmed.2024.102961},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102961},
  shortjournal = {Artif. Intell. Med.},
  title        = {FA-net: A hierarchical feature fusion and interactive attention-based network for dose prediction in liver cancer patients},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to the special issue on IEEE CBMS 2022 mining
healthcare: AI and machine learning for biomedicine. <em>ARTMED</em>,
<em>156</em>, 102954. (<a
href="https://doi.org/10.1016/j.artmed.2024.102954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Rosa Sicilia and Linlin Shen and Alejandro Rodríguez-González and KC Santosh and Peter J.F. Lucas},
  doi          = {10.1016/j.artmed.2024.102954},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102954},
  shortjournal = {Artif. Intell. Med.},
  title        = {Introduction to the special issue on IEEE CBMS 2022 mining healthcare: AI and machine learning for biomedicine},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). COPDVD: Automated classification of chronic obstructive
pulmonary disease on a new collected and evaluated voice dataset.
<em>ARTMED</em>, <em>156</em>, 102953. (<a
href="https://doi.org/10.1016/j.artmed.2024.102953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic obstructive pulmonary disease (COPD) is a severe condition affecting millions worldwide, leading to numerous annual deaths. The absence of significant symptoms in its early stages promotes high underdiagnosis rates for the affected people. Besides pulmonary function failure, another harmful problem of COPD is the systemic effects, e.g., heart failure or voice distortion. However, the systemic effects of COPD might provide valuable information for early detection. In other words, symptoms caused by systemic effects could be helpful to detect the condition in its early stages. The proposed study aims to explore whether the voice features extracted from the vowel “a” utterance carry any information that can be predictive of COPD by employing Machine Learning (ML) on a newly collected voice dataset. Forty-eight participants were recruited from the pool of research clinic visitors at Blekinge Institute of Technology (BTH) in Sweden between January 2022 and May 2023. A dataset consisting of 1246 recordings from 48 participants was gathered. The collection of voice recordings containing the vowel “a” utterance commenced following an information and consent meeting with each participant using the VoiceDiagnostic application. The collected voice data was subjected to silence segment removal, feature extraction of baseline acoustic features, and Mel Frequency Cepstrum Coefficients (MFCC). Sociodemographic data was also collected from the participants. Three ML models were investigated for the binary classification of COPD and healthy controls: Random Forest (RF), Support Vector Machine (SVM), and CatBoost (CB). A nested k-fold cross-validation approach was employed. Additionally, the hyperparameters were optimized using grid-search on each ML model. For best performance assessment, accuracy, F1-score, precision, and recall metrics were computed. Afterward, we further examined the best classifier by utilizing the Area Under the Curve (AUC), Average Precision (AP), and SHapley Additive exPlanations (SHAP) feature-importance measures. The classifiers RF, SVM, and CB achieved a maximum accuracy of 77 %, 69 %, and 78 % on the test set and 93 %, 78 % and 97 % on the validation set, respectively. The CB classifier outperformed RF and SVM. After further investigation of the best-performing classifier, CB demonstrated the highest performance, producing an AUC of 82 % and AP of 76 %. In addition to age and gender, the mean values of baseline acoustic and MFCC features demonstrate high importance and deterministic characteristics for classification performance in both test and validation sets, though in varied order. This study concludes that the utterance of vowel “a” recordings contain information that can be captured by the CatBoost classifier with high accuracy for the classification of COPD. Additionally, baseline acoustic and MFCC features, in conjunction with age and gender information, can be employed for classification purposes and benefit healthcare for decision support in COPD diagnosis. NCT05897944 .},
  archive      = {J_ARTMED},
  author       = {Alper Idrisoglu and Ana Luiza Dallora and Abbas Cheddad and Peter Anderberg and Andreas Jakobsson and Johan Sanmartin Berglund},
  doi          = {10.1016/j.artmed.2024.102953},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102953},
  shortjournal = {Artif. Intell. Med.},
  title        = {COPDVD: Automated classification of chronic obstructive pulmonary disease on a new collected and evaluated voice dataset},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic two-dimensional &amp; three-dimensional video
analysis with deep learning for movement disorders: A systematic review.
<em>ARTMED</em>, <em>156</em>, 102952. (<a
href="https://doi.org/10.1016/j.artmed.2024.102952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of computer vision technology and increased usage of video cameras in clinical settings have facilitated advancements in movement disorder analysis. This review investigated these advancements in terms of providing practical, low-cost solutions for the diagnosis and analysis of movement disorders, such as Parkinson’s disease, ataxia, dyskinesia, and Tourette syndrome. Traditional diagnostic methods for movement disorders are typically reliant on the subjective assessment of motor symptoms, which poses inherent challenges. Furthermore, early symptoms are often overlooked, and overlapping symptoms across diseases can complicate early diagnosis. Consequently, deep learning has been used for the objective video-based analysis of movement disorders. This study systematically reviewed the latest advancements in automatic two-dimensional &amp; three-dimensional video analysis using deep learning for movement disorders. We comprehensively analyzed the literature published until September 2023 by searching the Web of Science, PubMed, Scopus, and Embase databases. We identified 68 relevant studies and extracted information on their objectives, datasets, modalities, and methodologies. The study aimed to identify, catalogue, and present the most significant advancements, offering a consolidated knowledge base on the role of video analysis and deep learning in movement disorder analysis. First, the objectives, including specific PD symptom quantification, ataxia assessment, cerebral palsy assessment, gait disorder analysis, tremor assessment, tic detection (in the context of Tourette syndrome), dystonia assessment, and abnormal movement recognition were discussed. Thereafter, the datasets used in the study were examined. Subsequently, video modalities and deep learning methodologies related to the topic were investigated. Finally, the challenges and opportunities in terms of datasets, interpretability, evaluation methods, and home/remote monitoring were discussed.},
  archive      = {J_ARTMED},
  author       = {Wei Tang and Peter M.A. van Ooijen and Deborah A. Sival and Natasha M. Maurits},
  doi          = {10.1016/j.artmed.2024.102952},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102952},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automatic two-dimensional &amp; three-dimensional video analysis with deep learning for movement disorders: A systematic review},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ACP-ESM: A novel framework for classification of anticancer
peptides using protein-oriented transformer approach. <em>ARTMED</em>,
<em>156</em>, 102951. (<a
href="https://doi.org/10.1016/j.artmed.2024.102951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticancer peptides (ACPs) are a class of molecules that have gained significant attention in the field of cancer research and therapy. ACPs are short chains of amino acids, the building blocks of proteins, and they possess the ability to selectively target and kill cancer cells. One of the key advantages of ACPs is their ability to selectively target cancer cells while sparing healthy cells to a greater extent. This selectivity is often attributed to differences in the surface properties of cancer cells compared to normal cells. That is why ACPs are being investigated as potential candidates for cancer therapy. ACPs may be used alone or in combination with other treatment modalities like chemotherapy and radiation therapy. While ACPs hold promise as a novel approach to cancer treatment, there are challenges to overcome, including optimizing their stability, improving selectivity, and enhancing their delivery to cancer cells, continuous increasing in number of peptide sequences, developing a reliable and precise prediction model. In this work, we propose an efficient transformer-based framework to identify ACPs for by performing accurate a reliable and precise prediction model. For this purpose, four different transformer models, namely ESM, ProtBERT, BioBERT, and SciBERT are employed to detect ACPs from amino acid sequences. To demonstrate the contribution of the proposed framework, extensive experiments are carried on widely-used datasets in the literature, two versions of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of proposed model enhances classification accuracy when compared to the literature studies. The proposed framework, ESM, exhibits 96.45% of accuracy for AntiCp2 dataset, 97.66% of accuracy for cACP-DeepGram dataset, and 88.51% of accuracy for ACP-740 dataset, thence determining new state-of-the-art. The code of proposed framework is publicly available at github ( https://github.com/mstf-yalcin/acp-esm ).},
  archive      = {J_ARTMED},
  author       = {Zeynep Hilal Kilimci and Mustafa Yalcin},
  doi          = {10.1016/j.artmed.2024.102951},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102951},
  shortjournal = {Artif. Intell. Med.},
  title        = {ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning applications in preventive healthcare: A
systematic literature review on predictive analytics of disease
comorbidity from multiple perspectives. <em>ARTMED</em>, <em>156</em>,
102950. (<a href="https://doi.org/10.1016/j.artmed.2024.102950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence is constantly revolutionizing biomedical research and healthcare management. Disease comorbidity is a major threat to the quality of life for susceptible groups, especially middle-aged and elderly patients. The presence of multiple chronic diseases makes precision diagnosis challenging to realize and imposes a heavy burden on the healthcare system and economy. Given an enormous amount of accumulated health data, machine learning techniques show their capability in handling this puzzle. The present study conducts a review to uncover current research efforts in applying these methods to understanding comorbidity mechanisms and making clinical predictions considering these complex patterns. A descriptive metadata analysis of 791 unique publications aims to capture the overall research progression between January 2012 and June 2023. To delve into comorbidity-focused research, 61 of these scientific papers are systematically assessed. Four predictive analytics of tasks are detected: disease comorbidity data extraction, clustering, network, and risk prediction. It is observed that some machine learning-driven applications address inherent data deficiencies in healthcare datasets and provide a model interpretation that identifies significant risk factors of comorbidity development. Based on insights, both technical and practical, gained from relevant literature, this study intends to guide future interests in comorbidity research and draw conclusions about chronic disease prevention and diagnosis with managerial implications.},
  archive      = {J_ARTMED},
  author       = {Duo Xu and Zeshui Xu},
  doi          = {10.1016/j.artmed.2024.102950},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102950},
  shortjournal = {Artif. Intell. Med.},
  title        = {Machine learning applications in preventive healthcare: A systematic literature review on predictive analytics of disease comorbidity from multiple perspectives},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic review of few-shot learning in medical imaging.
<em>ARTMED</em>, <em>156</em>, 102949. (<a
href="https://doi.org/10.1016/j.artmed.2024.102949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis speed and robustness. This systematic review gives a comprehensive overview of few-shot learning methods for medical image analysis, aiming to establish a standard methodological pipeline for future research reference. With a particular emphasis on the role of meta-learning, we analysed 80 relevant articles published from 2018 to 2023, conducting a risk of bias assessment and extracting relevant information, especially regarding the employed learning techniques. From this, we delineated a comprehensive methodological pipeline shared among all studies. In addition, we performed a statistical analysis of the studies’ results concerning the clinical task and the meta-learning method employed while also presenting supplemental information such as imaging modalities and model robustness evaluation techniques. We discussed the findings of our analysis, providing a deep insight into the limitations of the state-of-the-art methods and the most promising approaches. Drawing on our investigation, we yielded recommendations on potential future research directions aiming to bridge the gap between research and clinical practice.},
  archive      = {J_ARTMED},
  author       = {Eva Pachetti and Sara Colantonio},
  doi          = {10.1016/j.artmed.2024.102949},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102949},
  shortjournal = {Artif. Intell. Med.},
  title        = {A systematic review of few-shot learning in medical imaging},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing metagenomic classification with compression-based
features. <em>ARTMED</em>, <em>156</em>, 102948. (<a
href="https://doi.org/10.1016/j.artmed.2024.102948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metagenomics is a rapidly expanding field that uses next-generation sequencing technology to analyze the genetic makeup of environmental samples. However, accurately identifying the organisms in a metagenomic sample can be complex, and traditional reference-based methods may need to be more effective in some instances. In this study, we present a novel approach for metagenomic identification, using data compressors as a feature for taxonomic classification. By evaluating a comprehensive set of compressors, including both general-purpose and genomic-specific, we demonstrate the effectiveness of this method in accurately identifying organisms in metagenomic samples. The results indicate that using features from multiple compressors can help identify taxonomy. An overall accuracy of 95% was achieved using this method using an imbalanced dataset with classes with limited samples. The study also showed that the correlation between compression and classification is insignificant, highlighting the need for a multi-faceted approach to metagenomic identification. This approach offers a significant advancement in the field of metagenomics, providing a reference-less method for taxonomic identification that is both effective and efficient while revealing insights into the statistical and algorithmic nature of genomic data. The code to validate this study is publicly available at https://github.com/ieeta-pt/xgTaxonomy .},
  archive      = {J_ARTMED},
  author       = {Jorge Miguel Silva and João Rafael Almeida},
  doi          = {10.1016/j.artmed.2024.102948},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102948},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing metagenomic classification with compression-based features},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning using privileged information with logistic
regression on acute respiratory distress syndrome detection.
<em>ARTMED</em>, <em>156</em>, 102947. (<a
href="https://doi.org/10.1016/j.artmed.2024.102947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advanced learning paradigm, learning using privileged information (LUPI), leverages information in training that is not present at the time of prediction. In this study, we developed privileged logistic regression (PLR) models under the LUPI paradigm to detect acute respiratory distress syndrome (ARDS), with mechanical ventilation variables or chest x-ray image features employed in the privileged domain and electronic health records in the base domain. In model training, the objective of privileged logistic regression was designed to incorporate data from the privileged domain and encourage knowledge transfer across the privileged and base domains. An asymptotic analysis was also performed, yielding sufficient conditions under which the addition of privileged information increases the rate of convergence in the proposed model. Results for ARDS detection show that PLR models achieve better classification performances than logistic regression models trained solely on the base domain, even when privileged information is partially available. Furthermore, PLR models demonstrate performance on par with or superior to state-of-the-art models under the LUPI paradigm. As the proposed models are effective, easy to interpret, and highly explainable, they are ideal for other clinical applications where privileged information is at least partially available.},
  archive      = {J_ARTMED},
  author       = {Zijun Gao and Shuyang Cheng and Emily Wittrup and Jonathan Gryak and Kayvan Najarian},
  doi          = {10.1016/j.artmed.2024.102947},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102947},
  shortjournal = {Artif. Intell. Med.},
  title        = {Learning using privileged information with logistic regression on acute respiratory distress syndrome detection},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special issue on human-centered artificial intelligence for
one health. <em>ARTMED</em>, <em>156</em>, 102946. (<a
href="https://doi.org/10.1016/j.artmed.2024.102946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Paolo Buono and Nadia Berthouze and Maria Francesca Costabile and Adela Grando and Andreas Holzinger},
  doi          = {10.1016/j.artmed.2024.102946},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102946},
  shortjournal = {Artif. Intell. Med.},
  title        = {Special issue on human-centered artificial intelligence for one health},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Walking representation and simulation based on multi-source
image fusion and multi-agent reinforcement learning for gait
rehabilitation. <em>ARTMED</em>, <em>156</em>, 102945. (<a
href="https://doi.org/10.1016/j.artmed.2024.102945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the formulation of strategies for walking rehabilitation, achieving precise identification of the current state and making rational predictions about the future state are crucial but often unrealized. To tackle this challenge, our study introduces a unified framework that integrates a novel 3D walking motion capture method using multi-source image fusion and a walking rehabilitation simulation approach based on multi-agent reinforcement learning. We found that, (i) the proposal achieved an accurate 3D walking motion capture and outperforms other advanced methods. Experimental evidence indicates that, compared to similar visual skeleton tracking methods, the proposed approach yields results with higher Pearson correlation ( r = 0 . 93 r=0.93 ), intra-class correlation coefficient ( I C C ( 2 , 1 ) = 0 . 91 ICC(2,1)=0.91 ), and narrower confidence intervals ( [ 0 . 90 , 0 . 95 ] [0.90,0.95] for r r , [ 0 . 88 , 0 . 94 ] [0.88,0.94] for I C C ( 2 , 1 ) ICC(2,1) ) when compared to standard results. The outcomes of the proposed approach also exhibit commendable correlation and concurrence with those obtained through the IMU-based skeleton tracking method in the assessment of gait parameters ( [ 0 . 85 , 0 . 89 ] [0.85,0.89] for r r , [ 0 . 75 , 0 . 81 ] [0.75,0.81] for I C C ( 2 , 1 ) ICC(2,1) ); (ii) multi-agent reinforcement learning has the potential to be used to solve the simulation task of gait rehabilitation. In mimicry experiment, our proposed simulation method for gait rehabilitation not only enables the intelligent agent to converge from the initial state to the target state, but also observes evolutionary patterns similar to those observed in clinical practice through motor state resolution. This study offers valuable contributions to walking rehabilitation, enabling precise assessment and simulation-based interventions, with potential implications for clinical practice and patient outcomes.},
  archive      = {J_ARTMED},
  author       = {Yean Zhu and Meirong Xiao and Dan Robbins and Xiaoying Wu and Wei Lu and Wensheng Hou},
  doi          = {10.1016/j.artmed.2024.102945},
  journal      = {Artificial Intelligence in Medicine},
  month        = {10},
  pages        = {102945},
  shortjournal = {Artif. Intell. Med.},
  title        = {Walking representation and simulation based on multi-source image fusion and multi-agent reinforcement learning for gait rehabilitation},
  volume       = {156},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MedExpQA: Multilingual benchmarking of large language models
for medical question answering. <em>ARTMED</em>, <em>155</em>, 102938.
(<a href="https://doi.org/10.1016/j.artmed.2024.102938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support. This potential has been illustrated by the state-of-the-art performance obtained by LLMs in Medical Question Answering, with striking results such as passing marks in licensing medical exams. However, while impressive, the required quality bar for medical applications remains far from being achieved. Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content. Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions. Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic. In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering. To the best of our knowledge, MedExpQA includes for the first time reference gold explanations, written by medical doctors, of the correct and incorrect options in the exams. Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs, with best results around 75 accuracy for English, still has large room for improvement, especially for languages other than English, for which accuracy drops 10 points. Therefore, despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering. Data, code, and fine-tuned models will be made publicly available. 1},
  archive      = {J_ARTMED},
  author       = {Iñigo Alonso and Maite Oronoz and Rodrigo Agerri},
  doi          = {10.1016/j.artmed.2024.102938},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102938},
  shortjournal = {Artif. Intell. Med.},
  title        = {MedExpQA: Multilingual benchmarking of large language models for medical question answering},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian network analysis of risk classification strategies
in the regulation of cellular products. <em>ARTMED</em>, <em>155</em>,
102937. (<a href="https://doi.org/10.1016/j.artmed.2024.102937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell therapy, a burgeoning therapeutic strategy, necessitates a scientific regulatory framework but faces challenges in risk-based regulation due to the lack of a global consensus on risk classification. This study applies Bayesian network analysis to compare and evaluate the risk classification strategies for cellular products proposed by the Food and Drug Administration (FDA), Ministry of Health, Labour and Welfare (MHLW), and World Health Organization (WHO), using real-world data to validate the models. The appropriateness of key risk factors is assessed within the three regulatory frameworks, along with their implications for clinical safety. The results indicate several directions for refining risk classification approaches. Additionally, a substudy focuses on a specific type of cell and gene therapy (CGT), chimeric antigen receptor (CAR) T cell therapy. It underscores the importance of considering CAR targets, tumor types, and costimulatory domains when assessing the safety risks of CAR T cell products. Overall, there is currently a lack of a regulatory framework based on real-world data for cellular products and a lack of risk-based classification review methods. This study aims to improve the regulatory system for cellular products, emphasizing risk-based classification. Furthermore, the study advocates for leveraging machine learning in regulatory science to enhance the assessment of cellular product safety, illustrating the role of Bayesian networks in aiding regulatory decision-making for the risk classification of cellular products.},
  archive      = {J_ARTMED},
  author       = {Guoshu Jia and Lixia Fu and Likun Wang and Dongning Yao and Yimin Cui},
  doi          = {10.1016/j.artmed.2024.102937},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102937},
  shortjournal = {Artif. Intell. Med.},
  title        = {Bayesian network analysis of risk classification strategies in the regulation of cellular products},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tackling heterogeneity in medical federated learning via
aligning vision transformers. <em>ARTMED</em>, <em>155</em>, 102936. (<a
href="https://doi.org/10.1016/j.artmed.2024.102936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables training models on distributed, privacy-sensitive medical imaging data. However, data heterogeneity across participating institutions leads to reduced model performance and fairness issues, especially for underrepresented datasets. To address these challenges, we propose leveraging the multi-head attention mechanism in Vision Transformers to align the representations of heterogeneous data across clients. By focusing on the attention mechanism as the alignment objective, our approach aims to improve both the accuracy and fairness of federated learning models in medical imaging applications. We evaluate our method on the IQ-OTH/NCCD Lung Cancer dataset, simulating various levels of data heterogeneity using Latent Dirichlet Allocation (LDA). Our results demonstrate that our approach achieves competitive performance compared to state-of-the-art federated learning methods across different heterogeneity levels and improves the performance of models for underrepresented clients, promoting fairness in the federated learning setting. These findings highlight the potential of leveraging the multi-head attention mechanism to address the challenges of data heterogeneity in medical federated learning.},
  archive      = {J_ARTMED},
  author       = {Erfan Darzi and Yiqing Shen and Yangming Ou and Nanna M. Sijtsema and P.M.A van Ooijen},
  doi          = {10.1016/j.artmed.2024.102936},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102936},
  shortjournal = {Artif. Intell. Med.},
  title        = {Tackling heterogeneity in medical federated learning via aligning vision transformers},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comprehensive review of deep learning in orthopaedics:
Applications, challenges, trustworthiness, and fusion. <em>ARTMED</em>,
<em>155</em>, 102935. (<a
href="https://doi.org/10.1016/j.artmed.2024.102935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) in orthopaedics has gained significant attention in recent years. Previous studies have shown that DL can be applied to a wide variety of orthopaedic tasks, including fracture detection, bone tumour diagnosis, implant recognition, and evaluation of osteoarthritis severity. The utilisation of DL is expected to increase, owing to its ability to present accurate diagnoses more efficiently than traditional methods in many scenarios. This reduces the time and cost of diagnosis for patients and orthopaedic surgeons. To our knowledge, no exclusive study has comprehensively reviewed all aspects of DL currently used in orthopaedic practice. This review addresses this knowledge gap using articles from Science Direct, Scopus, IEEE Xplore, and Web of Science between 2017 and 2023. The authors begin with the motivation for using DL in orthopaedics, including its ability to enhance diagnosis and treatment planning. The review then covers various applications of DL in orthopaedics, including fracture detection, detection of supraspinatus tears using MRI, osteoarthritis, prediction of types of arthroplasty implants, bone age assessment, and detection of joint-specific soft tissue disease. We also examine the challenges for implementing DL in orthopaedics, including the scarcity of data to train DL and the lack of interpretability, as well as possible solutions to these common pitfalls. Our work highlights the requirements to achieve trustworthiness in the outcomes generated by DL, including the need for accuracy, explainability, and fairness in the DL models. We pay particular attention to fusion techniques as one of the ways to increase trustworthiness, which have also been used to address the common multimodality in orthopaedics. Finally, we have reviewed the approval requirements set forth by the US Food and Drug Administration to enable the use of DL applications. As such, we aim to have this review function as a guide for researchers to develop a reliable DL application for orthopaedic tasks from scratch for use in the market.},
  archive      = {J_ARTMED},
  author       = {Laith Alzubaidi and Khamael AL-Dulaimi and Asma Salhi and Zaenab Alammar and Mohammed A. Fadhel and A.S. Albahri and A.H. Alamoodi and O.S. Albahri and Amjad F. Hasan and Jinshuai Bai and Luke Gilliland and Jing Peng and Marco Branni and Tristan Shuker and Kenneth Cutbush and Jose Santamaría and Catarina Moreira and Chun Ouyang and Ye Duan and Mohamed Manoufali and Mohammad Jomaa and Ashish Gupta and Amin Abbosh and Yuantong Gu},
  doi          = {10.1016/j.artmed.2024.102935},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102935},
  shortjournal = {Artif. Intell. Med.},
  title        = {Comprehensive review of deep learning in orthopaedics: Applications, challenges, trustworthiness, and fusion},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning algorithms for melanoma detection using
dermoscopic images: A systematic review and meta-analysis.
<em>ARTMED</em>, <em>155</em>, 102934. (<a
href="https://doi.org/10.1016/j.artmed.2024.102934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Melanoma is a serious risk to human health and early identification is vital for treatment success. Deep learning (DL) has the potential to detect cancer using imaging technologies and many studies provide evidence that DL algorithms can achieve high accuracy in melanoma diagnostics. To critically assess different DL performances in diagnosing melanoma using dermatoscopic images and discuss the relationship between dermatologists and DL. Ovid-Medline, Embase, IEEE Xplore, and the Cochrane Library were systematically searched from inception until 7th December 2021. Studies that reported diagnostic DL model performances in detecting melanoma using dermatoscopic images were included if they had specific outcomes and histopathologic confirmation. Binary diagnostic accuracy data and contingency tables were extracted to analyze outcomes of interest, which included sensitivity (SEN), specificity (SPE), and area under the curve (AUC). Subgroup analyses were performed according to human-machine comparison and cooperation. The study was registered in PROSPERO, CRD42022367824. 2309 records were initially retrieved, of which 37 studies met our inclusion criteria, and 27 provided sufficient data for meta-analytical synthesis. The pooled SEN was 82 % (range 77–86), SPE was 87 % (range 84–90), with an AUC of 0.92 (range 0.89–0.94). Human-machine comparison had pooled AUCs of 0.87 (0.84–0.90) and 0.83 (0.79–0.86) for DL and dermatologists, respectively. Pooled AUCs were 0.90 (0.87–0.93), 0.80 (0.76–0.83), and 0.88 (0.85–0.91) for DL, and junior and senior dermatologists, respectively. Analyses of human-machine cooperation were 0.88 (0.85–0.91) for DL, 0.76 (0.72–0.79) for unassisted, and 0.87 (0.84–0.90) for DL-assisted dermatologists. Evidence suggests that DL algorithms are as accurate as senior dermatologists in melanoma diagnostics. Therefore, DL could be used to support dermatologists in diagnostic decision-making. Although, further high-quality, large-scale multicenter studies are required to address the specific challenges associated with medical AI-based diagnostics.},
  archive      = {J_ARTMED},
  author       = {Zichen Ye and Daqian Zhang and Yuankai Zhao and Mingyang Chen and Huike Wang and Samuel Seery and Yimin Qu and Peng Xue and Yu Jiang},
  doi          = {10.1016/j.artmed.2024.102934},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102934},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning algorithms for melanoma detection using dermoscopic images: A systematic review and meta-analysis},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A human–AI interaction paradigm and its application to
rhinocytology. <em>ARTMED</em>, <em>155</em>, 102933. (<a
href="https://doi.org/10.1016/j.artmed.2024.102933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores Human-Centered Artificial Intelligence (HCAI) in medical cytology, with a focus on enhancing the interaction with AI. It presents a Human–AI interaction paradigm that emphasizes explainability and user control of AI systems. It is an iterative negotiation process based on three interaction strategies aimed to (i) elaborate the system outcomes through iterative steps ( Iterative Exploration ), (ii) explain the AI system’s behavior or decisions ( Clarification ), and (iii) allow non-expert users to trigger simple retraining of the AI model ( Reconfiguration ). This interaction paradigm is exploited in the redesign of an existing AI-based tool for microscopic analysis of the nasal mucosa. The resulting tool is tested with rhinocytologists. The article discusses the analysis of the results of the conducted evaluation and outlines lessons learned that are relevant for AI in medicine.},
  archive      = {J_ARTMED},
  author       = {Giuseppe Desolda and Giovanni Dimauro and Andrea Esposito and Rosa Lanzilotti and Maristella Matera and Massimo Zancanaro},
  doi          = {10.1016/j.artmed.2024.102933},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102933},
  shortjournal = {Artif. Intell. Med.},
  title        = {A Human–AI interaction paradigm and its application to rhinocytology},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CHNet: A multi-task global–local collaborative hybrid
network for KRAS mutation status prediction in colorectal cancer.
<em>ARTMED</em>, <em>155</em>, 102931. (<a
href="https://doi.org/10.1016/j.artmed.2024.102931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of Kirsten rat sarcoma (KRAS) mutation status is crucial for personalized treatment of advanced colorectal cancer patients. However, despite the excellent performance of deep learning models in certain aspects, they often overlook the synergistic promotion among multiple tasks and the consideration of both global and local information, which can significantly reduce prediction accuracy. To address these issues, this paper proposes an innovative method called the Multi-task Global–Local Collaborative Hybrid Network (CHNet) aimed at more accurately predicting patients’ KRAS mutation status. CHNet consists of two branches that can extract global and local features from segmentation and classification tasks, respectively, and exchange complementary information to collaborate in executing these tasks. Within the two branches, we have designed a Channel-wise Hybrid Transformer (CHT) and a Spatial-wise Hybrid Transformer (SHT). These transformers integrate the advantages of both Transformer and CNN, employing cascaded hybrid attention and convolution to capture global and local information from the two tasks. Additionally, we have created an Adaptive Collaborative Attention (ACA) module to facilitate the collaborative fusion of segmentation and classification features through guidance. Furthermore, we introduce a novel Class Activation Map (CAM) loss to encourage CHNet to learn complementary information between the two tasks. We evaluate CHNet on the T2-weighted MRI dataset, and achieve an accuracy of 88.93% in KRAS mutation status prediction, which outperforms the performance of representative KRAS mutation status prediction methods. The results suggest that our CHNet can more accurately predict KRAS mutation status in patients via a multi-task collaborative facilitation and considering global–local information way, which can assist doctors in formulating more personalized treatment strategies for patients.},
  archive      = {J_ARTMED},
  author       = {Meiling Cai and Lin Zhao and Yan Qiang and Long Wang and Juanjuan Zhao},
  doi          = {10.1016/j.artmed.2024.102931},
  journal      = {Artificial Intelligence in Medicine},
  month        = {9},
  pages        = {102931},
  shortjournal = {Artif. Intell. Med.},
  title        = {CHNet: A multi-task global–local collaborative hybrid network for KRAS mutation status prediction in colorectal cancer},
  volume       = {155},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction of freezing of gait in parkinson’s disease based
on multi-channel time-series neural network. <em>ARTMED</em>,
<em>154</em>, 102932. (<a
href="https://doi.org/10.1016/j.artmed.2024.102932">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Freezing of Gait (FOG) is a noticeable symptom of Parkinson’s disease, like being stuck in place and increasing the risk of falls. The wearable multi-channel sensor system is an efficient method to predict and monitor the FOG, thus warning the wearer to avoid falls and improving the quality of life. However, the existing approaches for the prediction of FOG mainly focus on a single sensor system and cannot handle the interference between multi-channel wearable sensors . Hence, we propose a novel multi-channel time-series neural network (MCT-Net) approach to merge multi-channel gait features into a comprehensive prediction framework, alerting patients to FOG symptoms in advance. Owing to the causal distributed convolution, MCT-Net is a real-time method available to give optimal prediction earlier and implemented in remote devices. Moreover, intra-channel and inter-channel transformers of MCT-Net extract and integrate different sensor position features into a unified deep learning model. Compared with four other state-of-the-art FOG prediction baselines, the proposed MCT-Net obtains 96.21% in accuracy and 80.46% in F1-score on average 2 s before FOG occurrence, demonstrating the superiority of MCT-Net.},
  archive      = {J_ARTMED},
  author       = {Boyan Wang and Xuegang Hu and Rongjun Ge and Chenchu Xu and Jinglin Zhang and Zhifan Gao and Shu Zhao and Kemal Polat},
  doi          = {10.1016/j.artmed.2024.102932},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102932},
  shortjournal = {Artif. Intell. Med.},
  title        = {Prediction of freezing of gait in parkinson’s disease based on multi-channel time-series neural network},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probing perfection: The relentless art of meddling for
pulmonary airway segmentation from HRCT via a human-AI collaboration
based active learning method. <em>ARTMED</em>, <em>154</em>, 102930. (<a
href="https://doi.org/10.1016/j.artmed.2024.102930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of pulmonary tracheal segmentation, the scarcity of annotated data stands as a prevalent pain point in most medical segmentation endeavors. Concurrently, most Deep Learning (DL) methodologies employed in this domain invariably grapple with other dual challenges: the inherent opacity of ‘black box’ models and the ongoing pursuit of performance enhancement. In response to these intertwined challenges, the core concept of our Human-Computer Interaction (HCI) based learning models (RS_UNet, LC_UNet, UUNet and WD_UNet) hinge on the versatile combination of diverse query strategies and an array of deep learning models . We train four HCI models based on the initial training dataset and sequentially repeat the following steps 1–4: (1) Query Strategy: Our proposed HCI models selects those samples which contribute the most additional representative information when labeled in each iteration of the query strategy (showing the names and sequence numbers of the samples to be annotated). Additionally, in this phase, the model selects the unlabeled samples with the greatest predictive disparity by calculating the Wasserstein Distance, Least Confidence, Entropy Sampling, and Random Sampling. (2) Central line correction: The selected samples in previous stage are then used for domain expert correction of the system-generated tracheal central lines in each training round. (3) Update training dataset: When domain experts are involved in each epoch of the DL model&#39;s training iterations, they update the training dataset with greater precision after each epoch, thereby enhancing the trustworthiness of the ‘black box’ DL model and improving the performance of models. (4) Model training: Proposed HCI model is trained using the updated training dataset and an enhanced version of existing UNet. Experimental results validate the effectiveness of this Human-Computer Interaction-based approaches, demonstrating that our proposed WD-UNet, LC-UNet, UUNet, RS-UNet achieve comparable or even superior performance than the state-of-the-art DL models, such as WD-UNet with only 15 %–35 % of the training data , leading to substantial reductions (65 %–85 % reduction of annotation effort) in physician annotation time.},
  archive      = {J_ARTMED},
  author       = {Shiyi Wang and Yang Nan and Sheng Zhang and Federico Felder and Xiaodan Xing and Yingying Fang and Javier Del Ser and Simon L.F. Walsh and Guang Yang},
  doi          = {10.1016/j.artmed.2024.102930},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102930},
  shortjournal = {Artif. Intell. Med.},
  title        = {Probing perfection: The relentless art of meddling for pulmonary airway segmentation from HRCT via a human-AI collaboration based active learning method},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Can physician judgment enhance model trustworthiness? A case
study on predicting pathological lymph nodes in rectal cancer.
<em>ARTMED</em>, <em>154</em>, 102929. (<a
href="https://doi.org/10.1016/j.artmed.2024.102929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainability is key to enhancing the trustworthiness of artificial intelligence in medicine. However, there exists a significant gap between physicians’ expectations for model explainability and the actual behavior of these models. This gap arises from the absence of a consensus on a physician-centered evaluation framework, which is needed to quantitatively assess the practical benefits that effective explainability should offer practitioners. Here, we hypothesize that superior attention maps, as a mechanism of model explanation, should align with the information that physicians focus on, potentially reducing prediction uncertainty and increasing model reliability. We employed a multimodal transformer to predict lymph node metastasis of rectal cancer using clinical data and magnetic resonance imaging. We explored how well attention maps, visualized through a state-of-the-art technique, can achieve agreement with physician understanding. Subsequently, we compared two distinct approaches for estimating uncertainty: a standalone estimation using only the variance of prediction probability, and a human-in-the-loop estimation that considers both the variance of prediction probability and the quantified agreement. Our findings revealed no significant advantage of the human-in-the-loop approach over the standalone one. In conclusion, this case study did not confirm the anticipated benefit of the explanation in enhancing model reliability. Superficial explanations could do more harm than good by misleading physicians into relying on uncertain predictions, suggesting that the current state of attention mechanisms should not be overestimated in the context of model explainability.},
  archive      = {J_ARTMED},
  author       = {Kazuma Kobayashi and Yasuyuki Takamizawa and Mototaka Miyake and Sono Ito and Lin Gu and Tatsuya Nakatsuka and Yu Akagi and Tatsuya Harada and Yukihide Kanemitsu and Ryuji Hamamoto},
  doi          = {10.1016/j.artmed.2024.102929},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102929},
  shortjournal = {Artif. Intell. Med.},
  title        = {Can physician judgment enhance model trustworthiness? a case study on predicting pathological lymph nodes in rectal cancer},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A systematic literature review on the significance of deep
learning and machine learning in predicting alzheimer’s disease.
<em>ARTMED</em>, <em>154</em>, 102928. (<a
href="https://doi.org/10.1016/j.artmed.2024.102928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is the most prevalent cause of dementia, characterized by a steady decline in mental, behavioral, and social abilities and impairs a person&#39;s capacity for independent functioning. It is a fatal neurodegenerative disease primarily affecting older adults. The purpose of this literature review is to investigate various AD detection techniques, datasets, input modalities, algorithms, libraries, and performance evaluation metrics used to determine which model or strategy may provide superior performance. The initial search yielded 807 papers, but only 100 research articles were chosen after applying the inclusion-exclusion criteria. This SLR analyzed research items published between January 2019 and December 2022. The ACM, Elsevier, IEEE Xplore Digital Library, PubMed, Springer and Taylor &amp; Francis were systematically searched. The current study considers articles that used Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), APOe4 genotype, Diffusion Tensor Imaging (DTI) and Cerebrospinal Fluid (CSF) biomarkers. The study was performed following Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guidelines. According to the literature survey, most studies ( n = 76) used the DL strategy. The datasets used by studies were primarily derived from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database. The majority of studies ( n = 73) used single-modality neuroimaging data, while the remaining used multi-modal input data. In a multi-modality approach, the combination of MRI and PET scans is commonly preferred. Also, Regarding the algorithm used, Convolution Neural Network (CNN) showed the highest accuracy, 100 %, in classifying AD vs. CN subjects whereas the SVM was the most common ML algorithm, with a maximum accuracy of 99.82 %.},
  archive      = {J_ARTMED},
  author       = {Arshdeep Kaur and Meenakshi Mittal and Jasvinder Singh Bhatti and Suresh Thareja and Satwinder Singh},
  doi          = {10.1016/j.artmed.2024.102928},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102928},
  shortjournal = {Artif. Intell. Med.},
  title        = {A systematic literature review on the significance of deep learning and machine learning in predicting alzheimer&#39;s disease},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhancing stroke risk and prognostic timeframe assessment
with deep learning and a broad range of retinal biomarkers.
<em>ARTMED</em>, <em>154</em>, 102927. (<a
href="https://doi.org/10.1016/j.artmed.2024.102927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke stands as a major global health issue, causing high death and disability rates and significant social and economic burdens. The effectiveness of existing stroke risk assessment methods is questionable due to their use of inconsistent and varying biomarkers, which may lead to unpredictable risk evaluations. This study introduces an automatic deep learning-based system for predicting stroke risk (both ischemic and hemorrhagic) and estimating the time frame of its occurrence, utilizing a comprehensive set of known retinal biomarkers from fundus images. Our system, tested on the UK Biobank and DRSSW datasets, achieved AUROC scores of 0.83 (95% CI: 0.79–0.85) and 0.93 (95% CI: 0.9–0.95), respectively. These results not only highlight our system’s advantage over established benchmarks but also underscore the predictive power of retinal biomarkers in assessing stroke risk and the unique effectiveness of each biomarker. Additionally, the correlation between retinal biomarkers and cardiovascular diseases broadens the potential application of our system, making it a versatile tool for predicting a wide range of cardiovascular conditions .},
  archive      = {J_ARTMED},
  author       = {Shvat Messica and Dan Presil and Yaacov Hoch and Tsvi Lev and Aviel Hadad and Or Katz and David R. Owens},
  doi          = {10.1016/j.artmed.2024.102927},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102927},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhancing stroke risk and prognostic timeframe assessment with deep learning and a broad range of retinal biomarkers},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient pyramid channel attention network for pathological
myopia recognition with pretraining-and-finetuning. <em>ARTMED</em>,
<em>154</em>, 102926. (<a
href="https://doi.org/10.1016/j.artmed.2024.102926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pathological myopia (PM) is the leading ocular disease for impaired vision worldwide. Clinically, the characteristics of pathology distribution in PM are global-local on the fundus image, which plays a significant role in assisting clinicians in diagnosing PM. However, most existing deep neural networks focused on designing complex architectures but rarely explored the pathology distribution prior of PM. To tackle this issue, we propose an efficient pyramid channel attention (EPCA) module, which fully leverages the potential of the clinical pathology prior of PM with pyramid pooling and multi-scale context fusion. Then, we construct EPCA-Net for automatic PM recognition based on fundus images by stacking a sequence of EPCA modules. Moreover, motivated by the recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained natural image models for PM recognition by freezing them and treating the EPCA and other attention modules as adapters. In addition, we construct a PM recognition benchmark termed PM-fundus by collecting fundus images of PM from publicly available datasets. The comprehensive experiments demonstrate the superiority of EPCA-Net over state-of-the-art methods in the PM recognition task. For example, EPCA-Net achieves 97.56% accuracy and outperforms ViT by 2.85% accuracy on the PM-fundus dataset. The results also show that our method based on the pretraining-and-finetuning paradigm achieves competitive performance through comparisons to part of previous methods based on traditional fine-tuning paradigm with fewer tunable parameters, which has the potential to leverage more natural image foundation models to address the PM recognition task in limited medical data regime.},
  archive      = {J_ARTMED},
  author       = {Xiaoqing Zhang and Jilu Zhao and Yan Li and Hao Wu and Xiangtian Zhou and Jiang Liu},
  doi          = {10.1016/j.artmed.2024.102926},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102926},
  shortjournal = {Artif. Intell. Med.},
  title        = {Efficient pyramid channel attention network for pathological myopia recognition with pretraining-and-finetuning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging VQ-VAE tokenization for autoregressive modeling
of medical time series. <em>ARTMED</em>, <em>154</em>, 102925. (<a
href="https://doi.org/10.1016/j.artmed.2024.102925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present CodeAR, a medical time series generative model for electronic health record (EHR) synthesis. CodeAR employs autoregressive modeling on discrete tokens obtained using a vector quantized-variational autoencoder (VQ-VAE), which addresses key challenges of accurate distribution modeling and patient privacy preservation in the medical domain. The proposed model is trained with next-token prediction instead of a regression problem for more accurate distribution modeling, where the autoregressive property of CodeAR is useful to capture the inherent causality in time series data . In addition, the compressive property of the VQ-VAE prevents CodeAR from memorizing the original training data , which ensures patient privacy. Experimental results demonstrate that CodeAR outperforms the baseline autoregressive-based and GAN-based models in terms of maximum mean discrepancy (MMD) and Train on Synthetic, Test on Real tests. Our results highlight the effectiveness of autoregressive modeling on discrete tokens, the utility of CodeAR in causal modeling, and its robustness against data memorization.},
  archive      = {J_ARTMED},
  author       = {Yoonhyung Lee and Younhyung Chae and Kyomin Jung},
  doi          = {10.1016/j.artmed.2024.102925},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102925},
  shortjournal = {Artif. Intell. Med.},
  title        = {Leveraging VQ-VAE tokenization for autoregressive modeling of medical time series},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reshaping free-text radiology notes into structured reports
with generative question answering transformers. <em>ARTMED</em>,
<em>154</em>, 102924. (<a
href="https://doi.org/10.1016/j.artmed.2024.102924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently, the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness, and information retrieval. We propose a pipeline to extract information from Italian free-text radiology reports that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. Our work aims to leverage the potential of Natural Language Processing and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 Italian radiology reports, we investigate a rule-free generative Question Answering approach based on the Italian-specific version of T5: IT5. To address information content discrepancies, we focus on the six most frequently filled items in the annotations made on the reports: three categorical (multichoice), one free-text (free-text), and two continuous numerical (factual). In the preprocessing phase, we encode also information that is not supposed to be entered. Two strategies (batch-truncation and ex-post combination) are implemented to comply with the IT5 context length limitations. Performance is evaluated in terms of strict accuracy, f1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model. Unlike multichoice and factual, free-text answers do not have 1-to-1 correspondence with their reference annotations. For this reason, we collect human-expert feedback on the similarity between medical annotations and generated free-text answers, using a 5-point Likert scale questionnaire (evaluating the criteria of correctness and completeness). The combination of fine-tuning and batch splitting allows IT5 ex-post combination to achieve notable results in terms of information extraction of different types of structured data, performing on par with GPT-3.5. Human-based assessment scores of free-text answers show a high correlation with the AI performance metrics f1 (Spearman&#39;s correlation coefficients&gt;0.5, p -values&lt;0.001) for both IT5 ex-post combination and GPT-3.5. The latter is better at generating plausible human-like statements, even if it systematically provides answers even when they are not supposed to be given. In our experimental setting, a fine-tuned Transformer-based model with a modest number of parameters (i.e., IT5, 220 M) performs well as a clinical information extraction system for automatic SR registry filling task. It can extract information from more than one place in the report, elaborating it in a manner that complies with the response specifications provided by the SR registry (for multichoice and factual items), or that closely approximates the work of a human-expert (free-text items); with the ability to discern when an answer is supposed to be given or not to a user query.},
  archive      = {J_ARTMED},
  author       = {Laura Bergomi and Tommaso M. Buonocore and Paolo Antonazzo and Lorenzo Alberghi and Riccardo Bellazzi and Lorenzo Preda and Chandra Bortolotto and Enea Parimbelli},
  doi          = {10.1016/j.artmed.2024.102924},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102924},
  shortjournal = {Artif. Intell. Med.},
  title        = {Reshaping free-text radiology notes into structured reports with generative question answering transformers},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Vision-based estimation of fatigue and engagement in
cognitive training sessions. <em>ARTMED</em>, <em>154</em>, 102923. (<a
href="https://doi.org/10.1016/j.artmed.2024.102923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computerized cognitive training (CCT) is a scalable, well-tolerated intervention that has promise for slowing cognitive decline. The effectiveness of CCT is often affected by a lack of effective engagement. Mental fatigue is a the primary factor for compromising effective engagement in CCT, particularly in older adults at risk for dementia. There is a need for scalable, automated measures that can constantly monitor and reliably detect mental fatigue during CCT. Here, we develop and validate a novel Recurrent Video Transformer (RVT) method for monitoring real-time mental fatigue in older adults with mild cognitive impairment using their video-recorded facial gestures during CCT. The RVT model achieved the highest balanced accuracy (79.58%) and precision (0.82) compared to the prior models for binary and multi-class classification of mental fatigue. We also validated our model by significantly relating to reaction time across CCT tasks ( Wald χ 2 = 5 . 16 , p = 0 . 023 Waldχ2=5.16,p=0.023 ). By leveraging dynamic temporal information, the RVT model demonstrates the potential to accurately measure real-time mental fatigue, laying the foundation for future CCT research aiming to enhance effective engagement by timely prevention of mental fatigue.},
  archive      = {J_ARTMED},
  author       = {Yanchen Wang and Adam Turnbull and Yunlong Xu and Kathi Heffner and Feng Vankee Lin and Ehsan Adeli},
  doi          = {10.1016/j.artmed.2024.102923},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102923},
  shortjournal = {Artif. Intell. Med.},
  title        = {Vision-based estimation of fatigue and engagement in cognitive training sessions},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ConvLSNet: A lightweight architecture based on ConvLSTM
model for the classification of pulmonary conditions using multichannel
lung sound recordings. <em>ARTMED</em>, <em>154</em>, 102922. (<a
href="https://doi.org/10.1016/j.artmed.2024.102922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterization of lung sounds (LS) is indispensable for diagnosing respiratory pathology. Although conventional neural networks (NNs) have been widely employed for the automatic diagnosis of lung sounds, deep neural networks can potentially be more useful than conventional NNs by allowing accurate classification without requiring preprocessing and feature extraction. Utilizing the long short-term memory (LSTM) layers to reveal the sequence-based properties of the LS time series, a novel architecture consisting of a cascade of convolutional long short-term memory (ConvLSTM) and LSTM layers, namely ConvLSNet is developed, which permits highly accurate diagnosis of pulmonary disease states. By modeling the multichannel lung sounds through the ConvLSTM layer, the proposed ConvLSNet architecture can concurrently deal with the spatial and temporal properties of the six-channel LS recordings without heavy preprocessing or data transformation. Notably, the proposed model achieves a classification accuracy of 97.4 % based on LS data corresponding to three pulmonary conditions, namely asthma, COPD , and the healthy state. Compared with architectures consisting exclusively of CNN or LSTM layers, as well as those employing a cascade integration of 2DCNN and LSTM layers, the proposed ConvLSNet architecture exhibited the highest classification accuracy, while imposing the lowest computational cost as quantified by the number of parameters, training time, and learning rate.},
  archive      = {J_ARTMED},
  author       = {Faezeh Majzoobi and Mohammad Bagher Khodabakhshi and Shahriar Jamasb and Sobhan Goudarzi},
  doi          = {10.1016/j.artmed.2024.102922},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102922},
  shortjournal = {Artif. Intell. Med.},
  title        = {ConvLSNet: A lightweight architecture based on ConvLSTM model for the classification of pulmonary conditions using multichannel lung sound recordings},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards a comprehensive bedside swallow screening protocol
using cross-domain transformation and high-resolution cervical
auscultation. <em>ARTMED</em>, <em>154</em>, 102921. (<a
href="https://doi.org/10.1016/j.artmed.2024.102921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution cervical auscultation (HRCA) is an emerging noninvasive and accessible option to assess swallowing by relying upon accelerometry and sound sensors. HRCA has shown tremendous promise and accuracy in identifying and predicting swallowing physiology and biomechanics with accuracies equivalent to trained human judges. These insights have historically been available only through instrumental swallowing evaluation methods, such as videofluoroscopy and endoscopy . HRCA uses supervised learning techniques to interpret swallowing physiology from the acquired signals, which are collected during radiographic assessment of swallowing using barium contrast. Conversely, bedside swallowing screening is typically conducted in non-radiographic settings using only water. This poses a challenge to translating and generalizing HRCA algorithms to bedside screening due to the rheological differences between barium and water. To address this gap, we proposed a cross-domain transformation framework that uses cycle generative adversarial networks to convert HRCA signals of water swallows into a domain compatible with the barium swallows-trained HRCA algorithms. The proposed framework achieved a cross-domain transformation accuracy that surpassed 90%. The authenticity of the generated signals was confirmed using a binary classifier to confirm the framework’s capability to produce indistinguishable signals. This framework was also assessed for retaining swallow physiological and biomechanical properties in the signals by applying an existing model from the literature that identifies the opening and closure of the upper esophageal sphincter . The outcomes of this model showed nearly identical results between the generated and original signals. These findings suggest that the proposed transformation framework is a feasible avenue to advance HCRA towards clinical deployment for water-based swallowing screenings.},
  archive      = {J_ARTMED},
  author       = {Ayman Anwar and Yassin Khalifa and Erin Lucatorto and James L. Coyle and Ervin Sejdic},
  doi          = {10.1016/j.artmed.2024.102921},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102921},
  shortjournal = {Artif. Intell. Med.},
  title        = {Towards a comprehensive bedside swallow screening protocol using cross-domain transformation and high-resolution cervical auscultation},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). End-to-end offline reinforcement learning for glycemia
control. <em>ARTMED</em>, <em>154</em>, 102920. (<a
href="https://doi.org/10.1016/j.artmed.2024.102920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of closed-loop systems for glycemia control in type I diabetes relies heavily on simulated patients. Improving the performances and adaptability of these close-loops raises the risk of over-fitting the simulator. This may have dire consequences, especially in unusual cases which were not faithfully – if at all – captured by the simulator. To address this, we propose to use model-free offline RL agents, trained on real patient data, to perform the glycemia control. To further improve the performances, we propose an end-to-end personalization pipeline, which leverages offline-policy evaluation methods to remove altogether the need of a simulator, while still enabling an estimation of clinically relevant metrics for diabetes.},
  archive      = {J_ARTMED},
  author       = {Tristan Beolet and Alice Adenis and Erik Huneker and Maxime Louis},
  doi          = {10.1016/j.artmed.2024.102920},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102920},
  shortjournal = {Artif. Intell. Med.},
  title        = {End-to-end offline reinforcement learning for glycemia control},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSM-net: Semi-supervised multi-task network for joint lesion
segmentation and classification from pancreatic EUS images.
<em>ARTMED</em>, <em>154</em>, 102919. (<a
href="https://doi.org/10.1016/j.artmed.2024.102919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic cancer does not show specific symptoms, which makes the diagnosis of early stages difficult with established image-based screening methods and therefore has the worst prognosis among all cancers. Although endoscopic ultrasonography (EUS) has a key role in diagnostic algorithms for pancreatic diseases , B-mode imaging of the pancreas can be affected by confounders such as chronic pancreatitis , which can make both pancreatic lesion segmentation and classification laborious and highly specialized. To address these challenges, this work proposes a semi-supervised multi-task network (SSM-Net) to leverage unlabeled and labeled EUS images for joint pancreatic lesion classification and segmentation. Specifically, we first devise a saliency-aware representation learning module (SRLM) on a large number of unlabeled images to train a feature extraction encoder network for labeled images by computing a contrastive loss with a semantic saliency map, which is obtained by our spectral residual module (SRM). Moreover, for labeled EUS images, we devise channel attention blocks (CABs) to refine the features extracted from the pre-trained encoder on unlabeled images for segmenting lesions, and then devise a merged global attention module (MGAM) and a feature similarity loss (FSL) for obtaining a lesion classification result . We collect a large-scale EUS-based pancreas image dataset (LS-EUSPI) consisting of 9,555 pathologically proven labeled EUS images (499 patients from four categories) and 15,500 unlabeled EUS images. Experimental results on the LS-EUSPI dataset and a public thyroid gland lesion dataset show that our SSM-Net clearly outperforms state-of-the-art methods.},
  archive      = {J_ARTMED},
  author       = {Jiajia Li and Pingping Zhang and Xia Yang and Lei Zhu and Teng Wang and Ping Zhang and Ruhan Liu and Bin Sheng and Kaixuan Wang},
  doi          = {10.1016/j.artmed.2024.102919},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102919},
  shortjournal = {Artif. Intell. Med.},
  title        = {SSM-net: Semi-supervised multi-task network for joint lesion segmentation and classification from pancreatic EUS images},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new methodology for determining the central pressure
waveform from peripheral measurement using fourier-based machine
learning. <em>ARTMED</em>, <em>154</em>, 102918. (<a
href="https://doi.org/10.1016/j.artmed.2024.102918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radial applanation tonometry is a well-established technique for hemodynamic monitoring and is becoming popular in affordable non-invasive wearable healthcare electronics. To assess the central aortic pressure using radial-based measurements, there is an essential need to develop mathematical approaches to estimate the central pressure waveform. In this study, we propose a new Fourier-based machine learning (F-ML) methodology to transfer non-invasive radial pressure measurements to the central pressure waveform. To test the method, collection of tonometry recordings of the radial and carotid pressure measurements are used from the Framingham Heart Study (2640 individuals, 55 % women) with mean (range) age of 66 (40–91) years. Method-derived estimates are significantly correlated with the measured ones for three major features of the pressure waveform (systolic blood pressure, r r =0.97, p &lt; 0.001; diastolic blood pressure , r r =0.99, p &lt; 0.001; and mean blood pressure , r r =0.99, p &lt; 0.001). In all cases, the Bland-Altman analysis shows negligible bias in the estimations and error is bounded to 5.4 mmHg. Findings also suggest that the F-ML approach reconstructs the shape of the central pressure waveform accurately with the average normalized root mean square error of 5.5 % in the testing population which is blinded to all stages of machine learning development. The results show that the F-ML transfer function outperforms the conventional generalized transfer function , particularly in terms of reconstructing the shape of the central pressure waveform morphology. The proposed F-ML transfer function can provide accurate estimates for the central pressure waveform, and ultimately expand the usage of non-invasive devices for central hemodynamic assessment.},
  archive      = {J_ARTMED},
  author       = {Arian Aghilinejad and Alessio Tamborini and Morteza Gharib},
  doi          = {10.1016/j.artmed.2024.102918},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102918},
  shortjournal = {Artif. Intell. Med.},
  title        = {A new methodology for determining the central pressure waveform from peripheral measurement using fourier-based machine learning},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AMFP-net: Adaptive multi-scale feature pyramid network for
diagnosis of pneumoconiosis from chest x-ray images. <em>ARTMED</em>,
<em>154</em>, 102917. (<a
href="https://doi.org/10.1016/j.artmed.2024.102917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of pneumoconiosis by routine health screening of workers in the mining industry is critical for preventing the progression of this incurable disease. Automated pneumoconiosis classification in chest X-ray images is challenging due to the low contrast of opacities, inter-class similarity, intra-class variation and the existence of artifacts. Compared to traditional methods, convolutional neural networks have shown significant improvement in pneumoconiosis classification tasks , however, accurate classification remains challenging due to mainly the inability to focus on semantically meaningful lesion opacities. Most existing networks focus on high level abstract information and ignore low level detailed object information . Different from natural images where an object occupies large space, the classification of pneumoconiosis depends on the density of small opacities inside the lung. To address this issue, we propose a novel two-stage adaptive multi-scale feature pyramid network called AMFP-Net for the diagnosis of pneumoconiosis from chest X-rays. The proposed model consists of 1) an adaptive multi-scale context block to extract rich contextual and discriminative information and 2) a weighted feature fusion module to effectively combine low level detailed and high level global semantic information. This two-stage network first segments the lungs to focus more on relevant regions by excluding irrelevant parts of the image, and then utilises the segmented lungs to classify pneumoconiosis into different categories. Extensive experiments on public and private datasets demonstrate that the proposed approach can outperform state-of-the-art methods for both segmentation and classification.},
  archive      = {J_ARTMED},
  author       = {Md. Shariful Alam and Dadong Wang and Arcot Sowmya},
  doi          = {10.1016/j.artmed.2024.102917},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102917},
  shortjournal = {Artif. Intell. Med.},
  title        = {AMFP-net: Adaptive multi-scale feature pyramid network for diagnosis of pneumoconiosis from chest X-ray images},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EHR coding with hybrid attention and features propagation on
disease knowledge graph. <em>ARTMED</em>, <em>154</em>, 102916. (<a
href="https://doi.org/10.1016/j.artmed.2024.102916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {And sentences associated with these attributes and relationships have been neglected. in this paper ►We propose an end-to-end model called K nowledge G raph E nhanced neural net work (KGENet) to address the above shortcomings. specifically ►We first construct a disease knowledge graph that focuses on the multi-view disease attributes of ICD codes and the disease relationships between these codes. we also use a long sequence encoder to get EHR document representation. most importantly ►KGENet leverages multi-view disease attributes and structured disease relationships for knowledge enhancement through hybrid attention and graph propagation ►Respectively. furthermore ►The above processes can provide attribute-aware and relationship-augmented explainability for the model prediction results based on our disease knowledge graph. experiments conducted on the MIMIC-III benchmark dataset show that KGENet outperforms state-of-the-art models in both model effectiveness and explainability Electronic health record (EHR) coding assigns International Classification of Diseases (ICD) codes to each EHR document. These standard medical codes represent diagnoses or procedures and play a critical role in medical applications. However, EHR is a long medical text that is difficult to represent, the ICD code label space is large, and the labels have an extremely unbalanced distribution. These factors pose challenges to automatic EHR coding. Previous studies have not explored the disease attributes (e.g., symptoms, tests, medications) of ICD codes and the disease relationships (e.g., causes, risk factors, comorbidities) between them. In addition, the important roles of medical},
  archive      = {J_ARTMED},
  author       = {Tianhan Xu and Bin Li and Ling Chen and Chao Yang and Yixun Gu and Xiang Gu},
  doi          = {10.1016/j.artmed.2024.102916},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102916},
  shortjournal = {Artif. Intell. Med.},
  title        = {EHR coding with hybrid attention and features propagation on disease knowledge graph},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A joint entity relation extraction method for document level
traditional chinese medicine texts. <em>ARTMED</em>, <em>154</em>,
102915. (<a href="https://doi.org/10.1016/j.artmed.2024.102915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese medicine is a unique and complex medical system with complete and rich scientific theories. The textual data of Traditional Chinese Medicine (TCM) contains a large amount of relevant knowledge in the field of TCM, which can serve as guidance for accurate disease diagnosis as well as efficient disease prevention and treatment. Existing TCM texts are disorganized and lack a uniform standard. For this reason, this paper proposes a joint extraction framework by using graph convolutional networks to extract joint entity relations on document-level TCM texts to achieve TCM entity relation mining. More specifically, we first finetune the pre-trained language model by using the TCM domain knowledge to obtain the task-specific model. Taking the integrity of TCM into account, we extract the complete entities as well as the relations corresponding to diagnosis and treatment from the document-level medical cases by using multiple features such as word fusion coding, TCM lexicon information, and multi-relational graph convolutional networks. The experimental results show that the proposed method outperforms the state-of-the-art methods. It has an F1-score of 90.7% for Name Entity Recognization and 76.14% for Relation Extraction on the TCM dataset, which significantly improves the ability to extract entity relations from TCM texts. Code is available at https://github.com/xxxxwx/TCMERE .},
  archive      = {J_ARTMED},
  author       = {Wenxuan Xu and Lin Wang and Mingchuan Zhang and Junlong Zhu and Junqiang Yan and Qingtao Wu},
  doi          = {10.1016/j.artmed.2024.102915},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102915},
  shortjournal = {Artif. Intell. Med.},
  title        = {A joint entity relation extraction method for document level traditional chinese medicine texts},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning for hand tracking in parkinson’s disease
video-based assessment: Current and future perspectives.
<em>ARTMED</em>, <em>154</em>, 102914. (<a
href="https://doi.org/10.1016/j.artmed.2024.102914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parkinson’s Disease (PD) demands early diagnosis and frequent assessment of symptoms. In particular, analysing hand movements is pivotal to understand disease progression. Advancements in hand tracking using Deep Learning (DL) allow for the automatic and objective disease evaluation from video recordings of standardised motor tasks, which are the foundation of neurological examinations. In view of this scenario, this narrative review aims to describe the state of the art and the future perspective of DL frameworks for hand tracking in video-based PD assessment. A rigorous search of PubMed, Web of Science, IEEE Explorer, and Scopus until October 2023 using primary keywords such as parkinson, hand tracking, and deep learning was performed to select eligible by focusing on video-based PD assessment through DL-driven hand tracking frameworks After accurate screening, 23 publications met the selection criteria. These studies used various solutions, from well-established pose estimation frameworks, like OpenPose and MediaPipe, to custom deep architectures designed to accurately track hand and finger movements and extract relevant disease features. Estimated hand tracking data were then used to differentiate PD patients from healthy individuals, characterise symptoms such as tremors and bradykinesia, or regress the Movement Disorder Society-Unified Parkinson’s Disease Rating Scale (MDS-UPDRS) by automatically assessing clinical tasks such as finger tapping, hand movements, and pronation-supination. DL-driven hand tracking holds promise for PD assessment, offering precise, objective measurements for early diagnosis and monitoring, especially in a telemedicine scenario. However, to ensure clinical acceptance, standardisation and validation are crucial. Future research should prioritise large open datasets, rigorous validation on patients, and the investigation of new frontiers such as tracking hand-hand and hand-object interactions for daily-life tasks assessment.},
  archive      = {J_ARTMED},
  author       = {Gianluca Amprimo and Giulia Masi and Gabriella Olmo and Claudia Ferraris},
  doi          = {10.1016/j.artmed.2024.102914},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102914},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning for hand tracking in parkinson’s disease video-based assessment: Current and future perspectives},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). M-ClustEHR: A multimodal clustering approach for electronic
health records. <em>ARTMED</em>, <em>154</em>, 102905. (<a
href="https://doi.org/10.1016/j.artmed.2024.102905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis refers to a potentially life-threatening situation where the immune system of the human body has an extreme response to an infection. In the presence of underlying comorbidities, the situation can become even worse and result in death. Employing unsupervised machine learning techniques , such as clustering, can assist in providing a better understanding of patient phenotypes by unveiling subgroups characterized by distinct sepsis progression and treatment patterns. More concretely, this study introduces M-ClustEHR , a clustering approach that utilizes medical data of multiple modalities by employing a multimodal autoencoder for learning comprehensive sepsis patient representations. M-ClustEHR consistently outperforms traditional clustering approaches in terms of several internal clustering performance metrics, as well as cluster stability in identifying phenotypes in the sepsis cohort. The unveiled patterns, supported by existing medical literature and clinicians, highlight the importance of multimodal clustering for advancing personalized sepsis care.},
  archive      = {J_ARTMED},
  author       = {Maria Bampa and Ioanna Miliou and Braslav Jovanovic and Panagiotis Papapetrou},
  doi          = {10.1016/j.artmed.2024.102905},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102905},
  shortjournal = {Artif. Intell. Med.},
  title        = {M-ClustEHR: A multimodal clustering approach for electronic health records},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pre-trained language models in medicine: A survey.
<em>ARTMED</em>, <em>154</em>, 102904. (<a
href="https://doi.org/10.1016/j.artmed.2024.102904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT , BioBERT, and ChatGPT have shown great potential in various medical NLP tasks . This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation , sentiment analysis , named entity recognition , information extraction, medical education , relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.},
  archive      = {J_ARTMED},
  author       = {Xudong Luo and Zhiqi Deng and Binxia Yang and Michael Y. Luo},
  doi          = {10.1016/j.artmed.2024.102904},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102904},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pre-trained language models in medicine: A survey},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TEE4EHR: Transformer event encoder for better representation
learning in electronic health records. <em>ARTMED</em>, <em>154</em>,
102903. (<a href="https://doi.org/10.1016/j.artmed.2024.102903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Irregular sampling of time series in electronic health records (EHRs) is one of the main challenges for developing machine learning models. Additionally, the pattern of missing values in certain clinical variables is not at random but depends on the decisions of clinicians and the state of the patient. Point process is a mathematical framework for analyzing event sequence data consistent with irregular sampling patterns. Our model, TEE4EHR , is a transformer event encoder (TEE) with point process loss that encodes the pattern of laboratory tests in EHRs. The utility of our TEE has been investigated in various benchmark event sequence datasets. Additionally, we conduct experiments on two real-world EHR databases to provide a more comprehensive evaluation of our model. Firstly, in a self-supervised learning approach, the TEE is jointly learned with an existing attention-based deep neural network , which gives superior performance in negative log-likelihood and future event prediction. Besides, we propose an algorithm for aggregating attention weights to reveal the events’ interactions. Secondly, we transfer and freeze the learned TEE to the downstream task for the outcome prediction, where it outperforms state-of-the-art models for handling irregularly sampled time series. Furthermore, our results demonstrate that our approach can improve representation learning in EHRs and be useful for clinical prediction tasks.},
  archive      = {J_ARTMED},
  author       = {Hojjat Karami and David Atienza and Anisoara Ionescu},
  doi          = {10.1016/j.artmed.2024.102903},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102903},
  shortjournal = {Artif. Intell. Med.},
  title        = {TEE4EHR: Transformer event encoder for better representation learning in electronic health records},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A comprehensive survey on the use of deep learning
techniques in glioblastoma. <em>ARTMED</em>, <em>154</em>, 102902. (<a
href="https://doi.org/10.1016/j.artmed.2024.102902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glioblastoma , characterized as a grade 4 astrocytoma , stands out as the most aggressive brain tumor, often leading to dire outcomes. The challenge of treating glioblastoma is exacerbated by the convergence of genetic mutations and disruptions in gene expression, driven by alterations in epigenetic mechanisms . The integration of artificial intelligence, inclusive of machine learning algorithms , has emerged as an indispensable asset in medical analyses. AI is becoming a necessary tool in medicine and beyond. Current research on Glioblastoma predominantly revolves around non-omics data modalities, prominently including magnetic resonance imaging, computed tomography , and positron emission tomography . Nonetheless, the assimilation of omic data—encompassing gene expression through transcriptomics and epigenomics—offers pivotal insights into patients&#39; conditions. These insights, reciprocally, hold significant value in refining diagnoses, guiding decision- making processes, and devising efficacious treatment strategies. This survey&#39;s core objective encompasses a comprehensive exploration of noteworthy applications of machine learning methodologies in the domain of glioblastoma, alongside closely associated research pursuits. The study accentuates the deployment of artificial intelligence techniques for both non-omics and omics data, encompassing a range of tasks. Furthermore, the survey underscores the intricate challenges posed by the inherent heterogeneity of Glioblastoma, delving into strategies aimed at addressing its multifaceted nature.},
  archive      = {J_ARTMED},
  author       = {Ichraq El Hachimy and Douae Kabelma and Chaimae Echcharef and Mohamed Hassani and Nabil Benamar and Nabil Hajji},
  doi          = {10.1016/j.artmed.2024.102902},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102902},
  shortjournal = {Artif. Intell. Med.},
  title        = {A comprehensive survey on the use of deep learning techniques in glioblastoma},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systematic literature review on reinforcement learning in
non-communicable disease interventions. <em>ARTMED</em>, <em>154</em>,
102901. (<a href="https://doi.org/10.1016/j.artmed.2024.102901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is evidence that reducing modifiable risk factors and strengthening medical and health interventions can reduce early mortality and economic losses from non-communicable diseases (NCDs). Machine learning (ML) algorithms have been successfully applied to preventing and controlling NCDs. Reinforcement learning (RL) is the most promising of these approaches because of its ability to dynamically adapt interventions to NCD disease progression and its commitment to achieving long-term intervention goals. This paper reviews the preferred algorithms, data sources, design details, and obstacles to clinical application in existing studies to facilitate the early application of RL algorithms in clinical practice research for NCD interventions. We screened 40 relevant papers for quantitative and qualitative analysis using the PRISMA review flow diagram. The results show that researchers tend to use Deep Q-Network (DQN) and Actor-Critic as well as their improved or hybrid algorithms to train and validate RL models on retrospective datasets. Often, the patient&#39;s physical condition is the main defining parameter of the state space, while interventions are the main defining parameter of the action space. Mostly, changes in the patient&#39;s physical condition are used as a basis for immediate rewards to the agent. Various attempts have been made to address the challenges to clinical application, and several approaches have been proposed from existing research. However, as there is currently no universally accepted solution, the use of RL algorithms in clinical practice for NCD interventions necessitates more comprehensive responses to the issues addressed in this paper, which are safety, interpretability, training efficiency, and the technical aspect of exploitation and exploration in RL algorithms.},
  archive      = {J_ARTMED},
  author       = {Yanfeng Zhao and Jun Kit Chaw and Lin Liu and Sook Hui Chaw and Mei Choo Ang and Tin Tin Ting},
  doi          = {10.1016/j.artmed.2024.102901},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102901},
  shortjournal = {Artif. Intell. Med.},
  title        = {Systematic literature review on reinforcement learning in non-communicable disease interventions},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transformers and large language models in healthcare: A
review. <em>ARTMED</em>, <em>154</em>, 102900. (<a
href="https://doi.org/10.1016/j.artmed.2024.102900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of healthcare data , including clinical NLP, medical imaging , structured Electronic Health Records (EHR), social media, bio-physiological signals, biomolecular sequences. Furthermore, which have also include the articles that used the transformer architecture for generating surgical instructions and predicting adverse outcomes after surgeries under the umbrella of critical care. Under diverse settings, these models have been used for clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. Finally, we also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability , fairness, alignment with human values, ethical implications, and environmental impact.},
  archive      = {J_ARTMED},
  author       = {Subhash Nerella and Sabyasachi Bandyopadhyay and Jiaqing Zhang and Miguel Contreras and Scott Siegel and Aysegul Bumin and Brandon Silva and Jessica Sena and Benjamin Shickel and Azra Bihorac and Kia Khezeli and Parisa Rashidi},
  doi          = {10.1016/j.artmed.2024.102900},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102900},
  shortjournal = {Artif. Intell. Med.},
  title        = {Transformers and large language models in healthcare: A review},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consensus modeling: Safer transfer learning for small health
systems. <em>ARTMED</em>, <em>154</em>, 102899. (<a
href="https://doi.org/10.1016/j.artmed.2024.102899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive modeling is becoming an essential tool for clinical decision support , but health systems with smaller sample sizes may construct suboptimal or overly specific models. Models become over-specific when beside true physiological effects , they also incorporate potentially volatile site-specific artifacts. These artifacts can change suddenly and can render the model unsafe. To obtain safer models, health systems with inadequate sample sizes may adopt one of the following options. First, they can use a generic model, such as one purchased from a vendor, but often such a model is not sufficiently specific to the patient population and is thus suboptimal. Second, they can participate in a research network. Paradoxically though, sites with smaller datasets contribute correspondingly less to the joint model, again rendering the final model suboptimal. Lastly, they can use transfer learning , starting from a model trained on a large data set and updating this model to the local population. This strategy can also result in a model that is over-specific. In this paper we present the consensus modeling paradigm, which uses the help of a large site (source) to reach a consensus model at the small site (target). We evaluate the approach on predicting postoperative complications at two health systems with 9,044 and 38,045 patients (rare outcomes at about 1% positive rate), and conduct a simulation study to understand the performance of consensus modeling relative to the other three approaches as a function of the available training sample size at the target site. We found that consensus modeling exhibited the least over-specificity at either the source or target site and achieved the highest combined predictive performance .},
  archive      = {J_ARTMED},
  author       = {Roshan Tourani and Dennis H. Murphree and Adam Sheka and Genevieve B. Melton and Daryl J. Kor and Gyorgy J. Simon},
  doi          = {10.1016/j.artmed.2024.102899},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102899},
  shortjournal = {Artif. Intell. Med.},
  title        = {Consensus modeling: Safer transfer learning for small health systems},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural topic models with survival supervision: Jointly
predicting time-to-event outcomes and learning how clinical features
relate. <em>ARTMED</em>, <em>154</em>, 102898. (<a
href="https://doi.org/10.1016/j.artmed.2024.102898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a neural network framework for learning a survival model to predict a time-to-event outcome while simultaneously learning a topic model that reveals feature relationships. In particular, we model each subject as a distribution over “topics”, where a topic could, for instance, correspond to an age group, a disorder, or a disease. The presence of a topic in a subject means that specific clinical features are more likely to appear for the subject. Topics encode information about related features and are learned in a supervised manner to predict a time-to-event outcome. Our framework supports combining many different topic and survival models; training the resulting joint survival-topic model readily scales to large datasets using standard neural net optimizers with minibatch gradient descent . For example, a special case is to combine LDA with a Cox model, in which case a subject’s distribution over topics serves as the input feature vector to the Cox model. We explain how to address practical implementation issues that arise when applying these neural survival-supervised topic models to clinical data , including how to visualize results to assist clinical interpretation. We study the effectiveness of our proposed framework on seven clinical datasets on predicting time until death as well as hospital ICU length of stay, where we find that neural survival-supervised topic models achieve competitive accuracy with existing approaches while yielding interpretable clinical topics that explain feature relationships. Our code is available at: https://github.com/georgehc/survival-topics},
  archive      = {J_ARTMED},
  author       = {George H. Chen and Linhong Li and Ren Zuo and Amanda Coston and Jeremy C. Weiss},
  doi          = {10.1016/j.artmed.2024.102898},
  journal      = {Artificial Intelligence in Medicine},
  month        = {8},
  pages        = {102898},
  shortjournal = {Artif. Intell. Med.},
  title        = {Neural topic models with survival supervision: Jointly predicting time-to-event outcomes and learning how clinical features relate},
  volume       = {154},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating the discrimination ability of 3D convolutional
neural networks applied to altered brain MRI parametric maps.
<em>ARTMED</em>, <em>153</em>, 102897. (<a
href="https://doi.org/10.1016/j.artmed.2024.102897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are gradually being recognized in the neuroimaging community as a powerful tool for image analysis. Despite their outstanding performances, some aspects of CNN functioning are still not fully understood by human operators. We postulated that the interpretability of CNNs applied to neuroimaging data could be improved by investigating their behavior when they are fed data with known characteristics. We analyzed the ability of 3D CNNs to discriminate between original and altered whole-brain parametric maps derived from diffusion-weighted magnetic resonance imaging. The alteration consisted in linearly changing the voxel intensity of either one (monoregion) or two (biregion) anatomical regions in each brain volume, but without mimicking any neuropathology . Performing ten-fold cross-validation and using a hold-out set for testing, we assessed the CNNs’ discrimination ability according to the intensity of the altered regions, comparing the latter’s size and relative position. Monoregion CNNs showed that the larger the modified region, the smaller the intensity increase needed to achieve good performances. Biregion CNNs systematically outperformed monoregion CNNs, but could only detect one of the two target regions when tested on the corresponding monoregion images. Exploiting prior information on training data allowed for a better understanding of CNN behavior, especially when altered regions were combined. This can inform about the complexity of CNN pattern retrieval and elucidate misclassified examples, particularly relevant for pathological data. The proposed analytical approach may serve to gain insights into CNN behavior and guide the design of enhanced detection systems exploiting our prior knowledge.},
  archive      = {J_ARTMED},
  author       = {Giulia Maria Mattia and Edouard Villain and Federico Nemmi and Marie-Véronique Le Lann and Xavier Franceries and Patrice Péran},
  doi          = {10.1016/j.artmed.2024.102897},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102897},
  shortjournal = {Artif. Intell. Med.},
  title        = {Investigating the discrimination ability of 3D convolutional neural networks applied to altered brain MRI parametric maps},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Oversampling effect in pretraining for bidirectional encoder
representations from transformers (BERT) to localize medical BERT and
enhance biomedical BERT. <em>ARTMED</em>, <em>153</em>, 102889. (<a
href="https://doi.org/10.1016/j.artmed.2024.102889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretraining large-scale neural language models on raw texts has made a significant contribution to improving transfer learning in natural language processing . With the introduction of transformer-based language models , such as bidirectional encoder representations from transformers (BERT), the performance of information extraction from free text has improved significantly in both the general and medical domains. However, it is difficult to train specific BERT models to perform well in domains for which few databases of a high quality and large size are publicly available. We hypothesized that this problem could be addressed by oversampling a domain-specific corpus and using it for pretraining with a larger corpus in a balanced manner. In the present study, we verified our hypothesis by developing pretraining models using our method and evaluating their performance. Our proposed method was based on the simultaneous pretraining of models with knowledge from distinct domains after oversampling. We conducted three experiments in which we generated (1) English biomedical BERT from a small biomedical corpus, (2) Japanese medical BERT from a small medical corpus, and (3) enhanced biomedical BERT pretrained with complete PubMed abstracts in a balanced manner. We then compared their performance with those of conventional models. Our English BERT pretrained using both general and small medical domain corpora performed sufficiently well for practical use on the biomedical language understanding evaluation (BLUE) benchmark. Moreover, our proposed method was more effective than the conventional methods for each biomedical corpus of the same corpus size in the general domain. Our Japanese medical BERT outperformed the other BERT models built using a conventional method for almost all the medical tasks. The model demonstrated the same trend as that of the first experiment in English. Further, our enhanced biomedical BERT model, which was not pretrained on clinical notes, achieved superior clinical and biomedical scores on the BLUE benchmark with an increase of 0.3 points in the clinical score and 0.5 points in the biomedical score. These scores were above those of the models trained without our proposed method. Well-balanced pretraining using oversampling instances derived from a corpus appropriate for the target task allowed us to construct a high-performance BERT model.},
  archive      = {J_ARTMED},
  author       = {Shoya Wada and Toshihiro Takeda and Katsuki Okada and Shirou Manabe and Shozo Konishi and Jun Kamohara and Yasushi Matsumura},
  doi          = {10.1016/j.artmed.2024.102889},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102889},
  shortjournal = {Artif. Intell. Med.},
  title        = {Oversampling effect in pretraining for bidirectional encoder representations from transformers (BERT) to localize medical BERT and enhance biomedical BERT},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time coronary artery segmentation in CAG images: A
semi-supervised deep learning strategy. <em>ARTMED</em>, <em>153</em>,
102888. (<a href="https://doi.org/10.1016/j.artmed.2024.102888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When treating patients with coronary artery disease and concurrent renal concerns, we often encounter a conundrum: how to achieve a clearer view of vascular details while minimizing the contrast and radiation doses during percutaneous coronary intervention (PCI). Our goal is to use deep learning (DL) to create a real-time roadmap for guiding PCI. To this end, segmentation, a critical first step, paves the way for detailed vascular analysis. Unlike traditional supervised learning, which demands extensive labeling time and manpower, our strategy leans toward semi-supervised learning. This method not only economizes on labeling efforts but also aims at reducing contrast and radiation exposure. CAG data sourced from eight tertiary centers in Taiwan, comprising 500 labeled and 8952 unlabeled images. Employing 400 labels for training and reserving 100 for validation, we built a U-Net based network within a teacher-student architecture. The initial teacher model was updated with 8952 unlabeled images inputted, employing a quality control strategy involving consistency regularization and RandAugment. The optimized teacher model produced pseudo-labels for label expansion, which were then utilized to train the final student model. We attained an average dice similarity coefficient of 0.9003 for segmentation, outperforming supervised learning methods with the same label count. Even with only 5 % labels for semi-supervised training, the results surpassed a supervised method with 100 % labels inputted. This semi-supervised approach&#39;s advantage extends beyond single-frame prediction, yielding consistently superior results in continuous angiography films. High labeling cost hinders DL training. Semi-supervised learning, quality control, and pseudo-label expansion can overcome this. DL-assisted segmentation potentially provides a real-time PCI roadmap and further diminishes radiation and contrast doses.},
  archive      = {J_ARTMED},
  author       = {Chih-Kuo Lee and Jhen-Wei Hong and Chia-Ling Wu and Jia-Ming Hou and Yen-An Lin and Kuan-Chih Huang and Po-Hsuan Tseng},
  doi          = {10.1016/j.artmed.2024.102888},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102888},
  shortjournal = {Artif. Intell. Med.},
  title        = {Real-time coronary artery segmentation in CAG images: A semi-supervised deep learning strategy},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data mining and machine learning in HIV infection risk
research: An overview and recommendations. <em>ARTMED</em>,
<em>153</em>, 102887. (<a
href="https://doi.org/10.1016/j.artmed.2024.102887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary era, the applications of data mining and machine learning have permeated extensively into medical research, significantly contributing to areas such as HIV studies. By reviewing 38 articles published in the past 15 years, the study presents a roadmap based on seven different aspects, utilizing various machine learning techniques for both novice researchers and experienced researchers seeking to comprehend the current state of the art in this area. While traditional regression modeling techniques have been commonly used, researchers are increasingly adopting more advanced fully supervised machine learning and deep learning techniques , which often outperform the traditional methods in predictive performance . Additionally, the study identifies nine new open research issues and outlines possible future research plans to enhance the outcomes of HIV infection risk research. This review is expected to be an insightful guide for researchers, illuminating current practices and suggesting advancements in the field.},
  archive      = {J_ARTMED},
  author       = {Qiwei Ge and Xinyu Lu and Run Jiang and Yuyu Zhang and Xun Zhuang},
  doi          = {10.1016/j.artmed.2024.102887},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102887},
  shortjournal = {Artif. Intell. Med.},
  title        = {Data mining and machine learning in HIV infection risk research: An overview and recommendations},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Enhanced differential evolution algorithm for feature
selection in tuberculous pleural effusion clinical characteristics
analysis. <em>ARTMED</em>, <em>153</em>, 102886. (<a
href="https://doi.org/10.1016/j.artmed.2024.102886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculous pleural effusion poses a significant threat to human health due to its potential for severe disease and mortality. Without timely treatment, it may lead to fatal consequences. Therefore, early identification and prompt treatment are crucial for preventing problems such as chronic lung disease , respiratory failure, and death. This study proposes an enhanced differential evolution algorithm based on colony predation and dispersed foraging strategies. A series of experiments conducted on the IEEE CEC 2017 competition dataset validated the global optimization capability of the method. Additionally, a binary version of the algorithm is introduced to assess the algorithm&#39;s ability to address feature selection problems. Comprehensive comparisons of the effectiveness of the proposed algorithm with 8 similar algorithms were conducted using public datasets with feature sizes ranging from 10 to 10,000. Experimental results demonstrate that the proposed method is an effective feature selection approach. Furthermore, a predictive model for tuberculous pleural effusion is established by integrating the proposed algorithm with support vector machines . The performance of the proposed model is validated using clinical records collected from 140 tuberculous pleural effusion patients, totaling 10,780 instances. Experimental results indicate that the proposed model can identify key correlated indicators such as pleural effusion adenosine deaminase , temperature, white blood cell count , and pleural effusion color, aiding in the clinical feature analysis of tuberculous pleural effusion and providing early warning for its treatment and prediction.},
  archive      = {J_ARTMED},
  author       = {Xinsen Zhou and Yi Chen and Wenyong Gui and Ali Asghar Heidari and Zhennao Cai and Mingjing Wang and Huiling Chen and Chengye Li},
  doi          = {10.1016/j.artmed.2024.102886},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102886},
  shortjournal = {Artif. Intell. Med.},
  title        = {Enhanced differential evolution algorithm for feature selection in tuberculous pleural effusion clinical characteristics analysis},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TSOANet: Time-sensitive orthogonal attention network for
medical event prediction. <em>ARTMED</em>, <em>153</em>, 102885. (<a
href="https://doi.org/10.1016/j.artmed.2024.102885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical Event Prediction (MEP) based on Electronic Medical Records (EMR) is an essential and valuable task for healthcare. For a patient, information in the EMR can be organized into a structured sequence, consisting of multiple visits each with details about visit time and various types of medical events. As the time intervals between neighboring visits are irregular and the medical events at different visits can vary significantly, MEP based on EMR is still challenging. Many studies have been proposed to model the irregular time intervals, relations among different types of medical events within each visit and relations among medical events across visits, and reported exciting results. However, most of these studies focus on two out of the three aspects mentioned above, with only a few addressing all the three aspects simultaneously. In this study, we propose a novel network, the Time-Sensitive Orthogonal Attention Network (TSOANet), which can fully utilize the irregular time intervals, relations among different types of medical events within and across visits. In particular, we design two key components: (1) Time-Sensitive Block, used to model the time intervals at both local and global levels to determine the impact of each visit in EMR; (2) Orthogonal Attention Block, used to model relations among different types of medical events within each visit and across visits in two axes, that is, event axis and time axis. Extensive experiments on two public real-world EMR datasets demonstrate that TSOANet outperforms the state-of-the-art models for various prediction tasks, thereby verifying the effectiveness of our approach. The source code of TSOANet is released at https://github.com/chh13502/TSOANet .},
  archive      = {J_ARTMED},
  author       = {Hao Chen and Junjie Zhang and Yang Xiang and Shengye Lu and Buzhou Tang},
  doi          = {10.1016/j.artmed.2024.102885},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102885},
  shortjournal = {Artif. Intell. Med.},
  title        = {TSOANet: Time-sensitive orthogonal attention network for medical event prediction},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying pediatric heart murmurs and distinguishing
innocent from pathologic using deep learning. <em>ARTMED</em>,
<em>153</em>, 102867. (<a
href="https://doi.org/10.1016/j.artmed.2024.102867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To develop a deep learning algorithm to perform multi-class classification of normal pediatric heart sounds , innocent murmurs, and pathologic murmurs. We prospectively enrolled children under age 18 being evaluated by the Division of Pediatric Cardiology . Parents provided consent for a deidentified recording of their child&#39;s heart sounds with a digital stethoscope. Innocent murmurs were validated by a pediatric cardiologist and pathologic murmurs were validated by echocardiogram . To augment our collection of normal heart sounds, we utilized a public database of pediatric heart sound recordings (Oliveira, 2022). We propose two novel approaches for this audio classification task. We train a vision transformer on either Markov transition field or Gramian angular field image representations of the frequency spectrum. We benchmark our results against a ResNet-50 CNN trained on spectrogram images. Our final dataset consisted of 366 normal heart sounds, 175 innocent murmurs, and 216 pathologic murmurs. Innocent murmurs collected include Still&#39;s murmur, venous hum, and flow murmurs. Pathologic murmurs included ventricular septal defect , tetralogy of Fallot , aortic regurgitation , aortic stenosis , pulmonary stenosis , mitral regurgitation and stenosis, and tricuspid regurgitation . We find that the Vision Transformer consistently outperforms the ResNet-50 on all three image representations, and that the Gramian angular field is the superior image representation for pediatric heart sounds. We calculated a one-vs-rest multi-class ROC curve for each of the three classes. Our best model achieves an area under the curve (AUC) value of 0.92 ± 0.05, 0.83 ± 0.04, and 0.88 ± 0.04 for identifying normal heart sounds, innocent murmurs, and pathologic murmurs, respectively. We present two novel methods for pediatric heart sound classification, which outperforms the current standard of using a convolutional neural network trained on spectrogram images. To our knowledge, we are the first to demonstrate multi-class classification of pediatric murmurs. Multiclass output affords a more explainable and interpretable model, which can facilitate further model improvement in the downstream model development cycle and enhance clinician trust and therefore adoption.},
  archive      = {J_ARTMED},
  author       = {George Zhou and Candace Chien and Justin Chen and Lucille Luan and Yunchan Chen and Sheila Carroll and Jeffrey Dayton and Maria Thanjan and Ken Bayle and Patrick Flynn},
  doi          = {10.1016/j.artmed.2024.102867},
  journal      = {Artificial Intelligence in Medicine},
  month        = {7},
  pages        = {102867},
  shortjournal = {Artif. Intell. Med.},
  title        = {Identifying pediatric heart murmurs and distinguishing innocent from pathologic using deep learning},
  volume       = {153},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Agent-based approaches for biological modeling in oncology:
A literature review. <em>ARTMED</em>, <em>152</em>, 102884. (<a
href="https://doi.org/10.1016/j.artmed.2024.102884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational modeling involves the use of computer simulations and models to study and understand real-world phenomena. Its application is particularly relevant in the study of potential interactions between biological elements. It is a promising approach to understand complex biological processes and predict their behavior under various conditions. This paper is a review of the recent literature on computational modeling of biological systems. Our study focuses on the field of oncology and the use of artificial intelligence (AI) and, in particular, agent-based modeling (ABM), between 2010 and May 2023. Most of the articles studied focus on improving the diagnosis and understanding the behaviors of biological entities, with metaheuristic algorithms being the models most used. Several challenges are highlighted regarding increasing and structuring knowledge about biological systems, developing holistic models that capture multiple scales and levels of organization, reproducing emergent behaviors of biological systems, validating models with experimental data, improving computational performance of models and algorithms, and ensuring privacy and personal data protection are discussed.},
  archive      = {J_ARTMED},
  author       = {Simon Stephan and Stéphane Galland and Ouassila Labbani Narsis and Kenji Shoji and Sébastien Vachenc and Stéphane Gerart and Christophe Nicolle},
  doi          = {10.1016/j.artmed.2024.102884},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102884},
  shortjournal = {Artif. Intell. Med.},
  title        = {Agent-based approaches for biological modeling in oncology: A literature review},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hematologic cancer diagnosis and classification using
machine and deep learning: State-of-the-art techniques and emerging
research directives. <em>ARTMED</em>, <em>152</em>, 102883. (<a
href="https://doi.org/10.1016/j.artmed.2024.102883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hematology is the study of diagnosis and treatment options for blood diseases , including cancer. Cancer is considered one of the deadliest diseases across all age categories. Diagnosing such a deadly disease at the initial stage is essential to cure the disease. Hematologists and pathologists rely on microscopic evaluation of blood or bone marrow smear images to diagnose blood-related ailments . The abundance of overlapping cells, cells of varying densities among platelets, non-illumination levels, and the amount of red and white blood cells make it more difficult to diagnose illness using blood cell images. Pathologists are required to put more effort into the traditional, time-consuming system. Nowadays, it becomes possible with machine learning and deep learning techniques, to automate the diagnostic processes, categorize microscopic blood cells, and improve the accuracy of the procedure and its speed as the models developed using these methods may guide an assisting tool. In this article, we have acquired, analyzed, scrutinized, and finally selected around 57 research papers from various machine learning and deep learning methodologies that have been employed in the diagnosis of leukemia and its classification over the past 20 years, which have been published between the years 2003 and 2023 by PubMed , IEEE, Science Direct, Google Scholar and other pertinent sources. Our primary emphasis is on evaluating the advantages and limitations of analogous research endeavors to provide a concise and valuable research directive that can be of significant utility to fellow researchers in the field.},
  archive      = {J_ARTMED},
  author       = {Hema Patel and Himal Shah and Gayatri Patel and Atul Patel},
  doi          = {10.1016/j.artmed.2024.102883},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102883},
  shortjournal = {Artif. Intell. Med.},
  title        = {Hematologic cancer diagnosis and classification using machine and deep learning: State-of-the-art techniques and emerging research directives},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The AI ethics of digital COVID-19 diagnosis and their legal,
medical, technological, and operational managerial implications.
<em>ARTMED</em>, <em>152</em>, 102873. (<a
href="https://doi.org/10.1016/j.artmed.2024.102873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has given rise to a broad range of research from fields alongside and beyond the core concerns of infectiology, epidemiology , and immunology . One significant subset of this work centers on machine learning-based approaches to supporting medical decision-making around COVID-19 diagnosis. To date, various challenges, including IT issues, have meant that, notwithstanding this strand of research on digital diagnosis of COVID-19, the actual use of these methods in medical facilities remains incipient at best, despite their potential to relieve pressure on scarce medical resources, prevent instances of infection, and help manage the difficulties and unpredictabilities surrounding the emergence of new mutations. The reasons behind this research-application gap are manifold and may imply an interdisciplinary dimension. We argue that the discipline of AI ethics can provide a framework for interdisciplinary discussion and create a roadmap for the application of digital COVID-19 diagnosis, taking into account all disciplinary stakeholders involved. This article proposes such an ethical framework for the practical use of digital COVID-19 diagnosis, considering legal, medical, operational managerial, and technological aspects of the issue in accordance with our diverse research backgrounds and noting the potential of the approach we set out here to guide future research.},
  archive      = {J_ARTMED},
  author       = {Christina C. Bartenschlager and Ulrich M. Gassner and Christoph Römmele and Jens O. Brunner and Kerstin Schlögl-Flierl and Paula Ziethmann},
  doi          = {10.1016/j.artmed.2024.102873},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102873},
  shortjournal = {Artif. Intell. Med.},
  title        = {The AI ethics of digital COVID-19 diagnosis and their legal, medical, technological, and operational managerial implications},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving multiple sclerosis lesion segmentation across
clinical sites: A federated learning approach with noise-resilient
training. <em>ARTMED</em>, <em>152</em>, 102872. (<a
href="https://doi.org/10.1016/j.artmed.2024.102872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately measuring the evolution of Multiple Sclerosis (MS) with magnetic resonance imaging (MRI) critically informs understanding of disease progression and helps to direct therapeutic strategy. Deep learning models have shown promise for automatically segmenting MS lesions, but the scarcity of accurately annotated data hinders progress in this area. Obtaining sufficient data from a single clinical site is challenging and does not address the heterogeneous need for model robustness. Conversely, the collection of data from multiple sites introduces data privacy concerns and potential label noise due to varying annotation standards. To address this dilemma, we explore the use of the federated learning framework while considering label noise. Our approach enables collaboration among multiple clinical sites without compromising data privacy under a federated learning paradigm that incorporates a noise-robust training strategy based on label correction. Specifically, we introduce a Decoupled Hard Label Correction (DHLC) strategy that considers the imbalanced distribution and fuzzy boundaries of MS lesions, enabling the correction of false annotations based on prediction confidence. We also introduce a Centrally Enhanced Label Correction (CELC) strategy, which leverages the aggregated central model as a correction teacher for all sites, enhancing the reliability of the correction process. Extensive experiments conducted on two multi-site datasets demonstrate the effectiveness and robustness of our proposed methods, indicating their potential for clinical applications in multi-site collaborations to train better deep learning models with lower cost in data collection and annotation.},
  archive      = {J_ARTMED},
  author       = {Lei Bai and Dongang Wang and Hengrui Wang and Michael Barnett and Mariano Cabezas and Weidong Cai and Fernando Calamante and Kain Kyle and Dongnan Liu and Linda Ly and Aria Nguyen and Chun-Chien Shieh and Ryan Sullivan and Geng Zhan and Wanli Ouyang and Chenyu Wang},
  doi          = {10.1016/j.artmed.2024.102872},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102872},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving multiple sclerosis lesion segmentation across clinical sites: A federated learning approach with noise-resilient training},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving diagnosis and outcome prediction of gastric cancer
via multimodal learning using whole slide pathological images and gene
expression. <em>ARTMED</em>, <em>152</em>, 102871. (<a
href="https://doi.org/10.1016/j.artmed.2024.102871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the diagnosis and outcome prediction of gastric cancer (GC), machine learning methods based on whole slide pathological images (WSIs) have shown promising performance and reduced the cost of manual analysis. Nevertheless, accurate prediction of GC outcome may rely on multiple modalities with complementary information, particularly gene expression data . Thus, there is a need to develop multimodal learning methods to enhance prediction performance. In this paper, we collect a dataset from Ruijin Hospital and propose a multimodal learning method for GC diagnosis and outcome prediction, called GaCaMML, which is featured by a cross-modal attention mechanism and Per-Slide training scheme. Additionally, we perform feature attribution analysis via integrated gradient (IG) to identify important input features. The proposed method improves prediction accuracy over the single-modal learning method on three tasks, i.e., survival prediction (by 4.9% on C-index), pathological stage classification (by 11.6% on accuracy), and lymph node classification (by 12.0% on accuracy). Especially, the Per-Slide strategy addresses the issue of a high WSI-to-patient ratio and leads to much better results compared with the Per-Person training scheme. For the interpretable analysis, we find that although WSIs dominate the prediction for most samples, there is still a substantial portion of samples whose prediction highly relies on gene expression information. This study demonstrates the great potential of multimodal learning in GC-related prediction tasks and investigates the contribution of WSIs and gene expression, respectively, which not only shows how the model makes a decision but also provides insights into the association between macroscopic pathological phenotypes and microscopic molecular features.},
  archive      = {J_ARTMED},
  author       = {Yuzhang Xie and Qingqing Sang and Qian Da and Guoshuai Niu and Shijie Deng and Haoran Feng and Yunqin Chen and Yuan-Yuan Li and Bingya Liu and Yang Yang and Wentao Dai},
  doi          = {10.1016/j.artmed.2024.102871},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102871},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving diagnosis and outcome prediction of gastric cancer via multimodal learning using whole slide pathological images and gene expression},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MICIL: Multiple-instance class-incremental learning for skin
cancer whole slide images. <em>ARTMED</em>, <em>152</em>, 102870. (<a
href="https://doi.org/10.1016/j.artmed.2024.102870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) agents encounter the problem of catastrophic forgetting when they are trained in sequentially with new data batches. This issue poses a barrier to the implementation of AI-based models in tasks that involve ongoing evolution, such as cancer prediction. Moreover, whole slide images (WSI) play a crucial role in cancer management, and their automated analysis has become increasingly popular in assisting pathologists during the diagnosis process. Incremental learning (IL) techniques aim to develop algorithms capable of retaining previously acquired information while also acquiring new insights to predict future data. Deep IL techniques need to address the challenges posed by the gigapixel scale of WSIs, which often necessitates the use of multiple instance learning (MIL) frameworks. In this paper, we introduce an IL algorithm tailored for analyzing WSIs within a MIL paradigm . The proposed M ultiple I nstance C lass- I ncremental L earning ( MICIL ) algorithm combines MIL with class-IL for the first time, allowing for the incremental prediction of multiple skin cancer subtypes from WSIs within a class-IL scenario. Our framework incorporates knowledge distillation and data rehearsal, along with a novel embedding-level distillation, aiming to preserve the latent space at the aggregated WSI level. Results demonstrate the algorithm’s effectiveness in addressing the challenge of balancing IL-specific metrics, such as intransigence and forgetting, and solving the plasticity-stability dilemma.},
  archive      = {J_ARTMED},
  author       = {Pablo Meseguer and Rocío del Amor and Valery Naranjo},
  doi          = {10.1016/j.artmed.2024.102870},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102870},
  shortjournal = {Artif. Intell. Med.},
  title        = {MICIL: Multiple-instance class-incremental learning for skin cancer whole slide images},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated detection of myocardial infarction based on an
improved state refinement module for LSTM/GRU. <em>ARTMED</em>,
<em>152</em>, 102865. (<a
href="https://doi.org/10.1016/j.artmed.2024.102865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Myocardial infarction (MI) is a common cardiovascular disease caused by the blockages of coronary arteries. The visual inspection of electrocardiogram (ECG) is the main diagnosis pattern, while it is taxing and time-consuming. Motivated from state refinement module for long short term memory (SRM-LSTM), we proposed two improved state refinement frameworks based on LSTM and gated recurrent unit (GRU) called ISRM-LSTM and ISRM-GRU. Both are capable of adaptively refining current states of sample points in ECG with a message passing mechanism than existing LSTM. To evaluate the validity, both are installed into convolutional network architecture and standard LSTM, GRU and Residual networks are employed as control groups across the Physikalisch-Technische Bundesanstalt database. Empirical results confirm noticeable performance improvements than control groups and several existing algorithms with an accuracy of 99.1%. To our knowledge, both modules are the first attempt to consider the interaction characteristics into deep network and improve interpretability exhibiting considerable potentials on lightweight devices thanks to only utilization of three channel ECGs.},
  archive      = {J_ARTMED},
  author       = {Jibin Wang and Xingtian Guo},
  doi          = {10.1016/j.artmed.2024.102865},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102865},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automated detection of myocardial infarction based on an improved state refinement module for LSTM/GRU},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning the cellular activity representation based on gene
regulatory networks for prediction of tumor response to drugs.
<em>ARTMED</em>, <em>152</em>, 102864. (<a
href="https://doi.org/10.1016/j.artmed.2024.102864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the response of tumor cells to anti-tumor drugs is critical to realizing cancer precision medicine. Currently, most existing methods ignore the regulatory relationships between genes and thus have unsatisfactory predictive performance. In this paper, we propose to predict anti-tumor drug efficacy via learning the activity representation of tumor cells based on a priori knowledge of gene regulation networks (GRNs). Specifically, the method simulates the cellular biosystem by synthesizing a cell-gene activity network and then infers a new low-dimensional activity representation for tumor cells from the raw high-dimensional expression profile. The simulated cell-gene network mainly comprises known gene regulatory networks collected from multiple resources and fuses tumor cells by linking them to hotspot genes that are over- or under-expressed in them. The resulting activity representation could not only reflect the shallow expression profile (hotspot genes) but also mines in-depth information of gene regulation activity in tumor cells before treatment. Finally, we build deep learning models on the activity representation for predicting drug efficacy in tumor cells. Experimental results on the benchmark GDSC dataset demonstrate the superior performance of the proposed method over SOTA methods with the highest AUC of 0.954 in the efficacy label prediction and the best R 2 of 0.834 in the regression of half maximal inhibitory concentration (IC50) values, suggesting the potential value of the proposed method in practice.},
  archive      = {J_ARTMED},
  author       = {Xinping Xie and Fengting Wang and Guanfu Wang and Weiwei Zhu and Xiaodong Du and Hongqiang Wang},
  doi          = {10.1016/j.artmed.2024.102864},
  journal      = {Artificial Intelligence in Medicine},
  month        = {6},
  pages        = {102864},
  shortjournal = {Artif. Intell. Med.},
  title        = {Learning the cellular activity representation based on gene regulatory networks for prediction of tumor response to drugs},
  volume       = {152},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harnessing machine learning for EEG signal analysis:
Innovations in depth of anaesthesia assessment. <em>ARTMED</em>,
<em>151</em>, 102869. (<a
href="https://doi.org/10.1016/j.artmed.2024.102869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anaesthesia, crucial to surgical practice, is undergoing renewed scrutiny due to the integration of artificial intelligence in its medical use. The precise control over the temporary loss of consciousness is vital to ensure safe, pain-free procedures. Traditional methods of depth of anaesthesia (DoA) assessment, reliant on physical characteristics, have proven inconsistent due to individual variations. In response, electroencephalography (EEG) techniques have emerged, with indices such as the Bispectral Index offering quantifiable assessments. This literature review explores the current scope and frontier of DoA research, emphasising methods utilising EEG signals for effective clinical monitoring. This review offers a critical synthesis of recent advances, specifically focusing on electroencephalography (EEG) techniques and their role in enhancing clinical monitoring. By examining 117 high-impact papers, the review delves into the nuances of feature extraction, model building, and algorithm design in EEG-based DoA analysis. Comparative assessments of these studies highlight their methodological approaches and performance, including clinical correlations with established indices like the Bispectral Index. The review identifies knowledge gaps, particularly the need for improved collaboration for data access, which is essential for developing superior machine learning models and real-time predictive algorithms for patient management. It also calls for refined model evaluation processes to ensure robustness across diverse patient demographics and anaesthetic agents. The review underscores the potential of technological advancements to enhance precision, safety, and patient outcomes in anaesthesia, paving the way for a new standard in anaesthetic care. The findings of this review contribute to the ongoing discourse on the application of EEG in anaesthesia, providing insights into the potential for technological advancement in this critical area of medical practice.},
  archive      = {J_ARTMED},
  author       = {Thomas Schmierer and Tianning Li and Yan Li},
  doi          = {10.1016/j.artmed.2024.102869},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102869},
  shortjournal = {Artif. Intell. Med.},
  title        = {Harnessing machine learning for EEG signal analysis: Innovations in depth of anaesthesia assessment},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Application of machine learning in affordable and accessible
insulin management for type 1 and 2 diabetes: A comprehensive review.
<em>ARTMED</em>, <em>151</em>, 102868. (<a
href="https://doi.org/10.1016/j.artmed.2024.102868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper insulin management is vital for maintaining stable blood sugar levels and preventing complications associated with diabetes. However, the soaring costs of insulin present significant challenges to ensuring affordable management. This paper conducts a comprehensive review of current literature on the application of machine learning (ML) in insulin management for diabetes patients, particularly focusing on enhancing affordability and accessibility within the United States. The review encompasses various facets of insulin management, including dosage calculation and response, prediction of blood glucose and insulin sensitivity , initial insulin estimation, resistance prediction, treatment adherence, complications, hypoglycemia prediction, and lifestyle modifications. Additionally, the study identifies key limitations in the utilization of ML within the insulin management literature and suggests future research directions aimed at furthering accessible and affordable insulin treatments . These proposed directions include exploring insurance coverage, optimizing insulin type selection, assessing the impact of biosimilar insulin and market competition, considering mental health factors, evaluating insulin delivery options, addressing cost-related issues affecting insulin usage and adherence, and selecting appropriate patient cost-sharing programs. By examining the potential of ML in addressing insulin management affordability and accessibility, this work aims to envision improved and cost-effective insulin management practices. It not only highlights existing research gaps but also offers insights into future directions, guiding the development of innovative solutions that have the potential to revolutionize insulin management and benefit patients reliant on this life-saving treatment.},
  archive      = {J_ARTMED},
  author       = {Maryam Eghbali-Zarch and Sara Masoud},
  doi          = {10.1016/j.artmed.2024.102868},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102868},
  shortjournal = {Artif. Intell. Med.},
  title        = {Application of machine learning in affordable and accessible insulin management for type 1 and 2 diabetes: A comprehensive review},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep learning supported echocardiogram analysis: A
comprehensive review. <em>ARTMED</em>, <em>151</em>, 102866. (<a
href="https://doi.org/10.1016/j.artmed.2024.102866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An echocardiogram is a sophisticated ultrasound imaging technique employed to diagnose heart conditions. The transthoracic echocardiogram, one of the most prevalent types, is instrumental in evaluating significant cardiac diseases . However, interpreting its results heavily relies on the clinician’s expertise. In this context, artificial intelligence has emerged as a vital tool for helping clinicians. This study critically analyzes key state-of-the-art research that uses deep learning techniques to automate transthoracic echocardiogram analysis and support clinical judgments . We have systematically organized and categorized articles that proffer solutions for view classification, enhancement of image quality and dataset, segmentation and identification of cardiac structures, detection of cardiac function abnormalities, and quantification of cardiac functions. We compared the performance of various deep learning approaches within each category, identifying the most promising methods. Additionally, we highlight limitations in current research and explore promising avenues for future exploration. These include addressing generalizability issues, incorporating novel AI approaches, and tackling the analysis of rare cardiac diseases.},
  archive      = {J_ARTMED},
  author       = {Sanjeevi G. and Uma Gopalakrishnan and Rahul Krishnan Parthinarupothi and Thushara Madathil},
  doi          = {10.1016/j.artmed.2024.102866},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102866},
  shortjournal = {Artif. Intell. Med.},
  title        = {Deep learning supported echocardiogram analysis: A comprehensive review},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learnable weight initialization for volumetric medical image
segmentation. <em>ARTMED</em>, <em>151</em>, 102863. (<a
href="https://doi.org/10.1016/j.artmed.2024.102863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid volumetric medical image segmentation models, combining the advantages of local convolution and global attention, have recently received considerable attention. While mainly focusing on architectural modifications, most existing hybrid approaches still use conventional data-independent weight initialization schemes which restrict their performance due to ignoring the inherent volumetric nature of the medical data. To address this issue, we propose a learnable weight initialization approach that utilizes the available medical training data to effectively learn the contextual and structural cues via the proposed self-supervised objectives. Our approach is easy to integrate into any hybrid model and requires no external training data. Experiments on multi-organ and lung cancer segmentation tasks demonstrate the effectiveness of our approach, leading to state-of-the-art segmentation performance . Our proposed data-dependent initialization approach performs favorably as compared to the Swin-UNETR model pretrained using large-scale datasets on multi-organ segmentation task. Our source code and models are available at: https://github.com/ShahinaKK/LWI-VMS .},
  archive      = {J_ARTMED},
  author       = {Shahina Kunhimon and Abdelrahman Shaker and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan},
  doi          = {10.1016/j.artmed.2024.102863},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102863},
  shortjournal = {Artif. Intell. Med.},
  title        = {Learnable weight initialization for volumetric medical image segmentation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-step interpretable modeling of ICU-AIs. <em>ARTMED</em>,
<em>151</em>, 102862. (<a
href="https://doi.org/10.1016/j.artmed.2024.102862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel methodology for integrating high resolution longitudinal data with the dynamic prediction capabilities of survival models. The aim is two-fold: to improve the predictive power while maintaining the interpretability of the models. To go beyond the black box paradigm of artificial neural networks , we propose a parsimonious and robust semi-parametric approach (i.e., a landmarking competing risks model) that combines routinely collected low-resolution data with predictive features extracted from a convolutional neural network , that was trained on high resolution time-dependent information. We then use saliency maps to analyze and explain the extra predictive power of this model. To illustrate our methodology, we focus on healthcare-associated infections in patients admitted to an intensive care unit .},
  archive      = {J_ARTMED},
  author       = {G. Lancia and M.R.J. Varkila and O.L. Cremer and C. Spitoni},
  doi          = {10.1016/j.artmed.2024.102862},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102862},
  shortjournal = {Artif. Intell. Med.},
  title        = {Two-step interpretable modeling of ICU-AIs},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Challenges and strategies for wide-scale artificial
intelligence (AI) deployment in healthcare practices: A perspective for
healthcare organizations. <em>ARTMED</em>, <em>151</em>, 102861. (<a
href="https://doi.org/10.1016/j.artmed.2024.102861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare organizations have realized that Artificial intelligence (AI) can provide a competitive edge through personalized patient experiences, improved patient outcomes, early diagnosis, augmented clinician capabilities, enhanced operational efficiencies, or improved medical service accessibility. However, deploying AI-driven tools in the healthcare ecosystem could be challenging. This paper categorizes AI applications in healthcare and comprehensively examines the challenges associated with deploying AI in medical practices at scale. As AI continues to make strides in healthcare, its integration presents various challenges, including production timelines, trust generation, privacy concerns, algorithmic biases, and data scarcity. The paper highlights that flawed business models and wrong workflows in healthcare practices cannot be rectified merely by deploying AI-driven tools. Healthcare organizations should re-evaluate root problems such as misaligned financial incentives (e.g., fee-for-service models), dysfunctional medical workflows (e.g., high rates of patient readmissions), poor care coordination between different providers, fragmented electronic health records systems, and inadequate patient education and engagement models in tandem with AI adoption. This study also explores the need for a cultural shift in viewing AI not as a threat but as an enabler that can enhance healthcare delivery and create new employment opportunities while emphasizing the importance of addressing underlying operational issues. The necessity of investments beyond finance is discussed, emphasizing the importance of human capital, continuous learning, and a supportive environment for AI integration. The paper also highlights the crucial role of clear regulations in building trust, ensuring safety, and guiding the ethical use of AI, calling for coherent frameworks addressing transparency, model accuracy, data quality control, liability, and ethics. Furthermore, this paper underscores the importance of advancing AI literacy within academia to prepare future healthcare professionals for an AI-driven landscape. Through careful navigation and proactive measures addressing these challenges, the healthcare community can harness AI&#39;s transformative power responsibly and effectively, revolutionizing healthcare delivery and patient care. The paper concludes with a vision and strategic suggestions for the future of healthcare with AI, emphasizing thoughtful, responsible, and innovative engagement as the pathway to realizing its full potential to unlock immense benefits for healthcare organizations, physicians, nurses, and patients while proactively mitigating risks.},
  archive      = {J_ARTMED},
  author       = {Pouyan Esmaeilzadeh},
  doi          = {10.1016/j.artmed.2024.102861},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102861},
  shortjournal = {Artif. Intell. Med.},
  title        = {Challenges and strategies for wide-scale artificial intelligence (AI) deployment in healthcare practices: A perspective for healthcare organizations},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). IAFPs-mv-BiTCN: Predicting antifungal peptides using
self-attention transformer embedding and transform evolutionary based
multi-view features with bidirectional temporal convolutional networks.
<em>ARTMED</em>, <em>151</em>, 102860. (<a
href="https://doi.org/10.1016/j.artmed.2024.102860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Globally, fungal infections have become a major health concern in humans. Fungal diseases generally occur due to the invading fungus appearing on a specific portion of the body and becoming hard for the human immune system to resist. The recent emergence of COVID-19 has intensely increased different nosocomial fungal infections . The existing wet-laboratory-based medications are expensive, time-consuming, and may have adverse side effects on normal cells. In the last decade, peptide therapeutics have gained significant attention due to their high specificity in targeting affected cells without affecting healthy cells. Motivated by the significance of peptide-based therapies, we developed a highly discriminative prediction scheme called iAFPs-Mv-BiTCN to predict antifungal peptides correctly. The training peptides are encoded using word embedding methods such as skip-gram and attention mechanism-based bidirectional encoder representation using transformer. Additionally, transform-based evolutionary features are generated using the Pseduo position-specific scoring matrix using discrete wavelet transform (PsePSSM-DWT). The fused vector of word embedding and evolutionary descriptors is formed to compensate for the limitations of single encoding methods. A Shapley Additive exPlanations (SHAP) based global interpolation approach is applied to reduce training costs by choosing the optimal feature set. The selected feature set is trained using a bi-directional temporal convolutional network (BiTCN). The proposed iAFPs-Mv-BiTCN model achieved a predictive accuracy of 98.15 % and an AUC of 0.99 using training samples. In the case of the independent samples, our model obtained an accuracy of 94.11 % and an AUC of 0.98. Our iAFPs-Mv-BiTCN model outperformed existing models with a ~4 % and ~5 % higher accuracy using training and independent samples, respectively. The reliability and efficacy of the proposed iAFPs-Mv-BiTCN model make it a valuable tool for scientists and may perform a beneficial role in pharmaceutical design and research academia.},
  archive      = {J_ARTMED},
  author       = {Shahid Akbar and Quan Zou and Ali Raza and Fawaz Khaled Alarfaj},
  doi          = {10.1016/j.artmed.2024.102860},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102860},
  shortjournal = {Artif. Intell. Med.},
  title        = {IAFPs-mv-BiTCN: Predicting antifungal peptides using self-attention transformer embedding and transform evolutionary based multi-view features with bidirectional temporal convolutional networks},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Ontology-based decision support systems for diabetes
nutrition therapy: A systematic literature review. <em>ARTMED</em>,
<em>151</em>, 102859. (<a
href="https://doi.org/10.1016/j.artmed.2024.102859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes is a non-communicable disease that has reached epidemic proportions, affecting 537 million people globally. Artificial Intelligence can support patients or clinicians in diabetes nutrition therapy – the first medical therapy in most cases of Type 1 and Type 2 diabetes. In particular, ontology-based recommender and decision support systems can deliver a computable representation of experts&#39; knowledge, thus delivering patient-tailored nutritional recommendations or supporting clinical personnel in identifying the most suitable diet. This work proposes a systematic literature review of the domain ontologies describing diabetes in such systems, identifying their underlying conceptualizations, the users targeted by the systems, the type(s) of diabetes tackled, and the nutritional recommendations provided. This review also delves into the structure of the domain ontologies, highlighting several aspects that may hinder (or foster) their adoption in recommender and decision support systems for diabetes nutrition therapy. The results of this review process allow to underline how recommendations are formulated and the role of clinical experts in developing domain ontologies, outlining the research trends characterizing this research area. The results also allow for identifying research directions that can foster a preeminent role for clinical experts and clinical guidelines in a cooperative effort to make ontologies more interoperable – thus enabling them to play a significant role in the decision-making processes about diabetes nutrition therapy.},
  archive      = {J_ARTMED},
  author       = {Daniele Spoladore and Martina Tosi and Erna Cecilia Lorenzini},
  doi          = {10.1016/j.artmed.2024.102859},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102859},
  shortjournal = {Artif. Intell. Med.},
  title        = {Ontology-based decision support systems for diabetes nutrition therapy: A systematic literature review},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Towards classification and comprehensive analysis of
AI-based COVID-19 diagnostic techniques: A survey. <em>ARTMED</em>,
<em>151</em>, 102858. (<a
href="https://doi.org/10.1016/j.artmed.2024.102858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unpredictable pandemic came to light at the end of December 2019, known as the novel coronavirus, also termed COVID-19, identified by the World Health Organization (WHO). The virus first originated in Wuhan (China) and rapidly affected most of the world&#39;s population. This outbreak&#39;s impact is experienced worldwide because it causes high mortality risk, many cases, and economic falls. Around the globe, the total number of cases and deaths reported till November 12, 2022, were &gt;600 million and 6.6 million, respectively. During the period of COVID-19, several diverse diagnostic techniques have been proposed. This work presents a systematic review of COVID-19 diagnostic techniques in response to such acts. Initially, these techniques are classified into different categories based on their working principle and detection modalities, i.e. chest X-ray imaging, cough sound or respiratory patterns, RT-PCR, antigen testing, and antibody testing. After that, a comparative analysis is performed to evaluate these techniques&#39; efficacy which may help to determine an optimum solution for a particular scenario. The findings of the proposed work show that Artificial Intelligence plays a vital role in developing COVID-19 diagnostic techniques which support the healthcare system. The related work can be a footprint for all the researchers, available under a single umbrella. Additionally, all the techniques are long-lasting and can be used for future pandemics.},
  archive      = {J_ARTMED},
  author       = {Amna Kosar and Muhammad Asif and Maaz Bin Ahmad and Waseem Akram and Khalid Mahmood and Saru Kumari},
  doi          = {10.1016/j.artmed.2024.102858},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102858},
  shortjournal = {Artif. Intell. Med.},
  title        = {Towards classification and comprehensive analysis of AI-based COVID-19 diagnostic techniques: A survey},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pathways to democratized healthcare: Envisioning
human-centered AI-as-a-service for customized diagnosis and
rehabilitation. <em>ARTMED</em>, <em>151</em>, 102850. (<a
href="https://doi.org/10.1016/j.artmed.2024.102850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ongoing digital revolution in the healthcare sector, emphasized by bodies like the US Food and Drug Administration (FDA), is paving the way for a shift towards person-centric healthcare models. These models consider individual needs, turning patients from passive recipients to active participants. A key factor in this shift is Artificial Intelligence (AI), which has the capacity to revolutionize healthcare delivery due to its ability to personalize it. With the rise of software in healthcare and the proliferation of the Internet of Things (IoT), a surge of digital data is being produced. This data, alongside improvements in AI’s explainability, is facilitating the spread of person-centric healthcare models, aiming at improving health management and patient experience. This paper outlines a human-centered methodology for the development of an AI-as-a-service platform with the goal of broadening access to personalized healthcare. This approach places humans at its core, aiming to augment, not replace, human capabilities and integrate in current processes. The primary research question guiding this study is: “How can Human-Centered AI principles be considered when designing an AI-as-a-service platform that democratizes access to personalized healthcare?” This informed both our research direction and investigation. Our approach involves a design fiction methodology, engaging clinicians from different domains to gather their perspectives on how AI can meet their needs by envisioning potential future scenarios and addressing possible ethical and social challenges. Additionally, we incorporate Meta-Design principles, investigating opportunities for users to modify the AI system based on their experiences. This promotes a platform that evolves with the user and considers many different perspectives.},
  archive      = {J_ARTMED},
  author       = {Tommaso Turchi and Giuseppe Prencipe and Alessio Malizia and Silvia Filogna and Francesco Latrofa and Giuseppina Sgandurra},
  doi          = {10.1016/j.artmed.2024.102850},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102850},
  shortjournal = {Artif. Intell. Med.},
  title        = {Pathways to democratized healthcare: Envisioning human-centered AI-as-a-service for customized diagnosis and rehabilitation},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cracking the chronic pain code: A scoping review of
artificial intelligence in chronic pain research. <em>ARTMED</em>,
<em>151</em>, 102849. (<a
href="https://doi.org/10.1016/j.artmed.2024.102849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this review is to identify gaps and provide a direction for future research in the utilization of Artificial Intelligence (AI) in chronic pain (CP) management. A comprehensive literature search was conducted using various databases, including Ovid MEDLINE, Web of Science Core Collection, IEEE Xplore, and ACM Digital Library. The search was limited to studies on AI in CP research, focusing on diagnosis, prognosis, clinical decision support, self-management, and rehabilitation. The studies were evaluated based on predefined inclusion criteria, including the reporting quality of AI algorithms used. After the screening process, 60 studies were reviewed, highlighting AI’s effectiveness in diagnosing and classifying CP while revealing gaps in the attention given to treatment and rehabilitation. It was found that the most commonly used algorithms in CP research were support vector machines , logistic regression and random forest classifiers. The review also pointed out that attention to CP mechanisms is negligible despite being the most effective way to treat CP. The review concludes that to achieve more effective outcomes in CP management, future research should prioritize identifying CP mechanisms, CP management, and rehabilitation while leveraging a wider range of algorithms and architectures. This review highlights the potential of AI in improving the management of CP, which is a significant personal and economic burden affecting more than 30% of the world’s population. The identified gaps and future research directions provide valuable insights to researchers and practitioners in the field, with the potential to improve healthcare utilization.},
  archive      = {J_ARTMED},
  author       = {Md Asif Khan and Ryan G.L. Koh and Sajjad Rashidiani and Theodore Liu and Victoria Tucci and Dinesh Kumbhare and Thomas E. Doyle},
  doi          = {10.1016/j.artmed.2024.102849},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102849},
  shortjournal = {Artif. Intell. Med.},
  title        = {Cracking the chronic pain code: A scoping review of artificial intelligence in chronic pain research},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Medical knowledge graph completion via fusion of entity
description and type information. <em>ARTMED</em>, <em>151</em>, 102848.
(<a href="https://doi.org/10.1016/j.artmed.2024.102848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical Knowledge Graphs (MKGs) are vital in propelling big data technologies in healthcare and facilitating the realization of medical intelligence. However, large-scale MKGs often exhibit characteristics of data sparsity and missing facts. Following the latest advances, knowledge embedding addresses these problems by performing knowledge graph completion. Most knowledge embedding algorithms rely solely on triplet structural information, overlooking the rich information hidden within entity property sets, leading to bottlenecks in performance enhancement when dealing with the intricate relations of MKGs. Inspired by the semantic sensitivity and explicit type constraints unique to the medical domain, we propose BioBERT-based graph embedding model . This model represents an evolvable framework that integrates graph embedding, language embedding, and type information, thereby optimizing the utility of MKGs. Our study utilizes not only WordNet as a benchmark dataset but also incorporates MedicalKG to compare and corroborate the specificity of medical knowledge. Experimental results on these datasets indicate that the proposed fusion framework achieves state-of-art (SOTA) performance compared to other baselines. We believe that this incremental improvement provides promising insights for future medical knowledge graph completion endeavors.},
  archive      = {J_ARTMED},
  author       = {Xiaochen Wang and Runtong Zhang and Butian Zhao and Yuhan Yao and Hongmei Zhao and Xiaomin Zhu},
  doi          = {10.1016/j.artmed.2024.102848},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102848},
  shortjournal = {Artif. Intell. Med.},
  title        = {Medical knowledge graph completion via fusion of entity description and type information},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Building large-scale registries from unstructured clinical
notes using a low-resource natural language processing pipeline.
<em>ARTMED</em>, <em>151</em>, 102847. (<a
href="https://doi.org/10.1016/j.artmed.2024.102847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building clinical registries is an important step in clinical research and improvement of patient care quality. Natural Language Processing (NLP) methods have shown promising results in extracting valuable information from unstructured clinical notes. However, the structure and nature of clinical notes are very different from regular text that state-of-the-art NLP models are trained and tested on, and they have their own set of challenges. In this study, we propose Sentence Extractor with Keywords (SE-K), an efficient and interpretable classification approach for extracting information from clinical notes and show that it outperforms more computationally expensive methods in text classification. Following the Institutional Review Board (IRB) approval, we used SE-K and two embedding based NLP approaches (Sentence Extractor with Embeddings (SE-E) and Bidirectional Encoder Representations from Transformers (BERT)) to develop comprehensive registry of anterior cruciate ligament surgeries from 20 years of unstructured clinical data at a multi-site tertiary-care regional children&#39;s hospital. The low-resource approach (SE-K) had better performance (average AUROC of 0.94 ± 0.04) than the embedding-based approaches (SE-E: 0.93 ± 0.04 and BERT: 0.87 ± 0.09) for out of sample validation, in addition to minimum performance drop between test and out-of-sample validation. Moreover, the SE-K approach was at least six times faster (on CPU) than SE-E (on CPU) and BERT (on GPU) and provides interpretability. Our proposed approach, SE-K, can be effectively used to extract relevant variables from clinic notes to build large-scale registries, with consistently better performance compared to the more resource-intensive approaches (e.g., BERT). Such approaches can facilitate information extraction from unstructured notes for registry building, quality improvement and adverse event monitoring.},
  archive      = {J_ARTMED},
  author       = {Nazgol Tavabi and James Pruneski and Shahriar Golchin and Mallika Singh and Ryan Sanborn and Benton Heyworth and Assaf Landschaft and Amir Kimia and Ata Kiapour},
  doi          = {10.1016/j.artmed.2024.102847},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102847},
  shortjournal = {Artif. Intell. Med.},
  title        = {Building large-scale registries from unstructured clinical notes using a low-resource natural language processing pipeline},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical medical image report adversarial generation
with hybrid discriminator. <em>ARTMED</em>, <em>151</em>, 102846. (<a
href="https://doi.org/10.1016/j.artmed.2024.102846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating coherent reports from medical images is an important task for reducing doctors&#39; workload. Unlike traditional image captioning tasks, the task of medical image report generation faces more challenges. Current models for generating reports from medical images often fail to characterize some abnormal findings, and some models generate reports with low quality. In this study, we propose a model to generate high-quality reports from medical images. In this paper, we propose a model called Hybrid Discriminator Generative Adversarial Network (HDGAN), which combines Generative Adversarial Network (GAN) with Reinforcement Learning (RL). The HDGAN model consists of a generator, a one-sentence discriminator, and a one-word discriminator. Specifically, the RL reward signals are judged on the one-sentence discriminator and one-word discriminator separately. The one-sentence discriminator can better learn sentence-level structural information, while the one-word discriminator can learn word diversity information effectively. Our approach performs better on the IU-X-ray and COV-CTR datasets than the baseline models . For the ROUGE metric, our method outperforms the state-of-the-art model by 0.36 on the IU-X-ray, 0.06 on the MIMIC-CXR and 0.156 on the COV-CTR. The compositional framework we proposed can generate more accurate medical image reports at different levels.},
  archive      = {J_ARTMED},
  author       = {Junsan Zhang and Ming Cheng and Qiaoqiao Cheng and Xiuxuan Shen and Yao Wan and Jie Zhu and Mengxuan Liu},
  doi          = {10.1016/j.artmed.2024.102846},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102846},
  shortjournal = {Artif. Intell. Med.},
  title        = {Hierarchical medical image report adversarial generation with hybrid discriminator},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). De-identification of clinical free text using natural
language processing: A systematic review of current approaches.
<em>ARTMED</em>, <em>151</em>, 102845. (<a
href="https://doi.org/10.1016/j.artmed.2024.102845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) are a valuable resource for data-driven medical research . However, the presence of protected health information (PHI) makes EHRs unsuitable to be shared for research purposes. De-identification, i.e. the process of removing PHI is a critical step in making EHR data accessible. Natural language processing has repeatedly demonstrated its feasibility in automating the de-identification process. Our study aims to provide systematic evidence on how the de-identification of clinical free text written in English has evolved in the last thirteen years, and to report on the performances and limitations of the current state-of-the-art systems for the English language. In addition, we aim to identify challenges and potential research opportunities in this field. A systematic search in PubMed , Web of Science, and the DBLP was conducted for studies published between January 2010 and February 2023. Titles and abstracts were examined to identify the relevant studies. Selected studies were then analysed in-depth, and information was collected on de-identification methodologies, data sources, and measured performance. A total of 2125 publications were identified for the title and abstract screening. 69 studies were found to be relevant. Machine learning (37 studies) and hybrid (26 studies) approaches are predominant, while six studies relied only on rules. The majority of the approaches were trained and evaluated on public corpora. The 2014 i2b2/UTHealth corpus is the most frequently used (36 studies), followed by the 2006 i2b2 (18 studies) and 2016 CEGS N-GRID (10 studies) corpora. Earlier de-identification approaches aimed at English were mainly rule and machine learning hybrids with extensive feature engineering and post-processing, while more recent performance improvements are due to feature-inferring recurrent neural networks . Current leading performance is achieved using attention-based neural models. Recent studies report state-of-the-art F1-scores (over 98 %) when evaluated in the manner usually adopted by the clinical natural language processing community. However, their performance needs to be more thoroughly assessed with different measures to judge their reliability to safely de-identify data in a real-world setting. Without additional manually labeled training data , state-of-the-art systems fail to generalise well across a wide range of clinical sub-domains.},
  archive      = {J_ARTMED},
  author       = {Aleksandar Kovačević and Bojana Bašaragin and Nikola Milošević and Goran Nenadić},
  doi          = {10.1016/j.artmed.2024.102845},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102845},
  shortjournal = {Artif. Intell. Med.},
  title        = {De-identification of clinical free text using natural language processing: A systematic review of current approaches},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An innovative artificial intelligence-based method to
compress complex models into explainable, model-agnostic and reduced
decision support systems with application to healthcare (NEAR).
<em>ARTMED</em>, <em>151</em>, 102841. (<a
href="https://doi.org/10.1016/j.artmed.2024.102841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In everyday clinical practice, medical decision is currently based on clinical guidelines which are often static and rigid, and do not account for population variability, while individualized, patient-oriented decision and/or treatment are the paradigm change necessary to enter into the era of precision medicine. Most of the limitations of a guideline-based system could be overcome through the adoption of Clinical Decision Support Systems (CDSSs) based on Artificial Intelligence (AI) algorithms. However, the black-box nature of AI algorithms has hampered a large adoption of AI-based CDSSs in clinical practice. In this study, an innovative AI-based method to compress AI-based prediction models into explainable, model-agnostic, and reduced decision support systems (NEAR) with application to healthcare is presented and validated. NEAR is based on the Shapley Additive Explanations framework and can be applied to complex input models to obtain the contributions of each input feature to the output. Technically, the simplified NEAR models approximate contributions from input features using a custom library and merge them to determine the final output. Finally, NEAR estimates the confidence error associated with the single input feature contributing to the final score, making the result more interpretable. Here, NEAR is evaluated on a clinical real-world use case, the mortality prediction in patients who experienced Acute Coronary Syndrome (ACS), applying three different Machine Learning/Deep Learning models as implementation examples. NEAR, when applied to the ACS use case, exhibits performances like the ones of the AI-based model from which it is derived, as in the case of the Adaptive Boosting classifier, whose Area Under the Curve is not statistically different from the NEAR one, even the model&#39;s simplification. Moreover, NEAR comes with intrinsic explainability and modularity, as it can be tested on the developed web application platform ( https://neardashboard.pythonanywhere.com/ ). An explainable and reliable CDSS tailored to single-patient analysis has been developed. The proposed AI-based system has the potential to be used alongside the clinical guidelines currently employed in the medical setting making them more personalized and dynamic and assisting doctors in taking their everyday clinical decisions.},
  archive      = {J_ARTMED},
  author       = {Karim Kassem and Michela Sperti and Andrea Cavallo and Andrea Mario Vergani and Davide Fassino and Monica Moz and Alessandro Liscio and Riccardo Banali and Michael Dahlweid and Luciano Benetti and Francesco Bruno and Guglielmo Gallone and Ovidio De Filippo and Mario Iannaccone and Fabrizio D&#39;Ascenzo and Gaetano Maria De Ferrari and Umberto Morbiducci and Emanuele Della Valle and Marco Agostino Deriu},
  doi          = {10.1016/j.artmed.2024.102841},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102841},
  shortjournal = {Artif. Intell. Med.},
  title        = {An innovative artificial intelligence-based method to compress complex models into explainable, model-agnostic and reduced decision support systems with application to healthcare (NEAR)},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stable feature selection utilizing graph convolutional
neural network and layer-wise relevance propagation for biomarker
discovery in breast cancer. <em>ARTMED</em>, <em>151</em>, 102840. (<a
href="https://doi.org/10.1016/j.artmed.2024.102840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-throughput technologies are becoming increasingly important in discovering prognostic biomarkers and in identifying novel drug targets. With Mammaprint, Oncotype DX, and many other prognostic molecular signatures breast cancer is one of the paradigmatic examples of the utility of high-throughput data to deliver prognostic biomarkers, that can be represented in a form of a rather short gene list. Such gene lists can be obtained as a set of features (genes) that are important for the decisions of a Machine Learning (ML) method applied to high-dimensional gene expression data . Several studies have identified predictive gene lists for patient prognosis in breast cancer, but these lists are unstable and have only a few genes in common. Instability of feature selection impedes biological interpretability: genes that are relevant for cancer pathology should be members of any predictive gene list obtained for the same clinical type of patients. Stability and interpretability of selected features can be improved by including information on molecular networks in ML methods. Graph Convolutional Neural Network (GCNN) is a contemporary deep learning approach applicable to gene expression data structured by a prior knowledge molecular network. Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) are methods to explain individual decisions of deep learning models. We used both GCNN+LRP and GCNN+SHAP techniques to construct feature sets by aggregating individual explanations. We suggest a methodology to systematically and quantitatively analyze the stability, the impact on the classification performance, and the interpretability of the selected feature sets. We used this methodology to compare GCNN+LRP to GCNN+SHAP and to more classical ML-based feature selection approaches. Utilizing a large breast cancer gene expression dataset we show that, while feature selection with SHAP is useful in applications where selected features have to be impactful for classification performance, among all studied methods GCNN+LRP delivers the most stable (reproducible) and interpretable gene lists.},
  archive      = {J_ARTMED},
  author       = {Hryhorii Chereda and Andreas Leha and Tim Beißbarth},
  doi          = {10.1016/j.artmed.2024.102840},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102840},
  shortjournal = {Artif. Intell. Med.},
  title        = {Stable feature selection utilizing graph convolutional neural network and layer-wise relevance propagation for biomarker discovery in breast cancer},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cellular data extraction from multiplexed brain imaging data
using self-supervised dual-loss adaptive masked autoencoder.
<em>ARTMED</em>, <em>151</em>, 102828. (<a
href="https://doi.org/10.1016/j.artmed.2024.102828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable large-scale cell detection and segmentation is the fundamental first step to understanding biological processes in the brain. The ability to phenotype cells at scale can accelerate preclinical drug evaluation and system-level brain histology studies. The impressive advances in deep learning offer a practical solution to cell image detection and segmentation. Unfortunately, categorizing cells and delineating their boundaries for training deep networks is an expensive process that requires skilled biologists. This paper presents a novel self-supervised Dual-Loss Adaptive Masked Autoencoder (DAMA) for learning rich features from multiplexed immunofluorescence brain images. DAMA’s objective function minimizes the conditional entropy in pixel-level reconstruction and feature-level regression. Unlike existing self-supervised learning methods based on a random image masking strategy, DAMA employs a novel adaptive mask sampling strategy to maximize mutual information and effectively learn brain cell data. To the best of our knowledge, this is the first effort to develop a self-supervised learning method for multiplexed immunofluorescence brain images. Our extensive experiments demonstrate that DAMA features enable superior cell detection, segmentation, and classification performance without requiring many annotations. In addition, to examine the generalizability of DAMA, we also experimented on TissueNet, a multiplexed imaging dataset comprised of two-channel fluorescence images from six distinct tissue types, captured using six different imaging platforms. Our code is publicly available at https://github.com/hula-ai/DAMA .},
  archive      = {J_ARTMED},
  author       = {Son T. Ly and Bai Lin and Hung Q. Vo and Dragan Maric and Badrinath Roysam and Hien V. Nguyen},
  doi          = {10.1016/j.artmed.2024.102828},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102828},
  shortjournal = {Artif. Intell. Med.},
  title        = {Cellular data extraction from multiplexed brain imaging data using self-supervised dual-loss adaptive masked autoencoder},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monitoring multistage healthcare processes using state space
models and a machine learning based framework. <em>ARTMED</em>,
<em>151</em>, 102826. (<a
href="https://doi.org/10.1016/j.artmed.2024.102826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring healthcare processes, such as surgical outcomes, with a keen focus on detecting changes and unnatural conditions at an early stage is crucial for healthcare professionals and administrators. In line with this goal, control charts, which are the most popular tool in the field of Statistical Process Monitoring, are widely employed to monitor therapeutic processes. Healthcare processes are often characterized by a multistage structure in which several components, states or stages form the final products or outcomes. In such complex scenarios, Multistage Process Monitoring (MPM) techniques become invaluable for monitoring distinct states of the process over time. However, the healthcare sector has seen limited studies employing MPM. This study aims to fill this gap by developing an MPM control chart tailored for healthcare data to promote early detection, confirmation, and patient safety. As it is important to detect unnatural conditions in healthcare processes at an early stage, the statistical control charts are combined with machine learning techniques (i.e., we deal with Intelligent Control Charting, ICC) to enhance detection ability. Through Monte Carlo simulations , our method demonstrates better performance compared to its statistical counterparts. To underline the practical application of the proposed ICC framework, real data from a two-stage thyroid cancer surgery is utilized. This real-world case serves as a compelling illustration of the effectiveness of the developed MPM control chart in a healthcare setting.},
  archive      = {J_ARTMED},
  author       = {Ali Yeganeh and Arne Johannssen and Nataliya Chukhrova and Mohammad Rasouli},
  doi          = {10.1016/j.artmed.2024.102826},
  journal      = {Artificial Intelligence in Medicine},
  month        = {5},
  pages        = {102826},
  shortjournal = {Artif. Intell. Med.},
  title        = {Monitoring multistage healthcare processes using state space models and a machine learning based framework},
  volume       = {151},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leveraging code-free deep learning for pill recognition in
clinical settings: A multicenter, real-world study of performance across
multiple platforms. <em>ARTMED</em>, <em>150</em>, 102844. (<a
href="https://doi.org/10.1016/j.artmed.2024.102844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preventable patient harm, particularly medication errors , represent significant challenges in healthcare settings. Dispensing the wrong medication is often associated with mix-up of lookalike and soundalike drugs in high workload environments. Replacing manual dispensing with automated unit dose and medication dispensing systems to reduce medication errors is not always feasible in clinical facilities experiencing high patient turn-around or frequent dose changes. Artificial intelligence (AI) based pill recognition tools and smartphone applications could potentially aid healthcare workers in identifying pills in situations where more advanced dispensing systems are not implemented. Most of the published research on pill recognition focuses on theoretical aspects of model development using traditional coding and deep learning methods. The use of code-free deep learning (CFDL) as a practical alternative for accessible model development, and implementation of such models in tools intended to aid decision making in clinical settings, remains largely unexplored. In this study, we sought to address this gap in existing literature by investigating whether CFDL is a viable approach for developing pill recognition models using a custom dataset, followed by a thorough evaluation of the model across various deployment scenarios , and in multicenter clinical settings. Furthermore, we aimed to highlight challenges and propose solutions to achieve optimal performance and real-world applicability of pill recognition models, including when deployed on smartphone applications . A pill recognition model was developed utilizing Microsoft Azure Custom Vision platform and a large custom training dataset of 26,880 images captured from the top 30 most dispensed solid oral dosage forms (SODFs) at the three participating hospitals. A comprehensive internal and external testing strategy was devised, model&#39;s performance was investigated through the online API, and offline using exported TensorFlow Lite model running on a Windows PC and on Android , using a tailor-made testing smartphone application. Additionally, model&#39;s calibration, degree of reliance on color features and device dependency was thoroughly evaluated. Real-world performance was assessed using images captured by hospital pharmacists at three participating clinical centers. The pill recognition model showed high performance in Microsoft Azure Custom Vision platform with 98.7 % precision, 95.1 % recall, and 98.2 % mean average precision (mAP), with thresholds set to 50 %. During internal testing utilizing the online API, the model reached 93.7 % precision, 88.96 % recall, 90.81 % F1-score and 87.35 % mAP. Testing the offline TensorFlow Lite model on Windows PC showed a slight performance reduction, with 91.16 % precision, 83.82 % recall, 86.18 % F1-score and 82.55 % mAP. Performance of the model running offline on the Android application was further reduced to 86.50 % precision, 75.00 % recall, 77.83 % F1-score and 69.24 % mAP. During external clinical testing through the online API an overall precision of 83.10 %, recall of 71.39 %, and F1-score of 75.76 % was achieved. Our study demonstrates that using a CFDL approach is a feasible and cost-effective method for developing AI-based pill recognition systems. Despite the limitations encountered, our model performed well, particularly when accessed through the online API. The use of CFDL facilitates interdisciplinary collaboration, resulting in human-centered AI models with enhanced real-world applicability. We suggest that rather than striving to build a universally applicable pill recognition system, models should be tailored to the medications in a regional formulary or needs of a specific clinic, which can in turn lead to improved performance in real-world deployment in these locations. Parallel to focusing on model development, it is crucial to employ a human centered approach by training the end users on how to properly interact with the AI based system to maximize benefits. Future research is needed on refining pill recognition models for broader adaptability. This includes investigating image pre-processing and optimization techniques to enhance offline performance and operation on handheld devices. Moreover, future studies should explore methods to overcome limitations of CFDL development to enhance the robustness of models and reduce overfitting. Collaborative efforts between researchers in this domain and sharing of best practices are vital to improve pill recognition systems, ultimately enhancing patient safety and healthcare outcomes.},
  archive      = {J_ARTMED},
  author       = {Amir Reza Ashraf and Anna Somogyi-Végh and Sára Merczel and Nóra Gyimesi and András Fittler},
  doi          = {10.1016/j.artmed.2024.102844},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102844},
  shortjournal = {Artif. Intell. Med.},
  title        = {Leveraging code-free deep learning for pill recognition in clinical settings: A multicenter, real-world study of performance across multiple platforms},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multicentric development and validation of a multi-scale and
multi-task deep learning model for comprehensive lower extremity
alignment analysis. <em>ARTMED</em>, <em>150</em>, 102843. (<a
href="https://doi.org/10.1016/j.artmed.2024.102843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis of the knee , a widespread cause of knee disability, is commonly treated in orthopedics due to its rising prevalence. Lower extremity misalignment, pivotal in knee injury etiology and management, necessitates comprehensive mechanical alignment evaluation via frequently-requested weight-bearing long leg radiographs (LLR). Despite LLR’s routine use, current analysis techniques are error-prone and time-consuming. To address this, we conducted a multicentric study to develop and validate a deep learning (DL) model for fully automated leg alignment assessment on anterior–posterior LLR, targeting enhanced reliability and efficiency. The DL model, developed using 594 patients’ LLR and a 60%/10%/30% data split for training, validation, and testing, executed alignment analyses via a multi-step process, employing a detection network and nine specialized networks. It was designed to assess all vital anatomical and mechanical parameters for standard clinical leg deformity analysis and preoperative planning. Accuracy, reliability, and assessment duration were compared with three specialized orthopedic surgeons across two distinct institutional datasets (136 and 143 radiographs). The algorithm exhibited equivalent performance to the surgeons in terms of alignment accuracy (DL: 0.21 ± 0.18°to 1.06 ± 1.3°vs. OS: 0.21 ± 0.16°to 1.72 ± 1.96°), interrater reliability (ICC DL: 0.90 ± 0.05 to 1.0 ± 0.0 vs. ICC OS: 0.90 ± 0.03 to 1.0 ± 0.0), and clinically acceptable accuracy (DL: 53.9%–100% vs OS 30.8%–100%). Further, automated analysis significantly reduced analysis time compared to manual annotation (DL: 22 ± 0.6 s vs. OS; 101.7 ± 7 s, p ≤ ≤ 0.01). By demonstrating that our algorithm not only matches the precision of expert surgeons but also significantly outpaces them in both speed and consistency of measurements, our research underscores a pivotal advancement in harnessing AI to enhance clinical efficiency and decision-making in orthopaedics.},
  archive      = {J_ARTMED},
  author       = {Nikolas J. Wilhelm and Claudio E. von Schacky and Felix J. Lindner and Matthias J. Feucht and Yannick Ehmann and Jonas Pogorzelski and Sami Haddadin and Jan Neumann and Florian Hinterwimmer and Rüdiger von Eisenhart-Rothe and Matthias Jung and Maximilian F. Russe and Kaywan Izadpanah and Sebastian Siebenlist and Rainer Burgkart and Marco-Christopher Rupp},
  doi          = {10.1016/j.artmed.2024.102843},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102843},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multicentric development and validation of a multi-scale and multi-task deep learning model for comprehensive lower extremity alignment analysis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GravityNet for end-to-end small lesion detection.
<em>ARTMED</em>, <em>150</em>, 102842. (<a
href="https://doi.org/10.1016/j.artmed.2024.102842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel one-stage end-to-end detector specifically designed to detect small lesions in medical images. Precise localization of small lesions presents challenges due to their appearance and the diverse contextual backgrounds in which they are found. To address this, our approach introduces a new type of pixel-based anchor that dynamically moves towards the targeted lesion for detection. We refer to this new architecture as GravityNet, and the novel anchors as gravity points since they appear to be “attracted” by the lesions. We conducted experiments on two well-established medical problems involving small lesions to evaluate the performance of the proposed approach: microcalcifications detection in digital mammograms and microaneurysms detection in digital fundus images. Our method demonstrates promising results in effectively detecting small lesions in these medical imaging tasks.},
  archive      = {J_ARTMED},
  author       = {Ciro Russo and Alessandro Bria and Claudio Marrocco},
  doi          = {10.1016/j.artmed.2024.102842},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102842},
  shortjournal = {Artif. Intell. Med.},
  title        = {GravityNet for end-to-end small lesion detection},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PGKD-net: Prior-guided and knowledge diffusive network for
choroid segmentation. <em>ARTMED</em>, <em>150</em>, 102837. (<a
href="https://doi.org/10.1016/j.artmed.2024.102837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thickness of the choroid is considered to be an important indicator of clinical diagnosis. Therefore, accurate choroid segmentation in retinal OCT images is crucial for monitoring various ophthalmic diseases. However, this is still challenging due to the blurry boundaries and interference from other lesions. To address these issues, we propose a novel prior-guided and knowledge diffusive network (PGKD-Net) to fully utilize retinal structural information to highlight choroidal region features and boost segmentation performance. Specifically, it is composed of two parts: a Prior-mask Guided Network (PG-Net) for coarse segmentation and a Knowledge Diffusive Network (KD-Net) for fine segmentation. In addition, we design two novel feature enhancement modules, Multi-Scale Context Aggregation (MSCA) and Multi-Level Feature Fusion (MLFF). The MSCA module captures the long-distance dependencies between features from different receptive fields and improves the model’s ability to learn global context. The MLFF module integrates the cascaded context knowledge learned from PG-Net to benefit fine-level segmentation. Comprehensive experiments are conducted to evaluate the performance of the proposed PGKD-Net. Experimental results show that our proposed method achieves superior segmentation accuracy over other state-of-the-art methods. Our code is made up publicly available at: https://github.com/yzh-hdu/choroid-segmentation .},
  archive      = {J_ARTMED},
  author       = {Yaqi Wang and Zehua Yang and Xindi Liu and Zhi Li and Chengyu Wu and Yizhen Wang and Kai Jin and Dechao Chen and Gangyong Jia and Xiaodiao Chen and Juan Ye and Xingru Huang},
  doi          = {10.1016/j.artmed.2024.102837},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102837},
  shortjournal = {Artif. Intell. Med.},
  title        = {PGKD-net: Prior-guided and knowledge diffusive network for choroid segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Trustworthy clinical AI solutions: A unified review of
uncertainty quantification in deep learning models for medical image
analysis. <em>ARTMED</em>, <em>150</em>, 102830. (<a
href="https://doi.org/10.1016/j.artmed.2024.102830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The full acceptance of Deep Learning (DL) models in the clinical field is rather low with respect to the quantity of high-performing solutions reported in the literature. End users are particularly reluctant to rely on the opaque predictions of DL models. Uncertainty quantification methods have been proposed in the literature as a potential solution, to reduce the black-box effect of DL models and increase the interpretability and the acceptability of the result by the final user. In this review, we propose an overview of the existing methods to quantify uncertainty associated with DL predictions. We focus on applications to medical image analysis, which present specific challenges due to the high dimensionality of images and their variable quality, as well as constraints associated with real-world clinical routine. Moreover, we discuss the concept of structural uncertainty, a corpus of methods to facilitate the alignment of segmentation uncertainty estimates with clinical attention. We then discuss the evaluation protocols to validate the relevance of uncertainty estimates. Finally, we highlight the open challenges for uncertainty quantification in the medical field.},
  archive      = {J_ARTMED},
  author       = {Benjamin Lambert and Florence Forbes and Senan Doyle and Harmonie Dehaene and Michel Dojat},
  doi          = {10.1016/j.artmed.2024.102830},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102830},
  shortjournal = {Artif. Intell. Med.},
  title        = {Trustworthy clinical AI solutions: A unified review of uncertainty quantification in deep learning models for medical image analysis},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HR-BGCN: Predicting readmission for heart failure from
electronic health records. <em>ARTMED</em>, <em>150</em>, 102829. (<a
href="https://doi.org/10.1016/j.artmed.2024.102829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart failure has become a huge public health problem, and failure to accurately predict readmission will further lead to the disease’s high cost and high mortality. The construction of readmission prediction model can assist doctors in making decisions to prevent patients from deteriorating and reduce the cost burden. This paper extracts the patient discharge records from the MIMIC-III database. It divides the patients into three research categories : no readmission, readmission within 30 days, and readmission after 30 days, to predict the readmission of patients. We propose the HR-BGCN model to predict the readmission of patients. First, we use the Adaptive-TMix to improve the prediction indicators of a few categories and reduce the impact of unbalanced categories. Then, the knowledge-informed graph attention mechanism is proposed. By introducing a document-level explicit diagram structure , the coding ability of graph node features is significantly improved. The paragraph-level representation obtained through graph learning is combined with the context token-level representation of BERT , and finally, the multi-classification task is carried out. We also compare several typical graph learning classification models to verify the model’s effectiveness, such as the IA-GCN model, GAT model, etc. The results show that the average F1 score of the HR-BGCN model proposed in this paper for 30-day readmission of heart failure patients is 88.26%, and the average accuracy is 90.47%. The HR-BGCN model is significantly better than the graph learning classification model for predicting heart failure readmission. It can help doctors predict the 30-day readmission of patients, then reduce the readmission rate of patients.},
  archive      = {J_ARTMED},
  author       = {Huiting Ma and Dengao Li and Jumin Zhao and Wenjing Li and Jian Fu and Chunxia Li},
  doi          = {10.1016/j.artmed.2024.102829},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102829},
  shortjournal = {Artif. Intell. Med.},
  title        = {HR-BGCN: Predicting readmission for heart failure from electronic health records},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RM-GPT: Enhance the comprehensive generative ability of
molecular GPT model via LocalRNN and RealFormer. <em>ARTMED</em>,
<em>150</em>, 102827. (<a
href="https://doi.org/10.1016/j.artmed.2024.102827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the surging of cost, artificial intelligence-assisted de novo drug design has supplanted conventional methods and become an emerging option for drug discovery . Although there have arisen many successful examples of applying generative models to the molecular field , these methods struggle to deal with conditional generation that meet chemists’ practical requirements which ask for a controllable process to generate new molecules or optimize basic molecules with appointed conditions. To address this problem, a Recurrent Molecular-Generative Pretrained Transformer model is proposed, supplemented by LocalRNN and Residual Attention Layer Transformer, referred to as RM-GPT. RM-GPT rebuilds GPT model’s architecture by incorporating LocalRNN and Residual Attention Layer Transformer so that it is able to extract local information and build connectivity between attention blocks. The incorporation of Transformer in these two modules enables leveraging the parallel computing advantages of multi-head attention mechanisms while extracting local structural information effectively. Through exploring and learning in a large chemical space, RM-GPT absorbs the ability to generate drug-like molecules with conditions in demand, such as desired properties and scaffolds, precisely and stably. RM-GPT achieved better results than SOTA methods on conditional generation.},
  archive      = {J_ARTMED},
  author       = {Wenfeng Fan and Yue He and Fei Zhu},
  doi          = {10.1016/j.artmed.2024.102827},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102827},
  shortjournal = {Artif. Intell. Med.},
  title        = {RM-GPT: Enhance the comprehensive generative ability of molecular GPT model via LocalRNN and RealFormer},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated peripancreatic vessel segmentation and labeling
based on iterative trunk growth and weakly supervised mechanism.
<em>ARTMED</em>, <em>150</em>, 102825. (<a
href="https://doi.org/10.1016/j.artmed.2024.102825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peripancreatic vessel segmentation and anatomical labeling are pivotal aspects in aiding surgical planning and prognosis for patients with pancreatic tumors . Nevertheless, prevailing techniques often fall short in achieving satisfactory segmentation performance for the peripancreatic vein (PPV), leading to predictions characterized by poor integrity and connectivity. Besides, unsupervised labeling algorithms usually cannot deal with complex anatomical variation while fully supervised methods require a large number of voxel-wise annotations for training, which is very labor-intensive and time-consuming. To address these two problems, we propose an A utomated P eripancreatic v E ssel S egmentation and l A beling ( APESA ) framework, to not only highly improve the segmentation performance for PPV, but also efficiently identify the peripancreatic artery (PPA) branches. There are two core modules in our proposed APESA framework: iterative trunk growth module (ITGM) for vein segmentation and weakly supervised labeling mechanism (WSLM) for artery labeling. The ITGM is composed of a series of iterative submodules, each of which chooses the largest connected component of the previous PPV segmentation as the trunk of a tree structure, seeks for the potential missing branches around the trunk by our designed branch proposal network, and facilitates trunk growth under the connectivity constraint. The WSLM incorporates the rule-based pseudo label generation with less expert participation, an anatomical labeling network to learn the branch distribution voxel by voxel, and adaptive radius-based postprocessing to refine the branch structures of the labeling predictions. Our achieved Dice of 94.01% for PPV segmentation on our collected dataset represents an approximately 10% accuracy improvement compared to state-of-the-art methods. Additionally, we attained a Dice of 97.01% for PPA segmentation and competitive labeling performance for PPA labeling compared to prior works. Our source codes will be publicly available at https://github.com/ZouLiwen-1999/APESA .},
  archive      = {J_ARTMED},
  author       = {Liwen Zou and Zhenghua Cai and Liang Mao and Ziwei Nie and Yudong Qiu and Xiaoping Yang},
  doi          = {10.1016/j.artmed.2024.102825},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102825},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automated peripancreatic vessel segmentation and labeling based on iterative trunk growth and weakly supervised mechanism},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning algorithms to predict outcomes in children
and adolescents with COVID-19: A systematic review. <em>ARTMED</em>,
<em>150</em>, 102824. (<a
href="https://doi.org/10.1016/j.artmed.2024.102824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aimed to analyze the study designs, modeling approaches, and performance evaluation metrics in studies using machine learning techniques to develop clinical prediction models for children and adolescents with COVID-19. We searched four databases for articles published between 01/01/2020 and 10/25/2023, describing the development of multivariable prediction models using any machine learning technique for predicting several outcomes in children and adolescents who had COVID-19. We included ten articles, six (60 % [95 % confidence interval (CI) 0.31 - 0.83]) were predictive diagnostic models and four (40% [95 % CI 0.170.69]) were prognostic models. All models were developed to predict a binary outcome (n= 10/10, 100 % [95 % CI 0.72-1]). The most frequently predicted outcome was disease detection (n=3/10, 30% [95 % CI 0.11-0.60]). The most commonly used machine learning models in the studies were tree-based (n=12/33, 36.3% [95 % CI 0.17-0.47]) and neural networks (n=9/27, 33.2% [95% CI 0.15-0.44]). Our review revealed that attention is required to address problems including small sample sizes , inconsistent reporting practices on data preparation, biases in data sources, lack of reporting metrics such as calibration and discrimination, hyperparameters and other aspects that allow reproducibility by other researchers and might improve the methodology.},
  archive      = {J_ARTMED},
  author       = {Adriano Lages dos Santos and Clara Pinhati and Jonathan Perdigão and Stella Galante and Ludmilla Silva and Isadora Veloso and Ana Cristina Simões e Silva and Eduardo Araújo Oliveira},
  doi          = {10.1016/j.artmed.2024.102824},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102824},
  shortjournal = {Artif. Intell. Med.},
  title        = {Machine learning algorithms to predict outcomes in children and adolescents with COVID-19: A systematic review},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A label information fused medical image report generation
framework. <em>ARTMED</em>, <em>150</em>, 102823. (<a
href="https://doi.org/10.1016/j.artmed.2024.102823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging is an important tool for clinical diagnosis. Nevertheless, it is very time-consuming and error-prone for physicians to prepare imaging diagnosis reports. Therefore, it is necessary to develop some methods to generate medical imaging reports automatically. Currently, the task of medical imaging report generation is challenging in at least two aspects: (1) medical images are very similar to each other. The differences between normal and abnormal images and between different abnormal images are usually trivial; (2) unrelated or incorrect keywords describing abnormal findings in the generated reports lead to mis-communications. In this paper, we propose a medical image report generation framework composed of four modules, including a Transformer encoder, a MIX-MLP multi-label classification network, a co-attention mechanism (CAM) based semantic and visual feature fusion , and a hierarchical LSTM decoder. The Transformer encoder can be used to learn long-range dependencies between images and labels, effectively extract visual and semantic features of images, and establish long-term dependent relationships between visual and semantic information to accurately extract abnormal features from images. The MIX-MLP multi-label classification network, the co-attention mechanism and the hierarchical LSTM network can better identify abnormalities, achieving visual and text alignment fusion and multi-label diagnostic classification to better facilitate report generation. The results of the experiments performed on two widely used radiology report datasets, IU X-RAY and MIMIC-CXR, show that our proposed framework outperforms current report generation models in terms of both natural linguistic generation metrics and clinical efficacy assessment metrics. The code of this work is available online at https://github.com/watersunhznu/LIFMRG .},
  archive      = {J_ARTMED},
  author       = {Shuifa Sun and Zhoujunsen Mei and Xiaolong Li and Tinglong Tang and Zhanglin Su and Yirong Wu},
  doi          = {10.1016/j.artmed.2024.102823},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102823},
  shortjournal = {Artif. Intell. Med.},
  title        = {A label information fused medical image report generation framework},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automatic quantitative stroke severity assessment based on
chinese clinical named entity recognition with domain-adaptive
pre-trained large language model. <em>ARTMED</em>, <em>150</em>, 102822.
(<a href="https://doi.org/10.1016/j.artmed.2024.102822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke is a prevalent disease with a significant global impact. Effective assessment of stroke severity is vital for an accurate diagnosis, appropriate treatment, and optimal clinical outcomes. The National Institutes of Health Stroke Scale (NIHSS) is a widely used scale for quantitatively assessing stroke severity. However, the current manual scoring of NIHSS is labor-intensive, time-consuming, and sometimes unreliable. Applying artificial intelligence (AI) techniques to automate the quantitative assessment of stroke on vast amounts of electronic health records (EHRs) has attracted much interest. This study aims to develop an automatic, quantitative stroke severity assessment framework through automating the entire NIHSS scoring process on Chinese clinical EHRs. Our approach consists of two major parts: Chinese clinical named entity recognition (CNER) with a domain-adaptive pre-trained large language model (LLM) and automated NIHSS scoring. To build a high-performing CNER model, we first construct a stroke-specific, densely annotated dataset “Chinese Stroke Clinical Records” (CSCR) from EHRs provided by our partner hospital, based on a stroke ontology that defines semantically related entities for stroke assessment. We then pre-train a Chinese clinical LLM coined “CliRoberta” through domain-adaptive transfer learning and construct a deep learning-based CNER model that can accurately extract entities directly from Chinese EHRs. Finally, an automated, end-to-end NIHSS scoring pipeline is proposed by mapping the extracted entities to relevant NIHSS items and values, to quantitatively assess the stroke severity. Results obtained on a benchmark dataset CCKS2019 and our newly created CSCR dataset demonstrate the superior performance of our domain-adaptive pre-trained LLM and the CNER model, compared with the existing benchmark LLMs and CNER models. The high F1 score of 0.990 ensures the reliability of our model in accurately extracting the entities for the subsequent automatic NIHSS scoring. Subsequently, our automated, end-to-end NIHSS scoring approach achieved excellent inter-rater agreement (0.823) and intraclass consistency (0.986) with the ground truth and significantly reduced the processing time from minutes to a few seconds. Our proposed automatic and quantitative framework for assessing stroke severity demonstrates exceptional performance and reliability through directly scoring the NIHSS from diagnostic notes in Chinese clinical EHRs. Moreover, this study also contributes a new clinical dataset, a pre-trained clinical LLM, and an effective deep learning-based CNER model. The deployment of these advanced algorithms can improve the accuracy and efficiency of clinical assessment, and help improve the quality, affordability and productivity of healthcare services .},
  archive      = {J_ARTMED},
  author       = {Zhanzhong Gu and Xiangjian He and Ping Yu and Wenjing Jia and Xiguang Yang and Gang Peng and Penghui Hu and Shiyan Chen and Hongjie Chen and Yiguang Lin},
  doi          = {10.1016/j.artmed.2024.102822},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102822},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automatic quantitative stroke severity assessment based on chinese clinical named entity recognition with domain-adaptive pre-trained large language model},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing entropy features in time-series data for pattern
recognition in neurological conditions. <em>ARTMED</em>, <em>150</em>,
102821. (<a href="https://doi.org/10.1016/j.artmed.2024.102821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of medical diagnosis and patient monitoring, effective pattern recognition in neurological time-series data is essential. Traditional methods predominantly based on statistical or probabilistic learning and inference often struggle with multivariate, multi-source, state-varying, and noisy data while also posing privacy risks due to excessive information collection and modeling. Furthermore, these methods often overlook critical statistical information, such as the distribution of data points and inherent uncertainties. To address these challenges, we introduce an information theory-based pipeline that leverages specialized features to identify patterns in neurological time-series data while minimizing privacy risks. We incorporate various entropy methods based on the characteristics of different scenarios and entropy. For stochastic state transition applications, we incorporate Shannon’s entropy, entropy rates, entropy production , and the von Neumann entropy of Markov chains . When state modeling is impractical, we select and employ approximate entropy, increment entropy, dispersion entropy, phase entropy, and slope entropy. The pipeline’s effectiveness and scalability are demonstrated through pattern analysis in a dementia care dataset and also an epileptic and a myocardial infarction dataset. The results indicate that our information theory-based pipeline can achieve average performance improvements across various models on the recall rate, F1 score, and accuracy by up to 13.08 percentage points, while enhancing inference efficiency by reducing the number of model parameters by an average of 3.10 times. Thus, our approach opens a promising avenue for improved, efficient, and critical statistical information-considered pattern recognition in medical time-series data.},
  archive      = {J_ARTMED},
  author       = {Yushan Huang and Yuchen Zhao and Alexander Capstick and Francesca Palermo and Hamed Haddadi and Payam Barnaghi},
  doi          = {10.1016/j.artmed.2024.102821},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102821},
  shortjournal = {Artif. Intell. Med.},
  title        = {Analyzing entropy features in time-series data for pattern recognition in neurological conditions},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting drug activity against cancer through genomic
profiles and SMILES. <em>ARTMED</em>, <em>150</em>, 102820. (<a
href="https://doi.org/10.1016/j.artmed.2024.102820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the constant increase in cancer rates, the disease has become a leading cause of death worldwide, enhancing the need for its detection and treatment. In the era of personalized medicine, the main goal is to incorporate individual variability in order to choose more precisely which therapy and prevention strategies suit each person. However, predicting the sensitivity of tumors to anticancer treatments remains a challenge. In this work, we propose two deep neural network models to predict the impact of anticancer drugs in tumors through the half-maximal inhibitory concentration (IC50). These models join biological and chemical data to apprehend relevant features of the genetic profile and the drug compounds, respectively. In order to predict the drug response in cancer cell lines , this study employed different DL methods, resorting to Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). In the first stage, two autoencoders were pre-trained with high-dimensional gene expression and mutation data of tumors. Afterward, this genetic background is transferred to the prediction models that return the IC50 value that portrays the potency of a substance in inhibiting a cancer cell line. When comparing RSEM Expected counts and TPM as methods for displaying gene expression data , RSEM has been shown to perform better in deep models and CNNs model can obtain better insight in these types of data. Moreover, the obtained results reflect the effectiveness of the extracted deep representations in the prediction of the IC50 value that portrays the potency of a substance in inhibiting a tumor, achieving a performance of a mean squared error of 1.06 and surpassing previous state-of-the-art models.},
  archive      = {J_ARTMED},
  author       = {Maryam Abbasi and Filipa G. Carvalho and Bernardete Ribeiro and Joel P. Arrais},
  doi          = {10.1016/j.artmed.2024.102820},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102820},
  shortjournal = {Artif. Intell. Med.},
  title        = {Predicting drug activity against cancer through genomic profiles and SMILES},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Never tell me the odds: Investigating pro-hoc explanations
in medical decision making. <em>ARTMED</em>, <em>150</em>, 102819. (<a
href="https://doi.org/10.1016/j.artmed.2024.102819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines a kind of explainable AI , centered around what we term pro-hoc explanations , that is a form of support that consists of offering alternative explanations (one for each possible outcome) instead of a specific post-hoc explanation following specific advice. Specifically, our support mechanism utilizes explanations by examples , featuring analogous cases for each category in a binary setting. Pro-hoc explanations are an instance of what we called frictional AI , a general class of decision support aimed at achieving a useful compromise between the increase of decision effectiveness and the mitigation of cognitive risks, such as over-reliance, automation bias and deskilling. To illustrate an instance of frictional AI, we conducted an empirical user study to investigate its impact on the task of radiological detection of vertebral fractures in x-rays. Our study engaged 16 orthopedists in a ‘human-first, second-opinion’ interaction protocol. In this protocol, clinicians first made initial assessments of the x-rays without AI assistance and then provided their final diagnosis after considering the pro-hoc explanations. Our findings indicate that physicians, particularly those with less experience, perceived pro-hoc XAI support as significantly beneficial, even though it did not notably enhance their diagnostic accuracy . However, their increased confidence in final diagnoses suggests a positive overall impact. Given the promisingly high effect size observed, our results advocate for further research into pro-hoc explanations specifically, and into the broader concept of frictional AI.},
  archive      = {J_ARTMED},
  author       = {Federico Cabitza and Chiara Natali and Lorenzo Famiglini and Andrea Campagner and Valerio Caccavella and Enrico Gallazzi},
  doi          = {10.1016/j.artmed.2024.102819},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102819},
  shortjournal = {Artif. Intell. Med.},
  title        = {Never tell me the odds: Investigating pro-hoc explanations in medical decision making},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ECG-based cardiac arrhythmias detection through ensemble
learning and fusion of deep spatial–temporal and long-range dependency
features. <em>ARTMED</em>, <em>150</em>, 102818. (<a
href="https://doi.org/10.1016/j.artmed.2024.102818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiac arrhythmia is one of the prime reasons for death globally. Early diagnosis of heart arrhythmia is crucial to provide timely medical treatment. Heart arrhythmias are diagnosed by analyzing the electrocardiogram (ECG) of patients. Manual analysis of ECG is time-consuming and challenging. Hence, effective automated detection of heart arrhythmias is important to produce reliable results. Different deep-learning techniques to detect heart arrhythmias such as Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), Transformer, and Hybrid CNN–LSTM were proposed. However, these techniques, when used individually, are not sufficient to effectively learn multiple features from the ECG signal. The fusion of CNN and LSTM overcomes the limitations of CNN in the existing studies as CNN–LSTM hybrids can extract spatiotemporal features. However, LSTMs suffer from long-range dependency issues due to which certain features may be ignored. Hence, to compensate for the drawbacks of the existing models, this paper proposes a more comprehensive feature fusion technique by merging CNN, LSTM, and Transformer models. The fusion of these models facilitates learning spatial, temporal, and long-range dependency features, hence, helping to capture different attributes of the ECG signal. These features are subsequently passed to a majority voting classifier equipped with three traditional base learners. The traditional learners are enriched with deep features instead of handcrafted features. Experiments are performed on the MIT-BIH arrhythmias database and the model performance is compared with that of the state-of-art models. Results reveal that the proposed model performs better than the existing models yielding an accuracy of 99.56%.},
  archive      = {J_ARTMED},
  author       = {Sadia Din and Marwa Qaraqe and Omar Mourad and Khalid Qaraqe and Erchin Serpedin},
  doi          = {10.1016/j.artmed.2024.102818},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102818},
  shortjournal = {Artif. Intell. Med.},
  title        = {ECG-based cardiac arrhythmias detection through ensemble learning and fusion of deep spatial–temporal and long-range dependency features},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting time-to-intubation after critical care admission
using machine learning and cured fraction information. <em>ARTMED</em>,
<em>150</em>, 102817. (<a
href="https://doi.org/10.1016/j.artmed.2024.102817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intubation for mechanical ventilation (MV) is one of the most common high-risk procedures performed in Intensive Care Units (ICUs). Early prediction of intubation may have a positive impact by providing timely alerts to clinicians and consequently avoiding high-risk late intubations. In this work, we propose a new machine learning method to predict the time to intubation during the first five days of ICU admission, based on the concept of cure survival models. Our approach combines classification and survival analysis, to effectively accommodate the fraction of patients not at risk of intubation, and provide a better estimate of time to intubation, for patients at risk. We tested our approach and compared it to other predictive models on a dataset collected from a secondary care hospital (AZ Groeninge, Kortrijk, Belgium) from 2015 to 2021, consisting of 3425 ICU stays. Furthermore, we utilised SHAP for feature importance analysis, extracting key insights into the relative significance of variables such as vital signs, blood gases, and patient characteristics in predicting intubation in ICU settings. The results corroborate that our approach improves the prediction of time to intubation in critically ill patients, by using routinely collected data within the first hours of admission in the ICU. Early warning of the need for intubation may be used to help clinicians predict the risk of intubation and rank patients according to their expected time to intubation.},
  archive      = {J_ARTMED},
  author       = {Michela Venturini and Ingrid Van Keilegom and Wouter De Corte and Celine Vens},
  doi          = {10.1016/j.artmed.2024.102817},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102817},
  shortjournal = {Artif. Intell. Med.},
  title        = {Predicting time-to-intubation after critical care admission using machine learning and cured fraction information},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Acknowledgements to our reviewers in 2023. <em>ARTMED</em>,
<em>150</em>, 102816. (<a
href="https://doi.org/10.1016/j.artmed.2024.102816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  doi          = {10.1016/j.artmed.2024.102816},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102816},
  shortjournal = {Artif. Intell. Med.},
  title        = {Acknowledgements to our reviewers in 2023},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Intelligent decision support systems for dementia care: A
scoping review. <em>ARTMED</em>, <em>150</em>, 102815. (<a
href="https://doi.org/10.1016/j.artmed.2024.102815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of dementia care, Artificial Intelligence (AI) powered clinical decision support systems have the potential to enhance diagnosis and management. However, the scope and challenges of applying these technologies remain unclear. This scoping review aims to investigate the current state of AI applications in the development of intelligent decision support systems for dementia care. We conducted a comprehensive scoping review of empirical studies that utilised AI-powered clinical decision support systems in dementia care. The results indicate that AI applications in dementia care primarily focus on diagnosis, with limited attention to other aspects outlined in the World Health Organization (WHO) Global Action Plan on the Public Health Response to Dementia 2017–2025 (GAPD). A trifecta of challenges, encompassing data availability, cost considerations, and AI algorithm performance, emerges as noteworthy barriers in adoption of AI applications in dementia care. To address these challenges and enhance AI reliability, we propose a novel approach: a digital twin-based patient journey model. Future research should address identified gaps in GAPD action areas, navigate data-related obstacles, and explore the implementation of digital twins. Additionally, it is imperative to emphasize that addressing trust and combating the stigma associated with AI in healthcare should be a central focus of future research directions.},
  archive      = {J_ARTMED},
  author       = {Amirhossein Eslami Andargoli and Nalika Ulapane and Tuan Anh Nguyen and Nadeem Shuakat and John Zelcer and Nilmini Wickramasinghe},
  doi          = {10.1016/j.artmed.2024.102815},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102815},
  shortjournal = {Artif. Intell. Med.},
  title        = {Intelligent decision support systems for dementia care: A scoping review},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online biomedical named entities recognition by data and
knowledge-driven model. <em>ARTMED</em>, <em>150</em>, 102813. (<a
href="https://doi.org/10.1016/j.artmed.2024.102813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Named entity recognition (NER) is an important task for the natural language processing of biomedical text. Currently, most NER studies standardized biomedical text, but NER for unstandardized biomedical text draws less attention from researchers. Named entities in online biomedical text exist with errors and polymorphisms, which negatively impact NER models’ performance and impede support from knowledge representation methods. In this paper, we propose a neural network method that can effectively recognize entities in unstandardized online medical/health text. We introduce a new pre-training scheme that uses large-scale online question-answering pairs to enhance transformers’ model capacity on online biomedical text. Moreover, we supply models with knowledge representations from a knowledge base called multi-channel knowledge labels, and this method overcomes the restriction from languages, like Chinese, that require word segmentation tools to represent knowledge. Our model outperforms other baseline methods significantly in experiments on a dataset for Chinese online medical entity recognition and achieves state-of-the-art results.},
  archive      = {J_ARTMED},
  author       = {Lulu Cao and Chaochen Wu and Guan Luo and Chao Guo and Anni Zheng},
  doi          = {10.1016/j.artmed.2024.102813},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102813},
  shortjournal = {Artif. Intell. Med.},
  title        = {Online biomedical named entities recognition by data and knowledge-driven model},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Clinical knowledge-guided deep reinforcement learning for
sepsis antibiotic dosing recommendations. <em>ARTMED</em>, <em>150</em>,
102811. (<a href="https://doi.org/10.1016/j.artmed.2024.102811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is the third leading cause of death worldwide. Antibiotics are an important component in the treatment of sepsis. The use of antibiotics is currently facing the challenge of increasing antibiotic resistance (Evans et al., 2021). Sepsis medication prediction can be modeled as a Markov decision process, but existing methods fail to integrate with medical knowledge, making the decision process potentially deviate from medical common sense and leading to underperformance. (Wang et al., 2021). In this paper, we use Deep Q-Network (DQN) to construct a Sepsis Anti-infection DQN (SAI-DQN) model to address the challenge of determining the optimal combination and duration of antibiotics in sepsis treatment. By setting sepsis clinical knowledge as reward functions to guide DQN complying with medical guidelines, we formed personalized treatment recommendations for antibiotic combinations. The results showed that our model had a higher average value for decision-making than clinical decisions. For the test set of patients, our model predicts that 79.07% of patients will achieve a favorable prognosis with the recommended combination of antibiotics. By statistically analyzing decision trajectories and drug action selection, our model was able to provide reasonable medication recommendations that comply with clinical practices. Our model was able to improve patient outcomes by recommending appropriate antibiotic combinations in line with certain clinical knowledge.},
  archive      = {J_ARTMED},
  author       = {Yuan Wang and Anqi Liu and Jucheng Yang and Lin Wang and Ning Xiong and Yisong Cheng and Qin Wu},
  doi          = {10.1016/j.artmed.2024.102811},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102811},
  shortjournal = {Artif. Intell. Med.},
  title        = {Clinical knowledge-guided deep reinforcement learning for sepsis antibiotic dosing recommendations},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patient-specific game-based transfer method for parkinson’s
disease severity prediction. <em>ARTMED</em>, <em>150</em>, 102810. (<a
href="https://doi.org/10.1016/j.artmed.2024.102810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dysphonia is one of the early symptoms of Parkinson&#39;s disease (PD). Most existing methods use feature selection methods to find the optimal subset of voice features for all PD patients. Few have considered the heterogeneity between patients, which implies the need to provide specific prediction models for different patients. However, building the specific model faces the challenge of small sample size, which makes it lack generalization ability . Instance transfer is an effective way to solve this problem. Therefore, this paper proposes a patient-specific game-based transfer (PSGT) method for PD severity prediction. First, a selection mechanism is used to select PD patients with similar disease trends to the target patient from the source domain, which reduces the risk of negative transfer. Then, the contribution of the transferred subjects and their instances to the disease estimation of the target subject is fairly evaluated by the Shapley value, which improves the interpretability of the method. Next, the proportion of valid instances in the transferred subjects is determined, and the instances with higher contribution are transferred to further reduce the difference between the transferred instance subset and the target subject. Finally, the selected subset of instances is added to the training set of the target subject, and the extended data is fed into the random forest to improve the performance of the method. Parkinson&#39;s telemonitoring dataset is used to evaluate the feasibility and effectiveness. The mean values of mean absolute error , root mean square error , and volatility obtained by predicting motor-UPDRS and total-UPDRS for target patients are 1.59, 1.95, 1.56 and 1.98, 2.54, 1.94, respectively. Experiment results show that the PSGT has better performance in both prediction error and stability over compared methods.},
  archive      = {J_ARTMED},
  author       = {Zaifa Xue and Huibin Lu and Tao Zhang and Max A. Little},
  doi          = {10.1016/j.artmed.2024.102810},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102810},
  shortjournal = {Artif. Intell. Med.},
  title        = {Patient-specific game-based transfer method for parkinson&#39;s disease severity prediction},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bimodal feature fusion convolutional neural network for
detecting obstructive sleep apnea/hypopnea from nasal airflow and
oximetry signals. <em>ARTMED</em>, <em>150</em>, 102808. (<a
href="https://doi.org/10.1016/j.artmed.2024.102808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most prevalent sleep-disordered breathing condition is Obstructive Sleep Apnea (OSA), which has been linked to various health consequences, including cardiovascular disease (CVD) and even sudden death . Therefore, early detection of OSA can effectively help patients prevent the diseases induced by it. However, many existing methods have low accuracy in detecting hypopnea events or even ignore them altogether. According to the guidelines provided by the American Academy of Sleep Medicine (AASM), two modal signals, namely nasal pressure airflow and pulse oxygen saturation (Sp O 2 O2 ), offer significant advantages in detecting OSA, particularly hypopnea events. Inspired by this notion, we propose a bimodal feature fusion CNN model that primarily comprises of a dual-branch CNN module and a feature fusion module for the classification of 10-second-long segments of nasal pressure airflow and Sp O 2 O2 . Additionally, an Efficient Channel Attention mechanism (ECA) is incorporated into the second module to adaptively weight feature map of each channel for improving classification accuracy . Furthermore, we design an OSA Severity Assessment Framework (OSAF) to aid physicians in effectively diagnosing OSA severity. The performance of both the bimodal feature fusion CNN model and OSAF is demonstrated to be excellent through per-segment and per-patient experimental results, based on the evaluation of our method using two real-world datasets consisting of polysomnography (PSG) recordings from 450 subjects.},
  archive      = {J_ARTMED},
  author       = {Dandan Peng and Huijun Yue and Wenjun Tan and Wenbin Lei and Guozhu Chen and Wen Shi and Yanchun Zhang},
  doi          = {10.1016/j.artmed.2024.102808},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102808},
  shortjournal = {Artif. Intell. Med.},
  title        = {A bimodal feature fusion convolutional neural network for detecting obstructive sleep apnea/hypopnea from nasal airflow and oximetry signals},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). GCNGAT: Drug–disease association prediction based on graph
convolution neural network and graph attention network. <em>ARTMED</em>,
<em>150</em>, 102805. (<a
href="https://doi.org/10.1016/j.artmed.2024.102805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting drug–disease associations can contribute to discovering new therapeutic potentials of drugs, and providing important association information for new drug research and development. Many existing drug–disease association prediction methods have not distinguished relevant background information for the same drug targeted to different diseases. Therefore, this paper proposes a drug–disease association prediction model based on graph convolutional network and graph attention network (GCNGAT) to reposition marketed drugs under the distinguishment of background information. Firstly, in order to obtain initial drug–disease information, a drug–disease heterogeneous graph structure is constructed based on all known drug–disease associations. Secondly, based on the heterogeneous graph structure, the corresponding subgraphs of each group of drug–disease association pairs are extracted to distinguish different background information for the same drug from different diseases. Finally, a model combining Graph neural network with global Average pooling (GnnAp) is designed to predict potential drug–disease associations by learning drug–disease interaction feature representations. The experimental results show that adding subgraph extraction can effectively improve the prediction performance of the model, and the graph representation learning module can fully extract the deep features of drug–disease. Using the 5-fold cross-validation, the proposed model (GCNGAT) achieves AUC (Area Under the receiver operating characteristic Curve) values of 0.9182 and 0.9417 on the PREDICT dataset and CDataset dataset, respectively. Compared with other predictors on the same dataset (PREDICT dataset), GCNGAT outperforms the existing best-performing model (PSGCN), with a 1.58% increase in the AUC value. It is anticipated that this model can provide experimental reference for drug repositioning and further promote the drug research and development process.},
  archive      = {J_ARTMED},
  author       = {Runtao Yang and Yao Fu and Qian Zhang and Lina Zhang},
  doi          = {10.1016/j.artmed.2024.102805},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102805},
  shortjournal = {Artif. Intell. Med.},
  title        = {GCNGAT: Drug–disease association prediction based on graph convolution neural network and graph attention network},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). CSCA u-net: A channel and space compound attention CNN for
medical image segmentation. <em>ARTMED</em>, <em>150</em>, 102800. (<a
href="https://doi.org/10.1016/j.artmed.2024.102800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image segmentation is one of the vital steps in medical image analysis. A large number of methods based on convolutional neural networks have emerged, which can extract abstract features from multiple-modality medical images, learn valuable information that is difficult to recognize by humans, and obtain more reliable results than traditional image segmentation approaches . U-Net, due to its simple structure and excellent performance, is widely used in medical image segmentation . In this paper, to further improve the performance of U-Net, we propose a channel and space compound attention (CSCA) convolutional neural network, CSCA U-Net in abbreviation, which increases the network depth and employs a double squeeze-and-excitation (DSE) block in the bottleneck layer to enhance feature extraction and obtain more high-level semantic features . Moreover, the characteristics of the proposed method are three-fold: (1) channel and space compound attention (CSCA) block, (2) cross-layer feature fusion (CLFF), and (3) deep supervision (DS). Extensive experiments on several available medical image datasets, including Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, ETIS, CVC-T, 2018 Data Science Bowl (2018 DSB), ISIC 2018, and JSUAH-Cerebellum, show that CSCA U-Net achieves competitive results and significantly improves generalization performance . The codes and trained models are available at https://github.com/xiaolanshu/CSCA-U-Net .},
  archive      = {J_ARTMED},
  author       = {Xin Shu and Jiashu Wang and Aoping Zhang and Jinlong Shi and Xiao-Jun Wu},
  doi          = {10.1016/j.artmed.2024.102800},
  journal      = {Artificial Intelligence in Medicine},
  month        = {4},
  pages        = {102800},
  shortjournal = {Artif. Intell. Med.},
  title        = {CSCA U-net: A channel and space compound attention CNN for medical image segmentation},
  volume       = {150},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Automated image label extraction from radiology reports — a
review. <em>ARTMED</em>, <em>149</em>, 102814. (<a
href="https://doi.org/10.1016/j.artmed.2024.102814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning models need large amounts of annotated data for training. In the field of medical imaging , labeled data is especially difficult to obtain because the annotations have to be performed by qualified physicians. Natural Language Processing (NLP) tools can be applied to radiology reports to extract labels for medical images automatically. Compared to manual labeling, this approach requires smaller annotation efforts and can therefore facilitate the creation of labeled medical image data sets. In this article, we summarize the literature on this topic spanning from 2013 to 2023, starting with a meta-analysis of the included articles, followed by a qualitative and quantitative systematization of the results. Overall, we found four types of studies on the extraction of labels from radiology reports: those describing systems based on symbolic NLP, statistical NLP , neural NLP, and those describing systems combining or comparing two or more of the latter. Despite the large variety of existing approaches, there is still room for further improvement. This work can contribute to the development of new techniques or the improvement of existing ones.},
  archive      = {J_ARTMED},
  author       = {Sofia C. Pereira and Ana Maria Mendonça and Aurélio Campilho and Pedro Sousa and Carla Teixeira Lopes},
  doi          = {10.1016/j.artmed.2024.102814},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102814},
  shortjournal = {Artif. Intell. Med.},
  title        = {Automated image label extraction from radiology reports — A review},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detecting mental and physical disorders using multi-task
learning equipped with knowledge graph attention network.
<em>ARTMED</em>, <em>149</em>, 102812. (<a
href="https://doi.org/10.1016/j.artmed.2024.102812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental and physical disorders (MPD) are inextricably linked in many medical cases; psychosomatic diseases can be induced by mental concerns and psychological discomfort can ensue from physiological diseases. However, existing medical informatics studies focus on identifying mental or physical disorders from a unilateral perspective. Consequently, no existing domain knowledge base , corpus, or detection modeling approach considers mental as well as physical aspects concurrently. This paper proposes a joint modeling approach to detect MPD. First, we crawl through online medical consultation records of patients from websites and build an MPD knowledge ontology by extracting the core conceptual features of the text. Based on the ontology, an MPD knowledge graph containing 12,673 nodes and 82,195 relations is obtained using term matching with a domain thesaurus of each concept. Subsequently, an MPD corpus with fine-grained severities ( None , Mild , Moderate , Severe , Dangerous ) and 8909 records is constructed by formulating MPD classification criteria and a data annotation process under the guidance of domain experts. Taking the knowledge graph and corpus as the dataset, we design a multi-task learning model to detect the MPD severity, in which a knowledge graph attention network (KGAT) is embedded to better extract knowledge features. Experiments are performed to demonstrate the effectiveness of our model. Furthermore, we employ ontology-based and centrality-based methods to discover additional potential inferred knowledge, which can be captured by KGAT so as to improve the prediction performance and interpretability of our model. Our dataset has been made publicly available, so it can be further used as a medical informatics reference in the fields of psychosomatic medicine , psychiatrics, physical co-morbidity, and so on.},
  archive      = {J_ARTMED},
  author       = {Wei Zhang and Ling Kong and Soobin Lee and Yan Chen and Guangxu Zhang and Hao Wang and Min Song},
  doi          = {10.1016/j.artmed.2024.102812},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102812},
  shortjournal = {Artif. Intell. Med.},
  title        = {Detecting mental and physical disorders using multi-task learning equipped with knowledge graph attention network},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving deep-learning electrocardiogram classification
with an effective coloring method. <em>ARTMED</em>, <em>149</em>,
102809. (<a href="https://doi.org/10.1016/j.artmed.2024.102809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases, particularly arrhythmias , remain a leading cause of mortality worldwide. Electrocardiogram (ECG) analysis plays a pivotal role in cardiovascular disease diagnosis. Although previous studies have focused on waveform analysis and model training, integrating additional clinical information, especially demographic data, remains challenging. In this study, we present an innovative approach to ECG classification by incorporating demographic information from patients’ medical histories through a colorization technique. Our proposed method maps demographic features onto the (R, G, B) color space through normalized scaling. Each demographic feature corresponds to a distinct color, allowing for different ECG leads to be colored. This approach preserves the relationships between data by maintaining the color correlations in the statistical features, enhancing ECG analytics and supporting precision medicine. We conducted experiments with PTB-XL dataset and achieved 1%–6% improvements in the area under the receiving operator characteristic curve performance compared with other methods for various classification problems. Notably, our method excelled in multiclass and challenging classification tasks . The combined use of color features and the original waveform shape features enhanced prediction accuracy for various deep learning models . Our findings suggest that colorization is a promising avenue for advancing ECG classification and diagnosis, contributing to improved prediction and diagnosis of cardiovascular diseases and ultimately enhancing clinical outcomes.},
  archive      = {J_ARTMED},
  author       = {Wei-Wen Chen and Chien-Chao Tseng and Ching-Chun Huang and Henry Horng-Shing Lu},
  doi          = {10.1016/j.artmed.2024.102809},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102809},
  shortjournal = {Artif. Intell. Med.},
  title        = {Improving deep-learning electrocardiogram classification with an effective coloring method},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Healthcare facilities management: A novel data-driven model
for predictive maintenance of computed tomography equipment.
<em>ARTMED</em>, <em>149</em>, 102807. (<a
href="https://doi.org/10.1016/j.artmed.2024.102807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The breakdown of healthcare facilities is a huge challenge for hospitals. Medical images obtained by Computed Tomography (CT) provide information about the patients&#39; physical conditions and play a critical role in diagnosis of disease. To deliver high-quality medical images on time, it is essential to minimize the occurrence frequencies of anomalies and failures of the equipment. We extracted the real-time CT equipment status time series data such as oil temperature, of three equipment, between May 19, 2020, and May 19, 2021. Tube arcing is treated as the classification label. We propose a dictionary-based data-driven model SAX-HCBOP, where the two methods, Histogram-based Information Gain Binning (HIGB) and Coefficient improved Bag of Pattern (CoBOP), are implemented to transform the data into the bag-of-words paradigm. We compare our model to the existing predictive maintenance models based on statistical and time series classification algorithms. The results show that the Accuracy, Recall, Precision and F1-score of the proposed model achieve 0.904, 0.747, 0.417, 0.535, respectively. The oil temperature is identified as the most important feature. The proposed model is superior to other models in predicting CT equipment anomalies. In addition, experiments on the public dataset also demonstrate the effectiveness of the proposed model. The two proposed methods can improve the performance of the dictionary-based time series classification methods in predictive maintenance. In addition, based on the proposed real-time anomaly prediction system, the model assists hospitals in making accurate healthcare facilities maintenance decisions.},
  archive      = {J_ARTMED},
  author       = {Haopeng Zhou and Qilin Liu and Haowen Liu and Zhu Chen and Zhenlin Li and Yixuan Zhuo and Kang Li and Changxi Wang and Jin Huang},
  doi          = {10.1016/j.artmed.2024.102807},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102807},
  shortjournal = {Artif. Intell. Med.},
  title        = {Healthcare facilities management: A novel data-driven model for predictive maintenance of computed tomography equipment},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Teleconsultation dynamic scheduling with a deep
reinforcement learning approach. <em>ARTMED</em>, <em>149</em>, 102806.
(<a href="https://doi.org/10.1016/j.artmed.2024.102806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the start time of teleconsultations is optimized for the clinical departments of class A tertiary hospitals to improve service quality and efficiency. For this purpose, first, a general teleconsultation scheduling model is formulated. In the formulation, the number of services (NS) is one of the objectives because of demand intermittency and service mobility. Demand intermittency means that demand has zero size in several periods. Service mobility means that specialists move between clinical departments and the National Telemedicine Center of China to provide the service. For problem-solving, the general model is converted into a Markov decision process (MDP) by elaborately defining the state, action, and reward. To solve the MDP, deep reinforcement learning (DRL) is applied to overcome the problem of inaccurate transition probability . To reduce the dimensions of the state–action space, a semi-fixed policy is developed and applied to the deep Q network (DQN) to construct an algorithm of the DQN with a semi-fixed policy (DQN-S). For efficient fitting, an early stop strategy is applied in DQN-S training. To verify the effectiveness of the proposed scheduling model and the model solving method DQN-S, scheduling experiments are carried out based on actual data of teleconsultation demand arrivals and service arrangements. The results show that DQN-S can improve the quality and efficiency of teleconsultations by reducing 9%–41% of the demand average waiting time, 3%–42% of the number of services, and 3%–33% of the total cost of services.},
  archive      = {J_ARTMED},
  author       = {Wenjia Chen and Jinlin Li},
  doi          = {10.1016/j.artmed.2024.102806},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102806},
  shortjournal = {Artif. Intell. Med.},
  title        = {Teleconsultation dynamic scheduling with a deep reinforcement learning approach},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prognostic prediction of sepsis patient using transformer
with skip connected token for tabular data. <em>ARTMED</em>,
<em>149</em>, 102804. (<a
href="https://doi.org/10.1016/j.artmed.2024.102804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is known as a common syndrome in intensive care units (ICU), and severe sepsis and septic shock are among the leading causes of death worldwide. The purpose of this study is to develop a deep learning model that supports clinicians in efficiently managing sepsis patients in the ICU by predicting mortality, ICU length of stay (&gt;14 days), and hospital length of stay (&gt;30 days). The proposed model was developed using 591 retrospective data with 16 tabular data related to a sequential organ failure assessment (SOFA) score. To analyze tabular data, we designed the modified architecture of the transformer that has achieved extraordinary success in the field of languages and computer vision tasks in recent years. The main idea of the proposed model is to use a skip-connected token, which combines both local (feature-wise token) and global (classification token) information as the output of a transformer encoder. The proposed model was compared with four machine learning models (ElasticNet, Extreme Gradient Boosting [XGBoost]), and Random Forest) and three deep learning models (Multi-Layer Perceptron [MLP], transformer, and Feature-Tokenizer transformer [FT-Transformer]) and achieved the best performance (mortality, area under the receiver operating characteristic (AUROC) 0.8047; ICU length of stay, AUROC 0.8314; hospital length of stay, AUROC 0.7342). We anticipate that the proposed model architecture will provide a promising approach to predict the various clinical endpoints using tabular data such as electronic health and medical records.},
  archive      = {J_ARTMED},
  author       = {Jee-Woo Choi and Minuk Yang and Jae-Woo Kim and Yoon Mi Shin and Yong-Goo Shin and Seung Park},
  doi          = {10.1016/j.artmed.2024.102804},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102804},
  shortjournal = {Artif. Intell. Med.},
  title        = {Prognostic prediction of sepsis patient using transformer with skip connected token for tabular data},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). DISCOVER: 2-d multiview summarization of optical coherence
tomography angiography for automatic diabetic retinopathy diagnosis.
<em>ARTMED</em>, <em>149</em>, 102803. (<a
href="https://doi.org/10.1016/j.artmed.2024.102803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic Retinopathy (DR), an ocular complication of diabetes, is a leading cause of blindness worldwide. Traditionally, DR is monitored using Color Fundus Photography (CFP), a widespread 2-D imaging modality. However, DR classifications based on CFP have poor predictive power, resulting in suboptimal DR management. Optical Coherence Tomography Angiography (OCTA) is a recent 3-D imaging modality offering enhanced structural and functional information (blood flow) with a wider field of view. This paper investigates automatic DR severity assessment using 3-D OCTA. A straightforward solution to this task is a 3-D neural network classifier. However, 3-D architectures have numerous parameters and typically require many training samples. A lighter solution consists in using 2-D neural network classifiers processing 2-D en-face (or frontal) projections and/or 2-D cross-sectional slices. Such an approach mimics the way ophthalmologists analyze OCTA acquisitions: (1) en-face flow maps are often used to detect avascular zones and neovascularization , and (2) cross-sectional slices are commonly analyzed to detect macular edemas , for instance. However, arbitrary data reduction or selection might result in information loss. Two complementary strategies are thus proposed to optimally summarize OCTA volumes with 2-D images: (1) a parametric en-face projection optimized through deep learning and (2) a cross-sectional slice selection process controlled through gradient-based attribution. The full summarization and DR classification pipeline is trained from end to end. The automatic 2-D summary can be displayed in a viewer or printed in a report to support the decision. We show that the proposed 2-D summarization and classification pipeline outperforms direct 3-D classification with the advantage of improved interpretability.},
  archive      = {J_ARTMED},
  author       = {Mostafa El Habib Daho and Yihao Li and Rachid Zeghlache and Hugo Le Boité and Pierre Deman and Laurent Borderie and Hugang Ren and Niranchana Mannivanan and Capucine Lepicard and Béatrice Cochener and Aude Couturier and Ramin Tadayoni and Pierre-Henri Conze and Mathieu Lamard and Gwenolé Quellec},
  doi          = {10.1016/j.artmed.2024.102803},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102803},
  shortjournal = {Artif. Intell. Med.},
  title        = {DISCOVER: 2-D multiview summarization of optical coherence tomography angiography for automatic diabetic retinopathy diagnosis},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal self-attention for risk prediction from electronic
health records using non-stationary kernel approximation.
<em>ARTMED</em>, <em>149</em>, 102802. (<a
href="https://doi.org/10.1016/j.artmed.2024.102802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective modeling of patient representation from electronic health records (EHRs) is increasingly becoming a vital research topic . Yet, modeling the non-stationarity in EHR data has received less attention. Most existing studies follow a strong assumption of stationarity in patient representation from EHRs. However, in practice, a patient’s visits are irregularly spaced over a relatively long period of time, and disease progression patterns exhibit non-stationarity. Furthermore, the time gaps between patient visits often encapsulate significant domain knowledge, potentially revealing undiscovered patterns that characterize specific medical conditions . To address these challenges, we introduce a new method which combines the self-attention mechanism with non-stationary kernel approximation to capture both contextual information and temporal relationships between patient visits in EHRs. To assess the effectiveness of our proposed approach, we use two real-world EHR datasets, comprising a total of 76,925 patients, for the task of predicting the next diagnosis code for a patient, given their EHR history. The first dataset is a general EHR cohort and consists of 11,451 patients with a total of 3,485 unique diagnosis codes. The second dataset is a disease-specific cohort that includes 65,474 pregnant patients and encompasses a total of 9,782 unique diagnosis codes. Our experimental evaluation involved nine prediction models, categorized into three distinct groups. Group 1 comprises the baselines: original self-attention with positional encoding model, RETAIN model, and LSTM model . Group 2 includes models employing self-attention with stationary kernel approximations, specifically incorporating three variations of Bochner’s feature maps. Lastly, Group 3 consists of models utilizing self-attention with non-stationary kernel approximations, including quadratic, cubic, and bi-quadratic polynomials. The experimental results demonstrate that non-stationary kernels significantly outperformed baseline methods for NDCG@10 and Hit@10 metrics in both datasets. The performance boost was more substantial in dataset 1 for the NDCG@10 metric. On the other hand, stationary Kernels showed significant but smaller gains over baselines and were nearly as effective as Non-stationary Kernels for Hit@10 in dataset 2. These findings robustly validate the efficacy of employing non-stationary kernels for temporal modeling of EHR data, and emphasize the importance of modeling non-stationary temporal information in healthcare prediction tasks.},
  archive      = {J_ARTMED},
  author       = {Rawan AlSaad and Qutaibah Malluhi and Alaa Abd-alrazaq and Sabri Boughorbel},
  doi          = {10.1016/j.artmed.2024.102802},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102802},
  shortjournal = {Artif. Intell. Med.},
  title        = {Temporal self-attention for risk prediction from electronic health records using non-stationary kernel approximation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Triplet-branch network with contrastive prior-knowledge
embedding for disease grading. <em>ARTMED</em>, <em>149</em>, 102801.
(<a href="https://doi.org/10.1016/j.artmed.2024.102801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since different disease grades require different treatments from physicians, i.e., the low-grade patients may recover with follow-up observations whereas the high-grade may need immediate surgery, the accuracy of disease grading is pivotal in clinical practice. In this paper, we propose a Triplet-Branch Network with ContRastive priOr-knoWledge embeddiNg (TBN-CROWN) for the accurate disease grading, which enables physicians to accordingly take appropriate treatments. Specifically, our TBN-CROWN has three branches, which are implemented for representation learning , classifier learning and grade-related prior-knowledge learning, respectively. The former two branches deal with the issue of class-imbalanced training samples , while the latter one embeds the grade-related prior-knowledge via a novel auxiliary module, termed contrastive embedding module. The proposed auxiliary module takes the features embedded by different branches as input, and accordingly constructs positive and negative embeddings for the model to deploy grade-related prior-knowledge via contrastive learning . Extensive experiments on our private and two publicly available disease grading datasets show that our TBN-CROWN can effectively tackle the class-imbalance problem and yield a satisfactory grading accuracy for various diseases, such as fatigue fracture , ulcerative colitis , and diabetic retinopathy .},
  archive      = {J_ARTMED},
  author       = {Yuexiang Li and Yanping Wang and Guang Lin and Yawen Huang and Jingxin Liu and Yi Lin and Dong Wei and Qirui Zhang and Kai Ma and Zhiqiang Zhang and Guangming Lu and Yefeng Zheng},
  doi          = {10.1016/j.artmed.2024.102801},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102801},
  shortjournal = {Artif. Intell. Med.},
  title        = {Triplet-branch network with contrastive prior-knowledge embedding for disease grading},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel intelligent model for visualized inference of
medical diagnosis: A case of TCM. <em>ARTMED</em>, <em>149</em>, 102799.
(<a href="https://doi.org/10.1016/j.artmed.2024.102799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to present an intelligent model based on known diagnostic knowledge to assist medical diagnosis and display the reasoning process is an interesting issue worth exploring. This study developed a novel intelligent model for visualized inference of medical diagnosis with a case of Traditional Chinese Medicine (TCM). Four classes of TCM&#39;s diagnosis composed of Yin deficiency , Liver Yin deficiency, Kidney Yin deficiency, and Liver-Kidney Yin deficiency were selected as research examples. According to the knowledge of diagnostic points in “Diagnostics of TCM”, a total of 2000 samples for training and testing were randomly generated for the four classes of TCM&#39;s diagnosis. In addition, a total of 60 clinical samples were collected from hospital clinical cases. Training samples were sent to the pre-training language model of Chinese Bert for training to generate intelligent diagnostic module. Simultaneously, a mathematical algorithm was developed to generate inferential digraphs . In order to evaluate the performance of the model, the values of accuracy, F1 score, Mse , Loss and other indicators were calculated for model training and testing. And the confusion matrices and ROC curves were plotted to estimate the predictive ability of the model. The novel model was also compared with RF and XGBOOST. And some instances of inferential digraphs with the model were displayed and analyzed. It may be a new attempt to solve the problem of interpretable and inferential intelligent models in the field of artificial intelligence on medical diagnosis of TCM.},
  archive      = {J_ARTMED},
  author       = {Jiang Qi-yu and Huang Wen-heng and Liang Jia-fen and Sun Xiao-sheng},
  doi          = {10.1016/j.artmed.2024.102799},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102799},
  shortjournal = {Artif. Intell. Med.},
  title        = {A novel intelligent model for visualized inference of medical diagnosis: A case of TCM},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). NPB-REC: A non-parametric bayesian deep-learning approach
for undersampled MRI reconstruction with uncertainty estimation.
<em>ARTMED</em>, <em>149</em>, 102798. (<a
href="https://doi.org/10.1016/j.artmed.2024.102798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to reconstruct high-quality images from undersampled MRI data is vital in improving MRI temporal resolution and reducing acquisition times. Deep learning methods have been proposed for this task, but the lack of verified methods to quantify the uncertainty in the reconstructed images hampered clinical applicability. We introduce “NPB-REC”, a non-parametric fully Bayesian framework , for MRI reconstruction from undersampled data with uncertainty estimation. We use Stochastic Gradient Langevin Dynamics during training to characterize the posterior distribution of the network parameters. This enables us to both improve the quality of the reconstructed images and quantify the uncertainty in the reconstructed images. We demonstrate the efficacy of our approach on a multi-coil MRI dataset from the fastMRI challenge and compare it to the baseline End-to-End Variational Network (E2E-VarNet). Our approach outperforms the baseline in terms of reconstruction accuracy by means of PSNR and SSIM (34.55, 0.908 vs. 33.08, 0.897, p &lt; 0 . 01 p&amp;lt;0.01 , acceleration rate R = 8 R=8 ) and provides uncertainty measures that correlate better with the reconstruction error (Pearson correlation, R = 0 . 94 R=0.94 vs. R = 0 . 91 R=0.91 ). Additionally, our approach exhibits better generalization capabilities against anatomical distribution shifts (PSNR and SSIM of 32.38, 0.849 vs. 31.63, 0.836, p &lt; 0 . 01 p&amp;lt;0.01 , training on brain data, inference on knee data, acceleration rate R = 8 R=8 ). NPB-REC has the potential to facilitate the safe utilization of deep learning-based methods for MRI reconstruction from undersampled data. Code and trained models are available at https://github.com/samahkh/NPB-REC .},
  archive      = {J_ARTMED},
  author       = {Samah Khawaled and Moti Freiman},
  doi          = {10.1016/j.artmed.2024.102798},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102798},
  shortjournal = {Artif. Intell. Med.},
  title        = {NPB-REC: A non-parametric bayesian deep-learning approach for undersampled MRI reconstruction with uncertainty estimation},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable swin transformer network for brain tumor
segmentation from incomplete MRI modalities. <em>ARTMED</em>,
<em>149</em>, 102788. (<a
href="https://doi.org/10.1016/j.artmed.2024.102788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning methods have shown great potential in processing multi-modal Magnetic Resonance Imaging (MRI) data, enabling improved accuracy in brain tumor segmentation. However, the performance of these methods can suffer when dealing with incomplete modalities, which is a common issue in clinical practice. Existing solutions, such as missing modality synthesis, knowledge distillation , and architecture-based methods, suffer from drawbacks such as long training times, high model complexity, and poor scalability. This paper proposes IMS 2 Trans, a novel lightweight scalable Swin Transformer network by utilizing a single encoder to extract latent feature maps from all available modalities. This unified feature extraction process enables efficient information sharing and fusion among the modalities, resulting in efficiency without compromising segmentation performance even in the presence of missing modalities. Two datasets, BraTS 2018 and BraTS 2020, containing incomplete modalities for brain tumor segmentation are evaluated against popular benchmarks. On the BraTS 2018 dataset, our model achieved higher average Dice similarity coefficient (DSC) scores for the whole tumor, tumor core, and enhancing tumor regions (86.57, 75.67, and 58.28, respectively), in comparison with a state-of-the-art model, i.e. mmFormer (86.45, 75.51, and 57.79, respectively). Similarly, on the BraTS 2020 dataset, our model scored higher DSC scores in these three brain tumor regions (87.33, 79.09, and 62.11, respectively) compared to mmFormer (86.17, 78.34, and 60.36, respectively). We also conducted a Wilcoxon test on the experimental results, and the generated p p -value confirmed that our model’s performance was statistically significant. Moreover, our model exhibits significantly reduced complexity with only 4.47 M parameters, 121.89G FLOPs, and a model size of 77.13 MB, whereas mmFormer comprises 34.96 M parameters, 265.79 G FLOPs, and a model size of 559.74 MB. These indicate our model, being light-weighted with significantly reduced parameters, is still able to achieve better performance than a state-of-the-art model. By leveraging a single encoder for processing the available modalities, IMS 2 Trans offers notable scalability advantages over methods that rely on multiple encoders. This streamlined approach eliminates the need for maintaining separate encoders for each modality, resulting in a lightweight and scalable network architecture . The source code of IMS 2 Trans and the associated weights are both publicly available at https://github.com/hudscomdz/IMS2Trans .},
  archive      = {J_ARTMED},
  author       = {Dongsong Zhang and Changjian Wang and Tianhua Chen and Weidao Chen and Yiqing Shen},
  doi          = {10.1016/j.artmed.2024.102788},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102788},
  shortjournal = {Artif. Intell. Med.},
  title        = {Scalable swin transformer network for brain tumor segmentation from incomplete MRI modalities},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling-based joint embedding of histology and genomics
using canonical correlation analysis for breast cancer survival
prediction. <em>ARTMED</em>, <em>149</em>, 102787. (<a
href="https://doi.org/10.1016/j.artmed.2024.102787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional approaches to predicting breast cancer patients’ survival outcomes were based on clinical subgroups, the PAM50 genes, or the histological tissue’s evaluation. With the growth of multi-modality datasets capturing diverse information (such as genomics, histology, radiology and clinical data) about the same cancer, information can be integrated using advanced tools and have improved survival prediction. These methods implicitly exploit the key observation that different modalities originate from the same cancer source and jointly provide a complete picture of the cancer. In this work, we investigate the benefits of explicitly modelling multi-modality data as originating from the same cancer under a probabilistic framework . Specifically, we consider histology and genomics as two modalities originating from the same breast cancer under a probabilistic graphical model (PGM). We construct maximum likelihood estimates of the PGM parameters based on canonical correlation analysis (CCA) and then infer the underlying properties of the cancer patient, such as survival. Equivalently, we construct CCA-based joint embeddings of the two modalities and input them to a learnable predictor. Real-world properties of sparsity and graph-structures are captured in the penalized variants of CCA (pCCA) and are better suited for cancer applications. For generating richer multi-dimensional embeddings with pCCA, we introduce two novel embedding schemes that encourage orthogonality to generate more informative embeddings. The efficacy of our proposed prediction pipeline is first demonstrated via low prediction errors of the hidden variable and the generation of informative embeddings on simulated data. When applied to breast cancer histology and RNA-sequencing expression data from The Cancer Genome Atlas (TCGA), our model can provide survival predictions with average concordance-indices of up to 68.32% along with interpretability. We also illustrate how the pCCA embeddings can be used for survival analysis through Kaplan–Meier curves.},
  archive      = {J_ARTMED},
  author       = {Vaishnavi Subramanian and Tanveer Syeda-Mahmood and Minh N. Do},
  doi          = {10.1016/j.artmed.2024.102787},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102787},
  shortjournal = {Artif. Intell. Med.},
  title        = {Modelling-based joint embedding of histology and genomics using canonical correlation analysis for breast cancer survival prediction},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subspace corrected relevance learning with application in
neuroimaging. <em>ARTMED</em>, <em>149</em>, 102786. (<a
href="https://doi.org/10.1016/j.artmed.2024.102786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In machine learning, data often comes from different sources, but combining them can introduce extraneous variation that affects both generalization and interpretability. For example, we investigate the classification of neurodegenerative diseases using FDG-PET data collected from multiple neuroimaging centers. However, data collected at different centers introduces unwanted variation due to differences in scanners, scanning protocols, and processing methods. To address this issue, we propose a two-step approach to limit the influence of center-dependent variation on the classification of healthy controls and early vs. late-stage Parkinson’s disease patients. First, we train a Generalized Matrix Learning Vector Quantization (GMLVQ) model on healthy control data to identify a “relevance space” that distinguishes between centers. Second, we use this space to construct a correction matrix that restricts a second GMLVQ system’s training on the diagnostic problem. We evaluate the effectiveness of this approach on the real-world multi-center datasets and simulated artificial dataset. Our results demonstrate that the approach produces machine learning systems with reduced bias - being more specific due to eliminating information related to center differences during the training process - and more informative relevance profiles that can be interpreted by medical experts. This method can be adapted to similar problems outside the neuroimaging domain, as long as an appropriate “relevance space” can be identified to construct the correction matrix.},
  archive      = {J_ARTMED},
  author       = {Rick van Veen and Neha Rajendra Bari Tamboli and Sofie Lövdal and Sanne K. Meles and Remco J. Renken and Gert-Jan de Vries and Dario Arnaldi and Silvia Morbelli and Pedro Clavero and José A. Obeso and Maria C. Rodriguez Oroz and Klaus L. Leenders and Thomas Villmann and Michael Biehl},
  doi          = {10.1016/j.artmed.2024.102786},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102786},
  shortjournal = {Artif. Intell. Med.},
  title        = {Subspace corrected relevance learning with application in neuroimaging},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Development and validation of a deep interpretable network
for continuous acute kidney injury prediction in critically ill
patients. <em>ARTMED</em>, <em>149</em>, 102785. (<a
href="https://doi.org/10.1016/j.artmed.2024.102785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of acute kidney injury (AKI) may provide a crucial window of opportunity to prevent further injury , which helps improve clinical outcomes. This study aimed to develop a deep interpretable network for continuously predicting the 24-hour AKI risk in real-time and evaluate its performance internally and externally in critically ill patients. A total of 21,163 patients&#39; electronic health records sourced from Beth Israel Deaconess Medical Center (BIDMC) were first included in building the model. Two external validation populations included 3025 patients from the Philips eICU Research Institute and 2625 patients from Zhongda Hospital Southeast University. A total of 152 intelligently engineered predictors were extracted on an hourly basis. The prediction model referred to as DeepAKI was designed with the basic framework of squeeze-and-excitation networks with dilated causal convolution embedded. The integrated gradients method was utilized to explain the prediction model. When performed on the internal validation set (3175 [15 %] patients from BIDMC) and the two external validation sets, DeepAKI obtained the area under the curve of 0.799 (95 % CI 0.791–0.806), 0.763 (95 % CI 0.755–0.771) and 0.676 (95 % CI 0.668–0.684) for continuousAKI prediction, respectively. For model interpretability, clinically relevant important variables contributing to the model prediction were informed, and individual explanations along the timeline were explored to show how AKI risk arose. The potential threats to generalisability in deep learning-based models when deployed across health systems in real-world settings were analyzed.},
  archive      = {J_ARTMED},
  author       = {Meicheng Yang and Songqiao Liu and Tong Hao and Caiyun Ma and Hui Chen and Yuwen Li and Changde Wu and Jianfeng Xie and Haibo Qiu and Jianqing Li and Yi Yang and Chengyu Liu},
  doi          = {10.1016/j.artmed.2024.102785},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102785},
  shortjournal = {Artif. Intell. Med.},
  title        = {Development and validation of a deep interpretable network for continuous acute kidney injury prediction in critically ill patients},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multilevel bayesian network to model child morbidity using
gibbs sampling. <em>ARTMED</em>, <em>149</em>, 102784. (<a
href="https://doi.org/10.1016/j.artmed.2024.102784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks (BNs) are suitable models for studying complex interdependencies between multiple health outcomes, simultaneously. However, these models fail the assumption of independent observation in the case of hierarchical data. Therefore, this study proposes a two and three-level random intercept multilevel Bayesian network (MBN) models to study the conditional dependencies between multiple outcomes. The structure of MBN was learned using the connected three parent set block Gibbs sampler, where each local network was included based on Bayesian information criteria (BIC) score of multilevel regression. These models were examined using simulated data assuming features of both multilevel models and BNs. The estimated area under the receiver operating characteristics for both models were above 0.8, indicating good fit. The MBN was then applied to real child morbidity data from the 2016 Ethiopian Demographic Health Survey (EDHS). The result shows a complex causal dependencies between malnutrition indicators and child morbidities such as anemia , acute respiratory infection (ARI) and diarrhea. According to this result, families and health professionals should give special attention to children who suffer from malnutrition and also have one of these illnesses, as the co-occurrence of both can worsen the health of a child.},
  archive      = {J_ARTMED},
  author       = {Bezalem Eshetu Yirdaw and Legesse Kassa Debusho},
  doi          = {10.1016/j.artmed.2024.102784},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102784},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multilevel bayesian network to model child morbidity using gibbs sampling},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An interpretable dual attention network for diabetic
retinopathy grading: IDANet. <em>ARTMED</em>, <em>149</em>, 102782. (<a
href="https://doi.org/10.1016/j.artmed.2024.102782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetic retinopathy (DR) is the most prevalent cause of visual impairment in adults worldwide. Typically, patients with DR do not show symptoms until later stages, by which time it may be too late to receive effective treatment. DR Grading is challenging because of the small size and variation in lesion patterns. The key to fine-grained DR grading is to discover more discriminating elements such as cotton wool, hard exudates, hemorrhages , microaneurysms etc. Although deep learning models like convolutional neural networks (CNN) seem ideal for the automated detection of abnormalities in advanced clinical imaging, small-size lesions are very hard to distinguish by using traditional networks. This work proposes a bi-directional spatial and channel-wise parallel attention based network to learn discriminative features for diabetic retinopathy grading. The proposed attention block plugged with a backbone network helps to extract features specific to fine-grained DR-grading. This scheme boosts classification performance along with the detection of small-sized lesion parts. Extensive experiments are performed on four widely used benchmark datasets for DR grading, and performance is evaluated on different quality metrics. Also, for model interpretability , activation maps are generated using the LIME method to visualize the predicted lesion parts. In comparison with state-of-the-art methods, the proposed IDANet exhibits better performance for DR grading and lesion detection.},
  archive      = {J_ARTMED},
  author       = {Amit Bhati and Neha Gour and Pritee Khanna and Aparajita Ojha and Naoufel Werghi},
  doi          = {10.1016/j.artmed.2024.102782},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102782},
  shortjournal = {Artif. Intell. Med.},
  title        = {An interpretable dual attention network for diabetic retinopathy grading: IDANet},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Designing explainable AI to improve human-AI team
performance: A medical stakeholder-driven scoping review.
<em>ARTMED</em>, <em>149</em>, 102780. (<a
href="https://doi.org/10.1016/j.artmed.2024.102780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of complex AI systems in healthcare and other sectors has led to a growing area of research called Explainable AI (XAI) designed to increase transparency. In this area, quantitative and qualitative studies focus on improving user trust and task performance by providing system- and prediction-level XAI features. We analyze stakeholder engagement events (interviews and workshops) on the use of AI for kidney transplantation . From this we identify themes which we use to frame a scoping literature review on current XAI features. The stakeholder engagement process lasted over nine months covering three stakeholder group&#39;s workflows, determining where AI could intervene and assessing a mock XAI decision support system. Based on the stakeholder engagement , we identify four major themes relevant to designing XAI systems – 1) use of AI predictions, 2) information included in AI predictions, 3) personalization of AI predictions for individual differences, and 4) customizing AI predictions for specific cases. Using these themes, our scoping literature review finds that providing AI predictions before, during, or after decision-making could be beneficial depending on the complexity of the stakeholder&#39;s task. Additionally, expert stakeholders like surgeons prefer minimal to no XAI features, AI prediction, and uncertainty estimates for easy use cases. However, almost all stakeholders prefer to have optional XAI features to review when needed, especially in hard-to-predict cases. The literature also suggests that providing both system- and prediction-level information is necessary to build the user&#39;s mental model of the system appropriately. Although XAI features improve users&#39; trust in the system, human-AI team performance is not always enhanced. Overall, stakeholders prefer to have agency over the XAI interface to control the level of information based on their needs and task complexity. We conclude with suggestions for future research, especially on customizing XAI features based on preferences and tasks.},
  archive      = {J_ARTMED},
  author       = {Harishankar V. Subramanian and Casey Canfield and Daniel B. Shank},
  doi          = {10.1016/j.artmed.2024.102780},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102780},
  shortjournal = {Artif. Intell. Med.},
  title        = {Designing explainable AI to improve human-AI team performance: A medical stakeholder-driven scoping review},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Opportunities and challenges of artificial intelligence and
distributed systems to improve the quality of healthcare service.
<em>ARTMED</em>, <em>149</em>, 102779. (<a
href="https://doi.org/10.1016/j.artmed.2024.102779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The healthcare sector, characterized by vast datasets and many diseases, is pivotal in shaping community health and overall quality of life . Traditional healthcare methods, often characterized by limitations in disease prevention, predominantly react to illnesses after their onset rather than proactively averting them. The advent of Artificial Intelligence (AI) has ushered in a wave of transformative applications designed to enhance healthcare services , with Machine Learning (ML) as a noteworthy subset of AI. ML empowers computers to analyze extensive datasets, while Deep Learning (DL), a specific ML methodology, excels at extracting meaningful patterns from these data troves. Despite notable technological advancements in recent years, the full potential of these applications within medical contexts remains largely untapped, primarily due to the medical community&#39;s cautious stance toward novel technologies. The motivation of this paper lies in recognizing the pivotal role of the healthcare sector in community well-being and the necessity for a shift toward proactive healthcare approaches. To our knowledge, there is a notable absence of a comprehensive published review that delves into ML, DL and distributed systems, all aimed at elevating the Quality of Service (QoS) in healthcare. This study seeks to bridge this gap by presenting a systematic and organized review of prevailing ML, DL, and distributed system algorithms as applied in healthcare settings. Within our work, we outline key challenges that both current and future developers may encounter, with a particular focus on aspects such as approach, data utilization, strategy, and development processes . Our study findings reveal that the Internet of Things (IoT) stands out as the most frequently utilized platform (44.3 %), with disease diagnosis emerging as the predominant healthcare application (47.8 %). Notably, discussions center significantly on the prevention and identification of cardiovascular diseases (29.2 %). The studies under examination employ a diverse range of ML and DL methods, along with distributed systems, with Convolutional Neural Networks (CNNs) being the most commonly used (16.7 %), followed by Long Short-Term Memory (LSTM) networks (14.6 %) and shallow learning networks (12.5 %). In evaluating QoS, the predominant emphasis revolves around the accuracy parameter (80 %). This study highlights how ML, DL, and distributed systems reshape healthcare. It contributes to advancing healthcare quality , bridging the gap between technology and medical adoption, and benefiting practitioners and patients.},
  archive      = {J_ARTMED},
  author       = {Sarina Aminizadeh and Arash Heidari and Mahshid Dehghan and Shiva Toumaj and Mahsa Rezaei and Nima Jafari Navimipour and Fabio Stroppa and Mehmet Unal},
  doi          = {10.1016/j.artmed.2024.102779},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102779},
  shortjournal = {Artif. Intell. Med.},
  title        = {Opportunities and challenges of artificial intelligence and distributed systems to improve the quality of healthcare service},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SSLDTI: A novel method for drug-target interaction
prediction based on self-supervised learning. <em>ARTMED</em>,
<em>149</em>, 102778. (<a
href="https://doi.org/10.1016/j.artmed.2024.102778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computational methods have been proposed to identify potential drug-target interactions (DTIs) to expedite drug development . Graph neural network (GNN) methods are considered to be one of the most effective approaches. However, shallow GNN methods can only aggregate local information from nodes. Also, deep GNN methods may result in over-smoothing while obtaining long-distance neighbourhood information. As a result, existing GNN methods struggle to extract the complete features of the graph. Additionally, the number of known DTIs is insufficient, and there are far more unknown drug-target pairs than known DTIs, leading to class imbalance . This article proposes a model that combines graph autoencoder and self-supervised learning to accurately encode multilevel features of graphs using only a small number of labelled samples. We introduce a positive sample compensation coefficient to the objective function to mitigate the impact of class imbalance. Experiments on two datasets demonstrated that our model outperforms the four baseline methods , and the new DTIs predicted by the SSLDTI model were verified by the DrugBank database.},
  archive      = {J_ARTMED},
  author       = {Zhixian Liu and Qingfeng Chen and Wei Lan and Huihui Lu and Shichao Zhang},
  doi          = {10.1016/j.artmed.2024.102778},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102778},
  shortjournal = {Artif. Intell. Med.},
  title        = {SSLDTI: A novel method for drug-target interaction prediction based on self-supervised learning},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrated block-wise neural network with auto-learning
search framework for finger gesture recognition using sEMG signals.
<em>ARTMED</em>, <em>149</em>, 102777. (<a
href="https://doi.org/10.1016/j.artmed.2024.102777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate finger gesture recognition with surface electromyography (sEMG) is essential and long-challenge in the muscle-computer interface, and many high-performance deep learning models have been developed to predict gestures. For these models, problem-specific tuning of network architecture is essential for improving the performance, yet it requires substantial knowledge of network architecture design and commitment of time and effort. This process thus imposes a major obstacle to the widespread and flexible application of modern deep learning . To address this issue, we present an auto-learning search framework (ALSF) to generate the integrated block-wised neural network (IBWNN) for sEMG-based gesture recognition. IBWNN contains several feature extraction blocks and dimensional reduction layers, and each feature extraction block integrates two sub-blocks (i.e., multi-branch convolutional block and triplet attention block). Meanwhile, ALSF generates optimal models for gesture recognition through the reinforcement learning method. The results show that the generated models yield state-of-the-art results compared to the modern popular networks on the open dataset Ninapro DB5. Moreover, compared to other networks, the generated models have fewer parameters and can be deployed in practical applications with less resource consumption.},
  archive      = {J_ARTMED},
  author       = {Shurun Wang and Hao Tang and Feng Chen and Qi Tan and Qi Jiang},
  doi          = {10.1016/j.artmed.2024.102777},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102777},
  shortjournal = {Artif. Intell. Med.},
  title        = {Integrated block-wise neural network with auto-learning search framework for finger gesture recognition using sEMG signals},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi input–multi output 3D CNN for dementia severity
assessment with incomplete multimodal data. <em>ARTMED</em>,
<em>149</em>, 102774. (<a
href="https://doi.org/10.1016/j.artmed.2024.102774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s Disease is the most common cause of dementia, whose progression spans in different stages, from very mild cognitive impairment to mild and severe conditions. In clinical trials , Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) are mostly used for the early diagnosis of neurodegenerative disorders since they provide volumetric and metabolic function information of the brain , respectively. In recent years, Deep Learning (DL) has been employed in medical imaging with promising results. Moreover, the use of the deep neural networks , especially Convolutional Neural Networks (CNNs), has also enabled the development of DL-based solutions in domains characterized by the need of leveraging information coming from multiple data sources , raising the Multimodal Deep Learning (MDL). In this paper, we conduct a systematic analysis of MDL approaches for dementia severity assessment exploiting MRI and PET scans. We propose a Multi Input–Multi Output 3D CNN whose training iterations change according to the characteristic of the input as it is able to handle incomplete acquisitions, in which one image modality is missed. Experiments performed on OASIS-3 dataset show the satisfactory results of the implemented network, which outperforms approaches exploiting both single image modality and different MDL fusion techniques.},
  archive      = {J_ARTMED},
  author       = {Michela Gravina and Angel García-Pedrero and Consuelo Gonzalo-Martín and Carlo Sansone and Paolo Soda},
  doi          = {10.1016/j.artmed.2024.102774},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102774},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multi input–Multi output 3D CNN for dementia severity assessment with incomplete multimodal data},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A clinical consensus-compliant deep learning approach to
quantitatively evaluate human in vitro fertilization early embryonic
development with optical microscope images. <em>ARTMED</em>,
<em>149</em>, 102773. (<a
href="https://doi.org/10.1016/j.artmed.2024.102773">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of embryos is a key for the success of in vitro fertilization (IVF). However, automatic quality assessment on human IVF embryos with optical microscope images is still challenging. In this study, we developed a clinical consensus-compliant deep learning approach, named Esava (Embryo Segmentation and Viability Assessment), to quantitatively evaluate the development of IVF embryos using optical microscope images. In total 551 optical microscope images of human IVF embryos of day-2 to day-3 were collected, preprocessed, and annotated. Using the Faster R-CNN model as baseline, our Esava model was constructed, refined, trained, and validated for precise and robust blastomere detection. A novel algorithm Crowd-NMS was proposed and employed in Esava to enhance the object detection and to precisely quantify the embryonic cells and their size uniformity. Additionally, an innovative GrabCut-based unsupervised module was integrated for the segmentation of blastomeres and embryos. Independently tested on 94 embryo images for blastomere detection, Esava obtained the high rates of 0.9940, 0.9121, and 0.9531 for precision, recall, and mAP respectively, and gained significant advances compared with previous computational methods. Intraclass correlation coefficients indicated the consistency between Esava and three experienced embryologists. Another test on 51 extra images demonstrated that Esava surpassed other tools significantly, achieving the highest average precision 0.9025. Moreover, it also accurately identified the borders of blastomeres with mIoU over 0.88 on the independent testing dataset . Esava is compliant with the Istanbul clinical consensus and compatible to senior embryologists. Taken together, Esava improves the accuracy and efficiency of embryonic development assessment with optical microscope images.},
  archive      = {J_ARTMED},
  author       = {Zaowen Liao and Chaoyu Yan and Jianbo Wang and Ningfeng Zhang and Huan Yang and Chenghao Lin and Haiyue Zhang and Wenjun Wang and Weizhong Li},
  doi          = {10.1016/j.artmed.2024.102773},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102773},
  shortjournal = {Artif. Intell. Med.},
  title        = {A clinical consensus-compliant deep learning approach to quantitatively evaluate human in vitro fertilization early embryonic development with optical microscope images},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A clinically actionable and explainable real-time risk
assessment framework for stroke-associated pneumonia. <em>ARTMED</em>,
<em>149</em>, 102772. (<a
href="https://doi.org/10.1016/j.artmed.2024.102772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current medical practice is more responsive rather than proactive, despite the widely recognized value of early disease detection, including improving the quality of care and reducing medical costs. One of the cornerstones of early disease detection is clinically actionable predictions, where predictions are expected to be accurate, stable, real-time and interpretable. As an example, we used stroke-associated pneumonia (SAP), setting up a transformer-encoder-based model that analyzes highly heterogeneous electronic health records in real-time. The model was proven accurate and stable on an independent test set. In addition, it issued at least one warning for 98.6 % of SAP patients, and on average, its alerts were ahead of physician diagnoses by 2.71 days. We applied Integrated Gradient to glean the model&#39;s reasoning process. Supplementing the risk scores, the model highlighted critical historical events on patients&#39; trajectories, which were shown to have high clinical relevance.},
  archive      = {J_ARTMED},
  author       = {Lutao Dai and Xin Yang and Hao Li and Xingquan Zhao and Lin Lin and Yong Jiang and Yongjun Wang and Zixiao Li and Haipeng Shen},
  doi          = {10.1016/j.artmed.2024.102772},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102772},
  shortjournal = {Artif. Intell. Med.},
  title        = {A clinically actionable and explainable real-time risk assessment framework for stroke-associated pneumonia},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessment and treatment of visuospatial neglect using
active learning with gaussian processes regression. <em>ARTMED</em>,
<em>149</em>, 102770. (<a
href="https://doi.org/10.1016/j.artmed.2024.102770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visuospatial neglect is a disorder characterised by impaired awareness for visual stimuli located in regions of space and frames of reference. It is often associated with stroke. Patients can struggle with all aspects of daily living and community participation. Assessment methods are limited and show several shortcomings, considering they are mainly performed on paper and do not implement the complexity of daily life. Similarly, treatment options are sparse and often show only small improvements. We present an artificial intelligence solution designed to accurately assess a patient’s visuospatial neglect in a three-dimensional setting. We implement an active learning method based on Gaussian process regression to reduce the effort it takes a patient to undergo an assessment. Furthermore, we describe how this model can be utilised in patient oriented treatment and how this opens the way to gamification , tele-rehabilitation and personalised healthcare, providing a promising avenue for improving patient engagement and rehabilitation outcomes. To validate our assessment module, we conducted clinical trials involving patients in a real-world setting. We compared the results obtained using our AI-based assessment with the widely used conventional visuospatial neglect tests currently employed in clinical practice. The validation process serves to establish the accuracy and reliability of our model, confirming its potential as a valuable tool for diagnosing and monitoring visuospatial neglect. Our VR application proves to be more sensitive, while intra-rater reliability remains high.},
  archive      = {J_ARTMED},
  author       = {Ivan De Boi and Elissa Embrechts and Quirine Schatteman and Rudi Penne and Steven Truijen and Wim Saeys},
  doi          = {10.1016/j.artmed.2024.102770},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102770},
  shortjournal = {Artif. Intell. Med.},
  title        = {Assessment and treatment of visuospatial neglect using active learning with gaussian processes regression},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI in medical diagnosis: AI prediction &amp; human judgment.
<em>ARTMED</em>, <em>149</em>, 102769. (<a
href="https://doi.org/10.1016/j.artmed.2024.102769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI has long been regarded as a panacea for decision-making and many other aspects of knowledge work; as something that will help humans get rid of their shortcomings. We believe that AI can be a useful asset to support decision-makers, but not that it should replace decision-makers. Decision-making uses algorithmic analysis, but it is not solely algorithmic analysis; it also involves other factors, many of which are very human, such as creativity, intuition, emotions, feelings, and value judgments. We have conducted semi-structured open-ended research interviews with 17 dermatologists to understand what they expect from an AI application to deliver to medical diagnosis. We have found four aggregate dimensions along which the thinking of dermatologists can be described: the ways in which our participants chose to interact with AI, responsibility, ‘explainability’, and the new way of thinking (mindset) needed for working with AI. We believe that our findings will help physicians who might consider using AI in their diagnosis to understand how to use AI beneficially. It will also be useful for AI vendors in improving their understanding of how medics want to use AI in diagnosis. Further research will be needed to examine if our findings have relevance in the wider medical field and beyond.},
  archive      = {J_ARTMED},
  author       = {Dóra Göndöcs and Viktor Dörfler},
  doi          = {10.1016/j.artmed.2024.102769},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102769},
  shortjournal = {Artif. Intell. Med.},
  title        = {AI in medical diagnosis: AI prediction &amp; human judgment},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum to “DeepGA for automatically estimating fetal
gestational age through ultrasound imaging” [artif. Intell. Med. 135
(2023) 102453]. <em>ARTMED</em>, <em>149</em>, 102768. (<a
href="https://doi.org/10.1016/j.artmed.2024.102768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_ARTMED},
  author       = {Tingting Dan and Xijie Chen and Miao He and Hongmei Guo and Xiaoqin He and Jiazhou Chen and Jianbo Xian and Yu Hu and Bin Zhang and Nan Wang and Hongning Xie and Hongmin Cai},
  doi          = {10.1016/j.artmed.2024.102768},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102768},
  shortjournal = {Artif. Intell. Med.},
  title        = {Corrigendum to “DeepGA for automatically estimating fetal gestational age through ultrasound imaging” [Artif. intell. med. 135 (2023) 102453]},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding neural correlates of depersonalisation/derealisation
disorder via explainable CNN-based analysis guided by clinical
assessment scores. <em>ARTMED</em>, <em>149</em>, 102755. (<a
href="https://doi.org/10.1016/j.artmed.2023.102755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental health disorders are typically diagnosed based on subjective reports (e.g., through questionnaires) followed by clinical interviews to evaluate the self-reported symptoms. Therefore, considering the interconnected nature of psychiatric disorders, their accurate diagnosis is a real challenge without indicators of underlying physiological dysfunction. Depersonalisation/derealisation disorder (DPD) is an example of dissociative disorder affecting 1–2 % of the population. DPD is characterised mainly by persistent disembodiment, detachment from surroundings, and feelings of emotional numbness, which can significantly impact patients&#39; quality of life . The underlying neural correlates of DPD have been investigated for years to understand and help with a more accurate and in-time diagnosis of the disorder. However, in terms of EEG studies, which hold great importance due to their convenient and inexpensive nature, the literature has often been based on hypotheses proposed by experts in the field, which require prior knowledge of the disorder. In addition, participants&#39; labelling in research experiments is often derived from the outcome of the Cambridge Depersonalisation Scale (CDS), a subjective assessment to quantify the level of depersonalisation/derealisation, the threshold and reliability of which might be challenged. As a result, we aimed to propose a novel end-to-end EEG processing pipeline based on deep neural networks for DPD biomarker discovery , which requires no prior handcrafted labelled data. Alternatively, it can assimilate knowledge from clinical outcomes like CDS as well as data-driven patterns that differentiate individual brain responses. In addition, the structure of the proposed model targets the uncertainty in CDS scores by using them as prior information only to guide the unsupervised learning task in a multi-task learning scenario. A comprehensive evaluation has been done to confirm the significance of the proposed deep structure, including new ways of network visualisation to investigate spectral, spatial, and temporal information derived in the learning process. We argued that the proposed EEG analytics could also be applied to investigate other psychological and mental disorders currently indicated on the basis of clinical assessment scores. The code to reproduce the results presented in this paper is openly accessible at https://github.com/AbbasSalami/DPD_Analysis .},
  archive      = {J_ARTMED},
  author       = {Abbas Salami and Javier Andreu-Perez and Helge Gillmeister},
  doi          = {10.1016/j.artmed.2023.102755},
  journal      = {Artificial Intelligence in Medicine},
  month        = {3},
  pages        = {102755},
  shortjournal = {Artif. Intell. Med.},
  title        = {Finding neural correlates of depersonalisation/derealisation disorder via explainable CNN-based analysis guided by clinical assessment scores},
  volume       = {149},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction on nature of cancer by fuzzy graphoidal covering
number using artificial neural network. <em>ARTMED</em>, <em>148</em>,
102783. (<a href="https://doi.org/10.1016/j.artmed.2024.102783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the chances of various types of cancers for different organs in the human body is a typical decision-making process in medicine and health. The signaling pathways have played a vital role in increasing or decreasing the possibility of the deadliest disease, cancer. To combine the pathways concept and ambiguity in the prediction techniques of such diseases, we have used the proposed research on fuzzy graphoidal covers of fuzzy graphs in this paper. Determining a path with uncertainty and shortest length is a challenging topic of graph theory, and a collection of such shortest paths maintaining specific conditions is defined as a fuzzy graphoidal cover for a fuzzy graph. Also, we have defined fuzzy graphoidal covering number as a new parameter, reflecting the measure of coverage by fuzzy graphoidal covering set in a system. Afterwards, some important characterizations of the fuzzy graphoidal covering number are established with justified proof. Also, specific limit values of this number are provided for particular cases. Then, we developed an efficient algorithm for finding the defined covering set with its space and time complexity. The findings of this proposed study have been composed with an artificial neural network to model a strong tool for resolving an essential issue of medical sciences, the prediction of cancer type in the human body. We have analyzed two types of neural networks such as one one-dimensional and two-dimensional specification, for clarity of the obtained results. Also, we have found out the most possible cancer type is breast cancer from the data of our considered case study as a concluding statement for any decision-maker in the field of health sciences. Finally, sensitivity analysis and comparative study have been done to show the stability of our proposed work.},
  archive      = {J_ARTMED},
  author       = {Anushree Bhattacharya and Madhumangal Pal},
  doi          = {10.1016/j.artmed.2024.102783},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102783},
  shortjournal = {Artif. Intell. Med.},
  title        = {Prediction on nature of cancer by fuzzy graphoidal covering number using artificial neural network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The concordance index decomposition: A measure for a deeper
understanding of survival prediction models. <em>ARTMED</em>,
<em>148</em>, 102781. (<a
href="https://doi.org/10.1016/j.artmed.2024.102781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Concordance Index (C-index) is a commonly used metric in Survival Analysis for evaluating the performance of a prediction model. In this paper, we propose a decomposition of the C-index into a weighted harmonic mean of two quantities: one for ranking observed events versus other observed events, and the other for ranking observed events versus censored cases. This decomposition enables a finer-grained analysis of the relative strengths and weaknesses between different survival prediction methods. The usefulness of this decomposition is demonstrated through benchmark comparisons against classical models and state-of-the-art methods, together with the new variational generative neural-network-based method (SurVED) proposed in this paper. The performance of the models is assessed using four publicly available datasets with varying levels of censoring. Using the C-index decomposition and synthetic censoring, the analysis shows that deep learning models utilize the observed events more effectively than other models. This allows them to keep a stable C-index in different censoring levels. In contrast to such deep learning methods, classical machine learning models deteriorate when the censoring level decreases due to their inability to improve on ranking the events versus other events.},
  archive      = {J_ARTMED},
  author       = {Abdallah Alabdallah and Mattias Ohlsson and Sepideh Pashami and Thorsteinn Rögnvaldsson},
  doi          = {10.1016/j.artmed.2024.102781},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102781},
  shortjournal = {Artif. Intell. Med.},
  title        = {The concordance index decomposition: A measure for a deeper understanding of survival prediction models},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A deep convolutional neural network for the automatic
segmentation of glioblastoma brain tumor: Joint spatial pyramid module
and attention mechanism network. <em>ARTMED</em>, <em>148</em>, 102776.
(<a href="https://doi.org/10.1016/j.artmed.2024.102776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a deep convolutional neural network for the automatic segmentation of glioblastoma brain tumors, aiming sat replacing the manual segmentation method that is both time-consuming and labor-intensive. There are many challenges for automatic segmentation to finely segment sub-regions from multi-sequence magnetic resonance images because of the complexity and variability of glioblastomas, such as the loss of boundary information, misclassified regions, and subregion size. To overcome these challenges, this study introduces a spatial pyramid module and attention mechanism to the automatic segmentation algorithm , which focuses on multi-scale spatial details and context information. The proposed method has been tested in the public benchmarks BraTS 2018, BraTS 2019, BraTS 2020 and BraTS 2021 datasets. The Dice score on the enhanced tumor, whole tumor, and tumor core were respectively 79.90 %, 89.63 %, and 85.89 % on the BraTS 2018 dataset, respectively 77.14 %, 89.58 %, and 83.33 % on the BraTS 2019 dataset, and respectively 77.80 %, 90.04 %, and 83.18 % on the BraTS 2020 dataset, and respectively 83.48 %, 90.70 %, and 88.94 % on the BraTS 2021 dataset offering performance on par with that of state-of-the-art methods with only 1.90 M parameters. In addition, our approach significantly reduced the requirements for experimental equipment, and the average time taken to segment one case was only 1.48 s; these two benefits rendered the proposed network intensely competitive for clinical practice.},
  archive      = {J_ARTMED},
  author       = {Hengxin Liu and Jingteng Huang and Qiang Li and Xin Guan and Minglang Tseng},
  doi          = {10.1016/j.artmed.2024.102776},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102776},
  shortjournal = {Artif. Intell. Med.},
  title        = {A deep convolutional neural network for the automatic segmentation of glioblastoma brain tumor: Joint spatial pyramid module and attention mechanism network},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). HoRDA: Learning higher-order structure information for
predicting RNA–disease associations. <em>ARTMED</em>, <em>148</em>,
102775. (<a href="https://doi.org/10.1016/j.artmed.2024.102775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CircRNA and miRNA are crucial non-coding RNAs , which are associated with biological diseases. Exploring the associations between RNAs and diseases often requires a significant time and financial investments, which has been greatly alleviated and improved with the application of deep learning methods in bioinformatics. However, existing methods often fail to achieve higher accuracy and cannot be universal between multiple RNAs. Moreover, complex RNA–disease associations hide important higher-order topology information. To address these issues, we learn higher-order structure information for predicting RNA–disease associations (HoRDA). Firstly, the correlations between RNAs and the correlations between diseases are fully explored by combining similarity and higher-order graph attention network. Then, a higher-order graph convolutional network is constructed to aggregate neighbor information, and further obtain the representations of RNAs and diseases. Meanwhile, due to the large number of complex and variable higher-order structures in biological networks, we design a higher-order negative sampling strategy to gain more desirable negative samples. Finally, the obtained embeddings of RNAs and diseases are feed into logistic regression model to acquire the probabilities of RNA–disease associations. Diverse simulation results demonstrate the superiority of the proposed method. In the end, the case study is conducted on breast neoplasms, colorectal neoplasms, and gastric neoplasms. We validate the proposed higher-order strategies through ablative and exploratory analyses and further demonstrate the practical applicability of HoRDA. HoRDA has a certain contribution in RNA–disease association prediction.},
  archive      = {J_ARTMED},
  author       = {Julong Li and Jianrui Chen and Zhihui Wang and Xiujuan Lei},
  doi          = {10.1016/j.artmed.2024.102775},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102775},
  shortjournal = {Artif. Intell. Med.},
  title        = {HoRDA: Learning higher-order structure information for predicting RNA–disease associations},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MSEF-net: Multi-scale edge fusion network for lumbosacral
plexus segmentation with MR image. <em>ARTMED</em>, <em>148</em>,
102771. (<a href="https://doi.org/10.1016/j.artmed.2024.102771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nerve damage of spine areas is a common cause of disability and paralysis. The lumbosacral plexus segmentation from magnetic resonance imaging (MRI) scans plays an important role in many computer-aided diagnoses and surgery of spinal nerve lesions. Due to the complex structure and low contrast of the lumbosacral plexus, it is difficult to delineate the regions of edges accurately. To address this issue, we propose a Multi-Scale Edge Fusion Network (MSEF-Net) to fully enhance the edge feature in the encoder and adaptively fuse multi-scale features in the decoder. Specifically, to highlight the edge structure feature, we propose an edge feature fusion module (EFFM) by combining the Sobel operator edge detection and the edge-guided attention module (EAM), respectively. To adaptively fuse the multi-scale feature map in the decoder, we introduce an adaptive multi-scale fusion module (AMSF). Our proposed MSEF-Net method was evaluated on the collected spinal MRI dataset with 89 patients (a total of 2848 MR images). Experimental results demonstrate that our MSEF-Net is effective for lumbosacral plexus segmentation with MR images, when compared with several state-of-the-art segmentation methods .},
  archive      = {J_ARTMED},
  author       = {Junyong Zhao and Liang Sun and Zhi Sun and Xin Zhou and Haipeng Si and Daoqiang Zhang},
  doi          = {10.1016/j.artmed.2024.102771},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102771},
  shortjournal = {Artif. Intell. Med.},
  title        = {MSEF-net: Multi-scale edge fusion network for lumbosacral plexus segmentation with MR image},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stacked deep learning approach for efficient SARS-CoV-2
detection in blood samples. <em>ARTMED</em>, <em>148</em>, 102767. (<a
href="https://doi.org/10.1016/j.artmed.2024.102767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying COVID-19 through blood sample analysis is crucial in managing the disease and improving patient outcomes. Despite its advantages, the current test demands certified laboratories, expensive equipment, trained personnel, and 3–4 h for results, with a notable false-negative rate of 15%–20%. This study proposes a stacked deep-learning approach for detecting COVID-19 in blood samples to distinguish uninfected individuals from those infected with the virus. Three stacked deep learning architectures, namely the StackMean, StackMax, and StackRF algorithms, are introduced to improve the detection quality of single deep learning models. To counter the class imbalance phenomenon in the training data , the Synthetic Minority Oversampling Technique (SMOTE) algorithm is also implemented, resulting in increased specificity and sensitivity. The efficacy of the methods is assessed by utilizing blood samples obtained from hospitals in Brazil and Italy. Results revealed that the StackMax method greatly boosted the deep learning and traditional machine learning methods’ capability to distinguish COVID-19-positive cases from normal cases, while SMOTE increased the specificity and sensitivity of the stacked models. Hypothesis testing is performed to determine if there is a significant statistical difference in the performance between the compared detection methods. Additionally, the significance of blood sample features in identifying COVID-19 is analyzed using the XGBoost (eXtreme Gradient Boosting) technique for feature importance identification. Overall, this methodology could potentially enhance the timely and precise identification of COVID-19 in blood samples.},
  archive      = {J_ARTMED},
  author       = {Wu Wang and Fouzi Harrou and Abdelkader Dairi and Ying Sun},
  doi          = {10.1016/j.artmed.2024.102767},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102767},
  shortjournal = {Artif. Intell. Med.},
  title        = {Stacked deep learning approach for efficient SARS-CoV-2 detection in blood samples},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new word embedding model integrated with medical knowledge
for deep learning-based sentiment classification. <em>ARTMED</em>,
<em>148</em>, 102758. (<a
href="https://doi.org/10.1016/j.artmed.2023.102758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of intelligent systems that use social media data for decision-making processes in numerous domains such as politics, business, marketing, and finance, has been made possible by the popularity of social media platforms . However, the utilization of textual data from social media in the healthcare management industry is still somewhat limited when it is compared to other industries. Investigating how current machine learning and natural language processing technologies can be used in the healthcare industry to gauge public sentiment is an important study. Earlier works on healthcare sentiment analysis have utilized traditional word embedding models trained on the general and medical corpus. However, integration of medical knowledge to pre-trained word embedding models has not been considered yet. Word embedding models trained on the general corpus led to the problem of lacking medical knowledge and the models trained on the small size of the medical corpus have limitations in capturing semantic and syntactic properties. This research proposes a new word embedding model named Word Embedding Integrated with Medical Knowledge Vector ( WE-iMKVec ). The proposed model integrates sentiment lexicons and medical knowledgebases into the pre-trained word embedding to enrich the properties of word embedding. A new medical-aware sentiment polarity score is proposed for the utilization in learning neural-network sentiment and these vectors incorporate with the original pre-trained word vectors. The resulting vectors are enriched with lexicon vectors and the medical knowledge vectors: Adverse Drug Reaction (ADR) vector and Unified Medical Language System (UMLS) vector are used to build the proposed WE-iMKVec model. WE-iMKVec is validated on the five different social media healthcare review datasets and the empirical results showed its superiority over traditional word embedding models in medical sentiment analysis. The highest improvement can be found in the patients.info medical condition dataset where the proposed model outperforms three conventional word2vec models (Google-News, PubMed-PMC, and Drug Reviews) by 12.7 %, 31.4 %, and 25.4 % respectively in terms of F1 score.},
  archive      = {J_ARTMED},
  author       = {Aye Hninn Khine and Wiphada Wettayaprasit and Jarunee Duangsuwan},
  doi          = {10.1016/j.artmed.2023.102758},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102758},
  shortjournal = {Artif. Intell. Med.},
  title        = {A new word embedding model integrated with medical knowledge for deep learning-based sentiment classification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semi-supervised image segmentation using a residual-driven
mean teacher and an exponential dice loss. <em>ARTMED</em>,
<em>148</em>, 102757. (<a
href="https://doi.org/10.1016/j.artmed.2023.102757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised segmentation plays an important role in computer vision and medical image analysis and can alleviate the burden of acquiring abundant expert-annotated images. In this paper, we developed a residual-driven semi-supervised segmentation method (termed RDMT) based on the classical mean teacher (MT) framework by introducing a novel model-level residual perturbation and an exponential Dice (eDice) loss. The introduced perturbation was integrated into the exponential moving average (EMA) scheme to enhance the performance of the MT, while the eDice loss was used to improve the detection sensitivity of a given network to object boundaries. We validated the developed method by applying it to segment 3D Left Atrium (LA) and 2D optic cup (OC) from the public LASC and REFUGE datasets based on the V-Net and U-Net, respectively. Extensive experiments demonstrated that the developed method achieved the average Dice score of 0.8776 and 0.7751, when trained on 10% and 20% labeled images, respectively for the LA and OC regions depicted on the LASC and REFUGE datasets. It significantly outperformed the MT and can compete with several existing semi-supervised segmentation methods (i.e., HCMT, UAMT, DTC and SASS).},
  archive      = {J_ARTMED},
  author       = {Chenyang Mei and Xiaoguo Yang and Mi Zhou and Shaodan Zhang and Hao Chen and Xiaokai Yang and Lei Wang},
  doi          = {10.1016/j.artmed.2023.102757},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102757},
  shortjournal = {Artif. Intell. Med.},
  title        = {Semi-supervised image segmentation using a residual-driven mean teacher and an exponential dice loss},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Overlapping cytoplasms segmentation via constrained
multi-shape evolution for cervical cancer screening. <em>ARTMED</em>,
<em>148</em>, 102756. (<a
href="https://doi.org/10.1016/j.artmed.2023.102756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting overlapping cytoplasms in cervical smear images is a clinically essential task for quantitatively measuring cell-level features to screen cervical cancer This task, however, remains rather challenging, mainly due to the deficiency of intensity (or color) information in the overlapping region Although shape prior-based models that compensate intensity deficiency by introducing prior shape information about cytoplasm are firmly established, they often yield visually implausible results, as they model shape priors only by limited shape hypotheses about cytoplasm, exploit cytoplasm-level shape priors alone, and impose no shape constraint on the resulting shape of the cytoplasm In this paper, we present an effective shape prior-based approach, called constrained multi-shape evolution, that segments all overlapping cytoplasms in the clump simultaneously by jointly evolving each cytoplasm’s shape guided by the modeled shape priors We model local shape priors (cytoplasm–level) by an infinitely large shape hypothesis set which contains all possible shapes of the cytoplasm In the shape evolution, we compensate intensity deficiency for the segmentation by introducing not only the modeled local shape priors but also global shape priors (clump–level) modeled by considering mutual shape constraints of cytoplasms in the clump We also constrain the resulting shape in each evolution to be in the built shape hypothesis set for further reducing implausible segmentation results We evaluated the proposed method in two typical cervical smear datasets, and the extensive experimental results confirm its effectiveness.},
  archive      = {J_ARTMED},
  author       = {Youyi Song and Ao Zhang and Jinglin Zhou and Yu Luo and Zhizhe Lin and Teng Zhou},
  doi          = {10.1016/j.artmed.2023.102756},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102756},
  shortjournal = {Artif. Intell. Med.},
  title        = {Overlapping cytoplasms segmentation via constrained multi-shape evolution for cervical cancer screening},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Combining general and personal models for epilepsy detection
with hyperdimensional computing. <em>ARTMED</em>, <em>148</em>, 102754.
(<a href="https://doi.org/10.1016/j.artmed.2023.102754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy is a highly prevalent chronic neurological disorder with great negative impact on patients’ daily lives. Despite this there is still no adequate technological support to enable epilepsy detection and continuous outpatient monitoring in everyday life. Hyperdimensional (HD) computing is a promising method for epilepsy detection via wearable devices , characterized by a simpler learning process and lower memory requirements compared to other methods. In this work, we demonstrate additional avenues in which HD computing and the manner in which its models are built and stored can be used to better understand, compare and create more advanced machine learning models for epilepsy detection. These possibilities are not feasible with other state-of-the-art models, such as random forests or neural networks . We compare inter-subject model similarity of different classes (seizure and non-seizure), study the process of creating general models from personal ones, and finally posit a method of combining personal and general models to create hybrid models. This results in an improved epilepsy detection performance. We also tested knowledge transfer between models trained on two different datasets. The attained insights are highly interesting not only from an engineering perspective, to create better models for wearables, but also from a neurological perspective, to better understand individual epilepsy patterns.},
  archive      = {J_ARTMED},
  author       = {Una Pale and Tomas Teijeiro and Sylvain Rheims and Philippe Ryvlin and David Atienza},
  doi          = {10.1016/j.artmed.2023.102754},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102754},
  shortjournal = {Artif. Intell. Med.},
  title        = {Combining general and personal models for epilepsy detection with hyperdimensional computing},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diseases diagnosis based on artificial intelligence and
ensemble classification. <em>ARTMED</em>, <em>148</em>, 102753. (<a
href="https://doi.org/10.1016/j.artmed.2023.102753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Computer Aided Diagnosis (CAD) has become an important research area that attracted a lot of researchers. In medical diagnostic systems , several attempts have been made to build and enhance CAD applications to avoid errors that can cause dangerously misleading medical treatments. The most exciting opportunity for promoting the performance of CAD system can be accomplished by integrating Artificial Intelligence (AI) in medicine. This allows the effective automation of traditional manual workflow, which is slow, inaccurate and affected by human errors. This paper aims to provide a complete Computer Aided Disease Diagnosis (CAD 2 ) strategy based on Machine Learning (ML) techniques that can help clinicians to make better medical decisions. The proposed CAD 2 consists of three main sequential phases, namely; (i) Outlier Rejection Phase (ORP), (ii) Feature Selection Phase (FSP), and (iii) Classification Phase (CP). ORP is implemented to reject outliers using new Outlier Rejection Technique (ORT) that contains two sequential stages called Fast Outlier Rejection (FOR) and Accurate Outlier Rejection (AOR). The most informative features are selected through FSP using Hybrid Selection Technique (HST). HST includes two main stages called Quick Selection Stage (QS 2 ) using fisher score as a filter method and Precise Selection Stage (PS 2 ) using a Hybrid Bio-inspired Optimization (HBO) technique as a wrapper method. Finally, actual diagnose takes place through CP, which relies on Ensemble Classification Technique (ECT). The proposed CAD 2 has been tested experimentally against recent disease diagnostic strategies using two different datasets in which the first contains several diseases, while the second includes data for Covid-19 patients only. Experimental results have proven the high efficiency of the proposed CAD 2 in terms of accuracy, error, precision, and recall compared with other competitors. Additionally, CAD 2 strategy provides the best Wilcoxon signed rank test and Friedman test measurements against other strategies according to both datasets. It is concluded that CAD 2 strategy based on ORP, FSP, and CP gave an accurate diagnosis compared to other strategies because it gave the highest accuracy and the lowest error and implementation time.},
  archive      = {J_ARTMED},
  author       = {Asmaa H. Rabie and Ahmed I. Saleh},
  doi          = {10.1016/j.artmed.2023.102753},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102753},
  shortjournal = {Artif. Intell. Med.},
  title        = {Diseases diagnosis based on artificial intelligence and ensemble classification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computational tumor growth model experience based on
molecular dynamics point of view using deep cellular automata.
<em>ARTMED</em>, <em>148</em>, 102752. (<a
href="https://doi.org/10.1016/j.artmed.2023.102752">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer, as identified by the World Health Organization, stands as the second leading cause of death globally. Its intricate nature makes it challenging to study solely based on biological knowledge, often leading to expensive research endeavors. While tremendous strides have been made in understanding cancer, gaps remain, especially in predicting tumor behavior across various stages. The integration of artificial intelligence in oncology research has accelerated our insights into tumor behavior, right from its genesis to metastasis. Nevertheless, there&#39;s a pressing need for a holistic understanding of the interactions between cancer cells, their microenvironment , and their subsequent interplay with the broader body environment. In this landscape, deep learning emerges as a potent tool with its multifaceted applications in diverse scientific challenges. Motivated by this, our study presents a novel approach to modeling cancer tumor growth from a molecular dynamics&#39; perspective, harnessing the capabilities of deep-learning cellular automata . This not only facilitates a microscopic examination of tumor behavior and growth but also delves deeper into its overarching behavioral patterns. Our work primarily focused on evaluating the developed tumor growth model through the proposed network, followed by a rigorous compatibility check with traditional mathematical tumor growth models using R and Matlab software. The outcomes notably aligned with the Gompertz growth model, accentuating the robustness of our approach. Our validated model stands out by offering adaptability to diverse tumor growth datasets, positioning itself as a valuable tool for predictions and further research.},
  archive      = {J_ARTMED},
  author       = {Hossein Nikravesh Matin and Saeed Setayeshi},
  doi          = {10.1016/j.artmed.2023.102752},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102752},
  shortjournal = {Artif. Intell. Med.},
  title        = {A computational tumor growth model experience based on molecular dynamics point of view using deep cellular automata},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluating the clinical utility of artificial intelligence
assistance and its explanation on the glioma grading task.
<em>ARTMED</em>, <em>148</em>, 102751. (<a
href="https://doi.org/10.1016/j.artmed.2023.102751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical evaluation evidence and model explainability are key gatekeepers to ensure the safe, accountable, and effective use of artificial intelligence (AI) in clinical settings. We conducted a clinical user-centered evaluation with 35 neurosurgeons to assess the utility of AI assistance and its explanation on the glioma grading task. Each participant read 25 brain MRI scans of patients with gliomas, and gave their judgment on the glioma grading without and with the assistance of AI prediction and explanation. The AI model was trained on the BraTS dataset with 88.0% accuracy. The AI explanation was generated using the explainable AI algorithm of SmoothGrad, which was selected from 16 algorithms based on the criterion of being truthful to the AI decision process. Results showed that compared to the average accuracy of 82 . 5 ± 8 . 7 % 82.5±8.7% when physicians performed the task alone, physicians’ task performance increased to 87 . 7 ± 7 . 3 % 87.7±7.3% with statistical significance ( p p -value = 0.002) when assisted by AI prediction, and remained at almost the same level of 88 . 5 ± 7 . 0 % 88.5±7.0% ( p p -value = 0.35) with the additional assistance of AI explanation. Based on quantitative and qualitative results, the observed improvement in physicians’ task performance assisted by AI prediction was mainly because physicians’ decision patterns converged to be similar to AI, as physicians only switched their decisions when disagreeing with AI. The insignificant change in physicians’ performance with the additional assistance of AI explanation was because the AI explanations did not provide explicit reasons, contexts, or descriptions of clinical features to help doctors discern potentially incorrect AI predictions. The evaluation showed the clinical utility of AI to assist physicians on the glioma grading task, and identified the limitations and clinical usage gaps of existing explainable AI techniques for future improvement.},
  archive      = {J_ARTMED},
  author       = {Weina Jin and Mostafa Fatehi and Ru Guo and Ghassan Hamarneh},
  doi          = {10.1016/j.artmed.2023.102751},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102751},
  shortjournal = {Artif. Intell. Med.},
  title        = {Evaluating the clinical utility of artificial intelligence assistance and its explanation on the glioma grading task},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel method leveraging time series data to improve
subphenotyping and application in critically ill patients with COVID-19.
<em>ARTMED</em>, <em>148</em>, 102750. (<a
href="https://doi.org/10.1016/j.artmed.2023.102750">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational subphenotyping, a data-driven approach to understanding disease subtypes, is a prominent topic in medical research. Numerous ongoing studies are dedicated to developing advanced computational subphenotyping methods for cross-sectional data. However, the potential of time-series data has been underexplored until now. Here, we propose a Multivariate Levenshtein Distance (MLD) that can account for address correlation in multiple discrete features over time-series data. Our algorithm has two distinct components: it integrates an optimal threshold score to enhance the sensitivity in discriminating between pairs of instances, and the MLD itself. We have applied the proposed distance metrics on the k-means clustering algorithm to derive temporal subphenotypes from time-series data of biomarkers and treatment administrations from 1039 critically ill patients with COVID-19 and compare its effectiveness to standard methods. In conclusion, the Multivariate Levenshtein Distance metric is a novel method to quantify the distance from multiple discrete features over time-series data and demonstrates superior clustering performance among competing time-series distance metrics.},
  archive      = {J_ARTMED},
  author       = {Wonsuk Oh and Pushkala Jayaraman and Pranai Tandon and Udit S. Chaddha and Patricia Kovatch and Alexander W. Charney and Benjamin S. Glicksberg and Girish N. Nadkarni},
  doi          = {10.1016/j.artmed.2023.102750},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102750},
  shortjournal = {Artif. Intell. Med.},
  title        = {A novel method leveraging time series data to improve subphenotyping and application in critically ill patients with COVID-19},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advanced decision support system for individuals with
diabetes on multiple daily injections therapy using reinforcement
learning and nearest-neighbors: In-silico and clinical results.
<em>ARTMED</em>, <em>148</em>, 102749. (<a
href="https://doi.org/10.1016/j.artmed.2023.102749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many individuals with diabetes on multiple daily insulin injections therapy use carbohydrate ratios (CRs) and correction factors (CFs) to determine mealtime and correction insulin boluses. The CRs and CFs vary over time due to physiological changes in individuals&#39; response to insulin. Errors in insulin dosing can lead to life-threatening abnormal glucose levels , increasing the risk of retinopathy , neuropathy , and nephropathy. Here, we present a novel learning algorithm that uses Q-learning to track optimal CRs and uses nearest-neighbors based Q-learning to track optimal CFs. The learning algorithm was compared with the run-to-run algorithm A and the run-to-run algorithm B, both proposed in the literature, over an 8-week period using a validated simulator with a realistic scenario created with suboptimal CRs and CFs values, carbohydrate counting errors, and random meals sizes at random ingestion times. From Week 1 to Week 8, the learning algorithm increased the percentage of time spent in target glucose range (4.0 to 10.0 mmol/L) from 51 % to 64 % compared to 61 % and 58 % with the run-to-run algorithm A and the run-to-run algorithm B, respectively. The learning algorithm decreased the percentage of time spent below 4.0 mmol/L from 9 % to 1.9 % compared to 3.4 % and 2.3 % with the run-to-run algorithm A and the run-to-run algorithm B, respectively. The algorithm was also assessed by comparing its recommendations with (i) the endocrinologist&#39;s recommendations on two type 1 diabetes individuals over a 16-week period and (ii) real-world individuals&#39; therapy settings changes of 23 individuals (19 type 2 and 4 type 1) over an 8-week period using the commercial Bigfoot Unity Diabetes Management System. The full agreements (i) were 89 % and 76 % for CRs and CFs for the type 1 diabetes individuals and (ii) was 62 % for mealtime doses for the individuals on the commercial Bigfoot system. Therefore, the proposed algorithm has the potential to improve glucose control in individuals with type 1 and type 2 diabetes .},
  archive      = {J_ARTMED},
  author       = {Adnan Jafar and Melissa-Rosina Pasqua and Byron Olson and Ahmad Haidar},
  doi          = {10.1016/j.artmed.2023.102749},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102749},
  shortjournal = {Artif. Intell. Med.},
  title        = {Advanced decision support system for individuals with diabetes on multiple daily injections therapy using reinforcement learning and nearest-neighbors: In-silico and clinical results},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient symptom inquiring and diagnosis via adaptive
alignment of reinforcement learning and classification. <em>ARTMED</em>,
<em>148</em>, 102748. (<a
href="https://doi.org/10.1016/j.artmed.2023.102748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical automatic diagnosis aims to organize real-world diagnostic processes similar to those from human doctors and to achieve accurate diagnoses by interacting with patients. The task is formulated as a sequential decision-making problem with a series of information inquiry steps (asking about symptoms and ordering examinations) and the final diagnosis. Recent research has studied incorporating reinforcement learning for information inquiry and classification techniques for disease diagnosis, respectively. However, studies on efficiently and effectively combining the two procedures are still lacking. To address this issue, we devised an adaptive mechanism to align reinforcement learning and classification methods using distribution entropy as the medium. Additionally, we created a new dataset for patient simulation to address the lack of large-scale evaluation benchmarks . The dataset is extracted from the MedlinePlus knowledge base and contains significantly more diseases and more comprehensive symptom and examination information than existing datasets. Experimental evaluation shows that our method outperforms three current state-of-the-art methods on different datasets by achieving higher medical diagnostic accuracy with fewer inquiring turns.},
  archive      = {J_ARTMED},
  author       = {Hongyi Yuan and Sheng Yu},
  doi          = {10.1016/j.artmed.2023.102748},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102748},
  shortjournal = {Artif. Intell. Med.},
  title        = {Efficient symptom inquiring and diagnosis via adaptive alignment of reinforcement learning and classification},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Contrastive image adaptation for acquisition shift reduction
in medical imaging. <em>ARTMED</em>, <em>148</em>, 102747. (<a
href="https://doi.org/10.1016/j.artmed.2023.102747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain shift, or acquisition shift in medical imaging, is responsible for potentially harmful differences between development and deployment conditions of medical image analysis techniques. There is a growing need in the community for advanced methods that could mitigate this issue better than conventional approaches. In this paper, we consider configurations in which we can expose a learning-based pixel level adaptor to a large variability of unlabeled images during its training, i.e. sufficient to span the acquisition shift expected during the training or testing of a downstream task model. We leverage the ability of convolutional architectures to efficiently learn domain-agnostic features and train a many-to-one unsupervised mapping between a source collection of heterogeneous images from multiple unknown domains subjected to the acquisition shift and a homogeneous subset of this source set of lower cardinality , potentially constituted of a single image. To this end, we propose a new cycle-free image-to-image architecture based on a combination of three loss functions : a contrastive PatchNCE loss, an adversarial loss and an edge preserving loss allowing for rich domain adaptation to the target image even under strong domain imbalance and low data regimes. Experiments support the interest of the proposed contrastive image adaptation approach for the regularization of downstream deep supervised segmentation and cross-modality synthesis models.},
  archive      = {J_ARTMED},
  author       = {Clément Hognon and Pierre-Henri Conze and Vincent Bourbonne and Olivier Gallinato and Thierry Colin and Vincent Jaouen and Dimitris Visvikis},
  doi          = {10.1016/j.artmed.2023.102747},
  journal      = {Artificial Intelligence in Medicine},
  month        = {2},
  pages        = {102747},
  shortjournal = {Artif. Intell. Med.},
  title        = {Contrastive image adaptation for acquisition shift reduction in medical imaging},
  volume       = {148},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-organ spatiotemporal information aware model for
sepsis mortality prediction. <em>ARTMED</em>, <em>147</em>, 102746. (<a
href="https://doi.org/10.1016/j.artmed.2023.102746">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a syndrome involving multi-organ dysfunction, and the mortality in sepsis patients correlates with the number of lesioned organs. Precise prognosis models play a pivotal role in enabling healthcare practitioners to administer timely and accurate interventions for sepsis, thereby augmenting patient outcomes. Nevertheless, the majority of available models consider the overall physiological attributes of patients, overlooking the asynchronous spatiotemporal interactions among multiple organ systems. These constraints hinder a full application of such models, particularly when dealing with limited clinical data. To surmount these challenges, a comprehensive model, denoted as recurrent Graph Attention Network-multi Gated Recurrent Unit (rGAT-mGRU), was proposed. Taking into account the intricate spatiotemporal interactions among multiple organ systems, the model predicted in-hospital mortality of sepsis using data collected within the 48-hour period post-diagnosis. Multiple parallel GRU sub-models were formulated to investigate the temporal physiological variations of single organ systems. Meanwhile, a GAT structure featuring a memory unit was constructed to capture spatiotemporal connections among multi-organ systems. Additionally, an attention-injection mechanism was employed to govern the data flowing within the network pertaining to multi-organ systems. The proposed model underwent training and testing using a dataset of 10,181 sepsis cases extracted from the Medical Information Mart for Intensive Care III (MIMIC-III) database. To evaluate the model&#39;s superiority, it was compared with the existing common baseline models. Furthermore, ablation experiments were designed to elucidate the rationale and robustness of the proposed model. Compared with the baseline models for predicting mortality of sepsis, the rGAT-mGRU model demonstrated the largest area under the receiver operating characteristic curve (AUROC) of 0.8777 ± ± 0.0039 and the maximum area under the precision-recall curve (AUPRC) of 0.5818 ± ± 0.0071, with sensitivity of 0.8358 ± ± 0.0302 and specificity of 0.7727 ± ± 0.0229, respectively. The proposed model was capable of delineating the varying contribution of the involved organ systems at distinct moments, as specifically illustrated by the attention weights. Furthermore, it exhibited consistent performance even in the face of limited clinical data. The rGAT-mGRU model has the potential to indicate sepsis prognosis by extracting the dynamic spatiotemporal interplay information inherent in multi-organ systems during critical diseases, thereby providing clinicians with auxiliary decision-making support.},
  archive      = {J_ARTMED},
  author       = {Xue Feng and Siyi Zhu and Yanfei Shen and Huaiping Zhu and Molei Yan and Guolong Cai and Gangmin Ning},
  doi          = {10.1016/j.artmed.2023.102746},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102746},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multi-organ spatiotemporal information aware model for sepsis mortality prediction},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of deep learning-based depression detection using
medical claims data. <em>ARTMED</em>, <em>147</em>, 102745. (<a
href="https://doi.org/10.1016/j.artmed.2023.102745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human accuracy in diagnosing psychiatric disorders is still low. Even though digitizing health care leads to more and more data, the successful adoption of AI-based digital decision support (DDSS) is rare. One reason is that AI algorithms are often not evaluated based on large, real-world data. This research shows the potential of using deep learning on the medical claims data of 812,853 people between 2018 and 2022, with 26 , 973 , 943 26,973,943 ICD-10-coded diseases, to predict depression (F32 and F33 ICD-10 codes). The dataset used represents almost the entire adult population of Estonia. Based on these data, to show the critical importance of the underlying temporal properties of the data for the detection of depression, we evaluate the performance of non-sequential models (LR, FNN), sequential models (LSTM, CNN-LSTM) and the sequential model with a decay factor (GRU- Δ t Δt , GRU-decay). Furthermore, since explainability is necessary for the medical domain, we combine a self-attention model with the GRU decay and evaluate its performance. We named this combination Att-GRU-decay. After extensive empirical experimentation, our model (Att-GRU-decay), with an AUC score of 0.990, an AUPRC score of 0.974, a specificity of 0.999 and a sensitivity of 0.944, proved to be the most accurate. The results of our novel Att-GRU-decay model outperform the current state of the art, demonstrating the potential usefulness of deep learning algorithms for DDSS development. We further expand this by describing a possible application scenario of the proposed algorithm for depression screening in a general practitioner (GP) setting—not only to decrease healthcare costs, but also to improve the quality of care and ultimately decrease people’s suffering.},
  archive      = {J_ARTMED},
  author       = {Markus Bertl and Nzamba Bignoumba and Peeter Ross and Sadok Ben Yahia and Dirk Draheim},
  doi          = {10.1016/j.artmed.2023.102745},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102745},
  shortjournal = {Artif. Intell. Med.},
  title        = {Evaluation of deep learning-based depression detection using medical claims data},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-invasive fractional flow reserve derived from
reduced-order coronary model and machine learning prediction of stenosis
flow resistance. <em>ARTMED</em>, <em>147</em>, 102744. (<a
href="https://doi.org/10.1016/j.artmed.2023.102744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computational fluid dynamics enables the non-invasive calculation of fractional flow reserve (FFR) based on 3D coronary model, but it is time-consuming. Currently, machine learning technique has emerged as an efficient and reliable approach for prediction, which allows saving a lot of analysis time. This study aimed at developing a simplified FFR prediction model for rapid and accurate assessment of functional significance of stenosis . A reduced-order lumped parameter model (LPM) of coronary system and cardiovascular system was constructed for rapidly simulating coronary flow , in which a machine learning model was embedded for accurately predicting stenosis flow resistance at a given flow from anatomical features of stenosis. Importantly, the LPM was personalized in both structures and parameters according to coronary geometries from computed tomography angiography and physiological measurements such as blood pressure and cardiac output for personalized simulations of coronary pressure and flow. Coronary lesions with invasive FFR ≤ 0.80 were defined as hemodynamically significant. A total of 91 patients (93 lesions) who underwent invasive FFR were involved in FFR derived from machine learning (FFR ML ) calculation. Of the 93 lesions, 27 lesions (29.0%) showed lesion-specific ischemia . The average time of FFR ML simulation was about 10 min. On a per-vessel basis, the FFR ML and FFR were significantly correlated (r = 0.86, p &lt; 0.001). The diagnostic accuracy, sensitivity, specificity, positive predictive value and negative predictive value were 91.4%, 92.6%, 90.9%, 80.6% and 96.8%, respectively. The area under the receiver-operating characteristic curve of FFR ML was 0.984. In this selected cohort of patients, the FFR ML improves the computational efficiency and ensures the accuracy. The favorable performance of FFR ML approach greatly facilitates its potential application in detecting hemodynamically significant coronary stenosis in future routine clinical practice.},
  archive      = {J_ARTMED},
  author       = {Yili Feng and Ruisen Fu and Hao Sun and Xue Wang and Yang Yang and Chuanqi Wen and Yaodong Hao and Yutong Sun and Bao Li and Na Li and Haisheng Yang and Quansheng Feng and Jian Liu and Zhuo Liu and Liyuan Zhang and Youjun Liu},
  doi          = {10.1016/j.artmed.2023.102744},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102744},
  shortjournal = {Artif. Intell. Med.},
  title        = {Non-invasive fractional flow reserve derived from reduced-order coronary model and machine learning prediction of stenosis flow resistance},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian network structure learning algorithm for highly
missing and non imputable data: Application to breast cancer
radiotherapy data. <em>ARTMED</em>, <em>147</em>, 102743. (<a
href="https://doi.org/10.1016/j.artmed.2023.102743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is not uncommon for real-life data produced in healthcare to have a higher proportion of missing data than in other scopes. To take into account these missing data, imputation is not always desired by healthcare experts, and complete case analysis can lead to a significant loss of data. The algorithm proposed here, allows the learning of Bayesian Network graphs when both imputation and complete case analysis are not possible. The learning process is based on a set of local bootstrap learnings performed on complete sub-datasets which are then aggregated and locally optimized. This learning method presents competitive results compared to other structure learning algorithms, whatever the mechanism of missing data.},
  archive      = {J_ARTMED},
  author       = {Mélanie Piot and Frédéric Bertrand and Sébastien Guihard and Jean-Baptiste Clavier and Myriam Maumy},
  doi          = {10.1016/j.artmed.2023.102743},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102743},
  shortjournal = {Artif. Intell. Med.},
  title        = {Bayesian network structure learning algorithm for highly missing and non imputable data: Application to breast cancer radiotherapy data},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Guideline-informed reinforcement learning for mechanical
ventilation in critical care. <em>ARTMED</em>, <em>147</em>, 102742. (<a
href="https://doi.org/10.1016/j.artmed.2023.102742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement Learning (RL) has recently found many applications in the healthcare domain thanks to its natural fit to clinical decision-making and ability to learn optimal decisions from observational data. A key challenge in adopting RL-based solution in clinical practice, however, is the inclusion of existing knowledge in learning a suitable solution. Existing knowledge from e.g. medical guidelines may improve the safety of solutions, produce a better balance between short- and long-term outcomes for patients and increase trust and adoption by clinicians. We present a framework for including knowledge available from medical guidelines in RL. The framework includes components for enforcing safety constraints and an approach that alters the learning signal to better balance short- and long-term outcomes based on these guidelines. We evaluate the framework by extending an existing RL-based mechanical ventilation (MV) approach with clinically established ventilation guidelines. Results from off-policy policy evaluation indicate that our approach has the potential to decrease 90-day mortality while ensuring lung protective ventilation. This framework provides an important stepping stone towards implementations of RL in clinical practice and opens up several avenues for further research.},
  archive      = {J_ARTMED},
  author       = {Floris den Hengst and Martijn Otten and Paul Elbers and Frank van Harmelen and Vincent François-Lavet and Mark Hoogendoorn},
  doi          = {10.1016/j.artmed.2023.102742},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102742},
  shortjournal = {Artif. Intell. Med.},
  title        = {Guideline-informed reinforcement learning for mechanical ventilation in critical care},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MS-CPFI: A model-agnostic counterfactual perturbation
feature importance algorithm for interpreting black-box multi-state
models. <em>ARTMED</em>, <em>147</em>, 102741. (<a
href="https://doi.org/10.1016/j.artmed.2023.102741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-state processes (Webster, 2019) are commonly used to model the complex clinical evolution of diseases where patients progress through different states. In recent years, machine learning and deep learning algorithms have been proposed to improve the accuracy of these models’ predictions (Wang et al., 2019). However, acceptability by patients and clinicians, as well as for regulatory compliance, require interpretability of these algorithms’s predictions. Existing methods, such as the Permutation Feature Importance algorithm, have been adapted for interpreting predictions in black-box models for 2-state processes (corresponding to survival analysis). For generalizing these methods to multi-state models, we introduce a novel model-agnostic interpretability algorithm called Multi-State Counterfactual Perturbation Feature Importance (MS-CPFI) that computes feature importance scores for each transition of a general multi-state model, including survival, competing-risks, and illness-death models. MS-CPFI uses a new counterfactual perturbation method that allows interpreting feature effects while capturing the non-linear effects and potentially capturing time-dependent effects. Experimental results on simulations show that MS-CPFI increases model interpretability in the case of non-linear effects. Additionally, results on a real-world dataset for patients with breast cancer confirm that MS-CPFI can detect clinically important features and provide information on the disease progression by displaying features that are protective factors versus features that are risk factors for each stage of the disease. Overall, MS-CPFI is a promising model-agnostic interpretability algorithm for multi-state models, which can improve the interpretability of machine learning and deep learning algorithms in healthcare.},
  archive      = {J_ARTMED},
  author       = {Aziliz Cottin and Marine Zulian and Nicolas Pécuchet and Agathe Guilloux and Sandrine Katsahian},
  doi          = {10.1016/j.artmed.2023.102741},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102741},
  shortjournal = {Artif. Intell. Med.},
  title        = {MS-CPFI: A model-agnostic counterfactual perturbation feature importance algorithm for interpreting black-box multi-state models},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A weighted distance-based dynamic ensemble regression
framework for gastric cancer survival time prediction. <em>ARTMED</em>,
<em>147</em>, 102740. (<a
href="https://doi.org/10.1016/j.artmed.2023.102740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of gastric cancer patient survival time is essential for clinical decision-making. However, unified static models lack specificity and flexibility in predictions owing to the varying survival outcomes among gastric cancer patients. We address these problems by using an ensemble learning approach and adaptively assigning greater weights to similar patients to make more targeted predictions when predicting an individual’s survival time . We treat these problems as regression problems and introduce a weighted dynamic ensemble regression framework. To better identify similar patients, we devise a method to measure patient similarity, considering the diverse impacts of features. Subsequently, we use this measure to design both a weighted K-means clustering method and a fuzzy K-means sampling technique to group patients and train corresponding base regressors . To achieve more targeted predictions, we calculate the weight of each base regressor based on the similarity between the patient to be predicted and the patient clusters, culminating in the integration of the results. The model is validated on a dataset of 7791 patients, outperforming other models in terms of three evaluation metrics , namely, the root mean square error , mean absolute error , and the coefficient of determination . The weighted dynamic ensemble regression strategy can improve the baseline model by 1.75%, 2.12%, and 13.45% in terms of the three respective metrics while also mitigating the imbalanced survival time distribution issue. This enhanced performance has been statistically validated, even when tested on six public datasets with different sizes. By considering feature variations, patients with distinct survival profiles can be effectively differentiated, and the model predictive performance can be enhanced. The results generated by our proposed model can be invaluable in guiding decisions related to treatment plans and resource allocation . Furthermore, the model has the potential for broader applications in prognosis for other types of cancers or similar regression problems in various domains.},
  archive      = {J_ARTMED},
  author       = {Liangchen Xu and Chonghui Guo and Mucan Liu},
  doi          = {10.1016/j.artmed.2023.102740},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102740},
  shortjournal = {Artif. Intell. Med.},
  title        = {A weighted distance-based dynamic ensemble regression framework for gastric cancer survival time prediction},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Diagnosis knowledge constrained network based on first-order
logic for syndrome differentiation. <em>ARTMED</em>, <em>147</em>,
102739. (<a href="https://doi.org/10.1016/j.artmed.2023.102739">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Chinese medicine (TCM) has been recognized worldwide as a valuable asset of human medicine. The procedure of TCM is to treatment based on syndrome differentiation. However, the effect of TCM syndrome differentiation relies heavily on the experience of doctors. The gratifying progress of machine learning research in recent years has brought new ideas for TCM syndrome differentiation. In this paper, we propose a deep network model for TCM syndrome differentiation, which improves network performance by injecting TCM syndrome differentiation knowledge in the form of first-order logic into the deep network. Experimental results show that the accuracy of our proposed model reaches 89%, which is significantly better than the deep learning model MLP and other traditional machine learning models. In addition, we present the collected and formatted TCM syndrome differentiation (TSD) dataset, which contains more than 40,000 TCM clinical records. Moreover, 45 symptoms (“ ”), 322 patterns(“ ”), and more than 500 symptoms are labeled in TSD respectively. To the best of our knowledge, this is the first TCM syndrome differentiation dataset labeling diseases, syndromes and pattern. Such detailed labeling is helpful to explore the relationship between various elements of syndrome differentiation.},
  archive      = {J_ARTMED},
  author       = {Meiwen Li and Lin Wang and Qingtao Wu and Junlong Zhu and Mingchuan Zhang},
  doi          = {10.1016/j.artmed.2023.102739},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102739},
  shortjournal = {Artif. Intell. Med.},
  title        = {Diagnosis knowledge constrained network based on first-order logic for syndrome differentiation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An in-depth survey on deep learning-based motor imagery
electroencephalogram (EEG) classification. <em>ARTMED</em>,
<em>147</em>, 102738. (<a
href="https://doi.org/10.1016/j.artmed.2023.102738">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based Brain–Computer Interfaces (BCIs) build a communication path between human brain and external devices. Among EEG-based BCI paradigms, the most commonly used one is motor imagery (MI). As a hot research topic , MI EEG-based BCI has largely contributed to medical fields and smart home industry. However, because of the low signal-to-noise ratio (SNR) and the non-stationary characteristic of EEG data, it is difficult to correctly classify different types of MI-EEG signals. Recently, the advances in Deep Learning (DL) significantly facilitate the development of MI EEG-based BCIs. In this paper, we provide a systematic survey of DL-based MI-EEG classification methods. Specifically, we first comprehensively discuss several important aspects of DL-based MI-EEG classification, covering input formulations, network architectures , public datasets, etc. Then, we summarize problems in model performance comparison and give guidelines to future studies for fair performance comparison. Next, we fairly evaluate the representative DL-based models using source code released by the authors and meticulously analyse the evaluation results. By performing ablation study on the network architecture, we found that (1) effective feature fusion is indispensable for multi-stream CNN-based models. (2) LSTM should be combined with spatial feature extraction techniques to obtain good classification performance. (3) the use of dropout contributes little to improving the model performance, and that (4) adding fully connected layers to the models significantly increases their parameters but it might not improve their performance. Finally, we raise several open issues in MI-EEG classification and provide possible future research directions.},
  archive      = {J_ARTMED},
  author       = {Xianheng Wang and Veronica Liesaputra and Zhaobin Liu and Yi Wang and Zhiyi Huang},
  doi          = {10.1016/j.artmed.2023.102738},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102738},
  shortjournal = {Artif. Intell. Med.},
  title        = {An in-depth survey on deep learning-based motor imagery electroencephalogram (EEG) classification},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). STERN: Attention-driven spatial transformer network for
abnormality detection in chest x-ray images. <em>ARTMED</em>,
<em>147</em>, 102737. (<a
href="https://doi.org/10.1016/j.artmed.2023.102737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray scans are frequently requested to detect the presence of abnormalities, due to their low-cost and non-invasive nature. The interpretation of these images can be automated to prioritize more urgent exams through deep learning models, but the presence of image artifacts, e.g. lettering, often generates a harmful bias in the classifiers and an increase of false positive results. Consequently, healthcare would benefit from a system that selects the thoracic region of interest prior to deciding whether an image is possibly pathologic. The current work tackles this binary classification exercise, in which an image is either normal or abnormal, using an attention-driven and spatially unsupervised S patial T ransform er N etwork (STERN), that takes advantage of a novel domain-specific loss to better frame the region of interest. Unlike the state of the art, in which this type of networks is usually employed for image alignment, this work proposes a spatial transformer module that is used specifically for attention, as an alternative to the standard object detection models that typically precede the classifier to crop out the region of interest. In sum, the proposed end-to-end architecture dynamically scales and aligns the input images to maximize the classifier’s performance, by selecting the thorax with translation and non-isotropic scaling transformations, and thus eliminating artifacts. Additionally, this paper provides an extensive and objective analysis of the selected regions of interest, by proposing a set of mathematical evaluation metrics. The results indicate that the STERN achieves similar results to using YOLO-cropped images, with reduced computational cost and without the need for localization labels. More specifically, the system is able to distinguish abnormal frontal images from the CheXpert dataset, with a mean AUC of 85.67% - a 2.55% improvement vs. the 0.98% improvement achieved by the YOLO-based counterpart in comparison to a standard baseline classifier. At the same time, the STERN approach requires less than 2/3 of the training parameters, while increasing the inference time per batch in less than 2 ms. Code available via GitHub .},
  archive      = {J_ARTMED},
  author       = {Joana Rocha and Sofia Cardoso Pereira and João Pedrosa and Aurélio Campilho and Ana Maria Mendonça},
  doi          = {10.1016/j.artmed.2023.102737},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102737},
  shortjournal = {Artif. Intell. Med.},
  title        = {STERN: Attention-driven spatial transformer network for abnormality detection in chest X-ray images},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). BGRL: Basal ganglia inspired reinforcement learning based
framework for deep brain stimulators. <em>ARTMED</em>, <em>147</em>,
102736. (<a href="https://doi.org/10.1016/j.artmed.2023.102736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Brain Stimulation (DBS) is an implantable medical device used for electrical stimulation to treat neurological disorders . Traditional DBS devices provide fixed frequency pulses, but personalized adjustment of stimulation parameters is crucial for optimal treatment. This paper introduces a Basal Ganglia inspired Reinforcement Learning (BGRL) approach, incorporating a closed-loop feedback mechanism to suppress neural synchrony during neurological fluctuations. The BGRL approach leverages the resemblance between the Basal Ganglia region of brain by incorporating the actor–critic architecture of reinforcement learning (RL). Simulation results demonstrate that BGRL significantly reduces synchronous electrical pulses compared to other standard RL algorithms. BGRL algorithm outperforms existing RL methods in terms of suppression capability and energy consumption, validated through comparisons using ensemble oscillators. Results shown in the paper demonstrate BGRL suppressed the synchronous electrical pulses across three signaling regimes namely regular, chaotic and bursting by 40%, 146% and 40% respectively as compared to soft actor–critic model. BGRL shows promise in effectively suppressing neural synchrony in DBS therapy , providing an efficient alternative to open-loop methodologies.},
  archive      = {J_ARTMED},
  author       = {Harsh Agarwal and Heena Rathore},
  doi          = {10.1016/j.artmed.2023.102736},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102736},
  shortjournal = {Artif. Intell. Med.},
  title        = {BGRL: Basal ganglia inspired reinforcement learning based framework for deep brain stimulators},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). FIT-graph: A multi-grained evolutionary graph based
framework for disease diagnosis. <em>ARTMED</em>, <em>147</em>, 102735.
(<a href="https://doi.org/10.1016/j.artmed.2023.102735">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early assessment, with the help of machine learning methods , can aid clinicians in optimizing the diagnosis and treatment process, allowing patients to receive critical treatment time. Due to the advantages of effective information organization and interpretable reasoning, knowledge graph-based methods have become one of the most widely used machine learning algorithms for this task. However, due to a lack of effective organization and use of multi-granularity and temporal information, current knowledge graph-based approaches are hard to fully and comprehensively exploit the information contained in medical records , restricting their capacity to make superior quality diagnoses. To address these challenges, we examine and study disease diagnosis applications in-depth, and propose a novel disease diagnosis framework named FIT-Graph . With novel medical multi-grained evolutionary graphs, FIT-Graph efficiently organizes the extracted information from various granularities and time stages, maximizing the retention of valuable information for disease inference and ensuring the comprehensiveness and validity of the final disease inference. We compare FIT-Graph with two real-world clinical datasets from cardiology and respiratory departments with the baseline. The experimental results show that its effect is better than the baseline model , and the baseline performance of the task is improved by about 5% in multiple indices.},
  archive      = {J_ARTMED},
  author       = {Zizhu Liu and Qing Cao and Nan Du and Huizhen Shu and Erheng Zhong and Nan Jiang and Qiaoran Chen and Ying Shen and Kang Chen},
  doi          = {10.1016/j.artmed.2023.102735},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102735},
  shortjournal = {Artif. Intell. Med.},
  title        = {FIT-graph: A multi-grained evolutionary graph based framework for disease diagnosis},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting sequenced dental treatment plans from electronic
dental records using deep learning. <em>ARTMED</em>, <em>147</em>,
102734. (<a href="https://doi.org/10.1016/j.artmed.2023.102734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing appropriate clinical dental treatment plans is an urgent need because a growing number of dental patients are suffering from partial edentulism with the population getting older. The aim of this study is to predict sequential treatment plans from electronic dental records . We construct a clinical decision support model, MultiTP, explores the unique topology of teeth information and the variation of complicated treatments, integrates deep learning models (convolutional neural network and recurrent neural network) adaptively, and embeds the attention mechanism to produce optimal treatment plans. MultiTP shows its promising performance with an AUC of 0.9079 and an F score of 0.8472 over five treatment plans. The interpretability analysis also indicates its capability in mining clinical knowledge from the textual data. MultiTP&#39;s novel problem formulation, neural network framework, and interpretability analysis techniques allow for broad applications of deep learning in dental healthcare, providing valuable support for predicting dental treatment plans in the clinic and benefiting dental patients. The MultiTP is an efficient tool that can be implemented in clinical practice and integrated into the existing EDR system. By predicting treatment plans for partial edentulism , the model will help dentists improve their clinical decisions.},
  archive      = {J_ARTMED},
  author       = {Haifan Chen and Pufan Liu and Zhaoxing Chen and Qingxiao Chen and Zaiwen Wen and Ziqing Xie},
  doi          = {10.1016/j.artmed.2023.102734},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102734},
  shortjournal = {Artif. Intell. Med.},
  title        = {Predicting sequenced dental treatment plans from electronic dental records using deep learning},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Value function assessment to different RL algorithms for
heparin treatment policy of patients with sepsis in ICU.
<em>ARTMED</em>, <em>147</em>, 102726. (<a
href="https://doi.org/10.1016/j.artmed.2023.102726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heparin is a critical aspect of managing sepsis after abdominal surgery, which can improve microcirculation , protect organ function, and reduce mortality. However, there is no clinical evidence to support decision-making for heparin dosage. This paper proposes a model called SOFA-MDP, which utilizes SOFA scores as states of MDP, to investigate clinic policies. Different algorithms provide different value functions, making it challenging to determine which value function is more reliable. Due to ethical restrictions, we cannot test all policies on patients. To address this issue, we proposed two value function assessment methods: action similarity rate and relative gain. We experimented with heparin treatment policies for sepsis patients after abdominal surgery using MIMIC-IV. In the experiments, TD ( 0 ) (0) shows the most reliable performance. Using the action similarity rate and relative gain to assess AI policy from TD ( 0 ) (0) , the agreement rates between AI policy and “good” physician’s actual treatment are 64.6% and 73.2%, while the agreement rates between AI policy and “bad” physician’s actual treatment are 44.1% and 35.8%, the gaps are 20.5% and 37.4%, respectively. External validation using action similarity rate and relative gain based on eICU resulted in agreement rates of 61.5% and 69.1% with the “good” physician’s treatment, and 45.2% and 38.3% with the “bad” physician’s treatment, with gaps of 16.3% and 30.8%, respectively. In conclusion, the model provides instructive support for clinical decisions, and the evaluation methods accurately distinguish reliable and unreasonable outcomes.},
  archive      = {J_ARTMED},
  author       = {Jiang Liu and Yihao Xie and Xin Shu and Yuwen Chen and Yizhu Sun and Kunhua Zhong and Hao Liang and Yujie Li and Chunyong Yang and Yan Han and Yuwei Zou and Ziting Zhuyi and Jiahao Huang and Junhong Li and Xiaoyan Hu and Bin Yi},
  doi          = {10.1016/j.artmed.2023.102726},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102726},
  shortjournal = {Artif. Intell. Med.},
  title        = {Value function assessment to different RL algorithms for heparin treatment policy of patients with sepsis in ICU},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robot assisted fetoscopic laser coagulation: Improvements in
navigation, re-location and coagulation. <em>ARTMED</em>, <em>147</em>,
102725. (<a href="https://doi.org/10.1016/j.artmed.2023.102725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fetoscopic Laser Coagulation (FLC) for Twin to Twin Transfusion Syndrome is a challenging intervention due to the working conditions: low quality images acquired from a 3 mm fetoscope inside a turbid liquid environment, local view of the placental surface, unstable surgical field and delicate tissue layers. FLC is based on locating, coagulating and reviewing anastomoses over the placenta’s surface. The procedure demands the surgeons to generate a mental map of the placenta with the distribution of the anastomoses, maintaining, at the same time, precision in coagulation and protecting the placenta and amniotic sac from potential damages. This paper describes a teleoperated platform with a cognitive-based control that provides assistance to improve patient safety and surgery performance during fetoscope navigation, target re-location and coagulation processes. A comparative study between manual and teleoperated operation, executed in dry laboratory conditions, analyzes basic fetoscopic skills: fetoscope navigation and laser coagulation. Two exercises are proposed: first, fetoscope guidance and precise coagulation. Second, a resolved placenta (all anastomoses are indicated) to evaluate navigation, re-location and coagulation. The results are analyzed in terms of economy of movement, execution time , coagulation accuracy, amount of coagulated placental surface and risk of placenta puncture. In addition, new metrics, based on navigation and coagulation maps evaluate robotic performance. The results validate the developed platform, showing noticeable improvements in all the metrics.},
  archive      = {J_ARTMED},
  author       = {Albert Hernansanz and Johanna Parra and Narcís Sayols and Elisenda Eixarch and Eduard Gratacós and Alícia Casals},
  doi          = {10.1016/j.artmed.2023.102725},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102725},
  shortjournal = {Artif. Intell. Med.},
  title        = {Robot assisted fetoscopic laser coagulation: Improvements in navigation, re-location and coagulation},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Human vs machine towards neonatal pain assessment: A
comprehensive analysis of the facial features extracted by health
professionals, parents, and convolutional neural networks.
<em>ARTMED</em>, <em>147</em>, 102724. (<a
href="https://doi.org/10.1016/j.artmed.2023.102724">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neonates are not able to verbally communicate pain, hindering the correct identification of this phenomenon. Several clinical scales have been proposed to assess pain, mainly using the facial features of the neonate, but a better comprehension of these features is yet required, since several related works have shown the subjectivity of these scales. Meanwhile, computational methods have been implemented to automate neonatal pain assessment and, although performing accurately, these methods still lack the interpretability of the corresponding decision-making processes. To address this issue, we propose in this work a facial feature extraction framework to gather information and investigate the human and machine neonatal pain assessments, comparing the visual attention of the facial features perceived by health-professionals and parents of neonates with the most relevant ones extracted by eXplainable Artificial Intelligence (XAI) methods, considering the VGG-Face and N-CNN deep learning architectures. Our experimental results show that the information extracted by the computational methods are clinically relevant to neonatal pain assessment, but yet do not agree with the facial visual attention of health-professionals and parents, suggesting that humans and machines can learn from each other to improve their decision-making processes. We believe that these findings might advance our understanding of how humans and machines code and decode neonatal facial responses to pain, enabling further improvements in clinical scales widely used in practical situations and in face-based automatic pain assessment tools as well.},
  archive      = {J_ARTMED},
  author       = {Lucas Pereira Carlini and Gabriel de Almeida Sá Coutrin and Leonardo Antunes Ferreira and Juliana do Carmo Azevedo Soares and Giselle Valério Teixeira Silva and Tatiany Marcondes Heiderich and Rita de Cássia Xavier Balda and Marina Carvalho de Moraes Barros and Ruth Guinsburg and Carlos Eduardo Thomaz},
  doi          = {10.1016/j.artmed.2023.102724},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102724},
  shortjournal = {Artif. Intell. Med.},
  title        = {Human vs machine towards neonatal pain assessment: A comprehensive analysis of the facial features extracted by health professionals, parents, and convolutional neural networks},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple mask and boundary scoring r-CNN with cGAN data
augmentation for bladder tumor segmentation in WLC videos.
<em>ARTMED</em>, <em>147</em>, 102723. (<a
href="https://doi.org/10.1016/j.artmed.2023.102723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic diagnosis systems capable of handling multiple pathologies are essential in clinical practice. This study focuses on enhancing precise lesion localization, classification and delineation in transurethral resection of bladder tumor (TURBT) to reduce cancer recurrence. Despite deep learning models success, medical applications face challenges like small and limited datasets and poor image characterization, including the absence lack of color/texture modeling. To address these issues, three solutions are proposed: (1) an improved texture-constrained version of the pix2pixHD cGAN for data augmentation , addressing the tradeoff of generating high-quality images with enough stochasticity using the Fréchet Inception Distance (FID) measure. (2) Introducing the Multiple Mask and Boundary Scoring R-CNN (MM&amp;BS R-CNN), a new mask sub-net scheme where multiple masks are generated from the different levels of the mask sub-net pipeline, improving segmentation accuracy by including a new scoring module to refine object boundaries. (3) A novel accelerated training strategy based on the SGD optimizer with the second momentum. Experimental results show significant mAP improvements: the data generation scheme improves by more than 12 %; MM&amp;BS R-CNN proposed architecture is responsible for an improvement of about 1.25 %, and the training algorithm based on the second-order momentum increases mAP by 2–3 %. The simultaneous use of all three proposals improved the state-of-the-art mAP by 17.44 %.},
  archive      = {J_ARTMED},
  author       = {Nuno R. Freitas and Pedro M. Vieira and Catarina Tinoco and Sara Anacleto and Jorge F. Oliveira and A. Ismael F. Vaz and M. Pilar Laguna and Estêvão Lima and Carlos S. Lima},
  doi          = {10.1016/j.artmed.2023.102723},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102723},
  shortjournal = {Artif. Intell. Med.},
  title        = {Multiple mask and boundary scoring R-CNN with cGAN data augmentation for bladder tumor segmentation in WLC videos},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting stroke outcome: A case for multimodal deep
learning methods with tabular and CT perfusion data. <em>ARTMED</em>,
<em>147</em>, 102719. (<a
href="https://doi.org/10.1016/j.artmed.2023.102719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute ischemic stroke is one of the leading causes of morbidity and disability worldwide, often followed by a long rehabilitation period. To improve and personalize stroke rehabilitation, it is essential to provide a reliable prognosis to caregivers and patients. Deep learning techniques might improve the predictions by incorporating different data modalities. We present a multimodal approach to predict the functional status of acute ischemic stroke patients after their discharge based on tabular data and CT perfusion imaging . We conducted experiments on tabular, imaging, and multimodal deep learning architectures to predict dichotomized mRS scores 3 months after the event. The dataset was collected from a Dutch hospital and includes 98 CVA patients with a visible occlusion on their CT perfusion scan. Tabular data is based on the Dutch Acute Stroke Audit data, and imaging data consists of summed-up CT perfusion maps. On the tabular data, TabNet outperformed our baselines with an AUC of 0.71, while ResNet-10 on the imaging data performed comparably with an AUC of 0.70. Our implementation of the multimodal DAFT architecture outperforms baselines as well as comparable studies by achieving an 0.75 AUC, and 0.80 F1 score. This was achieved with a final model of less than a hundred thousand optimizable parameters, and a dataset less than half the size of reference papers. Overall, we demonstrate the feasibility of predicting the functional outcome for ischemic stroke patients and the usability of multimodal deep learning architectures for this purpose.},
  archive      = {J_ARTMED},
  author       = {Balázs Borsos and Corinne G. Allaart and Aart van Halteren},
  doi          = {10.1016/j.artmed.2023.102719},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102719},
  shortjournal = {Artif. Intell. Med.},
  title        = {Predicting stroke outcome: A case for multimodal deep learning methods with tabular and CT perfusion data},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A few-shot disease diagnosis decision making model based on
meta-learning for general practice. <em>ARTMED</em>, <em>147</em>,
102718. (<a href="https://doi.org/10.1016/j.artmed.2023.102718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnostic errors have become the biggest threat to the safety of patients in primary health care. General practitioners, as the “gatekeepers” of primary health care, have a responsibility to accurately diagnose patients. However, many general practitioners have insufficient knowledge and clinical experience in some diseases. Clinical decision making tools need to be developed to effectively improve the diagnostic process in primary health care. The long-tailed class distributions of medical datasets are challenging for many popular decision making models based on deep learning , which have difficulty predicting few-shot diseases. Meta-learning is a new strategy for solving few-shot problems. In this study, a few-shot disease diagnosis decision making model based on a model-agnostic meta-learning algorithm (FSDD-MAML) is proposed. The MAML algorithm is applied in a knowledge graph-based disease diagnosis model to find the optimal model parameters. Moreover, FSDD-MAML can learn learning rates for all modules of the knowledge graph-based disease diagnosis model. For n n -way, k k -shot learning tasks, the inner loop of FSDD-MAML performs multiple gradient update steps to learn internal features in disease classification tasks using n × k n×k examples, and the outer loop of FSDD-MAML optimizes the meta-objective to find the associated optimal parameters and learning rates. FSDD-MAML is compared with the original knowledge graph-based disease diagnosis model and other meta-learning algorithms based on an abdominal disease dataset. Meta-learning algorithms can greatly improve the performance of models in top-1 evaluation compared with top-3, top-5, and top-10 evaluations. The proposed decision making model FSDD-MAML outperforms all the other models, with a precision@1 of 90.02 %. We achieve state-of-the-art performance in the diagnosis of all diseases, and the prediction performance for few-shot diseases is greatly improved. For the two groups with the fewest examples of diseases, FSDD-MAML achieves relative increases in precision@1 of 29.13 % and 21.63 % compared with the original knowledge graph-based disease diagnosis model. In addition, we analyze the reasoning process of several few-shot disease predictions and provide an explanation for the results. The decision making model based on meta-learning proposed in this paper can support the rapid diagnosis of diseases in general practice and is especially capable of helping general practitioners diagnose few-shot diseases. This study is of profound significance for the exploration and application of meta-learning to few-shot disease assessment in general practice.},
  archive      = {J_ARTMED},
  author       = {Qianghua Liu and Yu Tian and Tianshu Zhou and Kewei Lyu and Ran Xin and Yong Shang and Ying Liu and Jingjing Ren and Jingsong Li},
  doi          = {10.1016/j.artmed.2023.102718},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102718},
  shortjournal = {Artif. Intell. Med.},
  title        = {A few-shot disease diagnosis decision making model based on meta-learning for general practice},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Depression detection for twitter users using sentiment
analysis in english and arabic tweets. <em>ARTMED</em>, <em>147</em>,
102716. (<a href="https://doi.org/10.1016/j.artmed.2023.102716">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since depression often results in suicidal thoughts and leaves a person severely disabled daily, there is an elevated risk of premature mortality due to mental problems caused by depression. Therefore, it&#39;s crucial to identify the patient&#39;s mental illness as soon as possible. People are increasingly using social media platforms to express their opinions and share daily activities, which makes online platforms rich sources of early depression detection. The contribution of this paper is multifold. First , it presents five machine-learning models for Arabic and English depression detection using Twitter text. The best model for Arabic text achieved an f1-score of 96.6 % for binary classification to depressed and Non_dep. For English text without negation, the model achieved 92 % for binary classification and 88 % for multi-classification (depressed, indifferent, happy). For English text with negation, an 87 %, and 85 % f1 score was achieved for binary and multi-classification respectively. Second , the work introduced a manually annotated Arabic_Dep_tweets_10,000 corpus of 10.000 Arabic tweets, which covered neutral tweets as well as a variety of depressed and happy terms. In addition, two automatically annotated English corpora, Eng_without_negation_60.000 corpus of 60,172 English tweets and Eng_with_negation_57.000 corpus of 57,392 English tweets. Both covered a wide range of depressed and cheerful terms; however, Negation was included in the Eng_with_negation_57.000 corpus. Finally , this paper exposes a depression-detection web application which implements our optimal models to detect tweets that contain depression symptoms and predict depression trends for a person either using English or Arabic language .},
  archive      = {J_ARTMED},
  author       = {AbdelMoniem Helmy and Radwa Nassar and Nagy Ramdan},
  doi          = {10.1016/j.artmed.2023.102716},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102716},
  shortjournal = {Artif. Intell. Med.},
  title        = {Depression detection for twitter users using sentiment analysis in english and arabic tweets},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimisation-based modelling for explainable lead discovery
in malaria. <em>ARTMED</em>, <em>147</em>, 102700. (<a
href="https://doi.org/10.1016/j.artmed.2023.102700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The search for new antimalarial treatments is urgent due to growing resistance to existing therapies. The Open Source Malaria (OSM) project offers a promising starting point, having extensively screened various compounds for their effectiveness. Further analysis of the chemical space surrounding these compounds could provide the means for innovative drugs. We report an optimisation-based method for quantitative structure–activity relationship (QSAR) modelling that provides explainable modelling of ligand activity through a mathematical programming formulation. The methodology is based on piecewise regression principles and offers optimal detection of breakpoint features, efficient allocation of samples into distinct sub-groups based on breakpoint feature values, and insightful regression coefficients . Analysis of OSM antimalarial compounds yields interpretable results through rules generated by the model that reflect the contribution of individual fingerprint fragments in ligand activity prediction. Using knowledge of fragment prioritisation and screening of commercially available compound libraries, potential lead compounds for antimalarials are identified and evaluated experimentally via a Plasmodium falciparum asexual growth inhibition assay (PfGIA) and a human cell cytotoxicity assay . Three compounds are identified as potential leads for antimalarials using the methodology described above. This work illustrates how explainable predictive models based on mathematical optimisation can pave the way towards more efficient fragment-based lead discovery as applied in malaria.},
  archive      = {J_ARTMED},
  author       = {Yutong Li and Jonathan Cardoso-Silva and John M. Kelly and Michael J. Delves and Nicholas Furnham and Lazaros G. Papageorgiou and Sophia Tsoka},
  doi          = {10.1016/j.artmed.2023.102700},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102700},
  shortjournal = {Artif. Intell. Med.},
  title        = {Optimisation-based modelling for explainable lead discovery in malaria},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Understanding the factors influencing acceptability of AI in
medical imaging domains among healthcare professionals: A scoping
review. <em>ARTMED</em>, <em>147</em>, 102698. (<a
href="https://doi.org/10.1016/j.artmed.2023.102698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) technology has the potential to transform medical practice within the medical imaging industry and materially improve productivity and patient outcomes. However, low acceptability of AI as a digital healthcare intervention among medical professionals threatens to undermine user uptake levels, hinder meaningful and optimal value-added engagement, and ultimately prevent these promising benefits from being realised. Understanding the factors underpinning AI acceptability will be vital for medical institutions to pinpoint areas of deficiency and improvement within their AI implementation strategies. This scoping review aims to survey the literature to provide a comprehensive summary of the key factors influencing AI acceptability among healthcare professionals in medical imaging domains and the different approaches which have been taken to investigate them. A systematic literature search was performed across five academic databases including Medline, Cochrane Library, Web of Science, Compendex, and Scopus from January 2013 to September 2023. This was done in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines. Overall, 31 articles were deemed appropriate for inclusion in the scoping review. The literature has converged towards three overarching categories of factors underpinning AI acceptability including: user factors involving trust, system understanding, AI literacy, and technology receptiveness; system usage factors entailing value proposition, self-efficacy, burden, and workflow integration; and socio-organisational-cultural factors encompassing social influence, organisational readiness, ethicality, and perceived threat to professional identity. Yet, numerous studies have overlooked a meaningful subset of these factors that are integral to the use of medical AI systems such as the impact on clinical workflow practices, trust based on perceived risk and safety, and compatibility with the norms of medical professions . This is attributable to reliance on theoretical frameworks or ad-hoc approaches which do not explicitly account for healthcare-specific factors, the novelties of AI as software as a medical device (SaMD), and the nuances of human-AI interaction from the perspective of medical professionals rather than lay consumer or business end users. This is the first scoping review to survey the health informatics literature around the key factors influencing the acceptability of AI as a digital healthcare intervention in medical imaging contexts. The factors identified in this review suggest that existing theoretical frameworks used to study AI acceptability need to be modified to better capture the nuances of AI deployment in healthcare contexts where the user is a healthcare professional influenced by expert knowledge and disciplinary norms. Increasing AI acceptability among medical professionals will critically require designing human-centred AI systems which go beyond high algorithmic performance to consider accessibility to users with varying degrees of AI literacy, clinical workflow practices, the institutional and deployment context, and the cultural, ethical, and safety norms of healthcare professions. As investment into AI for healthcare increases, it would be valuable to conduct a systematic review and meta-analysis of the causal contribution of these factors to achieving high levels of AI acceptability among medical professionals.},
  archive      = {J_ARTMED},
  author       = {David Hua and Neysa Petrina and Noel Young and Jin-Gun Cho and Simon K. Poon},
  doi          = {10.1016/j.artmed.2023.102698},
  journal      = {Artificial Intelligence in Medicine},
  month        = {1},
  pages        = {102698},
  shortjournal = {Artif. Intell. Med.},
  title        = {Understanding the factors influencing acceptability of AI in medical imaging domains among healthcare professionals: A scoping review},
  volume       = {147},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
