<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>WSARAI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="wsarai---11">WSARAI - 11</h2>
<ul>
<li><details>
<summary>
(2024). Efficient 3D representation learning for medical image
analysis. <em>WSARAI</em>, <em>2</em>, 2450002. (<a
href="https://doi.org/10.1142/S2811032324500024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning approaches have significantly advanced the 3D medical images analysis, such as the CT and MRI scans, which enables improved diagnosis and treatment evaluation. These image volumes provide rich spatial context for understanding the internal brain and body anatomies. Typical medical image analysis tasks, such as segmentation, reconstruction and registration, are essential for characterizing this context. Related to 3D data formats, meshes, point clouds and others are used to represent the anatomical structures, each with unique applications. To better capture the spatial information and address data scarcity, self- and semi-supervised learning methods have emerged. However, efficient 3D representation learning remains challenging. Recently, Transformers have shown promise, leveraging the self-attention mechanisms that perform well on transfer learning and self-supervised methods. These techniques are applied for medical domains without extensive manual labeling. This work explores data-efficient models, scalable deep learning, semantic context utilization and transferability in 3D medical image analysis. We also evaluated the foundational models, self-supervised pre- training, transfer learning and prompt tuning, thus advancing this critical field.},
  archive      = {J_WSARAI},
  author       = {Yucheng Tang and Jie Liu and Zongwei Zhou and Xin Yu and Yuankai Huo},
  doi          = {10.1142/S2811032324500024},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2450002},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Efficient 3D representation learning for medical image analysis},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust structured declarative classifiers for point clouds.
<em>WSARAI</em>, <em>2</em>, 2450001. (<a
href="https://doi.org/10.1142/S2811032324500012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks for 3D point cloud classification, such as PointNet, have been demonstrated to be vulnerable to adversarial attacks. Current adversarial defenders often learn to denoise the (attacked) point clouds by reconstruction, and then feed them to the classifiers as input. In contrast to the literature, we propose a novel bilevel optimization framework for robust point cloud classification, where the internal optimization can effectively defend the adversarial attacks as denoising and the external optimization can learn the classifiers accordingly. As a demonstration, we further propose an effective and efficient instantiation of our approach, namely, Lattice Point Classifier (LPC), based on structured sparse coding in the permutohedral lattice and 2D convolutional neural networks (CNNs) that integrates both internal and external optimization problems into end-to-end trainable network architectures. We demonstrate state-of-the-art robust point cloud classification performance on ModelNet40 and ScanNet under seven different attackers. Our demo code is available at: https://github.com/Zhang-VISLab .},
  archive      = {J_WSARAI},
  author       = {Ziming Zhang and Kaidong Li and Guanghui Wang},
  doi          = {10.1142/S2811032324500012},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2450001},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Robust structured declarative classifiers for point clouds},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sampling strategies for efficient segmentation and object
detection of 3D point clouds. <em>WSARAI</em>, <em>2</em>, 2440007. (<a
href="https://doi.org/10.1142/S2811032324400071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key aspect of efficient point cloud processing is sampling, which significantly reduces computational overhead, memory requirements, and redundancy. In this paper, we first present a comprehensive review and in-depth analysis of existing representative heuristic-based and learning-based sampling methods, highlighting their merits and limitations. Subsequently, we introduce an efficient semantic segmentation method and an object detection method for large-scale 3D point clouds containing millions of points, with the point cloud sampling strategy serving as a crucial component. Through a detailed examination of the RandLA-Net and IA-SSD algorithms, we aim to provide valuable insights for future research and development in this domain. Our overarching goal is to advance the efficiency, accuracy, and scalability of point cloud processing techniques, ultimately benefiting a wide range of applications across various domains.},
  archive      = {J_WSARAI},
  author       = {Qingyong Hu},
  doi          = {10.1142/S2811032324400071},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440007},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Sampling strategies for efficient segmentation and object detection of 3D point clouds},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Label-efficient point cloud semantic segmentation: A
holistic active learning approach. <em>WSARAI</em>, <em>2</em>, 2440006.
(<a href="https://doi.org/10.1142/S281103232440006X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning models are the state of the art for semantic segmentation of point clouds, the success of which relies on the availability of large-scale annotated datasets. However, it can be prohibitively costly to prepare such datasets. In this work, we propose a holistic active learning (AL) approach to maximize model performance given limited annotation budgets. We investigate the appropriate sample granularity for active selection under the realistic “click” measurement of annotation cost, and demonstrate that superpoint-based selection allows for most efficient usage of the limited budget, when compared with point-level, polygon-level and instance/shape-level selection. We further propose new objective for AL acquisition function and exploit local consistency constraints to boost the performance of our superpoint-based approach. We evaluate our methods on three benchmark datasets, ShapeNet and PartNet and S3DIS. The results demonstrate that AL is an effective strategy to address the high annotation costs in semantic point cloud segmentation.},
  archive      = {J_WSARAI},
  author       = {Xian Shi and Lile Cai and Ke Chen and Chuan Sheng Foo and Kui Jia and Xun Xu},
  doi          = {10.1142/S281103232440006X},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440006},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Label-efficient point cloud semantic segmentation: A holistic active learning approach},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). You only need one thing one click: Self-training for weakly
supervised 3D scene understanding. <em>WSARAI</em>, <em>2</em>, 2440005.
(<a href="https://doi.org/10.1142/S2811032324400058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding 3D scenes, such as semantic segmentation and instance identification within point clouds, typically demands extensive annotated datasets. However, generating point-by-point labels is an overly laborious process. While recent techniques have been developed to train 3D networks with a minimal fraction of labeled points, our method, dubbed “One Thing One Click,” simplifies this by requiring just a single label per object. To effectively utilize these sparse annotations during network training, we’ve crafted an innovative self-training strategy. This involves alternating between training phases and label spreading, powered by a graph propagation module. Additionally, we integrate a relation network to create category-specific prototypes, improving pseudo label accuracy and steering the training process. Our approach also seamlessly integrates with 3D instance segmentation, incorporating a point-clustering technique. Our method demonstrates superior performance over other weakly supervised strategies for 3D semantic and instance segmentation, as evidenced by tests on both ScanNet-v2 and S3DIS datasets. Remarkably, the efficacy of our self-training method with limited annotations rivals that of fully supervised models. Codes and models are available at https://github.com/liuzhengzhe/One-Thing-One-Click .},
  archive      = {J_WSARAI},
  author       = {Zhengzhe Liu and Xiaojuan Qi and Chi-Wing Fu},
  doi          = {10.1142/S2811032324400058},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440005},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {You only need one thing one click: Self-training for weakly supervised 3D scene understanding},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). EDiGS: Extended divergence-guided shape implicit neural
representation for unoriented point clouds. <em>WSARAI</em>, <em>2</em>,
2440004. (<a href="https://doi.org/10.1142/S2811032324400046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new approach for learning shape implicit neural representations (INRs) from point cloud data that do not require normal vectors as input. We show that our method, which uses a soft constraint on the divergence of the distance function to the shape’s surface, can produce smooth solutions that accurately orient gradients to match the unknown normal at each point, even outperforming methods that use normal vectors directly. This work extends the latest work on divergence-guided sinusoidal activation INRs [Y. Ben-Shabat, C. H. Koneputugodage and S. Gould, Proc IEEE/CVF Conf Computer Vision and Pattern Recognition, 2022, pp. 19323–19332], to Gaussian activation INRs and provides extended theoretical analysis and results. We evaluate our approach on tasks related to surface reconstruction and shape space learning.},
  archive      = {J_WSARAI},
  author       = {Yizhak Ben-Shabat and Chamin Hewa Koneputugodage and Stephen Gould},
  doi          = {10.1142/S2811032324400046},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440004},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {EDiGS: Extended divergence-guided shape implicit neural representation for unoriented point clouds},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving monocular 3D object detection by synthetic images
with virtual depth. <em>WSARAI</em>, <em>2</em>, 2440003. (<a
href="https://doi.org/10.1142/S2811032324400034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploiting geometric features is a common approach to enhance monocular 3D object detection. However, their performance is limited due to the absence of depth information. To address this limitation, an external depth estimator can be employed to predict depth, but this approach significantly reduces the efficiency and flexibility of the model. Instead of relying on a costly depth estimator, we propose a depth-aware monocular 3D object detector that is trained using augmented training data. Specifically, we utilize reference images and their corresponding depth maps to train an efficient rendering module, which synthesizes a variety of photo-realistic images with different virtual depths. By learning from these images, the detector adapts its features to depth variations. Furthermore, we introduce an auxiliary module that guides the network to learn more informative representations from the depth images. Both modules are removed after training, resulting in no additional computational overhead during the final deployment.},
  archive      = {J_WSARAI},
  author       = {Chenhang He and Lei Zhang},
  doi          = {10.1142/S2811032324400034},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440003},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Improving monocular 3D object detection by synthetic images with virtual depth},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI-based 3D metrology and defect detection of HBMs in XRM
scans. <em>WSARAI</em>, <em>2</em>, 2440002. (<a
href="https://doi.org/10.1142/S2811032324400022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we employ the latest developments in 3D semi-supervised learning to create cutting-edge deep learning models for 3D object detection and segmentation of buried structures in high-resolution X-ray semiconductor scans. We illustrate our approach to locating the region of interest of High Bandwidth Memory (HBM) structures and their individual components and identifying various defects. We showcase how semi-supervised learning is utilized to capitalize on the vast amounts of available unlabeled data to enhance both detection and segmentation performance. Additionally, we explore the benefits of contrastive learning in the data pre-selection for our detection model and multi-scale Mean-Teacher training paradigm in 3D semantic segmentation to achieve better performance compared to the state of the art. We also provide an objective comparison for metrology-based defect detection with a 3D classification network. Our extensive experiments have shown that our approach outperforms the state of the art by up to 16% on object detection and 7.8% on semantic segmentation. Our fully-automated custom metrology package shows a mean error of less than 2 μ m for key features such as bond line thickness and provides better defect detection performance than the direct 3D classification approach. Overall, our method achieves state-of-the-art performance and can be used to improve the accuracy and efficiency of a wide range of failure analysis applications in semiconductor manufacturing. Finally, we also increase the segmentation models flexibility and adaptability to new data. We propose a generic training strategy and a new loss function that reduces the training time by 60% and the required amount of data by 48% making the training process more efficient.},
  archive      = {J_WSARAI},
  author       = {Richard Chang and Wang Jie and Namrata Thakur and Ramanpreet Singh Pahwa},
  doi          = {10.1142/S2811032324400022},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440002},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {AI-based 3D metrology and defect detection of HBMs in XRM scans},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Masked autoencoders for 3D point cloud self-supervised
learning. <em>WSARAI</em>, <em>2</em>, 2440001. (<a
href="https://doi.org/10.1142/S2811032324400010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning methods are able to learn latent features from unlabeled data samples by designing pre-text tasks, and hence have attracted a great deal of interest in terms of sample-efficient learning. As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision, but has not been introduced to point cloud yet. In this paper, we propose a novel scheme of masked autoencoders for 3D point cloud self-supervised learning, addressing the special challenges posed by point cloud, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer-based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. Apart from the proposed method, we will also introduce potential directions for 3D point cloud self-supervised learning, including improvements for masked autoencoding, developments for point cloud scene understanding, etc.},
  archive      = {J_WSARAI},
  author       = {Yatian Pang and Eng Hock Francis Tay and Li Yuan and Zhenghua Chen},
  doi          = {10.1142/S2811032324400010},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2440001},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Masked autoencoders for 3D point cloud self-supervised learning},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Introduction to 3D deep learning. <em>WSARAI</em>,
<em>2</em>, 2430001. (<a
href="https://doi.org/10.1142/S2811032324300019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D deep learning, i.e., deep learning with 3D data, has been attracting increasing attention due to its tremendous research values and broad real-world applications. In this paper, we first provide an overview of 3D data, its unique characteristics, and the challenges associated, as well as the wide range of applications of 3D data in various fields. We then introduce the fundamentals of 3D deep learning, including the basic concepts, the key technical challenges, the recent developments, and the emerging real-world applications of 3D deep learning.},
  archive      = {J_WSARAI},
  author       = {Xulei Yang and Xiao Li and Hao Su},
  doi          = {10.1142/S2811032324300019},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2430001},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Introduction to 3D deep learning},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithm-system-hardware co-design for efficient 3D deep
learning. <em>WSARAI</em>, <em>2</em>, 2340003. (<a
href="https://doi.org/10.1142/S2811032323400039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D point cloud deep learning has received significant attention due to its wide applications, such as augmented/mixed reality, autonomous vehicles/drones, and robots. Despite its remarkable accuracy, the computational cost and sparse nature of 3D point cloud deep learning models greatly hinder their use in real-world, latency-sensitive applications. In this paper, we develop efficient algorithms, systems, and hardware for 3D deep learning to overcome the computational challenges (especially sparsity) of 3D point cloud data, making it more applicable in a broader range of real-world scenarios. From the algorithm perspective, we introduce novel 3D building blocks (PVCNN and SPVNAS ∗ , https://pvnas.mit.edu ) that are tailor-made for point cloud data to reduce sparse and irregular overheads. On the system level, we develop a high-performance library (TorchSparse ∗ , https://torchsparse.mit.edu ) that can handle sparse and irregular workloads on general-purpose hardware, which is typically optimized for dense and regular workloads. Furthermore, we develop a specialized hardware accelerator (PointAcc ∗ , https://pointacc.mit.edu ) that can support various types of point cloud deep learning primitives and mitigate memory bottlenecks for sparse point cloud computing.},
  archive      = {J_WSARAI},
  author       = {Zhijian Liu and Haotian Tang and Yujun Lin and Song Han},
  doi          = {10.1142/S2811032323400039},
  journal      = {World Scientific Annual Review of Artificial Intelligence},
  pages        = {2340003},
  shortjournal = {World Sci. Ann. Rev. Artif. Intell.},
  title        = {Algorithm-system-hardware co-design for efficient 3D deep learning},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
