<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJAIRR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijairr---10">IJAIRR - 10</h2>
<ul>
<li><details>
<summary>
(2024). Vision-guided grasping policy learning from demonstrations
for robotic manipulators. <em>IJAIRR</em>, <em>1</em>(2), 2450006. (<a
href="https://doi.org/10.1142/S2972335324500066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of robotics into domestic environments poses significant challenges due to the dynamic and varied nature of these settings. This paper introduces a new framework that combines vision-guided object recognition with adaptive grasping policies learned from human demonstrations. By harnessing computer vision technology, our system employs deep learning algorithms, particularly Convolutional Neural Networks (CNNs), to precisely detect and classify household objects. Simultaneously, the system uses imitation learning to refine grasping policies, enabling the robotic manipulator to dynamically adapt to new target objects. We validated our framework through a series of experimental setups that simulate typical kitchen tasks, such as manipulating utensils and preparing ingredients. These tasks, which primarily involve picking up and placing objects, served as practical tests for our system. The results demonstrate the system’s ability to effectively recognize a broad array of objects and adapt its grasping policies, thereby enhancing operational efficiency.},
  archive      = {J_IJAIRR},
  author       = {Lei Jiang and Feiyan Wang and Yueyue Liu},
  doi          = {10.1142/S2972335324500066},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {2},
  pages        = {2450006},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Vision-guided grasping policy learning from demonstrations for robotic manipulators},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Iterative prompt refinement for mining gene relationships
from ChatGPT. <em>IJAIRR</em>, <em>1</em>(2), 2450005. (<a
href="https://doi.org/10.1142/S2972335324500054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT has demonstrated its potential as a surrogate knowledge graph. Trained on extensive data sources, including open-access publications, peer-reviewed research articles, and biomedical websites, ChatGPT extracted information on gene relationships and biological pathways so that it can be used to predict them. However, a major challenge is model hallucination, that is, high false positive rates. To assess and address this challenge, we systematically evaluated ChatGPT’s capacity for predicting gene relationships using GPT-3.5-turbo, GPT-4, and GPT-4o. Benchmarking against the KEGG Pathway Database as the ground truth, we experimented with diverse prompting strategies, targeting gene relationships of activation, inhibition, and phosphorylation. We introduced an innovative iterative prompt refinement technique. By assessing prompt efficacy using metrics such as F-1 score, precision, and recall, GPT-4 suggested improved prompts. A refined prompt, which combines a specialized role with explanatory text, significantly enhanced the performance. Going beyond pairwise gene relationships, we also deciphered complex gene interplays, such as gene interaction chains and pathways pertinent to diseases such as non-small cell lung cancer. Direct prompts showed limited success, but “least-to-most” prompting exhibited significant potentials for such network constructions. The methods in this study may be used for other bioinformatics prediction problems.},
  archive      = {J_IJAIRR},
  author       = {Yibo Chen and Jeffrey Gao and Marius Petruc and Richard D. Hammer and Mihail Popescu and Dong Xu},
  doi          = {10.1142/S2972335324500054},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {2},
  pages        = {2450005},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Iterative prompt refinement for mining gene relationships from ChatGPT},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). TWOSOME: An efficient online framework to align LLMs with
embodied environments via reinforcement learning. <em>IJAIRR</em>,
<em>1</em>(2), 2450004. (<a
href="https://doi.org/10.1142/S2972335324500042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive performance across numerous tasks, Large Language Models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, Reinforcement Learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. First, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with LOw-Rank Adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. (i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. (ii) Benefiting from LLMs’ open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. (iii) Under our framework, there is no significant loss of the LLMs’ original ability during online PPO finetuning. a},
  archive      = {J_IJAIRR},
  author       = {Weihao Tan and Wentao Zhang and Shanqi Liu and Longtao Zheng and Xinrun Wang and Bo An},
  doi          = {10.1142/S2972335324500042},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {2},
  pages        = {2450004},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {TWOSOME: An efficient online framework to align LLMs with embodied environments via reinforcement learning},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rule by example: Harnessing logical rules for explainable
hate speech detection. <em>IJAIRR</em>, <em>1</em>(2), 2450003. (<a
href="https://doi.org/10.1142/S2972335324500030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classic approaches to content moderation typically apply a rule-based heuristic approach to flag content. While rules are easily customizable and intuitive for humans to interpret, they are inherently fragile and lack the flexibility or robustness needed to moderate the vast amount of undesirable content found online today. Recent advances in deep learning have demonstrated the promise of using highly effective deep neural models to overcome these challenges. However, despite the improved performance, these data-driven models lack transparency and explainability, often leading to mistrust from everyday users and a lack of adoption by many platforms. In this paper, we present Rule By Example (RBE): a novel exemplar-based contrastive learning approach for learning from logical rules for the task of textual content moderation. RBE is capable of providing rule-grounded predictions, allowing for more explainable and customizable predictions compared to typical deep learning-based approaches. We demonstrate that our approach is capable of learning rich rule embedding representations using only a few data examples. Experimental results on three popular hate speech classification datasets show that RBE is able to outperform state-of-the-art deep learning classifiers as well as the use of rules in both supervised and unsupervised settings while providing explainable model predictions via rule-grounding.},
  archive      = {J_IJAIRR},
  author       = {Christopher Clarke and Matthew Hall and Gaurav Mittal and Ye Yu and Sandra Sajeev and Jason Mars and Mei Chen},
  doi          = {10.1142/S2972335324500030},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {2},
  pages        = {2450003},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Rule by example: Harnessing logical rules for explainable hate speech detection},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consolidating trees of robotic plans generated using large
language models to improve reliability. <em>IJAIRR</em>, <em>1</em>(1),
2450002. (<a href="https://doi.org/10.1142/S2972335324500029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent probabilistic nature of Large Language Models (LLMs) introduces an element of unpredictability, raising concerns about potential discrepancies in their output. This paper presents a novel approach designed to generate correct and optimal robotic task plans for diverse real-world demands and scenarios. LLMs have been used to generate task plans, but they are unreliable and may contain wrong, questionable, or high-cost steps. The proposed approach uses LLM to generate a number of task plans as trees and amalgamates them into a graph by removing questionable paths. Then an optimal task tree can be retrieved to circumvent questionable and high-cost nodes, thereby improving planning accuracy and execution efficiency. The approach is further improved by incorporating a large knowledge network. Leveraging GPT-4 further, the high-level task plan is converted into a low-level Planning Domain Definition Language (PDDL) plan executable by a robot. Evaluation results highlight the superior accuracy and efficiency of our approach compared to previous methodologies in the field of task planning.},
  archive      = {J_IJAIRR},
  author       = {Md Sadman Sakib and Yu Sun},
  doi          = {10.1142/S2972335324500029},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2450002},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Consolidating trees of robotic plans generated using large language models to improve reliability},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multi-modal multi-channel american sign language
recognition. <em>IJAIRR</em>, <em>1</em>(1), 2450001. (<a
href="https://doi.org/10.1142/S2972335324500017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a machine learning-based multi-stream framework to recognize American Sign Language (ASL) manual signs and nonmanual gestures (face and head movements) in real time from RGB-D videos. Our approach is based on 3D Convolutional Neural Networks (3D CNNs) by fusing the multi-modal features including hand gestures, facial expressions, and body poses from multiple channels (RGB, Depth, Motion, and Skeleton joints). To learn the overall temporal dynamics in a video, a proxy video is generated by selecting a subset of frames for each video which are then used to train the proposed 3D CNN model. We collected a new ASL dataset, ASL-100-RGBD, which contains 42 RGB-D videos captured by a Microsoft Kinect V2 camera. Each video consists of 100 ASL manual signs, along with RGB channel, Depth maps, Skeleton joints, Face features, and HD face. The dataset is fully annotated for each semantic region (i.e. the time duration of each sign that the human signer performs). Our proposed method achieves 92.88% accuracy for recognizing 100 ASL sign glosses in our newly collected ASL-100-RGBD dataset. The effectiveness of our framework for recognizing hand gestures from RGB-D videos is further demonstrated on a large-scale dataset, ChaLearn IsoGD, achieving the state-of-the-art results.},
  archive      = {J_IJAIRR},
  author       = {Elahe Vahdani and Longlong Jing and Matt Huenerfauth and Yingli Tian},
  doi          = {10.1142/S2972335324500017},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2450001},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Multi-modal multi-channel american sign language recognition},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inaugural issue of the international journal of artificial
intelligence and robotics research (IJAIRR): The emergence of an
interdisciplinary nexus. <em>IJAIRR</em>, <em>1</em>(1), 2401002. (<a
href="https://doi.org/10.1142/S2972335324010026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the inaugural issue of the International Journal of Artificial Intelligence and Robotics Research (IJAIRR), I am honored to present an editorial that encapsulates the essence and ambition of this cutting-edge publication. IJAIRR emerges at a time when Artificial Intelligence and Robotics (AIR) are not merely technological novelties but fundamental drivers of progress across various scientific and practical domains. This journal aims to be at the forefront of documenting, analyzing, and guiding the interdisciplinary integration of AI, robotics, and fundamental sciences.},
  archive      = {J_IJAIRR},
  author       = {Yu Sun and Dong Xu and Xiaorui Zhu},
  doi          = {10.1142/S2972335324010026},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2401002},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Inaugural issue of the international journal of artificial intelligence and robotics research (IJAIRR): The emergence of an interdisciplinary nexus},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative AI for complex scenarios: Language models are
sequence processors. <em>IJAIRR</em>, <em>1</em>(1), 2401001. (<a
href="https://doi.org/10.1142/S2972335324010014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), exemplified by GPT-4, have transcended traditional boundaries in language processing, demonstrating remarkable capabilities in understanding and generating nuanced text. Crucially, these models are pioneering a paradigm shift in Artificial Intelligence (AI) applications — from solving narrowly defined problems to navigating complex, real-world scenarios. Such a shift is based on a simple and fundamental principle: LLMs can process any data that can be serialized and tokenized, enabling them to engage in multifaceted reasoning and utilize diverse tools. This capability positions LLMs to operate effectively in broader, more intricate contexts, marking a leap in AI’s practical applicability and potential.},
  archive      = {J_IJAIRR},
  author       = {Bowen Zhou and Ning Ding},
  doi          = {10.1142/S2972335324010014},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2401001},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Generative AI for complex scenarios: Language models are sequence processors},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic edge of chaos as guiding principle for neural
network training. <em>IJAIRR</em>, <em>1</em>(1), 2350001. (<a
href="https://doi.org/10.1142/S2972335323500011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been recently demonstrated that optimal neural networks operate near the asymptotic edge of chaos for state-of-the-art feed-forward neural networks, where its generalization power is maximal due to the highest number of asymptotic metastable states. However, how to leverage this principle to improve the model training process remains open. Here, by mapping the model evolution during training to the phase diagram in the classic analytic result of Sherrington–Kirkpatrick model in spin glasses, we illustrate on a simple neural network model that one can provide principled training of the network without manually tuning the training hyper-parameters. In particular, we provide a semi-analytical method to set the optimal weight decay strength, such that the model will converge toward the edge of chaos during training. Consequently, such hyper-parameter setting leads the model to achieve the highest test accuracy. Another benefit for restricting the model at the edge of chaos is its robustness against the common practical problem of label noise, as we find that it automatically avoids fitting the shuffled labels in the training samples while maintaining good fitting to the correct labels, providing simple means of achieving good performance on noisy labels without any additional treatment.},
  archive      = {J_IJAIRR},
  author       = {Lin Zhang and Ling Feng and Kan Chen and Choy Heng Lai},
  doi          = {10.1142/S2972335323500011},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2350001},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {Asymptotic edge of chaos as guiding principle for neural network training},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AI, thinking machines and a vast active living intelligent
system. <em>IJAIRR</em>, <em>1</em>(1), 2302001. (<a
href="https://doi.org/10.1142/S2972335323020015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJAIRR},
  author       = {Bud Mishra},
  doi          = {10.1142/S2972335323020015},
  journal      = {International Journal of Artificial Intelligence and Robotics Research},
  number       = {1},
  pages        = {2302001},
  shortjournal = {Int. J. Artif. Intell. Robot. Res.},
  title        = {AI, thinking machines and a vast active living intelligent system},
  volume       = {1},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
