<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDSMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijdsms---9">IJDSMS - 9</h2>
<ul>
<li><details>
<summary>
(2024). Average multiplicities. <em>IJDSMS</em>, <em>2</em>(2),
115–121. (<a href="https://doi.org/10.1142/S2810939224400045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note summarizes my talk at the ICERM workshop on Murmurations and Arithmetic, 6 June 2023 to 8 June 2023. The talk was based on the paper [14], to which the reader is referred for details and proofs. The two addenda at the end appeared neither in the talk nor in [14] but are complementary to both. It is a pleasure to thank ICERM and the organizers of the workshop for inviting me to participate in a meeting that was both enjoyable and productive.},
  archive      = {J_IJDSMS},
  author       = {David E. Rohrlich},
  doi          = {10.1142/S2810939224400045},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {115-121},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Average multiplicities},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applications of moments of dirichlet coefficients in
elliptic curve families. <em>IJDSMS</em>, <em>2</em>(2), 99–113. (<a
href="https://doi.org/10.1142/S2810939224400033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The moments of the coefficients of elliptic curve L -functions are related to numerous important arithmetic problems. Rosen and Silverman proved a conjecture of Nagao relating the first moment of one-parameter families satisfying Tate’s conjecture to the rank of the corresponding elliptic surface over ℚ ( T ) ; one can also construct families of moderate rank by finding families with large first moments. Michel proved that if j ( T ) is not constant, then the second moment of the family is of size p 2 + O ( p 3 / 2 ) ; these two moments show that for suitably small support the behavior of zeros near the central point agree with that of eigenvalues from random matrix ensembles, with the higher moments impacting the rate of convergence. In his thesis, Miller noticed a negative bias in the second moment of every one-parameter family of elliptic curves over ℚ whose second moment had a (by him) calculable closed-form expression, specifically the first lower-order term which does not average to zero is on average negative . This Bias Conjecture has now been confirmed for many families; however, these are highly non-generic families as they are specially chosen so that the resulting Legendre sums can be determined. For cohomological reasons, each subsequent term in the second moment expansion is smaller than the previous by a factor on the order of p , and thus numerically, it is hard to see a term of size p with a small negative average as it can be masked by a term of size p 3 / 2 which averages to zero. Inspired by the recent successes by Yang-Hui He, Kyu-Hwan Lee, Thomas Oliver, Alexey Pozdnyakov and others in investigations of murmurations of elliptic curve coefficients with machine learning techniques, we pose a similar problem for trying to understand the Bias Conjecture. As a start to this program, we numerically investigate the bias conjecture and provide a visual representation of the bias for the second moment. We find a one-parameter family of elliptic curves whose bias is positive for half the primes. However, the numerics do not offer conclusive evidence that negative bias for the other half is enough to overwhelm the positive bias. Without an explicit expansion for the second moment, we are not able to extract potential negative bias of the order p term.},
  archive      = {J_IJDSMS},
  author       = {Zoë Batterman and Aditya Jambhale and Steven J. Miller and Akash L. Narayanan and Kishan Sharma and Andrew Yang and Chris Yao},
  doi          = {10.1142/S2810939224400033},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {99-113},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Applications of moments of dirichlet coefficients in elliptic curve families},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PCA, arithmetic, and murmurations. <em>IJDSMS</em>,
<em>2</em>(2), 89–97. (<a
href="https://doi.org/10.1142/S2810939224400021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We outline how murmurations were first observed in the context of data scientific exploration. Specifically, we highlight the relationship between murmurations and PCA. Subsequently, we apply PCA to datasets of real quadratic fields, and speculate on an emergent perspective on arithmetic suggested by these experiments.},
  archive      = {J_IJDSMS},
  author       = {Thomas Oliver},
  doi          = {10.1142/S2810939224400021},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {2},
  pages        = {89-97},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {PCA, arithmetic, and murmurations},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Machine learning kronecker coefficients. <em>IJDSMS</em>,
<em>2</em>(1), 79–86. (<a
href="https://doi.org/10.1142/S2810939224400094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Kronecker coefficients are the decomposition multiplicities of the tensor product of two irreducible representations of the symmetric group. Unlike the Littlewood–Richardson coefficients, which are the analogues for the general linear group, there is no known combinatorial description of the Kronecker coefficients, and it is an NP-hard problem to decide whether a given Kronecker coefficient is zero or not. In this paper, we show that standard machine learning algorithms such as Nearest Neighbors, Convolutional Neural Networks and Gradient Boosting Decision Trees may be trained to predict whether a given Kronecker coefficient is zero or not. Our results show that a trained machine can efficiently perform this binary classification with high accuracy ( ≈ 0.98).},
  archive      = {J_IJDSMS},
  author       = {Kyu-Hwan Lee},
  doi          = {10.1142/S2810939224400094},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {79-86},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Machine learning kronecker coefficients},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using data as an input to parameterized polynomial systems.
<em>IJDSMS</em>, <em>2</em>(1), 63–77. (<a
href="https://doi.org/10.1142/S2810939224400082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameterized systems of polynomial equations arise in many applications including computer vision, chemistry, and kinematics. Numerical homotopy continuation methods are a fundamental technique within numerical algebraic geometry for both solving these polynomial systems and determining more refined information about their structure. Imperative to these solving methods is the use of data either synthetic or from the application itself, such as image pixel data for computer vision and leg length parameters for kinematics. This paper will describe projects that incorporate the use of data to find real solutions and/or the structure of real solutions for problems in applications. Illustrative examples are given to highlight various uses of data within computer vision, machine learning, and kinematics applications.},
  archive      = {J_IJDSMS},
  author       = {Margaret H. Regan},
  doi          = {10.1142/S2810939224400082},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {63-77},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Using data as an input to parameterized polynomial systems},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Murmurations of mestre–nagao sums. <em>IJDSMS</em>,
<em>2</em>(1), 51–61. (<a
href="https://doi.org/10.1142/S2810939224400070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the detection of the rank of elliptic curves with ranks 0 and 1, employing a heuristic known as the Mestre–Nagao sum Our observations reveal an oscillatory behavior in the sums, closely associated with the recently discovered phenomena of murmurations of elliptic curves [Y.-H. He, K.-H. Lee, T. Oliver and A. Pozdnyakov, Murmurations of elliptic curves, preprint (2022), arXiv:2204.10140.]. Surprisingly, this suggests that in some cases, opting for a smaller value of B yields a more accurate classification than choosing a larger one. For instance, when considering elliptic curves with conductors within the range of [ 4 0 0 0 0 , 4 5 0 0 0 ] , the rank classification based on a p ’s with p &lt; B = 3 2 0 0 produces better results compared to using B = 5 0 0 0 0 . This phenomenon finds partial explanation in the recent work of Zubrilina [Murmurations (2023)].},
  archive      = {J_IJDSMS},
  author       = {Zvonimir Bujanović and Matija Kazalicki and Lukas Novak},
  doi          = {10.1142/S2810939224400070},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {51-61},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Murmurations of Mestre–Nagao sums},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New calabi–yau manifolds from genetic algorithms.
<em>IJDSMS</em>, <em>2</em>(1), 39–49. (<a
href="https://doi.org/10.1142/S2810939224400069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From n -dimensional reflexive polytopes one can construct n − 1 complex-dimensional Calabi–Yau manifolds as hypersurfaces in toric varieties. In this contribution to the DANGER 3: Data Numbers and Geometry conference proceedings, we summarise previous work [P. Berglund, Y.-H. He, E. Heyes, E. Hirst, V. Jejjala and A. Lukas, New Calabi–Yau Manifolds from Genetic Algorithms (2023)] generating reflexive polytopes using genetic algorithms. As a proof of principle, we demonstrate that the genetic algorithm can reproduce known reflexive polytopes in two, three and four dimensions. Motivated by this result, we construct five-dimensional reflexive polytopes and establish that many of these are not in existing datasets and therefore give rise to new Calabi–Yau four-folds.},
  archive      = {J_IJDSMS},
  author       = {Elli Heyes},
  doi          = {10.1142/S2810939224400069},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {39-49},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {New Calabi–Yau manifolds from genetic algorithms},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting root numbers with neural networks.
<em>IJDSMS</em>, <em>2</em>(1), 15–37. (<a
href="https://doi.org/10.1142/S2810939224400057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we report on two machine learning experiments in search of statistical relationships between Dirichlet coefficients and root numbers or analytic ranks of certain low-degree L -functions. The first experiment is to construct interpretable models based on murmurations, a recently discovered correlation between Dirichlet coefficients and root numbers. We show experimentally that these models achieve high accuracy by learning a combination of Mestre–Nagao-type heuristics and murmurations, noting that the relative importance of these features varies with degree. The second experiment is to search for a low-complexity statistic of Dirichlet coefficients that can be used to predict root numbers in polynomial time. We give experimental evidence and provide heuristics that suggest this cannot be done with standard machine learning techniques.},
  archive      = {J_IJDSMS},
  author       = {Alexey Pozdnyakov},
  doi          = {10.1142/S2810939224400057},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {15-37},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Predicting root numbers with neural networks},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Calabi–yau links and machine learning. <em>IJDSMS</em>,
<em>2</em>(1), 3–14. (<a
href="https://doi.org/10.1142/S281093922440001X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calabi–Yau links are specific S 1 -fibrations over Calabi–Yau manifolds, when the link is 7-dimensional they exhibit both Sasakian and G2 structures. In this invited contribution to the DANGER proceedings, previous work exhaustively computing Calabi–Yau links and selected topological properties is summarized. Machine learning of these properties inspires new conjectures about their computation, as well as the respective Gröbner bases.},
  archive      = {J_IJDSMS},
  author       = {Edward Hirst},
  doi          = {10.1142/S281093922440001X},
  journal      = {International Journal of Data Science in the Mathematical Sciences},
  number       = {1},
  pages        = {3-14},
  shortjournal = {Int. J. Data Sci. Math. Sci.},
  title        = {Calabi–Yau links and machine learning},
  volume       = {2},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
