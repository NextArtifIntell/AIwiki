<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJOO_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijoo---12">IJOO - 12</h2>
<ul>
<li><details>
<summary>
(2024). Facilitating compromise in redistricting with transfer
distance midpoints. <em>IJOO</em>, <em>6</em>(3-4), 214–239. (<a
href="https://doi.org/10.1287/ijoo.2023.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {States in the United States redraw their electoral district boundaries every 10 years. This redistricting process can be contentious and has long-lasting consequences for political representation. To reduce bias in the redistricting process, some states require bipartisan commissions; however, bipartisan commissions can still involve partisan tension if the political parties cannot compromise. We propose an optimization framework to facilitate compromise between two redistricting stakeholders. This framework seeks a midpoint between two stakeholder plans with respect to a distance metric. A midpoint can help the stakeholders visualize a potential compromise that incorporates district structure common to both of their proposed plans. First, we consider multiple distance metrics and evaluate whether midpoints with respect to these metrics are achievable and align with redistricting requirements. Then we formulate a mixed-integer linear program to find a midpoint (or any fractional point) between two given plans with respect to the transfer distance. This formulation incorporates district structure from both given plans by fixing variables; consequently, it is possible to solve some realistically sized instances exactly in reasonable amounts of time. We present experiments on grid instances and Missouri’s congressional redistricting instance to demonstrate how this method can quickly generate compromise options that align with redistricting requirements. Funding: This material is based on work supported by the National Science Foundation Graduate Research Fellowship Program [Grant DGE-1746047]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2023.0029 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0029},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer-Fall},
  number       = {3-4},
  pages        = {214-239},
  shortjournal = {INFORMS J. Optim.},
  title        = {Facilitating compromise in redistricting with transfer distance midpoints},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximizing engagement in large-scale social networks.
<em>IJOO</em>, <em>6</em>(3-4), 196–213. (<a
href="https://doi.org/10.1287/ijoo.2022.0024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the importance of user engagement as a crucial element in cascading leaving of users from a social network, we study identifying a largest relaxed variant of a degree-based cohesive subgraph: the maximum anchored k -core problem. Given graph G = ( V , E ) and integers k and b , the maximum anchored k -core problem seeks to find a largest subset of vertices S ⊆ V that induces a subgraph with at least | S | − b vertices of degree at least k . We introduce a new integer programming (IP) formulation for the maximum anchored k -core problem and conduct a polyhedral study on the polytope of the problem. We show the linear programming relaxation of the proposed IP model is at least as strong as that of a naïve formulation. We also identify facet-defining inequalities of the IP formulation. Furthermore, we develop inequalities and fixing procedures to improve the computational performance of our IP model. We use benchmark instances to compare the computational performance of the IP model with (i) the naïve IP formulation and (ii) two existing heuristic algorithms. Our proposed IP model can optimally solve half of the benchmark instances that cannot be solved to optimality either by the naïve model or the existing heuristic approaches. Funding: This work is funded by the National Science Foundation (NSF) [Grant DMS-2318790] titled AMPS: Novel Combinatorial Optimization Techniques for Smartgrids and Power Networks. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2022.0024 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0024},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer-Fall},
  number       = {3-4},
  pages        = {196-213},
  shortjournal = {INFORMS J. Optim.},
  title        = {Maximizing engagement in large-scale social networks},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A stochastic inexact sequential quadratic optimization
algorithm for nonlinear equality-constrained optimization.
<em>IJOO</em>, <em>6</em>(3-4), 173–195. (<a
href="https://doi.org/10.1287/ijoo.2022.0008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stochastic algorithm is proposed, analyzed, and tested experimentally for solving continuous optimization problems with nonlinear equality constraints. It is assumed that constraint function and derivative values can be computed but that only stochastic approximations are available for the objective function and its derivatives. The algorithm is of the sequential quadratic optimization variety. Distinguishing features of the algorithm are that it only employs stochastic objective gradient estimates that satisfy a relatively weak set of assumptions (while using neither objective function values nor estimates of them) and that it allows inexact subproblem solutions to be employed, the latter of which is particularly useful in large-scale settings when the matrices defining the subproblems are too large to form and/or factorize. Conditions are imposed on the inexact subproblem solutions that account for the fact that only stochastic objective gradient estimates are employed. Convergence results are established for the method. Numerical experiments show that the proposed method vastly outperforms a stochastic subgradient method and can outperform an alternative sequential quadratic programming algorithm that employs highly accurate subproblem solutions in every iteration. Funding: This material is based upon work supported by the National Science Foundation [Awards CCF-1740796 and CCF-2139735] and the Office of Naval Research [Award N00014-21-1-2532].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0008},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer-Fall},
  number       = {3-4},
  pages        = {173-195},
  shortjournal = {INFORMS J. Optim.},
  title        = {A stochastic inexact sequential quadratic optimization algorithm for nonlinear equality-constrained optimization},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monte carlo fictitious play for finding pure nash equilibria
in identical interest games. <em>IJOO</em>, <em>6</em>(3-4), 155–172.
(<a href="https://doi.org/10.1287/ijoo.2021.0024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing equilibria in large-scale games is an important topic in many areas. One approach is to define a dynamic procedure such as fictitious play (FP) that converges to a mixed Nash equilibrium (NE) in identical interest games (among other classes) but suffers from exponential iteration complexity. Recent variants of FP reduce the computational burden, but many still do not guarantee convergence to a pure NE. We analyze a procedure—Monte Carlo fictitious play (MCFP)—that overcomes these limitations and efficiently discovers a pure NE in finite time with probability one in identical interest games. We also show a variant of MCFP finds a pure NE with optimal utility with probability one. Numerical results demonstrate the comparative performance of several variants of MCFP. Funding: This research was supported by the DARPA ARCOS program under contract #FA8750-20-C-0507 and under AFRL contract #FA8750-22-9-0001 and by NSF#2046588.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0024},
  journal      = {INFORMS Journal on Optimization},
  month        = {Summer-Fall},
  number       = {3-4},
  pages        = {155-172},
  shortjournal = {INFORMS J. Optim.},
  title        = {Monte carlo fictitious play for finding pure nash equilibria in identical interest games},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integer program for pricing support points of exact
barycenters. <em>IJOO</em>, <em>6</em>(2), 137–153. (<a
href="https://doi.org/10.1287/ijoo.2022.0028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computation of exact barycenters for a set of discrete measures is of interest in applications where sparse solutions are desired and to assess the quality of solutions returned by approximate algorithms and heuristics. The task is known to be NP-hard for growing dimension and, even in low dimensions, extremely challenging in practice due to an exponential scaling of the linear programming formulations associated with the search for sparse solutions. A common approach to facilitate practical computations is an approximation based on the choice of a small, fixed set of combinations of support points from the measures that may be assigned mass. Through classic integer programming techniques, we model an integer program to compute additional combinations of support points that, when added to the fixed set, allow for a better approximation of the underlying exact barycenter problem. The approach improves on the scalability of previous column generation approaches: instead of a pricing problem that has to evaluate exponentially many reduced cost values, we solve a mixed-integer program of quadratic size. The properties of the model and practical computations reveal a tailored branch-and-bound routine as a good solution strategy. Funding: S. Borgwardt was supported by the Air Force Office of Scientific Research [Grant FA9550-21-1-0233], the National Science Foundation [Grant 2006183], Algorithmic Foundations, Division of Computing and Communication Foundations, and the Simons Foundation [Grant 524210].},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0028},
  journal      = {INFORMS Journal on Optimization},
  month        = {Spring},
  number       = {2},
  pages        = {137-153},
  shortjournal = {INFORMS J. Optim.},
  title        = {An integer program for pricing support points of exact barycenters},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). First-order algorithms without lipschitz gradient: A
sequential local optimization approach. <em>IJOO</em>, <em>6</em>(2),
118–136. (<a href="https://doi.org/10.1287/ijoo.2021.0029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most first-order methods rely on the global Lipschitz continuity of the objective gradient, which fails to hold in many problems. This paper develops a sequential local optimization (SLO) framework for first-order algorithms to optimize problems without Lipschitz gradient. Operating on the assumption that the gradient is locally Lipschitz continuous over any compact set, SLO develops a careful scheme to control the distance between successive iterates. The proposed framework can easily adapt to the existing first-order methods, such as projected gradient descent (PGD), truncated gradient descent (TGD), and a parameter-free variant of Armijo linesearch. We show that SLO requires O ( ϵ − 1 L 1 ( Y ) ) gradient evaluations to find an ϵ -stationary point, where Y is certain compact set with O ( ϵ − 1 / 2 ) radius, and L i ( Y ) denotes the Lipschitz constant of the i -th order derivatives in Y . It is worth noting that our analysis provides the first nonasymptotic convergence rate for the (slight variant of) Armijo linesearch algorithm without globally Lipschitz continuous gradient or convexity. As a generic framework, we also show that SLO can incorporate more complicated subroutines, such as a variant of the accelerated gradient descent (AGD) method that can harness the problem’s second-order smoothness without Hessian computation, which achieves an improved O ˜ ( ϵ − 7 / 8 L 1 1 / 2 ( Y ) L 2 1 / 4 ( Y ) ) complexity. Funding: J. Zhang is supported by the MOE AcRF [Grant A-0009530-04-00], from Singapore Ministry of Education. M. Hong is supported by NSF [Grants CIF-1910385 and EPCN-2311007]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2021.0029 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2021.0029},
  journal      = {INFORMS Journal on Optimization},
  month        = {Spring},
  number       = {2},
  pages        = {118-136},
  shortjournal = {INFORMS J. Optim.},
  title        = {First-order algorithms without lipschitz gradient: A sequential local optimization approach},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scenario-based robust optimization for two-stage decision
making under binary uncertainty. <em>IJOO</em>, <em>6</em>(2), 84–117.
(<a href="https://doi.org/10.1287/ijoo.2020.0038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses problems of two-stage optimization under binary uncertainty. We define a scenario-based robust optimization (ScRO) formulation that combines principles of stochastic optimization (by constructing probabilistic scenarios) and robust optimization (by protecting against adversarial perturbations within discrete uncertainty sets). To solve it, we develop a sparse row generation algorithm that iterates between a master problem (which provides a lower bound based on minimal uncertainty sets) and a history-based subproblem (which generates an upper bound and updates minimal uncertainty sets). We generate scenarios and uncertainty sets from element-wise probabilities using a deviation likelihood method or from historical samples using a sample clustering approach. Using public data sets, results suggest that (i) our ScRO formulation outperforms benchmarks based on deterministic, stochastic, and robust optimization; (ii) our deviation likelihood and sample clustering approaches outperform scenario generation baselines; and (iii) our sparse row generation algorithm outperforms off-the-shelf implementation and state-of-the-art cutting plane benchmarks. An application to a real-world ambulance dispatch case study suggests that the proposed modeling and algorithmic approach can reduce the number of late responses by more than 25%. Funding: K. Wang’s research was supported by the National Natural Science Foundation of China [Grants 72322002, 52221005, and 52220105001]},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2020.0038},
  journal      = {INFORMS Journal on Optimization},
  month        = {Spring},
  number       = {2},
  pages        = {84-117},
  shortjournal = {INFORMS J. Optim.},
  title        = {Scenario-based robust optimization for two-stage decision making under binary uncertainty},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the hardness of learning from censored and nonstationary
demand. <em>IJOO</em>, <em>6</em>(2), 63–83. (<a
href="https://doi.org/10.1287/ijoo.2022.0017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a repeated newsvendor problem in which the inventory manager has no prior information about the demand and can access only censored/sales data. In analogy to multiarmed bandit problems, the manager needs to simultaneously “explore” and “exploit” with inventory decisions in order to minimize the cumulative cost. Our goal is to understand the hardness of the problem disentangled from any probabilistic assumptions on the demand sequence—importantly, independence or time stationarity—and, correspondingly, to develop policies that perform well with respect to the regret criterion. We design a cost estimator that is tailored to the special structure of the censoring problem, and we show that, if coupled with the classic exponentially weighted forecaster, it achieves optimal scaling of the expected regret (up to logarithmic factors) with respect to both the number of time periods and available actions. This result also leads to two important insights: the benefit from “information stalking” as well as the cost of censoring are both negligible, at least in terms of the regret. We demonstrate the flexibility of our technique by combining it with the fixed share forecaster to provide strong guarantees in terms of tracking regret, a powerful notion of regret that uses a large class of time-varying action sequences as benchmark. Numerical experiments suggest that the resulting policy outperforms existing policies (that are tailored to or facilitated by time stationarity) on nonstationary demand models with time-varying noise, trend, and seasonality components. Finally, we consider the “combinatorial” version of the repeated newsvendor problem, that is, single-warehouse, multiretailer inventory management of a perishable product. We extend the proposed approach so that, again, it achieves near-optimal performance in terms of the regret. Funding: G. Lugosi was supported by the Spanish Ministry of Economy, Industry and Competitiveness [Grant MTM2015-67304-P (AEI/FEDER, UE)]. M. G. Markakis was supported by the Spanish Ministry of Economy and Competitiveness [Grant ECO2016-75905-R (AEI/FEDER, UE)] and a Juan de la Cierva fellowship as well as the Spanish Ministry of Science and Innovation through a Ramón y Cajal fellowship. G. Neu was supported by the UPFellows Fellowship (Marie Curie COFUND program) [Grant 600387]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/ijoo.2022.0017 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0017},
  journal      = {INFORMS Journal on Optimization},
  month        = {Spring},
  number       = {2},
  pages        = {63-83},
  shortjournal = {INFORMS J. Optim.},
  title        = {On the hardness of learning from censored and nonstationary demand},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Temporal bin packing with half-capacity jobs. <em>IJOO</em>,
<em>6</em>(1), 46–62. (<a
href="https://doi.org/10.1287/ijoo.2023.0002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by applications in cloud computing, we study a temporal bin packing problem with jobs that occupy half of a bin’s capacity. An instance is given by a set of jobs, each with a start and end time during which it must be processed (i.e., assigned to a bin). A bin can accommodate two jobs simultaneously, and the objective is an assignment that minimizes the time-averaged number of open or active bins over the horizon; this problem is known to be NP hard. We demonstrate that a well-known “static” lower bound may have a significant gap even in relatively simple instances, which motivates us to introduce a novel combinatorial lower bound and an integer programming formulation, both based on an interpretation of the model as a series of connected matching problems. We theoretically compare the static bound, the new matching-based bounds, and various linear programming bounds. We perform a computational study using both synthetic and application-based instances and show that our bounds offer significant improvement over existing methods, particularly for sparse instances. Funding: This work was supported by the National Science Foundation [Grants CMMI-1552479 and NSF GRFP]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoo.2023.0002 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0002},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {46-62},
  shortjournal = {INFORMS J. Optim.},
  title        = {Temporal bin packing with half-capacity jobs},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A computationally efficient benders decomposition for energy
systems planning problems with detailed operations and time-coupling
constraints. <em>IJOO</em>, <em>6</em>(1), 32–45. (<a
href="https://doi.org/10.1287/ijoo.2023.0005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy systems planning models identify least-cost strategies for expansion and operation of energy systems and provide decision support for investment, planning, regulation, and policy. Most are formulated as linear programming (LP) or mixed integer linear programming (MILP) problems. Despite the relative efficiency and maturity of LP and MILP solvers, large scale problems are often intractable without abstractions that impact quality of results and generalizability of findings. We consider a macro-energy systems planning problem with detailed operations and policy constraints and formulate a computationally efficient Benders decomposition separating investments from operations and decoupling operational timesteps using budgeting variables in the master model. This novel approach enables parallelization of operational subproblems and permits modeling of relevant constraints coupling decisions across time periods (e.g., policy constraints) within a decomposed framework. Runtime scales linearly with temporal resolution; tests demonstrate substantial runtime improvement for all MILP formulations and for some LP formulations depending on problem size relative to analogous monolithic models solved with state-of-the-art commercial solvers. Our algorithm is applicable to planning problems in other domains (e.g., water, transportation networks, production processes) and can solve large-scale problems otherwise intractable. We show that the increased resolution enabled by this algorithm mitigates structural uncertainty, improving recommendation accuracy. Funding: Funding for this work was provided by the Princeton Carbon Mitigation Initiative (funded by a gift from BP) and the Princeton Zero-carbon Technology Consortium (funded by gifts from GE, Google, ClearPath, and Breakthrough Energy). Supplemental Material: The e-companion is available at https://doi.org/10.1287/ijoo.2023.0005 .},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0005},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {32-45},
  shortjournal = {INFORMS J. Optim.},
  title        = {A computationally efficient benders decomposition for energy systems planning problems with detailed operations and time-coupling constraints},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the linear convergence of extragradient methods for
nonconvex–nonconcave minimax problems. <em>IJOO</em>, <em>6</em>(1),
19–31. (<a href="https://doi.org/10.1287/ijoo.2022.0004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, minimax optimization has received renewed focus due to modern applications in machine learning, robust optimization, and reinforcement learning. The scale of these applications naturally leads to the use of first-order methods. However, the nonconvexities and nonconcavities present in these problems, prevents the application of typical gradient descent/ascent, which is known to diverge even in bilinear problems. Recently, it was shown that the proximal point method (PPM) converges linearly for a family of nonconvex–nonconcave problems. In this paper, we study the convergence of a damped version of the extragradient method (EGM), which avoids potentially costly proximal computations, relying only on gradient evaluation. We show that the EGM converges linearly for smooth minimax optimization problems satisfying the same nonconvex–nonconcave condition needed by the PPM. Funding: H. Lu was supported by The University of Chicago Booth School of Business Benjamin Grimmer was supported by Johns Hopkins Applied Mathematics and Statistics Department.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2022.0004},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {19-31},
  shortjournal = {INFORMS J. Optim.},
  title        = {On the linear convergence of extragradient methods for Nonconvex–Nonconcave minimax problems},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic routing and wavelength assignment with reinforcement
learning. <em>IJOO</em>, <em>6</em>(1), 1–18. (<a
href="https://doi.org/10.1287/ijoo.2023.0092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid developments in communication systems, and considering their dynamic nature, all-optical networks are becoming increasingly complex. This study proposes a novel method based on deep reinforcement learning for the routing and wavelength assignment problem in all-optical wavelength-decision-multiplexing networks. We consider dynamic incoming requests, in which their arrival and holding times are not known in advance. The objective is to devise a strategy that minimizes the number of rejected packages due to the lack of resources in the long term. We use graph neural networks to capture crucial latent information from the graph-structured input to develop the optimal strategy. The proposed deep reinforcement learning algorithm selects a route and a wavelength simultaneously for each incoming traffic connection as they arrive. The results demonstrate that the learned agent outperforms the methods used in practice and can be generalized on network topologies that did not participate in training.},
  archive      = {J_IJOO},
  doi          = {10.1287/ijoo.2023.0092},
  journal      = {INFORMS Journal on Optimization},
  month        = {Winter},
  number       = {1},
  pages        = {1-18},
  shortjournal = {INFORMS J. Optim.},
  title        = {Dynamic routing and wavelength assignment with reinforcement learning},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
