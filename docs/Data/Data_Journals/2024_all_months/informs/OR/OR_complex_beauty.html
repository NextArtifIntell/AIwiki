<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="or---151">OR - 151</h2>
<ul>
<li><details>
<summary>
(2024). Least squares monte carlo and pathwise optimization for
merchant energy production. <em>OR</em>, <em>72</em>(6), 2758–2775. (<a
href="https://doi.org/10.1287/opre.2018.0341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study merchant energy production modeled as a compound switching and timing option. The resulting Markov decision process is intractable. Least squares Monte Carlo combined with information relaxation and duality is a state-of-the-art reinforcement learning methodology to obtain operating policies and optimality gaps for related models. Pathwise optimization is a competing technique developed for optimal stopping settings, in which it typically provides superior results compared with this approach, albeit with a larger computational effort. We apply these procedures to merchant energy production. Using pathwise optimization requires methodological extensions. We use principal component analysis and block coordinate descent in novel ways to respectively precondition and solve the ensuing ill-conditioned and large-scale linear program, which even a cutting-edge commercial solver is unable to handle directly. Both techniques yield near optimal operating policies on realistic ethanol production instances. However, at the cost of both considerably longer run times and greater memory usage, which limits the number of stages of the instances that it can handle, pathwise optimization leads to substantially tighter dual bounds compared with least squares Monte Carlo, even when specified in a simple fashion, complementing it in this case. Thus, it plays a critical role in obtaining small optimality gaps. Our numerical observations on the magnitudes of these bound improvements differ from what is currently known. This research has potential relevance for other commodity merchant operations contexts and motivates additional algorithmic work in the area of pathwise optimization. Funding: This work was supported by the Division of Civil, Mechanical, and Manufacturing Innovation, National Science Foundation [Grant 1761742] and a grant from the Scott Institute for Energy Innovation at Carnegie Mellon University. S. Nadarajah acknowledges support from the University of Illinois at Chicago College of Business [Dean’s Research Grant]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2018.0341 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2018.0341},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2758-2775},
  shortjournal = {Oper. Res.},
  title        = {Least squares monte carlo and pathwise optimization for merchant energy production},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conditional distributionally robust functionals.
<em>OR</em>, <em>72</em>(6), 2745–2757. (<a
href="https://doi.org/10.1287/opre.2023.2470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many decisions, in particular decisions in a managerial context, are subject to uncertainty. Risk measures cope with uncertainty by involving more than one candidate probability. The corresponding risk averse decision takes all potential candidate probabilities into account and is robust with respect to all potential probabilities. This paper considers conditional robust decision making, where decisions are subject to additional prior knowledge or information. The literature discusses various definitions to characterize the corresponding conditional risk measure, which determines further the decision. The aim of this paper is to compare two different approaches for the construction of conditional functionals used in multistage distributionally robust optimization. As an application, we discuss conditional counterparts of a distance between probability measures. Funding: A. Shapiro was partly supported by the Air Force Office of Scientific Research [Grant FA9550-22-1-0244]. A. Pichler was funded by the Deutsche Forschungsgemeinschaft [Project 416228727–SFB 1410].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2470},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2745-2757},
  shortjournal = {Oper. Res.},
  title        = {Conditional distributionally robust functionals},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic pricing with unknown nonparametric demand and
limited price changes. <em>OR</em>, <em>72</em>(6), 2726–2744. (<a
href="https://doi.org/10.1287/opre.2020.0445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the dynamic pricing problem of a retailer who does not have any information on the underlying demand for a product. The retailer aims to maximize cumulative revenue collected over a finite time horizon by balancing two objectives: learning demand and maximizing revenue. The retailer also seeks to reduce the amount of price experimentation because of the potential costs associated with price changes. Existing literature solves this problem in the case where the unknown demand is parametric. We consider the pricing problem when demand is nonparametric. We construct a pricing algorithm that uses second order approximations of the unknown demand function and establish when the proposed policy achieves near-optimal rate of regret, O ˜ ( T ) , while making O ( log log T ) price changes. Hence, we show considerable reduction in price changes from the previously known O ( log T ) rate of price change guarantee in the literature. We also perform extensive numerical experiments to show that the algorithm substantially improves over existing methods in terms of the total price changes, with comparable performance on the cumulative regret metric. Funding: This work was supported by the National Science Foundation [Grant CMMI-156334]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2020.0445 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0445},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2726-2744},
  shortjournal = {Oper. Res.},
  title        = {Dynamic pricing with unknown nonparametric demand and limited price changes},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic approximation of symmetric nash equilibria in
queueing games. <em>OR</em>, <em>72</em>(6), 2698–2725. (<a
href="https://doi.org/10.1287/opre.2021.0306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest a novel stochastic-approximation algorithm to compute a symmetric Nash-equilibrium strategy in a general queueing game with a finite action space. The algorithm involves a single simulation of the queueing process with dynamic updating of the strategy at regeneration times. Under mild assumptions on the utility function and on the regenerative structure of the queueing process, the algorithm converges to a symmetric equilibrium strategy almost surely. This yields a powerful tool that can be used to approximate equilibrium strategies in a broad range of strategic queueing models in which direct analysis is impracticable. Funding: This work was supported by Columbia University and the Shenzhen Research Institute for Big Data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0306},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2698-2725},
  shortjournal = {Oper. Res.},
  title        = {Stochastic approximation of symmetric nash equilibria in queueing games},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An online learning approach to dynamic pricing and capacity
sizing in service systems. <em>OR</em>, <em>72</em>(6), 2677–2697. (<a
href="https://doi.org/10.1287/opre.2020.0612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a dynamic pricing and capacity sizing problem in a G I / G I / 1 queue, in which the service provider’s objective is to obtain the optimal service fee p and service capacity μ so as to maximize the cumulative expected profit (the service revenue minus the staffing cost and delay penalty). Because of the complex nature of the queueing dynamics, such a problem has no analytic solution so that previous research often resorts to heavy-traffic analysis in which both the arrival and service rates are sent to infinity. In this work, we propose an online learning framework designed for solving this problem that does not require the system’s scale to increase. Our framework is dubbed gradient-based online learning in queue (GOLiQ). GOLiQ organizes the time horizon into successive operational cycles and prescribes an efficient procedure to obtain improved pricing and staffing policies in each cycle using data collected in previous cycles. Data here include the number of customer arrivals, waiting times, and the server’s busy times. The ingenuity of this approach lies in its online nature, which allows the service provider to do better by interacting with the environment. Effectiveness of GOLiQ is substantiated by (i) theoretical results, including the algorithm convergence and regret analysis (with a logarithmic regret bound), and (ii) engineering confirmation via simulation experiments of a variety of representative G I / G I / 1 queues. Funding: X. Chen acknowledges support [Grants NSFC72171205, NSFC11901493, and RCYX20210609103124047]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0612 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0612},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2677-2697},
  shortjournal = {Oper. Res.},
  title        = {An online learning approach to dynamic pricing and capacity sizing in service systems},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A splitting method for band control of brownian motion: With
application to mutual reserve optimization. <em>OR</em>, <em>72</em>(6),
2665–2676. (<a href="https://doi.org/10.1287/opre.2011.0427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a splitting solution method for two-sided impulse control of Brownian motion, which leads to an expanding range of band control applications and studies, such as monetary reserves (including the previously studied cash management problem, exchange rate control in central banks, and marine mutual insurance reserves), inventory systems, and lately natural resources and energy reservation. It has been shown since earlier studies in 1970s that the optimal two-sided impulse control can be characterized by a two-band control policy of four parameters ( a , A , B , b ) with a &lt; A ≤ B &lt; b , of which the dynamic programming characteristics leads to a quasi-variational inequality (QVI) with two sides. Thus far, the focus of band control problems has been on determination of optimal band policy parameters. Its solution methods, as far as we can ascertain from the current literature, have centered on finding the four parameters by solving simultaneously characteristic systems of QVI inequalities, of which analytical solutions of closed form remain unattainable and computational solutions are still largely intractable. The key contributions of this paper are (1) development of a splitting method of decomposing a general two-sided band control problem into two iterative one-sided band control problems, each iteration being reduced to a one-dimension optimization; (2) obtaining a theorem on geometrical characterization of band-splitting control, including QVI and computational analytics and characteristics of band-splitting functions and solutions; and (3) development of a band-splitting solution algorithm for the two-sided impulse control, including an effective initial-point selection method that is termed the separate approximation of geometric conditions method. Numerical comparison experiments are carried out to validate and test the effectiveness and accuracy of the splitting solution method. The method is not only computationally effective, but also useful for proving theoretical results. Funding: A. Bensoussan is supported in part by the National Science Foundation [Grant DMS-2204795]. The study was supported by the General Research Fund [Grants 5230/06E and 5225/07E], both of which J. J. Liu was the principal investigator as chair professor of maritime studies, and was based on the PhD thesis of J. Yuan, who was jointly supervised by A. Bensoussan, all at Hong Kong Polytechnic University. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2011.0427 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2011.0427},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2665-2676},
  shortjournal = {Oper. Res.},
  title        = {A splitting method for band control of brownian motion: With application to mutual reserve optimization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assortment optimization under the multi-purchase multinomial
logit choice model. <em>OR</em>, <em>72</em>(6), 2631–2664. (<a
href="https://doi.org/10.1287/opre.2023.2463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce the Multi-Purchase Multinomial Logit choice model, which extends the random utility maximization framework of the classical Multinomial Logit model to a multiple-purchase setting. In this model, customers sample random utilities for each offered product as in the Multinomial Logit model. However, rather than focusing on a single product, they concurrently sample a “budget” parameter M , which indicates the maximum number of products that the customer is willing to purchase. Subsequently, the M highest utility products are purchased, out of those whose utilities exceed that of the no-purchase option. When fewer than M products satisfy the latter condition, only these products will be purchased. Our primary contribution resides in proposing the first multi-purchase choice model that can be fully operationalized. Specifically, we provide a recursive procedure to compute the choice probabilities in this model, which in turn provides a framework to study its resulting assortment problem, where the goal is to select a subset of products to make available for purchase so as to maximize expected revenue. Our main algorithmic results consist of two distinct polynomial time approximation schemes (PTAS); the first, and simpler of the two, caters to a setting where each customer may buy only a constant number of products, whereas the second more nuanced algorithm applies to our multi-purchase model in its general form. Additionally, we study the revenue potential of making assortment decisions that account for multi-purchase behavior in comparison with those that overlook this phenomenon. In particular, we relate both the structure and revenue performance of the optimal assortment under a traditional single-purchase model to that of the optimal assortment in the multi-purchase setting. Finally, we complement our theoretical work with an extensive set of computational experiments, where the efficacy of our proposed PTAS is tested against natural heuristics. Ultimately, we find that our approximation scheme outperforms these approaches by 1% to 5% on average. Funding: The work of D. Segev on this project is supported by Israel Science Foundation grant 1407/20. The work of H. Topaloglu was supported by a seed grant from Urban Tech Hub at Cornell Tech and National Foundation Grant CMMI-1825406.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2463},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2631-2664},
  shortjournal = {Oper. Res.},
  title        = {Assortment optimization under the multi-purchase multinomial logit choice model},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive importance sampling for efficient stochastic root
finding and quantile estimation. <em>OR</em>, <em>72</em>(6), 2612–2630.
(<a href="https://doi.org/10.1287/opre.2023.2484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In solving simulation-based stochastic root-finding or optimization problems that involve rare events, such as in extreme quantile estimation, running crude Monte Carlo can be prohibitively inefficient. To address this issue, importance sampling can be employed to drive down the sampling error to a desirable level. However, selecting a good importance sampler requires knowledge of the solution to the problem at hand, which is the goal to begin with and thus forms a circular challenge. We investigate the use of adaptive importance sampling to untie this circularity. Our procedure sequentially updates the importance sampler to reach the optimal sampler and the optimal solution simultaneously, and can be embedded in both sample-average-approximation-type algorithms and stochastic-approximation-type algorithms. Our theoretical analysis establishes strong consistency and asymptotic normality of the resulting estimators. We also demonstrate, via a minimax perspective, the key role of using adaptivity in controlling asymptotic errors. Finally, we illustrate the effectiveness of our approach via numerical experiments. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72293562, 72121001, and 72171060], the National Science Foundation [Grants CAREER CMMI-1834710 and IIS-1849280], and the Air Force Office of Scientific Research [Grant FA95502010211]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2484 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2484},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2612-2630},
  shortjournal = {Oper. Res.},
  title        = {Adaptive importance sampling for efficient stochastic root finding and quantile estimation},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is separately modeling subpopulations beneficial for
sequential decision-making? <em>OR</em>, <em>72</em>(6), 2595–2611. (<a
href="https://doi.org/10.1287/opre.2023.2474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent applications of Markov decision processes (MDPs), it is common to estimate transition probabilities and rewards from transition data. In healthcare and some other applications, transition data are collected from a population of different entities, such as patients. Thus, one faces a modeling question of whether to estimate different models for subpopulations (e.g., divided by smoking status). For instance, there may be a subpopulation whose disease status progresses faster than others, and for such a group, estimating a separate model and applying the corresponding optimal treatment plan can improve their outcomes. This work provides theoretical results and empirical methods for making the decision of whether to model subpopulations separately (called stratifying) or not. We also present how to use our results to select the best stratification among many. We illustrate our results and methods numerically using random instances and a medical decision-making problem from the literature. Because improving medical decisions by tailoring to each subpopulation is a building block of precision medicine, this work advances the use of MDPs in medical decision-making toward the precision medicine paradigm. Funding: This research was funded by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-03960] and the Alberta School of Business [Xerox Faculty Fellowship 2018–2019, CN Western Economic Research Fellowship 2021–2024]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2474 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2474},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2595-2611},
  shortjournal = {Oper. Res.},
  title        = {Is separately modeling subpopulations beneficial for sequential decision-making?},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A (slightly) improved approximation algorithm for metric
TSP. <em>OR</em>, <em>72</em>(6), 2543–2594. (<a
href="https://doi.org/10.1287/opre.2022.2338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For some ϵ &gt; 10 − 36 , we give a randomized 3 / 2 − ϵ approximation algorithm for metric TSP. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA9550-20-1-0212], the Alfred P. Sloan Foundation [Grant FG-2019-12196], and the Division of Computing and Communication Foundations [Grants CCF-1552097, CCF-1813135, and CCF-1907845].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2338},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2543-2594},
  shortjournal = {Oper. Res.},
  title        = {A (Slightly) improved approximation algorithm for metric TSP},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential learning with a similarity selection index.
<em>OR</em>, <em>72</em>(6), 2526–2542. (<a
href="https://doi.org/10.1287/opre.2023.2478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of selecting the best alternative in a setting where prior similarity information between the performance output of different alternatives can be learned from data. Incorporating similarity information enables efficient budget allocation for faster identification of the best alternative in sequential selection. Using a new selection criterion, the similarity selection index, we develop two new allocation methods: one based on a mathematical programming characterization of the asymptotically optimal budget allocation and the other based on a myopic expected improvement measure. For the former, we present a novel sequential implementation that provably learns the optimal allocation without tuning. For the latter, we derive its asymptotic sampling ratios. We also propose a practical way to update the prior similarity information as new samples are collected. Numerical results illustrate the effectiveness of both methods. Funding: This work was supported by the Air Force Office of Scientific Research [Grant FA95502010211]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2478 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2478},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2526-2542},
  shortjournal = {Oper. Res.},
  title        = {Sequential learning with a similarity selection index},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How should we score athletes and candidates: Geometric
scoring rules. <em>OR</em>, <em>72</em>(6), 2507–2525. (<a
href="https://doi.org/10.1287/opre.2023.2473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scoring rules are widely used to rank athletes in sports and candidates in elections. Each position in each individual ranking is worth a certain number of points; the total sum of points determines the aggregate ranking. The question is how to choose a scoring rule for a specific application. First, we derive a one-parameter family with geometric scores that satisfies two principles of independence: once an extremely strong or weak candidate is removed, the aggregate ranking ought to remain intact. This family includes Borda count, generalized plurality (medal count), and generalized antiplurality (threshold rule) as edge cases, and we find which additional axioms characterize these rules. Second, we introduce a one-parameter family with optimal scores: the athletes should be ranked according to their expected overall quality. Finally, using historical data from biathlon, golf, and athletics, we demonstrate how the geometric and optimal scores can simplify the selection of suitable scoring rules, show that these scores closely resemble the actual scores used by the organizers, and provide an explanation for empirical phenomena observed in biathlon and golf tournaments. We see that geometric scores approximate the optimal scores well in events in which the distribution of athletes’ performances is roughly uniform. Funding: This work was supported by HSE University (Basic Research Program, Priority 2030 Program). Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2473 . All data are available on https://github.com/EIanovski/GeometricScoringRules .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2473},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2507-2525},
  shortjournal = {Oper. Res.},
  title        = {How should we score athletes and candidates: Geometric scoring rules},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The generalized c/μ rule for queues with heterogeneous
server pools. <em>OR</em>, <em>72</em>(6), 2488–2506. (<a
href="https://doi.org/10.1287/opre.2023.2472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the optimal control of a queueing model with a single customer class and heterogeneous server pools. The main objective is to strike a balance between the holding cost of the queue and the operating costs of the server pools. We introduce a target-allocation policy, which assigns higher priority to the queue or pools without enough customers for general cost functions. Although we can prove its asymptotic optimality, implementation requires solving a nonlinear optimization problem. When the cost functions are convex, we propose a dynamic priority policy referred to as the Gc / µ rule, which is much easier to implement. When the cost functions are concave, it turns out that a fixed priority policy is optimal. We also consider an extension to minimize the operating cost of the server pools and satisfy a service-level target for customers waiting in the queue. We develop hybrid routing policies, combining a threshold policy for the queue and the aforementioned policies for the server pools, for different types of operating cost functions. Moreover, the hybrid routing policies coincide with several classic policies in the literature in special cases. Extensive simulation experiments demonstrate the efficacy of our proposed policies. Funding: Z. Long received support from the National Natural Science Foundation of China [Grants 72101112 and 72132005] and Jiangsu Province, China [Grant BK20210171]. H. Zhang received support from the National Natural Science Foundation of China [Grants 72192805 and 72201231], the Shenzhen Science and Technology Innovation Commission [Grant RCYX20210609103124047], and the Shenzhen Research Institute of Big Data [Grant T00120220004]. J. Zhang received support from the Hong Kong Research Grants Council [GRF Grants 16208120 and 16214121]. Z.G. Zhang received support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2019-06364]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2472 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2472},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2488-2506},
  shortjournal = {Oper. Res.},
  title        = {The generalized c/μ rule for queues with heterogeneous server pools},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Market entry and competition under network effects.
<em>OR</em>, <em>72</em>(6), 2467–2487. (<a
href="https://doi.org/10.1287/opre.2022.0275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a three-stage game in which, first, a large number of potential firms make entry decisions, then those who choose to stay in the market decide on the investment (quality) level in each product, and last, customers with heterogeneous preferences arrive sequentially to make (random) purchase decisions based on product quality and historical sales under the network effect according to a discrete choice model. We characterize such a random purchase process and show that a growing network effect always contributes to more sales concentration ex post on a small number of products. Perhaps surprisingly, we further show several phase-changing phenomena regarding equilibrium outcomes with respect to the network effect’s strength. In particular, the equilibrium product variety (respectively, quality investment) first decreases (respectively, increases) and then increases (respectively, decreases) as the network effect grows. Specifically, when the strength of the network effect is below a threshold, an increasing network effect would shift more sales toward those products with higher quality, preventing more products from entering the market ex ante and inducing firms to adopt the high-budget equilibrium strategy by making a small number of high-quality products, which is consistent with the blockbuster phenomenon. When the strength of the network effect is above the threshold, the network effect would easily cause the market to be concentrated on a few products ex post; even some low-quality products may have a chance to become a “hit.” Interestingly, in this case, when the network effect is growing, the ex ante equilibrium product variety will be wider, and firms adopt the low-budget equilibrium strategy by making a (relatively) large number of low-quality products, a finding consistent with the long tail theory. We then establish the robustness of the previous main insights by accounting for endogenized pricing and multiproducts carried by each firm. Funding: Y. Feng was financially supported by the Major Program of National Natural Science Foundation of China [Grants 72192830 and 7219283X], Fundamental Research Funds for the Central Universities, and Program for Innovative Research of Shanghai University of Finance and Economics. M. Hu was supported by the Natural Sciences and Engineering Research Council of Canada [Grants RGPIN-2015-06757 and RGPIN-2021-04295]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0275 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0275},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2467-2487},
  shortjournal = {Oper. Res.},
  title        = {Market entry and competition under network effects},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Pricing and positioning of horizontally differentiated
products with incomplete demand information. <em>OR</em>,
<em>72</em>(6), 2446–2466. (<a
href="https://doi.org/10.1287/opre.2021.0093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of determining the optimal prices and product configurations of horizontally differentiated products when customers purchase according to a locational (Hotelling) choice model and where the problem parameters are initially unknown to the decision maker. Both for the single-product and multiple-product setting, we propose a data-driven algorithm that learns the optimal prices and product configurations from accumulating sales data, and we show that their regret—the expected cumulative loss caused by not using optimal decisions—after T time periods is O ( T 1 / 2 + o ( 1 ) ) . We accompany this result by showing that, even in the single-product setting, the regret of any algorithm is bounded from below by a constant time T 1 / 2 , implying that our algorithms are asymptotically near optimal. In an extension, we show how our algorithm can be adapted for the case of fixed locations. A numerical study that compares our algorithms with three benchmarks shows that our algorithm is also competitive on a finite time horizon. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0093 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0093},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2446-2466},
  shortjournal = {Oper. Res.},
  title        = {Pricing and positioning of horizontally differentiated products with incomplete demand information},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-based reinforcement learning for offline zero-sum
markov games. <em>OR</em>, <em>72</em>(6), 2430–2445. (<a
href="https://doi.org/10.1287/opre.2022.0342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper makes progress toward learning Nash equilibria in two-player, zero-sum Markov games from offline data. Specifically, consider a γ -discounted, infinite-horizon Markov game with S states, in which the max-player has A actions and the min-player has B actions. We propose a pessimistic model–based algorithm with Bernstein-style lower confidence bounds—called the value iteration with lower confidence bounds for zero-sum Markov games—that provably finds an ε -approximate Nash equilibrium with a sample complexity no larger than C clipped ⋆ S ( A + B ) ( 1 − γ ) 3 ε 2 (up to some log factor). Here, C clipped ⋆ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-à-vis the target data), and the target accuracy ε can be any value within ( 0 , 1 1 − γ ] . Our sample complexity bound strengthens prior art by a factor of min { A , B } , achieving minimax optimality for a broad regime of interest. An appealing feature of our result lies in its algorithmic simplicity, which reveals the unnecessity of variance reduction and sample splitting in achieving sample optimality. Funding: Y. Yan is supported in part by the Charlotte Elizabeth Procter Honorific Fellowship from Princeton University and the Norbert Wiener Postdoctoral Fellowship from MIT. Y. Chen is supported in part by the Alfred P. Sloan Research Fellowship, the Google Research Scholar Award, the Air Force Office of Scientific Research [Grant FA9550-22-1-0198], the Office of Naval Research [Grant N00014-22-1-2354], and the National Science Foundation [Grants CCF-2221009, CCF-1907661, IIS-2218713, DMS-2014279, and IIS-2218773]. J. Fan is supported in part by the National Science Foundation [Grants DMS-1712591, DMS-2052926, DMS-2053832, and DMS-2210833] and Office of Naval Research [Grant N00014-22-1-2340]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0342 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0342},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2430-2445},
  shortjournal = {Oper. Res.},
  title        = {Model-based reinforcement learning for offline zero-sum markov games},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal auction design with deferred inspection and reward.
<em>OR</em>, <em>72</em>(6), 2413–2429. (<a
href="https://doi.org/10.1287/opre.2020.0651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a mechanism run by an auctioneer who can use both payment and inspection instruments to incentivize agents. The timeline of the events is as follows. Based on a prespecified allocation rule and the reported values of agents, the auctioneer allocates the item and secures the reported values as deposits. The auctioneer then inspects the values of agents and, using a prespecified reward rule, rewards the ones who have reported truthfully. Using techniques from convex analysis and calculus of variations, for any distribution of values, we fully characterize the optimal mechanism for a single agent. Using Border’s theorem and duality, we find conditions under which our characterization extends to multiple agents. Interestingly, the optimal allocation function, unlike the classic settings without inspection, is not a threshold strategy and instead is an increasing and continuous function of the types. We also present an implementation of our optimal auction and show that it achieves a higher revenue than auctions in classic settings without inspection. This is because the inspection enables the auctioneer to charge payments closer to the agents’ true values without creating incentives for them to deviate to lower types. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0651 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0651},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2413-2429},
  shortjournal = {Oper. Res.},
  title        = {Optimal auction design with deferred inspection and reward},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). To interfere or not to interfere: Information revelation and
price-setting incentives in a multiagent learning environment.
<em>OR</em>, <em>72</em>(6), 2391–2412. (<a
href="https://doi.org/10.1287/opre.2023.0363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a platform in which multiple sellers offer their products for sale over a time horizon of T periods. Each seller sets its own price. The platform collects a fraction of the sales revenue and provides price-setting incentives to the sellers to maximize its own revenue. The demand for each seller’s product is a function of all sellers’ prices and some customer features. Initially, neither the platform nor the sellers know the demand function, but they can learn about it through sales observations: each seller observes its own sales, whereas the platform observes all sellers’ sales as well as the customer feature information. We measure the platform’s performance by comparing its expected revenue with the full-information optimal revenue, and we design policies that enable the platform to manage information revelation and price-setting incentives. Perhaps surprisingly, a simple “do-nothing” policy does not always exhibit poor revenue performance and can perform exceptionally well under certain conditions. With a more conservative policy that reveals information to make price-setting incentives more effective, the platform can always protect itself from large revenue losses caused by demand model uncertainty. We develop a strategic reveal-and-incentivize policy that combines the benefits of the aforementioned policies and thereby achieves asymptotically optimal revenue performance as T grows large. Funding: This work was supported by Duke University Fuqua School of Business, University of Chicago Booth School of Business, and CUHK Business School. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.0363 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0363},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2391-2412},
  shortjournal = {Oper. Res.},
  title        = {To interfere or not to interfere: Information revelation and price-setting incentives in a multiagent learning environment},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outcome-driven dynamic refugee assignment with allocation
balancing. <em>OR</em>, <em>72</em>(6), 2375–2390. (<a
href="https://doi.org/10.1287/opre.2022.0445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes two new dynamic assignment algorithms to match refugees and asylum seekers to geographic localities within a host country. The first, currently implemented in a multiyear randomized control trial in Switzerland, seeks to maximize the average predicted employment level (or any measured outcome of interest) of refugees through a minimum-discord online assignment algorithm. The performance of this algorithm is tested on real refugee resettlement data from both the United States and Switzerland, where we find that it is able to achieve near-optimal expected employment, compared with the hindsight-optimal solution, and is able to improve upon the status quo procedure by 40%–50%. However, pure outcome maximization can result in a periodically imbalanced allocation to the localities over time, leading to implementation difficulties and an undesirable workflow for resettlement resources and agents. To address these problems, the second algorithm balances the goal of improving refugee outcomes with the desire for an even allocation over time. We find that this algorithm can achieve near-perfect balance over time with only a small loss in expected employment compared with the employment-maximizing algorithm. In addition, the allocation balancing algorithm offers a number of ancillary benefits compared with pure outcome maximization, including robustness to unknown arrival flows and greater exploration. Funding: Financial support from the Charles Koch Foundation, Stanford Impact Labs, the Rockefeller Foundation, Google.org, Schmidt Futures, the Stanford Institute for Human-Centered Artificial Intelligence, and Stanford University is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0445 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0445},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2375-2390},
  shortjournal = {Oper. Res.},
  title        = {Outcome-driven dynamic refugee assignment with allocation balancing},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A random consideration set model for demand estimation,
assortment optimization, and pricing. <em>OR</em>, <em>72</em>(6),
2358–2374. (<a href="https://doi.org/10.1287/opre.2019.0333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we operationalize the random consideration set (RCS) choice model proposed by Manzini and Mariotti which assumes that consumers make purchase decisions based on a fixed preference ordering and random consideration sets drawn from independent attention probabilities. We provide a necessary condition, a sufficient condition, and a fast algorithm to estimate preference ordering and attention probabilities from sales transaction data, thereby uniquely identifying the model parameters. Additionally, we propose a greedy-like algorithm to find an assortment that maximizes total expected revenues. This algorithm can be easily adapted to handle cardinality constraints or to discover all efficient sets. Additionally, we explore profit optimization for a price-aware version of the RCS model and demonstrate that optimal profit margins and consumer surplus have the same ordering as the value gaps. Furthermore, we expand the model to include a random consideration-utility maximization framework, where consumers can have random preferences for products once consideration sets are formed. We develop an approximation algorithm with a 1/2 performance guarantee for this extended model. To validate our findings, we conduct a computational study using data provided by a major United States-based airline. The data set comprises 107 different flights serving various markets over a period of 120 days. By testing our models on this airline partner’s data, we demonstrate that the RCS model outperforms the mixed multinomial logit model in nearly half of the markets. These results align with the insights obtained from our synthetic study, indicating that the RCS model exhibits greater robustness when applied to input data sets with limited variations and smaller sizes. Funding: This work was supported by Collaborative Research Funding (CRF) Hong Kong [Grant (CRF) C6032-21G], RGC Hong Kong [Grant 16502819]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2019.0333 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2019.0333},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2358-2374},
  shortjournal = {Oper. Res.},
  title        = {A random consideration set model for demand estimation, assortment optimization, and pricing},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Monte carlo estimation of CoVaR. <em>OR</em>,
<em>72</em>(6), 2337–2357. (<a
href="https://doi.org/10.1287/opre.2023.0211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CoVaR is one of the most important measures of financial systemic risks. It is defined as the risk of a financial portfolio conditional on another financial portfolio being at risk. In this paper we first develop a Monte Carlo simulation–based batching estimator of CoVaR and study its consistency and asymptotic normality. We show that the best rate of convergence that the batching estimator can achieve is n − 1 / 3 , where n is the sample size. We then develop an importance sampling–inspired estimator under the delta-gamma approximations to the portfolio losses and show that the best rate of convergence that the estimator can achieve is n − 1 / 2 . Numerical experiments support our theoretical findings and show that both estimators work well. Funding: This work was partially supported by the National Natural Science Foundation of China [Projects 72161160340 and 12301601]. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0211 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0211},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2337-2357},
  shortjournal = {Oper. Res.},
  title        = {Monte carlo estimation of CoVaR},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incentive-aware models of financial networks. <em>OR</em>,
<em>72</em>(6), 2321–2336. (<a
href="https://doi.org/10.1287/opre.2022.0678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial networks help firms manage risk but also enable financial shocks to spread. Despite their importance, existing models of financial networks have several limitations. Prior works often consider a static network with a simple structure (e.g., a ring) or a model that assumes conditional independence between edges. We propose a new model where the network emerges from interactions between heterogeneous utility-maximizing firms. Edges correspond to contract agreements between pairs of firms, with the contract size being the edge weight. We show that, almost always, there is a unique “stable network.” All edge weights in this stable network depend on all firms’ beliefs. Furthermore, firms can find the stable network via iterative pairwise negotiations. When beliefs change, the stable network changes. We show that under realistic settings, a regulator cannot pin down the changed beliefs that caused the network changes. Also, each firm can use its view of the network to inform its beliefs. For instance, it can detect outlier firms whose beliefs deviate from their peers. However, it cannot identify the deviant belief: Increased risk-seeking is indistinguishable from increased expected profits. Seemingly minor news may settle the dilemma, triggering significant changes in the network. Funding: This work was supported by the National Science Foundation [Grants 2217069, 2019844, and DMS 2109155], McCombs Research Excellence Grants, and a Dell Faculty Award. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0678 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0678},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2321-2336},
  shortjournal = {Oper. Res.},
  title        = {Incentive-aware models of financial networks},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information relaxation and a duality-driven algorithm for
stochastic dynamic programs. <em>OR</em>, <em>72</em>(6), 2302–2320. (<a
href="https://doi.org/10.1287/opre.2020.0464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use the technique of information relaxation to develop a duality-driven iterative approach (DDP) to obtain and improve confidence interval estimates for the true value of finite-horizon stochastic dynamic programming problems. Each iteration of the algorithm solves an optimization-expectation procedure. We show that the sequence of dual value estimates yielded from the proposed approach monotonically converges to the true value function in a finite number of dual iterations. Aiming to overcome the curse of dimensionality in various applications, we also introduce a regression-based Monte Carlo algorithm for implementation. The new approach can assess the quality of heuristic policies and, more importantly, improve them if we find that their duality gap is large. We obtain the convergence rate of our Monte Carlo method in terms of the amounts of both basis functions and the sampled states. Finally, we demonstrate the effectiveness of our method using an optimal order execution problem with market friction. The experiments show that our method can significantly improve various heuristics commonly used in the literature to obtain new policies with a satisfactory performance guarantee. When we implement DDP in the numerical example, some local optimization routines are used in the optimization step. Inspired by the work of Brown and Smith [Brown DB, Smith JE (2014) Information relaxations, duality and convex stochastic dynamic programs. Oper. Res . 62:1394–1415.], we propose an ex-post method for smooth convex dynamic programs to assess how the local optimality of the inner optimization impacts the convergence of the DDP algorithm. Funding: This work was supported by the National Natural Science Foundation of China [Grants 71991474, 72271249, and U1811462] and the Research Grants Council, University Grants Committee, Hong Kong [GRF 14211023 and 14208620]. Part of N. Chen’s work is supported by the InnoHK Initiative of the Hong Kong SAR Government and Laboratory for AI-powered Financial Technology. Supplemental Material: The computer code and data that support the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2020.0464 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0464},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2302-2320},
  shortjournal = {Oper. Res.},
  title        = {Information relaxation and a duality-driven algorithm for stochastic dynamic programs},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Side-constrained dynamic traffic equilibria. <em>OR</em>,
<em>72</em>(6), 2279–2301. (<a
href="https://doi.org/10.1287/opre.2023.0577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study dynamic traffic assignment with side constraints. We first give a counter-example to a previous result from the literature regarding the existence of dynamic equilibria for volume-constrained traffic models in the classical linear edge-delay model. Our counter-example shows that the feasible flow space need not be convex, and it further reveals that classical infinite dimensional variational inequalities are not suited for the definition of general side-constrained dynamic equilibria. We then propose a new framework for side-constrained dynamic equilibria based on the concept of admissible γ -deviations of flow particles in space and time. We show under which assumptions the resulting equilibria can still be characterized by means of quasi-variational and variational inequalities, respectively. Finally, we establish first existence results for side-constrained dynamic equilibria for the nonconvex setting of volume-constraints. Funding: This work was supported by the Deutsche Forschungsgemeinschaft [Grants HA 8041/1-1 and HA 8041/4-1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0577 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0577},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2279-2301},
  shortjournal = {Oper. Res.},
  title        = {Side-constrained dynamic traffic equilibria},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Business model choice for heavy equipment manufacturers.
<em>OR</em>, <em>72</em>(6), 2263–2278. (<a
href="https://doi.org/10.1287/opre.2023.0656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances enable new business models for heavy equipment manufacturers wherein customers access equipment without ownership. We seek to understand the profitability and environmental performance of different emerging business models in light of salient economic and operational factors. We develop a game-theoretic model to identify the optimal choice between a traditional ownership-based business model and two access-based models: servicization and peer-to-peer sharing. After-sales services, equipment characteristics, usage environments, and fuel prices affect this choice. We also provide a novel framework to analyze business models’ environmental impact, which incorporates trade-offs between economic value and environmental costs and shows that all models may create win-win situations for the manufacturer and the environment. Supplemental Material: The computer code and data that support the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2023.0656 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0656},
  journal      = {Operations Research},
  month        = {11-12},
  number       = {6},
  pages        = {2263-2278},
  shortjournal = {Oper. Res.},
  title        = {Business model choice for heavy equipment manufacturers},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utility preference robust optimization with moment-type
information structure. <em>OR</em>, <em>72</em>(5), 2241–2261. (<a
href="https://doi.org/10.1287/opre.2023.2464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Utility preference robust optimization (PRO) models have recently been proposed to deal with decision-making problems where the decision-maker’s true utility function is unknown and the optimal decision is based on the worst-case utility function in an ambiguity set of utility functions. In this paper, we consider the case where the ambiguity set is constructed using some moment-type conditions. We propose piecewise linear approximation of the utility functions in the ambiguity set. The approximate maximin problem can be solved directly by derivative-free methods when the utility functions are nonconcave. Alternatively, we can reformulate the approximate problem as a single mixed integer linear program (MILP) and solve the MILP by existing solvers such as Gurobi. To justify the approximation scheme, we derive error bounds for the approximate ambiguity set, the optimal value and optimal solutions of the approximate maximin problem. To address the data perturbation/contamination issues arising from the construction of the ambiguity set, we derive some stability results which quantify the variation of the ambiguity set against perturbations in the elicitation data and its propagation to the optimal value and optimal solutions of the PRO model. Moreover, we extend the PRO models to allow some inconsistencies in the process of eliciting the decision-maker’s preferences. Finally, we carry out numerical tests to evaluate the performances of the proposed numerical schemes and show that the computational schemes work fairly efficiently, the PRO model is resilient against small perturbations in data (with respect to both exogenous uncertainty data and preference elicitation data), and there is a potential to improve the efficiency of the preference elicitation by incorporating an optimal selection strategy. Funding: This project is supported by Hong Kong RGC [Grant 14500620] and The Chinese University of Hong Kong start-up grant. The research of S. Guo is supported by the National Key R&amp;D Program of China [Grant 2022YFA1004000] and the Natural Science Foundation of China [Grant 12271077]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2464 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2464},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2241-2261},
  shortjournal = {Oper. Res.},
  title        = {Utility preference robust optimization with moment-type information structure},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An exponential cone programming approach for managing
electric vehicle charging. <em>OR</em>, <em>72</em>(5), 2215–2240. (<a
href="https://doi.org/10.1287/opre.2023.2460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To support the rapid growth in global electric vehicle adoption, public charging of electric vehicles is crucial. We study the problem of an electric vehicle charging service provider, which faces (1) stochastic arrival of customers with distinctive arrival/departure times and energy requirements and (2) a total electricity cost including demand charges, which are costs related to the highest per-period electricity used in a finite horizon. We formulate its problem of scheduling vehicle charging to minimize the expected total cost as a stochastic program (SP). As this SP is large-scale, we solve it using exponential cone program (ECP) approximations. For the SP with unlimited chargers, we derive an ECP as an upper bound and characterize the bound on the gap between their theoretical performances. For the SP with limited chargers, we then extend this ECP by also leveraging the idea from distributionally robust optimization (DRO) of using an entropic dominance ambiguity set: Instead of using DRO to mitigate distributional ambiguity, we use it to derive an ECP as a tractable upper bound of the SP. We benchmark our ECP approach with sample average approximation (SAA) and a DRO approach using a semidefinite program (SDP) on numerical instances calibrated to real data. As our numerical instances are large-scale, we find that although SDP cannot be solved, ECP scales well and runs efficiently (about 50 times faster than SAA) and consequently results in a lower mean total cost than SAA. We then show that our ECP continues to perform well considering practical implementation issues, including a data-driven setting and an adaptive charging environment. We finally extend our ECP approaches (for both the uncapacitated and capacitated cases) to include the pricing decision and propose an alternating optimization algorithm, which performs better than SAA on our numerical instances. Our method of constructing ECPs can be potentially applicable to approximate more general two-stage linear SPs with fixed recourse. We also use ECP to generate managerial insights for both charging service providers and policymakers. Funding: L. Chen gratefully acknowledges the financial support from the Singapore Ministry of Education with the 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2460 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2460},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2215-2240},
  shortjournal = {Oper. Res.},
  title        = {An exponential cone programming approach for managing electric vehicle charging},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A converging benders’ decomposition algorithm for two-stage
mixed-integer recourse models. <em>OR</em>, <em>72</em>(5), 2190–2214.
(<a href="https://doi.org/10.1287/opre.2021.2223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new solution method for two-stage mixed-integer recourse models. In contrast to existing approaches, we can handle general mixed-integer variables in both stages. Our solution method is a Benders’ decomposition, in which we iteratively construct tighter approximations of the expected second stage cost function using a new family of optimality cuts. We derive these optimality cuts by parametrically solving extended formulations of the second stage problems using deterministic mixed-integer programming techniques. We establish convergence by proving that the optimality cuts recover the convex envelope of the expected second stage cost function. Finally, we demonstrate the potential of our approach by conducting numerical experiments on several investment planning and capacity expansion problems. Funding: The research of W. Romeijnders has been supported by the Netherlands Organisation for Scientific Research [Grant 451-17-034 4043].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2223},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2190-2214},
  shortjournal = {Oper. Res.},
  title        = {A converging benders’ decomposition algorithm for two-stage mixed-integer recourse models},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Survey of dynamic resource-constrained reward collection
problems: Unified model and analysis. <em>OR</em>, <em>72</em>(5),
2168–2189. (<a href="https://doi.org/10.1287/opre.2023.2441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic resource allocation problems arise under a variety of settings and have been studied across disciplines such as operations research and computer science. The present paper introduces a unifying model for a very large class of dynamic optimization problems that we call dynamic resource-constrained reward collection ( DRC 2 ) problems. We show that this class encompasses a variety of disparate and classical dynamic optimization problems such as dynamic pricing with capacity constraints, dynamic bidding with budgets, network revenue management, online matching, and order fulfillment, to name a few. Furthermore, we establish that the class of DRC 2 problems, although highly general, is amenable to analysis. In particular, we characterize the performance of the fluid certainty-equivalent control heuristic for this class. Notably, this very general result recovers as corollaries some existing specialized results, generalizes other existing results by weakening the assumptions required, and also yields new results in specialized settings for which no such characterization was available. As such, the DRC 2 class isolates some common features of a broad class of problems and offers a new object of analysis. Funding: The work of D. Pizarro was supported by the Artificial and Natural Intelligence Toulouse Institute, which is funded by the French “Investing for the Future—PIA3” program [Grant ANR-19-P3IA-0004]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2441 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2441},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2168-2189},
  shortjournal = {Oper. Res.},
  title        = {Survey of dynamic resource-constrained reward collection problems: Unified model and analysis},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New mixed-integer nonlinear programming formulations for the
unit commitment problems with ramping constraints. <em>OR</em>,
<em>72</em>(5), 2153–2167. (<a
href="https://doi.org/10.1287/opre.2023.2435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unit commitment (UC) problem in electrical power production requires to optimally operate a set of power generation units over a short time horizon. Operational constraints of each unit depend on its type and can be rather complex. For thermal units, typical ones concern minimum and maximum power output, minimum up- and down-time, start-up and shut-down limits, ramp-up and ramp-down limits, and nonlinear objective function. In this work, we present the first mixed-integer nonlinear program formulation that describes the convex hull of the feasible solutions of the single-unit commitment problem comprising all the above constraints and convex power generation costs. The new formulation has a polynomial number of both variables and constraints, and it is based on the efficient dynamic programming algorithm proposed by Frangioni and Gentile together with the perspective reformulation . The proof that the formulation is exact is based on a new extension of a result previously only available in the polyhedral case, which is potentially of interest in itself. We then analyze the effect of using it to develop tight formulations for the more general UC problem. Because the formulation is rather large, we also propose two new formulations, based on partial aggregations of variables, with different trade-offs between quality of the bound and cost of solving the continuous relaxation. Our results show that navigating these trade-offs may lead to improved performances. Funding: A. Frangioni acknowledges the partial financial support by the European Union Horizon 2020 research and innovation programme [Grant 773897 “plan4res”]. A. Frangioni and C. Gentile acknowledge the partial financial support by the European Union Horizon 2020 Marie Skłodowska-Curie Actions [Grant 764759 “MINOA”]. T. Bacci, A. Frangioni, and C. Gentile acknowledge the partial financial support by the Italian Ministry of Education program MIUR-PRIN [Grant 2015B5F27W “Nonlinear and Combinatorial Aspects of Complex Networks”]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2435 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2435},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2153-2167},
  shortjournal = {Oper. Res.},
  title        = {New mixed-integer nonlinear programming formulations for the unit commitment problems with ramping constraints},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minkowski centers via robust optimization: Computation and
applications. <em>OR</em>, <em>72</em>(5), 2135–2152. (<a
href="https://doi.org/10.1287/opre.2023.2448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centers of convex sets are geometric objects that have received extensive attention in the mathematical and optimization literature, both from a theoretical and practical standpoint. For instance, they serve as initialization points for many algorithms such as interior-point, hit-and-run, or cutting-planes methods. First, we observe that computing a Minkowski center of a convex set can be formulated as the solution of a robust optimization problem. As such, we can derive tractable formulations for computing Minkowski centers of polyhedra and convex hulls. Computationally, we illustrate that using Minkowski centers, instead of analytic or Chebyshev centers, improves the convergence of hit-and-run and cutting-plane algorithms. We also provide efficient numerical strategies for computing centers of the projection of polyhedra and of the intersection of two ellipsoids. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2448 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2448},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2135-2152},
  shortjournal = {Oper. Res.},
  title        = {Minkowski centers via robust optimization: Computation and applications},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bonferroni-free and indifference-zone-flexible sequential
elimination procedures for ranking and selection. <em>OR</em>,
<em>72</em>(5), 2119–2134. (<a
href="https://doi.org/10.1287/opre.2023.2447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes two fully sequential procedures for selecting the best system with a guaranteed probability of correct selection (PCS). The main features of the proposed procedures include the following: (1) adopting a Bonferroni-free model that overcomes the conservativeness of the Bonferroni correction and delivers the exact probabilistic guarantee without overshooting; (2) conducting always valid and fully sequential hypothesis tests that enable continuous monitoring of each candidate system and control the type I error rate (or equivalently, PCS) at a prescribed level; and (3) assuming an indifference-zone-flexible formulation, which means that the indifference-zone parameter is not indispensable but could be helpful if provided. We establish statistical validity and asymptotic efficiency for the proposed procedures under normality settings with and without the knowledge of true variances. Numerical studies conducted under various configurations corroborate the theoretical findings and demonstrate the superiority of the proposed procedures. Funding: W. Wang and H. Wan were supported in part by CollinStar Capital Pty Ltd. X. Chen was supported in part by the National Science Foundation [Grant IIS-1849300 and CAREER CMMI-1846663]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2447 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2447},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2119-2134},
  shortjournal = {Oper. Res.},
  title        = {Bonferroni-free and indifference-zone-flexible sequential elimination procedures for ranking and selection},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Real-time spatial–intertemporal pricing and relocation in a
ride-hailing network: Near-optimal policies and the value of dynamic
pricing. <em>OR</em>, <em>72</em>(5), 2097–2118. (<a
href="https://doi.org/10.1287/opre.2022.2425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the growth of ride-hailing services in urban areas, we study a (tactical) real-time spatial–intertemporal dynamic pricing problem where a firm uses a pool of homogeneous servers (e.g., a fleet of taxis) to serve price-sensitive customers (i.e., a rider requesting a trip from an origin to a destination) within a finite horizon (e.g., a day). We consider a revenue maximization problem in a model that captures the stochastic and nonstationary nature of demands, and the nonnegligible travel time from one location to another location. We first show that the relative revenue loss of any static pricing policy is at least in the order of n − 1 / 2 in a large system regime where the demand arrival rate and the number of servers scale linearly with n , which highlights the limitation of static pricing control. We also propose a static pricing control with a matching performance (up to a multiplicative logarithmic term). Next, we develop a novel state-dependent dynamic pricing control with a reduced relative revenue loss of order n − 2 / 3 . The key idea is to dynamically adjust the prices in a way that reduces the impact of past “errors” on the balance of future distributions of servers and customers across the network. Our extensive numerical studies using both a synthetic data set and a real data set from the New York City Taxi and Limousine Commission, confirm our theoretical findings and highlight the benefit of dynamic pricing over static pricing, especially when dealing with nonstationary demands. Interestingly, we also observe that the revenue improvement under our proposed policy primarily comes from an increase in the number of customers served instead of from an increase in the average prices compared with the static pricing policy. This suggests that dynamic pricing can be potentially used to simultaneously increase both revenue and the number of customers served (i.e., service level). Finally, as an extension, we discuss how to generalize the proposed policy to a setting where the firm can also actively relocate some of the available servers to different locations in the network in addition to implementing dynamic pricing. Funding: Y. (M.) Lei was partially supported by the Natural Sciences and Engineering Research Council of Canada [Fund 1378108, Grant RGPIN-2021-02973]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2425 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2425},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2097-2118},
  shortjournal = {Oper. Res.},
  title        = {Real-time Spatial–Intertemporal pricing and relocation in a ride-hailing network: Near-optimal policies and the value of dynamic pricing},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—characterizing and computing the set of nash
equilibria via vector optimization. <em>OR</em>, <em>72</em>(5),
2082–2096. (<a href="https://doi.org/10.1287/opre.2023.2457">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nash equilibria and Pareto optimality are two distinct concepts when dealing with multiple criteria. It is well known that the two concepts do not coincide. However, in this work, we show that it is possible to characterize the set of all Nash equilibria for any noncooperative game as the Pareto-optimal solutions of a certain vector optimization problem. To accomplish this task, we increase the dimensionality of the objective function and formulate a nonconvex ordering cone under which Nash equilibria are Pareto efficient. We demonstrate these results, first, for shared-constraint games in which a joint constraint is applied to all players in a noncooperative game. In doing so, we directly relate our proposed Pareto-optimal solutions to the best response functions of each player. These results are then extended to generalized Nash games, where, in addition to providing an extension of the above characterization, we deduce two vector optimization problems providing necessary and sufficient conditions, respectively, for generalized Nash equilibria. Finally, we show that all prior results hold for vector-valued games as well. Multiple numerical examples are given and demonstrate that our proposed vector optimization formulation readily finds the set of all Nash equilibria.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2457},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2082-2096},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Characterizing and computing the set of nash equilibria via vector optimization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—on hiring secretaries with stochastic
departures. <em>OR</em>, <em>72</em>(5), 2076–2081. (<a
href="https://doi.org/10.1287/opre.2023.2476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a generalization of the secretary problem, where decisions do not have to be made immediately upon applicants’ arrivals. After arriving, each applicant stays in the system for some (random) amount of time and then leaves, whereupon the algorithm has to decide irrevocably whether to select this applicant or not. The arrival and waiting times are drawn from known distributions, and the decision maker’s goal is to maximize the probability of selecting the best applicant overall. Our first main result is a characterization of the optimal policy for this setting. We show that when deciding whether to select an applicant, it suffices to know only the time and the number of applicants that have arrived so far. Furthermore, the policy is monotone nondecreasing in the number of applicants seen so far, and, under certain natural conditions, monotone nonincreasing in time. Our second main result is that when the number of applicants is large, a single threshold policy is almost optimal. Funding: A. Psomas is supported in part by the National Science Foundation [Grant CCF-2144208], a Google Research Scholar Award, and by the Algorand Centres of Excellence program managed by Algorand Foundation. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2476 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2476},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2076-2081},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—On hiring secretaries with stochastic departures},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An algorithmic solution to the blotto game using
multimarginal couplings. <em>OR</em>, <em>72</em>(5), 2061–2075. (<a
href="https://doi.org/10.1287/opre.2023.0049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an efficient algorithm to compute solutions for the general two-player Blotto game on n battlefields with heterogeneous values. Whereas explicit constructions for such solutions have been limited to specific, largely symmetric or homogeneous setups, this algorithmic resolution covers the most general situation to date: a value-asymmetric game with an asymmetric budget with sufficient symmetry and homogeneity. The proposed algorithm rests on recent theoretical advances regarding Sinkhorn iterations for matrix and tensor scaling. An important case that had been out of reach of previous attempts is that of heterogeneous but symmetric battlefield values with asymmetric budgets. In this case, the Blotto game is constant-sum, so optimal solutions exist, and our algorithm samples from an ε -optimal solution in time O ˜ ( n 2 + ε − 4 ) , independent of budgets and battlefield values, up to some natural normalization. In the case of asymmetric values where optimal solutions need not exist but Nash equilibria do, our algorithm samples from an ε -Nash equilibrium with similar complexity but where implicit constants depend on various parameters of the game such as battlefield values. Funding: V. Perchet acknowledges support from the French National Research Agency (ANR) [Grant ANR-19-CE23-0026] as well as the support grant, and Investissements d’Avenir [Grant LabEx Ecodec/ANR-11-LABX-0047]. P. Rigollet is supported by the NSF [Grants IIS-1838071, DMS-2022448, and CCF-2106377]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0049 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0049},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2061-2075},
  shortjournal = {Oper. Res.},
  title        = {An algorithmic solution to the blotto game using multimarginal couplings},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Randomized assortment optimization. <em>OR</em>,
<em>72</em>(5), 2042–2060. (<a
href="https://doi.org/10.1287/opre.2022.0129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When a firm selects an assortment of products to offer to customers, it uses a choice model to anticipate their probability of purchasing each product. In practice, the estimation of these models is subject to statistical errors, which may lead to significantly suboptimal assortment decisions. Recent work has addressed this issue using robust optimization, where the true parameter values are assumed unknown and the firm chooses an assortment that maximizes its worst-case expected revenues over an uncertainty set of likely parameter values, thus mitigating estimation errors. In this paper, we introduce the concept of randomization into the robust assortment optimization literature. We show that the standard approach of deterministically selecting a single assortment to offer is not always optimal in the robust assortment optimization problem. Instead, the firm can improve its worst-case expected revenues by selecting an assortment randomly according to a prudently designed probability distribution. We demonstrate this potential benefit of randomization both theoretically in an abstract problem formulation as well as empirically across three popular choice models: the multinomial logit model, the Markov chain model, and the preference ranking model. We show how an optimal randomization strategy can be determined exactly and heuristically. Besides the superior in-sample performance of randomized assortments, we demonstrate improved out-of-sample performance in a data-driven setting that combines estimation with optimization. Our results suggest that more general versions of the assortment optimization problem—incorporating business constraints, more flexible choice models and/or more general uncertainty sets—tend to be more receptive to the benefits of randomization. Funding: Z. Wang acknowledges funding from the Imperial College President’s PhD Scholarship programme. W. Wiesemann acknowledges funding from the Engineering and Physical Sciences Research Council [Grants EP/R045518/1, EP/T024712/1, and EP/W003317/1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0129 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0129},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2042-2060},
  shortjournal = {Oper. Res.},
  title        = {Randomized assortment optimization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A mutual catastrophe insurance framework for horizontal
collaboration in prepositioning strategic reserves. <em>OR</em>,
<em>72</em>(5), 2014–2041. (<a
href="https://doi.org/10.1287/opre.2021.0141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a mutual catastrophe insurance framework for the prepositioning of strategic reserves to foster horizontal collaboration in preparedness against low-probability high-impact natural disasters. The framework consists of a risk-averse insurer pooling the risks of a portfolio of risk-averse policyholders. It encompasses the operational functions of planning the prepositioning network in preparedness for incoming insurance claims, in the form of units of strategic reserves, setting coverage deductibles and limits of policyholders, and providing insurance coverage to the claims in the emergency response phase. It also encompasses the financial functions of ensuring the insurer’s solvency by efficiently managing its capital and allocating yearly premiums among policyholders. We model the framework as a very large-scale nonlinear multistage stochastic program, and solve it through a Benders decomposition algorithm. We study the case of Caribbean countries establishing a horizontal collaboration for hurricane preparedness. Our results show that the collaboration is more effective when established over a longer planning horizon, and is more beneficial when outsourcing becomes expensive. Moreover, the correlation of policyholders affected simultaneously under the extreme realizations and the position of their claims in their global claims distribution directly affects which policyholders get deductibles and limits. This underlines the importance of prenegotiating policyholders’ indemnification policies at the onset of collaboration. Funding: G. Laporte and M.-È. Rancourt were funded by the Canadian Natural Sciences and Engineering Research Council (NSERC) [Grants 2015-06189 and 2022-04846]. Funding was also provided by the Institute for Data Valorisation (IVADO) and the Canada Research Chair in Humanitarian Supply Chain Analytics. B. Balcik was partially supported by a grant from the Scientific and Technological Research Council of Turkey (TUBITAK) 2219 program. This support is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0141 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0141},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2014-2041},
  shortjournal = {Oper. Res.},
  title        = {A mutual catastrophe insurance framework for horizontal collaboration in prepositioning strategic reserves},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selection and ordering policies for hiring pipelines via
linear programming. <em>OR</em>, <em>72</em>(5), 2000–2013. (<a
href="https://doi.org/10.1287/opre.2023.0061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by hiring pipelines, we study three selection and ordering problems in which applicants for a finite set of positions are interviewed or sent offers. There is a finite time budget for interviewing/sending offers, and every interview/offer is followed by a stochastic realization of discovering the applicant’s quality or acceptance decision, leading to computationally challenging problems. In the first problem, we study sequential interviewing and show that a computationally tractable, nonadaptive policy that must make offers immediately after interviewing is near optimal, assuming offers are always accepted. We further show how to use this policy as a subroutine for obtaining a polynomial-time approximation scheme. In the second problem, we assume that applicants have already been interviewed but only accept offers with some probability; we develop a computationally tractable policy that makes offers for the different positions in parallel, which can be used even if positions are heterogeneous, and is near optimal relative to a policy that can make the same total number of offers one by one. In the third problem, we introduce a parsimonious model of overbooking where all offers are sent simultaneously, and a linear penalty is incurred for each acceptance beyond the number of positions; we provide nearly tight bounds on the performance of practically motivated value-ordered policies. All in all, our paper takes a unified approach to three different hiring problems based on linear programming. Our results in the first two problems generalize and improve the existing guarantees in the literature that were between 1/8 and 1/2 to new guarantees that are at least 1 − 1 / e ≈ 63.2 % . We also numerically compare three different settings of making offers to candidates (sequentially, in parallel, or simultaneously), providing insight into when a firm should favor each one. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2023.0061 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0061},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {2000-2013},
  shortjournal = {Oper. Res.},
  title        = {Selection and ordering policies for hiring pipelines via linear programming},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A pareto dominance principle for data-driven optimization.
<em>OR</em>, <em>72</em>(5), 1976–1999. (<a
href="https://doi.org/10.1287/opre.2021.0609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a statistically optimal approach to construct data-driven decisions for stochastic optimization problems. Fundamentally, a data-driven decision is simply a function that maps the available training data to a feasible action. It can always be expressed as the minimizer of a surrogate optimization model constructed from the data. The quality of a data-driven decision is measured by its out-of-sample risk. An additional quality measure is its out-of-sample disappointment, which we define as the probability that the out-of-sample risk exceeds the optimal value of the surrogate optimization model. The crux of data-driven optimization is that the data-generating probability measure is unknown. An ideal data-driven decision should therefore minimize the out-of-sample risk simultaneously with respect to every conceivable probability measure (and thus in particular with respect to the unknown true measure). Unfortunately, such ideal data-driven decisions are generally unavailable. This prompts us to seek data-driven decisions that minimize the in-sample risk subject to an upper bound on the out-of-sample disappointment—again simultaneously with respect to every conceivable probability measure. We prove that such Pareto dominant data-driven decisions exist under conditions that allow for interesting applications: The unknown data-generating probability measure must belong to a parametric ambiguity set, and the corresponding parameters must admit a sufficient statistic that satisfies a large deviation principle. If these conditions hold, we can further prove that the surrogate optimization model generating the optimal data-driven decision must be a distributionally robust optimization problem constructed from the sufficient statistic and the rate function of its large deviation principle. This shows that the optimal method for mapping data to decisions is, in a rigorous statistical sense, to solve a distributionally robust optimization model. Maybe surprisingly, this result holds irrespective of whether the original stochastic optimization problem is convex or not and holds even when the training data are not independent and identically distributed. As a byproduct, our analysis reveals how the structural properties of the data-generating stochastic process impact the shape of the ambiguity set underlying the optimal distributionally robust optimization model. Funding: This research was supported by the Swiss National Science Foundation under the NCCR Automation [Grant Agreement 51NF40_180545]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2021.0609 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0609},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1976-1999},
  shortjournal = {Oper. Res.},
  title        = {A pareto dominance principle for data-driven optimization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A dynamic model for managing volunteer engagement.
<em>OR</em>, <em>72</em>(5), 1958–1975. (<a
href="https://doi.org/10.1287/opre.2021.0419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonprofit organizations that provide food, shelter, and other services to people in need, rely on volunteers to deliver their services. Unlike paid labor, nonprofit organizations have less control over unpaid volunteers’ schedules, efforts, and reliability. However, these organizations can invest in volunteer engagement activities to ensure a steady and adequate supply of volunteer labor. We study a key operational question of how a nonprofit organization can manage its volunteer workforce capacity to ensure consistent provision of services. In particular, we formulate a multiclass queueing network model to characterize the optimal engagement activities for the nonprofit organization to minimize the costs of enhancing volunteer engagement, while maximizing productive work done by volunteers. Because this problem appears intractable, we formulate an approximating Brownian control problem in the heavy traffic limit and study the dynamic control of that system. Our solution is a nested threshold policy with explicit congestion thresholds that indicate when the nonprofit should optimally pursue various types of volunteer engagement activities. A numerical example calibrated using data from a large food bank shows that our dynamic policy for deploying engagement activities can significantly reduce the food bank’s total annual cost of its volunteer operations while still maintaining almost the same level of social impact. This improvement in performance does not require any additional resources—it only requires that the food bank strategically deploy its engagement activities based on the number of volunteers signed up to work volunteer shifts. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0419 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0419},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1958-1975},
  shortjournal = {Oper. Res.},
  title        = {A dynamic model for managing volunteer engagement},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quality selection in two-sided markets: A constrained price
discrimination approach. <em>OR</em>, <em>72</em>(5), 1928–1957. (<a
href="https://doi.org/10.1287/opre.2020.0754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online platforms collect rich information about participants and then share some of this information back with them to improve market outcomes. In this paper, we study the following information disclosure problem in two-sided markets: if a platform wants to maximize revenue, which sellers should the platform allow to participate, and how much of its available information about participating sellers’ quality should the platform share with buyers? We study this information disclosure problem in the context of two distinct two-sided market models: one in which the platform chooses prices and the sellers choose quantities (similar to ride sharing), and one in which the sellers choose prices (similar to e-commerce). Our main results provide conditions under which simple information structures commonly observed in practice, such as banning certain sellers from the platform and not distinguishing between participating sellers, maximize the platform’s revenue. The platform’s information disclosure problem naturally transforms into a constrained price discrimination problem in which the constraints are determined by the equilibrium outcomes of the specific two-sided market model being studied. We analyze this constrained price discrimination problem to obtain our structural results. Funding: This work was supported by the National Science Foundation [Grants 1839229 and 1931696].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0754},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1928-1957},
  shortjournal = {Oper. Res.},
  title        = {Quality selection in two-sided markets: A constrained price discrimination approach},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Global optimality guarantees for policy gradient methods.
<em>OR</em>, <em>72</em>(5), 1906–1927. (<a
href="https://doi.org/10.1287/opre.2021.0014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Policy gradients methods apply to complex, poorly understood, control problems by performing stochastic gradient descent over a parameterized class of polices. Unfortunately, even for simple control problems solvable by standard dynamic programming techniques, policy gradient algorithms face nonconvex optimization problems and are widely understood to converge only to a stationary point. This work identifies structural properties, shared by several classic control problems, that ensure the policy gradient objective function has no suboptimal stationary points despite being nonconvex. When these conditions are strengthened, this objective satisfies a Polyak-lojasiewicz (gradient dominance) condition that yields convergence rates. We also provide bounds on the optimality gap of any stationary point when some of these conditions are relaxed. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0014 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0014},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1906-1927},
  shortjournal = {Oper. Res.},
  title        = {Global optimality guarantees for policy gradient methods},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simple monotonic readjustment policies with applications to
markdown pricing and pricing in the presence of strategic customers.
<em>OR</em>, <em>72</em>(5), 1893–1905. (<a
href="https://doi.org/10.1287/opre.2020.0774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a canonical revenue management problem wherein a monopolist seller seeks to maximize expected total revenues from selling a fixed inventory of a product to customers who arrive sequentially over time, and the seller is restricted to implement a pricing policy that is monotonic (either nonincreasing or nondecreasing) over time. Gallego and Van Ryzin [Gallego G, Van Ryzin G (1994) Optimal dynamic pricing of inventories with stochastic demand over finite horizons. Management Sci . 40(8):999–1020] show that the simplest monotonic price policy, the fixed price policy, is asymptotically optimal in the high-volume regime in which both the seller’s initial inventory and the length of the selling horizon are proportionally scaled. Specifically, the revenue loss of the fixed price policy is O ( k 1 / 2 ) , where k is the system’s scaling parameter. Following the publication of Gallego and Van Ryzin [Gallego G, Van Ryzin G (1994) Optimal dynamic pricing of inventories with stochastic demand over finite horizons. Management Sci . 40(8):999–1020], several papers have attempted to improve the performance of the fixed price policy. Among them, Jasin [Jasin S (2014) Reoptimization and self-adjusting price control for network revenue management. Oper. Res . 62(5):1168–1178] develops a simple modification of the fixed price policy (that allows prices to move either up or down) with a guaranteed revenue loss of order O ( ln k ) . In this paper, we propose a novel family of monotonic readjustment policy, which restricts the prices to only move in one direction (i.e., either up or down). We show that, if the seller updates the price for only a single time, then the revenue loss of our policy is O ( k 1 / 3 ( ln k ) 2 α ) for some α &gt; 1 / 2 . If, however, the seller updates the prices with a frequency O ( ln k / ln ln k ) , then the revenue loss of our policy is O ( ( ln k ) 7 α ) for some α &gt; 1 / 2 . These results show the power of dynamic pricing even in the presence of monotonic price restriction. We discuss two applications of our policy: markdown pricing and pricing in the presence of strategic customers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0774 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0774},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1893-1905},
  shortjournal = {Oper. Res.},
  title        = {Simple monotonic readjustment policies with applications to markdown pricing and pricing in the presence of strategic customers},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven surgical tray optimization to improve operating
room efficiency. <em>OR</em>, <em>72</em>(5), 1874–1892. (<a
href="https://doi.org/10.1287/opre.2022.2426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surgical procedures account for over 60% of the operating cost of a hospital. About 15% of these costs are related to surgical instruments and supplies. Hospitals spend several million dollars annually on instrument sterilization, instrument tray assembly, and instrument repurchase costs. However, in a large majority of hospitals, less than 20%–30% of reusable instruments supplied to a surgery are used on average. Prior implementations of surgical tray rationalizations have typically been expert driven. This is a labor-intensive effort typically focused on a small set of trays. On the other hand, past mathematical programming model-based studies have typically tested models with simulated data because of the difficulty in obtaining actual instrument-level usage data. We obtained actual surgical instrument usage at a large multispecialty hospital in partnership with OpFlow, a healthcare software company. We formulate a data-driven mathematical optimization model for surgical tray configuration and assignment with the goal of reducing costs of unused instruments, such as sterilization, instrument purchase, and tray assembly costs. We develop a solution methodology that scales to thousands of surgeries, thousands of instruments, and hundreds of surgical trays. This methodology decomposes the problem into a tray rationalization and an add-on tray creation step. At each step, we solve a restricted version of the full problem. We perform extensive out-of-sample testing of our solution. Our model-based approach identifies improvements in tray configuration and assignment, which would lead to a 54% reduction in unused instruments per surgery compared with the current tray configuration used at this hospital. We also validated our model with an expert-recommended solution for a subset of trays. We find that our model-based solution leads to 20% lower overage and 21% lower underage than the expert-recommended solution. We estimate projected annual cost savings of 35% in instrument sterilization, tray assembly costs, and instrument repurchase costs from using the recommendations of our model. Our solution was implemented at the UNC Rex Hospital, and we report on the results of our implementation. This analysis has quantified the value of collecting point-of-usage data to be at least $1.39 million per year from using the model-recommended solution at the hospital. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.2426 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2426},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1874-1892},
  shortjournal = {Oper. Res.},
  title        = {Data-driven surgical tray optimization to improve operating room efficiency},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—near-optimal bayesian online assortment of
reusable resources. <em>OR</em>, <em>72</em>(5), 1861–1873. (<a
href="https://doi.org/10.1287/opre.2020.0687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the applications of rental services in e-commerce, we consider revenue maximization in online assortment of reusable resources for a stream of arriving consumers with different types. We design competitive online algorithms with respect to the optimum online policy in the Bayesian setting in which types are drawn independently from known heterogeneous distributions over time. In the regime where the minimum of initial inventories c min is large, our main result is a near-optimal 1 − min ( 1 2 , log ( c min ) / c min ) competitive algorithm for the general case of reusable resources. Our algorithm relies on an expected LP benchmark for the problem, solves this LP, and simulates the solution through an independent randomized rounding. The main challenge is obtaining point-wise inventory feasibility in a computationally efficient fashion from these simulation-based algorithms. To this end, we use several technical ingredients to design discarding policies— one for each resource. These policies handle the trade-off between the inventory feasibility under reusability and the revenue loss of each of the resources. However, discarding a unit of a resource changes the future consumption of other resources. To handle this new challenge, we also introduce postprocessing assortment procedures that help with designing and analyzing our discarding policies as they run in parallel, which might be of independent interest. As a side result, by leveraging techniques from the literature on prophet inequality, we further show an improved near-optimal 1 − 1 / c min + 3 competitive algorithm for the special case of nonreusable resources. We finally evaluate the performance of our algorithms using the numerical simulations on the synthetic data. Funding: R. Niazadeh’s research is partially supported by an Asness Junior Faculty Fellowship from the University of Chicago Booth School of Business. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2020.0687 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0687},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1861-1873},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Near-optimal bayesian online assortment of reusable resources},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Audit and remediation strategies in the presence of evasion
capabilities. <em>OR</em>, <em>72</em>(5), 1843–1860. (<a
href="https://doi.org/10.1287/opre.2022.0289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore how to uncover an adverse issue that may occur in organizations with the capability to evade detection. To that end, we formalize the problem of designing efficient auditing and remedial strategies as a dynamic mechanism design model. In this setup, a principal seeks to uncover and remedy an issue that occurs to an agent at a random point in time and that harms the principal if not addressed promptly. Only the agent observes the issue’s occurrence, but the principal may uncover it by auditing the agent at a cost. The agent, however, can exert effort to reduce the audit’s effectiveness in discovering the issue. We first establish that this setup reduces to the optimal stochastic control of a piecewise deterministic Markov process. The analysis of this process reveals that the principal should implement a dynamic cyclic auditing and remedial cost-sharing mechanism, which we characterize in closed form. Importantly, we find that the principal should randomly audit the agent unless the agent’s evasion capacity is not very effective, and the agent cannot afford to self-correct the issue. In this latter case, the principal should follow predetermined audit schedules. Funding: This work was supported by the Deutsche Forschungsgemeinschaft (German Research Foundation) [“Audit Schedules in the Presence of Concealing Effort”; Grant 387250733]. Supplemental Material: The computer code and data that supports the findings of this study and the online appendix are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0289 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0289},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1843-1860},
  shortjournal = {Oper. Res.},
  title        = {Audit and remediation strategies in the presence of evasion capabilities},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust financial networks. <em>OR</em>, <em>72</em>(5),
1827–1842. (<a href="https://doi.org/10.1287/opre.2022.0272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study networks of financial institutions where only aggregate information on liabilities is available. We introduce the robust liability network, that is, the network with the worst expected losses among all networks with the same aggregate liabilities and assets. We provide an algorithm to identify the robust liability network and, using aggregate data provided by bank holding companies to the Federal Reserve in form FR Y-9C, determine robust liability networks for U.S. banks under various network configurations. We find that the robust liability network is sparse, with links between institutions that hold highly correlated portfolios. We illustrate the potential of our approach in two ways: We study the evolution of robust liability networks around the onset of the COVID-19 pandemic and evaluate the importance of network structure for financial institutions that are subject to a regulation that limits risk-taking based on each institution’s conditional value-at-risk. Supplemental Material: The online appendices and data files are available at https://doi.org/10.1287/opre.2022.0272 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0272},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1827-1842},
  shortjournal = {Oper. Res.},
  title        = {Robust financial networks},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal cash management with payables finance. <em>OR</em>,
<em>72</em>(5), 1806–1826. (<a
href="https://doi.org/10.1287/opre.2022.0196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Payables finance, also known as reverse factoring or supply chain finance, is a form of trade finance arrangement that provides a supplier with the option to receive a buyer’s payables early while allowing the buyer to extend its payment due date. The recent adoption of the blockchain technology has the potential to make payables finance more efficient and secure. In this paper, we study the supplier’s optimal cash policy under such a “frictionless” payables finance arrangement. Our work extends the classic cash flow management literature in four fronts: (1) we introduce the salient features of payables finance into the cash flow problem; (2) we allow cash flows to be temporally correlated; (3) we adopt a general risk-aversion utility framework for the problem; and (4) we consider a more realistic integrated cash balance model, that is, all interest gains and costs are allowed to accrue together with the cash balance in a single sum. We find the optimal cash policy possesses the “non-borrow-up-to” and “non-invest-down-to” features that differ from the classic ( L , U ) policy known in the literature. We further quantify the value of payables finance to the supplier and determine the equilibrium payment term extension for the buyer. We show that it is the cash liquidity enabled by payables finance to hedge against cash flow uncertainty that generates value to the supplier. To tackle the computational challenge of the problem, we derive easy-to-compute heuristic policies and system bounds. Numerical studies show that heuristic policies achieve near-optimal performance. Finally, we present results from applying our model to data sets obtained from a major U.S. chemical company. Supplemental Material: The computer code and data that supports the findings of this study are available within this article’s supplemental material at https://doi.org/10.1287/opre.2022.0196 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0196},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1806-1826},
  shortjournal = {Oper. Res.},
  title        = {Optimal cash management with payables finance},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Projected inventory-level policies for lost sales inventory
systems: Asymptotic optimality in two regimes. <em>OR</em>,
<em>72</em>(5), 1790–1805. (<a
href="https://doi.org/10.1287/opre.2021.0032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the canonical periodic review lost sales inventory system with positive lead times and stochastic i.i.d. demand under the average cost criterion. We introduce a new policy that places orders such that the expected inventory level at the time of arrival of an order is at a fixed level and call it the projected inventory-level policy. We prove that this policy has a cost rate superior to the equivalent system where excess demand is back-ordered instead of lost and therefore, is asymptotically optimal as the cost of losing a sale approaches infinity under mild distributional assumptions. We further show that this policy dominates the constant-order policy for any finite lead time and therefore, is asymptotically optimal as the lead time approaches infinity for the case of exponentially distributed demand per period. Numerical results show that this policy also performs superior relative to other policies. Funding: This work was supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant 451-16-025]. Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/opre.2021.0032 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0032},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1790-1805},
  shortjournal = {Oper. Res.},
  title        = {Projected inventory-level policies for lost sales inventory systems: Asymptotic optimality in two regimes},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal impact portfolios with general dependence and
marginals. <em>OR</em>, <em>72</em>(5), 1775–1789. (<a
href="https://doi.org/10.1287/opre.2023.0400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a mathematical framework for constructing optimal impact portfolios and quantifying their financial performance by characterizing the returns of impact-ranked assets using induced order statistics and copulas. The distribution of induced order statistics can be represented by a mixture of order statistics and uniformly distributed random variables, where the mixture function is determined by the dependence structure between residual returns and impact factors—characterized by copulas—and the marginal distribution of residual returns. This representation theorem allows us to explicitly and efficiently compute optimal portfolio weights under any copula. This framework provides a systematic approach for constructing and quantifying the performance of optimal impact portfolios with arbitrary dependence structures and return distributions. Funding: Research funding from the China National Key R&amp;D Program [Grant 2022YFA1007900], the China National Natural Science Foundation [Grants 12271013, 72342004], the Fundamental Research Funds for the Central Universities (Peking University), and the MIT Laboratory for Financial Engineering is gratefully acknowledged. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.0400 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0400},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1775-1789},
  shortjournal = {Oper. Res.},
  title        = {Optimal impact portfolios with general dependence and marginals},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic scaling of optimal cost and asymptotic optimality
of base-stock policy in several multidimensional inventory systems.
<em>OR</em>, <em>72</em>(5), 1765–1774. (<a
href="https://doi.org/10.1287/opre.2022.0488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider three classes of inventory systems under long-run average cost: (i) periodic-review systems with lost sales, positive lead times, and a nonstationary demand process; (ii) periodic-review systems for a perishable product with partial backorders and a nonstationary demand process; and (iii) continuous-review systems with fixed lead times, Poisson demand process, and lost sales. The state spaces for these systems are multidimensional, and computations of their optimal control policies/costs are intractable. Because the unit shortage penalty cost is typically much higher than the unit holding cost, we analyze these systems in the regime of large unit penalty cost. When the lead-time demand is unbounded, we establish the asymptotic optimality of the best (modified) base-stock policy and obtain an explicit form solution for the optimal cost rate in each of these systems. This explicit form solution is given in terms of a simple fractile solution of lead-time demand distribution. We also characterize the asymptotic scaling of the optimal cost in the first two systems when the lead-time demand is bounded. Funding: This work was partially supported by the Hong Kong Research Grants Council’s Early Career Scheme [Grant 25505322 to J. Bu] and the General Research Fund [Grant 15507423 to J. Bu and Grant CUHK14500120 to X. Gong]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0488 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0488},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1765-1774},
  shortjournal = {Oper. Res.},
  title        = {Asymptotic scaling of optimal cost and asymptotic optimality of base-stock policy in several multidimensional inventory systems},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—production management with general demands
and lost sales. <em>OR</em>, <em>72</em>(5), 1751–1764. (<a
href="https://doi.org/10.1287/opre.2022.0191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider continuous-review, single-product inventory systems with a constant replenishment rate, Lévy demand, general inventory holding cost, and general lost-sales penalty. The Lévy demand encompasses various demand dynamics used in the inventory literature. We obtain optimal replenishment rates that minimize the time-average cost and expected discounted costs. We can solve this problem explicitly for the optimal replenishment rate by utilizing the renewal theorem for the time-average cost objective. For a more complex expected discounted cost minimization problem, we first obtain the Laplace transform of the cost objective in terms of the unique positive root of the corresponding Lundberg equation. Then, we devise a Fourier-cosine scheme to numerically compute the original cost objective together with a detailed error analysis to determine the optimal production rate. In particular cases, we obtain closed-form expressions of the optimal replenishment rates. The numerical examples further illustrate our numerical method’s accuracy, stability, and robustness. Finally, our Fourier-cosine method can be applied to compute risk analytics, including but not limited to the stockout probability and expected shortfall of the production-inventory system. Funding: S. P. Sethi acknowledges financial support from the Eugene McDermott Chair Professorship. C. C. Siu acknowledges financial support from the Research Grants Council of Hong Kong [Grant “Generalized Sethi Advertising Model and Extensions” with Project UGC/FDS14/P02/20]. S. C. P. Yam acknowledges financial support from [Grant HKGRF-14301321 with Project “General Theory for Infinite Dimensional Stochastic Control: Mean Field and Some Classical Problems” and Grant HKGRF-14300123 with Project “Well-Posedness of Some Poisson-Driven Mean Field Learning Models and Their Applications”]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0191 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0191},
  journal      = {Operations Research},
  month        = {9-10},
  number       = {5},
  pages        = {1751-1764},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Production management with general demands and lost sales},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “competitive two-agent scheduling and its
applications.” <em>OR</em>, <em>72</em>(4), 1749–1750. (<a
href="https://doi.org/10.1287/opre.2023.0368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leung et al. (2010) [Leung JY-T, Pinedo M, Wan G (2010) Competitive two-agent scheduling and its applications. Oper. Res . 58:458–469] considered a two-agent nonpreemptive single-machine scheduling problem. Agent A is responsible for n 1 jobs with due dates d 1 , … , d n and has as the objective the minimization of the total tardiness of the n 1 jobs. Agent B is responsible for n 2 jobs and has as the objective the minimization of the total completion time of the n 2 jobs. The problem is to find a schedule for the n 1 + n 2 jobs that minimizes the objective of agent A (with regard to his n 1 jobs) while keeping the objective of agent B (with regard to his n 2 jobs) below or at a fixed level Q . Leung et al. (2010) [Leung JY-T, Pinedo M, Wan G (2010) Competitive two-agent scheduling and its applications. Oper. Res. 58:458–469], in their theorem 3, showed that this problem can be solved through dynamic programming in pseudopolynomial time. However, in the proof of their theorem and in their dynamic programming formulation, there is an error that requires some changes in their proof.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0368},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1749-1750},
  shortjournal = {Oper. Res.},
  title        = {Erratum to “Competitive two-agent scheduling and its applications”},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—asymptotically optimal control of omnichannel
service systems with pick-up guarantees. <em>OR</em>, <em>72</em>(4),
1739–1748. (<a href="https://doi.org/10.1287/opre.2022.2416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the recent popularity of omnichannel service systems, we analyze the joint admission and scheduling control of a queueing system with two classes of customers: online and walk-in. Unlike walk-in customers, online customers are given a target time for pick up upon placing an order. Thus, in addition to minimizing the waiting costs of walk-in customers and the rejection cost of both classes, we need to minimize the earliness and tardiness costs of online customers. Such a distinctive objective makes the control problem difficult to analyze. We develop a novel analysis by adopting the idea of proving H = λ G to establish an asymptotic relationship between the waiting time cost and the queue length cost under general control policies in the heavy-traffic regime. We also design a policy and show that it is asymptotically optimal. Funding: X. Gao’s research is supported in part by the Hong Kong Research Grants Council [Grants 14201520, 14201421, and 14212522]. J. Huang’s research is supported in part by the Hong Kong Research Grants Council [Grants 14500819, 14505820, and 14501621] and the National Natural Science Foundation of China [Grant 72222023]. J. Zhang’s research is supported in part by the Hong Kong Research Grants Council [Grants 16208120 and 16214121].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2416},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1739-1748},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Asymptotically optimal control of omnichannel service systems with pick-up guarantees},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—risk-averse regret minimization in multistage
stochastic programs. <em>OR</em>, <em>72</em>(4), 1727–1738. (<a
href="https://doi.org/10.1287/opre.2022.2429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the context of optimization under uncertainty, a well-known alternative to minimizing expected value or the worst-case scenario consists in minimizing regret. In a multistage stochastic programming setting with a discrete probability distribution, we explore the idea of risk-averse regret minimization, where the benchmark policy can only benefit from foreseeing Δ steps into the future. The Δ-regret model naturally interpolates between the popular ex ante and ex post regret models. We provide theoretical and numerical insights about this family of models under popular coherent risk measures and shed new light on the conservatism of the Δ-regret minimizing solutions. Funding: This work was supported by Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2016-05208], the Canada Research Chair program [Grant 950-230057], and the Fonds de recherche du Québec–Nature et technologies [Grant 271693]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2429 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2429},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1727-1738},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Risk-averse regret minimization in multistage stochastic programs},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On proportionally consistent solutions to the
divorced-parents problem. <em>OR</em>, <em>72</em>(4), 1710–1726. (<a
href="https://doi.org/10.1287/opre.2022.0470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When Dutch parents divorce, Dutch law dictates that the parental contributions to cover the financial needs of the children have to be proportionally consistent. This rule is clear when parents only have common children. However, cases can be considerably more complicated, for example, when parents have financial responsibilities to children from previous marriages. We show that, mathematically, this settlement problem can be modeled as a bipartite rationing problem for which a unique global proportionally proportional solution exists. Moreover, we develop two efficient algorithms for obtaining this proportionally proportional solution, and we show numerically that both algorithms are considerably faster than standard convex optimization techniques. The first algorithm is a novel tailor-made fixed-point iteration algorithm (FPA), whereas the second algorithm only iteratively applies simple lawsuits involving a single child and its parents. The inspiration for this latter algorithm comes from our main convergence proof in which we show that iteratively applying settlements on smaller subnetworks eventually leads to the same settlement on the network as a whole. This has significant societal importance because, in practice, lawsuits are often only held between two or a few parents. Moreover, our iterative algorithm is easy to understand, also by parents, legal counselors, and judges, which is crucial for its acceptance in practice. Finally, as the method provides a unique solution to any dispute, it removes the legal inequality perceived by parents. Consequently, it may considerably reduce the workload of courts because parents and lawyers can compute the proportionally proportional parental contributions before bringing their case to court.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0470},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1710-1726},
  shortjournal = {Oper. Res.},
  title        = {On proportionally consistent solutions to the divorced-parents problem},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Uncertainty quantification and exploration for reinforcement
learning. <em>OR</em>, <em>72</em>(4), 1689–1709. (<a
href="https://doi.org/10.1287/opre.2023.2436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate statistical uncertainty quantification for reinforcement learning (RL) and its implications in exploration policy. Despite ever-growing literature on RL applications, fundamental questions about inference and error quantification, such as large-sample behaviors, appear to remain quite open. In this paper, we fill in the literature gap by studying the central limit theorem behaviors of estimated Q-values and value functions under various RL settings. In particular, we explicitly identify closed-form expressions of the asymptotic variances, which allow us to efficiently construct asymptotically valid confidence regions for key RL quantities. Furthermore, we utilize these asymptotic expressions to design an effective exploration strategy, which we call Q-value-based Optimal Computing Budget Allocation (Q-OCBA). The policy relies on maximizing the relative discrepancies among the Q-value estimates. Numerical experiments show superior performances of our exploration strategy than other benchmark policies. Funding: This work was supported by the National Science Foundation (1720433).},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2436},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1689-1709},
  shortjournal = {Oper. Res.},
  title        = {Uncertainty quantification and exploration for reinforcement learning},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Best of both worlds: Ex ante and ex post fairness in
resource allocation. <em>OR</em>, <em>72</em>(4), 1674–1688. (<a
href="https://doi.org/10.1287/opre.2022.2432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of allocating indivisible goods among agents with additive valuations. When randomization is allowed, it is possible to achieve compelling notions of fairness such as envy-freeness, which states that no agent should prefer any other agent’s allocation to their own. When allocations must be deterministic, achieving exact fairness is impossible but approximate notions such as envy-freeness up to one good can be guaranteed. Our goal in this work is to achieve both simultaneously, by constructing a randomized allocation that is exactly fair ex ante (before the randomness is realized) and approximately fair ex post (after the randomness is realized). The key question we address is whether ex ante envy-freeness can be achieved in combination with ex post envy-freeness up to one good. We settle this positively by designing an efficient algorithm that achieves both properties simultaneously. The algorithm can be viewed as a desirable way to instantiate a lottery for the probabilistic serial rule. If we additionally require economic efficiency, we obtain three impossibility results that show that ex post or ex ante Pareto optimality is impossible to achieve in conjunction with combinations of fairness properties. Hence, we slightly relax our ex post fairness guarantees and present a different algorithm that can be viewed as a fair way to instantiate a lottery for the maximum Nash welfare allocation rule. Funding: This work was supported by the Office of Naval Research [Grant N00014-17-1-2621] and DST INSPIRE [Grant DST/INSPIRE/04/2020/000107].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2432},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1674-1688},
  shortjournal = {Oper. Res.},
  title        = {Best of both worlds: Ex ante and ex post fairness in resource allocation},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New algorithms for hierarchical optimization in kidney
exchange programs. <em>OR</em>, <em>72</em>(4), 1654–1673. (<a
href="https://doi.org/10.1287/opre.2022.2374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many kidney exchange programs (KEPs) use integer linear programming (ILP) based on a hierarchical set of objectives to determine optimal sets of transplants. We propose innovative techniques to remove barriers in existing mathematical models, vastly reducing solution times and allowing significant increases in potential KEP pool sizes. Our techniques include two methods to avoid unnecessary variables, and a diving algorithm that reduces the need to solve multiple complex ILP models while still guaranteeing optimality of a final solution. We also show how to transition between two existing formulations (namely, the cycle formulation and the position-indexed chain-edge formulation) when optimizing successive objective functions. We use this technique to devise a new algorithm, which, among other features, intelligently exploits the different advantages of the prior two models. We demonstrate the performance of our new algorithms with extensive computational experiments modeling the UK KEP, where we show that our improvements reduce running times by three orders of magnitude compared with the cycle formulation. We also provide substantial empirical evidence that the new methodology offers equally spectacular improvements when applied to the Spanish and Dutch KEP objectives, suggesting that our approach is not just viable, but a significant performance improvement, for many KEPs worldwide. Funding: This work was supported by the Engineering and Physical Sciences Research Council [Grants EP/P028306/1 and EP/P029825/1]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2374 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2374},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1654-1673},
  shortjournal = {Oper. Res.},
  title        = {New algorithms for hierarchical optimization in kidney exchange programs},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unified moment-based modeling of integrated stochastic
processes. <em>OR</em>, <em>72</em>(4), 1630–1653. (<a
href="https://doi.org/10.1287/opre.2022.2422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new method for simulating integrals of stochastic processes. We focus on the nontrivial case of time integrals, conditional on the state variable levels at the endpoints of a time interval through a moment-based probability distribution construction. We present different classes of models with important uses in finance, medicine, epidemiology, climatology, bioeconomics, and physics. The method is generally applicable in well-posed moment problem settings. We study its convergence, point out its advantages through a series of numerical experiments, and compare its performance against existing schemes. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2422 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2422},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1630-1653},
  shortjournal = {Oper. Res.},
  title        = {Unified moment-based modeling of integrated stochastic processes},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for aggregation of malmquist
productivity indices. <em>OR</em>, <em>72</em>(4), 1615–1629. (<a
href="https://doi.org/10.1287/opre.2022.2424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Malmquist productivity index (MPI) has gained popularity among studies on the dynamic change of productivity of decision-making units (DMUs). In practice, this index is frequently reported at aggregate levels (e.g., public and private firms) in the form of simple, equally weighted arithmetic or geometric means of individual MPIs. A number of studies emphasize that it is necessary to account for the relative importance of individual DMUs in the aggregations of indices in general and of the MPI in particular. Whereas more suitable aggregations of MPIs have been introduced in the literature, their statistical properties have not been revealed yet, preventing applied researchers from making essential statistical inferences, such as confidence intervals and hypothesis testing. In this paper, we fill this gap by developing a full asymptotic theory for an appealing aggregation of MPIs. On the basis of this, meaningful statistical inferences are proposed, their finite-sample performances are verified via extensive Monte Carlo experiments, and the importance of the proposed theoretical developments is illustrated with an empirical application to real data. Funding: M. Pham acknowledges support from an Australian Government Research Training Program Scholarship. V. Zelenyuk acknowledges financial support from the University of Queensland and the Australian Research Council [Grant FT170100401]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2424 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2424},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1615-1629},
  shortjournal = {Oper. Res.},
  title        = {Statistical inference for aggregation of malmquist productivity indices},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust assortment optimization under the markov chain choice
model. <em>OR</em>, <em>72</em>(4), 1595–1614. (<a
href="https://doi.org/10.1287/opre.2022.2420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assortment optimization arises widely in many practical applications, such as retailing and online advertising. In this problem, the goal is to select a subset from a universe of substitutable products to offer customers in order to maximize the expected revenue. We study a robust assortment optimization problem under the Markov chain choice model. In this formulation, the parameters of the choice model are assumed to be uncertain, and the goal is to maximize the worst case expected revenue over all parameter values in an uncertainty set. Our main contribution is to prove a min-max duality result when the uncertainty set is row-wise. The result is surprising as the objective function does not satisfy the properties usually needed for known min-max results. Inspired by the duality result, we develop an efficient iterative algorithm for computing the optimal robust assortment under the Markov chain choice model. Moreover, our results yield operational insights into the effect of changing the uncertainty set on the optimal robust assortment. In particular, consistent with previous literature, we find that bigger uncertainty sets always lead to bigger assortments, and a firm should offer larger assortments to hedge against uncertainty. Funding: V. Goyal is supported by NSF Grants CMMI 1351838 and 1636046. B. Jiang was supported by the National Natural Science Foundation of China [Grants 11831002, 72150001, and 72171141] and the Program for Innovative Research Team of Shanghai University of Finance and Economics. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2420 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2420},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1595-1614},
  shortjournal = {Oper. Res.},
  title        = {Robust assortment optimization under the markov chain choice model},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two-stage stochastic matching and pricing with applications
to ride hailing. <em>OR</em>, <em>72</em>(4), 1574–1594. (<a
href="https://doi.org/10.1287/opre.2022.2398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching and pricing are two critical levers in two-sided marketplaces to connect demand and supply. The platform can produce more efficient matching and pricing decisions by batching the demand requests. We initiate the study of the two-stage stochastic matching problem, with or without pricing, to enable the platform to make improved decisions in a batch with an eye toward the imminent future demand requests. This problem is motivated in part by applications in online marketplaces, such as ride-hailing platforms. We design online competitive algorithms for vertex-weighted (or unweighted) two-stage stochastic matching for maximizing supply efficiency and two-stage joint matching and pricing for maximizing market efficiency. In the former problem, using a randomized primal-dual algorithm applied to a family of “balancing” convex programs, we obtain the optimal 3/4 competitive ratio against the optimum off-line benchmark. Using a factor-revealing program and connections to submodular optimization, we improve this ratio against the optimum online benchmark to ( 1 − 1 / e + 1 / e 2 ) ≈ 0.767 for the unweighted and 0.761 for the weighted case. In the latter problem, we design an optimal 1/2-competitive joint pricing and matching algorithm by borrowing ideas from the ex ante prophet inequality literature. We also show an improved ( 1 − 1 / e ) -competitive algorithm for the special case of demand efficiency objective using the correlation gap of submodular functions. Finally, we complement our theoretical study by using DiDi’s ride-sharing data set for Chengdu city and numerically evaluating the performance of our proposed algorithms in practical instances of this problem. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2398 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2398},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1574-1594},
  shortjournal = {Oper. Res.},
  title        = {Two-stage stochastic matching and pricing with applications to ride hailing},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distinguishing useful and wasteful slack. <em>OR</em>,
<em>72</em>(4), 1556–1573. (<a
href="https://doi.org/10.1287/opre.2022.2415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature on organization and strategic management suggests that slack in the form of excess resources may be useful. It may, for example, serve as a buffer against environmental shocks, help decouple organizations, ease planning and implementation, support innovation, and enable effective responses to competitors. In contrast, the economic literature tends to view slack as wasteful. When the same products and services can be produced with fewer resources and slack per se is not assigned any value, slack should be eliminated. The aim of this paper is to reconcile these two perspectives. We acknowledge that slack may be both useful and wasteful. The challenge is how to separate the two. Our approach relies on the simple Pareto idea. If an organization can maintain the same levels of output and slack at lower cost, there is wasteful or nonrationalizable spending. We develop ways to measure the extent to which total spending can be rationalized and show how to statistically estimate and test the usefulness of the available slack using bootstrapping. Funding: Financial support from Det Frie Forskningsråd [Grant 9038-00042A] is greatly appreciated. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2415 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2415},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1556-1573},
  shortjournal = {Oper. Res.},
  title        = {Distinguishing useful and wasteful slack},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heavy-traffic universality of redundancy systems with
assignment constraints. <em>OR</em>, <em>72</em>(4), 1539–1555. (<a
href="https://doi.org/10.1287/opre.2022.2385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service systems often face task-server assignment constraints because of skill-based routing or geographical conditions. Redundancy scheduling responds to this limited flexibility by replicating tasks to specific servers in agreement with these assignment constraints. We gain insight from product-form stationary distributions and weak local stability conditions to establish a state space collapse in heavy traffic. In this limiting regime, the parallel-server system with redundancy scheduling operates as a multiclass single-server system, achieving full resource pooling and exhibiting strong insensitivity to the underlying assignment constraints. In particular, the performance of a fully flexible (unconstrained) system can be matched even with rather strict assignment constraints. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2385 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2385},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1539-1555},
  shortjournal = {Oper. Res.},
  title        = {Heavy-traffic universality of redundancy systems with assignment constraints},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum to “consumer choice under limited attention when
alternatives have different information costs.” <em>OR</em>,
<em>72</em>(4), 1536–1538. (<a
href="https://doi.org/10.1287/opre.2023.0476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an error in one of the results of our paper [Huettner F, Boyacı T, Akçay Y (2019) Consumer choice under limited attention when alternatives have different information costs. Oper. Res. 67(3):671–699]. In this erratum, we point out the error and provide a correction based on Walker-Jones [(2023) Rational inattention with multiple attributes. J. Econom. Theory 212:105688]. Our key characterizations, insights, and numerical examples do not depend on this error and, hence, remain valid. The main implication is on the stopping condition used in the algorithm. We propose a fix based on the new sufficient condition and, if needed, standard convex optimization techniques.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0476},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1536-1538},
  shortjournal = {Oper. Res.},
  title        = {Erratum to “Consumer choice under limited attention when alternatives have different information costs”},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—the generalized sethi advertising model.
<em>OR</em>, <em>72</em>(4), 1526–1535. (<a
href="https://doi.org/10.1287/opre.2021.0717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a flexible yet tractable dynamic advertising model called the generalized Sethi model to capture different market penetration rates across various media and markets via advertising. Specifically, the generalized Sethi model employs a Cobb–Douglas production function of advertising expenditure and the untapped market share with constant returns to scale. It encompasses some standard dynamic advertising models as particular cases. Moreover, the model’s flexibility does not compromise its tractability. We demonstrate it by showing single- and multifirm advertising problems involving Nash and Stackelberg games to admit closed-form expressions for the firms’ optimal advertising strategies and value functions under the generalized model. Sensitivity analysis of the model parameters also yields novel economic insights regarding the firms’ optimal advertising strategies and value functions. Funding: A. P. Kennedy acknowledges the Hong Kong University Grant Council for their support of him pursuing his PhD at the Chinese University of Hong Kong. S. P. Sethi acknowledges financial support from the Eugene McDermott Chair Professorship. C. C. Siu acknowledges financial support from the Research Grants Council of Hong Kong [Grant “Generalized Sethi Advertising Model and Extensions” (Project UGC/FDS14/P02/20)]. S. C. P. Yam acknowledges financial support from HKGRF-14301321 [Project “General Theory for Infinite Dimensional Stochastic Control: Mean Field and Some Classical Problems] and General Research Fund by the Research Grants Council of Hong Kong-14300123 [Project “Well-Posedness of Some Poisson-Driven Mean Field Learning Models and Their Applications”]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0717 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0717},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1526-1535},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—The generalized sethi advertising model},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Harvesting solar power foments prices in a vicious cycle:
Breaking the cycle with price mechanisms. <em>OR</em>, <em>72</em>(4),
1505–1525. (<a href="https://doi.org/10.1287/opre.2021.0756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed solar power generation is growing but not necessarily benefiting the utility firms. Reducing the demand, it hinders the coverage of utility costs with reasonable retail electricity prices. Utilities raise prices, unintentionally reducing both demand and affordability of electricity, and are said to be caught in a utility (death) spiral. The reduced affordability adversely affects consumers who cannot invest in solar generation. Environmentally desirable solar power paradoxically can be socially undesirable. Market regulators are challenged to keep prices low within the current pricing mechanisms. We provide a profit maximization formulation for a regulated utility and reveal the interaction between optimal price increases and growing solar power adoption. Iterating with this interaction, we study the utility death spiral for myopic and forward-looking consumers. We consider new pricing mechanisms with a buyback price and a subscription fee paid only by solar power-generating consumers. The fee mitigates the optimal retail price increase by allowing for the coverage of fixed costs in part. We find appropriate values for the buyback price and subscription fee to, respectively, slow or stop the utility spiral. These mechanisms are important not only for the utility and its regulator but also for all electricity consumers. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0756 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0756},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1505-1525},
  shortjournal = {Oper. Res.},
  title        = {Harvesting solar power foments prices in a vicious cycle: Breaking the cycle with price mechanisms},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Screening with limited information: A dual perspective.
<em>OR</em>, <em>72</em>(4), 1487–1504. (<a
href="https://doi.org/10.1287/opre.2022.0016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a seller seeking a selling mechanism to maximize the worst-case revenue obtained from a buyer whose valuation distribution lies in a certain ambiguity set. Such a mechanism design problem with one product and one buyer is known as the screening problem. For a generic convex ambiguity set, we show via the minimax theorem that strong duality holds between the problem of finding the optimal robust mechanism and a minimax pricing problem where the adversary first chooses a worst-case distribution, and then the seller decides the best posted price mechanism. This implies that the extra value of optimizing over more sophisticated mechanisms amounts exactly to the value of eliminating distributional ambiguity under a posted price mechanism. The duality result also connects prior literature that separately studies the primal (robust screening) and problems related to the dual (e.g., robust pricing, buyer-optimal pricing, and personalized pricing). We further analytically solve the minimax pricing problem (as well as the robust pricing problem) for several important ambiguity sets, such as the ones with mean and various dispersion measures, and with the Wasserstein metric, and we provide a unified geometric intuition behind our approach. The solutions are then used to construct the optimal robust mechanism and to compare with the solutions to the robust pricing problem. We also establish the uniqueness of the worst-case distribution for some cases. Funding: The research is funded in part by the Hong Kong Research Grants Council under its General Research Fund (CUHK-11502422) and the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 (Grant MOE-2019-T3-1-010). Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0016 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0016},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1487-1504},
  shortjournal = {Oper. Res.},
  title        = {Screening with limited information: A dual perspective},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reshaping national organ allocation policy. <em>OR</em>,
<em>72</em>(4), 1475–1486. (<a
href="https://doi.org/10.1287/opre.2022.0035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Organ Procurement &amp; Transplantation Network (OPTN) initiated in 2018 a major overhaul of all U.S. deceased-donor organ allocation policies, aiming to gradually migrate them to a so-called continuous distribution model, with the goal of creating an allocation system that is more efficient, more equitable, and more inclusive. Development of policies within this model, however, represents a major challenge because multiple efficiency and fairness objectives need to be delicately balanced. We introduce a novel analytical framework that leverages machine learning, simulation, and optimization to illuminate policy tradeoffs and enable dynamic exploration of the efficient frontier of policy options. In collaboration with the OPTN, we applied the framework to design a new national allocation policy for lungs. Since March 9, 2023, all deceased-donor lungs in the United States have been allocated according to this policy that we helped design, projected to reduce waitlist mortality by approximately 20% compared with current policy based on simulations. We discuss how we extended our collaboration with the OPTN to the redesign of kidney, pancreas, heart, and liver allocation and how our framework can be applied to other application domains, such as school choice or public housing allocation systems. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0035 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0035},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1475-1486},
  shortjournal = {Oper. Res.},
  title        = {Reshaping national organ allocation policy},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assortment optimization under the multinomial logit model
with utility-based rank cutoffs. <em>OR</em>, <em>72</em>(4), 1453–1474.
(<a href="https://doi.org/10.1287/opre.2021.0060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study assortment optimization problems under a natural variant of the multinomial logit model where the customers are willing to focus only on a certain number of products that provide the largest utilities. In particular, each customer has a rank cutoff, characterizing the number of products that she will focus on during the course of her choice process. Given that we offer a certain assortment of products, the choice process of a customer with rank cutoff k proceeds as follows. The customer associates random utilities with all of the products as well as the no-purchase option. The customer ignores all alternatives whose utilities are not within the k largest utilities. Among the remaining alternatives, the customer chooses the available alternative that provides the largest utility. Under the assumption that the utilities follow Gumbel distributions with the same scale parameter, we provide a recursion to compute the choice probabilities. Considering the assortment optimization problem to find the revenue-maximizing assortment of products to offer, we show that the problem is NP-hard and give a polynomial time approximation scheme. Because the customers ignore the products below their rank cutoffs in our variant of the multinomial logit model, intuitively speaking, our variant captures choosier choice behavior than the standard multinomial logit model. Accordingly, we show that the revenue-maximizing assortment under our variant includes the revenue-maximizing assortment under the standard multinomial logit model, so choosier behavior leads to larger assortments offered to maximize the expected revenue. We conduct computational experiments on both synthetic and real data sets to demonstrate that incorporating rank cutoffs can yield better predictions of customer choices and yield more profitable assortment recommendations. Funding: The work of Y. Bai and H. Topaloglu was partially supported by the National Science Foundation [Grant No. CMMI-1825406]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0060 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0060},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1453-1474},
  shortjournal = {Oper. Res.},
  title        = {Assortment optimization under the multinomial logit model with utility-based rank cutoffs},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fair and efficient online allocations. <em>OR</em>,
<em>72</em>(4), 1438–1452. (<a
href="https://doi.org/10.1287/opre.2022.0332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study trade-offs between fairness and efficiency when allocating indivisible items online. We attempt to minimize envy, the extent to which any agent prefers another’s allocation to their own, while being Pareto efficient. We provide matching lower and upper bounds against a sequence of progressively weaker adversaries. Against worst-case adversaries, we find a sharp trade-off; no allocation algorithm can simultaneously provide both nontrivial fairness and nontrivial efficiency guarantees. In a slightly weaker adversary regime where item values are drawn from (potentially correlated) distributions, it is possible to achieve the best of both worlds. We give an algorithm that is Pareto efficient ex post and either envy free up to one good or envy free with high probability. Neither guarantee can be improved, even in isolation. En route, we give a constructive proof for a structural result of independent interest. Specifically, there always exists a Pareto-efficient fractional allocation that is strongly envy free with respect to pairs of agents with substantially different utilities while allocating identical bundles to agents with identical utilities (up to multiplicative factors). Funding: A. Psomas is supported in part by the National Science Foundation [Award CCF-2144208] and Google [the AI for Social Good Award and the Research Scholar Award]. This work was partially supported by the National Science Foundation [Grants IIS-2147187, IIS-2229881, and CCF-2007080] and the Office of Naval Research [Grant N00014-20-1-2488]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0332 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0332},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1438-1452},
  shortjournal = {Oper. Res.},
  title        = {Fair and efficient online allocations},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Treatment planning for victims with heterogeneous time
sensitivities in mass casualty incidents. <em>OR</em>, <em>72</em>(4),
1400–1420. (<a href="https://doi.org/10.1287/opre.2021.0310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current emergency response guidelines suggest giving priority of treatment to those victims whose initial health conditions are more critical. Although this makes intuitive sense, it does not consider potential deterioration of less critical victims. Deterioration may lead to longer treatment time and irrecoverable health damage, but could be avoided if these victims were to receive care in time. Informed by a unique timestamps data set of surgeries carried out in a field hospital set up in response to a large-scale earthquake, we develop scheduling models to aid treatment planning for mass casualty incidents (MCIs). A distinguishing feature of our modeling framework is to simultaneously consider victim health deterioration and wait-dependent service times in making decisions. We identify conditions under which victims with a less critical initial condition have higher or lower priority than their counterparts in an optimal schedule—the priority order depends on victim deterioration trajectories and the resource (i.e., treatment time) availability. A counterfactual analysis based on our data shows that adopting our model would significantly reduce the surgical makespan and the total numbers of overdue and deteriorated victims compared with using the then-implemented treatment plan; dynamic adjustment of treatment plans (if a second batch of victims arrive) and care coordination among surgical teams could further improve operational efficiency and health outcomes. By demonstrating the value of adopting data-driven approaches in MCI response, our research holds strong potentials to improve emergency response and to inform its policy making. Funding: G. Wan’s work was supported in part by the National Natural Science Foundation of China [Grant 71931008]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0310 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0310},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1400-1420},
  shortjournal = {Oper. Res.},
  title        = {Treatment planning for victims with heterogeneous time sensitivities in mass casualty incidents},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Static pricing for multi-unit prophet inequalities.
<em>OR</em>, <em>72</em>(4), 1388–1399. (<a
href="https://doi.org/10.1287/opre.2023.0031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a pricing problem where a seller has k identical copies of a product, buyers arrive sequentially, and the seller prices the items aiming to maximize social welfare. When k = 1, this is the so-called prophet inequality problem for which there is a simple pricing scheme achieving a competitive ratio of 1/2. On the other end of the spectrum, as k goes to infinity, the asymptotic performance of both static and adaptive pricing is well understood. We provide a static pricing scheme for the small-supply regime: where k is small but larger than one. Prior to our work, the best competitive ratio known for this setting was the 1/2 that follows from the single-unit prophet inequality. Our pricing scheme is easy to describe as well as practical; it is anonymous, nonadaptive, and order oblivious. We pick a single price that equalizes the expected fraction of items sold and the probability that the supply does not sell out before all customers are served; this price is then offered to each customer while supply lasts. This extends an approach introduced by Samuel-Cahn for the case of k = 1. This pricing scheme achieves a competitive ratio that increases gradually with the supply. Subsequent work shows that our pricing scheme is the optimal static pricing for every value of k . Funding: This work was supported by the National Science Foundation [Grants CCF-2008006 and SHF-1704117]. T. Lykouris would like to acknowledge funding from Google Research.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.0031},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1388-1399},
  shortjournal = {Oper. Res.},
  title        = {Static pricing for multi-unit prophet inequalities},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Comparing sequential forecasters. <em>OR</em>,
<em>72</em>(4), 1368–1387. (<a
href="https://doi.org/10.1287/opre.2021.0792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider two forecasters, each making a single prediction for a sequence of events over time. We ask a relatively basic question: how might we compare these forecasters, either online or post hoc, avoiding unverifiable assumptions on how the forecasts and outcomes were generated? In this paper, we present a rigorous answer to this question by designing novel sequential inference procedures for estimating the time-varying difference in forecast scores. To do this, we employ confidence sequences (CS), which are sequences of confidence intervals that can be continuously monitored and are valid at arbitrary data-dependent stopping times (“anytime-valid”). The widths of our CSs are adaptive to the underlying variance of the score differences. Underlying their construction is a game-theoretic statistical framework in which we further identify e-processes and p-processes for sequentially testing a weak null hypothesis—whether one forecaster outperforms another on average (rather than always). Our methods do not make distributional assumptions on the forecasts or outcomes; our main theorems apply to any bounded scores, and we later provide alternative methods for unbounded scores. We empirically validate our approaches by comparing real-world baseball and weather forecasters. Funding: A. Ramdas acknowledges funding from the National Science Foundation Division of Mathematical Sciences [Grant 1916320]. Research reported in this paper was sponsored in part by the DEVCOM Army Research Laboratory under Cooperative Agreement W911NF-17-2-0196 (ARL IoBT CRA). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0792 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0792},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1368-1387},
  shortjournal = {Oper. Res.},
  title        = {Comparing sequential forecasters},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A lyapunov theory for finite-sample guarantees of markovian
stochastic approximation. <em>OR</em>, <em>72</em>(4), 1352–1367. (<a
href="https://doi.org/10.1287/opre.2022.0249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a unified Lyapunov framework for finite-sample analysis of a Markovian stochastic approximation (SA) algorithm under a contraction operator with respect to an arbitrary norm. The main novelty lies in the construction of a valid Lyapunov function called the generalized Moreau envelope . The smoothness and an approximation property of the generalized Moreau envelope enable us to derive a one-step Lyapunov drift inequality, which is the key to establishing the finite-sample bounds. Our SA result has wide applications, especially in the context of reinforcement learning (RL). Specifically, we show that a large class of value-based RL algorithms can be modeled in the exact form of our Markovian SA algorithm. Therefore, our SA results immediately imply finite-sample guarantees for popular RL algorithms such as n -step temporal difference (TD) learning, TD ( λ ) , off-policy V -trace, and Q -learning. As byproducts, by analyzing the convergence bounds of n -step TD and TD ( λ ) , we provide theoretical insight into the problem about the efficiency of bootstrapping. Moreover, our finite-sample bounds of off-policy V -trace explicitly capture the tradeoff between the variance of the stochastic iterates and the bias in the limit. Funding: This work was supported by RTX, the National Science Foundation [Grants 2019844, 2107037, 211247, 2112533, 2144316, and 2240982], and the Machine Learning Laboratory at University of Texas at Austin. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0249 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0249},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1352-1367},
  shortjournal = {Oper. Res.},
  title        = {A lyapunov theory for finite-sample guarantees of markovian stochastic approximation},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Shipping emission control area optimization considering
carbon emission reduction. <em>OR</em>, <em>72</em>(4), 1333–1351. (<a
href="https://doi.org/10.1287/opre.2022.0361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sulfur emission control areas (ECAs) are crucial for reducing global shipping emissions and protecting the environment. The main plank of an ECA policy is usually a fuel sulfur limit. However, the approaches to setting sulfur limits are relatively subjective and lack scientific support. This paper investigates the design of ECA policies, especially sulfur limits, for sailing legs with ECAs. The objective is to minimize the social costs of shipping operations, local sulfur oxides (SO x ) emissions, and global carbon dioxide (CO 2 ) emissions. First, a case with a no-ECA policy and a case with the current ECA policy are analyzed. Then, two new voyage-dependent ECA policies with sulfur limits, designated sailing paths, and speed limits are proposed. Stackelberg game models are developed to solve the research problem with the two proposed policies and two players: the ECA regulator and a shipping company aiming to minimize social costs and company costs, respectively. The ECA regulator determines the sulfur limit, sailing path, and speed limit, and the shipping company optimizes the sailing speed accordingly. We also compare and analyze each type of cost under different ECA policies (i.e., no ECA, the current ECA policy, and the proposed ECA policies). The research problem is then extended from a sailing leg to a shipping network to improve the practicality of the findings. A dynamic programming-based algorithm is developed to optimize the ECA policies for the shipping network from the perspective of the ECA regulator. Mathematical derivation shows that the proposed ECA policies can reduce the social costs of shipping. The results of extensive numerical experiments further demonstrate the ability of the proposed policies to reduce social costs, providing important insights for voyage-dependent ECA policy design. Funding: This work was supported by the National Natural Science Foundation of China [Grants 72025103, 72201163, 71831008, 72071173, 72371221, 72394360, 72394362, and 72361137001]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0361 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0361},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1333-1351},
  shortjournal = {Oper. Res.},
  title        = {Shipping emission control area optimization considering carbon emission reduction},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). UCB-type learning algorithms with kaplan–meier estimator for
lost-sales inventory models with lead times. <em>OR</em>,
<em>72</em>(4), 1317–1332. (<a
href="https://doi.org/10.1287/opre.2022.0273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a classic periodic-review lost-sales inventory system with lead times, which is notoriously challenging to optimize with a wide range of real-world applications. We consider a joint learning and optimization problem in which the decision maker does not know the demand distribution a priori and can only use past sales information (i.e., censored demand). Departing from existing learning algorithms on this learning problem that require the convexity property of the underlying system, we develop an upper confidence bound (UCB)-type learning framework that incorporates simulations with the Kaplan–Meier estimator and demonstrate its applicability to learning not only the optimal capped base-stock policy in which convexity no longer holds, but also the optimal base-stock policy with a regret that matches the best existing result. Compared with a classic multi-armed bandit problem, our problem has unique challenges because of the nature of the inventory system, because (1) each action has long-term impacts on future costs, and (2) the system state space is exponentially large in the lead time. As such, our learning algorithms are not naive adoptions of the classic UCB algorithm; in fact, the design of the simulation steps with the Kaplan–Meier estimator and averaging steps is novel in our algorithms, and the confidence width in the UCB index is also different from the classic one. We prove the regrets of our learning algorithms are tight up to a logarithmic term in the planning horizon T . Our extensive numerical experiments suggest the proposed algorithms (almost) dominate existing learning algorithms. We also demonstrate how to select which learning algorithm to use with limited demand data. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0273 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0273},
  journal      = {Operations Research},
  month        = {7},
  number       = {4},
  pages        = {1317-1332},
  shortjournal = {Oper. Res.},
  title        = {UCB-type learning algorithms with Kaplan–Meier estimator for lost-sales inventory models with lead times},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven optimization with distributionally robust second
order stochastic dominance constraints. <em>OR</em>, <em>72</em>(3),
1298–1316. (<a href="https://doi.org/10.1287/opre.2022.2387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization with stochastic dominance constraints has recently received an increasing amount of attention in the quantitative risk management literature. Instead of requiring that the probabilistic description of the uncertain parameters be exactly known, this paper presents a comprehensive study of a data-driven formulation of the distributionally robust second order stochastic dominance constrained problem (DRSSDCP) that hinges on using a type-1 Wasserstein ambiguity set. This formulation allows us to identify solutions with finite sample guarantees and solutions that are asymptotically consistent when observations are independent and identically distributed. It is, furthermore, shown to be axiomatically motivated in an environment with distribution ambiguity. Leveraging recent results in the field of robust optimization, we further formulate the DRSSDCP as a multistage robust optimization problem and further propose a tractable conservative approximation that exploits finite adaptability and a scenario-based lower bounding problem, both of which can reduce to linear programs under mild conditions. We then propose, to the best of our knowledge, the first exact optimization algorithm for this DRSSDCP, the efficiency of which is confirmed by our numerical results. Finally, we illustrate how the data-driven DRSSDCP can be applied in practice on resource-allocation problems with both synthetic and real data. Our empirical results show that, with a proper adjustment of the size of the Wasserstein ball, DRSSDCP can reach “acceptable” out-of-sample feasibility yet still generating strictly better performance than what is achieved by the reference strategy. Funding: This research was partially supported by the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2016-05208], the Canada Research Chair program [Grant 950-230057], and Groupe d’études et de recherche en analyse des décisions, and it was enabled in part by support provided by Calcul Quebec ( https://www.calculquebec.ca/en/ ) and the Digital Research Alliance of Canada ( https://www.alliancecan.ca/ ). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2387 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2387},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1298-1316},
  shortjournal = {Oper. Res.},
  title        = {Data-driven optimization with distributionally robust second order stochastic dominance constraints},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On-demand ride-matching in a spatial model with abandonment
and cancellation. <em>OR</em>, <em>72</em>(3), 1278–1297. (<a
href="https://doi.org/10.1287/opre.2022.2399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ride-hailing platforms, such as Uber, Lyft, and DiDi, coordinate supply and demand by matching passengers and drivers. The platform has to promptly dispatch drivers when receiving requests because, otherwise, passengers may lose patience and abandon the service by switching to alternative transportation methods. However, having fewer idle drivers results in a possible lengthy pickup time, which is a waste of system capacity and may cause passengers to cancel the service after they are matched. Because of the complex spatial and queueing dynamics, analysis of the matching decision is challenging. In this paper, we propose a spatial model to approximate the pickup time based on the number of waiting passengers and idle drivers. We analyze the dynamics of passengers and drivers in a queueing model in which the platform can control the matching process by setting a threshold on the expected pickup time. Applying fluid approximations, we obtain accurate performance evaluations and an elegant optimality condition, based on which we propose a policy that adapts to time-varying demand. Funding: H. Zhang’s research is partially supported by the National Natural Science Foundation of China [Grants 72201231 and 72192805], the Shenzhen Science and Technology Innovation Commission [Grant RCYX20210609103124047], the Shenzhen Research Institute of Big Data [Grant JSQ202210001], and the Guangdong Provincial Key Laboratory of Big Data Computing at the Chinese University of Hong Kong, Shenzhen. J. Zhang’s research is supported in part by the General Research Fund [Grants 16204718, 16208120, and 16214121] from the Hong Kong Research Grants Council. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2399 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2399},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1278-1297},
  shortjournal = {Oper. Res.},
  title        = {On-demand ride-matching in a spatial model with abandonment and cancellation},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A low-rank approximation for MDPs via moment coupling.
<em>OR</em>, <em>72</em>(3), 1255–1277. (<a
href="https://doi.org/10.1287/opre.2022.2392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a framework to approximate Markov decision processes (MDPs) that stands on two pillars: (i) state aggregation, as the algorithmic infrastructure, and (ii) central-limit-theorem-type approximations, as the mathematical underpinning of optimality guarantees. The theory is grounded in recent work by Braverman et al. [Braverman A, Gurvich I, Huang J (2020) On the Taylor expansion of value functions. Oper. Res. 68(2):631–65] that relates the solution of the Bellman equation to that of a partial differential equation (PDE) where, in the spirit of the central limit theorem, the transition matrix is reduced to its local first and second moments. Solving the PDE is not required by our method. Instead, we construct a “sister” (controlled) Markov chain whose two local transition moments are approximately identical with those of the focal chain. Because of this moment matching , the original chain and its sister are coupled through the PDE, a coupling that facilitates optimality guarantees. Embedded into standard soft aggregation algorithms, moment matching provides a disciplined mechanism to tune the aggregation and disaggregation probabilities. Computational gains arise from the reduction of the effective state space from N to N 1 2 + ϵ is as one might intuitively expect from approximations grounded in the central limit theorem. Funding: This work was supported by the National Science Foundation [Grant CMMI-1662294]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2392 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2392},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1255-1277},
  shortjournal = {Oper. Res.},
  title        = {A low-rank approximation for MDPs via moment coupling},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). S-convexity and gross substitutability. <em>OR</em>,
<em>72</em>(3), 1242–1254. (<a
href="https://doi.org/10.1287/opre.2022.2394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new concept of S-convex functions (and its variant, semistrictly quasi-S- (SSQS)-convex functions) to study substitute structures in economics and operations models with continuous variables. We develop a host of fundamental properties and characterizations of S-convex functions, including various preservation properties, conjugate relationships with submodular and convex functions, and characterizations using Hessians. For a divisible market, we show that the utility function satisfies gross substitutability if and only if it is S-concave under mild regularity conditions. In a parametric maximization model with a box constraint, we show that the set of optimal solutions is nonincreasing in the parameters if the objective function is (SSQS-) S-concave. Furthermore, we prove that S-convexity is necessary for the property of nonincreasing optimal solutions under some conditions. Our monotonicity result is applied to analyze two notable inventory models: a single-product inventory model with multiple unreliable suppliers and a classic multiproduct dynamic inventory model with lost sales. Funding: This work was supported by the National Science Foundation [Grants CMMI-1538451 and CMMI-1635160] and the gift fund from JD.com. Supplemental Material: The online companion is available at https://doi.org/10.1287/opre.2022.2394 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2394},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1242-1254},
  shortjournal = {Oper. Res.},
  title        = {S-convexity and gross substitutability},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online learning and pricing for service systems with
reusable resources. <em>OR</em>, <em>72</em>(3), 1203–1241. (<a
href="https://doi.org/10.1287/opre.2022.2381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a price-based revenue management problem with finite reusable resources over a finite time horizon T . Customers arrive following a price-dependent Poisson process, and each customer requests one unit of c homogeneous reusable resources. If there is an available unit, the customer gets served within a price-dependent exponentially distributed service time; otherwise, the customer waits in a queue until the next available unit. In this paper, we assume that the firm does not know how the arrival and service rates depend on posted prices, and thus it makes adaptive pricing decisions in each period based only on past observations to maximize the cumulative revenue. Given a discrete price set with cardinality P , we propose two online learning algorithms, termed batch upper confidence bound (BUCB) and batch Thompson sampling (BTS), and prove that the cumulative regret upper bound is O ˜ ( P T ) , which matches the regret lower bound. In establishing the regret, we bound the transient system performance upon price changes via a novel coupling argument, and also generalize bandits to accommodate subexponential rewards. We also extend our approach to models with balking and reneging customers and discuss a continuous price setting. Our numerical experiments demonstrate the efficacy of the proposed BUCB and BTS algorithms. Funding: This research was partially supported by an Amazon research award and the Department of Energy [Award DE-SC0018018].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2381},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1203-1241},
  shortjournal = {Oper. Res.},
  title        = {Online learning and pricing for service systems with reusable resources},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integer factorization: Why two-item joint replenishment is
hard. <em>OR</em>, <em>72</em>(3), 1192–1202. (<a
href="https://doi.org/10.1287/opre.2022.2390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distribution networks with periodically repeating events often hold great promise to exploit economies of scale. Joint replenishment problems are fundamental in inventory management, manufacturing, and logistics and capture these effects. However, finding an efficient algorithm that optimally solves these models or showing that none may exist have long been open regardless of whether empty joint orders are possible or not. In either case, we show that finding optimal solutions to joint replenishment instances with just two items is at least as difficult as integer factorization. To the best of the authors’ knowledge, this is the first time integer factorization is used to explain the computational hardness of any optimization problem. We can even prove that the two-item joint replenishment problem with possibly empty joint-ordering points is NP-complete under randomized reductions. This implies that even quantum computers may not be able to solve it efficiently. By relating the computational complexity of joint replenishment to cryptography, prime decomposition, and other aspects of prime numbers, a similar approach may help to establish the (integer-factorization) hardness of additional periodic problems in supply chain management and beyond, whose computational complexity has not been resolved yet. Funding: The first author gratefully acknowledges the support of the Alexander von Humboldt Foundation and the German Federal Ministry of Education and Research. The second author gratefully acknowledges the support of Agencia Nacional de Investigación y Desarrollo [Grant ANID/FONDECYT Iniciación 11200616] and Programa Regional STIC AMSUD [Grant 22-STIC-09].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2390},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1192-1202},
  shortjournal = {Oper. Res.},
  title        = {Integer factorization: Why two-item joint replenishment is hard},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein distributionally robust optimization and
variation regularization. <em>OR</em>, <em>72</em>(3), 1177–1191. (<a
href="https://doi.org/10.1287/opre.2022.2383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wasserstein distributionally robust optimization (DRO) is an approach to optimization under uncertainty in which the decision maker hedges against a set of probability distributions, specified by a Wasserstein ball, for the uncertain parameters. This approach facilitates robust machine learning, resulting in models that sustain good performance when the data are to some extent different from the training data. This robustness is related to the well-studied effect of regularization. The connection between Wasserstein DRO and regularization has been established in several settings. However, existing results often require restrictive assumptions, such as smoothness or convexity, that are not satisfied by many important problems. In this paper, we develop a general theory for the variation regularization effect of the Wasserstein DRO—a new form of regularization that generalizes total-variation regularization, Lipschitz regularization, and gradient regularization. Our results cover possibly nonconvex and nonsmooth losses and losses on non-Euclidean spaces and highlight the bias-variation tradeoff intrinsic in the Wasserstein DRO, which balances between the empirical mean of the loss and the variation of the loss . Example applications include multi-item newsvendor, linear prediction, neural networks, manifold learning, and intensity estimation for Poisson processes. We also use our theory of variation regularization to derive new generalization guarantees for adversarial robust learning. Funding: X. Chen is supported by the National Science Foundation [Grant IIS-1845444]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2383 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2383},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1177-1191},
  shortjournal = {Oper. Res.},
  title        = {Wasserstein distributionally robust optimization and variation regularization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The power of adaptivity for stochastic submodular cover.
<em>OR</em>, <em>72</em>(3), 1156–1176. (<a
href="https://doi.org/10.1287/opre.2022.2388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the stochastic submodular cover problem, the goal is to select a subset of stochastic items of minimum expected cost to cover a submodular function. Solutions in this setting correspond to sequential decision processes that select items one by one adaptively (depending on prior observations). Whereas such adaptive solutions achieve the best objective, the inherently sequential nature makes them undesirable in many applications. We show how to obtain solutions that approximate fully adaptive solutions using only a few “rounds” of adaptivity. We study both independent and correlated settings, proving smooth trade-offs between the number of adaptive rounds and the solution quality. Experiments on synthetic and real data sets show qualitative improvements in the solutions as we allow more rounds of adaptivity; in practice, solutions with a few rounds of adaptivity are nearly as good as fully adaptive solutions. Funding: R. Ghuge and V. Nagarajan were supported in part by the National Science Foundation (NSF) Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1940766] and Division of Computing and Communication Foundations [Grant CCF-2006778]. A. Gupta was supported in part by the NSF [Grants CCF-1907820, CCF1955785, and CCF-2006953]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2388 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2388},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1156-1176},
  shortjournal = {Oper. Res.},
  title        = {The power of adaptivity for stochastic submodular cover},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—greedy algorithm for multiway matching with
bounded regret. <em>OR</em>, <em>72</em>(3), 1139–1155. (<a
href="https://doi.org/10.1287/opre.2022.2400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we prove the efficacy of a simple greedy algorithm for a finite horizon online resource allocation/matching problem when the corresponding static planning linear program (SPP) exhibits a nondegeneracy condition called the general position gap (GPG). The key intuition that we formalize is that the solution of the reward-maximizing SPP is the same as a feasibility linear program restricted to the optimal basic activities, and under GPG, this solution can be tracked with bounded regret by a greedy algorithm, that is, without the commonly used technique of periodically resolving the SPP. The goal of the decision maker is to combine resources (from a finite set of resource types) into configurations (from a finite set of feasible configurations) in which each configuration is specified by the number of resources consumed of each type and a reward. The resources are further subdivided into three types—off-line (whose quantity is known and available at time 0), online-queueable (which arrive online and can be stored in a buffer), and online-nonqueueable (which arrive online and must be matched on arrival or lost). Under GPG, we prove that (i) our greedy algorithm gets bounded anytime regret of O ( 1 / ϵ 0 ) for matching reward ( ϵ 0 is a measure of the GPG) when no configuration contains both an online-queueable and an online-nonqueueable resource and (ii) O ( log t ) expected anytime regret otherwise (we also prove a matching lower bound). By considering the three types of resources, our matching framework encompasses several well-studied problems, such as dynamic multisided matching, network revenue management, online stochastic packing, and multiclass queueing systems. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2400 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2400},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1139-1155},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Greedy algorithm for multiway matching with bounded regret},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—an improved analysis of LP-based control for
revenue management. <em>OR</em>, <em>72</em>(3), 1124–1138. (<a
href="https://doi.org/10.1287/opre.2022.2358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a class of revenue-management problems, where the decision maker aims to maximize the total revenue subject to budget constraints on multiple types of resources over a finite horizon. At each time, a new order/customer/bid is revealed with a request of some resource(s) and a reward, and the decision maker needs to either accept or reject the order. Upon the acceptance of the order, the resource request must be satisfied, and the associated revenue (reward) can be collected. We consider a stochastic setting where all the orders are independent and identically distributed-sampled—that is, the reward-request pair at each time is drawn from an unknown distribution with finite support. The formulation contains many classic applications, such as the quantity-based network revenue-management problem and the Adwords problem. We focus on the classic linear program (LP)-based adaptive algorithm and consider regret as the performance measure defined by the gap between the optimal objective value of the certainty-equivalent LP and the expected revenue obtained by the online algorithm. Our contribution is twofold: (i) When the underlying LP is nondegenerate, the algorithm achieves a problem-dependent regret upper bound that is independent of the horizon/number of time periods T ; and (ii) when the underlying LP is degenerate, the algorithm achieves a tight regret upper bound that scales on the order of T log ( T ) and matches the lower bound up to a logarithmic order. To our knowledge, both results are new and improve the best existing bounds for the LP-based adaptive algorithm in the corresponding setting. We conclude with numerical experiments to further demonstrate our findings. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2358 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2358},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1124-1138},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—An improved analysis of LP-based control for revenue management},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal and differentially private data acquisition: Central
and local mechanisms. <em>OR</em>, <em>72</em>(3), 1105–1123. (<a
href="https://doi.org/10.1287/opre.2022.0014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a platform’s problem of collecting data from privacy sensitive users to estimate an underlying parameter of interest. We formulate this question as a Bayesian-optimal mechanism design problem, in which an individual can share their (verifiable) data in exchange for a monetary reward or services, but at the same time has a (private) heterogeneous privacy cost which we quantify using differential privacy. We consider two popular differential privacy settings for providing privacy guarantees for the users: central and local. In both settings, we establish minimax lower bounds for the estimation error and derive (near) optimal estimators for given heterogeneous privacy loss levels for users. Building on this characterization, we pose the mechanism design problem as the optimal selection of an estimator and payments that will elicit truthful reporting of users’ privacy sensitivities. Under a regularity condition on the distribution of privacy sensitivities, we develop efficient algorithmic mechanisms to solve this problem in both privacy settings. Our mechanism in the central setting can be implemented in time O ( n log n ) where n is the number of users and our mechanism in the local setting admits a polynomial time approximation scheme (PTAS). Funding: A. Fallah acknowledges support from the Apple Scholars in AI/ML PhD fellowship. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0014 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0014},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1105-1123},
  shortjournal = {Oper. Res.},
  title        = {Optimal and differentially private data acquisition: Central and local mechanisms},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic placement in refugee resettlement. <em>OR</em>,
<em>72</em>(3), 1087–1104. (<a
href="https://doi.org/10.1287/opre.2021.0534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employment outcomes of resettled refugees depend strongly on where they are initially placed in the host country. Each week, a resettlement agency is allocated a set of refugees by the U.S. government. The agency must place these refugees in its local affiliates while respecting the affiliates’ annual capacities. We develop an allocation system that recommends where to place an incoming refugee family to improve total employment success. Our algorithm is based on two-stage stochastic programming and achieves over 98% of the hindsight-optimal employment, compared with under 90% of current greedy-like approaches. This dramatic improvement persists even when we incorporate a vast array of practical features of the refugee resettlement process including inseparable families, batching, and uncertainty with respect to the number of future arrivals. Our algorithm is now part of the Annie™ MOORE optimization software used by a leading American refugee resettlement agency. Funding: This work was supported by the UK Economic and Social Research Council [Grant ES/R007470/1]; the Office of Naval Research [Grant N00014-20-1-2488]; and the National Science Foundation’s Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1825348], Division of Computing and Communication Foundations [Grants CCF-1733556 and CCF-2007080], Division of Mathematical Sciences [Grant DMS-1928930], and Division of Information and Intelligent Systems [Grant IIS-2024287]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0534 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0534},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1087-1104},
  shortjournal = {Oper. Res.},
  title        = {Dynamic placement in refugee resettlement},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proximal reinforcement learning: Efficient off-policy
evaluation in partially observed markov decision processes. <em>OR</em>,
<em>72</em>(3), 1071–1086. (<a
href="https://doi.org/10.1287/opre.2021.0781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in an unknown POMDP given observations of trajectories with only partial state observations and generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functions. We term the resulting framework proximal reinforcement learning (PRL). We then show how to construct estimators in these settings and prove they are semiparametrically efficient. We demonstrate the benefits of PRL in an extensive simulation study and on the problem of sepsis management. Funding: This work was supported by the National Science Foundation [Grant 1846210]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0781 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0781},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1071-1086},
  shortjournal = {Oper. Res.},
  title        = {Proximal reinforcement learning: Efficient off-policy evaluation in partially observed markov decision processes},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient decentralized multi-agent learning in asymmetric
bipartite queueing systems. <em>OR</em>, <em>72</em>(3), 1049–1070. (<a
href="https://doi.org/10.1287/opre.2022.0291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study decentralized multiagent learning in bipartite queueing systems, a standard model for service systems. In particular, N agents request service from K servers in a fully decentralized way, that is, by running the same algorithm without communication. Previous decentralized algorithms are restricted to symmetric systems, have performance that is degrading exponentially in the number of servers, require communication through shared randomness and unique agent identities, and are computationally demanding. In contrast, we provide a simple learning algorithm that, when run decentrally by each agent, leads the queueing system to have efficient performance in general asymmetric bipartite queueing systems while also having additional robustness properties. Along the way, we provide the first provably efficient upper confidence bound–based algorithm for the centralized case of the problem. Funding: T. Lykouris would like to acknowledge funding from Google Research. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0291 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0291},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1049-1070},
  shortjournal = {Oper. Res.},
  title        = {Efficient decentralized multi-agent learning in asymmetric bipartite queueing systems},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal dynamic control of an epidemic. <em>OR</em>,
<em>72</em>(3), 1031–1048. (<a
href="https://doi.org/10.1287/opre.2022.0161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze how to optimally engage in social distancing in order to minimize the spread of an infectious disease. We identify conditions under which any optimal policy is single peaked (i.e., first engages in increasingly more social distancing and subsequently decreases its intensity). We show that an optimal policy might substantially delay measures that decrease the transmission rate to create herd immunity and that engaging in social distancing suboptimally early can increase the number of fatalities. Finally, we find that optimal social distancing can be an effective measure to reduce the death rate of a disease. Funding: P. Strack was supported by a Sloan Fellowship.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0161},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1031-1048},
  shortjournal = {Oper. Res.},
  title        = {Optimal dynamic control of an epidemic},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adversarial robustness for latent models: Revisiting the
robust-standard accuracies tradeoff. <em>OR</em>, <em>72</em>(3),
1016–1030. (<a href="https://doi.org/10.1287/opre.2022.0162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past few years, several adversarial training methods have been proposed to improve the robustness of machine learning models against adversarial perturbations in the input. Despite remarkable progress in this regard, adversarial training is often observed to drop the standard test accuracy. This phenomenon has intrigued the research community to investigate the potential tradeoff between standard accuracy (a.k.a generalization) and robust accuracy (a.k.a robust generalization) as two performance measures. In this paper, we revisit this tradeoff for latent models and argue that this tradeoff is mitigated when the data enjoys a low-dimensional structure. In particular, we consider binary classification under two data generative models, namely Gaussian mixture model and generalized linear model, where the features data lie on a low-dimensional manifold. We develop a theory to show that the low-dimensional manifold structure allows one to obtain models that are nearly optimal with respect to both, the standard accuracy and the robust accuracy measures. We further corroborate our theory with several numerical experiments, including Mixture of Factor Analyzers (MFA) model trained on the MNIST data set. Funding: A. Javanmard was partially supported by the Sloan Research Fellowship in mathematics, an Adobe Data Science Faculty Research Award, the National Science Foundation Career Award DMS-1844481, and the National Science Foundation Award 2311024. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0162 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0162},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {1016-1030},
  shortjournal = {Oper. Res.},
  title        = {Adversarial robustness for latent models: Revisiting the robust-standard accuracies tradeoff},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust dynamic assortment optimization in the presence of
outlier customers. <em>OR</em>, <em>72</em>(3), 999–1015. (<a
href="https://doi.org/10.1287/opre.2020.0281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the dynamic assortment optimization problem under the multinomial logit model with unknown utility parameters. The main question investigated in this paper is model mis-specification under the ε -contamination model, which is a fundamental model in robust statistics and machine learning. In particular, throughout a selling horizon of length T , we assume that customers make purchases according to a well-specified underlying multinomial logit choice model in a ( 1 − ε ) -fraction of the time periods and make arbitrary purchasing decisions instead in the remaining ε -fraction of the time periods. In this model, we develop a new robust online assortment optimization policy via an active-elimination strategy. We establish both upper and lower bounds on the regret, and we show that our policy is optimal up to a logarithmic factor in T when the assortment capacity is constant. We further develop a fully adaptive policy that does not require any prior knowledge of the contamination parameter ε . In the case of the existence of a suboptimality gap between optimal and suboptimal products, we also established gap-dependent logarithmic regret upper bounds and lower bounds in both the known- ε and unknown- ε cases. Our simulation study shows that our policy outperforms the existing policies based on upper confidence bounds and Thompson sampling. Funding: X. Chen acknowledges support from the National Science Foundation [Grant IIS-1845444]. Supplemental Material: The supplementary material is available at https://doi.org/10.1287/opre.2020.0281 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0281},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {999-1015},
  shortjournal = {Oper. Res.},
  title        = {Robust dynamic assortment optimization in the presence of outlier customers},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exploration and incentives in reinforcement learning.
<em>OR</em>, <em>72</em>(3), 983–998. (<a
href="https://doi.org/10.1287/opre.2022.0495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do you incentivize self-interested agents to explore when they prefer to exploit ? We consider complex exploration problems, where each agent faces the same (but unknown) Markov decision process (MDP). In contrast with traditional formulations of reinforcement learning, agents control the choice of policies, whereas an algorithm can only issue recommendations. However, the algorithm controls the flow of information, and can incentivize the agents to explore via information asymmetry. We design an algorithm which explores all reachable states in the MDP. We achieve provable guarantees similar to those for incentivizing exploration in static, stateless exploration problems studied previously. To the best of our knowledge, this is the first work to consider mechanism design in a stateful, reinforcement learning setting. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0495 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0495},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {983-998},
  shortjournal = {Oper. Res.},
  title        = {Exploration and incentives in reinforcement learning},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). In congestion games, taxes achieve optimal approximation.
<em>OR</em>, <em>72</em>(3), 966–982. (<a
href="https://doi.org/10.1287/opre.2021.0526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we address the problem of minimizing social cost in atomic congestion games. For this problem, we present lower bounds on the approximation ratio achievable in polynomial time and demonstrate that efficiently computable taxes result in polynomial time algorithms matching such bounds. Perhaps surprisingly, these results show that indirect interventions, in the form of efficiently computed taxation mechanisms, yield the same performance achievable by the best polynomial time algorithm, even when the latter has full control over the agents’ actions. It follows that no other tractable approach geared at incentivizing desirable system behavior can improve upon this result, regardless of whether it is based on taxations, coordination mechanisms, information provision, or any other principle. In short: Judiciously chosen taxes achieve optimal approximation. Three technical contributions underpin this conclusion. First, we show that computing the minimum social cost is NP -hard to approximate within a given factor depending solely on the admissible cost functions. Second, we design a polynomially computable taxation mechanism whose efficiency (price of anarchy) matches this hardness factor, and thus is optimal among all tractable mechanisms. As these results extend to coarse correlated equilibria, any no-regret algorithm inherits the same performances, allowing us to devise polynomial time algorithms with optimal approximation. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0526 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0526},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {966-982},
  shortjournal = {Oper. Res.},
  title        = {In congestion games, taxes achieve optimal approximation},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—risk sensitivity and firm power: Price
competition with mean2-variance profit objective under multinomial logit
demand. <em>OR</em>, <em>72</em>(3), 957–965. (<a
href="https://doi.org/10.1287/opre.2023.2466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is the first in the literature to address a risk-sensitive price competition under the multinomial logit choice model, with each participating firm maximizing a risk-adjusted profit objective. We find that, at equilibrium, a subset of firms earns a positive profit, whereas others are driven to zero profit, contrasting with the risk-neutral equilibrium in which all firms earn a positive profit regardless of quality and cost. We identify a power index —the ratio of effective product attractiveness to risk sensitivity—and show that the set of profitable firms is power index ordered. Risk aversion drives firms toward a more aggressive equilibrium pricing strategy and intensifies competition. However, the relative profit impact across firms is not monotone; although high risk aversion handicaps a firm, moderate risk aversion may place a firm at an advantage over an otherwise equivalent competitor that is less risk sensitive, contrary to intuition. In an equilibrium with market entry cost, we show that the set of active firms at equilibrium follows a generalized power index order that depends on entry cost. Furthermore, although the power index is decreasing in risk aversion, the generalized power index may initially be increasing in risk aversion. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2466 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2466},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {957-965},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Risk sensitivity and firm power: Price competition with mean2-variance profit objective under multinomial logit demand},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—multimarket cournot equilibria with
heterogeneous resource-constrained firms. <em>OR</em>, <em>72</em>(3),
940–956. (<a href="https://doi.org/10.1287/opre.2022.0664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Cournot competition among firms in a multimarket framework where each of the firms face different budget/capacity constraints. We assume independent linear inverse demand functions for each market and completely characterize the resulting unique equilibrium. Specifically, we introduce the notions of augmented and cutoff budgets for firms and markets, respectively. We show, for example, that firm i operates in market j if and only if firm i ’s augmented budget is greater than market j ’s cutoff budget. We also study the properties of the equilibrium as a function of the number of firms N while keeping the aggregate budget fixed. In a numerical study, we show that increasing N increases the total output across all markets although this monotonicity can fail to hold at the individual market level. Similarly, we show that that, although the firms’ cumulative payoff decreases in N , the consumer surplus and social surplus increase in N . Funding: R. Caldentey thanks the University of Chicago Booth School of Business for financial support.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0664},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {940-956},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Multimarket cournot equilibria with heterogeneous resource-constrained firms},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Single-product assemble-to-order systems with exogenous lead
times. <em>OR</em>, <em>72</em>(3), 916–939. (<a
href="https://doi.org/10.1287/opre.2009.0365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a single-product assemble-to-order (ATO) system with exogenous lead times operated under a component base stock policy. Both demand and component lead times are random and are assumed to have finite supports. The challenge of evaluating a base stock policy in an ATO system with random lead times lies in the fact that one needs to compute the distribution of the minimum of n correlated random variables, where n is the number of components. The correlation arises because the replenishment quantities of different components are all contingent on the demand for the final product. We tackle this problem by first looking into two special cases, namely, the one with independent and identically distributed (i.i.d.) lead times and the one with sequential lead times. We give two algorithms for the i.i.d. lead time case, but both of them have exponential complexity in some system parameter. Then, we investigate the case of sequential lead times and utilize its particular structure to develop an algorithm with polynomial complexity. This is the first efficient algorithm for the performance evaluation of base stock policies in an assemble-to-order system with random lead times. Furthermore, using the method as an evaluation oracle in a steepest descent algorithm, we also obtain a polynomial time algorithm to optimize base stock for the case of sequential lead times. For the general case of exogenous lead times, we provide efficiently computable upper and lower bounds, which are identified based on the idea of comparing the level of correlation among component orders. Via extensive numerical studies, we test the performance of approximation methods developed from the identified bounds. We find that our proposed methods have advantages over other approximation methods in the literature, and both of them perform well as part of an approximated base stock optimization algorithm. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2009.0365 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2009.0365},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {916-939},
  shortjournal = {Oper. Res.},
  title        = {Single-product assemble-to-order systems with exogenous lead times},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic contracting in asset management under the
investor-partner-manager relationship. <em>OR</em>, <em>72</em>(3),
903–915. (<a href="https://doi.org/10.1287/opre.2021.0031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study incentive contracts in asset management business under dynamic actions and relationships between an investor, a partner of an investment company, and a fund manager of the company. Both the manager and the partner exert costly effort to manage the investments. The investor cannot perfectly observe the partner’s and manager’s actions, and similarly, the partner cannot perfectly observe the manager’s actions. Thus, we consider a hierarchical contracting framework under hidden efforts, where the investor contracts with the partner and the partner contracts with the manager. We show how the actions of the participants and the costs of their actions interact. For instance, the optimal effort of the manager falls in the effort cost of the partner. We extend the model to a case with an investor, a partner, and multiple managers. In this case, each manager’s effort rises in the effectiveness of the managers’ cooperation and falls in the other managers’ effort cost. Funding: J. Keppo acknowledges funding support from the Singapore Ministry of Education [Grant R-252-000-A08-112] and from the National Research Foundation (Singapore) and Agency for Science, Technology and Research (Singapore) Industry Alignment Fund–Industry Collaboration Projects [Grant I2001E0059]. N. Touzi acknowledges funding from the Chaire Finance and Sustainable development, Risk Foundation, Louis Bachelier Institute. R. Zuo acknowledges funding support from Guangzhou Municipal Science and Technology Project [2023A03J0163 and 2024A04J4546] and the Guangzhou-HKUST(GZ) joint funding program [2024A03J0630]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0031 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0031},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {903-915},
  shortjournal = {Oper. Res.},
  title        = {Dynamic contracting in asset management under the investor-partner-manager relationship},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selling quality-differentiated products in a markovian
market with unknown transition probabilities. <em>OR</em>,
<em>72</em>(3), 885–902. (<a
href="https://doi.org/10.1287/opre.2022.0316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study a firm’s dynamic pricing problem in the presence of unknown and time-varying heterogeneity in customers’ preferences for quality. The firm offers a standard product as well as a premium product to deal with this heterogeneity. First, we consider a benchmark case in which the transition structure of customer heterogeneity is known. In this case, we analyze the firm’s optimal pricing policy and characterize its key structural properties. Thereafter, we investigate the case of unknown market transition structure and design a simple and practically implementable policy, called the bounded learning policy , which is a combination of two policies that perform poorly in isolation. Measuring performance by regret (i.e., the revenue loss relative to a clairvoyant who knows the underlying changes in the market), we prove that our bounded learning policy achieves the fastest possible convergence rate of regret in terms of the frequency of market shifts. Thus, our policy performs well without relying on precise knowledge of the market transition structure. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.0316 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0316},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {885-902},
  shortjournal = {Oper. Res.},
  title        = {Selling quality-differentiated products in a markovian market with unknown transition probabilities},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Endogenous credit, business cycle, and portfolio selection.
<em>OR</em>, <em>72</em>(3), 871–884. (<a
href="https://doi.org/10.1287/opre.2021.0351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a continuous-time model of consumption and portfolio selection of an agent with a limited ability to commit to a debt contract in which the credit limit is endogenously determined. We consider the case where the agent borrows against future income and/or collateral assets. We also study the determination of the credit limit in a general equilibrium model. We derive the credit limit in closed form. The credit limit is smaller than the natural limit because of limited commitment and an increasing function of both income and the collateral price. We extend the baseline model to the case with a regime switch and show that the credit limit is cyclical; it is lower (higher) when the Sharpe ratio is high (low). The model predicts that the rich tend to increase the proportion of risky investments in downturns, whereas the poor decrease them. Funding: H. K. Koo acknowledges the support by the National Research Foundation of Korea grant funded by the Korean Government [NRF-2020R1A2C1A01006134, NRF-2021S1A5A2A03065678], and B. H. Lim acknowledges support by the National Research Foundation of Korea grant funded by the Korean Government (NRF-2020R1F1A1A01076116). Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0351 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0351},
  journal      = {Operations Research},
  month        = {5},
  number       = {3},
  pages        = {871-884},
  shortjournal = {Oper. Res.},
  title        = {Endogenous credit, business cycle, and portfolio selection},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiasing in-sample policy performance for small-data,
large-scale optimization. <em>OR</em>, <em>72</em>(2), 848–870. (<a
href="https://doi.org/10.1287/opre.2022.2377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the poor performance of cross-validation in settings where data are scarce, we propose a novel estimator of the out-of-sample performance of a policy in data-driven optimization. Our approach exploits the optimization problem’s sensitivity analysis to estimate the gradient of the optimal objective value with respect to the amount of noise in the data and uses the estimated gradient to debias the policy’s in-sample performance. Unlike cross-validation techniques, our approach avoids sacrificing data for a test set and uses all data when training and hence is well suited to settings where data are scarce. We prove bounds on the bias and variance of our estimator for optimization problems with uncertain linear objectives but known, potentially nonconvex, feasible regions. For more specialized optimization problems where the feasible region is “weakly coupled” in a certain sense, we prove stronger results. Specifically, we provide explicit high-probability bounds on the error of our estimator that hold uniformly over a policy class and depends on the problem’s dimension and policy class’s complexity. Our bounds show that under mild conditions, the error of our estimator vanishes as the dimension of the optimization problem grows, even if the amount of available data remains small and constant. Said differently, we prove our estimator performs well in the small-data, large-scale regime. Finally, we numerically compare our proposed method to state-of-the-art approaches through a case-study on dispatching emergency medical response services using real data. Our method provides more accurate estimates of out-of-sample performance and learns better-performing policies. Funding: This work was partially supported by the National Science Foundation, Division of Civil, Mechanical and Manufacturing Innovation [Grant 1661732]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.2377 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2377},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {848-870},
  shortjournal = {Oper. Res.},
  title        = {Debiasing in-sample policy performance for small-data, large-scale optimization},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal investment, heterogeneous consumption, and best time
for retirement. <em>OR</em>, <em>72</em>(2), 832–847. (<a
href="https://doi.org/10.1287/opre.2022.2328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an optimal investment and consumption problem with heterogeneous consumption of basic and luxury goods, together with the choice of time for retirement. The utility for luxury goods is not necessarily a concave function. The optimal heterogeneous consumption strategies for a class of nonhomothetic utility maximizer are shown to consume only basic goods when the wealth is small, to consume basic goods and make savings when the wealth is intermediate, and to consume almost all in luxury goods when the wealth is large. The optimal retirement policy is shown to be both universal, in the sense that all individuals should retire at the same level of marginal utility that is determined only by income, labor cost, discount factor and market parameters, and not universal, in the sense that all individuals can achieve the same marginal utility with different utility and wealth. It is also shown that individuals prefer to retire as time goes by if the marginal labor cost increases faster than that of income. The main tools used in analyzing the problem are from a partial differential equation and stochastic control theory including variational inequality and dual transformation. We finally conduct the simulation analysis for the featured model parameters to investigate practical and economic implications by providing their figures. Funding: This work was supported by Hong Kong Research Grants Council General Research Fund [Grants 15202421 and 15202817], the National Research Foundation of Korea [Grant 2021R1C1C1004647], the PolyU-SDU Joint Research Center on Financial Mathematics, the CAS AMSS-PolyU Joint Laboratory of Applied Mathematics, and Hong Kong Polytechnic University, the National Natural Science Foundation of China [Grant 11971409], and the Engineering and Physical Sciences Research Council (UK) [Grant EP/V008331/1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2328 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2328},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {832-847},
  shortjournal = {Oper. Res.},
  title        = {Optimal investment, heterogeneous consumption, and best time for retirement},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equilibrium identification and selection in finite games.
<em>OR</em>, <em>72</em>(2), 816–831. (<a
href="https://doi.org/10.1287/opre.2022.2413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite games provide a framework to model simultaneous competitive decisions among a finite set of players (competitors), each choosing from a finite set of strategies. Potential applications include decisions on competitive production volumes, over capacity decisions to location selection among competitors. The predominant solution concept for finite games is the identification of a Nash equilibrium. We are interested in larger finite games, which cannot efficiently be represented in normal form. For these games, there are algorithms capable of identifying a single equilibrium or all pure equilibria (which may fail to exist in general), however, they do not enumerate all equilibria and cannot select the most likely equilibrium. We propose a solution method for finite games, in which we combine sampling techniques and equilibrium selection theory within one algorithm that determines all equilibria and identifies the most probable equilibrium. We use simultaneous column-and-row generation, by dividing the n -player finite game into a MIP-master problem, capable of identifying equilibria in a sample, and two subproblems tasked with sampling (i) best-responses and (ii) additional solution candidates. We show algorithmic performance in two- and three-player knapsack and facility location and design games and highlight differences in solutions between the proposed approach and state of the art, enabling decision makers in competitive scenarios to base their actions on the most probable equilibrium. Funding: T. Crönert received financial support from Deutsche Forschungsgemeinschaft (AdONE GRK2201 (project number 277991500)). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2413 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2413},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {816-831},
  shortjournal = {Oper. Res.},
  title        = {Equilibrium identification and selection in finite games},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Black-box acceleration of monotone convex program solvers.
<em>OR</em>, <em>72</em>(2), 796–815. (<a
href="https://doi.org/10.1287/opre.2022.2352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a black-box framework for accelerating packing optimization solvers. Our method applies to packing linear programming problems and a family of convex programming problems with linear constraints. The framework is designed for high-dimensional problems, for which the number of variables n is much larger than the number of measurements m . Given an ( m × n ) problem, we construct a smaller ( m × ϵ n ) problem, whose solution we use to find an approximation to the optimal solution. Our framework can accelerate both exact and approximate solvers. If the solver being accelerated produces an α -approximation, then we produce a ( 1 − ϵ ) / α 2 -approximation of the optimal solution to the original problem. We present worst-case guarantees on run time and empirically demonstrate speedups of two orders of magnitude. Funding: Financial support from the National Science Foundation [Grants AitF-1637598, CNS-151894, and CPS-154471] and the Linde Institute is gratefully acknowledged.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2352},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {796-815},
  shortjournal = {Oper. Res.},
  title        = {Black-box acceleration of monotone convex program solvers},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data-driven ranking and selection under input uncertainty.
<em>OR</em>, <em>72</em>(2), 781–795. (<a
href="https://doi.org/10.1287/opre.2022.2375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a simulation-based ranking and selection (R&amp;S) problem with input uncertainty, in which unknown input distributions can be estimated using input data arriving in batches of varying sizes over time. Each time a batch arrives, additional simulations can be run using updated input distribution estimates. The goal is to confidently identify the best design after collecting as few batches as possible. We first introduce a moving average estimator for aggregating simulation outputs generated under heterogenous input distributions. Then, based on a sequential elimination framework, we devise two major R&amp;S procedures by establishing exact and asymptotic confidence bands for the estimator. We also extend our procedures to the indifference zone setting, which helps save simulation effort for practical usage. Numerical results show the effectiveness and necessity of our procedures in controlling error from input uncertainty. Moreover, the efficiency can be further boosted through optimizing the “drop rate” parameter, which is the proportion of past simulation outputs to discard, of the moving average estimator. Funding: The authors gratefully acknowledge support by the National Science Foundation Division of Civil, Mechanical and Manufacturing Innovation [Grant CMMI-1453934] and Division of Mathematical Sciences [Grant DMS2053489] and the Air Force Office of Scientific Research [Grants FA9550-19-1-0283 and FA9550-22-1-0244]. Supplemental Material: The electronic companion is available at https://doi.org/10.1287/opre.2022.2375 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2375},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {781-795},
  shortjournal = {Oper. Res.},
  title        = {Data-driven ranking and selection under input uncertainty},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast quantum subroutines for the simplex method.
<em>OR</em>, <em>72</em>(2), 763–780. (<a
href="https://doi.org/10.1287/opre.2022.2341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose quantum subroutines for the simplex method that avoid classical computation of the basis inverse. We show how to quantize all steps of the simplex algorithm, including checking optimality, unboundedness, and identifying a pivot (i.e., pricing the columns and performing the ratio test) according to Dantzig’s rule or the steepest edge rule. The quantized subroutines obtain a polynomial speedup in the dimension of the problem but have worse dependence on other numerical parameters. For example, for a problem with m constraints, n variables, at most d c nonzero elements per column of the costraint matrix, at most d nonzero elements per column or row of the basis, basis condition number κ , and optimality tolerance ϵ, pricing can be performed in O ˜ ( ϵ − 1 κ d n ( d c n + d m ) ) time, where the O ˜ notation hides polylogarithmic factors; classically, pricing requires O ( d c 0.7 m 1.9 + m 2 + o ( 1 ) + d c n ) time in the worst case using the fastest known algorithm for sparse matrix multiplication. For well-conditioned sparse problems, the quantum subroutines scale better in m and n and may therefore have an advantage for very large problems. The running time of the quantum subroutines can be improved if the constraint matrix admits an efficient algorithmic description or if quantum RAM is available. Funding: This work was supported by the Army Research Office [Grant W911NF-20-1-0014] and the Air Force Research Laboratory [Grant FA8750-C-18-0098]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2341 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2341},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {763-780},
  shortjournal = {Oper. Res.},
  title        = {Fast quantum subroutines for the simplex method},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint inventory and pricing for a one-warehouse multistore
problem: Spiraling phenomena, near optimal policies, and the value of
dynamic pricing. <em>OR</em>, <em>72</em>(2), 738–762. (<a
href="https://doi.org/10.1287/opre.2022.2389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a joint inventory and pricing problem with one warehouse and multiple stores in which the retailer makes a one-time decision on the amount of inventory to be placed at the warehouse at the beginning of the selling season, followed by periodic joint replenishment and pricing decisions for each store throughout the season. Demand at each store follows a Poisson distribution, and unmet demand is immediately lost. The retailer incurs the usual variable ordering, inventory holding, and lost sales costs, and the retailer’s objective is to maximize the expected total profits. The optimal control (or policy) for this problem is unknown and numerically challenging to compute. We first analyze the performance of two popular and simple heuristic policies that directly implement the solution of a deterministic approximation of the original stochastic problem. We show that simple reoptimization of the deterministic problem may yield a very poor performance by causing a “spiraling down” movement in price trajectory, which, in turn, yields a “spiraling up” movement in expected lost sales quantity (i.e., the expected lost sales quantity continues to increase as we reoptimize more frequently). This finding cautions against a naive use of simple reoptimizations without first understanding the dynamics of the model. We then propose two improved heuristic policies based on the optimal solution of a deterministic relaxation of the original stochastic problem. Our first heuristic policy computes static prices and order-up-to levels for the warehouse and stores and then replenishes each store at the beginning of each batch of periods. Our second heuristic policy builds on the first heuristic and dynamically adjusts prices over time based on realized demands. We show that both policies have near-optimal performance when the annual market size is large with the second policy outperforming the first one. Finally, we also prove a fundamental theoretical lower bound on the performance of any policy that relies on static prices. This lower bound highlights the true value of dynamic pricing, whose effect on performance in our setting cannot be duplicated by simply implementing a more sophisticated replenishment policy. Funding: Y. (M.) Lei is partially supported by the Natural Sciences and Engineering Research Council of Canada [Fund 1378108, Grant RGPIN-2021-02973]. S. Liu is partially supported by the Natural Sciences and Engineering Research Council of Canada [Fund 513987, Grant RGPIN-2022-04950]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2389 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2389},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {738-762},
  shortjournal = {Oper. Res.},
  title        = {Joint inventory and pricing for a one-warehouse multistore problem: Spiraling phenomena, near optimal policies, and the value of dynamic pricing},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Lagrangian dual decision rules for multistage stochastic
mixed-integer programming. <em>OR</em>, <em>72</em>(2), 717–737. (<a
href="https://doi.org/10.1287/opre.2022.2366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistage stochastic programs can be approximated by restricting policies to follow decision rules. Directly applying this idea to problems with integer decisions is difficult because of the need for decision rules that lead to integral decisions. In this work, we introduce Lagrangian dual decision rules (LDDRs) for multistage stochastic mixed-integer programming (MSMIP), which overcome this difficulty by applying decision rules in a Lagrangian dual of the MSMIP. We propose two new bounding techniques based on stagewise (SW) and nonanticipative (NA) Lagrangian duals, where the Lagrangian multiplier policies are restricted by LDDRs. We demonstrate how the solutions from these duals can be used to drive primal policies. Our proposal requires fewer assumptions than most existing MSMIP methods. We compare the theoretical strength of the restricted duals and show that the restricted NA dual can provide relaxation bounds at least as good as the ones obtained by the restricted SW dual. In our numerical study on two problem classes, one traditional and one novel, we observe that the proposed LDDR approaches yield significant optimality-gap reductions compared with existing general-purpose bounding methods for MSMIP problems. Funding: Financial support from the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2018-04984]; the National Science Foundation [Grant CMMI-1634597]; and the Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics [Contract Number DE-AC02- 06CH113] is gratefully acknowledged. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2366 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2366},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {717-737},
  shortjournal = {Oper. Res.},
  title        = {Lagrangian dual decision rules for multistage stochastic mixed-integer programming},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliable off-policy evaluation for reinforcement learning.
<em>OR</em>, <em>72</em>(2), 699–716. (<a
href="https://doi.org/10.1287/opre.2022.2382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a sequential decision-making problem, off-policy evaluation estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. Reinforcement learning in high-stake environments, such as healthcare and education, is often limited to off-policy settings due to safety or ethical concerns or inability of exploration. Hence, it is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. In this paper, we propose a novel framework that provides robust and optimistic cumulative reward estimates using one or multiple logged trajectories data. Leveraging methodologies from distributionally robust optimization, we show that with proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with nonasymptotic and asymptotic guarantees under stochastic or adversarial environments. Our results are also generalized to batch reinforcement learning and are supported by empirical analysis. Funding: This work was supported by the Shenzhen Institute of Artificial Intelligence and Robotics for Society, Shenzhen Science and Technology Program [Grant JCYJ20210324120011032]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2382 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2382},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {699-716},
  shortjournal = {Oper. Res.},
  title        = {Reliable off-policy evaluation for reinforcement learning},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investment in the common good: Free rider effect and the
stability of mixed strategy equilibria. <em>OR</em>, <em>72</em>(2),
684–698. (<a href="https://doi.org/10.1287/opre.2022.2371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the game of investment in the common good, the free rider problem can delay the stakeholders’ actions in the form of a mixed strategy equilibrium. However, it has been recently shown that the commonly known form of mixed strategy equilibria of the stochastic war of attrition is destabilized by even the slightest degree of asymmetry between the players. Such extreme instability is contrary to the widely accepted notion that a mixed strategy equilibrium is the hallmark of the war of attrition. Motivated by this quandary, we search for a mixed strategy equilibrium in a stochastic game of investment in the common good. Our results show that, despite asymmetry, a mixed strategy equilibrium exists if the model takes into account the repeated investment opportunities. This class of mixed strategy equilibria disappear only if the asymmetry is sufficiently high. Because the mixed strategy equilibrium is less efficient than pure strategy equilibria, it behooves policy makers to prevent it by promoting a sufficiently high degree of asymmetry between the stakeholders through, for example, asymmetric subsidy. Funding: Y. Kim gratefully acknowledges the financial support from the ISM department in the Culverhouse College of Business. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2371 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2371},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {684-698},
  shortjournal = {Oper. Res.},
  title        = {Investment in the common good: Free rider effect and the stability of mixed strategy equilibria},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sample and computationally efficient stochastic kriging in
high dimensions. <em>OR</em>, <em>72</em>(2), 660–683. (<a
href="https://doi.org/10.1287/opre.2022.2367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic kriging has been widely employed for simulation metamodeling to predict the response surface of complex simulation models. However, its use is limited to cases where the design space is low-dimensional because in general the sample complexity (i.e., the number of design points required for stochastic kriging to produce an accurate prediction) grows exponentially in the dimensionality of the design space. The large sample size results in both a prohibitive sample cost for running the simulation model and a severe computational challenge due to the need to invert large covariance matrices. Based on tensor Markov kernels and sparse grid experimental designs, we develop a novel methodology that dramatically alleviates the curse of dimensionality. We show that the sample complexity of the proposed methodology grows only slightly in the dimensionality, even under model misspecification. We also develop fast algorithms that compute stochastic kriging in its exact form without any approximation schemes. We demonstrate via extensive numerical experiments that our methodology can handle problems with a design space of more than 10,000 dimensions, improving both prediction accuracy and computational efficiency by orders of magnitude relative to typical alternative methods in practice. Funding: We gratefully acknowledge financial support from the Hong Kong Research Grants Council (GRF 17206821). Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2367 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2367},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {660-683},
  shortjournal = {Oper. Res.},
  title        = {Sample and computationally efficient stochastic kriging in high dimensions},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Advances in MINLP to identify energy-efficient distillation
configurations. <em>OR</em>, <em>72</em>(2), 639–659. (<a
href="https://doi.org/10.1287/opre.2022.2340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we describe the first mixed-integer nonlinear programming (MINLP)-based solution approach that successfully identifies the most energy-efficient distillation configuration sequence for a given separation. Current sequence design strategies are largely heuristic. The rigorous approach presented here can help reduce the significant energy consumption and consequent greenhouse gas emissions by separation processes. First, we model discrete choices using a formulation that is provably tighter than previous formulations. Second, we highlight the use of partial fraction decomposition alongside reformulation-linearization technique (RLT). Third, we obtain convex hull results for various special structures. Fourth, we develop new ways to discretize the MINLP. Finally, we provide computational evidence to demonstrate that our approach significantly outperforms the state-of-the-art techniques. Funding: This work was supported by the U.S. Department of Energy [Grant DE-EE0005768], the Bilsland Dissertation Fellowship, and the National Science Foundation [Grant EEC-1647722]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2340 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2340},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {639-659},
  shortjournal = {Oper. Res.},
  title        = {Advances in MINLP to identify energy-efficient distillation configurations},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient learning for clustering and optimizing
context-dependent designs. <em>OR</em>, <em>72</em>(2), 617–638. (<a
href="https://doi.org/10.1287/opre.2022.2368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a simulation optimization problem for context-dependent decision making. Under a Gaussian mixture model-based Bayesian framework, we develop a dynamic sampling policy to maximize the worst-case probability of correctly selecting the best design over all contexts, which utilizes both global clustering information and local performance information. In particular, we design a computationally efficient approximation method to learn these sources of information, thereby leading to an implementable dynamic sampling policy. The proposed sampling policy is proved to be consistent and achieve the asymptotically optimal sampling ratio. Numerical experiments show that the proposed approximation method makes a good balance between the performance and complexity, and the proposed sampling policy significantly improves the efficiency in context-dependent simulation optimization. Funding: This work was supported in part by the National Natural Science Foundation of China [Grants 72022001, 92146003, and 71901003], by the National Science Foundation [Awards ECCS-1462409, CMMI-1462787, CAREER CMMI-1834710, and IIS-1849280], and by the China Scholarship Council [scholarship under Grant China Scholarship Council]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2368 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2368},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {617-638},
  shortjournal = {Oper. Res.},
  title        = {Efficient learning for clustering and optimizing context-dependent designs},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-order steady-state diffusion approximations.
<em>OR</em>, <em>72</em>(2), 604–616. (<a
href="https://doi.org/10.1287/opre.2022.2362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive and analyze new diffusion approximations of stationary distributions of Markov chains that are based on second- and higher-order terms in the expansion of the Markov chain generator. Our approximations achieve a higher degree of accuracy compared with diffusion approximations widely used for the last 50 years while retaining a similar computational complexity. To support our approximations, we present a combination of theoretical and numerical results across three different models. Our approximations are derived recursively through Stein/Poisson equations, and the theoretical results are proved using Stein’s method. Funding: X. Fang is partially supported by Hong Kong RGC [Grants 24301617, 14302418, and 14304917], a CUHK direct grant, and a CUHK start-up grant. J. G. Dai is partially supported by NSF [Grant CMMI-1537795]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2022.2362 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2362},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {604-616},
  shortjournal = {Oper. Res.},
  title        = {High-order steady-state diffusion approximations},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—masking anstreicher’s linx bound for improved
entropy bounds. <em>OR</em>, <em>72</em>(2), 591–603. (<a
href="https://doi.org/10.1287/opre.2022.2324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum-entropy sampling problem is the NP-hard problem of maximizing the (log) determinant of an order- s principal submatrix of a given order n covariance matrix C . Exact algorithms are based on a branch-and-bound framework. The problem has wide applicability in spatial statistics and in particular in environmental monitoring. Probably the best upper bound for the maximum empirically is Anstreicher’s scaled “linx” bound. An earlier methodology for potentially improving any upper-bounding method is by masking, that is, applying the bounding method to C ∘ M , where M is any correlation matrix. We establish that the linx bound can be improved via masking by an amount that is at least linear in n , even when optimal scaling parameters are used. We also extend an earlier result that the linx bound is convex in the logarithm of a scaling parameter, making a full characterization of its behavior and providing an efficient means of calculating its limiting behavior in all cases. Funding: J. Lee was supported by the Air Force Office of Scientific Research [Grant FA9550-19-1-0175]. M. Fampa was supported by the Conselho Nacional de Desenvolvimento Científico e Tecnológico [Grants 303898/2016-0 and 434683/2018-3].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2324},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {591-603},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Masking anstreicher’s linx bound for improved entropy bounds},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flattening energy-consumption curves by monthly constrained
direct load control contracts. <em>OR</em>, <em>72</em>(2), 570–590. (<a
href="https://doi.org/10.1287/opre.2021.0638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing electricity demand and supply is one of the most critical tasks that utility firms perform to maintain grid stability and reduce system cost. Demand-response programs are among the strategies that utilities use to reduce electricity consumption during peak hours and flatten the energy-consumption curve. Direct load control contracts (DLCCs) are a class of incentive-based demand-response programs that allow utilities to assign “calls” to customer groups to reduce their energy usage by a prespecified amount for a given length of time. Given the rapid expansion of such contracts, in this paper, we develop an integer stochastic dynamic optimization problem for executing DLCCs that minimizes total system cost subject to monthly and annual constraints on the number of times and hours customers can be called. We develop a hierarchical approximation approach, which consists of an annual problem and monthly problems, to solve the DLCC implementation problem effectively and in a reasonable amount of time. Motivated by the practice in a large utility firm in California, we incorporate a reduce-to-threshold policy that attempts to flatten energy-consumption curves whenever demand exceeds a given threshold. We verified the quality of our proposed approach on real data from the California Independent System Operator, which is the umbrella organization of the utility firms in California, and measured the quality of our solution against a lower bound. A large utility firm in California implemented our model and informed us that the additional reduction in cost was approximately 4%. Our sensitivity analysis reports the impact of managerial concerns on some policies to enhance customer experience and provides insights for improving the features of DLCC contracts. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0638 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0638},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {570-590},
  shortjournal = {Oper. Res.},
  title        = {Flattening energy-consumption curves by monthly constrained direct load control contracts},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent agents in networks: Estimation and targeting.
<em>OR</em>, <em>72</em>(2), 549–569. (<a
href="https://doi.org/10.1287/opre.2023.2485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a platform that serves (observable) agents, who belong to a larger network that also includes additional agents who are not served by the platform. We refer to the latter group of agents as latent agents. Associated with each agent are the agent’s covariate and outcome. The platform has access to past covariates and outcomes of the observable agents, but no data for the latent agents is available to the platform. Crucially, the agents influence each other’s outcome through a certain influence structure. In particular, observable agents influence each other both directly and indirectly through the influence they exert on the latent agents. The platform doesn’t know the inference structure of either the observable or the latent parts of the network. We investigate how the platform can estimate the dependence of the observable agents’ outcomes on their covariates, taking the presence of the latent agents into account. First, we show that a certain matrix succinctly captures the relationship between the outcomes and the covariates. We provide an algorithm that estimates this matrix using historical data of covariates and outcomes for the observable agents under a suitable approximate sparsity condition. We also establish convergence rates for the proposed estimator despite the high dimensionality that allows more agents than observations. Second, we show that the approximate sparsity condition holds under the standard conditions used in the literature. Hence, our results apply to a large class of networks. Finally, we illustrate the applications to a targeted advertising problem. We show that, by using the available historical data with our estimator, it is possible to obtain asymptotically optimal advertising decisions despite the presence of latent agents. Funding: O. Candogan acknowledges NSF award 2216912 for “Institute for Data, Econometrics, Algorithms and Learning”. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2485 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2485},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {549-569},
  shortjournal = {Oper. Res.},
  title        = {Latent agents in networks: Estimation and targeting},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing, voluntary social distancing, and the spread of an
infection. <em>OR</em>, <em>72</em>(2), 533–548. (<a
href="https://doi.org/10.1287/opre.2021.2220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effects of testing policy on voluntary social distancing and the spread of an infection. Agents decide their social activity level, which determines a social network over which the virus spreads. Testing enables the isolation of infected individuals, slowing down the infection. However, greater testing also reduces voluntary social distancing or increases social activity, exacerbating the spread of the virus. We show that the effect of testing on infections is nonmonotone. This nonmonotonicity also implies that the optimal testing policy may leave some of the testing capacity of society unused. Funding: The authors acknowledge support from C3.DTI funding. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.2220 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.2220},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {533-548},
  shortjournal = {Oper. Res.},
  title        = {Testing, voluntary social distancing, and the spread of an infection},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Data tracking under competition. <em>OR</em>,
<em>72</em>(2), 514–532. (<a
href="https://doi.org/10.1287/opre.2023.2489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the welfare implications of data-tracking technologies that enable firms to collect consumer data and use it for price discrimination. The model we develop centers around two features: competition between firms and consumers’ level of sophistication. Our baseline environment features a firm that can collect information about the consumers it transacts with in a duopoly market, which it can then use in a second, monopoly market. We characterize and compare the equilibrium outcomes in three settings: (i) an economy with myopic consumers, who, when making purchase decisions, do not internalize the fact that firms track their behavior and use this information in future transactions; (ii) an economy with forward-looking consumers, who take into account the implications of data tracking when determining their actions; and (iii) an economy where no data-tracking technologies are used due to technological or regulatory constraints. We find that the absence of data tracking may lead to a decrease in consumer surplus, even when consumers are myopic. Importantly, this result relies critically on competition : Consumer surplus may be higher when data-tracking technologies are used only when multiple firms offer substitutable products. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2489 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2489},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {514-532},
  shortjournal = {Oper. Res.},
  title        = {Data tracking under competition},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Best principal submatrix selection for the maximum entropy
sampling problem: Scalable algorithms and performance guarantees.
<em>OR</em>, <em>72</em>(2), 493–513. (<a
href="https://doi.org/10.1287/opre.2023.2488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a classic maximum entropy sampling problem (MESP), which aims to select the most informative principal submatrix of a prespecified size from a covariance matrix. By investigating its Lagrangian dual and primal characterization, we derive a novel convex integer program for MESP and show that its continuous relaxation yields a near-optimal solution. The results motivate us to develop a sampling algorithm and derive its approximation bound for MESP, which improves the best known bound in literature. We then provide an efficient deterministic implementation of the sampling algorithm with the same approximation bound. Besides, we investigate the widely used local search algorithm and prove its first known approximation bound for MESP. The proof techniques further inspire for us an efficient implementation of the local search algorithm. Our numerical experiments demonstrate that these approximation algorithms can efficiently solve medium-size and large-scale instances to near optimality. Finally, we extend the analyses to the A-optimal MESP, for which the objective is to minimize the trace of the inverse of the selected principal submatrix. Funding: This work was supported by the National Science Foundation Division of Information and Intelligent Systems [Grant 2246417] and Division of Civil, Mechanical and Manufacturing Innovation [Grant 2246414]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2023.2488 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2488},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {493-513},
  shortjournal = {Oper. Res.},
  title        = {Best principal submatrix selection for the maximum entropy sampling problem: Scalable algorithms and performance guarantees},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—dynamic pricing and learning with
discounting. <em>OR</em>, <em>72</em>(2), 481–492. (<a
href="https://doi.org/10.1287/opre.2023.2477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical settings, learning algorithms can take a substantial amount of time to converge, thereby raising the need to understand the role of discounting in learning. We illustrate the impact of discounting on the performance of learning algorithms by examining two classic and representative dynamic-pricing and learning problems studied in Broder and Rusmevichientong (BR) [Broder J, Rusmevichientong P (2012) Dynamic pricing under a general parametric choice model. Oper. Res. 60(4):965–980] and Keskin and Zeevi (KZ) [Keskin NB, Zeevi A (2014) Dynamic pricing with an unknown demand model: Asymptotically optimal semi-myopic policies. Oper. Res. 62(5):1142–1167]. In both settings, a seller sells a product with unlimited inventory over T periods. The seller initially does not know the parameters of the general choice model in BR (respectively, the linear demand curve in KZ). Given a discount factor ρ , the retailer’s objective is to determine a pricing policy to maximize the expected discounted revenue over T periods. In both settings, we establish lower bounds on the regret under any policy and show limiting bounds of Ω ( 1 / ( 1 − ρ ) ) and Ω ( T ) when T → ∞ and ρ → 1 , respectively. In the model of BR with discounting, we propose an asymptotically tight learning policy and show that the regret under our policy as well that under the MLE-CYCLE policy in BR is O ( 1 / ( 1 − ρ ) ) (respectively, O ( T ) ) when T → ∞ (respectively, ρ → 1 ). In the model of KZ with discounting, we present sufficient conditions for a learning policy to guarantee asymptotic optimality and show that the regret under any policy satisfying these conditions is O ( log ( 1 / ( 1 − ρ ) ) 1 / ( 1 − ρ ) ) (respectively, O ( log T T ) ) when T → ∞ (respectively, ρ → 1 ). We show that three different policies—namely, the two variants of the greedy iterated least squares policy in KZ and a different policy that we propose—achieve this upper bound on the regret. We numerically examine the behavior of the regret under our policies as well as those in BR and KZ in the presence of discounting. We also analyze a setting in which the discount factor per period is a function of the number of decision periods in the planning horizon. Funding: Z. Feng received support from the National Natural Science Foundation of China [Grant 72201256]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2477 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2477},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {481-492},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Dynamic pricing and learning with discounting},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust queue inference from waiting times. <em>OR</em>,
<em>72</em>(2), 459–480. (<a
href="https://doi.org/10.1287/opre.2022.0091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Observational data from queueing systems are of great practical interest in many application areas because they can be leveraged for better statistical inference of service processes. However, these observations often only provide partial information of the system for various reasons in real-world settings. Moreover, their complex temporal dependence on the queueing dynamics and the absence of distributional information on the model primitives render estimation of queueing systems remarkably challenging. To this end, we consider the problem of inferring service times from waiting time observations. Specifically, we propose an inference framework based on robust optimization, where service times are described via sets that are calibrated by the observed waiting times. We provide conditions under which these data-driven uncertainty sets become asymptotically confident estimators of the service process; that is, they contain unknown service times almost surely as the number of observations grows. We also introduce tractable optimization formulations to compute bounds of various service time characteristics such as moments and risk measures. In this way, our approach is data driven and free of distributional assumptions on unknown model primitives, which is required by existing methods. We also generalize the proposed inference framework to tandem queues and feed-forward networks, offering broader capability in estimation of real-world queueing systems. Our simulation study demonstrates that the proposed approach easily incorporates information of arrival processes such as moments and correlations and performs consistently well on queueing networks under various settings. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0091 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0091},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {459-480},
  shortjournal = {Oper. Res.},
  title        = {Robust queue inference from waiting times},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic liquidity as a proxy for nonlinear price impact.
<em>OR</em>, <em>72</em>(2), 444–458. (<a
href="https://doi.org/10.1287/opre.2022.0627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal execution and trading algorithms rely on price impact models, such as the propagator model, to quantify trading costs. Empirically, price impact is concave in trade sizes, leading to nonlinear models for which optimization problems are intractable, and even qualitative properties, such as price manipulation, are poorly understood. However, we show that in the diffusion limit of small and frequent orders, the nonlinear model converges to a tractable linear model. In this high-frequency limit, a stochastic liquidity parameter approximates the original impact function’s nonlinearity. We illustrate the approximation’s practical performance using limit order data.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0627},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {444-458},
  shortjournal = {Oper. Res.},
  title        = {Stochastic liquidity as a proxy for nonlinear price impact},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A unifying framework for the capacitated vehicle routing
problem under risk and ambiguity. <em>OR</em>, <em>72</em>(2), 425–443.
(<a href="https://doi.org/10.1287/opre.2021.0669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a generic model for the capacitated vehicle routing problem (CVRP) under demand uncertainty. By combining risk measures, satisficing measures, or disutility functions with complete or partial characterizations of the probability distribution governing the demands, our formulation bridges the popular but often independently studied paradigms of stochastic programming and distributionally robust optimization. We characterize when an uncertainty-affected CVRP is (not) amenable to a solution via a popular branch-and-cut scheme, and we elucidate how this solvability relates to the interplay between the employed decision criterion and the available description of the uncertainty. Our framework offers a unified treatment of several CVRP variants from the recent literature, such as formulations that optimize the requirements violation or the essential riskiness indices, and it, at the same time, allows us to study new problem variants, such as formulations that optimize the worst case expected disutility over Wasserstein or ϕ -divergence ambiguity sets. All of our formulations can be solved by the same branch-and-cut algorithm with only minimal adaptations, which makes them attractive for practical implementations. Funding: C. P. Ho sincerely acknowledges funding from the National Natural Science Foundation of China [Grant 72032005], the City University of Hong Kong (CityU) Start-Up Grant [Grant 9610481], and the CityU Strategic Research Grant [Grants 7005688 and 7005891]. W. Wiesemann gratefully acknowledges funding from the Engineering and Physical Sciences Research Council [Grant EP/W003317/1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0669 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0669},
  journal      = {Operations Research},
  month        = {3},
  number       = {2},
  pages        = {425-443},
  shortjournal = {Oper. Res.},
  title        = {A unifying framework for the capacitated vehicle routing problem under risk and ambiguity},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—data-driven chance constrained programs over
wasserstein balls. <em>OR</em>, <em>72</em>(1), 410–424. (<a
href="https://doi.org/10.1287/opre.2022.2330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide an exact deterministic reformulation for data-driven, chance-constrained programs over Wasserstein balls. For individual chance constraints as well as joint chance constraints with right-hand-side uncertainty, our reformulation amounts to a mixed-integer conic program. In the special case of a Wasserstein ball with the 1-norm or the ∞ -norm, the cone is the nonnegative orthant, and the chance-constrained program can be reformulated as a mixed-integer linear program. Our reformulation compares favorably to several state-of-the-art data-driven optimization schemes in our numerical experiments. Funding: The authors gratefully acknowledge financial support from the Hong Kong Research Grants Council [Early Career Scheme CityU 21502820], the Swiss National Science Foundation [Grant BSCGI0_157733] as well as the Engineering and Physical Sciences Research Council [Grant EP/N020030/1].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2330},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {410-424},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Data-driven chance constrained programs over wasserstein balls},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identifying merger opportunities: The case of air traffic
control. <em>OR</em>, <em>72</em>(1), 389–409. (<a
href="https://doi.org/10.1287/opre.2022.2348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Horizontal mergers and acquisitions offer firms the means to grow. However, forecasting these actions’ potential effects on the market is not a simple task. We propose a model that identifies optimal horizontal merger configurations for an industry. The model endogenizes the merger choice by maximizing the overall potential efficiency gain at the level of an industry or firm with multiple branches. We further extend the model to consider mergers that create contiguous firms, should network effects be a consideration. The optimal solution, estimated as a consequence of a change in industry structure, is decomposed into individual learning inefficiencies in addition to harmony and scale effects. The efficiency gains are estimated using a nonradial, directional distance function to facilitate this decomposition. An application of the model to the European air traffic control market suggests that the market ought to be reduced to 4 contiguous firms, replacing the 29 analyzed and the 9 proposed in the Single European Skies initiative. This is likely to lead to overall savings of around €3.3 billion annually, of which approximately 82% is directly attributable to merger synergies. Furthermore, this represents an additional annual saving of €1.2 billion over that achieved by the second best: the Single European Skies initiative. Funding: N. Adler received partial funding from the Asper Center for Entrepreneurship at the Hebrew University Business School.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2348},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {389-409},
  shortjournal = {Oper. Res.},
  title        = {Identifying merger opportunities: The case of air traffic control},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An approximate dynamic programming approach to repeated
games with vector losses. <em>OR</em>, <em>72</em>(1), 373–388. (<a
href="https://doi.org/10.1287/opre.2022.2334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an approximate dynamic programming (ADP) approach to compute approximations of the optimal strategies and of the minimal losses that can be guaranteed in discounted repeated games with vector-valued losses. Among other applications, such vector-valued games prominently arise in the analysis of worst-case regret in repeated decision making in unknown environments, also known as the adversarial online learning framework. At the core of our approach is a characterization of the lower Pareto frontier of the set of expected losses that a player can guarantee in these games as the unique fixed point of a set-valued dynamic programming operator. When applied to the problem of worst-case regret minimization with discounted losses, our approach yields algorithms that achieve markedly improved performance bounds compared with off-the-shelf online learning algorithms like Hedge. These results thus suggest the significant potential of ADP-based approaches in adversarial online learning. Funding: This work has been partially supported by the Multidisciplinary Institute in Artificial Intelligence (MIAI) at Grenoble Alpes (ANR-19-P3IA-0003), by the French National Research Agency (ANR) [Grant ANR-20-CE23-0007], by the U.S. Airforce Office of Scientific Research (AFOSR) [Grant MURI FA9550-10-1-0573], by the France-Berkeley Fund, and by the Alexander von Humboldt Foundation. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2334 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2334},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {373-388},
  shortjournal = {Oper. Res.},
  title        = {An approximate dynamic programming approach to repeated games with vector losses},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mechanism design under approximate incentive compatibility.
<em>OR</em>, <em>72</em>(1), 355–372. (<a
href="https://doi.org/10.1287/opre.2022.2359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental assumption in classical mechanism design is that buyers are perfect optimizers. However, in practice, buyers may be limited by their computational capabilities or a lack of information and may not be able to perfectly optimize their response to a mechanism. This has motivated the introduction of approximate incentive compatibility (IC) as an appealing solution concept for practical mechanism design. Although most of the literature has focused on the analysis of particular approximate IC mechanisms, this paper is the first to study the design of optimal mechanisms in the space of approximate IC mechanisms and to explore how much revenue can be garnered by moving from exact to approximate incentive constraints. In particular, we study the problem of a seller facing one buyer with private values and analyze optimal selling mechanisms under ε -incentive compatibility. We establish that the gains that can be garnered depend on the local curvature of the seller’s revenue function around the optimal posted price when the buyer is a perfect optimizer. If the revenue function behaves locally like an α -power for α ∈ ( 1 , ∞ ) , then no mechanism can garner gains higher than order ε α / ( 2 α − 1 ) . This improves on state-of-the-art results that imply maximum gains of ε 1 / 2 by providing the first parametric bounds that capture the impact of revenue function’s curvature on revenue gains. Furthermore, we establish that an optimal mechanism needs to randomize as soon as ε &gt; 0 and construct a randomized mechanism that is guaranteed to achieve order ε α / ( 2 α − 1 ) additional revenues, leading to a tight characterization of the revenue implications of approximate IC constraints. Our study sheds light on a novel class of optimization problems and the challenges that emerge when relaxing IC constraints. In particular, it brings forward the need to optimize not only over allocations and payments but also over best responses, and we develop a new framework to address this challenge. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2359 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2359},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {355-372},
  shortjournal = {Oper. Res.},
  title        = {Mechanism design under approximate incentive compatibility},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parallel adaptive survivor selection. <em>OR</em>,
<em>72</em>(1), 336–354. (<a
href="https://doi.org/10.1287/opre.2022.2343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We reconsider the ranking and selection (R&amp;S) problem in stochastic simulation optimization in light of high-performance, parallel computing, where we take “R&amp;S” to mean any procedure that simulates all systems (feasible solutions) to provide some statistical guarantee on the selected systems. We argue that when the number of systems is very large, and the parallel processing capability is also substantial, then neither the standard statistical guarantees such as probability of correct selection nor the usual observation-saving methods such as elimination via paired comparisons or complex budget allocation serve the experimenter well. As an alternative, we propose a guarantee on the expected false elimination rate that avoids the curse of multiplicity and a method to achieve it that is designed to scale computationally with problem size and parallel computing capacity . To facilitate this approach, we present a new mathematical representation, prove small-sample and asymptotic properties, evaluate variations of the method, and demonstrate a specific implementation on a problem with over 1 , 100 , 000 systems using only 21 parallel processors. Although we focus on inference about the best system here, our parallel adaptive survivor selection framework can be generalized to many other useful definitions of “good” systems. Funding: This work was supported by the National Science Foundation [Grants CMMI-1537060 and CMMI-1554144]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2343 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2343},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {336-354},
  shortjournal = {Oper. Res.},
  title        = {Parallel adaptive survivor selection},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic interday and intraday scheduling. <em>OR</em>,
<em>72</em>(1), 317–335. (<a
href="https://doi.org/10.1287/opre.2022.2342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous consideration of appointment day (interday scheduling) and time of day (intraday scheduling) in dynamic scheduling decisions is a theoretical and practical problem that has remained open. We introduce a novel dynamic programming framework that incorporates jointly these scheduling decisions in two timescales. Our model is designed with the intention of bridging the two streams of literature on interday and intraday scheduling and to leverage their latest theoretical developments in tackling the joint problem. We establish theoretical connections between two recent studies by proving novel theoretical results in discrete convex analysis regarding constrained multimodular function minimization. Grounded on our theory, we develop a practically implementable and computationally tractable scheduling paradigm with performance guarantees. Numerical experiments demonstrate that the optimality gap is less than 1% for practical instances of the problem.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2342},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {317-335},
  shortjournal = {Oper. Res.},
  title        = {Dynamic interday and intraday scheduling},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sequential competitive facility location: Exact and
approximate algorithms. <em>OR</em>, <em>72</em>(1), 300–316. (<a
href="https://doi.org/10.1287/opre.2022.2339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a competitive facility location problem (CFLP), where two firms sequentially open new facilities within their budgets, in order to maximize their market shares of demand that follows a probabilistic choice model. This process is a Stackelberg game and admits a bilevel mixed-integer nonlinear program (MINLP) formulation. We derive an equivalent, single-level MINLP reformulation and exploit the problem structures to derive two valid inequalities based on submodularity and concave overestimation, respectively. We use the two valid inequalities in a branch-and-cut algorithm to find globally optimal solutions. Then, we propose an approximation algorithm to find good-quality solutions with a constant approximation guarantee. We develop several extensions by considering general facility-opening costs and outside competitors as well as diverse facility-planning decisions, and we discuss solution approaches for each extension. We conduct numerical studies to demonstrate that the exact algorithm significantly accelerates the computation of CFLP on large-sized instances that have not been solved optimally or even heuristically by existing methods, and the approximation algorithm can quickly find high-quality solutions. We derive managerial insights based on sensitivity analysis of different settings that affect customers’ probabilistic choices and the ensuing demand. Funding: M. Qi was partially supported by the National Natural Science Foundation of China [Grant 71772100] to work on this project. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.2339 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.2339},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {300-316},
  shortjournal = {Oper. Res.},
  title        = {Sequential competitive facility location: Exact and approximate algorithms},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—incomplete information VCG contracts for
common agency. <em>OR</em>, <em>72</em>(1), 288–299. (<a
href="https://doi.org/10.1287/opre.2023.2475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study contract design for welfare maximization in the well-known “common agency” model introduced in 1986 by Bernheim and Whinston. This model combines the challenges of coordinating multiple principals with the fundamental challenge of contract design: that principals have incomplete information of the agent’s choice of action. Our goal is to design contracts that satisfy truthfulness of the principals, welfare maximization by the agent, and two fundamental properties of individual rationality (IR) for the principals and limited liability (LL) for the agent. Our results reveal an inherent impossibility. Whereas for every common agency setting there exists a truthful and welfare-maximizing contract, which we refer to as “incomplete information Vickrey–Clarke–Groves contracts,” there is no such contract that also satisfies IR and LL for all settings. As our main results, we show that the class of settings for which there exists a contract that satisfies truthfulness, welfare maximization, LL, and IR is identifiable by a polynomial-time algorithm. Furthermore, for these settings, we design a polynomial-time computable contract: given valuation reports from the principals, it returns, if possible for the setting, a payment scheme for the agent that constitutes a contract with all desired properties. We also give a sufficient graph-theoretic condition on the population of principals that ensures the existence of such a contract and two truthful and welfare-maximizing contracts, in which one satisfies LL and the other one satisfies IR. Funding: Funding is by the European Union (ERC, ALGOCONTRACT) [Grant 101077862].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2475},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {288-299},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Incomplete information VCG contracts for common agency},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—a note on state-independent policies in
network revenue management. <em>OR</em>, <em>72</em>(1), 277–287. (<a
href="https://doi.org/10.1287/opre.2023.2471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit two classical price-based and choice-based network revenue management problems studied in the literature. The setting for the problems is as follows: A firm sells multiple products over a finite horizon using a limited supply of resources. Product demands are stochastic. The demand rate for each product depends on the current price-vector (respectively, assortment displayed). The firm&#39;s goal is to obtain a pricing (respectively, assortment) policy that maximizes its expected revenue. The main result for the price-based problem is that the optimality gaps incurred by two state-independent policies scale proportionally to k , where k is the scale of demand and supply. The analysis in the literature implicitly assumes that the demand-price relationship is separable among the products. In this paper, we derive these results for the more general setting where the demand-price relationship need not be separable. We also consider an important practical variant of the price-based problem in which the price of each product is restricted to a discrete and finite set and show the k result for this problem. For the choice-based problem, to our knowledge, there is no result in the literature on the asymptotic convergence rate of any policy. We show that this problem is mathematically equivalent to the discrete-price variant of the price-based problem and use this equivalence to show that the choice-based deterministic linear program policy in the literature for the choice-based problem also inherits the k bound on the optimality gap. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2471 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2471},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {277-287},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—A note on state-independent policies in network revenue management},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating large-scale tree logit models. <em>OR</em>,
<em>72</em>(1), 257–276. (<a
href="https://doi.org/10.1287/opre.2023.2479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe an efficient estimation method for large-scale tree logit models, using a novel change-of-variables transformation that allows us to express the negative log-likelihood as a strictly convex function in the leaf node parameters and a difference of strictly convex functions in the nonleaf node parameters. Exploiting this representation, we design a fast iterative method that computes a sequence of parameter estimates using simple closed-form updates. Our algorithm relies only on first-order information (function and gradients values), but unlike other first-order methods, it does not require any step size tuning or costly projection steps. The sequence of parameter estimates yields increasing likelihood values, and we establish sublinear convergence to a stationary point of the maximum likelihood problem. Numerical results on both synthetic and real data show that our algorithm outperforms state-of-the-art optimization methods, especially for large-scale tree logit models with thousands of nodes. Funding: This work was supported by the Division of Civil, Mechanical, and Manufacturing Innovation [Grants 1433396 and 1454310]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2479 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2479},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {257-276},
  shortjournal = {Oper. Res.},
  title        = {Estimating large-scale tree logit models},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Recursive importance sketching for rank constrained least
squares: Algorithms and high-order convergence. <em>OR</em>,
<em>72</em>(1), 237–256. (<a
href="https://doi.org/10.1287/opre.2023.2445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a recursive importance sketching algorithm for rank-constrained least squares optimization (RISRO). The key step of RISRO is recursive importance sketching, a new sketching framework based on deterministically designed recursive projections, and it significantly differs from the randomized sketching in the literature. Several existing algorithms in the literature can be reinterpreted under this new sketching framework, and RISRO offers clear advantages over them. RISRO is easy to implement and computationally efficient, and the core procedure in each iteration is to solve a dimension-reduced least squares problem. We establish the local quadratic-linear and quadratic rate of convergence for RISRO under some mild conditions. We also discover a deep connection of RISRO to the Riemannian Gauss–Newton algorithm on fixed rank matrices. The effectiveness of RISRO is demonstrated in two applications in machine learning and statistics: low-rank matrix trace regression and phase retrieval. Simulation studies demonstrate the superior numerical performance of RISRO. Funding: Y. Luo and A. Zhang were partially supported by the National Science Foundation [Grant CAREER-2203741]. W. Huang was partially supported by the Fundamental Research Funds for the Central Universities [Grant 20720190060] and the National Natural Science Foundation of China [Grant 12001455]. X. Li was partially supported by the National Natural Science Foundation of China [Grants 62141407 and 12271107], the Chenguang Program by the Shanghai Education Development Foundation and Shanghai Municipal Education Commission [Grant 19CG02], and the Shanghai Science and Technology Program [Grant 21JC1400600]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2445 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2445},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {237-256},
  shortjournal = {Oper. Res.},
  title        = {Recursive importance sketching for rank constrained least squares: Algorithms and high-order convergence},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Is q-learning minimax optimal? A tight sample complexity
analysis. <em>OR</em>, <em>72</em>(1), 222–236. (<a
href="https://doi.org/10.1287/opre.2023.2450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state–action pairs are drawn from a generative model in each iteration), substantial progress has been made toward understanding the sample efficiency of Q-learning. Consider a γ -discounted infinite-horizon MDP with state space S and action space A : to yield an entry-wise ε -approximation of the optimal Q-function, state-of-the-art theory for Q-learning requires a sample size exceeding the order of | S ‖ A | ( 1 − γ ) 5 ε 2 , which fails to match existing minimax lower bounds. This gives rise to natural questions: What is the sharp sample complexity of Q-learning? Is Q-learning provably suboptimal? This paper addresses these questions for the synchronous setting: (1) when the action space contains a single action (so that Q-learning reduces to TD learning), we prove that the sample complexity of TD learning is minimax optimal and scales as | S | ( 1 − γ ) 3 ε 2 (up to log factor); (2) when the action space contains at least two actions, we settle the sample complexity of Q-learning to be on the order of | S ‖ A | ( 1 − γ ) 4 ε 2 (up to log factor). Our theory unveils the strict suboptimality of Q-learning when the action space contains at least two actions and rigorizes the negative impact of overestimation in Q-learning. Finally, we extend our analysis to accommodate asynchronous Q-learning (i.e., the case with Markovian samples), sharpening the horizon dependency of its sample complexity to be 1 ( 1 − γ ) 4 . Funding: Y. Chen is supported in part by the Alfred P. Sloan Research Fellowship, the Google Research Scholar Award, the Air Force Office of Scientific Research [Grant FA9550-22-1-0198], the Office of Naval Research (ONR) [Grant N00014-22-1-2354], and the National Science Foundation (NSF) [Grants CCF-2221009, CCF-1907661, DMS-2014279, IIS-2218713, and IIS-2218773]. Y. Wei is supported in part by the Google Research Scholar Award and the NSF [Grants CCF-2106778, DMS-2147546/2015447, and CAREER award DMS-2143215]. Y. Chi is supported in part by the ONR [Grants N00014-18-1-2142 and N00014-19-1-2404] and the NSF [Grants CCF-1806154, CCF-2007911, CCF-2106778, ECCS-2126634, and DMS-2134080]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2450 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2450},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {222-236},
  shortjournal = {Oper. Res.},
  title        = {Is Q-learning minimax optimal? a tight sample complexity analysis},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Breaking the sample size barrier in model-based
reinforcement learning with a generative model. <em>OR</em>,
<em>72</em>(1), 203–221. (<a
href="https://doi.org/10.1287/opre.2023.2451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider γ -discounted infinite-horizon Markov decision processes (MDPs) with state space S and action space A . Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy has yet to be determined. In particular, all prior results suffer from a severe sample size barrier in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least | S ‖ A | ( 1 − γ ) 2 . The current paper overcomes this barrier by certifying the minimax optimality of two algorithms—a perturbed model-based algorithm and a conservative model-based algorithm—as soon as the sample size exceeds the order of | S ‖ A | 1 − γ (modulo some log factor). Moving beyond infinite-horizon MDPs, we further study time-inhomogeneous finite-horizon MDPs and prove that a plain model-based planning algorithm suffices to achieve minimax-optimal sample complexity given any target accuracy level. To the best of our knowledge, this work delivers the first minimax-optimal guarantees that accommodate the entire range of sample sizes (beyond which finding a meaningful policy is information theoretically infeasible). Funding: Y. Wei is supported in part by the Google Research Scholar Award and the National Science Foundation [Grants CCF-2106778, DMS-2147546, and DMS-2143215]. Y. Chi is supported in part by the Office of Naval Research [Grants N00014-18-1-2142 and N00014-19-1-2404] and the National Science Foundation [Grants CCF-1806154, CCF-2007911, and CCF-2106778]. Y. Chen is supported in part by the Alfred P. Sloan Foundation [research fellowship], Google [research scholar award], the Air Force Office of Scientific Research [Grants FA9550-19-1-0030 and FA9550-22-1-0198], the Office of Naval Research [Grant N00014-22-1-2354], and the National Science Foundation [Grants CCF-2221009, CCF-1907661, DMS-2014279, IIS-2218713, and IIS-2218773]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2451 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2451},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {203-221},
  shortjournal = {Oper. Res.},
  title        = {Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A planner-trader decomposition for multimarket hydro
scheduling. <em>OR</em>, <em>72</em>(1), 185–202. (<a
href="https://doi.org/10.1287/opre.2023.2456">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Peak/off-peak spreads on European electricity forward and spot markets are eroding due to the ongoing nuclear phaseout in Germany and the steady growth in photovoltaic capacity. The reduced profitability of peak/off-peak arbitrage forces hydropower producers to recover part of their original profitability on the reserve markets. We propose a bilayer stochastic programming framework for the optimal operation of a fleet of interconnected hydropower plants that sells energy on both the spot and the reserve markets. The outer layer (the planner’s problem ) optimizes end-of-day reservoir filling levels over one year, whereas the inner layer (the trader’s problem ) selects optimal hourly market bids within each day. Using an information restriction whereby the planner prescribes the end-of-day reservoir targets one day in advance, we prove that the trader’s problem simplifies from an infinite-dimensional stochastic program with 25 stages to a finite two-stage stochastic program with only two scenarios. Substituting this reformulation back into the outer layer and approximating the reservoir targets by affine decision rules allows us to simplify the planner’s problem from an infinite-dimensional stochastic program with 365 stages to a two-stage stochastic program that can conveniently be solved via the sample average approximation. Numerical experiments based on a cascade in the Salzburg region of Austria demonstrate the effectiveness of the suggested framework. Funding: This research was supported by the Ministry of Education, Singapore, under its 2019 Academic Research Fund Tier 3 [Grant MOE-2019-T3-1-010], the National University of Singapore [Grant A-0009135-01-00], and the Swiss National Science Foundation [Grant BSCGI0_157733] as well as the Engineering and Physical Sciences Research Council [Grant EP/R045518/1]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2456 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2456},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {185-202},
  shortjournal = {Oper. Res.},
  title        = {A planner-trader decomposition for multimarket hydro scheduling},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Resource-aware cost-sharing methods for scheduling games.
<em>OR</em>, <em>72</em>(1), 167–184. (<a
href="https://doi.org/10.1287/opre.2023.2434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the performance of cost-sharing methods in a selfish scheduling setting where a group of users schedule their jobs on machines with load-dependent cost functions, aiming to minimize their own cost. Anticipating this user behavior, the system designer chooses a decentralized protocol that defines how the cost generated on each machine is to be shared among its users, and the performance of the protocol is evaluated over the Nash equilibria of the induced game. Previous work on selfish scheduling has focused on two extreme models: omniscient protocols that are aware of every machine and every job that is active at any given time, and oblivious protocols that are aware of nothing beyond the machine they control. We focus on a well-motivated middle-ground model of resource-aware protocols, which are aware of the set of machines in the system, but unaware of what jobs are active at any given time. Furthermore, we study the extent to which appropriately overcharging the users can lead to improved performance. We provide protocols that achieve small constant price of anarchy bounds when the cost functions are convex or concave, and we complement our positive results with impossibility results for general cost functions. Funding: This work was supported by the Royal Society [Grant LT140046], the Engineering and Physical Sciences Research Council [Grant EP/M008118/1], the National Science Foundation [Grants CCF-1161813, CCF-1216073, and CCF-1408635; CAREER Award CCF-2047907], and the Lise Meitner Award Fellowship.},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2434},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {167-184},
  shortjournal = {Oper. Res.},
  title        = {Resource-aware cost-sharing methods for scheduling games},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Persuading risk-conscious agents: A geometric approach.
<em>OR</em>, <em>72</em>(1), 151–166. (<a
href="https://doi.org/10.1287/opre.2023.2438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a persuasion problem between a sender and a receiver where utility may be nonlinear in the latter’s belief; we call such receivers risk conscious . Such utility models arise when the receiver exhibits systematic biases away from expected utility maximization, such as uncertainty aversion (e.g., from sensitivity to the variance of the waiting time for a service). Because of this nonlinearity, the standard approach to finding the optimal persuasion mechanism using revelation principle fails. To overcome this difficulty, we use the underlying geometry of the problem to develop a convex optimization framework to find the optimal persuasion mechanism. We define the notion of full persuasion and use our framework to characterize conditions under which full persuasion can be achieved. We use our approach to study binary persuasion , where the receiver has two actions and the sender strictly prefers one of them at every state. Under a convexity assumption, we show that the binary persuasion problem reduces to a linear program and establish a canonical set of signals where each signal either reveals the state or induces in the receiver uncertainty between two states. Finally, we discuss the broader applicability of our methods to more general contexts, and we illustrate our methodology by studying information sharing of waiting times in service systems. Funding: The second and the third authors gratefully acknowledge support from the Division of Civil, Mechanical and Manufacturing Innovation of the National Science Foundation [Grants CMMI-1633920 and CMMI-2002156]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2023.2438 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2023.2438},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {151-166},
  shortjournal = {Oper. Res.},
  title        = {Persuading risk-conscious agents: A geometric approach},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Technical note—cloud cost optimization: Model, bounds, and
asymptotics. <em>OR</em>, <em>72</em>(1), 132–150. (<a
href="https://doi.org/10.1287/opre.2022.0362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the rapid growth of the cloud cost management and optimization (CCMO) industry to support the exploding cloud-computing market, we study an infinite-horizon, stochastic optimization problem from the viewpoint of a firm that employs cloud resources to process incoming orders (or jobs) over time. We model the following vital practical features of CCMO in our problem. There are several types of resources that differ in their costs and performance attributes (e.g., processor speed, memory, storage). For each type of resource, capacity can either be reserved over the long term at a discounted price or be used on demand at a relatively higher price. Orders of several types arrive stochastically through time; orders differ in their completion-time deadlines and in their resource-specific processing-time distributions. Moreover, the progress of an order can be observed periodically, and if required, the order can be moved from one resource type to another. Penalty costs are incurred for orders not completed by their deadlines. The firm’s goal is to minimize the long-run average expected cost per period, taking into account reserved-capacity costs, on-demand capacity costs, and order-delay costs. We derive a lower bound on the optimal cost by considering a set of decoupled problems, one for each order. The solutions of these problems are then used to construct a feasible policy for the original problem and derive an upper bound on that policy’s optimality gap. Importantly, we show that our policy is asymptotically optimal ; when the demand rates of the orders are scaled by a factor θ &gt; 0 , the policy’s optimality gap scales proportional to 1 / θ . We also report results of a comprehensive numerical study—on a test bed informed by capacity and pricing data from Amazon Web Services—to demonstrate the impressive performance of our policy. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2022.0362 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0362},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {132-150},
  shortjournal = {Oper. Res.},
  title        = {Technical Note—Cloud cost optimization: Model, bounds, and asymptotics},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Systemic portfolio diversification. <em>OR</em>,
<em>72</em>(1), 110–131. (<a
href="https://doi.org/10.1287/opre.2022.0290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the portfolio choice problem of banks, taking into account losses due to fire-sale spillovers. We show that the optimal asset allocation can be recovered as the unique Nash equilibrium of a potential game. Our analysis highlights the key tradeoff between individual diversification and systemic risk. In a stylized model economy featuring two banks and two assets, we show that sacrificing individual diversification to reduce portfolio commonality increases the likelihood of a sale event, while simultaneously decreasing the probability of a costly systemic sell-off. Banks have stronger incentives to achieve systemic diversification if there is more heterogeneity in leverage among them, leading to a decrease in the overall vulnerability of the system. We provide numerical evidence that our conclusions are robust with respect to the number of banks and assets in the system. Funding: The research of A. Capponi has been supported by the NSF/CMMI CAREER-1752326 award. The research of M. Weber has been supported by the NUS Start-Up Grant [A-0004587-00-00].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0290},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {110-131},
  shortjournal = {Oper. Res.},
  title        = {Systemic portfolio diversification},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Provably good region partitioning for on-time last-mile
delivery. <em>OR</em>, <em>72</em>(1), 91–109. (<a
href="https://doi.org/10.1287/opre.2021.0588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-time last-mile delivery is expanding rapidly as people expect faster delivery of goods ranging from grocery to medicines. Managing on-time delivery systems is challenging because of the underlying uncertainties and combinatorial nature of the routing decision. In practice, the efficiency of such systems also hinges on the driver’s familiarity with the local neighborhood. This paper studies the optimal region partitioning policy to minimize the expected delivery time of customer orders in a stochastic and dynamic setting. We allow both the order locations and on-site service times to be random and generally distributed. This policy assigns every driver to a subregion, hence making sure drivers will only be dispatched to their own territories. We characterize the structure of the optimal partitioning policy and show its expected on-time performance converges to that of the flexible dispatching policy in heavy traffic. The optimal characterization features two insightful conditions that are critical to the on-time performance of last-mile delivery systems. We then develop partitioning algorithms with performance guarantees, leveraging ham sandwich cuts and three-partitions from discrete geometry. This algorithmic development can be of independent interest for other logistics problems. We demonstrate the efficiency of the proposed region partitioning policy via numerical experiments using synthetic and real-world data sets. Funding: The first author gratefully acknowledges the support of the Office of Naval Research [Grant N00014-21-1-2208] and METRANS [Grant PSR-21-22]. The second author gratefully acknowledges the support of the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and the National Natural Science Foundation of China [Grant 72242106]. The third author gratefully acknowledges the support of the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2023-04453]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/opre.2021.0588 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0588},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {91-109},
  shortjournal = {Oper. Res.},
  title        = {Provably good region partitioning for on-time last-mile delivery},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A sequential model for high-volume recruitment under random
yields. <em>OR</em>, <em>72</em>(1), 60–90. (<a
href="https://doi.org/10.1287/opre.2021.0562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model a multiphase and high-volume recruitment process as a large-scale dynamic program. The success of the process is measured by a reward, which is the total assessment score of accepted candidates minus the penalty cost of the number of accepted candidates in the end deviating from a preset hiring target. For a recruiter, two questions are important: How many offers should be made in each phase? And how does the number of phases affect the reward? We consider an upper bound, which is obtained when the information about all candidates is available at the beginning, and a lower bound, which is obtained when the recruiter sets the number of offers to make in each phase before assessing candidates. We show that when the volume (i.e., arrival rate of candidates and the target) is large, the upper bound, the lower bound, and the optimal policy all converge to the same limit. Motivated by the convergence results, we design four easily computable heuristics that are all asymptotically optimal when the volume is large. With simple yet effective heuristics in hand, we can compute the number of offers to make in each phase and examine the impact of the number of phases in the process on the reward. We apply our modeling framework and heuristics to the recruitment process of graduate students in a business program. Our study is the first to model a high-volume recruitment process as a dynamic program and test it in a case study. Funding: L. Du and Q. Li were supported by the University Grants Committee Research Grants Council [General Research Fund Grant 16502820]. P. Yu was supported by the National Natural Science Foundation of China [Grants 72371038 and 72033003].},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0562},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {60-90},
  shortjournal = {Oper. Res.},
  title        = {A sequential model for high-volume recruitment under random yields},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic development contests. <em>OR</em>, <em>72</em>(1),
43–59. (<a href="https://doi.org/10.1287/opre.2021.0420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public, private, and not-for-profit organizations find advanced technology and product development projects challenging to manage due to the time and budget pressures, and turn to their development partners and suppliers to address their development needs. We study how dynamic development contests with enriched rank-based incentives and carefully tailored information design can help these organizations leverage their suppliers for their development projects while seeking to minimize project lead time by stimulating competition among them. We find that an organization using dynamically adjusted flexible rewards can achieve the minimum expected project lead time at a significantly lower cost than a fixed-reward policy. Importantly, the derived flexible-reward policy pays the minimum expected reward (i.e., achieves the first best). We further examine the case where the organization may not have sufficient budget to offer a reward that attains the minimum expected lead time. In this case, the organization uses the whole reward budget and supplements it with strategic information disclosure. Specifically, we derive an optimal information disclosure policy whereby any change in the state of competition is disclosed immediately with some probability that is weakly increasing over time. Our results indicate that dynamic rewards and strategic information disclosure are powerful tools to help organizations fulfill their development needs swiftly and cost effectively. Funding: V. V. Krishnan is grateful for funding support from the Jacobs Family Foundation. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2021.0420 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2021.0420},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {43-59},
  shortjournal = {Oper. Res.},
  title        = {Dynamic development contests},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Demand estimation under uncertain consideration sets.
<em>OR</em>, <em>72</em>(1), 19–42. (<a
href="https://doi.org/10.1287/opre.2022.0006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate customer demand, choice models rely both on what the individuals do and do not purchase. A customer may not purchase a product because it was not offered but also because it was not considered. To account for this behavior, existing literature has proposed the so-called consider-then-choose (CTC) models, which posit that customers sample a consideration set and then choose the most preferred product from the intersection of the offer set and the consideration set. CTC models have been studied quite extensively in the marketing literature. More recently, they have gained popularity within the operations management (OM) literature to make assortment and pricing decisions. Despite their richness, CTC models are difficult to estimate in practice because firms typically do not observe customers’ consideration sets. Therefore, the common assumption in OM has been that customers consider everything on offer, so the consideration set is the same as the offer set. This raises the following question: When firms only collect transaction data, do CTC models provide any predictive advantage over classic choice models? More precisely, under what conditions do CTC models outperform (if ever) classic choice models in terms of prediction accuracy? In this work, we study a general class of CTC models. We propose techniques to estimate these models efficiently from sales transaction data. We then compare their performance against the classic approach. We find that CTC models outperform standard choice models when there is noise in the offer set information and the noise is asymmetric across the training and test offer sets but otherwise lead to no particular predictive advantage over the classic approach. We also demonstrate the benefits of using CTC models in real-world retail settings. In particular, we show that CTC models calibrated on retail transaction data are better at long-term and warehouse level sales forecasts. We also evaluate their performance in the context of an online platform setting: a peer-to-peer car sharing company. In this context, offer sets are even difficult to define. We observe a remarkable performance of CTC models over standard choice models therein. Funding: This work was supported in part by the National Science Foundation Division of Civil, Mechanical, and Manufacturing Innovation [Grant 1454310]. Supplemental Material: The online appendices are available at https://doi.org/10.1287/opre.2022.0006 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2022.0006},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {19-42},
  shortjournal = {Oper. Res.},
  title        = {Demand estimation under uncertain consideration sets},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust risk quantification via shock propagation in
financial networks. <em>OR</em>, <em>72</em>(1), 1–18. (<a
href="https://doi.org/10.1287/opre.2020.0722">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given limited network information, we consider robust risk quantification under the Eisenberg–Noe model for financial networks. To be more specific, motivated by the fact that the structure of the interbank network is not completely known in practice, we propose a robust optimization approach to obtain worst-case default probabilities and associated capital requirements for a specific group of banks (e.g., systemically important financial institutions) under network information uncertainty. Using this tool, we analyze the effects of various incomplete network information structures on these worst-case quantities and provide regulatory insights into the collection of actionable network information. All claims are numerically illustrated using data from the European banking system. Funding: The work of D. Ahn was supported by the Hong Kong Research Grants Council, University Grants Committee [Early Career Scheme Grant 24210420]. N. Chen acknowledges funding support from the Hong Kong Research Grants Council, University Grants Committee [General Research Fund Grant 14207918 and General Research Fund Grant 14208620]. The work by K.-K. Kim was supported by the National Research Foundation of Korea [Grant NRF-2019R1A2C1003144]. Supplemental Material: The e-companion is available at https://doi.org/10.1287/opre.2020.0722 .},
  archive      = {J_OR},
  doi          = {10.1287/opre.2020.0722},
  journal      = {Operations Research},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {Oper. Res.},
  title        = {Robust risk quantification via shock propagation in financial networks},
  volume       = {72},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
