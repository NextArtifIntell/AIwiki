<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJDS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijds---12">IJDS - 12</h2>
<ul>
<li><details>
<summary>
(2024). Multivariate functional clustering with variable selection
and application to sensor data from engineering systems. <em>IJDS</em>,
<em>3</em>(2), 203–218. (<a
href="https://doi.org/10.1287/ijds.2022.0034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multisensor data that track system operating behaviors are widely available nowadays from various engineering systems. Measurements from each sensor over time form a curve and can be viewed as functional data. Clustering of these multivariate functional curves is important for studying the operating patterns of systems. One complication in such applications is the possible presence of sensors whose data do not contain relevant information. Hence, it is desirable for the clustering method to equip with an automatic sensor selection procedure. Motivated by a real engineering application, we propose a functional data clustering method that simultaneously removes noninformative sensors and groups functional curves into clusters using informative sensors. Functional principal component analysis is used to transform multivariate functional data into a coefficient matrix for data reduction. We then model the transformed data by a Gaussian mixture distribution to perform model-based clustering with variable selection. Three types of penalties, the individual, variable, and group penalties, are considered to achieve automatic variable selection. Extensive simulations are conducted to assess the clustering and variable selection performance of the proposed methods. The application of the proposed methods to an engineering system with multiple sensors shows the promise of the methods and reveals interesting patterns in the sensor data. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: The research by J. Min and Y. Hong was partially supported by the National Science Foundation [Grant CMMI-1904165] to Virginia Tech. The work by Y. Hong was partially supported by the Virginia Tech College of Science Research Equipment Fund. Data Ethics &amp; Reproducibility Note: The original data set is proprietary and cannot be shared. The full code to replicate the results in this paper, based on summary statistics of the original data, is available at https://github.com/jiem3/MultiFuncClustering . The code applied to a simplified version is available at https://codeocean.com/capsule/4041000/tree/v1 , which covers the data analysis and part of the simulation scenarios with a single data set under each scenario using a fixed set of hyperparameters, for reducing computation time, and at https://doi.org/10.1287/ijds.2022.0034 .},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0034},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {203-218},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Multivariate functional clustering with variable selection and application to sensor data from engineering systems},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Conjecturing-based discovery of patterns in data.
<em>IJDS</em>, <em>3</em>(2), 179–202. (<a
href="https://doi.org/10.1287/ijds.2021.0043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the use of a conjecturing machine that suggests feature relationships in the form of bounds involving nonlinear terms for numerical features and Boolean expressions for categorical features. The proposed C onjecturing framework recovers known nonlinear and Boolean relationships among features from data. In both settings, true underlying relationships are revealed. We then compare the method to a previously proposed framework for symbolic regression on the ability to recover equations that are satisfied among features in a data set. The framework is then applied to patient-level data regarding COVID-19 outcomes to suggest possible risk factors that are confirmed in the medical literature. Discovering patterns in data is a first step toward establishing causal relationships, which can be the basis for effective decision making. Data Ethics &amp; Reproducibility Note: Code and data to reproduce results are available at https://github.com/jpbrooks/conjecturing . COVID-19 synthetic patient data were obtained as part of the Veterans Health Administration (VHA) Innovation Ecosystem and precisionFDA COVID-19 Risk Factor Modeling Challenge and are used here with permission from the Food and Drug Administration (FDA). The code capsule is available on Code Ocean at https://codeocean.com/capsule/1538321/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2021.0043 ). History: Olivia Sheng served as the senior editor for this article.},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2021.0043},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {179-202},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Conjecturing-based discovery of patterns in data},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A statistical model for multisource remote-sensing data
streams of wildfire aerosol optical depth. <em>IJDS</em>, <em>3</em>(2),
162–178. (<a href="https://doi.org/10.1287/ijds.2021.0058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly frequent wildfires have significant impact on solar energy production as the atmospheric aerosols generated by wildfires diminish the incoming solar radiation. Atmospheric aerosols can be measured by aerosol optical depth (AOD), and multiple spatiotemporal AOD data streams are available from geostationary satellites. Although these multisource remote-sensing data streams are measurements of the same underlying AOD process, they often present heterogeneous characteristics, such as different measurement biases and errors, data missing rates, etc. It is usually not known which data source provides more accurate measurements, and simply averaging multiple data streams is not always the best practice. Hence, this paper proposes a statistical model that is capable of inferring the underlying true AOD process by simultaneously integrating heterogeneous multisource remote-sensing data streams and the advection-diffusion equation that governs the dynamics of the AOD process. A bias correction process is included in the model to account for the bias of the physics model and the truncation error of the Fourier series. The proposed approach is applied to California wildfires AOD data obtained from the GOES East (GOES-16) and GOES West (GOES-17) satellites operated by the National Oceanic and Atmospheric Administration. Comprehensive numerical examples are provided to demonstrate the predictive capabilities and model the interpretability of the proposed approach. History: Kwok-Leung Tsui served as the senior editor for this article. Funding: This work was funded by the U.S. National Science Foundation [Grant 2143695]. Data Ethics &amp; Reproducibility Note: Our code requires packages that are not available on CodeOcean (for example, the Rfast package in R). Hence, code and sample data are provided on GitHub: https://github.com/gz-wei/Physics-Informed-Statistical-Modeling-for-Wildfire-Aerosols-Propagation . The online appendix is available at https://doi.org/10.1287/ijds.2021.0058 .},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2021.0058},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {162-178},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A statistical model for multisource remote-sensing data streams of wildfire aerosol optical depth},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Thompson sampling-based partially observable online change
detection for exponential families. <em>IJDS</em>, <em>3</em>(2),
145–161. (<a href="https://doi.org/10.1287/ijds.2022.00011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a holistic sequential change detection framework for partially observable high-dimensional data streams with exponential-family distributions. The framework first proposes a general composite decomposition for exponential-family distributed data by projecting its natural parameter onto normal bases and abnormal bases, which enables efficient inference for sparse changes. Then, the inference results are used for detection scheme construction, and different types of test statistics can be compacted in our framework. Last, by further designing the test statistic as the reward function in the combinatorial multi-armed bandit problem, a Thompson sampling-based sensor allocation strategy is constructed to select the most anomalous variables. Theoretical properties of the detection framework are discussed. Finally, examples of Gaussian, Poisson, and binomial distributed data streams are given in numerical studies and case studies to evaluate the performance of our proposed method. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: C. Zhang is partially supported by the NSFC [Grants 71932006 and 72271138], the BNSF [Grant 9222014], and the ASFC [Grant 2020Z063058001]. H. Yan is partially supported by NIH [R21 AI157618] and NSF [CMMI 2316654]. Data Ethics &amp; Reproducibility Note: No data ethics considerations are foreseen related to this paper. The code capsule is available on Code Ocean at https://codeocean.com/capsule/8794940/tree/v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.00011 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.00011},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {145-161},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Thompson sampling-based partially observable online change detection for exponential families},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An optimization-based order-and-cut approach for fair
clustering of data sets. <em>IJDS</em>, <em>3</em>(2), 124–144. (<a
href="https://doi.org/10.1287/ijds.2022.0005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms have been increasingly integrated into applications that significantly affect human lives. This surged an interest in designing algorithms that train machine learning models to minimize training error and imposing a certain level of fairness. In this paper, we consider the problem of fair clustering of data sets. In particular, given a set of items each associated with a vector of nonsensitive attribute values and a categorical sensitive attribute (e.g., gender, race, etc.), our goal is to find a clustering of the items that minimizes the loss (i.e., clustering objective) function and imposes fairness measured by Rényi correlation. We propose an efficient and scalable in-processing algorithm, driven by findings from the field of combinatorial optimization, that heuristically solves the underlying optimization problem and allows for regulating the trade-off between clustering quality and fairness. The approach does not restrict the analysis to a specific loss function, but instead considers a more general form that satisfies certain desirable properties. This broadens the scope of the algorithm’s applicability. We demonstrate the effectiveness of the algorithm for the specific case of k -means clustering as it is one of the most extensively studied and widely adopted clustering schemes. Our numerical experiments reveal the proposed algorithm significantly outperforms existing methods by providing a more effective mechanism to regulate the trade-off between loss and fairness. History: Rema Padman served as the senior editor for this article. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.9556728.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.0005 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0005},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {124-144},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {An optimization-based order-and-cut approach for fair clustering of data sets},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Registration-free localization of defects in
three-dimensional parts from mesh metrology data using functional maps.
<em>IJDS</em>, <em>3</em>(2), 105–123. (<a
href="https://doi.org/10.1287/ijds.2023.0030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a common problem occurring after using a statistical process control (SPC) method based on three-dimensional measurements: locate where on the surface of the part that triggered an out-of-control alarm there is a significant shape difference with respect to either an in-control part or its nominal (computer-aided design (CAD)) design. In the past, only registration-based solutions existed for this problem, which first orient and locate the part and its nominal design under the same frame of reference. Recently, spectral Laplacian methods have been proposed for the SPC of discrete parts and their measured surface meshes. These techniques provide an intrinsic solution to the SPC problem: that is, a solution exclusively based on data whose coordinates lie on the surfaces without making reference to their ambient space, thus avoiding registration. Registration-free methods avoid the computationally expensive, nonconvex registration step needed to align the parts as required by previous methods, eliminating registration errors, and they are important in industry because of the increasing use of portable noncontact scanners. In this paper, we first present a new registration-free solution to the post-SPC part defect localization problem. The approach uses a spectral decomposition of the Laplace–Beltrami operator in order to construct a functional map between the CAD and measured manifolds to locate defects on the suspected part. A computational complexity analysis demonstrates the approach scales better with the mesh size and is more stable than a registration-based approach. To reduce computational expense, a new mesh partitioning algorithm is presented to find a region of interest on the surface of the part where defects are more likely to exist. The functional map method involves a large number of point-to-point comparisons based on noisy measurements, and a new statistical thresholding method used to filter the false positives in the underlying massive multiple comparisons problem is also provided. History: Bianca Maria Colosimo served as the senior editor for this article. Funding: This research was partially funded by the National Science Foundation [Grant CMMI 2121625]. Data Ethics &amp; Reproducibility Note: There are no data ethics considerations. The code capsule is available on Code Ocean at https://codeocean.com/capsule/4615101/tree/v1 and in the e-Companion to this article (available https://doi.org/10.1287/ijds.2023.0030 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0030},
  journal      = {INFORMS Journal on Data Science},
  month        = {10-12},
  number       = {2},
  pages        = {105-123},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Registration-free localization of defects in three-dimensional parts from mesh metrology data using functional maps},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A supervised tensor dimension reduction-based prognostic
model for applications with incomplete imaging data. <em>IJDS</em>,
<em>3</em>(1), 84–104. (<a
href="https://doi.org/10.1287/ijds.2022.x022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging data-based prognostic models focus on using an asset’s degradation images to predict its time to failure (TTF). Most image-based prognostic models have two common limitations. First, they require degradation images to be complete (i.e., images are observed continuously and regularly over time). Second, they usually employ an unsupervised dimension reduction method to extract low-dimensional features and then use the features for TTF prediction. Because unsupervised dimension reduction is conducted on the degradation images without the involvement of TTFs, there is no guarantee that the extracted features are effective for failure time prediction. To address these challenges, this article develops a supervised tensor dimension reduction-based prognostic model. The model first proposes a supervised dimension reduction method for tensor data. It uses historical TTFs to guide the detection of a tensor subspace to extract low-dimensional features from high-dimensional incomplete degradation imaging data. Next, the extracted features are used to construct a prognostic model based on (log)-location-scale regression. An optimization algorithm for parameter estimation is proposed, and analytical solutions are discussed. Simulated data and a real-world data set are used to validate the performance of the proposed model. History: Bianca Maria Colosimo served as the senior editor for this article Funding: This work was supported by National Science Foundation [2229245]. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://github.com/czhou9/Code-and-Data-for-IJDS and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2022.x022 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.x022},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {84-104},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {A supervised tensor dimension reduction-based prognostic model for applications with incomplete imaging data},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive exploration and optimization of materials crystal
structures. <em>IJDS</em>, <em>3</em>(1), 68–83. (<a
href="https://doi.org/10.1287/ijds.2023.0028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A central problem of materials science is to determine whether a hypothetical material is stable without being synthesized, which is mathematically equivalent to a global optimization problem on a highly nonlinear and multimodal potential energy surface (PES). This optimization problem poses multiple outstanding challenges, including the exceedingly high dimensionality of the PES, and that PES must be constructed from a reliable, sophisticated, parameters-free, and thus very expensive computational method, for which density functional theory (DFT) is an example. DFT is a quantum mechanics-based method that can predict, among other things, the total potential energy of a given configuration of atoms. DFT, although accurate, is computationally expensive. In this work, we propose a novel expansion-exploration-exploitation framework to find the global minimum of the PES. Starting from a few atomic configurations, this “known” space is expanded to construct a big candidate set. The expansion begins in a nonadaptive manner, where new configurations are added without their potential energy being considered. A novel feature of this step is that it tends to generate a space-filling design without the knowledge of the boundaries of the domain space. If needed, the nonadaptive expansion of the space of configurations is followed by adaptive expansion, where “promising regions” of the domain space (those with low-energy configurations) are further expanded. Once a candidate set of configurations is obtained, it is simultaneously explored and exploited using Bayesian optimization to find the global minimum. The methodology is demonstrated using a problem of finding the most stable crystal structure of aluminum. History: Kwok Tsui served as the senior editor for this article. Funding: The authors acknowledge a U.S. National Science Foundation Grant DMREF-1921873 and XSEDE through Grant DMR170031. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.3366149.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0028 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0028},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {68-83},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Adaptive exploration and optimization of materials crystal structures},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cost patterns of multiple chronic conditions: A novel
modeling approach using a condition hierarchy. <em>IJDS</em>,
<em>3</em>(1), 49–67. (<a
href="https://doi.org/10.1287/ijds.2022.0010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare cost predictions are widely used throughout the healthcare system. However, predicting these costs is complex because of both uncertainty and the complex interactions of multiple chronic diseases: chronic disease treatment decisions related to one condition are impacted by the presence of the other conditions. We propose a novel modeling approach inspired by backward elimination, designed to minimize information loss. Our approach is based on a cost hierarchy: the cost of each condition is modeled as a function of the number of other, more expensive chronic conditions the individual member has. Using this approach, we estimate the additive cost of chronic diseases and study their cost patterns. Using large-scale claims data collected from 2007 to 2012, we identify members that suffer from one or more chronic conditions and estimate their total 2012 healthcare expenditures. We apply regression analysis and clustering to characterize the cost patterns of 69 chronic conditions. We observe that the estimated cost of some conditions (for example, organic brain problem) decreases as the member’s number of more expensive chronic conditions increases. Other conditions, such as obesity and paralysis, demonstrate the opposite pattern; their contribution to the overall cost increases as the member’s number of other more serious chronic conditions increases. The modeling framework allows us to account for the complex interactions of multimorbidity and healthcare costs and, therefore, offers a deeper and more nuanced understanding of the cost burden of chronic conditions, which can be utilized by practitioners and policy makers to plan, design better intervention, and identify subpopulations that require additional resources. More broadly, our hierarchical model approach captures complex interactions and can be applied to improve decision making when the enumeration of all possible factor combinations is not possible, for example, in financial risk scoring and pay structure design. History: Rema Padman served as senior editor for this article. Data Ethics &amp; Reproducibility Note: This study is based on proprietary deidentified insurance claims data, so it is not possible to share the original data. To assist in reproducibility, the complete output of the model and statistics related to the cost and prevalence of the conditions studied as well as the diagnosis codes used are included in the online supplement. The modeling approach in this study utilizes healthcare costs as a proxy for severity, which can cause racial disparities. We discuss this in more detail in the Discussion section. The research plan for this study was approved by the institutional review board at the University of Maryland College Park on April 28, 2020. The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.6703019.v1 and https://doi.org/10.24433/CO.1745085.v1 and in the e-companion to this article (available at https://doi.org/10.1287/ijds.2022.0010 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2022.0010},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {49-67},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Cost patterns of multiple chronic conditions: A novel modeling approach using a condition hierarchy},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse density trees and lists: An interpretable alternative
to high-dimensional histograms. <em>IJDS</em>, <em>3</em>(1), 28–48. (<a
href="https://doi.org/10.1287/ijds.2021.0001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present sparse tree-based and list-based density estimation methods for binary/categorical data. Our density estimation models are higher-dimensional analogies to variable bin-width histograms. In each leaf of the tree (or list), the density is constant, similar to the flat density within the bin of a histogram. Histograms, however, cannot easily be visualized in more than two dimensions, whereas our models can. The accuracy of histograms fades as dimensions increase, whereas our models have priors that help with generalization. Our models are sparse, unlike high-dimensional fixed-bin histograms. We present three generative modeling methods, where the first one allows the user to specify the preferred number of leaves in the tree within a Bayesian prior. The second method allows the user to specify the preferred number of branches within the prior. The third method returns density lists (rather than trees) and allows the user to specify the preferred number of rules and the length of rules within the prior. The new approaches often yield a better balance between sparsity and accuracy of density estimates than other methods for this task. We present an application to crime analysis, where we estimate how unusual each type of modus operandi is for a house break-in. History: David Martens served as senior editor for this article. Funding: The authors acknowledge support from NIDA [Grant R01 DA054994]. Data Ethics &amp; Reproducibility Note: There are no ethical issues with this algorithm that we are aware of. Data sets for testing the algorithm are either simulated or publicly available through the UCI Machine Learning Repository (Markelle Kelly, Rachel Longjohn, Kolby Nottingham, The UCI Machine Learning Repository, https://archive.ics.uci.edu ). The housebreak data were obtained through the Cambridge Police Department, Cambridge, MA. The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.2985251.v1 and in the e-companion to this article (available at https://doi.org/10.1287/ijds.2021.0001 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2021.0001},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {28-48},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Sparse density trees and lists: An interpretable alternative to high-dimensional histograms},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The interplay between individual mobility, health risk, and
economic choice: A holistic model for COVID-19 policy intervention.
<em>IJDS</em>, <em>3</em>(1), 6–27. (<a
href="https://doi.org/10.1287/ijds.2023.0013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper was motivated by the need to simultaneously address two competing policy objectives during the course of the COVID pandemic: namely, the public health objective, which required people to be less mobile, and the economic objective, which aimed to ensure that the economy was not adversely affected by the constraints imposed by the first objective. To realize these objectives, we developed a data-informed approach to model human mobility, health risk, and economic activity jointly. This approach computes equilibrium between epidemic models of public health and economic activity under policy interventions that could be used to change people’s mobility behavior. Our approach is distinctive in its capacity to assemble proprietary data sets from public and private sectors at the individual and the zip code levels, which heretofore had not been used together. These data enabled customization of the population-level epidemic models widely used in public health (e.g., the SIR model) with individual-level data traces of mobility behaviors for assessment of public health risks. The outputs of the proposed model enabled parameterization of economic choice models of individuals’ economic decision-making. Various policy interventions and their capacities to shift the equilibrium between economic activity and public health were investigated in this study. Whereas the data-informed joint modeling approach was developed and tested in the pandemic context, it is generalizable for the evaluation of any counterfactual policy interventions. History: Olivia R. Liu Sheng and W. Nick Street served as senior editors for this article. Data Ethics &amp; Reproducibility Note: The code capsule is available on Code Ocean at https://doi.org/10.24433/CO.4390192.v1 and in the e-Companion to this article (available at https://doi.org/10.1287/ijds.2023.0013 ).},
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2023.0013},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {6-27},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {The interplay between individual mobility, health risk, and economic choice: A holistic model for COVID-19 policy intervention},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Chronicles of a new journal: Reflections by the inaugural
editor-in-chief. <em>IJDS</em>, <em>3</em>(1), 1–5. (<a
href="https://doi.org/10.1287/ijds.2024.editorial.v3.n1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJDS},
  doi          = {10.1287/ijds.2024.editorial.v3.n1},
  journal      = {INFORMS Journal on Data Science},
  month        = {4},
  number       = {1},
  pages        = {1-5},
  shortjournal = {INFORMS J. Data Sci.},
  title        = {Chronicles of a new journal: Reflections by the inaugural editor-in-chief},
  volume       = {3},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
