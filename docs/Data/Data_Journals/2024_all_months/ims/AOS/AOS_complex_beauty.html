<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aos---115">AOS - 115</h2>
<ul>
<li><details>
<summary>
(2024). Increasing dimension asymptotics for two-way crossed mixed
effect models. <em>AOS</em>, <em>52</em>(6), 2956–2978. (<a
href="https://doi.org/10.1214/24-AOS2469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents asymptotic results for the maximum likelihood and restricted maximum likelihood (REML) estimators within a two-way crossed mixed effect model, when the number of rows, columns, and the number of observations per cell tend to infinity. The relative growth rate for the number of rows, columns, and cells is unrestricted, whether considered pairwise or collectively. Under very mild conditions (which include moment conditions instead of requiring normality for either the random effects or errors), the estimators are proven to be asymptotically normal, with a structured covariance matrix. We also discuss the case where the number of observations per cell is fixed at 1.},
  archive      = {J_AOS},
  author       = {Ziyang Lyu and S.A. Sisson and A.H. Welsh},
  doi          = {10.1214/24-AOS2469},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2956-2978},
  shortjournal = {Ann. Statist.},
  title        = {Increasing dimension asymptotics for two-way crossed mixed effect models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for decentralized federated learning.
<em>AOS</em>, <em>52</em>(6), 2931–2955. (<a
href="https://doi.org/10.1214/24-AOS2452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers decentralized Federated Learning (FL) under heterogeneous distributions among distributed clients or data blocks for the M-estimation. The mean squared error and consensus error across the estimators from different clients via the decentralized stochastic gradient descent algorithm are derived. The asymptotic normality of the Polyak–Ruppert (PR) averaged estimator in the decentralized distributed setting is attained, which shows that its statistical efficiency comes at a cost as it is more restrictive on the number of clients than that in the distributed M-estimation. To overcome the restriction, a one-step estimator is proposed which permits a much larger number of clients while still achieving the same efficiency as the original PR-averaged estimator in the nondistributed setting. The confidence regions based on both the PR-averaged estimator and the proposed one-step estimator are constructed to facilitate statistical inference for decentralized FL.},
  archive      = {J_AOS},
  author       = {Jia Gu and Song Xi Chen},
  doi          = {10.1214/24-AOS2452},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2931-2955},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference for decentralized federated learning},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change-point analysis with irregular signals. <em>AOS</em>,
<em>52</em>(6), 2913–2930. (<a
href="https://doi.org/10.1214/24-AOS2451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of testing and estimation of change point where signals after the change point can be highly irregular, which departs from the existing literature that assumes signals after the change point to be piecewise constant or vary smoothly. A two-step approach is proposed to effectively estimate the location of the change point. The first step consists of a preliminary estimation of the change point that allows us to obtain unknown parameters for the second step. In the second step, we use a new procedure to determine the position of the change point. We show that, under suitable conditions, the desirable OP(1) rate of convergence of the estimated change point can be obtained. We apply our method to analyze the Baidu search index of COVID-19 related symptoms and find December 8, 2019, to be the starting date of the COVID-19 pandemic.},
  archive      = {J_AOS},
  author       = {Tobias Kley and Yuhan Philip Liu and Hongyuan Cao and Wei Biao Wu},
  doi          = {10.1214/24-AOS2451},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2913-2930},
  shortjournal = {Ann. Statist.},
  title        = {Change-point analysis with irregular signals},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dimension free ridge regression. <em>AOS</em>,
<em>52</em>(6), 2879–2912. (<a
href="https://doi.org/10.1214/24-AOS2449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random matrix theory has become a widely useful tool in high-dimensional statistics and theoretical machine learning. However, random matrix theory is largely focused on the proportional asymptotics in which the number of columns grows proportionally to the number of rows of the data matrix. This is not always the most natural setting in statistics where columns correspond to covariates and rows to samples. With the objective to move beyond the proportional asymptotics, we revisit ridge regression (ℓ2-penalized least squares) on i.i.d. data (xi,yi), i≤n, where xi is a feature vector and yi=⟨β,xi⟩+εi∈R is a response. We allow the feature vector to be high-dimensional, or even infinite-dimensional, in which case it belongs to a separable Hilbert space, and assume either zi:=Σ−1/2xi to have i.i.d. entries, or to satisfy a certain convex concentration property. Within this setting, we establish nonasymptotic bounds that approximate the bias and variance of ridge regression in terms of the bias and variance of an “equivalent” sequence model (a regression model with diagonal design matrix). The approximation is up to multiplicative factors bounded by (1±Δ) for some explicitly small Δ. Previously, such an approximation result was known only in the proportional regime and only up to additive errors: in particular, it did not allow to characterize the behavior of the excess risk when this converges to 0. Our general theory recovers earlier results in the proportional regime (with better error rates). As a new application, we obtain a completely explicit and sharp characterization of ridge regression for Hilbert covariates with regularly varying spectrum. Finally, we analyze the overparametrized near-interpolation setting and obtain sharp “benign overfitting” guarantees.},
  archive      = {J_AOS},
  author       = {Chen Cheng and Andrea Montanari},
  doi          = {10.1214/24-AOS2449},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2879-2912},
  shortjournal = {Ann. Statist.},
  title        = {Dimension free ridge regression},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The projected covariance measure for assumption-lean
variable significance testing. <em>AOS</em>, <em>52</em>(6), 2851–2878.
(<a href="https://doi.org/10.1214/24-AOS2447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the significance of a variable or group of variables X for predicting a response Y, given additional covariates Z, is a ubiquitous task in statistics. A simple but common approach is to specify a linear model, and then test whether the regression coefficient for X is nonzero. However, when the model is misspecified, the test may have poor power, for example, when X is involved in complex interactions, or lead to many false rejections. In this work, we study the problem of testing the model-free null of conditional mean independence, that is, that the conditional mean of Y given X and Z does not depend on X. We propose a simple and general framework that can leverage flexible nonparametric or machine learning methods, such as additive models or random forests, to yield both robust error control and high power. The procedure involves using these methods to perform regressions, first to estimate a form of projection of Y on X and Z using one-half of the data, and then to estimate the expected conditional covariance between this projection and Y on the remaining half of the data. While the approach is general, we show that a version of our procedure using spline regression achieves what we show is the minimax optimal rate in this nonparametric testing problem. Numerical experiments demonstrate the effectiveness of our approach both in terms of maintaining Type I error control, and power, compared to several existing approaches.},
  archive      = {J_AOS},
  author       = {Anton Rask Lundborg and Ilmun Kim and Rajen D. Shah and Richard J. Samworth},
  doi          = {10.1214/24-AOS2447},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2851-2878},
  shortjournal = {Ann. Statist.},
  title        = {The projected covariance measure for assumption-lean variable significance testing},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Noisy recovery from random linear observations: Sharp
minimax rates under elliptical constraints. <em>AOS</em>,
<em>52</em>(6), 2816–2850. (<a
href="https://doi.org/10.1214/24-AOS2446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation problems with constrained parameter spaces arise in various settings. In many of these problems, the observations available to the statistician can be modelled as arising from the noisy realization of the image of a random linear operator; an important special case is random design regression. We derive sharp rates of estimation for arbitrary compact elliptical parameter sets and demonstrate how they depend on the distribution of the random linear operator. Our main result is a functional that characterizes the minimax rate of estimation in terms of the noise level, the law of the random operator, and elliptical norms that define the error metric and the parameter space. This nonasymptotic result is sharp up to an explicit universal constant, and it becomes asymptotically exact as the radius of the parameter space is allowed to grow. We demonstrate the generality of the result by applying it to both parametric and nonparametric regression problems.},
  archive      = {J_AOS},
  author       = {Reese Pathak and Martin J. Wainwright and Lin Xiao},
  doi          = {10.1214/24-AOS2446},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2816-2850},
  shortjournal = {Ann. Statist.},
  title        = {Noisy recovery from random linear observations: Sharp minimax rates under elliptical constraints},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convex regression in multidimensions: Suboptimality of least
squares estimators. <em>AOS</em>, <em>52</em>(6), 2791–2815. (<a
href="https://doi.org/10.1214/24-AOS2445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the usual nonparametric regression model with Gaussian errors, Least Squares Estimators (LSEs) over natural subclasses of convex functions are shown to be suboptimal for estimating a d-dimensional convex function in squared error loss when the dimension d is 5 or larger. The specific function classes considered include: (i) bounded convex functions supported on a polytope (in random design), (ii) Lipschitz convex functions supported on any convex domain (in random design) and (iii) convex functions supported on a polytope (in fixed design). For each of these classes, the risk of the LSE is proved to be of the order n−2/d (up to logarithmic factors) while the minimax risk is n−4/(d+4), when d≥5. In addition, the first rate of convergence results (worst case and adaptive) for the unrestricted convex LSE are established in fixed design for polytopal domains for all d≥1. Some new metric entropy results for convex functions are also proved, which are of independent interest.},
  archive      = {J_AOS},
  author       = {Gil Kur and Fuchang Gao and Adityanand Guntuboyina and Bodhisattva Sen},
  doi          = {10.1214/24-AOS2445},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2791-2815},
  shortjournal = {Ann. Statist.},
  title        = {Convex regression in multidimensions: Suboptimality of least squares estimators},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the statistical complexity of sample amplification.
<em>AOS</em>, <em>52</em>(6), 2767–2790. (<a
href="https://doi.org/10.1214/24-AOS2444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “sample amplification” problem formalizes the following question: Given n i.i.d. samples drawn from an unknown distribution P, when is it possible to produce a larger set of n+m samples which cannot be distinguished from n+m i.i.d. samples drawn from P? In this work, we provide a firm statistical foundation for this problem by deriving generally applicable amplification procedures, lower bound techniques and connections to existing statistical notions. Our techniques apply to a large class of distributions including the exponential family, and establish a rigorous connection between sample amplification and distribution learning.},
  archive      = {J_AOS},
  author       = {Brian Axelrod and Shivam Garg and Yanjun Han and Vatsal Sharan and Gregory Valiant},
  doi          = {10.1214/24-AOS2444},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2767-2790},
  shortjournal = {Ann. Statist.},
  title        = {On the statistical complexity of sample amplification},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep neural networks for nonparametric interaction models
with diverging dimension. <em>AOS</em>, <em>52</em>(6), 2738–2766. (<a
href="https://doi.org/10.1214/24-AOS2442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have achieved tremendous success due to their representation power and adaptation to low-dimensional structures. Their potential for estimating structured regression functions has been recently established in the literature. However, most of the studies require the input dimension to be fixed, and consequently, they ignore the effect of dimension on the rate of convergence and hamper their applications to modern big data with high dimensionality. In this paper, we bridge this gap by analyzing a k-way nonparametric interaction model in both growing dimension scenarios (d grows with n but at a slower rate) and in high dimension (d≳n). In the latter case, sparsity assumptions and associated regularization are required to obtain optimal convergence rates. A new challenge in diverging dimension setting is in calculation mean-square error; the covariance terms among estimated additive components are an order of magnitude larger than those of the variances and can deteriorate statistical properties without proper care. We introduce a critical debiasing technique to amend the problem. We show that under certain standard assumptions, debiased deep neural networks achieve a minimax optimal rate both in terms of (n,d). Our proof techniques rely crucially on a novel debiasing technique that makes the covariances of additive components negligible in the mean-square error calculation. In addition, we establish the matching lower bounds.},
  archive      = {J_AOS},
  author       = {Sohom Bhattacharya and Jianqing Fan and Debarghya Mukherjee},
  doi          = {10.1214/24-AOS2442},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2738-2766},
  shortjournal = {Ann. Statist.},
  title        = {Deep neural networks for nonparametric interaction models with diverging dimension},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Skewed bernstein–von mises theorem and skew-modal
approximations. <em>AOS</em>, <em>52</em>(6), 2714–2737. (<a
href="https://doi.org/10.1214/24-AOS2429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian deterministic approximations are routinely employed in Bayesian statistics to ease inference when the target posterior is intractable. While these approximations are justified, in asymptotic regimes, by Bernstein–von Mises type results, in practice the expected Gaussian behavior might poorly represent the actual shape of the target posterior, thus affecting approximation accuracy. Motivated by these considerations, we derive an improved class of closed-form and valid deterministic approximations of posterior distributions that arise from a novel treatment of a third-order version of the Laplace method yielding approximations within a tractable family of skew-symmetric distributions. Under general assumptions accounting for misspecified models and non-i.i.d. settings, such a family of approximations is shown to have a total variation distance from the target posterior whose convergence rate improves by at least one order of magnitude the one achieved by the Gaussian from the classical Bernstein–von Mises theorem. Specializing this result to the case of regular parametric models shows that the same accuracy improvement can be also established for the posterior expectation of polynomially bounded functions. Unlike available higher-order approximations based on, for example, Edgeworth expansions, our results prove that it is possible to derive closed-form and valid densities which provide a more accurate, yet similarly tractable, alternative to Gaussian approximations of the target posterior, while inheriting its limiting frequentist properties. We strengthen these arguments by developing a practical skew-modal approximation for both joint and marginal posteriors which preserves the guarantees of its theoretical counterpart by replacing the unknown model parameters with the corresponding maximum a posteriori estimate. Simulation studies and real-data applications confirm that our theoretical results closely match the empirical gains observed in practice.},
  archive      = {J_AOS},
  author       = {Daniele Durante and Francesco Pozza and Botond Szabo},
  doi          = {10.1214/24-AOS2429},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2714-2737},
  shortjournal = {Ann. Statist.},
  title        = {Skewed bernstein–von mises theorem and skew-modal approximations},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stereographic markov chain monte carlo. <em>AOS</em>,
<em>52</em>(6), 2692–2713. (<a
href="https://doi.org/10.1214/24-AOS2426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional distributions, especially those with heavy tails, are notoriously difficult for off-the-shelf MCMC samplers: the combination of unbounded state spaces, diminishing gradient information, and local moves results in empirically observed “stickiness” and poor theoretical mixing properties—lack of geometric ergodicity. In this paper, we introduce a new class of MCMC samplers that map the original high-dimensional problem in Euclidean space onto a sphere and remedy these notorious mixing problems. In particular, we develop random-walk Metropolis type algorithms as well as versions of the Bouncy Particle Sampler that are uniformly ergodic for a large class of light and heavy-tailed distributions and also empirically exhibit rapid convergence in high dimensions. In the best scenario, the proposed samplers can enjoy the “blessings of dimensionality” that the convergence is faster in higher dimensions.},
  archive      = {J_AOS},
  author       = {Jun Yang and Krzysztof Łatuszyński and Gareth O. Roberts},
  doi          = {10.1214/24-AOS2426},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2692-2713},
  shortjournal = {Ann. Statist.},
  title        = {Stereographic markov chain monte carlo},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for four-regime segmented regression
models. <em>AOS</em>, <em>52</em>(6), 2668–2691. (<a
href="https://doi.org/10.1214/24-AOS2417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmented regression models offer model flexibility and interpretability as compared to the global parametric and the nonparametric models, and yet are challenging in both estimation and inference. We consider a four-regime segmented model for temporally dependent data with segmenting boundaries depending on multivariate covariates with nondiminishing boundary effects. A mixed integer quadratic programming algorithm is formulated to facilitate the least square estimation of the regression and the boundary parameters. The rates of convergence and the asymptotic distributions of the least square estimators are obtained for the regression and the boundary coefficients, respectively. We propose a smoothed regression bootstrap to facilitate inference on the parameters and a model selection procedure to select the most suitable model within the model class with at most four segments. Numerical simulations and a case study on air pollution in Beijing are conducted to demonstrate the proposed approach, which shows that the segmented models with three or four regimes are suitable for the modeling of the meteorological effects on the PM2.5 concentration.},
  archive      = {J_AOS},
  author       = {Han Yan and Song Xi Chen},
  doi          = {10.1214/24-AOS2417},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2668-2691},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference for four-regime segmented regression models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor factor model estimation by iterative projection.
<em>AOS</em>, <em>52</em>(6), 2641–2667. (<a
href="https://doi.org/10.1214/24-AOS2412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor time series, which is a time series consisting of tensorial observations, has become ubiquitous. It typically exhibits high dimensionality. One approach for dimension reduction is to use a factor model structure, in a form similar to Tucker tensor decomposition, except that the time dimension is treated as a dynamic process with a time dependent structure. In this paper, we introduce two approaches to estimate such a tensor factor model by using iterative orthogonal projections of the original tensor time series. These approaches extend the existing estimation procedures and improve the estimation accuracy and convergence rate significantly as proven in our theoretical investigation. Our algorithms are similar to the higher-order orthogonal projection method for tensor decomposition, but with significant differences due to the need to unfold tensors in the iterations and the use of autocorrelation. Consequently, our analysis is significantly different from the existing ones. Computational and statistical lower bounds are derived to prove the optimality of the sample size requirement and convergence rate for the proposed methods. Simulation study is conducted to further illustrate the statistical properties of these estimators.},
  archive      = {J_AOS},
  author       = {Yuefeng Han and Rong Chen and Dan Yang and Cun-Hui Zhang},
  doi          = {10.1214/24-AOS2412},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2641-2667},
  shortjournal = {Ann. Statist.},
  title        = {Tensor factor model estimation by iterative projection},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Time-uniform central limit theory and asymptotic confidence
sequences. <em>AOS</em>, <em>52</em>(6), 2613–2640. (<a
href="https://doi.org/10.1214/24-AOS2408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Confidence intervals based on the central limit theorem (CLT) are a cornerstone of classical statistics. Despite being only asymptotically valid, they are ubiquitous because they permit statistical inference under weak assumptions and can often be applied to problems even when nonasymptotic inference is impossible. This paper introduces time-uniform analogues of such asymptotic confidence intervals, adding to the literature on confidence sequences (CS)—sequences of confidence intervals that are uniformly valid over time—which provide valid inference at arbitrary stopping times and incur no penalties for “peeking” at the data, unlike classical confidence intervals which require the sample size to be fixed in advance. Existing CSs in the literature are nonasymptotic, enjoying finite-sample guarantees but not the aforementioned broad applicability of asymptotic confidence intervals. This work provides a definition for “asymptotic CSs” and a general recipe for deriving them. Asymptotic CSs forgo nonasymptotic validity for CLT-like versatility and (asymptotic) time-uniform guarantees. While the CLT approximates the distribution of a sample average by that of a Gaussian for a fixed sample size, we use strong invariance principles (stemming from the seminal 1960s work of Strassen) to uniformly approximate the entire sample average process by an implicit Gaussian process. As an illustration, we derive asymptotic CSs for the average treatment effect in observational studies (for which nonasymptotic bounds are essentially impossible to derive even in the fixed-time regime) as well as randomized experiments, enabling causal inference in sequential environments.},
  archive      = {J_AOS},
  author       = {Ian Waudby-Smith and David Arbour and Ritwik Sinha and Edward H. Kennedy and Aaditya Ramdas},
  doi          = {10.1214/24-AOS2408},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2613-2640},
  shortjournal = {Ann. Statist.},
  title        = {Time-uniform central limit theory and asymptotic confidence sequences},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor-on-tensor regression: Riemannian optimization,
over-parameterization, statistical-computational gap and their
interplay. <em>AOS</em>, <em>52</em>(6), 2583–2612. (<a
href="https://doi.org/10.1214/24-AOS2396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low Tucker rank parameter tensor/matrix without prior knowledge of its intrinsic rank. We propose the Riemannian gradient descent (RGD) and Riemannian Gauss–Newton (RGN) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. We provide the first convergence guarantee for the general tensor-on-tensor regression by showing that RGD and RGN respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. Our theory reveals an intriguing phenomenon: Riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. We also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. Our theory demonstrates a “blessing of statistical-computational gap” phenomenon: in a wide range of scenarios in tensor-on-tensor regression for tensors of order three or higher, the computationally required sample size matches what is needed by moderate rank over-parameterization when considering computationally feasible estimators, while there are no such benefits in the matrix settings. This shows moderate rank over-parameterization is essentially “cost-free” in terms of sample size in tensor-on-tensor regression of order three or higher. Finally, we conduct simulation studies to show the advantages of our proposed methods and to corroborate our theoretical findings.},
  archive      = {J_AOS},
  author       = {Yuetian Luo and Anru R. Zhang},
  doi          = {10.1214/24-AOS2396},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2583-2612},
  shortjournal = {Ann. Statist.},
  title        = {Tensor-on-tensor regression: Riemannian optimization, over-parameterization, statistical-computational gap and their interplay},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical complexity and optimal algorithms for nonlinear
ridge bandits. <em>AOS</em>, <em>52</em>(6), 2557–2582. (<a
href="https://doi.org/10.1214/24-AOS2395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the sequential decision-making problem where the mean outcome is a nonlinear function of the chosen action. Compared with the linear model, two curious phenomena arise in nonlinear models: first, in addition to the “learning phase” with a standard parametric rate for estimation or regret, there is an “burn-in period” with a fixed cost determined by the nonlinear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of nonlinear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal.},
  archive      = {J_AOS},
  author       = {Nived Rajaraman and Yanjun Han and Jiantao Jiao and Kannan Ramchandran},
  doi          = {10.1214/24-AOS2395},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2557-2582},
  shortjournal = {Ann. Statist.},
  title        = {Statistical complexity and optimal algorithms for nonlinear ridge bandits},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation of the spectral measure from convex combinations
of regularly varying random vectors. <em>AOS</em>, <em>52</em>(6),
2529–2556. (<a href="https://doi.org/10.1214/24-AOS2387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extremal dependence structure of a regularly varying random vector X is fully described by its limiting spectral measure. In this paper, we investigate how to recover characteristics of the measure, such as extremal coefficients, from the extremal behaviour of convex combinations of components of X. Our considerations result in a class of new estimators of moments of the corresponding combinations for the spectral vector. We show asymptotic normality by means of a functional limit theorem and, focusing on the estimation of extremal coefficients, we verify that the minimal asymptotic variance can be achieved by a plug-in estimator using subsampling bootstrap. We illustrate the benefits of our approach on simulated and real data.},
  archive      = {J_AOS},
  author       = {Marco Oesting and Olivier Wintenberger},
  doi          = {10.1214/24-AOS2387},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2529-2556},
  shortjournal = {Ann. Statist.},
  title        = {Estimation of the spectral measure from convex combinations of regularly varying random vectors},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Non-independent component analysis. <em>AOS</em>,
<em>52</em>(6), 2506–2528. (<a
href="https://doi.org/10.1214/24-AOS2373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A seminal result in the ICA literature states that for AY=ε, if the components of ε are independent and at most one is Gaussian, then A is identified up to sign and permutation of its rows (Signal Process. 36 (1994)). In this paper we study to which extent the independence assumption can be relaxed by replacing it with restrictions on higher order moment or cumulant tensors of ε. We document new conditions that establish identification for several nonindependent component models, for example, common variance models, and propose efficient estimation methods based on the identification results. We show that in situations where independence cannot be assumed the efficiency gains can be significant relative to methods that rely on independence.},
  archive      = {J_AOS},
  author       = {Geert Mesters and Piotr Zwiernik},
  doi          = {10.1214/24-AOS2373},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2506-2528},
  shortjournal = {Ann. Statist.},
  title        = {Non-independent component analysis},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing network correlation efficiently via counting trees.
<em>AOS</em>, <em>52</em>(6), 2483–2505. (<a
href="https://doi.org/10.1214/23-AOS2261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new procedure for testing whether two networks are edge-correlated through some latent vertex correspondence. The test statistic is based on counting the cooccurrences of signed trees for a family of nonisomorphic trees. When the two networks are Erdős–Rényi random graphs G(n,q) that are either independent or correlated with correlation coefficient ρ, our test runs in n2+o(1) time and succeeds with high probability as n→∞, provided that nmin{q,1−q}≥n−o(1) and ρ2&gt;α≈0.338, where α is Otter’s constant so that the number of unlabeled trees with K edges grows as (1/α)K. This significantly improves the prior work in terms of statistical accuracy, running time and graph sparsity.},
  archive      = {J_AOS},
  author       = {Cheng Mao and Yihong Wu and Jiaming Xu and Sophie H. Yu},
  doi          = {10.1214/23-AOS2261},
  journal      = {The Annals of Statistics},
  month        = {12},
  number       = {6},
  pages        = {2483-2505},
  shortjournal = {Ann. Statist.},
  title        = {Testing network correlation efficiently via counting trees},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A gaussian process approach to model checks. <em>AOS</em>,
<em>52</em>(5), 2456–2481. (<a
href="https://doi.org/10.1214/24-AOS2443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a Gaussian process (GP) approach for testing conditional moment restrictions. Tests are based on squared Neyman orthogonal function-parametric processes integrated with respect to a GP distribution. This methodology leads to a general unified framework of kernel-based tests having the following properties: (i) bootstrap tests are easy to implement in the presence of nuisance parameters (they are simple quadratic forms, and there is no need to reestimate the nuisance parameters in each bootstrap replication); and (ii) the new tests are valid under general conditions, including higher-order conditional moments of unknown form, regularized estimators (e.g., Lasso) or parameters at the boundary of the parameter space. Novel applications include distance kernel tests for zero conditional treatment effects. The paper introduces Neyman orthogonal kernels, a new asymptotic theory and a detailed local power analysis. Monte Carlo experiments and a real data application illustrate the sensitivity of tests to the dimension of covariates and to the mean and covariance kernel of the GP.},
  archive      = {J_AOS},
  author       = {Juan Carlos Escanciano},
  doi          = {10.1214/24-AOS2443},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2456-2481},
  shortjournal = {Ann. Statist.},
  title        = {A gaussian process approach to model checks},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational and statistical thresholds in multi-layer
stochastic block models. <em>AOS</em>, <em>52</em>(5), 2431–2455. (<a
href="https://doi.org/10.1214/24-AOS2441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of community recovery and detection in multi-layer stochastic block models, focusing on the critical network density threshold for consistent community structure inference. Using a prototypical two-block model, we reveal a computational barrier for such multilayer stochastic block models that does not exist for its single-layer counterpart: When there are no computational constraints, the density threshold depends linearly on the number of layers. However, when restricted to polynomial-time algorithms, the density threshold scales with the square root of the number of layers, assuming correctness of a low-degree polynomial hardness conjecture. Our results provide a nearly complete picture of the optimal inference in multiple-layer stochastic block models and partially settle the open question in (J. Amer. Statist. Assoc. 118 (2023) 2433–2445) regarding the optimality of the bias-adjusted spectral method.},
  archive      = {J_AOS},
  author       = {Jing Lei and Anru R. Zhang and Zihan Zhu},
  doi          = {10.1214/24-AOS2441},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2431-2455},
  shortjournal = {Ann. Statist.},
  title        = {Computational and statistical thresholds in multi-layer stochastic block models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multivariate trend filtering for lattice data. <em>AOS</em>,
<em>52</em>(5), 2400–2430. (<a
href="https://doi.org/10.1214/24-AOS2440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study a multivariate version of trend filtering, called Kronecker trend filtering or KTF, for the case in which the design points form a lattice in d dimensions. KTF is a natural extension of univariate trend filtering (Int. J. Comput. Vis. 70 (2006) 214–255; SIAM Rev. 51 (2009) 339–360; Ann. Statist. 42 (2014) 285–323), and is defined by minimizing a penalized least squares problem whose penalty term sums the absolute (higher-order) differences of the parameter to be estimated along each of the coordinate directions. The corresponding penalty operator can be written in terms of Kronecker products of univariate trend filtering penalty operators, hence the name Kronecker trend filtering. Equivalently, one can view KTF in terms of an ℓ1-penalized basis regression problem where the basis functions are tensor products of falling factorial functions, which is a piecewise polynomial (discrete spline) basis that underlies univariate trend filtering. This paper is a unification and extension of the results in (In Advances in Neural Information Processing Systems (2016); in Advances in Neural Information Processing Systems (2017)). We develop a complete set of theoretical results that describe the behavior of kth-order Kronecker trend filtering in d dimensions, for every k≥0 and d≥1. This reveals a number of interesting phenomena, including the dominance of KTF over linear smoothers in estimating heterogeneously smooth functions, and a phase transition at d=2(k+1), a boundary past which (on the high dimension-to-smoothness side) linear smoothers fail to be consistent entirely. We also leverage recent results on discrete splines from (Tibshirani (2020)), in particular, discrete spline interpolation results that enable us to extend the KTF estimate to any off-lattice location in constant-time (independent of the size of the lattice n).},
  archive      = {J_AOS},
  author       = {Veeranjaneyulu Sadhanala and Yu-Xiang Wang and Addison J. Hu and Ryan J. Tibshirani},
  doi          = {10.1214/24-AOS2440},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2400-2430},
  shortjournal = {Ann. Statist.},
  title        = {Multivariate trend filtering for lattice data},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Simultaneous statistical inference for second order
parameters of time series under weak conditions. <em>AOS</em>,
<em>52</em>(5), 2375–2399. (<a
href="https://doi.org/10.1214/24-AOS2439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strict stationarity is an assumption commonly used in time-series analysis in order to derive asymptotic distributional results for second-order statistics, like sample autocovariances and sample autocorrelations. Focusing on weak stationarity, this paper derives the asymptotic distribution of the maximum of sample autocovariances and sample autocorrelations under weak conditions by using Gaussian approximation techniques. The asymptotic theory for parameter estimators obtained by fitting a (linear) autoregressive model to a general weakly stationary time series is revisited and a Gaussian approximation theorem for the maximum of the estimators of the autoregressive coefficients is derived. To perform statistical inference for the aforementioned second-order parameters of interest, a bootstrap algorithm, the so-called second-order wild bootstrap is applied. Consistency of the bootstrap procedure is proven without imposing strict stationary conditions or structural process assumptions, like linearity. The good finite sample performance of the second-order wild bootstrap is demonstrated by means of simulations.},
  archive      = {J_AOS},
  author       = {Yunyi Zhang and Efstathios Paparoditis and Dimitris N. Politis},
  doi          = {10.1214/24-AOS2439},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2375-2399},
  shortjournal = {Ann. Statist.},
  title        = {Simultaneous statistical inference for second order parameters of time series under weak conditions},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric test for elliptical distribution based on
kernel embedding of probabilities. <em>AOS</em>, <em>52</em>(5),
2349–2374. (<a href="https://doi.org/10.1214/24-AOS2438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elliptical distribution is a basic assumption underlying many multivariate statistical methods. For example, in sufficient dimension reduction and statistical graphical models, this assumption is routinely imposed to simplify the data dependence structure. Before applying such methods, we need to decide whether the data are elliptically distributed. Currently existing tests either focus exclusively on spherical distributions, or rely on bootstrap to determine the null distribution, or require specific forms of the alternative distribution. In this paper, we introduce a general nonparametric test for elliptical distribution based on kernel embedding of the probability measure that embodies the two properties that characterize an elliptical distribution: namely, after centering and rescaling, (1) the direction and length of the random vector are independent, and (2) the directional vector is uniformly distributed on the unit sphere. We derive the asymptotic distributions of the test statistic via von Mises expansion, develop the sample-level procedure to determine the rejection region, and establish the consistency and validity of the proposed test. We also develop the concentration bounds of the test statistic, allowing the dimension to grow with the sample size, and further establish the consistency in this high-dimension setting. We compare our method with several existing methods via simulation studies, and apply our test to a SENIC dataset with and without a transformation aimed to achieve ellipticity.},
  archive      = {J_AOS},
  author       = {Yin Tang and Bing Li},
  doi          = {10.1214/24-AOS2438},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2349-2374},
  shortjournal = {Ann. Statist.},
  title        = {A nonparametric test for elliptical distribution based on kernel embedding of probabilities},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computational lower bounds for graphon estimation via
low-degree polynomials. <em>AOS</em>, <em>52</em>(5), 2318–2348. (<a
href="https://doi.org/10.1214/24-AOS2437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphon estimation has been one of the most fundamental problems in network analysis and has received considerable attention in the past decade. From the statistical perspective, the minimax error rate of graphon estimation has been established by (Ann. Statist. 43 (2015) 2624–2652) for both stochastic block model (SBM) and nonparametric graphon estimation. The statistical optimal estimators are based on constrained least squares and have computational complexity exponential in the dimension. From the computational perspective, the best-known, polynomial-time estimator is based on universal singular value thresholding (USVT), but it can only achieve a much slower estimation error rate than the minimax one. It is natural to wonder if such a gap is essential. The computational optimality of the USVT or the existence of a computational barrier in graphon estimation has been a long-standing open problem. In this work, we take the first step toward it and provide rigorous evidence for the computational barrier in graphon estimation via low-degree polynomials. Specifically, in SBM graphon estimation, we show that for low-degree polynomial estimators, their estimation error rates cannot be significantly better than that of the USVT under a wide range of parameter regimes and in nonparametric graphon estimation, we show low-degree polynomial estimators achieve estimation error rates strictly slower than the minimax rate. Our results are proved based on the recent development of low-degree polynomials by (Ann. Statist. 50 (2022) 1833–1858), while we overcome a few key challenges in applying it to the general graphon estimation problem. By leveraging our main results, we also provide a computational lower bound on the clustering error for community detection in SBM with a growing number of communities and this yields a new piece of evidence for the conjectured Kesten–Stigum threshold for efficient community recovery. Finally, we extend our computational lower bounds to sparse graphon estimation and biclustering with additive Gaussian noise, and provide discussion on the optimality of our results.},
  archive      = {J_AOS},
  author       = {Yuetian Luo and Chao Gao},
  doi          = {10.1214/24-AOS2437},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2318-2348},
  shortjournal = {Ann. Statist.},
  title        = {Computational lower bounds for graphon estimation via low-degree polynomials},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gaussian approximation for nonstationary time series with
optimal rate and explicit construction. <em>AOS</em>, <em>52</em>(5),
2293–2317. (<a href="https://doi.org/10.1214/24-AOS2436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical inference for time series such as curve estimation for time-varying models or testing for existence of a change point have garnered significant attention. However, these works are generally restricted to the assumption of independence and/or stationarity at its best. The main obstacle is that the existing Gaussian approximation results for nonstationary processes only provide an existential proof, and thus they are difficult to apply. In this paper, we provide two clear paths to construct such a Gaussian approximation for nonstationary series. While the first one is theoretically more natural, the second one is practically implementable. Our Gaussian approximation results are applicable for a very large class of nonstationary time series, obtain optimal rates and yet have good applicability. Building on such approximations, we also show theoretical results for change-point detection and simultaneous inference in presence of nonstationary errors. Finally, we substantiate our theoretical results with simulation studies and real data analysis.},
  archive      = {J_AOS},
  author       = {Soham Bonnerjee and Sayar Karmakar and Wei Biao Wu},
  doi          = {10.1214/24-AOS2436},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2293-2317},
  shortjournal = {Ann. Statist.},
  title        = {Gaussian approximation for nonstationary time series with optimal rate and explicit construction},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Environment invariant linear least squares. <em>AOS</em>,
<em>52</em>(5), 2268–2292. (<a
href="https://doi.org/10.1214/24-AOS2435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a multi-environment linear regression model in which data from multiple experimental settings are collected. The joint distribution of the response variable and covariates may vary across different environments, yet the conditional expectations of the response variable, given the unknown set of important variables, are invariant. Such a statistical model is related to the problem of endogeneity, causal inference, and transfer learning. The motivation behind it is illustrated by how the goals of prediction and attribution are inherent in estimating the true parameter and the important variable set. We construct a novel environment invariant linear least squares (EILLS) objective function, a multi-environment version of linear least squares regression that leverages the above conditional expectation invariance structure and heterogeneity among different environments to determine the true parameter. Our proposed method is applicable without any additional structural knowledge and can identify the true parameter under a near-minimal identification condition related to the heterogeneity of the environments. We establish nonasymptotic ℓ2 error bounds on the estimation error for the EILLS estimator in the presence of spurious variables. Moreover, we further show that the ℓ0 penalized EILLS estimator can achieve variable selection consistency in high-dimensional regimes. These nonasymptotic results demonstrate the sample efficiency of the EILLS estimator and its capability to circumvent the curse of endogeneity in an algorithmic manner without any additional prior structural knowledge. To the best of our knowledge, this paper is the first to realize statistically efficient invariance learning in the general linear model.},
  archive      = {J_AOS},
  author       = {Jianqing Fan and Cong Fang and Yihong Gu and Tong Zhang},
  doi          = {10.1214/24-AOS2435},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2268-2292},
  shortjournal = {Ann. Statist.},
  title        = {Environment invariant linear least squares},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the existence of powerful p-values and e-values for
composite hypotheses. <em>AOS</em>, <em>52</em>(5), 2241–2267. (<a
href="https://doi.org/10.1214/24-AOS2434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a composite null P and composite alternative Q, when and how can we construct a p-value whose distribution is exactly uniform under the null, and stochastically smaller than uniform under the alternative? Similarly, when and how can we construct an e-value whose expectation exactly equals one under the null, but its expected logarithm under the alternative is positive? We answer these basic questions, and other related ones, when P and Q are convex polytopes (in the space of probability measures). We prove that such constructions are possible if and only if Q does not intersect the span of P. If the p-value is allowed to be stochastically larger than uniform under P∈P, and the e-value can have expectation at most one under P∈P, then it is achievable whenever P and Q are disjoint. More generally, even when P and Q are not polytopes, we characterize the existence of a bounded nontrivial e-variable whose expectation exactly equals one under any P∈P. The proofs utilize recently developed techniques in simultaneous optimal transport. A key role is played by coarsening the filtration: sometimes, no such p-value or e-value exists in the richest data filtration, but it does exist in some reduced filtration, and our work provides the first general characterization of this phenomenon. We also provide an iterative construction that explicitly constructs such processes, and under certain conditions it finds the one that grows fastest under a specific alternative Q. We discuss implications for the construction of composite nonnegative (super)martingales, and end with some conjectures and open problems.},
  archive      = {J_AOS},
  author       = {Zhenyuan Zhang and Aaditya Ramdas and Ruodu Wang},
  doi          = {10.1214/24-AOS2434},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2241-2267},
  shortjournal = {Ann. Statist.},
  title        = {On the existence of powerful p-values and e-values for composite hypotheses},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new test for high-dimensional two-sample mean problems
with consideration of correlation structure. <em>AOS</em>,
<em>52</em>(5), 2217–2240. (<a
href="https://doi.org/10.1214/24-AOS2433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with high-dimensional two-sample mean problems, which receive considerable attention in recent literature. To utilize the correlation information among variables for enhancing the power of two-sample mean tests, we consider the setting in which the precision matrix of high-dimensional data possesses a linear structure. Thus, we first propose a new precision matrix estimation procedure with considering its linear structure, and further develop regularization methods to select the true basis matrices and remove irrelevant basis matrices. With the aid of an estimated precision matrix, we propose a new test statistic for the two-sample mean problems by replacing the inverse of sample covariance matrix in the Hotelling test by the estimated precision matrix. The proposed test is applicable for both the low-dimensional setting and high-dimensional setting even if the dimension of the data exceeds the sample size. The limiting null distributions of the proposed test statistic under both null and alternative hypotheses are derived. We further derive the asymptotical power function of the proposed test and compare its asymptotic power with some existing tests. We find the estimation error of the precision matrix does not have impact on the asymptotical power function. Moreover, asymptotic relative efficiency of the proposed test to the classical Hotelling test tends to infinity when the ratio of the dimension of data to the sample size tends to 1. We conduct a Monte Carlo simulation study to assess the finite sample performance of the proposed precision matrix estimation procedure and the proposed high-dimensional two-sample mean test. Our numerical results imply that the proposed regularization method is able to effectively remove irrelevant basis matrices. The proposed test performs well compared with the existing methods especially when the elements of the vector have unequal variances. We also illustrate the proposed methodology by an empirical analysis of a real-world data set.},
  archive      = {J_AOS},
  author       = {Songshan Yang and Shurong Zheng and Runze Li},
  doi          = {10.1214/24-AOS2433},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2217-2240},
  shortjournal = {Ann. Statist.},
  title        = {A new test for high-dimensional two-sample mean problems with consideration of correlation structure},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantile processes and their applications in finite
populations. <em>AOS</em>, <em>52</em>(5), 2194–2216. (<a
href="https://doi.org/10.1214/24-AOS2432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The weak convergence of the quantile processes, which are constructed based on different estimators of the finite population quantiles, is shown under various well-known sampling designs based on a superpopulation model. The results related to the weak convergence of these quantile processes are applied to find asymptotic distributions of the smooth L-estimators and the estimators of smooth functions of finite population quantiles. Based on these asymptotic distributions, confidence intervals are constructed for several finite population parameters like the median, the α-trimmed means, the interquartile range and the quantile based measure of skewness. Comparisons of various estimators are carried out based on their asymptotic distributions. We show that the use of the auxiliary information in the construction of the estimators sometimes has an adverse effect on the performances of the smooth L-estimators and the estimators of smooth functions of finite population quantiles under several sampling designs. Further, the performance of each of the above-mentioned estimators sometimes becomes worse under sampling designs, which use the auxiliary information, than their performances under simple random sampling without replacement (SRSWOR).},
  archive      = {J_AOS},
  author       = {Anurag Dey and Probal Chaudhuri},
  doi          = {10.1214/24-AOS2432},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2194-2216},
  shortjournal = {Ann. Statist.},
  title        = {Quantile processes and their applications in finite populations},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein generative adversarial networks are minimax
optimal distribution estimators. <em>AOS</em>, <em>52</em>(5),
2167–2193. (<a href="https://doi.org/10.1214/24-AOS2430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide nonasymptotic rates of convergence of the Wasserstein Generative Adversarial networks (WGAN) estimator. We build neural networks classes representing the generators and discriminators which yield a GAN that achieves the minimax optimal rate for estimating a certain probability measure μ with support in Rp. The probability μ is considered to be the push forward of the Lebesgue measure on the d-dimensional torus Td by a map g⋆:Td→Rp of smoothness β+1. Measuring the error with the γ-Hölder Integral Probability Metric (IPM), we obtain up to logarithmic factors, the minimax optimal rate O(n−β+γ2β+d∨n−12) where n is the sample size, β determines the smoothness of the target measure μ, γ is the smoothness of the IPM (γ=1 is the Wasserstein case) and d≤p is the intrinsic dimension of μ. In the process, we derive a sharp interpolation inequality between Hölder IPMs. This novel result of theory of functions spaces generalizes classical interpolation inequalities to the case where the measures involved have densities on different manifolds.},
  archive      = {J_AOS},
  author       = {Arthur Stéphanovitch and Eddie Aamari and Clément Levrard},
  doi          = {10.1214/24-AOS2430},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2167-2193},
  shortjournal = {Ann. Statist.},
  title        = {Wasserstein generative adversarial networks are minimax optimal distribution estimators},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiency in local differential privacy. <em>AOS</em>,
<em>52</em>(5), 2139–2166. (<a
href="https://doi.org/10.1214/24-AOS2425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a theory of asymptotic efficiency in regular parametric models when data confidentiality is ensured by local differential privacy (LDP). Even though efficient parameter estimation is a classical and well-studied problem in mathematical statistics, it leads to several nontrivial obstacles that need to be tackled when dealing with the LDP case. Starting from a regular parametric model P=(Pθ)θ∈Θ, Θ⊆Rp, for the i.i.d. unobserved sensitive data X1,…,Xn, we establish local asymptotic mixed normality (along subsequences) of the model Q(n)P=(Q(n)Pθn)θ∈Θ generating the sanitized observations Z1,…,Zn, where Q(n) is an arbitrary sequence of sequentially interactive privacy mechanisms. This result readily implies convolution and local asymptotic minimax theorems. In case p=1, the optimal asymptotic variance is found to be the inverse of the supremal Fisher information supQ∈QαIθ(QP)∈R, where the supremum runs over all α-differentially private (marginal) Markov kernels. We present an algorithm for finding a (nearly) optimal privacy mechanism Qˆ and an estimator θˆn(Z1,…,Zn) based on the corresponding sanitized data that achieves this asymptotically optimal variance.},
  archive      = {J_AOS},
  author       = {Lukas Steinberger},
  doi          = {10.1214/24-AOS2425},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2139-2166},
  shortjournal = {Ann. Statist.},
  title        = {Efficiency in local differential privacy},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exact minimax optimality of spectral methods in phase
synchronization and orthogonal group synchronization. <em>AOS</em>,
<em>52</em>(5), 2112–2138. (<a
href="https://doi.org/10.1214/24-AOS2424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the performance of the spectral method for the phase synchronization problem with additive Gaussian noises and incomplete data. The spectral method utilizes the leading eigenvector of the data matrix followed by a normalization step. We prove that it achieves the minimax lower bound of the problem with a matching leading constant under a squared ℓ2 loss. This shows that the spectral method has the same performance as more sophisticated procedures including maximum likelihood estimation, generalized power method, and semidefinite programming, as long as consistent parameter estimation is possible. To establish our result, we first have a novel choice of the population eigenvector, which enables us to establish the exact recovery of the spectral method when there is no additive noise. We then develop a new perturbation analysis toolkit for the leading eigenvector and show it can be well-approximated by its first-order approximation with a small ℓ2 error. We further extend our analysis to establish the exact minimax optimality of the spectral method for the orthogonal group synchronization.},
  archive      = {J_AOS},
  author       = {Anderson Ye Zhang},
  doi          = {10.1214/24-AOS2424},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2112-2138},
  shortjournal = {Ann. Statist.},
  title        = {Exact minimax optimality of spectral methods in phase synchronization and orthogonal group synchronization},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating a density near an unknown manifold: A bayesian
nonparametric approach. <em>AOS</em>, <em>52</em>(5), 2081–2111. (<a
href="https://doi.org/10.1214/24-AOS2423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Bayesian density estimation of data living in the offset of an unknown submanifold of the Euclidean space. In this perspective, we introduce a new notion of anisotropic Hölder for the underlying density and obtain posterior rates that are minimax optimal and adaptive to the regularity of the density, to the intrinsic dimension of the manifold, and to the size of the offset, provided that the latter is not too small—while still allowed to go to zero. Our Bayesian procedure, based on location-scale mixtures of Gaussians, appears to be convenient to implement and yields good practical results, even for quite singular data.},
  archive      = {J_AOS},
  author       = {Clément Berenfeld and Paul Rosa and Judith Rousseau},
  doi          = {10.1214/24-AOS2423},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2081-2111},
  shortjournal = {Ann. Statist.},
  title        = {Estimating a density near an unknown manifold: A bayesian nonparametric approach},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A conformal test of linear models via permutation-augmented
regressions. <em>AOS</em>, <em>52</em>(5), 2059–2080. (<a
href="https://doi.org/10.1214/24-AOS2421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permutation tests are widely recognized as robust alternatives to tests based on normal theory. Random permutation tests have been frequently employed to assess the significance of variables in linear models. Despite their widespread use, existing random permutation tests lack finite-sample and assumption-free guarantees for controlling type I error in partial correlation tests. To address this ongoing challenge, we have developed a conformal test through permutation-augmented regressions, which we refer to as PALMRT. PALMRT not only achieves power competitive with conventional methods but also provides reliable control of type I errors at no more than 2α, given any targeted level α, for arbitrary fixed designs and error distributions. We have confirmed this through extensive simulations. Compared to the cyclic permutation test (CPT) and residual permutation test (RPT), which also offer theoretical guarantees, PALMRT does not compromise as much on power or set stringent requirements on the sample size, making it suitable for diverse biomedical applications. We further illustrate the differences in a long-Covid study where PALMRT validated key findings previously identified using the t-test after multiple corrections, while both CPT and RPT suffered from a drastic loss of power and failed to identify any discoveries. We endorse PALMRT as a robust and practical hypothesis test in scientific research for its superior error control, power preservation, and simplicity.},
  archive      = {J_AOS},
  author       = {Leying Guan},
  doi          = {10.1214/24-AOS2421},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2059-2080},
  shortjournal = {Ann. Statist.},
  title        = {A conformal test of linear models via permutation-augmented regressions},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing high-dimensional regression coefficients in linear
models. <em>AOS</em>, <em>52</em>(5), 2034–2058. (<a
href="https://doi.org/10.1214/24-AOS2420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with statistical inference for regression coefficients in high-dimensional linear regression models. We propose a new method for testing the coefficient vector of the high-dimensional linear models, and establish the asymptotic normality of our proposed test statistic with the aid of the martingale central limit theorem. We derive the asymptotical relative efficiency (ARE) of the proposed test with respect to the test proposed in Zhong and Chen (J. Amer. Statist. Assoc. 106 (2011) 260–274), and show that the ARE is always greater or equal to one under the local alternative studied in this paper. Our numerical studies imply that the proposed test with critical values derived from its asymptotical normal distribution may retain Type I error rate very well. Our numerical comparison demonstrates the proposed test performs better than existing ones in terms of powers. We further illustrate our proposed method with a real data example.},
  archive      = {J_AOS},
  author       = {Alex Zhao and Changcheng Li and Runze Li and Zhe Zhang},
  doi          = {10.1214/24-AOS2420},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2034-2058},
  shortjournal = {Ann. Statist.},
  title        = {Testing high-dimensional regression coefficients in linear models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Leave-one-out singular subspace perturbation analysis for
spectral clustering. <em>AOS</em>, <em>52</em>(5), 2004–2033. (<a
href="https://doi.org/10.1214/24-AOS2418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The singular subspaces perturbation theory is of fundamental importance in probability and statistics. It has various applications across different fields. We consider two arbitrary matrices where one is a leave-one-column-out submatrix of the other one and establish a novel perturbation upper bound for the distance between the two corresponding singular subspaces. It is well suited for mixture models and results in a sharper and finer statistical analysis than classical perturbation bounds such as Wedin’s theorem. Empowered by this leave-one-out perturbation theory, we provide a deterministic entrywise analysis for the performance of spectral clustering under mixture models. Our analysis leads to an explicit exponential error rate for spectral clustering of sub-Gaussian mixture models. For the mixture of isotropic Gaussians, the rate is optimal under a weaker signal-to-noise condition than that of Löffler et al. (2021).},
  archive      = {J_AOS},
  author       = {Anderson Y. Zhang and Harrison Y. Zhou},
  doi          = {10.1214/24-AOS2418},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {2004-2033},
  shortjournal = {Ann. Statist.},
  title        = {Leave-one-out singular subspace perturbation analysis for spectral clustering},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Debiased inverse propensity score weighting for estimation
of average treatment effects with high-dimensional confounders.
<em>AOS</em>, <em>52</em>(5), 1978–2003. (<a
href="https://doi.org/10.1214/24-AOS2409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimation of average treatment effects given observational data with high-dimensional pretreatment variables. Existing methods for this problem typically assume some form of sparsity for the regression functions. In this work, we introduce a debiased inverse propensity score weighting (DIPW) scheme for average treatment effect estimation that delivers n-consistent estimates when the propensity score follows a sparse logistic regression model; the outcome regression functions are permitted to be arbitrarily complex. We further demonstrate how confidence intervals centred on our estimates may be constructed. Our theoretical results quantify the price to pay for permitting the regression functions to be unestimable, which shows up as an inflation of the variance of the estimator compared to the semiparametric efficient variance by a constant factor, under mild conditions. We also show that when outcome regressions can be estimated consistently, our estimator achieves semiparametric efficiency. As our results accommodate arbitrary outcome regression functions, averages of transformed responses under each treatment may also be estimated at the n rate. Thus, for example, the variances of the potential outcomes may be estimated. We discuss extensions to estimating linear projections of the heterogeneous treatment effect function and explain how propensity score models with more general link functions may be handled within our framework. An R package dipw implementing our methodology is available on CRAN.},
  archive      = {J_AOS},
  author       = {Yuhao Wang and Rajen D. Shah},
  doi          = {10.1214/24-AOS2409},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {1978-2003},
  shortjournal = {Ann. Statist.},
  title        = {Debiased inverse propensity score weighting for estimation of average treatment effects with high-dimensional confounders},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved covariance estimation: Optimal robustness and
sub-gaussian guarantees under heavy tails. <em>AOS</em>, <em>52</em>(5),
1953–1977. (<a href="https://doi.org/10.1214/24-AOS2407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an estimator of the covariance matrix Σ of random d-dimensional vector from an i.i.d. sample of size n. Our sole assumption is that this vector satisfies a bounded Lp−L2 moment assumption over its one-dimensional marginals, for some p≥4. Given this, we show that Σ can be estimated from the sample with the same high-probability error rates that the sample covariance matrix achieves in the case of Gaussian data. This holds even though we allow for very general distributions that may not have moments of order &gt; p. Moreover, our estimator can be made to be optimally robust to adversarial contamination. This result improves the recent contributions by Mendelson and Zhivotovskiy and Catoni and Giulini, and matches parallel work by Abdalla and Zhivotovskiy (the exact relationship with this last work is described in the paper).},
  archive      = {J_AOS},
  author       = {Roberto I. Oliveira and Zoraida F. Rico},
  doi          = {10.1214/24-AOS2407},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {1953-1977},
  shortjournal = {Ann. Statist.},
  title        = {Improved covariance estimation: Optimal robustness and sub-gaussian guarantees under heavy tails},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal policy evaluation using kernel-based temporal
difference methods. <em>AOS</em>, <em>52</em>(5), 1927–1952. (<a
href="https://doi.org/10.1214/24-AOS2399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study nonparametric methods for estimating the value function of an infinite-horizon discounted Markov reward process (MRP). We analyze the kernel-based least-squares temporal difference (LSTD) estimate, which can be understood either as a nonparametric instrumental variables method, or as a projected approximation to the Bellman fixed point equation. Our analysis imposes no assumptions on the transition operator of the Markov chain, but rather only conditions on the reward function and population-level kernel LSTD solutions. Using empirical process theory and concentration inequalities, we establish a nonasymptotic upper bound on the error with explicit dependence on the effective horizon H=(1−γ)−1 of the Markov reward process, the eigenvalues of the associated kernel operator, as well as the instance-dependent variance of the Bellman residual error. In addition, we prove minimax lower bounds over subclasses of MRPs, which shows that our guarantees are optimal in terms of the sample size n and the effective horizon H. Whereas existing worst-case theory predicts cubic scaling (H3) in the effective horizon, our theory reveals a much wider range of scalings, depending on the kernel, the stationary distribution, and the variance of the Bellman residual error. Notably, it is only parametric and near-parametric problems that can ever achieve the worst-case cubic scaling.},
  archive      = {J_AOS},
  author       = {Yaqi Duan and Mengdi Wang and Martin J. Wainwright},
  doi          = {10.1214/24-AOS2399},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {1927-1952},
  shortjournal = {Ann. Statist.},
  title        = {Optimal policy evaluation using kernel-based temporal difference methods},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint sequential detection and isolation for dependent data
streams. <em>AOS</em>, <em>52</em>(5), 1899–1926. (<a
href="https://doi.org/10.1214/24-AOS2385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of joint sequential detection and isolation is considered in the context of multiple, not necessarily independent, data streams. A multiple testing framework is proposed, where each hypothesis corresponds to a different subset of data streams, the sample size is a stopping time of the observations, and the probabilities of four kinds of error are controlled below distinct, user-specified levels. Two of these errors reflect the detection component of the formulation, whereas the other two the isolation component. The optimal expected sample size is characterized to a first-order asymptotic approximation as the error probabilities go to 0. Different asymptotic regimes, expressing different prioritizations of the detection and isolation tasks, are considered. A novel, versatile family of testing procedures is proposed, in which two distinct, in general, statistics are computed for each hypothesis, one addressing the detection task and the other the isolation task. Tests in this family, of various computational complexities, are shown to be asymptotically optimal under different setups. The general theory is applied to the detection and isolation of anomalous, not necessarily independent, data streams, as well as to the detection and isolation of an unknown dependence structure.},
  archive      = {J_AOS},
  author       = {Anamitra Chaudhuri and Georgios Fellouris},
  doi          = {10.1214/24-AOS2385},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {1899-1926},
  shortjournal = {Ann. Statist.},
  title        = {Joint sequential detection and isolation for dependent data streams},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral statistics of sample block correlation matrices.
<em>AOS</em>, <em>52</em>(5), 1873–1898. (<a
href="https://doi.org/10.1214/24-AOS2375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental concept in multivariate statistics, the sample correlation matrix, is often used to infer the correlation/dependence structure among random variables, when the population mean and covariance are unknown. A natural block extension of it, the sample block correlation matrix, is proposed to take on the same role, when random variables are generalized to random subvectors. In this paper, we establish a spectral theory of the sample block correlation matrices and apply it to group independent tests and related problems, under the high-dimensional setting. More specifically, we consider a random vector of dimension p, consisting of k subvectors of dimension pt’s, where pt’s can vary from 1 to order p. Our primary goal is to investigate the dependence of the k subvectors. We construct a random matrix model called sample block correlation matrix based on N samples for this purpose. The spectral statistics of the sample block correlation matrix include the classical Wilks’ statistic and Schott’s statistic as special cases. It turns out that the spectral statistics do not depend on the unknown population mean and covariance, under the null hypothesis that the subvectors are independent. Further, the limiting behavior of the spectral statistics can be described with the aid of the free probability theory. Specifically, under three different settings of possibly N-dependent k and pt’s, we show that the empirical spectral distribution of the sample block correlation matrix converges to the free Poisson binomial distribution, free Poisson distribution (Marchenko–Pastur law) and free Gaussian distribution (semicircle law), respectively. We then further derive the CLTs for the linear spectral statistics of the block correlation matrix under a general setting. Our results are established under the general distribution assumption on the random vector. It turns out that the CLTs are universal and do not depend on the 4th cumulants of the vector components, due to a self-normalizing effect of the correlation-type matrices. We further derive the CLT under the alternative hypothesis and discuss the power of our statistics. Based on our theory, real data analysis on stock return data and gene data is also conducted.},
  archive      = {J_AOS},
  author       = {Zhigang Bao and Jiang Hu and Xiaocong Xu and Xiaozhuo Zhang},
  doi          = {10.1214/24-AOS2375},
  journal      = {The Annals of Statistics},
  month        = {10},
  number       = {5},
  pages        = {1873-1898},
  shortjournal = {Ann. Statist.},
  title        = {Spectral statistics of sample block correlation matrices},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How do noise tails impact on deep ReLU networks?
<em>AOS</em>, <em>52</em>(4), 1845–1871. (<a
href="https://doi.org/10.1214/24-AOS2428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the stability of deep ReLU neural networks for nonparametric regression under the assumption that the noise has only a finite pth moment. We unveil how the optimal rate of convergence depends on p, the degree of smoothness and the intrinsic dimension in a class of nonparametric regression functions with hierarchical composition structure when both the adaptive Huber loss and deep ReLU neural networks are used. This optimal rate of convergence cannot be obtained by the ordinary least squares but can be achieved by the Huber loss with a properly chosen parameter that adapts to the sample size, smoothness, and moment parameters. A concentration inequality for the adaptive Huber ReLU neural network estimators with allowable optimization errors is also derived. To establish a matching lower bound within the class of neural network estimators using the Huber loss, we employ a different strategy from the traditional route: constructing a deep ReLU network estimator that has a better empirical loss than the true function and the difference between these two functions furnishes a low bound. This step is related to the Huberization bias, yet more critically to the approximability of deep ReLU networks. As a result, we also contribute some new results on the approximation theory of deep ReLU neural networks.},
  archive      = {J_AOS},
  author       = {Jianqing Fan and Yihong Gu and Wen-Xin Zhou},
  doi          = {10.1214/24-AOS2428},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1845-1871},
  shortjournal = {Ann. Statist.},
  title        = {How do noise tails impact on deep ReLU networks?},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On posterior consistency of data assimilation with gaussian
process priors: The 2D-navier–stokes equations. <em>AOS</em>,
<em>52</em>(4), 1825–1844. (<a
href="https://doi.org/10.1214/24-AOS2427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a nonlinear Bayesian data assimilation model for the periodic two-dimensional Navier–Stokes equations with initial condition modelled by a Gaussian process prior. We show that if the system is updated with sufficiently many discrete noisy measurements of the velocity field, then the posterior distribution eventually concentrates near the ground truth solution of the time evolution equation, and in particular that the initial condition is recovered consistently by the posterior mean vector field. We further show that the convergence rate can in general not be faster than inverse logarithmic in sample size, but describe specific conditions on the initial conditions when faster rates are possible. In the proofs, we provide an explicit quantitative estimate for backward uniqueness of solutions of the two-dimensional Navier–Stokes equations.},
  archive      = {J_AOS},
  author       = {Richard Nickl and Edriss S. Titi},
  doi          = {10.1214/24-AOS2427},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1825-1844},
  shortjournal = {Ann. Statist.},
  title        = {On posterior consistency of data assimilation with gaussian process priors: The 2D-Navier–Stokes equations},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and multiply robust risk estimation under general
forms of dataset shift. <em>AOS</em>, <em>52</em>(4), 1796–1824. (<a
href="https://doi.org/10.1214/24-AOS2422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such dataset shift conditions are known as domain adaptation or transfer learning. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population. In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions—covariate, label and concept shift—as special cases. We allow for partially nonoverlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.},
  archive      = {J_AOS},
  author       = {Hongxiang Qiu and Eric Tchetgen Tchetgen and Edgar Dobriban},
  doi          = {10.1214/24-AOS2422},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1796-1824},
  shortjournal = {Ann. Statist.},
  title        = {Efficient and multiply robust risk estimation under general forms of dataset shift},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning gaussian mixtures using the wasserstein–fisher–rao
gradient flow. <em>AOS</em>, <em>52</em>(4), 1774–1795. (<a
href="https://doi.org/10.1214/24-AOS2416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian mixture models form a flexible and expressive parametric family of distributions that has found a variety of applications. Unfortunately, fitting these models to data is a notoriously hard problem from a computational perspective. Currently, only moment-based methods enjoy theoretical guarantees while likelihood-based methods are dominated by heuristics such as Expectation-Maximization that are known to fail in simple examples. In this work, we propose a new algorithm to compute the nonparametric maximum likelihood estimator (NPMLE) in a Gaussian mixture model. Our method is based on gradient descent over the space of probability measures equipped with the Wasserstein–Fisher–Rao geometry for which we establish convergence guarantees. In practice, it can be approximated using an interacting particle system where the weight and location of particles are updated alternately. We conduct extensive numerical experiments to confirm the effectiveness of the proposed algorithm compared not only to classical benchmarks but also to similar gradient descent algorithms with respect to simpler geometries. In particular, these simulations illustrate the benefit of updating both weight and location of the interacting particles.},
  archive      = {J_AOS},
  author       = {Yuling Yan and Kaizheng Wang and Philippe Rigollet},
  doi          = {10.1214/24-AOS2416},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1774-1795},
  shortjournal = {Ann. Statist.},
  title        = {Learning gaussian mixtures using the Wasserstein–Fisher–Rao gradient flow},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient functional lasso kernel smoothing for
high-dimensional additive regression. <em>AOS</em>, <em>52</em>(4),
1741–1773. (<a href="https://doi.org/10.1214/24-AOS2415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smooth backfitting has been proposed and proved as a powerful nonparametric estimation technique for additive regression models in various settings. Existing studies are restricted to cases with a moderate number of covariates and are not directly applicable to high dimensional settings. In this paper, we develop new kernel estimators based on the idea of smooth backfitting for high dimensional additive models. We introduce a novel penalization scheme, combining the idea of functional Lasso with the smooth backfitting technique. We investigate the theoretical properties of the functional Lasso smooth backfitting estimation. For the implementation of the proposed method, we devise a simple iterative algorithm where the iteration is defined by a truncated projection operator. The algorithm has only an additional thresholding operator over the projection-based iteration of the smooth backfitting algorithm. We further present a debiased version of the proposed estimator with implementation details, and investigate its theoretical properties for statistical inference. We demonstrate the finite sample performance of the methods via simulation and real data analysis.},
  archive      = {J_AOS},
  author       = {Eun Ryung Lee and Seyoung Park and Enno Mammen and Byeong U. Park},
  doi          = {10.1214/24-AOS2415},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1741-1773},
  shortjournal = {Ann. Statist.},
  title        = {Efficient functional lasso kernel smoothing for high-dimensional additive regression},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Detection and estimation of structural breaks in
high-dimensional functional time series. <em>AOS</em>, <em>52</em>(4),
1716–1740. (<a href="https://doi.org/10.1214/24-AOS2414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider detecting and estimating breaks in heterogenous mean functions of high-dimensional functional time series which are allowed to be cross-sectionally correlated. A new test statistic combining the functional CUSUM statistic and power enhancement component is proposed with asymptotic null distribution comparable to the conventional CUSUM theory derived for a single functional time series. In particular, the extra power enhancement component enlarges the region where the proposed test has power, and results in stable power performance when breaks are sparse in the alternative hypothesis. Furthermore, we impose a latent group structure on the subjects with heterogenous break points and introduce an easy-to-implement clustering algorithm with an information criterion to consistently estimate the unknown group number and membership. The estimated group structure improves the convergence property of the break point estimate. Monte Carlo simulation studies and empirical applications show that the proposed estimation and testing techniques have satisfactory performance in finite samples.},
  archive      = {J_AOS},
  author       = {Degui Li and Runze Li and Han Lin Shang},
  doi          = {10.1214/24-AOS2414},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1716-1740},
  shortjournal = {Ann. Statist.},
  title        = {Detection and estimation of structural breaks in high-dimensional functional time series},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Wasserstein convergence in bayesian and frequentist
deconvolution models. <em>AOS</em>, <em>52</em>(4), 1691–1715. (<a
href="https://doi.org/10.1214/24-AOS2413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the multivariate deconvolution problem of recovering the distribution of a signal from independent and identically distributed observations additively contaminated with random errors having known distribution. For errors with ordinary smooth distribution, we recast the multidimensional problem as a one-dimensional problem leveraging the equivalence between the L1-Wasserstein and the max-sliced L1-Wasserstein metrics and derive an inversion inequality relating the L1-Wasserstein distance between two distributions of the signal to the L1-distance between the corresponding mixture densities of the observations. This smoothing inequality outperforms existing inversion inequalities. We apply it to derive L1-Wasserstein rates of convergence for the distribution of the signal. As an application to the Bayesian framework, we consider L1-Wasserstein deconvolution with the Laplace noise in dimension one using a Dirichlet process mixture of normal densities as a prior measure for the mixing distribution. We construct an adaptive approximation of the sampling density by convolving the Laplace density with a well-chosen mixture of Gaussian densities and show that the posterior measure contracts at a nearly minimax-optimal rate, up to a log-factor, in the L1-distance. The rate automatically adapts to the unknown Sobolev regularity of the mixing density, thus leading to a new Bayesian adaptive estimation procedure over the full scale of regularity levels. We illustrate the utility of the inversion inequality also in a frequentist setting by showing that a minimum distance estimator attains the minimax convergence rates for L1-Wasserstein deconvolution in any dimension d≥1, lower bounds being derived here.},
  archive      = {J_AOS},
  author       = {Judith Rousseau and Catia Scricciolo},
  doi          = {10.1214/24-AOS2413},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1691-1715},
  shortjournal = {Ann. Statist.},
  title        = {Wasserstein convergence in bayesian and frequentist deconvolution models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Majority vote for distributed differentially private sign
selection. <em>AOS</em>, <em>52</em>(4), 1671–1690. (<a
href="https://doi.org/10.1214/24-AOS2411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving data analysis has become more prevalent in recent years. In this study, we propose a distributed group differentially private Majority Vote mechanism, for the sign selection problem in a distributed setup. To achieve this, we apply the iterative peeling to the stability function and use the exponential mechanism to recover the signs. For enhanced applicability, we study the private sign selection for mean estimation and linear regression problems, in distributed systems. Our method recovers the support and signs with the optimal signal-to-noise ratio as in the nonprivate scenario, which is better than contemporary works of private variable selections. Moreover, the sign selection consistency is justified by theoretical guarantees. Simulation studies are conducted to demonstrate the effectiveness of the proposed method.},
  archive      = {J_AOS},
  author       = {Weidong Liu and Jiyuan Tu and Xiaojun Mao and Xi Chen},
  doi          = {10.1214/24-AOS2411},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1671-1690},
  shortjournal = {Ann. Statist.},
  title        = {Majority vote for distributed differentially private sign selection},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online change-point detection for matrix-valued time series
with latent two-way factor structure. <em>AOS</em>, <em>52</em>(4),
1646–1670. (<a href="https://doi.org/10.1214/24-AOS2410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel methodology for the online detection of changepoints in the factor structure of large matrix time series. Our approach is based on the well-known fact that, in the presence of a changepoint, the number of spiked eigenvalues in the second moment matrix of the data increases (e.g., in the presence of a change in the loadings, or if a new factor emerges). Based on this, we propose two families of procedures—one based on the fluctuations of partial sums, and one based on extreme value theory—to monitor whether the first nonspiked eigenvalue diverges after a point in time in the monitoring horizon, thereby indicating the presence of a changepoint. Our procedure is based only on rates; at each point in time, we randomise the estimated eigenvalue, thus obtaining a normally distributed sequence which is i.i.d. with mean zero under the null of no break, whereas it diverges to positive infinity in the presence of a changepoint. We base our monitoring procedures on such sequence. Extensive simulation studies and empirical analysis justify the theory. An R package implementing the procedure is available on CRAN. (https://cran.r-project.org/web/packages/OLCPM/index.html.)},
  archive      = {J_AOS},
  author       = {Yong He and Xinbing Kong and Lorenzo Trapani and Long Yu},
  doi          = {10.1214/24-AOS2410},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1646-1670},
  shortjournal = {Ann. Statist.},
  title        = {Online change-point detection for matrix-valued time series with latent two-way factor structure},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gromov–wasserstein distances: Entropic regularization,
duality and sample complexity. <em>AOS</em>, <em>52</em>(4), 1616–1645.
(<a href="https://doi.org/10.1214/24-AOS2406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gromov–Wasserstein (GW) distance, rooted in optimal transport (OT) theory, quantifies dissimilarity between metric measure spaces and provides a framework for aligning heterogeneous datasets. While computational aspects of the GW problem have been widely studied, a duality theory and fundamental statistical questions concerning empirical convergence rates remained obscure. This work closes these gaps for the quadratic GW distance over Euclidean spaces of different dimensions dx and dy. We treat both the standard and the entropically regularized GW distance, and derive dual forms that represent them in terms of the well-understood OT and entropic OT (EOT) problems, respectively. This enables employing proof techniques from statistical OT based on regularity analysis of dual potentials and empirical process theory, using which we establish the first GW empirical convergence rates. The derived two-sample rates are n−2/max{min{dx,dy},4} (up to a log factor when min{dx,dy}=4) for standard GW and n−1/2 for entropic GW (EGW), which matches the corresponding rates for standard and entropic OT. The parametric rate for EGW is evidently optimal, while for standard GW we provide matching lower bounds, which establish sharpness of the derived rates. We also study stability of EGW in the entropic regularization parameter and prove approximation and continuity results for the cost and optimal couplings. Lastly, the duality is leveraged to shed new light on the open problem of the one-dimensional GW distance between uniform distributions on n points, illuminating why the identity and anti-identity permutations may not be optimal. Our results serve as a first step towards a comprehensive statistical theory as well as computational advancements for GW distances, based on the discovered dual formulations.},
  archive      = {J_AOS},
  author       = {Zhengxin Zhang and Ziv Goldfeld and Youssef Mroueh and Bharath K. Sriperumbudur},
  doi          = {10.1214/24-AOS2406},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1616-1645},
  shortjournal = {Ann. Statist.},
  title        = {Gromov–Wasserstein distances: Entropic regularization, duality and sample complexity},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric doubly robust test for a continuous
treatment effect. <em>AOS</em>, <em>52</em>(4), 1592–1615. (<a
href="https://doi.org/10.1214/24-AOS2405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vast majority of literature on evaluating the significance of a treatment effect based on observational data has been confined to discrete treatments. These methods are not applicable to drawing inference for a continuous treatment, which arises in many important applications. To adjust for confounders when evaluating a continuous treatment, existing inference methods often rely on discretizing the treatment or using (possibly misspecified) parametric models for the effect curve. Recently, Kennedy et al. (J. R. Stat. Soc. Ser. B. Stat. Methodol. 79 (2017) 1229–1245) proposed nonparametric doubly robust estimation for a continuous treatment effect in observational studies. However, inference for the continuous treatment effect is a harder problem. To the best of our knowledge, a completely nonparametric doubly robust approach for inference in this setting is not yet available. We develop such a nonparametric doubly robust procedure in this paper for making inference on the continuous treatment effect curve. Using empirical process techniques for local U- and V-processes, we establish the test statistic’s asymptotic distribution. Furthermore, we propose a wild bootstrap procedure for implementing the test in practice. In addition, we define a version of the test procedure based on sample splitting. We illustrate the new method(s) via simulations and a study of a constructed dataset relating the effect of nurse staffing hours on hospital performance. We implement our doubly robust dose response test in the R package DRDRtest on CRAN.},
  archive      = {J_AOS},
  author       = {Charles R. Doss and Guangwei Weng and Lan Wang and Ira Moscovice and Tongtan Chantarat},
  doi          = {10.1214/24-AOS2405},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1592-1615},
  shortjournal = {Ann. Statist.},
  title        = {A nonparametric doubly robust test for a continuous treatment effect},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharp multiple testing boundary for sparse sequences.
<em>AOS</em>, <em>52</em>(4), 1564–1591. (<a
href="https://doi.org/10.1214/24-AOS2404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates multiple testing by considering minimax separation rates in the sparse sequence model, when the testing risk is measured as the sum FDR+FNR (False Discovery Rate plus False Negative Rate). First, using the popular beta-min separation condition, with all nonzero signals separated from 0 by at least some amount, we determine the sharp minimax testing risk asymptotically and thereby explicitly describe the transition from “achievable multiple testing with vanishing risk” to “impossible multiple testing.” Adaptive multiple testing procedures achieving the corresponding optimal boundary are provided: the Benjamini–Hochberg procedure with a properly tuned level, and an empirical Bayes ℓ-value (‘local FDR’) procedure. We prove that the FDR and FNR make nonsymmetric contributions to the testing risk for most optimal procedures, the FNR part being dominant at the boundary. The multiple testing hardness is then investigated for classes of arbitrary sparse signals. A number of extensions, including results for classification losses and convergence rates in the case of large signals, are also investigated.},
  archive      = {J_AOS},
  author       = {Kweku Abraham and Ismaël Castillo and Étienne Roquain},
  doi          = {10.1214/24-AOS2404},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1564-1591},
  shortjournal = {Ann. Statist.},
  title        = {Sharp multiple testing boundary for sparse sequences},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). One-step estimation of differentiable hilbert-valued
parameters. <em>AOS</em>, <em>52</em>(4), 1534–1563. (<a
href="https://doi.org/10.1214/24-AOS2403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present estimators for smooth Hilbert-valued parameters, where smoothness is characterized by a pathwise differentiability condition. When the parameter space is a reproducing kernel Hilbert space, we provide a means to obtain efficient, root-n rate estimators and corresponding confidence sets. These estimators correspond to generalizations of cross-fitted one-step estimators based on Hilbert-valued efficient influence functions. We give theoretical guarantees even when arbitrary estimators of nuisance functions are used, including those based on machine learning techniques. We show that these results naturally extend to Hilbert spaces that lack a reproducing kernel, as long as the parameter has an efficient influence function. However, we also uncover the unfortunate fact that, when there is no reproducing kernel, many interesting parameters fail to have an efficient influence function, even though they are pathwise differentiable. To handle these cases, we propose a regularized one-step estimator and associated confidence sets. We also show that pathwise differentiability, which is a central requirement of our approach, holds in many cases. Specifically, we provide multiple examples of pathwise differentiable parameters and develop corresponding estimators and confidence sets. Among these examples, four are particularly relevant to ongoing research by the causal inference community: the counterfactual density function, dose-response function, conditional average treatment effect function, and counterfactual kernel mean embedding.},
  archive      = {J_AOS},
  author       = {Alex Luedtke and Incheoul Chung},
  doi          = {10.1214/24-AOS2403},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1534-1563},
  shortjournal = {Ann. Statist.},
  title        = {One-step estimation of differentiable hilbert-valued parameters},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bootstrap-assisted inference for generalized grenander-type
estimators. <em>AOS</em>, <em>52</em>(4), 1509–1533. (<a
href="https://doi.org/10.1214/24-AOS2402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Westling and Carone (Ann. Statist. 48 (2020) 1001–1024) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made “flatness robust” in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.},
  archive      = {J_AOS},
  author       = {Matias D. Cattaneo and Michael Jansson and Kenichi Nagasawa},
  doi          = {10.1214/24-AOS2402},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1509-1533},
  shortjournal = {Ann. Statist.},
  title        = {Bootstrap-assisted inference for generalized grenander-type estimators},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotic normality and optimality in nonsmooth stochastic
approximation. <em>AOS</em>, <em>52</em>(4), 1485–1508. (<a
href="https://doi.org/10.1214/24-AOS2401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In their seminal work, Polyak and Juditsky showed that stochastic approximation algorithms for solving smooth equations enjoy a central limit theorem. Moreover, it has since been argued that the asymptotic covariance of the method is best possible among any estimation procedure in a local minimax sense of Hájek and Le Cam. A long-standing open question in this line of work is whether similar guarantees hold for important nonsmooth problems, such as stochastic nonlinear programming or stochastic variational inequalities. In this work, we show that this is indeed the case.},
  archive      = {J_AOS},
  author       = {Damek Davis and Dmitriy Drusvyatskiy and Liwei Jiang},
  doi          = {10.1214/24-AOS2401},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1485-1508},
  shortjournal = {Ann. Statist.},
  title        = {Asymptotic normality and optimality in nonsmooth stochastic approximation},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fundamental limits of low-rank matrix estimation with
diverging aspect ratios. <em>AOS</em>, <em>52</em>(4), 1460–1484. (<a
href="https://doi.org/10.1214/24-AOS2400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of estimating the factors of a low-rank n×d matrix, when this is corrupted by additive Gaussian noise. A special example of our setting corresponds to clustering mixtures of Gaussians with equal (known) covariances. Simple spectral methods do not take into account the distribution of the entries of these factors and are therefore often suboptimal. Here, we characterize the asymptotics of the minimum estimation error under the assumption that the distribution of the entries is known to the statistician. Our results apply to the high-dimensional regime n,d→∞ and d/n→∞ (or d/n→0) and generalize earlier work that focused on the proportional asymptotics n,d→∞, d/n→δ∈(0,∞). We outline an interesting signal strength regime in which d/n→∞ and partial recovery is possible for the left singular vectors while impossible for the right singular vectors. We illustrate the general theory by deriving consequences for Gaussian mixture clustering and carrying out a numerical study on genomics data.},
  archive      = {J_AOS},
  author       = {Andrea Montanari and Yuchen Wu},
  doi          = {10.1214/24-AOS2400},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1460-1484},
  shortjournal = {Ann. Statist.},
  title        = {Fundamental limits of low-rank matrix estimation with diverging aspect ratios},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Heavy-tailed bayesian nonparametric adaptation.
<em>AOS</em>, <em>52</em>(4), 1433–1459. (<a
href="https://doi.org/10.1214/24-AOS2397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new Bayesian strategy for adaptation to smoothness in nonparametric models based on heavy-tailed series priors. We illustrate it in a variety of settings, showing in particular that the corresponding Bayesian posterior distributions achieve adaptive rates of contraction in the minimax sense (up to logarithmic factors) without the need to sample hyperparameters. Unlike many existing procedures, where a form of direct model (or estimator) selection is performed, the method can be seen as performing a soft selection through the prior tail. In Gaussian regression, such heavy-tailed priors are shown to lead to (near-)optimal simultaneous adaptation both in the L2- and L∞-sense. Results are also derived for linear inverse problems, for anisotropic Besov classes, and for certain losses in more general models through the use of tempered posterior distributions. We present numerical simulations corroborating the theory.},
  archive      = {J_AOS},
  author       = {Sergios Agapiou and Ismaël Castillo},
  doi          = {10.1214/24-AOS2397},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1433-1459},
  shortjournal = {Ann. Statist.},
  title        = {Heavy-tailed bayesian nonparametric adaptation},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). E-statistics, group invariance and anytime-valid testing.
<em>AOS</em>, <em>52</em>(4), 1410–1432. (<a
href="https://doi.org/10.1214/24-AOS2394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study worst-case-growth-rate-optimal (GROW) e-statistics for hypothesis testing between two group models. It is known that under a mild condition on the action of the underlying group G on the data, there exists a maximally invariant statistic. We show that among all e-statistics, invariant or not, the likelihood ratio of the maximally invariant statistic is GROW, both in the absolute and in the relative sense, and that an anytime-valid test can be based on it. The GROW e-statistic is equal to a Bayes factor with a right Haar prior on G. Our treatment avoids nonuniqueness issues that sometimes arise for such priors in Bayesian contexts. A crucial assumption on the group G is its amenability, a well-known group-theoretical condition, which holds, for instance, in scale-location families. Our results also apply to finite-dimensional linear regression.},
  archive      = {J_AOS},
  author       = {Muriel Felipe Pérez-Ortiz and Tyron Lardy and Rianne de Heide and Peter D. Grünwald},
  doi          = {10.1214/24-AOS2394},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1410-1432},
  shortjournal = {Ann. Statist.},
  title        = {E-statistics, group invariance and anytime-valid testing},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the approximation accuracy of gaussian variational
inference. <em>AOS</em>, <em>52</em>(4), 1384–1409. (<a
href="https://doi.org/10.1214/24-AOS2393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main computational challenge in Bayesian inference is to compute integrals against a high-dimensional posterior distribution. In the past decades, variational inference (VI) has emerged as a tractable approximation to these integrals, and a viable alternative to the more established paradigm of Markov chain Monte Carlo. However, little is known about the approximation accuracy of VI. In this work, we bound the TV error and the mean and covariance approximation error of Gaussian VI in terms of dimension and sample size. Our error analysis relies on a Hermite series expansion of the log posterior whose first terms are precisely cancelled out by the first order optimality conditions associated to the Gaussian VI optimization problem.},
  archive      = {J_AOS},
  author       = {Anya Katsevich and Philippe Rigollet},
  doi          = {10.1214/24-AOS2393},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1384-1409},
  shortjournal = {Ann. Statist.},
  title        = {On the approximation accuracy of gaussian variational inference},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Higher-order coverage errors of batching methods via
edgeworth expansions on t-statistics. <em>AOS</em>, <em>52</em>(4),
1360–1383. (<a href="https://doi.org/10.1214/24-AOS2377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While batching methods have been widely used in simulation and statistics, their higher-order coverage behaviors and relative advantages in this regard remain open. We develop techniques to obtain higher-order coverage errors for batching methods by building Edgeworth-type expansions on t-statistics. The coefficients in these expansions are intricate analytically, but we provide algorithms to estimate the coefficients of the n−1 error terms via Monte Carlo simulation. We provide insights on the effect of the number of batches on the coverage error, where we demonstrate generally nonmonotonic relations. We also compare different batching methods both theoretically and numerically, and argue that none of the methods is uniformly better than others in terms of coverage. However, when the number of batches is large, sectioned jackknife has the best coverage among all.},
  archive      = {J_AOS},
  author       = {Shengyi He and Henry Lam},
  doi          = {10.1214/24-AOS2377},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1360-1383},
  shortjournal = {Ann. Statist.},
  title        = {Higher-order coverage errors of batching methods via edgeworth expansions on t-statistics},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal estimation of schatten norms of a rectangular
matrix. <em>AOS</em>, <em>52</em>(4), 1334–1359. (<a
href="https://doi.org/10.1214/24-AOS2374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the twin problems of estimating the effective rank and the Schatten norms ‖A‖s of a rectangular p×q matrix A from noisy observations. When s is an even integer, we introduce a polynomial-time estimator of ‖A‖s that achieves the minimax rate (pq)1/4. Interestingly, this optimal rate does not depend on the underlying rank of the matrix A. When s is not an even integer, the optimal rate is much slower. A simple thresholding estimator of the singular values achieves the rate (q∧p)(pq)1/4, which turns out to be optimal up to a logarithmic multiplicative term. The tight minimax rate is achieved by a more involved polynomial approximation method. This allows us to build estimators for a class of effective rank indices. As a byproduct, we also characterize the minimax rate for estimating the sequence of singular values of a matrix.},
  archive      = {J_AOS},
  author       = {Solène Thépaut and Nicolas Verzelen},
  doi          = {10.1214/24-AOS2374},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1334-1359},
  shortjournal = {Ann. Statist.},
  title        = {Optimal estimation of schatten norms of a rectangular matrix},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal parameter estimation for linear SPDEs from multiple
measurements. <em>AOS</em>, <em>52</em>(4), 1307–1333. (<a
href="https://doi.org/10.1214/24-AOS2364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The coefficients in a second order parabolic linear stochastic partial differential equation (SPDE) are estimated from multiple spatially localised measurements. Assuming that the spatial resolution tends to zero and the number of measurements is nondecreasing, the rate of convergence for each coefficient depends on its differential order and is faster for higher order coefficients. Based on an explicit analysis of the reproducing kernel Hilbert space of a general stochastic evolution equation, a Gaussian lower bound scheme is introduced. As a result, minimax optimality of the rates as well as sufficient and necessary conditions for consistent estimation are established.},
  archive      = {J_AOS},
  author       = {Randolf Altmeyer and Anton Tiepner and Martin Wahl},
  doi          = {10.1214/24-AOS2364},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1307-1333},
  shortjournal = {Ann. Statist.},
  title        = {Optimal parameter estimation for linear SPDEs from multiple measurements},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical inference for rough volatility: Minimax theory.
<em>AOS</em>, <em>52</em>(4), 1277–1306. (<a
href="https://doi.org/10.1214/23-AOS2343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, rough volatility models have gained considerable attention in quantitative finance. In this paradigm, the stochastic volatility of the price of an asset has quantitative properties similar to that of a fractional Brownian motion with small Hurst index H&lt;1/2. In this work, we provide the first rigorous statistical analysis of the problem of estimating H from historical observations of the underlying asset. We establish minimax lower bounds and design optimal procedures based on adaptive estimation of quadratic functionals based on wavelets. We prove in particular that the optimal rate of convergence for estimating H based on price observations at n time points is of order n−1/(4H+2) as n grows to infinity, extending results that were known only for H&gt;1/2. Our study positively answers the question whether H can be inferred, although it is the regularity of a latent process (the volatility); in rough models, when H is close to 0, we even obtain an accuracy comparable to usual n-consistent regular statistical models.},
  archive      = {J_AOS},
  author       = {Carsten H. Chong and Marc Hoffmann and Yanghui Liu and Mathieu Rosenbaum and Grégoire Szymansky},
  doi          = {10.1214/23-AOS2343},
  journal      = {The Annals of Statistics},
  month        = {8},
  number       = {4},
  pages        = {1277-1306},
  shortjournal = {Ann. Statist.},
  title        = {Statistical inference for rough volatility: Minimax theory},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral analysis of gram matrices with missing at random
observations: Convergence, central limit theorems, and applications in
statistical inference. <em>AOS</em>, <em>52</em>(3), 1254–1275. (<a
href="https://doi.org/10.1214/24-AOS2392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the statistical inference using the Gram matrix in the context of missing at random observations, this paper investigates the spectral properties of the random matrices Sn=1nZZ∗, where Z=D∘(Σ1/2X) represents a Hadamard random matrix with entries determined by independent Bernoulli variables D. Operating within the high-dimensional framework, we establish the convergence of the empirical spectral distribution of Sn to a well-defined limiting distribution. In addition, we explore the impact of the missing mechanism on the second-order properties of the spectral distribution of the Gram matrix Sn. We establish the central limit theorem for the linear spectral statistics of Sn, shedding light on their fluctuations. Surprisingly, our analysis reveals that even in the ideal Gaussian distribution scenario, the fluctuations of statistics generated by eigenvalues are influenced by the eigenvectors of the population covariance matrix in the missing-at-random case. This discovery uncovers a remarkable phenomenon that starkly contrasts with the classical case. Subsequently, we demonstrate the practical application of our central limit theorem in hypothesis testing for the population covariance matrix.},
  archive      = {J_AOS},
  author       = {Huiqin Li and Guangming Pan and Yanqing Yin and Wang Zhou},
  doi          = {10.1214/24-AOS2392},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1254-1275},
  shortjournal = {Ann. Statist.},
  title        = {Spectral analysis of gram matrices with missing at random observations: Convergence, central limit theorems, and applications in statistical inference},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Locally simultaneous inference. <em>AOS</em>,
<em>52</em>(3), 1227–1253. (<a
href="https://doi.org/10.1214/24-AOS2391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selective inference is the problem of giving valid answers to statistical questions chosen in a data-driven manner. A standard solution to selective inference is simultaneous inference, which delivers valid answers to the set of all questions that could possibly have been asked. However, simultaneous inference can be unnecessarily conservative if this set includes many questions that were unlikely to be asked in the first place. We introduce a less conservative solution to selective inference that we call locally simultaneous inference, which only answers those questions that could plausibly have been asked in light of the observed data, all the while preserving rigorous type I error guarantees. For example, if the objective is to construct a confidence interval for the “winning” treatment effect in a clinical trial with multiple treatments, and it is obvious in hindsight that only one treatment had a chance to win, then our approach will return an interval that is nearly the same as the uncorrected, standard interval. Locally simultaneous inference is implemented by refining any method for simultaneous inference of interest. Under mild conditions satisfied by common confidence intervals, locally simultaneous inference strictly dominates its underlying simultaneous inference method, meaning it can never yield less statistical power but only more. Compared to conditional selective inference, which demands stronger guarantees, locally simultaneous inference is more easily applicable in nonparametric settings and is more numerically stable.},
  archive      = {J_AOS},
  author       = {Tijana Zrnic and William Fithian},
  doi          = {10.1214/24-AOS2391},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1227-1253},
  shortjournal = {Ann. Statist.},
  title        = {Locally simultaneous inference},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deep nonlinear sufficient dimension reduction. <em>AOS</em>,
<em>52</em>(3), 1201–1226. (<a
href="https://doi.org/10.1214/24-AOS2390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear sufficient dimension reduction, as exemplified by sliced inverse regression, has seen substantial development in the past thirty years. However, with the advent of more complex scenarios, nonlinear dimension reduction has gained considerable interest recently. This paper introduces a novel method for nonlinear sufficient dimension reduction, utilizing the generalized martingale difference divergence measure in conjunction with deep neural networks. The optimal solution of the proposed objective function is shown to be unbiased at the general level of σ-fields. And two optimization schemes, based on the fascinating deep neural networks, exhibit higher efficiency and flexibility compared to the classical eigendecomposition of linear operators. Moreover, we systematically investigate the slow rate and fast rate for the estimation error based on advanced U-process theory. Remarkably, the fast rate almost coincides with the minimax rate of nonparametric regression. The validity of our deep nonlinear sufficient dimension reduction methods is demonstrated through simulations and real data analysis.},
  archive      = {J_AOS},
  author       = {YinFeng Chen and YuLing Jiao and Rui Qiu and Zhou Yu},
  doi          = {10.1214/24-AOS2390},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1201-1226},
  shortjournal = {Ann. Statist.},
  title        = {Deep nonlinear sufficient dimension reduction},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonparametric classification with missing data.
<em>AOS</em>, <em>52</em>(3), 1178–1200. (<a
href="https://doi.org/10.1214/24-AOS2389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new nonparametric framework for classification problems in the presence of missing data. The key aspect of our framework is that the regression function decomposes into an anova-type sum of orthogonal functions, of which some (or even many) may be zero. Working under a general missingness setting, which allows features to be missing not at random, our main goal is to derive the minimax rate for the excess risk in this problem. In addition to the decomposition property, the rate depends on parameters that control the tail behaviour of the marginal feature distributions, the smoothness of the regression function and a margin condition. The ambient data dimension does not appear in the minimax rate, which can therefore be faster than in the classical nonparametric setting. We further propose a new method, called the Hard-thresholding Anova Missing data (HAM) classifier, based on a careful combination of a k-nearest neighbour algorithm and a thresholding step. The HAM classifier attains the minimax rate up to polylogarithmic factors and numerical experiments further illustrate its utility.},
  archive      = {J_AOS},
  author       = {Torben Sell and Thomas B. Berrett and Timothy I. Cannings},
  doi          = {10.1214/24-AOS2389},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1178-1200},
  shortjournal = {Ann. Statist.},
  title        = {Nonparametric classification with missing data},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A blockwise empirical likelihood method for time series in
frequency domain inference. <em>AOS</em>, <em>52</em>(3), 1152–1177. (<a
href="https://doi.org/10.1214/24-AOS2388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency domain analysis of time series is often difficult, as periodogram-based statistics involve non-linear averages with complicated variances. Due to the latter, nonparametric approximations from resampling or empirical likelihood (EL) are useful. However, current versions of periodogram-based EL for time series are highly restricted: these are valid only for linear processes and for special parameters (i.e., ratios). For general frequency domain inference with stationary, weakly dependent time series, we develop a spectral EL (SEL) method by combining two previously separate EL frameworks for time series: block-based EL and periodogram-based EL. This hybridization strategy is new and theoretically non-trivial, particularly as existing block-based EL relies on time domain averages that differ substantially from frequency domain counterparts. We formulate SEL statistics for parameters based on spectral estimating functions and periodogram subsamples. Under mild conditions, SEL log-ratio statistics are shown to be well-defined, admitting chi-square limits. Further, we formally establish an effective bootstrap procedure coupled with SEL. As a result, the SEL method can be used for nonparametric, asymptotically correct confidence regions and tests for frequency domain inference without explicit estimation of intricate variances of periodogram-based statistics. This broadly extends the applicability of EL for time series in three directions: (i) SEL can treat any spectral mean parameters; (ii) SEL is valid for both linear and non-linear processes; and (iii) SEL has a provable bootstrap development, which is rare for time series EL, and provides a novel alternative to other resampling approximations in the frequency domain. Simulation studies suggest the proposed method performs well compared to other non-EL approaches. A real data example demonstrates that SEL has application and extension to complicated scenarios.},
  archive      = {J_AOS},
  author       = {Haihan Yu and Mark S. Kaiser and Daniel J. Nordman},
  doi          = {10.1214/24-AOS2388},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1152-1177},
  shortjournal = {Ann. Statist.},
  title        = {A blockwise empirical likelihood method for time series in frequency domain inference},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharp adaptive and pathwise stable similarity testing for
scalar ergodic diffusions. <em>AOS</em>, <em>52</em>(3), 1127–1151. (<a
href="https://doi.org/10.1214/24-AOS2386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the nonparametric diffusion model, we develop a multiple test to infer about similarity of an unknown drift b to some reference drift b0: At prescribed significance, we simultaneously identify those regions where violation from similarity occurs, without a priori knowledge of their number, size and location. This test is shown to be minimax-optimal and adaptive. At the same time, the procedure is robust under small deviation from Brownian motion as the driving noise process. A detailed investigation for fractional driving noise, which is neither a semimartingale nor a Markov process, is provided for Hurst indices close to the Brownian motion case.},
  archive      = {J_AOS},
  author       = {Johannes Brutsche and Angelika Rohde},
  doi          = {10.1214/24-AOS2386},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1127-1151},
  shortjournal = {Ann. Statist.},
  title        = {Sharp adaptive and pathwise stable similarity testing for scalar ergodic diffusions},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MARS via LASSO. <em>AOS</em>, <em>52</em>(3), 1102–1126. (<a
href="https://doi.org/10.1214/24-AOS2384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate adaptive regression splines (MARS) is a popular method for nonparametric regression introduced by Friedman in 1991. MARS fits simple nonlinear and non-additive functions to regression data. We propose and study a natural lasso variant of the MARS method. Our method is based on least squares estimation over a convex class of functions obtained by considering infinite-dimensional linear combinations of functions in the MARS basis and imposing a variation based complexity constraint. Our estimator can be computed via finite-dimensional convex optimization, although it is defined as a solution to an infinite-dimensional optimization problem. Under a few standard design assumptions, we prove that our estimator achieves a rate of convergence that depends only logarithmically on dimension and thus avoids the usual curse of dimensionality to some extent. We also show that our method is naturally connected to nonparametric estimation techniques based on smoothness constraints. We implement our method with a cross-validation scheme for the selection of the involved tuning parameter and compare it to the usual MARS method in various simulation and real data settings.},
  archive      = {J_AOS},
  author       = {Dohyeong Ki and Billy Fang and Adityanand Guntuboyina},
  doi          = {10.1214/24-AOS2384},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1102-1126},
  shortjournal = {Ann. Statist.},
  title        = {MARS via LASSO},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral regularized kernel two-sample tests. <em>AOS</em>,
<em>52</em>(3), 1076–1101. (<a
href="https://doi.org/10.1214/24-AOS2383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last decade, an approach that has gained a lot of popularity to tackle nonparametric testing problems on general (i.e., non-Euclidean) domains is based on the notion of reproducing kernel Hilbert space (RKHS) embedding of probability distributions. The main goal of our work is to understand the optimality of two-sample tests constructed based on this approach. First, we show the popular MMD (maximum mean discrepancy) two-sample test to be not optimal in terms of the separation boundary measured in Hellinger distance. Second, we propose a modification to the MMD test based on spectral regularization by taking into account the covariance information (which is not captured by the MMD test) and prove the proposed test to be minimax optimal with a smaller separation boundary than that achieved by the MMD test. Third, we propose an adaptive version of the above test which involves a data-driven strategy to choose the regularization parameter and show the adaptive test to be almost minimax optimal up to a logarithmic factor. Moreover, our results hold for the permutation variant of the test where the test threshold is chosen elegantly through the permutation of the samples. Through numerical experiments on synthetic and real data, we demonstrate the superior performance of the proposed test in comparison to the MMD test and other popular tests in the literature.},
  archive      = {J_AOS},
  author       = {Omar Hagrass and Bharath Sriperumbudur and Bing Li},
  doi          = {10.1214/24-AOS2383},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1076-1101},
  shortjournal = {Ann. Statist.},
  title        = {Spectral regularized kernel two-sample tests},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change acceleration and detection. <em>AOS</em>,
<em>52</em>(3), 1050–1075. (<a
href="https://doi.org/10.1214/24-AOS2382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.},
  archive      = {J_AOS},
  author       = {Yanglei Song and Georgios Fellouris},
  doi          = {10.1214/24-AOS2382},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1050-1075},
  shortjournal = {Ann. Statist.},
  title        = {Change acceleration and detection},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional covariance matrices under dynamic
volatility models: Asymptotics and shrinkage estimation. <em>AOS</em>,
<em>52</em>(3), 1027–1049. (<a
href="https://doi.org/10.1214/24-AOS2381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the estimation of high-dimensional covariance matrices and their empirical spectral distributions under dynamic volatility models. Data under such models have nonlinear dependency both cross-sectionally and temporally. We establish the condition under which the limiting spectral distribution (LSD) of the sample covariance matrix under scalar BEKK models is different from the i.i.d. case. We then propose a time-variation adjusted (TV-adj) sample covariance matrix and prove that its LSD follows the Marčenko–Pastur law. Based on the asymptotics of the TV-adj sample covariance matrix, we develop a consistent population spectrum estimator and an asymptotically optimal nonlinear shrinkage estimator of the unconditional covariance matrix.},
  archive      = {J_AOS},
  author       = {Yi Ding and Xinghua Zheng},
  doi          = {10.1214/24-AOS2381},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {1027-1049},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional covariance matrices under dynamic volatility models: Asymptotics and shrinkage estimation},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change-point inference in high-dimensional regression models
under temporal dependence. <em>AOS</em>, <em>52</em>(3), 999–1026. (<a
href="https://doi.org/10.1214/24-AOS2380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the limiting distributions of change-point estimators, in a high-dimensional linear regression time-series context, where a regression object (yt,Xt)∈R×Rp is observed at every time point t∈{1,…,n}. At unknown time points, called change points, the regression coefficients change, with the jump sizes measured in ℓ2-norm. We provide limiting distributions of the change-point estimators in the regimes where the minimal jump size vanishes and where it remains a constant. We allow for both the covariate and noise sequences to be temporally dependent, in the functional dependence framework, which is the first time seen in the change-point inference literature. We show that a block-type long-run variance estimator is consistent under the functional dependence, which facilitates the practical implementation of our derived limiting distributions. We also present a few important byproducts of our analysis, which are of their own interest. These include a novel variant of the dynamic programming algorithm to boost the computational efficiency, consistent change-point localization rates under temporal dependence and a new Bernstein inequality for data possessing functional dependence. Extensive numerical results are provided to support our theoretical results. The proposed methods are implemented in the R package changepoints (Xu et al. (2022)).},
  archive      = {J_AOS},
  author       = {Haotian Xu and Daren Wang and Zifeng Zhao and Yi Yu},
  doi          = {10.1214/24-AOS2380},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {999-1026},
  shortjournal = {Ann. Statist.},
  title        = {Change-point inference in high-dimensional regression models under temporal dependence},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Plugin estimation of smooth optimal transport maps.
<em>AOS</em>, <em>52</em>(3), 966–998. (<a
href="https://doi.org/10.1214/24-AOS2379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a number of natural estimators for the optimal transport map between two distributions and show that they are minimax optimal. We adopt the plugin approach: our estimators are simply optimal couplings between measures derived from our observations, appropriately extended so that they define functions on Rd. When the underlying map is assumed to be Lipschitz, we show that computing the optimal coupling between the empirical measures, and extending it using linear smoothers, already gives a minimax optimal estimator. When the underlying map enjoys higher regularity, we show that the optimal coupling between appropriate nonparametric density estimates yields faster rates. Our work also provides new bounds on the risk of corresponding plugin estimators for the quadratic Wasserstein distance, and we show how this problem relates to that of estimating optimal transport maps using stability arguments for smooth and strongly convex Brenier potentials. As an application of our results, we derive central limit theorems for plugin estimators of the squared Wasserstein distance, which are centered at their population counterpart when the underlying distributions have sufficiently smooth densities. In contrast to known central limit theorems for empirical estimators, this result easily lends itself to statistical inference for the quadratic Wasserstein distance.},
  archive      = {J_AOS},
  author       = {Tudor Manole and Sivaraman Balakrishnan and Jonathan Niles-Weed and Larry Wasserman},
  doi          = {10.1214/24-AOS2379},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {966-998},
  shortjournal = {Ann. Statist.},
  title        = {Plugin estimation of smooth optimal transport maps},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On blockwise and reference panel-based estimators for
genetic data prediction in high dimensions. <em>AOS</em>,
<em>52</em>(3), 948–965. (<a
href="https://doi.org/10.1214/24-AOS2378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic prediction holds immense promise for translating genetic discoveries into medical advances. As the high-dimensional covariance matrix (or the linkage disequilibrium (LD) pattern) of genetic variants often presents a block-diagonal structure, numerous methods account for the dependence among variants in predetermined local LD blocks. Moreover, due to privacy considerations and data protection concerns, genetic variant dependence in each LD block is typically estimated from external reference panels rather than the original training data set. This paper presents a unified analysis of blockwise and reference panel-based estimators in a high-dimensional prediction framework without sparsity restrictions. We find that, surprisingly, even when the covariance matrix has a block-diagonal structure with well-defined boundaries, blockwise estimation methods adjusting for local dependence can be substantially less accurate than methods controlling for the whole covariance matrix. Further, estimation methods built on the original training data set and external reference panels are likely to have varying performance in high dimensions, which may reflect the cost of having only access to summary level data from the training data set. This analysis is based on novel results in random matrix theory for block-diagonal covariance matrix. We numerically evaluate our results using extensive simulations and real data analysis in the UK Biobank.},
  archive      = {J_AOS},
  author       = {Bingxin Zhao and Shurong Zheng and Hongtu Zhu},
  doi          = {10.1214/24-AOS2378},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {948-965},
  shortjournal = {Ann. Statist.},
  title        = {On blockwise and reference panel-based estimators for genetic data prediction in high dimensions},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed estimation and inference for semiparametric
binary response models. <em>AOS</em>, <em>52</em>(3), 922–947. (<a
href="https://doi.org/10.1214/24-AOS2376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of modern technology has enabled data collection of unprecedented size, which poses new challenges to many statistical estimation and inference problems. This paper studies the maximum score estimator of a semiparametric binary choice model under a distributed computing environment without prespecifying the noise distribution. An intuitive divide-and-conquer estimator is computationally expensive and restricted by a nonregular constraint on the number of machines, due to the highly nonsmooth nature of the objective function. We propose (1) a one-shot divide-and-conquer estimator after smoothing the objective to relax the constraint, and (2) a multiround estimator to completely remove the constraint via iterative smoothing. We specify an adaptive choice of kernel smoother with a sequentially shrinking bandwidth to achieve the superlinear improvement of the optimization error over multiple iterations. The improved statistical accuracy per iteration is derived, and a quadratic convergence up to the optimal statistical error rate is established. We further provide two generalizations to handle the heterogeneity of data sets and high-dimensional problems where the parameter of interest is sparse.},
  archive      = {J_AOS},
  author       = {Xi Chen and Wenbo Jing and Weidong Liu and Yichen Zhang},
  doi          = {10.1214/24-AOS2376},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {922-947},
  shortjournal = {Ann. Statist.},
  title        = {Distributed estimation and inference for semiparametric binary response models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reconciling model-x and doubly robust approaches to
conditional independence testing. <em>AOS</em>, <em>52</em>(3), 895–921.
(<a href="https://doi.org/10.1214/24-AOS2372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-X approaches to testing conditional independence between a predictor and an outcome variable given a vector of covariates usually assume exact knowledge of the conditional distribution of the predictor given the covariates. Nevertheless, model-X methodologies are often deployed with this conditional distribution learned in sample. We investigate the consequences of this choice through the lens of the distilled conditional randomization test (dCRT). We find that Type-I error control is still possible, but only if the mean of the outcome variable given the covariates is estimated well enough. This demonstrates that the dCRT is doubly robust, and motivates a comparison to the generalized covariance measure (GCM) test, another doubly robust conditional independence test. We prove that these two tests are asymptotically equivalent, and show that the GCM test is optimal against (generalized) partially linear alternatives by leveraging semiparametric efficiency theory. In an extensive simulation study, we compare the dCRT to the GCM test. These two tests have broadly similar Type-I error and power, though dCRT can have somewhat better Type-I error control but somewhat worse power in small samples or when the response is discrete. We also find that post-lasso based test statistics (as compared to lasso based statistics) can dramatically improve Type-I error control for both methods.},
  archive      = {J_AOS},
  author       = {Ziang Niu and Abhinav Chakraborty and Oliver Dukes and Eugene Katsevich},
  doi          = {10.1214/24-AOS2372},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {895-921},
  shortjournal = {Ann. Statist.},
  title        = {Reconciling model-X and doubly robust approaches to conditional independence testing},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dimension-free mixing times of gibbs samplers for bayesian
hierarchical models. <em>AOS</em>, <em>52</em>(3), 869–894. (<a
href="https://doi.org/10.1214/24-AOS2367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gibbs samplers are popular algorithms to approximate posterior distributions arising from Bayesian hierarchical models. Despite their popularity and good empirical performance, however, there are still relatively few quantitative results on their convergence properties, for example, much less than for gradient-based sampling methods. In this work, we analyse the behaviour of total variation mixing times of Gibbs samplers targeting hierarchical models using tools from Bayesian asymptotics. We obtain dimension-free convergence results under random data-generating assumptions for a broad class of two-level models with generic likelihood function. Specific examples with Gaussian, binomial and categorical likelihoods are discussed.},
  archive      = {J_AOS},
  author       = {Filippo Ascolani and Giacomo Zanella},
  doi          = {10.1214/24-AOS2367},
  journal      = {The Annals of Statistics},
  month        = {6},
  number       = {3},
  pages        = {869-894},
  shortjournal = {Ann. Statist.},
  title        = {Dimension-free mixing times of gibbs samplers for bayesian hierarchical models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameter estimation in nonlinear multivariate stochastic
differential equations based on splitting schemes. <em>AOS</em>,
<em>52</em>(2), 842–867. (<a
href="https://doi.org/10.1214/24-AOS2371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The likelihood functions for discretely observed nonlinear continuous time models based on stochastic differential equations are not available except for a few cases. Various parameter estimation techniques have been proposed, each with advantages, disadvantages and limitations depending on the application. Most applications still use the Euler–Maruyama discretization, despite many proofs of its bias. More sophisticated methods, such as Kessler’s Gaussian approximation, Ozaki’s local linearization, Aït–Sahalia’s Hermite expansions or MCMC methods, might be complex to implement, do not scale well with increasing model dimension or can be numerically unstable. We propose two efficient and easy-to-implement likelihood-based estimators based on the Lie–Trotter (LT) and the Strang (S) splitting schemes. We prove that S has Lp convergence rate of order 1, a property already known for LT. We show that the estimators are consistent and asymptotically efficient under the less restrictive one-sided Lipschitz assumption. A numerical study on the 3-dimensional stochastic Lorenz system complements our theoretical findings. The simulation shows that the S estimator performs the best when measured on precision and computational speed compared to the state-of-the-art.},
  archive      = {J_AOS},
  author       = {Predrag Pilipovic and Adeline Samson and Susanne Ditlevsen},
  doi          = {10.1214/24-AOS2371},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {842-867},
  shortjournal = {Ann. Statist.},
  title        = {Parameter estimation in nonlinear multivariate stochastic differential equations based on splitting schemes},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The online closure principle. <em>AOS</em>, <em>52</em>(2),
817–841. (<a href="https://doi.org/10.1214/24-AOS2370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The closure principle is fundamental in multiple testing and has been used to derive many efficient procedures with familywise error rate control. However, it is often unsuitable for modern research, which involves flexible multiple testing settings where not all hypotheses are known at the beginning of the evaluation. In this paper, we focus on online multiple testing where a possibly infinite sequence of hypotheses is tested over time. At each step, it must be decided on the current hypothesis without having any information about the hypotheses that have not been tested yet. Our main contribution is a general and stringent mathematical definition of online multiple testing and a new online closure principle, which ensures that the resulting closed procedure can be applied in the online setting. We prove that any familywise error rate controlling online procedure can be derived by this online closure principle and provide admissibility results. In addition, we demonstrate how shortcuts of these online closed procedures can be obtained under a suitable consonance property.},
  archive      = {J_AOS},
  author       = {Lasse Fischer and Marta Bofill Roig and Werner Brannath},
  doi          = {10.1214/24-AOS2370},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {817-841},
  shortjournal = {Ann. Statist.},
  title        = {The online closure principle},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Minimax rates for heterogeneous causal effect estimation.
<em>AOS</em>, <em>52</em>(2), 793–816. (<a
href="https://doi.org/10.1214/24-AOS2369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of heterogeneous causal effects—that is, how effects of policies and treatments vary across subjects—is a fundamental task in causal inference. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but questions surrounding optimality have remained largely unanswered. In particular, a minimax theory of optimality has yet to be developed, with the minimax rate of convergence and construction of rate-optimal estimators remaining open problems. In this paper, we derive the minimax rate for CATE estimation, in a Hölder-smooth nonparametric model, and present a new local polynomial estimator, giving high-level conditions under which it is minimax optimal. Our minimax lower bound is derived via a localized version of the method of fuzzy hypotheses, combining lower bound constructions for nonparametric regression and functional estimation. Our proposed estimator can be viewed as a local polynomial R-Learner, based on a localized modification of higher-order influence function methods. The minimax rate we find exhibits several interesting features, including a nonstandard elbow phenomenon and an unusual interpolation between nonparametric regression and functional estimation rates. The latter quantifies how the CATE, as an estimand, can be viewed as a regression/functional hybrid.},
  archive      = {J_AOS},
  author       = {Edward H. Kennedy and Sivaraman Balakrishnan and James M. Robins and Larry Wasserman},
  doi          = {10.1214/24-AOS2369},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {793-816},
  shortjournal = {Ann. Statist.},
  title        = {Minimax rates for heterogeneous causal effect estimation},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Metric statistics: Exploration and inference for random
objects with distance profiles. <em>AOS</em>, <em>52</em>(2), 757–792.
(<a href="https://doi.org/10.1214/24-AOS2368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides an overview on the statistical modeling of complex data as increasingly encountered in modern data analysis. It is argued that such data can often be described as elements of a metric space that satisfies certain structural conditions and features a probability measure. We refer to the random elements of such spaces as random objects and to the emerging field that deals with their statistical analysis as metric statistics. Metric statistics provides methodology, theory and visualization tools for the statistical description, quantification of variation, centrality and quantiles, regression and inference for populations of random objects, inferring these quantities from available data and samples. In addition to a brief review of current concepts, we focus on distance profiles as a major tool for object data in conjunction with the pairwise Wasserstein transports of the underlying one-dimensional distance distributions. These pairwise transports lead to the definition of intuitive and interpretable notions of transport ranks and transport quantiles as well as two-sample inference. An associated profile metric complements the original metric of the object space and may reveal important features of the object data in data analysis. We demonstrate these tools for the analysis of complex data through various examples and visualizations.},
  archive      = {J_AOS},
  author       = {Paromita Dubey and Yaqing Chen and Hans-Georg Müller},
  doi          = {10.1214/24-AOS2368},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {757-792},
  shortjournal = {Ann. Statist.},
  title        = {Metric statistics: Exploration and inference for random objects with distance profiles},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inference for heteroskedastic PCA with missing data.
<em>AOS</em>, <em>52</em>(2), 729–756. (<a
href="https://doi.org/10.1214/24-AOS2366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies how to construct confidence regions for principal component analysis (PCA) in high dimension, a problem that has been vastly underexplored. While computing measures of uncertainty for nonlinear/nonconvex estimators is in general difficult in high dimension, the challenge is further compounded by the prevalent presence of missing data and heteroskedastic noise. We propose a novel approach to performing valid inference on the principal subspace, on the basis of an estimator called HeteroPCA (Ann. Statist. 50 (2022b) 53–80). We develop nonasymptotic distributional guarantees for HeteroPCA, and demonstrate how these can be invoked to compute both confidence regions for the principal subspace and entrywise confidence intervals for the spiked covariance matrix. Our inference procedures are fully data-driven and adaptive to heteroskedastic random noise, without requiring prior knowledge about the noise levels.},
  archive      = {J_AOS},
  author       = {Yuling Yan and Yuxin Chen and Jianqing Fan},
  doi          = {10.1214/24-AOS2366},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {729-756},
  shortjournal = {Ann. Statist.},
  title        = {Inference for heteroskedastic PCA with missing data},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Edge differentially private estimation in the β-model via
jittering and method of moments. <em>AOS</em>, <em>52</em>(2), 708–728.
(<a href="https://doi.org/10.1214/24-AOS2365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standing challenge in data privacy is the trade-off between the level of privacy and the efficiency of statistical inference. Here, we conduct an in-depth study of this trade-off for parameter estimation in the β-model (Ann. Appl. Probab. 21 (2011) 1400–1435) for edge differentially private network data released via jittering (J. R. Stat. Soc. Ser. C. Appl. Stat. 66 (2017) 481–500). Unlike most previous approaches based on maximum likelihood estimation for this network model, we proceed via the method of moments. This choice facilitates our exploration of a substantially broader range of privacy levels—corresponding to stricter privacy—than has been to date. Over this new range, we discover our proposed estimator for the parameters exhibits an interesting phase transition, with both its convergence rate and asymptotic variance following one of three different regimes of behavior depending on the level of privacy. Because identification of the operable regime is difficult, if not impossible in practice, we devise a novel adaptive bootstrap procedure to construct uniform inference across different phases. In fact, leveraging this bootstrap we are able to provide for simultaneous inference of all parameters in the β-model (i.e., equal to the number of nodes), which, to our best knowledge, is the first result of its kind. Numerical experiments confirm the competitive and reliable finite sample performance of the proposed inference methods, next to a comparable maximum likelihood method, as well as significant advantages in terms of computational speed and memory.},
  archive      = {J_AOS},
  author       = {Jinyuan Chang and Qiao Hu and Eric D. Kolaczyk and Qiwei Yao and Fengting Yi},
  doi          = {10.1214/24-AOS2365},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {708-728},
  shortjournal = {Ann. Statist.},
  title        = {Edge differentially private estimation in the β-model via jittering and method of moments},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finding the optimal dynamic treatment regimes using smooth
fisher consistent surrogate loss. <em>AOS</em>, <em>52</em>(2), 679–707.
(<a href="https://doi.org/10.1214/24-AOS2363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large health care data repositories such as electronic health records (EHR) open new opportunities to derive individualized treatment strategies for complicated diseases such as sepsis. In this paper, we consider the problem of estimating sequential treatment rules tailored to a patient’s individual characteristics, often referred to as dynamic treatment regimes (DTRs). Our main objective is to find the optimal DTR that maximizes a discontinuous value function through direct maximization of Fisher consistent surrogate loss functions. In this regard, we demonstrate that a large class of concave surrogates fails to be Fisher consistent—a behavior that differs from the classical binary classification problems. We further characterize a nonconcave family of Fisher consistent smooth surrogate functions, which is amenable to gradient-descent type optimization algorithms. Compared to the existing direct search approach under the support vector machine framework (J. Amer. Statist. Assoc. 110 (2015) 583–598), our proposed DTR estimation via surrogate loss optimization (DTRESLO) method is more computationally scalable to large sample sizes and allows for broader functional classes for treatment policies. We establish theoretical properties for our proposed DTR estimator and obtain a sharp upper bound on the regret corresponding to our DTRESLO method. The finite sample performance of our proposed estimator is evaluated through extensive simulations. We also illustrate the working principles and benefits of our method for estimating an optimal DTR for treating sepsis using EHR data from sepsis patients admitted to intensive care units.},
  archive      = {J_AOS},
  author       = {Nilanjana Laha and Aaron Sonabend-W and Rajarshi Mukherjee and Tianxi Cai},
  doi          = {10.1214/24-AOS2363},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {679-707},
  shortjournal = {Ann. Statist.},
  title        = {Finding the optimal dynamic treatment regimes using smooth fisher consistent surrogate loss},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning for functional mean estimation: Phase
transition and adaptive algorithms. <em>AOS</em>, <em>52</em>(2),
654–678. (<a href="https://doi.org/10.1214/24-AOS2362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies transfer learning for estimating the mean of random functions based on discretely sampled data, where in addition to observations from the target distribution, auxiliary samples from similar but distinct source distributions are available. The paper considers both common and independent designs and establishes the minimax rates of convergence for both designs. The results reveal an interesting phase transition phenomenon under the two designs and demonstrate the benefits of utilizing the source samples in the low sampling frequency regime. For practical applications, this paper proposes novel data-driven adaptive algorithms that attain the optimal rates of convergence within a logarithmic factor simultaneously over a large collection of parameter spaces. The theoretical findings are complemented by a simulation study that further supports the effectiveness of the proposed algorithms.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Dongwoo Kim and Hongming Pu},
  doi          = {10.1214/24-AOS2362},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {654-678},
  shortjournal = {Ann. Statist.},
  title        = {Transfer learning for functional mean estimation: Phase transition and adaptive algorithms},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for practically significant dependencies in high
dimensions via bootstrapping maxima of u-statistics. <em>AOS</em>,
<em>52</em>(2), 628–653. (<a
href="https://doi.org/10.1214/24-AOS2361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper takes a different look on the problem of testing the mutual independence of the components of a high-dimensional vector. Instead of testing if all pairwise associations (e.g., all pairwise Kendall’s τ) between the components vanish, we are interested in the (null) hypothesis that all pairwise associations do not exceed a certain threshold in absolute value. The consideration of these hypotheses is motivated by the observation that in the high-dimensional regime, it is rare, and perhaps impossible, to have a null hypothesis that can be exactly modeled by assuming that all pairwise associations are precisely equal to zero. The formulation of the null hypothesis as a composite hypothesis makes the problem of constructing tests nonstandard and in this paper we provide a solution for a broad class of dependence measures, which can be estimated by U-statistics. In particular, we develop an asymptotic and a bootstrap level α-test for the new hypotheses in the high-dimensional regime. We also prove that the new tests are minimax-optimal and investigate their finite sample properties by means of a small simulation study and a data example.},
  archive      = {J_AOS},
  author       = {Patrick Bastian and Holger Dette and Johannes Heiny},
  doi          = {10.1214/24-AOS2361},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {628-653},
  shortjournal = {Ann. Statist.},
  title        = {Testing for practically significant dependencies in high dimensions via bootstrapping maxima of U-statistics},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ℓ2 inference for change points in high-dimensional time
series via a two-way MOSUM. <em>AOS</em>, <em>52</em>(2), 602–627. (<a
href="https://doi.org/10.1214/24-AOS2360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an inference method for detecting multiple change points in high-dimensional time series, targeting dense or spatially clustered signals. Our method aggregates moving sum (MOSUM) statistics cross-sectionally by an ℓ2-norm and maximizes them over time. We further introduce a novel Two-Way MOSUM, which utilizes spatial-temporal moving regions to search for breaks, with the added advantage of enhancing testing power when breaks occur in only a few groups. The limiting distribution of an ℓ2-aggregated statistic is established for testing break existence by extending a high-dimensional Gaussian approximation theorem to spatial-temporal nonstationary processes. Simulation studies exhibit promising performance of our test in detecting nonsparse weak signals. Two applications on equity returns and COVID-19 cases in the United States show the real-world relevance of our algorithms. The R package “L2hdchange” is available on CRAN.},
  archive      = {J_AOS},
  author       = {Jiaqi Li and Likai Chen and Weining Wang and Wei Biao Wu},
  doi          = {10.1214/24-AOS2360},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {602-627},
  shortjournal = {Ann. Statist.},
  title        = {ℓ2 inference for change points in high-dimensional time series via a two-way MOSUM},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The edge of discovery: Controlling the local false discovery
rate at the margin. <em>AOS</em>, <em>52</em>(2), 580–601. (<a
href="https://doi.org/10.1214/24-AOS2359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the popularity of the false discovery rate (FDR) as an error control metric for large-scale multiple testing, its close Bayesian counterpart the local false discovery rate (lfdr), defined as the posterior probability that a particular null hypothesis is false, is a more directly relevant standard for justifying and interpreting individual rejections. However, the lfdr is difficult to work with in small samples, as the prior distribution is typically unknown. We propose a simple multiple testing procedure and prove that it controls the expectation of the maximum lfdr across all rejections; equivalently, it controls the probability that the rejection with the largest p-value is a false discovery. Our method operates without knowledge of the prior, assuming only that the p-value density is uniform under the null and decreasing under the alternative. We also show that our method asymptotically implements the oracle Bayes procedure for a weighted classification risk, optimally trading off between false positives and false negatives. We derive the limiting distribution of the attained maximum lfdr over the rejections, and the limiting empirical Bayes regret relative to the oracle procedure.},
  archive      = {J_AOS},
  author       = {Jake A. Soloff and Daniel Xiang and William Fithian},
  doi          = {10.1214/24-AOS2359},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {580-601},
  shortjournal = {Ann. Statist.},
  title        = {The edge of discovery: Controlling the local false discovery rate at the margin},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A general framework to quantify deviations from structural
assumptions in the analysis of nonstationary function-valued processes.
<em>AOS</em>, <em>52</em>(2), 550–579. (<a
href="https://doi.org/10.1214/24-AOS2358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general theory to quantify the uncertainty from imposing structural assumptions on the second-order structure of nonstationary Hilbert space-valued processes, which can be measured via functionals of time-dependent spectral density operators. The second-order dynamics are well known to be elements of the space of trace class operators, the latter is a Banach space of type 1 and of cotype 2, which makes the development of statistical inference tools more challenging. A part of our contribution is to obtain a weak invariance principle as well as concentration inequalities for (functionals of) the sequential time-varying spectral density operator. In addition, we introduce deviation measures in the nonstationary context, and derive corresponding estimators that are asymptotically pivotal. We then apply this framework and propose statistical methodology to investigate the validity of structural assumptions for nonstationary functional data, such as low-rank assumptions in the context of time-varying dynamic fPCA and principle separable component analysis, deviations from stationarity with respect to the square root distance, and deviations from zero functional canonical coherency.},
  archive      = {J_AOS},
  author       = {Anne van Delft and Holger Dette},
  doi          = {10.1214/24-AOS2358},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {550-579},
  shortjournal = {Ann. Statist.},
  title        = {A general framework to quantify deviations from structural assumptions in the analysis of nonstationary function-valued processes},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Consistent inference for diffusions from low frequency
measurements. <em>AOS</em>, <em>52</em>(2), 519–549. (<a
href="https://doi.org/10.1214/24-AOS2357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let (Xt) be a reflected diffusion process in a bounded convex domain in Rd, solving the stochastic differential equation dXt=∇f(Xt)dt+ 2f(Xt)dWt,t≥0, with Wt a d-dimensional Brownian motion. The data X0,XD,…,XND consist of discrete measurements and the time interval D between consecutive observations is fixed so that one cannot ‘zoom’ into the observed path of the process. The goal is to infer the diffusivity f and the associated transition operator Pt,f. We prove injectivity theorems and stability inequalities for the maps f↦Pt,f↦PD,f, t&lt;D. Using these estimates, we establish the statistical consistency of a class of Bayesian algorithms based on Gaussian process priors for the infinite-dimensional parameter f, and show optimality of some of the convergence rates obtained. We discuss an underlying relationship between the degree of ill-posedness of this inverse problem and the ‘hot spots’ conjecture from spectral geometry.},
  archive      = {J_AOS},
  author       = {Richard Nickl},
  doi          = {10.1214/24-AOS2357},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {519-549},
  shortjournal = {Ann. Statist.},
  title        = {Consistent inference for diffusions from low frequency measurements},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early stopping for l2-boosting in high-dimensional linear
models. <em>AOS</em>, <em>52</em>(2), 491–518. (<a
href="https://doi.org/10.1214/24-AOS2356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasingly high-dimensional data sets require that estimation methods do not only satisfy statistical guarantees but also remain computationally feasible. In this context, we consider L2-boosting via orthogonal matching pursuit in a high-dimensional linear model and analyze a data-driven early stopping time τ of the algorithm, which is sequential in the sense that its computation is based on the first τ iterations only. This approach is much less costly than established model selection criteria, that require the computation of the full boosting path, which may even be computationally infeasible in truly high-dimensional applications. We prove that sequential early stopping preserves statistical optimality in this setting in terms of a fully general oracle inequality for the empirical risk and recently established optimal convergence rates for the population risk. Finally, an extensive simulation study shows that at a significantly reduced computational cost, the performance of early stopping methods is on par with other state of the art algorithms such as the cross-validated Lasso or model selection via a high-dimensional Akaike criterion based on the full boosting path.},
  archive      = {J_AOS},
  author       = {Bernhard Stankewitz},
  doi          = {10.1214/24-AOS2356},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {491-518},
  shortjournal = {Ann. Statist.},
  title        = {Early stopping for l2-boosting in high-dimensional linear models},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Convergence rates of oblique regression trees for flexible
function libraries. <em>AOS</em>, <em>52</em>(2), 466–490. (<a
href="https://doi.org/10.1214/24-AOS2354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a theoretical framework for the analysis of oblique decision trees, where the splits at each decision node occur at linear combinations of the covariates (as opposed to conventional tree constructions that force axis-aligned splits involving only a single covariate). While this methodology has garnered significant attention from the computer science and optimization communities since the mid-80s, the advantages they offer over their axis-aligned counterparts remain only empirically justified, and explanations for their success are largely based on heuristics. Filling this long-standing gap between theory and practice, we show that oblique regression trees (constructed by recursively minimizing squared error) satisfy a type of oracle inequality and can adapt to a rich library of regression models consisting of linear combinations of ridge functions and their limit points. This provides a quantitative baseline to compare and contrast decision trees with other less interpretable methods, such as projection pursuit regression and neural networks, which target similar model forms. Contrary to popular belief, one needs not always trade-off interpretability with accuracy. Specifically, we show that, under suitable conditions, oblique decision trees achieve similar predictive accuracy as neural networks for the same library of regression models. To address the combinatorial complexity of finding the optimal splitting hyperplane at each decision node, our proposed theoretical framework can accommodate many existing computational tools in the literature. Our results rely on (arguably surprising) connections between recursive adaptive partitioning and sequential greedy approximation algorithms for convex optimization problems (e.g., orthogonal greedy algorithms), which may be of independent theoretical interest. Using our theory and methods, we also study oblique random forests.},
  archive      = {J_AOS},
  author       = {Matias D. Cattaneo and Rajita Chandak and Jason M. Klusowski},
  doi          = {10.1214/24-AOS2354},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {466-490},
  shortjournal = {Ann. Statist.},
  title        = {Convergence rates of oblique regression trees for flexible function libraries},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The curse of overparametrization in adversarial training:
Precise analysis of robust generalization for random features
regression. <em>AOS</em>, <em>52</em>(2), 441–465. (<a
href="https://doi.org/10.1214/24-AOS2353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Successful deep learning models often involve training neural network architectures that contain more parameters than the number of training samples. Such overparametrized models have recently been extensively studied, and the virtues of overparametrization have been established from both the statistical perspective, via the double-descent phenomenon, and the computational perspective via the structural properties of the optimization landscape. Despite this success, it is also well known that these models are highly vulnerable to small adversarial perturbations in their inputs. Even when adversarially trained, their performance on perturbed inputs (robust generalization) is considerably worse than their best attainable performance on benign inputs (standard generalization). It is thus imperative to understand how overparametrization fundamentally affects robustness. In this paper, we will provide a precise characterization of the role of overparametrization on robustness by focusing on random features regression models (two-layer neural networks with random first layer weights). We consider a regime where the sample size, the input dimension and the number of parameters grow proportionally, and derive an asymptotically exact formula for the robust generalization error when the model is adversarially trained. Our developed theory reveals the nontrivial effect of overparametrization on robustness and indicates that high overparametrization can hurt robust generalization.},
  archive      = {J_AOS},
  author       = {Hamed Hassani and Adel Javanmard},
  doi          = {10.1214/24-AOS2353},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {441-465},
  shortjournal = {Ann. Statist.},
  title        = {The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional inference for dynamic treatment effects.
<em>AOS</em>, <em>52</em>(2), 415–440. (<a
href="https://doi.org/10.1214/24-AOS2352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating dynamic treatment effects is a crucial endeavor in causal inference, particularly when confronted with high-dimensional confounders. Doubly robust (DR) approaches have emerged as promising tools for estimating treatment effects due to their flexibility. However, we showcase that the traditional DR approaches that only focus on the DR representation of the expected outcomes may fall short of delivering optimal results. In this paper, we propose a novel DR representation for intermediate conditional outcome models that leads to superior robustness guarantees. The proposed method achieves consistency even with high-dimensional confounders, as long as at least one nuisance function is appropriately parametrized for each exposure time and treatment path. Our results represent a significant step forward as they provide faster convergence rates and new robustness guarantees. The key to achieving these results lies in utilizing DR representations for intermediate conditional outcome models, which offer superior inferential performance while requiring weaker assumptions. Lastly, we examine finite sample behavior through simulations and a real data application.},
  archive      = {J_AOS},
  author       = {Jelena Bradic and Weijie Ji and Yuqian Zhang},
  doi          = {10.1214/24-AOS2352},
  journal      = {The Annals of Statistics},
  month        = {4},
  number       = {2},
  pages        = {415-440},
  shortjournal = {Ann. Statist.},
  title        = {High-dimensional inference for dynamic treatment effects},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to “nonparametric regression using deep neural
networks with ReLU activation function.” <em>AOS</em>, <em>52</em>(1),
413–414. (<a href="https://doi.org/10.1214/24-AOS2351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOS},
  author       = {Johannes Schmidt-Hieber and Don Vu},
  doi          = {10.1214/24-AOS2351},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {413-414},
  shortjournal = {Ann. Statist.},
  title        = {Correction to “Nonparametric regression using deep neural networks with ReLU activation function”},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Erratum: Improved multivariate normal mean estimation with
unknown covariance when p is greater than n. <em>AOS</em>,
<em>52</em>(1), 412. (<a
href="https://doi.org/10.1214/23-AOS2344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This erratum offers a correction to Chételat and Wells ((2012) Ann. Statist. 40 3137–3160), following the note of Foroushani and Nkurunziza ((2023) arXiv:2311.13140).},
  archive      = {J_AOS},
  author       = {Didier Chételat},
  doi          = {10.1214/23-AOS2344},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {412},
  shortjournal = {Ann. Statist.},
  title        = {Erratum: Improved multivariate normal mean estimation with unknown covariance when p is greater than n},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimation and inference for minimizer and minimum of convex
functions: Optimality, adaptivity and uncertainty principles.
<em>AOS</em>, <em>52</em>(1), 392–411. (<a
href="https://doi.org/10.1214/24-AOS2355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal estimation and inference for both the minimizer and minimum of a convex regression function under the white noise and nonparametric regression models are studied in a nonasymptotic local minimax framework, where the performance of a procedure is evaluated at individual functions. Fully adaptive and computationally efficient algorithms are proposed and sharp minimax lower bounds are given for both the estimation accuracy and expected length of confidence intervals for the minimizer and minimum. The nonasymptotic local minimax framework brings out new phenomena in simultaneous estimation and inference for the minimizer and minimum. We establish a novel uncertainty principle that provides a fundamental limit on how well the minimizer and minimum can be estimated simultaneously for any convex regression function. A similar result holds for the expected length of the confidence intervals for the minimizer and minimum.},
  archive      = {J_AOS},
  author       = {T. Tony Cai and Ran Chen and Yuancheng Zhu},
  doi          = {10.1214/24-AOS2355},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {392-411},
  shortjournal = {Ann. Statist.},
  title        = {Estimation and inference for minimizer and minimum of convex functions: Optimality, adaptivity and uncertainty principles},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rank and factor loadings estimation in time series tensor
factor model by pre-averaging. <em>AOS</em>, <em>52</em>(1), 364–391.
(<a href="https://doi.org/10.1214/23-AOS2350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The idiosyncratic components of a tensor time series factor model can exhibit serial correlations, (e.g., finance or economic data), ruling out many state-of-the-art methods that assume white/independent idiosyncratic components. While the traditional higher order orthogonal iteration (HOOI) is proved to be convergent to a set of factor loading matrices, the closeness of them to the true underlying factor loading matrices are in general not established, or only under i.i.d. Gaussian noises. Under the presence of serial and cross-correlations in the idiosyncratic components and time series variables with only bounded fourth-order moments, for tensor time series data with tensor order two or above, we propose a pre-averaging procedure that can be considered a random projection method. The estimated directions corresponding to the strongest factors are then used for projecting the data for a potentially improved re-estimation of the factor loading spaces themselves, with theoretical guarantees and rate of convergence spelt out when not all factors are pervasive. We also propose a new rank estimation method, which utilizes correlation information from the projected data. Extensive simulations are performed and compared to other state-of-the-art or traditional alternatives. A set of tensor-valued NYC taxi data is also analyzed.},
  archive      = {J_AOS},
  author       = {Weilin Chen and Clifford Lam},
  doi          = {10.1214/23-AOS2350},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {364-391},
  shortjournal = {Ann. Statist.},
  title        = {Rank and factor loadings estimation in time series tensor factor model by pre-averaging},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive variational bayes: Optimality, computation and
applications. <em>AOS</em>, <em>52</em>(1), 335–363. (<a
href="https://doi.org/10.1214/23-AOS2349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore adaptive inference based on variational Bayes. Although several studies have been conducted to analyze the contraction properties of variational posteriors, there is still a lack of a general and computationally tractable variational Bayes method that performs adaptive inference. To fill this gap, we propose a novel adaptive variational Bayes framework, which can operate on a collection of models. The proposed framework first computes a variational posterior over each individual model separately and then combines them with certain weights to produce a variational posterior over the entire model. It turns out that this combined variational posterior is the closest member to the posterior over the entire model in a predefined family of approximating distributions. We show that the adaptive variational Bayes attains optimal contraction rates adaptively under very general conditions. We also provide a methodology to maintain the tractability and adaptive optimality of the adaptive variational Bayes even in the presence of an enormous number of individual models, such as sparse models. We apply the general results to several examples, including deep learning and sparse factor models, and derive new and adaptive inference results. In addition, we characterize an implicit regularization effect of variational Bayes and show that the adaptive variational posterior can utilize this.},
  archive      = {J_AOS},
  author       = {Ilsang Ohn and Lizhen Lin},
  doi          = {10.1214/23-AOS2349},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {335-363},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive variational bayes: Optimality, computation and applications},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for independence in high dimensions based on
empirical copulas. <em>AOS</em>, <em>52</em>(1), 311–334. (<a
href="https://doi.org/10.1214/23-AOS2348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing for pairwise independence for the case where the number of variables may be of the same size or even larger than the sample size has received increasing attention in the recent years. We contribute to this branch of the literature by considering tests that allow to detect higher-order dependencies. The proposed methods are based on connecting the problem to copulas and making use of the Moebius transformation of the empirical copula process; an approach that is related to Lancaster interactions and that has already been used successfully for the case where the number of variables is fixed. Based on a martingale central limit theorem, it is shown that respective test statistics converge to the standard normal distribution, allowing for straightforward definition of critical values. The results are illustrated by a Monte Carlo simulation study.},
  archive      = {J_AOS},
  author       = {Axel Bücher and Cambyse Pakzad},
  doi          = {10.1214/23-AOS2348},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {311-334},
  shortjournal = {Ann. Statist.},
  title        = {Testing for independence in high dimensions based on empirical copulas},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Supervised homogeneity fusion: A combinatorial approach.
<em>AOS</em>, <em>52</em>(1), 285–310. (<a
href="https://doi.org/10.1214/23-AOS2347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusing regression coefficients into homogeneous groups can unveil those coefficients that share a common value within each group. Such groupwise homogeneity reduces the intrinsic dimension of the parameter space and unleashes sharper statistical accuracy. We propose and investigate a new combinatorial grouping approach called L0-Fusion that is amenable to mixed integer optimization (MIO). On the statistical aspect, we identify a fundamental quantity called MSE grouping sensitivity that underpins the difficulty of recovering the true groups. We show that L0-Fusion achieves grouping consistency under the weakest possible requirement of the grouping sensitivity: if this requirement is violated, then the minimax risk of group misspecification will fail to converge to zero. Moreover, we show that in the high-dimensional regime, one can apply L0-Fusion with a sure screening set of features without any essential loss of statistical efficiency, while reducing the computational cost substantially. On the algorithmic aspect, we provide an MIO formulation for L0-Fusion along with a warm start strategy. Simulation and real data analysis demonstrate that L0-Fusion exhibits superiority over its competitors in terms of grouping accuracy.},
  archive      = {J_AOS},
  author       = {Wen Wang and Shihao Wu and Ziwei Zhu and Ling Zhou and Peter X.-K. Song},
  doi          = {10.1214/23-AOS2347},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {285-310},
  shortjournal = {Ann. Statist.},
  title        = {Supervised homogeneity fusion: A combinatorial approach},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rates of estimation for high-dimensional multireference
alignment. <em>AOS</em>, <em>52</em>(1), 261–284. (<a
href="https://doi.org/10.1214/23-AOS2346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the continuous multireference alignment model of estimating a periodic function on the circle from noisy and circularly-rotated observations. Motivated by analogous high-dimensional problems that arise in cryo-electron microscopy, we establish minimax rates for estimating generic signals that are explicit in the dimension K. In a high-noise regime with noise variance σ2≳K, for signals with Fourier coefficients of roughly uniform magnitude, the rate scales as σ6 and has no further dependence on the dimension. This rate is achieved by a bispectrum inversion procedure, and our analyses provide new stability bounds for bispectrum inversion that may be of independent interest. In a low-noise regime where σ2≲K/logK, the rate scales instead as Kσ2, and we establish this rate by a sharp analysis of the maximum likelihood estimator that marginalizes over latent rotations. A complementary lower bound that interpolates between these two regimes is obtained using Assouad’s hypercube lemma. We extend these analyses also to signals whose Fourier coefficients have a slow power law decay.},
  archive      = {J_AOS},
  author       = {Zehao Dou and Zhou Fan and Harrison H. Zhou},
  doi          = {10.1214/23-AOS2346},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {261-284},
  shortjournal = {Ann. Statist.},
  title        = {Rates of estimation for high-dimensional multireference alignment},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Settling the sample complexity of model-based offline
reinforcement learning. <em>AOS</em>, <em>52</em>(1), 233–260. (<a
href="https://doi.org/10.1214/23-AOS2342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with offline reinforcement learning (RL), which learns using precollected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior results either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications. We demonstrate that the model-based (or “plug-in”) approach achieves minimax-optimal sample complexity without any burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a γ-discounted infinite-horizon (resp., finite-horizon) MDP with S states and effective horizon 11−γ (resp., horizon H), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient Cclipped⋆. We prove that model-based offline RL yields ε-accuracy with a sample complexity of SCclipped⋆ (1−γ)3ε2(infinite-horizon MDPs),H4SCclipped⋆ ε2(finite-horizon MDPs), up to log factor, which is minimax optimal for the entire ε-range. The proposed algorithms are “pessimistic” variants of value iteration with Bernstein-style penalties, and do not require sophisticated variance reduction. Our analysis framework is established upon delicate leave-one-out decoupling arguments in conjunction with careful self-bounding techniques tailored to MDPs.},
  archive      = {J_AOS},
  author       = {Gen Li and Laixi Shi and Yuxin Chen and Yuejie Chi and Yuting Wei},
  doi          = {10.1214/23-AOS2342},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {233-260},
  shortjournal = {Ann. Statist.},
  title        = {Settling the sample complexity of model-based offline reinforcement learning},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Transfer learning for contextual multi-armed bandits.
<em>AOS</em>, <em>52</em>(1), 207–232. (<a
href="https://doi.org/10.1214/23-AOS2341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a range of applications, we study in this paper the problem of transfer learning for nonparametric contextual multi-armed bandits under the covariate shift model, where we have data collected from source bandits before the start of the target bandit learning. The minimax rate of convergence for the cumulative regret is established and a novel transfer learning algorithm that attains the minimax regret is proposed. The results quantify the contribution of the data from the source domains for learning in the target domain in the context of nonparametric contextual multi-armed bandits. In view of the general impossibility of adaptation to unknown smoothness, we develop a data-driven algorithm that achieves near-optimal statistical guarantees (up to a logarithmic factor) while automatically adapting to the unknown parameters over a large collection of parameter spaces under an additional self-similarity assumption. A simulation study is carried out to illustrate the benefits of utilizing the data from the source domains for learning in the target domain.},
  archive      = {J_AOS},
  author       = {Changxiao Cai and T. Tony Cai and Hongzhe Li},
  doi          = {10.1214/23-AOS2341},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {207-232},
  shortjournal = {Ann. Statist.},
  title        = {Transfer learning for contextual multi-armed bandits},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rank-based indices for testing independence between two
high-dimensional vectors. <em>AOS</em>, <em>52</em>(1), 184–206. (<a
href="https://doi.org/10.1214/23-AOS2339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To test independence between two high-dimensional random vectors, we propose three tests based on the rank-based indices derived from Hoeffding’s D, Blum–Kiefer–Rosenblatt’s R and Bergsma–Dassios–Yanagimoto’s τ∗. Under the null hypothesis of independence, we show that the distributions of the proposed test statistics converge to normal ones if the dimensions diverge arbitrarily with the sample size. We further derive an explicit rate of convergence. Thanks to the monotone transformation-invariant property, these distribution-free tests can be readily used to generally distributed random vectors including heavily-tailed ones. We further study the local power of the proposed tests and compare their relative efficiencies with two classic distance covariance/correlation based tests in high-dimensional settings. We establish explicit relationships between D, R, τ∗ and Pearson’s correlation for bivariate normal random variables. The relationships serve as a basis for power comparison. Our theoretical results show that under a Gaussian equicorrelation alternative: (i) the proposed tests are superior to the two classic distance covariance/correlation based tests if the components of random vectors have very different scales; (ii) the asymptotic efficiency of the proposed tests based on D, τ∗ and R are sorted in a descending order.},
  archive      = {J_AOS},
  author       = {Yeqing Zhou and Kai Xu and Liping Zhu and Runze Li},
  doi          = {10.1214/23-AOS2339},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {184-206},
  shortjournal = {Ann. Statist.},
  title        = {Rank-based indices for testing independence between two high-dimensional vectors},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Adaptive novelty detection with false discovery rate
guarantee. <em>AOS</em>, <em>52</em>(1), 157–183. (<a
href="https://doi.org/10.1214/23-AOS2338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the semisupervised novelty detection problem where a set of “typical” measurements is available to the researcher. Motivated by recent advances in multiple testing and conformal inference, we propose AdaDetect, a flexible method that is able to wrap around any probabilistic classification algorithm and control the false discovery rate (FDR) on detected novelties in finite samples without any distributional assumption other than exchangeability. In contrast to classical FDR-controlling procedures that are often committed to a pre-specified p-value function, AdaDetect learns the transformation in a data-adaptive manner to focus the power on the directions that distinguish between inliers and outliers. Inspired by the multiple testing literature, we further propose variants of AdaDetect that are adaptive to the proportion of nulls while maintaining the finite-sample FDR control. The methods are illustrated on synthetic datasets and real-world datasets, including an application in astrophysics.},
  archive      = {J_AOS},
  author       = {Ariane Marandon and Lihua Lei and David Mary and Etienne Roquain},
  doi          = {10.1214/23-AOS2338},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {157-183},
  shortjournal = {Ann. Statist.},
  title        = {Adaptive novelty detection with false discovery rate guarantee},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical-computational trade-offs in tensor PCA and
related problems via communication complexity. <em>AOS</em>,
<em>52</em>(1), 131–156. (<a
href="https://doi.org/10.1214/23-AOS2331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, that is, a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtained for non-Gaussian component analysis, a family of statistical estimation problems in which low-order moment tensors carry no information about the unknown parameter. Finally, stronger lower bounds are obtained for an asymmetric variant of Tensor PCA and related statistical estimation problems. These results explain why many estimators for these problems use a memory state that is significantly larger than the effective dimensionality of the parameter of interest.},
  archive      = {J_AOS},
  author       = {Rishabh Dudeja and Daniel Hsu},
  doi          = {10.1214/23-AOS2331},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {131-156},
  shortjournal = {Ann. Statist.},
  title        = {Statistical-computational trade-offs in tensor PCA and related problems via communication complexity},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Characterization of causal ancestral graphs for time series
with latent confounders. <em>AOS</em>, <em>52</em>(1), 103–130. (<a
href="https://doi.org/10.1214/23-AOS2325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel class of graphical models for representing time-lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences—without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.},
  archive      = {J_AOS},
  author       = {Andreas Gerhardus},
  doi          = {10.1214/23-AOS2325},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {103-130},
  shortjournal = {Ann. Statist.},
  title        = {Characterization of causal ancestral graphs for time series with latent confounders},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). StarTrek: Combinatorial variable selection with false
discovery rate control. <em>AOS</em>, <em>52</em>(1), 78–102. (<a
href="https://doi.org/10.1214/23-AOS2296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection on the large-scale networks has been extensively studied in the literature. While most of the existing methods are limited to the local functionals especially the graph edges, this paper focuses on selecting the discrete hub structures of the networks. Specifically, we propose an inferential method, called StarTrek filter, to select the hub nodes with degrees larger than a certain thresholding level in the high-dimensional graphical models and control the false discovery rate (FDR). Discovering hub nodes in the networks is challenging: there is no straightforward statistic for testing the degree of a node due to the combinatorial structures; complicated dependence in the multiple testing problem is hard to characterize and control. In methodology, the StarTrek filter overcomes this by constructing p-values based on the maximum test statistics via the Gaussian multiplier bootstrap. In theory, we show that the StarTrek filter can control the FDR by providing accurate bounds on the approximation errors of the quantile estimation and addressing the dependence structures among the maximal statistics. To this end, we establish novel Cramér-type comparison bounds for the high-dimensional Gaussian random vectors. Compared to the Gaussian comparison bound via the Kolmogorov distance established by Chernozhukov, Chetverikov and Kato (Ann. Statist. 42 (2014) 1787–1818), our Cramér-type comparison bounds establish the relative difference between the distribution functions of two high-dimensional Gaussian random vectors, which is essential in the theoretical analysis of FDR control. Moreover, the StarTrek filter can be applied to general statistical models for FDR control of discovering discrete structures such as simultaneously testing the sparsity levels of multiple high-dimensional linear models. We illustrate the validity of the StarTrek filter in a series of numerical experiments and apply it to the genotype-tissue expression dataset to discover central regulator genes.},
  archive      = {J_AOS},
  author       = {Lu Zhang and Junwei Lu},
  doi          = {10.1214/23-AOS2296},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {78-102},
  shortjournal = {Ann. Statist.},
  title        = {StarTrek: Combinatorial variable selection with false discovery rate control},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Maximum likelihood for high-noise group orbit estimation and
single-particle cryo-EM. <em>AOS</em>, <em>52</em>(1), 52–77. (<a
href="https://doi.org/10.1214/23-AOS2292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by applications to single-particle cryo-electron microscopy (cryo-EM), we study several problems of function estimation in a high noise regime, where samples are observed after random rotation and possible linear projection of the function domain. We describe a stratification of the Fisher information eigenvalues according to transcendence degrees of graded pieces of the algebra of group invariants, and we relate critical points of the log-likelihood landscape to a sequence of moment optimization problems, extending previous results for a discrete rotation group without projections. We then compute the transcendence degrees and forms of these optimization problems for several examples of function estimation under SO(2) and SO(3) rotations, including a simplified model of cryo-EM as introduced by Bandeira, Blum-Smith, Kileel, Niles-Weed, Perry and Wein. We affirmatively resolve conjectures that third-order moments are sufficient to locally identify a generic signal up to its rotational orbit in these examples. For low-dimensional approximations of the electric potential maps of two small protein molecules, we empirically verify that the noise scalings of the Fisher information eigenvalues conform with our theoretical predictions over a range of SNR, in a model of SO(3) rotations without projections.},
  archive      = {J_AOS},
  author       = {Zhou Fan and Roy R. Lederman and Yi Sun and Tianhao Wang and Sheng Xu},
  doi          = {10.1214/23-AOS2292},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {52-77},
  shortjournal = {Ann. Statist.},
  title        = {Maximum likelihood for high-noise group orbit estimation and single-particle cryo-EM},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The right complexity measure in locally private estimation:
It is not the fisher information. <em>AOS</em>, <em>52</em>(1), 1–51.
(<a href="https://doi.org/10.1214/22-AOS2227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We identify fundamental tradeoffs between statistical utility and privacy under local models of privacy in which data is kept private even from the statistician, providing instance-specific bounds for private estimation and learning problems by developing the local minimax risk. In contrast to approaches based on worst-case (minimax) error, which are conservative, this allows us to evaluate the difficulty of individual problem instances and delineate the possibilities for adaptation in private estimation and inference. Our main results show that the local modulus of continuity of the estimand with respect to the variation distance—as opposed to the Hellinger distance central to classical statistics—characterizes rates of convergence under locally private estimation for many notions of privacy, including differential privacy and its relaxations. As consequences of these results, we identify an alternative to the Fisher information for private estimation, giving a more nuanced understanding of the challenges of adaptivity and optimality.},
  archive      = {J_AOS},
  author       = {John C. Duchi and Feng Ruan},
  doi          = {10.1214/22-AOS2227},
  journal      = {The Annals of Statistics},
  month        = {2},
  number       = {1},
  pages        = {1-51},
  shortjournal = {Ann. Statist.},
  title        = {The right complexity measure in locally private estimation: It is not the fisher information},
  volume       = {52},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
