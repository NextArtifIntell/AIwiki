<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aoas---169">AOAS - 169</h2>
<ul>
<li><details>
<summary>
(2024). DeepMap: Deep learning-based single-cell data integration
using iterative cell matching and structure preservation constraints.
<em>AOAS</em>, <em>18</em>(4), 3596–3613. (<a
href="https://doi.org/10.1214/24-AOAS1954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective integration of single-cell data can facilitate the discovery of cell-type specific gene expression patterns and cellular interactions, ultimately leading to a better understanding of various biological processes and diseases. However, datasets from different platforms, species, and modalities exhibit various levels of heterogeneities, posing significant challenges in data alignment using a unified approach. Here we propose DeepMap, a flexible and efficient method for single-cell data integration, by taking advantage of the deep learning framework. Our method utilizes iterative cell matching based on mutual nearest neighbors, leverages an autoencoder framework to learn harmonized representations of cells from various datasets, and incorporates a covariance penalty term into the framework for structure preservation. In addition to harmonization of data from different datasets, we specifically take account of the preservation of important biological variations within dataset, which is crucial to reliable downstream analysis. Comprehensive real data analysis demonstrates the flexibility of DeepMap for diverse datasets from different platforms, species, and modalities, and highlights its marked ability in preserving structures over existing integration methods with enhanced computational efficiency and optimized memory usage. The robust DeepMap-integrated data offers promising prospects for advancing our understanding of cell biology, hence making it a highly attractive option for integrative single-cell data analysis.},
  archive      = {J_AOAS},
  author       = {Shuntuo Xu and Zhou Yu and Jingsi Ming},
  doi          = {10.1214/24-AOAS1954},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3596-3613},
  shortjournal = {Ann. Appl. Stat.},
  title        = {DeepMap: Deep learning-based single-cell data integration using iterative cell matching and structure preservation constraints},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Deconvolution analysis of spatial transcriptomics by
multiplicative-additive poisson-gamma models. <em>AOAS</em>,
<em>18</em>(4), 3570–3595. (<a
href="https://doi.org/10.1214/24-AOAS1953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding cell type composition and gene expression of spatial transcriptomic data is crucial for comprehending phenotypic variability and detecting key factors that influence disease susceptibility of complex traits. Detecting cell type specific expression patterns from spatial transcriptome profiles is important in studying the cellular components and gene expression of individual cell classes and structural architecture. In this paper we develop mixed effect multiplicative-additive Poisson-gamma models to analyze spatial (MAPS) transcriptomics data using cell type-specific gene expressions in single cell RNA-sequenceing (scRNA-seq) data. To build the mixed effect multiplicative-additive Poisson-gamma models, the gene expression counts of spatial transcriptomics data are treated as dependent variables, and the mean and variance parameters of scRNA-seq data are used to construct independent variables to explain the dependent variables on the basis of Poisson-gamma mixture. One novelty of the proposed mixed models is that the variance parameters of scRNA-seq are used to describe the within-cell-type variations or stochasticity. We develop iteratively analytical formulae to estimate the cell type proportions and dispersion parameters. To address the important research problems and help with intensive spatial transcriptomics data analysis, a readily available software, MAPS, is developed to implement the proposed methods. By simulation study and real data analysis, MAPS is found to perform better than or similar to robust cell type decomposition (RCTD), SpatialDWLS (dampened weighted least squares), conditional autoregressive-based deconvolution (CARD), and a Spatially weighted pOissoN-gAmma Regression model (SONAR). Computationally, MAPS is significantly faster than RCTD and SpatialDWLS. MAPS provides a novel way for mapping spatial tissue architecture.},
  archive      = {J_AOAS},
  author       = {Yutong Luo and Joan E. Bailey-Wilson and Christopher Albanese and Ruzong Fan},
  doi          = {10.1214/24-AOAS1953},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3570-3595},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Deconvolution analysis of spatial transcriptomics by multiplicative-additive poisson-gamma models},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatio-temporal analysis of dependent risk with an
application to cyberattacks data. <em>AOAS</em>, <em>18</em>(4),
3549–3569. (<a href="https://doi.org/10.1214/24-AOAS1952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity is an important issue given the increasing risks due to cyberattacks in many areas. Cyberattacks could result in huge losses such as data breaches, failures in the control systems of infrastructures, physical damages in manufacturing industries, etc. As a result, cybersecurity-related research has grown rapidly for in-depth analysis. One main interest is to understand the correlated nature of cyberattack data. To understand such characteristics, we propose a spatio-temporal model for the hostwisely aggregated cyberattack data by incorporating the characteristics of the attackers. We develop a new dissimilarity measure as a proxy of spatial distance to be integrated into the model. The proposed model can be considered as a spatial extension of the GARCH model. The estimation is carried out using a Bayesian approach, which is demonstrated to work well in simulations. The proposed model is applied to publicly available honeypot data after the data are divided by selected features of the attackers via clustering. The estimated model parameters vary by groups of attackers, which was not revealed by modeling the entire dataset.},
  archive      = {J_AOAS},
  author       = {Songhyun Kim and Chae Young Lim and Yeonwoo Rho},
  doi          = {10.1214/24-AOAS1952},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3549-3569},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatio-temporal analysis of dependent risk with an application to cyberattacks data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable test of statistical significance for protein-DNA
binding changes with insertion and deletion of bases in the genome.
<em>AOAS</em>, <em>18</em>(4), 3528–3548. (<a
href="https://doi.org/10.1214/24-AOAS1950">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutations in the noncoding DNA, which represents approximately 99% of the human genome, have been crucial to understanding disease mechanisms through dysregulation of disease-associated genes. One key element in gene regulation that noncoding mutations mediate is the binding of proteins to DNA sequences. Insertion and deletion of bases (InDels) are the second most common type of mutations, following single nucleotide polymorphisms, that may impact protein-DNA binding. However, no existing methods can estimate and test the effects of InDels on the process of protein-DNA binding. We develop a novel test of statistical significance, namely, the binding change test (BC test), using a Markov model to evaluate the impact and identify InDels altering protein-DNA binding. The test predicts binding changer InDels of regulatory significance with an efficient importance sampling algorithm generating background sequences in favor of large binding affinity changes. Simulation studies demonstrate its excellent performance. The application to human leukemia data uncovers, in critical cis-regulatory elements, candidate pathological InDels on modulating TF binding in leukemic patients. We develop an R package atIndel, which is available on GitHub.},
  archive      = {J_AOAS},
  author       = {Qinyi Zhou and Chandler Zuo and Yuannyu Zhang and Min Chen and Jian Xu and Sunyoung Shin},
  doi          = {10.1214/24-AOAS1950},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3528-3548},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Scalable test of statistical significance for protein-DNA binding changes with insertion and deletion of bases in the genome},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new design for observational studies applied to the study
of the effects of high school football on cognition late in life.
<em>AOAS</em>, <em>18</em>(4), 3507–3527. (<a
href="https://doi.org/10.1214/24-AOAS1949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Do the impacts that occur when playing high school football have concussive effects that accelerate cognitive decline late in life? We examine this possibility using newly available cognitive data describing people in 2020 who graduated high school in 1957. Someone who was 18 in 1957 would be 81 in 2020. For this comparison we develop a new design for an observational study, called a triples design, and discuss its advantages and construction. A triples design consists of M blocks of size 3, where a block contains either one treated individual and two controls or two treated individuals and one control. A triples design is the simplest design that uses weights, with just two weights. The “entire number” is {1−e(x)}/e(x), where e(x) is the propensity score at covariate x, so it is the ratio of controls-to-treated expected at x. Unlike a matched pairs design, which can remove the bias from observed covariates when the “entire number” exceeds 1, the triples design can succeed when the entire number exceeds 1/2, reflecting the possibility of matching two treated individuals to the same control. Like full matching, a triples design can match more people than can matched pairs, yet have smaller within-block covariate distances. Unlike full matching, there are no matched pairs. Like matching with multiple controls, a triples design will have a larger design sensitivity than a design which includes matched pairs, under simple models for continuous outcomes; that is, in favorable situations the design is expected to report greater insensitivity to unmeasured biases. Because there are just two weights, it is easy to construct weighted graphics for exploratory displays from triples designs. A heuristic algorithm containing network optimization constructs the design.},
  archive      = {J_AOAS},
  author       = {Katherine Brumberg and Dylan S. Small and Paul R. Rosenbaum},
  doi          = {10.1214/24-AOAS1949},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3507-3527},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A new design for observational studies applied to the study of the effects of high school football on cognition late in life},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting milk traits from spectral data using bayesian
probabilistic partial least squares regression. <em>AOAS</em>,
<em>18</em>(4), 3486–3506. (<a
href="https://doi.org/10.1214/24-AOAS1947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional spectral data—routinely generated in dairy production—are used to predict a range of traits in milk products. Partial least squares (PLS) regression is ubiquitously used for these prediction tasks. However, PLS regression is not typically viewed as arising from a probabilistic model, and parameter uncertainty is rarely quantified. Additionally, PLS regression does not easily lend itself to model-based modifications, coherent prediction intervals are not readily available, and the process of choosing the latent-space dimension, Q, can be subjective and sensitive to data size. We introduce a Bayesian latent-variable model, emulating the desirable properties of PLS regression while accounting for parameter uncertainty in prediction. The need to choose Q is eschewed through a nonparametric shrinkage prior. The flexibility of the proposed Bayesian partial least squares (BPLS) regression framework is exemplified by considering sparsity modifications and allowing for multivariate response prediction. The BPLS regression framework is used in two motivating settings: (1) multivariate trait prediction from mid-infrared spectral analyses of milk samples and (2) milk pH prediction from surface-enhanced Raman spectral data. The prediction performance of BPLS regression at least matches that of PLS regression. Additionally, the provision of correctly calibrated prediction intervals objectively provides richer, more informative inference for stakeholders in dairy production.},
  archive      = {J_AOAS},
  author       = {Szymon Urbas and Pierre Lovera and Robert Daly and Alan O’Riordan and Donagh Berry and Isobel Claire Gormley},
  doi          = {10.1214/24-AOAS1947},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3486-3506},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Predicting milk traits from spectral data using bayesian probabilistic partial least squares regression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple latent clustering model for the inference of RNA
life-cycle kinetic rates from sequencing data. <em>AOAS</em>,
<em>18</em>(4), 3467–3485. (<a
href="https://doi.org/10.1214/24-AOAS1945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a hierarchical Bayesian model to infer RNA synthesis, processing, and degradation rates from time-course RNA sequencing data, based on an ordinary differential equation system that models the RNA life cycle. We parametrize the latent kinetic rates, which rule the system, with a novel functional form and estimate their parameters through three Dirichlet process mixture models. Owing to the complexity of this approach, we are able to simultaneously perform inference, clustering, and model selection. We apply our method to investigate transcriptional and post-transcriptional responses of murine fibroblasts to the activation of the proto-oncogene Myc. Our approach uncovers simultaneous regulations of the rates, which had been largely missed in previous analyses of this biological system.},
  archive      = {J_AOAS},
  author       = {Gianluca Mastrantonio and Enrico Bibbona and Mattia Furlan},
  doi          = {10.1214/24-AOAS1945},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3467-3485},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multiple latent clustering model for the inference of RNA life-cycle kinetic rates from sequencing data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A spatially varying hierarchical random effects model for
longitudinal macular structural data in glaucoma patients.
<em>AOAS</em>, <em>18</em>(4), 3444–3466. (<a
href="https://doi.org/10.1214/24-AOAS1944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model longitudinal macular thickness measurements to monitor the course of glaucoma and prevent vision loss due to disease progression. The macular thickness varies over a 6 × 6 grid of locations on the retina, with additional variability arising from the imaging process at each visit. Currently, ophthalmologists estimate slopes using repeated simple linear regression for each subject and location. To estimate slopes more precisely, we develop a novel Bayesian hierarchical model for multiple subjects with spatially varying population-level and subject-level coefficients, borrowing information over subjects and measurement locations. We augment the model with visit effects to account for observed spatially correlated visit-specific errors. We model spatially varying: (a) intercepts, (b) slopes, and (c) log-residual standard deviations (SD) with multivariate Gaussian process priors with Matérn cross-covariance functions. Each marginal process assumes an exponential kernel with its own SD and spatial correlation matrix. We develop our models for and apply them to data from the Advanced Glaucoma Progression Study. We show that including visit effects in the model reduces error in predicting future thickness measurements and greatly improves model fit.},
  archive      = {J_AOAS},
  author       = {Erica Su and Robert E. Weiss and Kouros Nouri-Mahdavi and Andrew J. Holbrook},
  doi          = {10.1214/24-AOAS1944},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3444-3466},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A spatially varying hierarchical random effects model for longitudinal macular structural data in glaucoma patients},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling trajectories using functional linear differential
equations. <em>AOAS</em>, <em>18</em>(4), 3425–3443. (<a
href="https://doi.org/10.1214/24-AOAS1943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are motivated by a study that seeks to better understand the dynamic relationship between muscle activation and paw position during locomotion. For each gait cycle in this experiment, activation in the biceps and triceps is measured continuously and in parallel with paw position as a mouse trotted on a treadmill. We propose an innovative general regression method that draws from both ordinary differential equations and functional data analysis to model the relationship between these functional inputs and responses as a dynamical system that evolves over time. Specifically, our model addresses gaps in both literatures and borrows strength across curves estimating ODE parameters across all curves simultaneously rather than separately modeling each functional observation. Our approach compares favorably to related functional data methods in simulations and in cross-validated predictive accuracy of paw position in the gait data. In the analysis of the gait cycles, we find that paw speed and position are dynamically influenced by inputs from the biceps and triceps muscles and that the effect of muscle activation persists beyond the activation itself.},
  archive      = {J_AOAS},
  author       = {Julia Wrobel and Britton Sauerbrei and Eric A. Kirk and Jian-Zhong Guo and Adam Hantman and Jeff Goldsmith},
  doi          = {10.1214/24-AOAS1943},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3425-3443},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling trajectories using functional linear differential equations},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning brain connectivity in social cognition with dynamic
network regression. <em>AOAS</em>, <em>18</em>(4), 3405–3424. (<a
href="https://doi.org/10.1214/24-AOAS1942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks have been increasingly used to characterize brain connectivity that varies during resting and task states. In such characterizations a connectivity network is typically measured at each time point for a subject over a common set of nodes representing brain regions, together with rich subject-level information. A common approach to analyzing such data is an edge-based method that models the connectivity between each pair of nodes separately. However, such approach may have limited performance when the noise level is high and the number of subjects is limited, as it does not take advantage of the inherent network structure. To better understand if and how the subject-level covariates affect the dynamic brain connectivity, we introduce a semiparametric dynamic network response regression that relates a dynamic brain connectivity network to a vector of subject-level covariates. A key advantage of our method is to exploit the structure of dynamic imaging coefficients in the form of high-order tensors. We develop an efficient estimation algorithm and evaluate the efficacy of our approach through simulation studies. Finally, we present our results on the analysis of a task-related study on social cognition in the Human Connectome Project, where we identify known sex-specific effects on brain connectivity that cannot be inferred using alternative methods.},
  archive      = {J_AOAS},
  author       = {Maoyu Zhang and Biao Cai and Wenlin Dai and Dehan Kong and Hongyu Zhao and Jingfei Zhang},
  doi          = {10.1214/24-AOAS1942},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3405-3424},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning brain connectivity in social cognition with dynamic network regression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Predicting COVID-19 hospitalisation using a mixture of
bayesian predictive syntheses. <em>AOAS</em>, <em>18</em>(4), 3383–3404.
(<a href="https://doi.org/10.1214/24-AOAS1941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel methodology called the mixture of Bayesian predictive syntheses (MBPS) for multiple time series count data for the challenging task of predicting the numbers of COVID-19 inpatients and isolated cases in Japan and Korea at the subnational level. MBPS combines a set of predictive models and partitions the multiple time series into clusters based on their contribution to predicting the outcome. In this way MBPS leverages the shared information within each cluster and is suitable for predicting COVID-19 inpatients since the data exhibit similar dynamics over multiple areas. Also, MBPS avoids using a multivariate count model, which is generally cumbersome to develop and implement. Our Japanese and Korean data analyses demonstrate that the proposed MBPS methodology has improved predictive accuracy and uncertainty quantification.},
  archive      = {J_AOAS},
  author       = {Genya Kobayashi and Shonosuke Sugasawa and Yuki Kawakubo and Dongu Han and Taeryon Choi},
  doi          = {10.1214/24-AOAS1941},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3383-3404},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Predicting COVID-19 hospitalisation using a mixture of bayesian predictive syntheses},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling urban crime occurrences via network regularized
regression. <em>AOAS</em>, <em>18</em>(4), 3364–3382. (<a
href="https://doi.org/10.1214/24-AOAS1940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyses of occurrences of residential burglary in urban areas have shown that crime rates are not spatially homogeneous: rates vary across the network of city streets, resulting in some areas being far more susceptible to crime than others. The explanation for why a certain segment of the city experiences high crime may be different than why a neighboring area experiences high crime. Motivated by the importance of understanding spatial patterns such as these, we consider a statistical model of burglary defined on the street network of Boston, Massachusetts. Leveraging ideas from functional data analysis, our proposed solution consists of a generalized linear model with vertex-indexed covariates, allowing for an interpretation of the covariate effects at the street level. We employ a regularization procedure cast as a prior distribution on the regression coefficients under a Bayesian setup so that the predicted responses vary smoothly according to the connectivity of the city. We introduce a novel variable selection procedure, examine computationally efficient methods for sampling from the posterior distribution of the model parameters, and demonstrate the flexibility of our proposed modeling structure. The resulting model and interpretations provide insight into the spatial network patterns and dynamics of residential burglary in Boston.},
  archive      = {J_AOAS},
  author       = {Elizabeth Upton and Luis Carvalho},
  doi          = {10.1214/24-AOAS1940},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3364-3382},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling urban crime occurrences via network regularized regression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Poisson–birnbaum–saunders regression model for clustered
count data. <em>AOAS</em>, <em>18</em>(4), 3338–3363. (<a
href="https://doi.org/10.1214/24-AOAS1939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study the number of inpatient admissions by individuals to hospital emergency rooms reported by the 2003 Medical Expenditure Panel Survey (MEPS), which the United States Agency for Health Research and Quality conducts. Explanatory variables such as health status, access, use, and costs of health services in the U.S.A. are considered. Our main goal is to properly model the number of inpatient admissions, according to the geographical U.S. regions, as a tool for measuring the volume of diagnostic procedures in the health care system. In the analysis four clusters were determined according to the regions in the U.S., namely, the midwest, northeast, south, and west. The clustered analysis of this count data from the MEPS is a novel contribution to the best of our knowledge. Our analysis demonstrated that a clustered negative binomial (CNB) regression (Poisson model with latent gamma effects) might not be a suitable choice for analyzing the MEPS data. This fact motivates us to introduce a new regression model to handle clustered count data. To account for correlation within clusters, we propose a Poisson regression model where the observations within the same cluster are driven by the same latent random effect that follows a Birnbaum–Saunders distribution with a parameter that controls the strength of dependence among the individuals. This novel multivariate count model is called Clustered Poisson–Birnbaum–Saunders (CPBS) regression. The CPBS model is analytically tractable, and its moment structure can be explicitly obtained. We also derive theoretical/methodological studies to advise when the Birnbaum–Saunders effect should be preferred over the gamma effect (and vice-versa) in terms of probability tail. Estimation is performed through the maximum likelihood method. Here we also developed an expectation-maximization (EM) algorithm for estimation. Simulation results that evaluate the finite-sample performance of our proposed estimators are presented. Studies on the potential impact of model misspecification were conducted, and comparisons between our model and a CNB regression were also addressed. A full statistical analysis of the MEPS data reveals that, compared to the CNB model, the CPBS regression model produces better results in terms of prediction and goodness-of-fit.},
  archive      = {J_AOAS},
  author       = {Jussiane Nader Gonçalves and Wagner Barreto-Souza and Hernando Ombao},
  doi          = {10.1214/24-AOAS1939},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3338-3363},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Poisson–Birnbaum–Saunders regression model for clustered count data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A semiparametric method for risk prediction using integrated
electronic health record data. <em>AOAS</em>, <em>18</em>(4), 3318–3337.
(<a href="https://doi.org/10.1214/24-AOAS1938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using electronic health records (EHRs) for clinical and translational research, additional data is often available from external sources to enrich the information extracted from EHRs. For example, academic biobanks have more granular data available, and patient reported data is often collected through small-scale surveys. It is common that the external data is available only for a small subset of patients who have EHR information. We propose efficient and robust methods for building and evaluating models for predicting the risk of binary outcomes using such integrated EHR data. Our method is built upon an idea derived from the two-phase design literature that modeling the availability of a patient’s external data as a function of an EHR-based preliminary predictive score leads to effective utilization of the EHR data. Through both theoretical and simulation studies, we show that our method has high efficiency for estimating log-odds ratio parameters, the area under the ROC curve, as well as other measures for quantifying predictive accuracy. We apply our method to develop a model for predicting the short-term mortality risk of oncology patients, where the data was extracted from the University of Pennsylvania hospital system EHR and combined with survey-based patient reported outcome data.},
  archive      = {J_AOAS},
  author       = {Jill Hasler and Yanyuan Ma and Yizheng Wei and Ravi Parikh and Jinbo Chen},
  doi          = {10.1214/24-AOAS1938},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3318-3337},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A semiparametric method for risk prediction using integrated electronic health record data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A robust bayesian meta-analysis for estimating the hubble
constant via time delay cosmography. <em>AOAS</em>, <em>18</em>(4),
3297–3317. (<a href="https://doi.org/10.1214/24-AOAS1937">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian meta-analysis to infer the current expansion rate of the Universe, called the Hubble constant (H0), via time delay cosmography. Inputs of the meta-analysis are estimates of two properties for each pair of gravitationally lensed images; time delay and Fermat potential difference estimates with their standard errors. A meta-analysis can be appealing in practice because obtaining each estimate from even a single lens system involves substantial human efforts, and thus estimates are often separately obtained and published. Moreover, numerous estimates are expected to be available once the Rubin Observatory starts monitoring thousands of strong gravitational lens systems. This work focuses on combining these estimates from independent studies to infer H0 in a robust manner. The robustness is crucial because currently up to eight lens systems are used to infer H0, and thus any biased input can severely affect the resulting H0 estimate. For this purpose we adopt Student’s t error for the input estimates. We investigate properties of the resulting H0 estimate via two simulation studies with realistic imaging data. It turns out that the meta-analysis can infer H0 with subpercent bias and about 1% level of coefficient of variation, even when 30% of inputs are manipulated to be outliers. We also apply the meta-analysis to three gravitationally lensed systems to obtain an H0 estimate and compare it with existing estimates. An R package h0 is publicly available for fitting the proposed meta-analysis.},
  archive      = {J_AOAS},
  author       = {Hyungsuk Tak and Xuheng Ding},
  doi          = {10.1214/24-AOAS1937},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3297-3317},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A robust bayesian meta-analysis for estimating the hubble constant via time delay cosmography},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian robust learning in chain graph models for
integrative pharmacogenomics. <em>AOAS</em>, <em>18</em>(4), 3274–3296.
(<a href="https://doi.org/10.1214/24-AOAS1936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrative analysis of multilevel pharmacogenomic data for modeling dependencies across various biological domains is crucial for developing genomic-testing based treatments. Chain graphs characterize conditional dependence structures of such multilevel data where variables are naturally partitioned into multiple ordered layers, consisting of both directed and undirected edges. Existing literature mostly focus on Gaussian chain graphs, which are ill-suited for nonnormal distributions with heavy-tailed marginals, potentially leading to inaccurate inferences. We propose a Bayesian robust chain graph model (RCGM) based on random transformations of marginals using Gaussian scale mixtures to account for node-level nonnormality in continuous multivariate data. This flexible modeling strategy facilitates identification of conditional sign dependencies among nonnormal nodes while still being able to infer conditional dependencies among normal nodes. In simulations we demonstrate that RCGM outperforms existing Gaussian chain graph inference methods in data generated from various nonnormal mechanisms. We apply our method to genomic, transcriptomic and proteomic data to understand underlying biological processes holistically for drug response and resistance in lung cancer cell lines. Our analysis reveals inter- and intra-platform dependencies of key signaling pathways to monotherapies of icotinib, erlotinib and osimertinib among other drugs, along with shared patterns of molecular mechanisms behind drug actions.},
  archive      = {J_AOAS},
  author       = {Moumita Chakraborty and Veerabhadran Baladandayuthapani and Anindya Bhadra and Min Jin Ha},
  doi          = {10.1214/24-AOAS1936},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3274-3296},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian robust learning in chain graph models for integrative pharmacogenomics},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent variable mixture model for
composition-on-composition regression with application to chemical
recycling. <em>AOAS</em>, <em>18</em>(4), 3253–3273. (<a
href="https://doi.org/10.1214/24-AOAS1935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is quite common to encounter compositional data in a regression framework in data analysis. When both responses and predictors are compositional, most existing models rely on a family of log-ratio based transformations to move the analysis from the simplex to the reals. This often makes the interpretation of the model more complex. A transformation-free regression model was recently developed, but it only allows for a single compositional predictor. However, many datasets include multiple compositional predictors of interest. Motivated by an application to hydrothermal liquefaction (HTL) data, a novel extension of this transformation-free regression model is provided that allows for two (or more) compositional predictors to be used via a latent variable mixture. A modified expectation-maximization algorithm is proposed to estimate model parameters, which are shown to have natural interpretations. Conformal inference is used to obtain prediction limits on the compositional response. The resulting methodology is applied to the HTL dataset. Extensions to multiple predictors are discussed.},
  archive      = {J_AOAS},
  author       = {Nicholas Rios and Lingzhou Xue and Xiang Zhan},
  doi          = {10.1214/24-AOAS1935},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3253-3273},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A latent variable mixture model for composition-on-composition regression with application to chemical recycling},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Extended beta models for poverty mapping. An application
integrating survey and remote sensing data in bangladesh. <em>AOAS</em>,
<em>18</em>(4), 3229–3252. (<a
href="https://doi.org/10.1214/24-AOAS1934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper targets the estimation of a poverty rate at the upazila level in Bangladesh through the use of demographic and health survey (DHS) data. Upazilas are administrative regions equivalent to counties or boroughs whose sample sizes are not large enough to provide reliable estimates or are even absent. We tackle this issue by proposing a small-area estimation model complementing survey data with remote sensing information at the area level. We specify an extended Beta mixed regression model within the Bayesian framework, allowing it to accommodate the peculiarities of sample data and to predict out-of-sample rates. Specifically, it enables to include estimates equal to either 0 or 1 and to model the strong intra-cluster correlation. We aim at proposing a method that can be implemented by statistical offices as a routine. In this spirit we consider a regularizing prior for coefficients, rather than a model selection approach, to deal with a large number of auxiliary variables. We compare our methods with existing alternatives using a design-based simulation exercise and illustrate its potential with the motivating application.},
  archive      = {J_AOAS},
  author       = {Silvia De Nicolò and Enrico Fabrizi and Aldo Gardini},
  doi          = {10.1214/24-AOAS1934},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3229-3252},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Extended beta models for poverty mapping. an application integrating survey and remote sensing data in bangladesh},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning risk preferences in markov decision processes: An
application to the fourth down decision in the national football league.
<em>AOAS</em>, <em>18</em>(4), 3205–3228. (<a
href="https://doi.org/10.1214/24-AOAS1933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For decades National Football League (NFL) coaches’ observed fourth down decisions have been largely inconsistent with prescriptions based on statistical models. In this paper we develop a framework to explain this discrepancy using an inverse optimization approach. We model the fourth down decision and the subsequent sequence of plays in a game as a Markov decision process (MDP), the dynamics of which we estimate from NFL play-by-play data from the 2014 through 2022 seasons. We assume that coaches’ observed decisions are optimal but that the risk preferences governing their decisions are unknown. This yields an inverse decision problem for which the optimality criterion, or risk measure, of the MDP is the estimand. Using the quantile function to parameterize risk, we estimate which quantile-optimal policy yields the coaches’ observed decisions as minimally suboptimal. In general, we find that coaches’ fourth-down behavior is consistent with optimizing low quantiles of the next-state value distribution, which corresponds to conservative risk preferences. We also find that coaches exhibit higher risk tolerances when making decisions in the opponent’s half of the field, as opposed to their own half, and that league average fourth down risk tolerances have increased over time.},
  archive      = {J_AOAS},
  author       = {Nathan Sandholtz and Lucas Wu and Martin Puterman and Timothy C. Y. Chan},
  doi          = {10.1214/24-AOAS1933},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3205-3228},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning risk preferences in markov decision processes: An application to the fourth down decision in the national football league},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reliability study of battery lives: A functional degradation
analysis approach. <em>AOAS</em>, <em>18</em>(4), 3185–3204. (<a
href="https://doi.org/10.1214/24-AOAS1931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Renewable energy is critical for combating climate change, whose first step is the storage of electricity generated from renewable energy sources. Li-ion batteries are a popular kind of storage units. Their continuous usage through charge-discharge cycles eventually leads to degradation. This can be visualized by plotting voltage discharge curves (VDCs) over discharge cycles. Studies of battery degradation have mostly concentrated on modeling degradation through one scalar measurement summarizing each VDC. Such simplification of curves can lead to inaccurate predictive models. Here we analyze the degradation of rechargeable Li-ion batteries from a NASA data set through modeling and predicting their full VDCs. With techniques from longitudinal and functional data analysis, we propose a new two-step predictive modeling procedure for functional responses residing on heterogeneous domains. We first predict the shapes and domain end points of VDCs using functional regression models. Then we integrate these predictions to perform a degradation analysis. Our functional approach allows the incorporation of usage information, produces predictions in a curve form and thus provides flexibility in the assessment of battery degradation. Through extensive simulation studies and cross-validated data analysis, our approach demonstrates better prediction than the existing approach of modeling degradation directly with aggregated data.},
  archive      = {J_AOAS},
  author       = {Youngjin Cho and Quyen Do and Pang Du and Yili Hong},
  doi          = {10.1214/24-AOAS1931},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3185-3204},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Reliability study of battery lives: A functional degradation analysis approach},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic topic language model on heterogeneous children’s
mental health clinical notes. <em>AOAS</em>, <em>18</em>(4), 3165–3184.
(<a href="https://doi.org/10.1214/24-AOAS1930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental health diseases which affect children’s lives and well-beings have received increased attention since the COVID-19 pandemic. Analyzing psychiatric clinical notes with topic models is critical to evaluating children’s mental status over time. However, few topic models are built for longitudinal settings, and most existing approaches fail to capture temporal trajectories for each document. To address these challenges, we develop a dynamic topic model with consistent topics and individualized temporal dependencies on the evolving document metadata. Our model preserves the semantic meaning of discovered topics over time and incorporates heterogeneity among documents. In particular, when documents can be categorized, we propose a classifier-free approach to maximize topic heterogeneity across different document groups. We also present an efficient variational optimization procedure adapted for the multistage longitudinal setting. In this case study, we apply our method to the psychiatric clinical notes from a large tertiary pediatric hospital in Southern California and achieve a 38% increase in the overall coherence of extracted topics. Our real data analysis reveals that children tend to express more negative emotions during state shutdowns and more positive when schools reopen. Furthermore, it suggests that sexual and gender minority (SGM) children display more pronounced reactions to major COVID-19 events and a greater sensitivity to vaccine-related news than non-SGM children. This study examines children’s mental health progression during the pandemic and offers clinicians valuable insights to recognize disparities in children’s mental health related to their sexual and gender identities.},
  archive      = {J_AOAS},
  author       = {Hanwen Ye and Tatiana Moreno and Adrianne Alpern and Louis Ehwerhemuepha and Annie Qu},
  doi          = {10.1214/24-AOAS1930},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3165-3184},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dynamic topic language model on heterogeneous children’s mental health clinical notes},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian model of underreporting for sexual assault on
college campuses. <em>AOAS</em>, <em>18</em>(4), 3146–3164. (<a
href="https://doi.org/10.1214/24-AOAS1928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an effort to quantify and combat sexual assault, U.S. colleges and universities are required to disclose the number of reported sexual assaults on their campuses each year. However, many instances of sexual assault are never reported to authorities, and consequently, the number of reported assaults does not fully reflect the true total number of assaults that occurred; the reported values could arise from many combinations of reporting rate and true incidence. In this paper we estimate these underlying quantities via a hierarchical Bayesian model of the reported number of assaults. We use informative priors, based on national crime statistics, to act as a tiebreaker to help distinguish between reporting rates and incidence. We outline a Hamiltonian Monte Carlo (HMC) sampling scheme for posterior inference regarding reporting rates and assault incidence at each school and apply this method to campus sexual assault data from 2014–2019. Results suggest an increasing trend in reporting rates for the overall college population during this time. However, the extent of underreporting varies widely across schools. That variation has implications for how individual schools should interpret their reported crime statistics.},
  archive      = {J_AOAS},
  author       = {Casey Bradshaw and David M. Blei},
  doi          = {10.1214/24-AOAS1928},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3146-3164},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian model of underreporting for sexual assault on college campuses},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Utilizing a capture–recapture strategy to accelerate
infectious disease surveillance. <em>AOAS</em>, <em>18</em>(4),
3130–3145. (<a href="https://doi.org/10.1214/24-AOAS1927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring key elements of disease dynamics (e.g., prevalence, case counts) is of great importance in infectious disease prevention and control, as emphasized during the COVID-19 pandemic. To facilitate this effort, we propose a new capture–recapture (CRC) analysis strategy that adjusts for misclassification stemming from the use of easily administered but imperfect diagnostic test kits, such as rapid antigen test-kits or saliva tests. Our method is based on a recently proposed “anchor stream” design, whereby an existing voluntary surveillance data stream is augmented by a smaller and judiciously drawn random sample. It incorporates manufacturer-specified sensitivity and specificity parameters to account for imperfect diagnostic results in one or both data streams. For inference to accompany case count estimation, we improve upon traditional Wald-type confidence intervals by developing an adapted Bayesian credible interval for the CRC estimator that yields favorable frequentist coverage properties. When feasible, the proposed design and analytic strategy provides a more efficient solution than traditional CRC methods or random sampling-based bias-corrected estimation to monitor disease prevalence while accounting for misclassification. We demonstrate the benefits of this approach through simulation studies and a numerical example that underscore its potential utility in practice for economical disease monitoring among a registered closed population.},
  archive      = {J_AOAS},
  author       = {Lin Ge and Yuzi Zhang and Lance Waller and Robert Lyles},
  doi          = {10.1214/24-AOAS1927},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3130-3145},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Utilizing a capture–recapture strategy to accelerate infectious disease surveillance},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiple change point detection in functional data with
applications to biomechanical fatigue data. <em>AOAS</em>,
<em>18</em>(4), 3109–3129. (<a
href="https://doi.org/10.1214/24-AOAS1926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Injuries to the lower extremity joints are often debilitating, particularly for professional athletes. Understanding the onset of stressful conditions on these joints is, therefore, important in order to ensure prevention of injuries as well as individualised training for enhanced athletic performance. We study the biomechanical joint angles from the hip, knee and ankle for runners who are experiencing fatigue. The data is cyclic in nature and densely collected by body-worn sensors, which makes it ideal to work with in the functional data analysis (FDA) framework. We develop a new method for multiple change point detection for functional data, which improves the state of the art with respect to at least two novel aspects. First, the curves are compared with respect to their maximum absolute deviation, which leads to a better interpretation of local changes in the functional data compared to classical L2-approaches. Second, as slight aberrations are to be often expected in a human movement data, our method will not detect arbitrarily small changes but hunts for relevant changes, where maximum absolute deviation between the curves exceeds a specified threshold, say Δ&gt;0. We recover multiple changes in a long functional time series of biomechanical knee angle data, which are larger than the desired threshold Δ, allowing us to identify changes purely due to fatigue. In this work we analyse data from both controlled indoor as well as from an uncontrolled outdoor (marathon) setting.},
  archive      = {J_AOAS},
  author       = {Patrick Bastian and Rupsa Basu and Holger Dette},
  doi          = {10.1214/24-AOAS1926},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3109-3129},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multiple change point detection in functional data with applications to biomechanical fatigue data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian modeling of insurance claims for hail damage.
<em>AOAS</em>, <em>18</em>(4), 3091–3108. (<a
href="https://doi.org/10.1214/24-AOAS1925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite its importance for insurance, there is almost no literature on statistical hail damage modeling. Statistical models for hailstorms exist, though they are generally not open-source, but no study appears to have developed a stochastic hail impact function. In this paper we use hail-related insurance claim data to build a Gaussian line process with extreme marks in order to model both the geographical footprint of a hailstorm and the damage to buildings that hailstones can cause. We build a model for the claim counts and claim values, and compare it to the use of a benchmark deterministic hail impact function. Our model proves to be better than the benchmark at capturing hail spatial patterns and allows for localized and extreme damage, which is seen in the insurance data. The evaluation of both the claim counts and value predictions shows that performance is improved compared to the benchmark, especially for extreme damage. Our model appears to be the first to provide realistic estimates for hail damage to individual buildings.},
  archive      = {J_AOAS},
  author       = {Ophélia Miralles and Anthony C. Davison},
  doi          = {10.1214/24-AOAS1925},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3091-3108},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian modeling of insurance claims for hail damage},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing marine mammal abundance: A novel data fusion.
<em>AOAS</em>, <em>18</em>(4), 3071–3090. (<a
href="https://doi.org/10.1214/24-AOAS1924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine mammals are increasingly vulnerable to human disturbance and climate change. Their diving behavior leads to limited visual access during data collection, making studying the abundance and distribution of marine mammals challenging. In theory, using data from more than one observation modality should lead to better informed predictions of abundance and distribution. With focus on North Atlantic right whales, we consider the fusion of two data sources to inform about their abundance and distribution. The first source is aerial distance sampling, which provides the spatial locations of whales detected in the region. The second source is passive acoustic monitoring (PAM), returning calls received at hydrophones placed on the ocean floor. Due to limited time on the surface and detection limitations arising from sampling effort, aerial distance sampling only provides a partial realization of locations. With PAM we never observe numbers or locations of individuals. To address these challenges, we develop a novel thinned point pattern data fusion. Our approach leads to improved inference regarding abundance and distribution of North Atlantic right whales throughout Cape Cod Bay, Massachusetts in the U.S. We demonstrate performance gains of our approach compared to that from a single source through both simulation and real data.},
  archive      = {J_AOAS},
  author       = {Erin M. Schliep and Alan E. Gelfand and Christopher W. Clark and Charles A. Mayo and Brigid McKenna and Susan E. Parks and Tina M. Yack and Robert S. Schick},
  doi          = {10.1214/24-AOAS1924},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3071-3090},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing marine mammal abundance: A novel data fusion},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hidden markov model for natural history of
colorectal cancer: Handling misclassified observations, varying
observation schemes and unobserved data. <em>AOAS</em>, <em>18</em>(4),
3050–3070. (<a href="https://doi.org/10.1214/24-AOAS1922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical modelling of individual-level event history data arising from varying observation schemes is a challenging problem, particularly due to unobserved and possibly misclassified individual states. Commonly used approaches rely on the hidden Markov models (HMM) to incorporate true underlying states. Each approach needs to account for the underlying data generating process and related external information and requires assumptions for estimation. This article develops a Bayesian HMM for natural history of colorectal cancer (CRC), combining data on latent disease states from randomised screening study and on observed clinical cancers from the population-based cancer registry. With our modelling approach and study design, we are able to provide estimates for latent state occupancy probabilities not only for screening-attenders but also for the control group and those who never attended screening—despite data on latent states only existing for the attenders. We use simulation-based calibration to ensure that posterior distributions can be reliably estimated despite the challenges brought in by the sampling scheme. We apply Bayesian computation to obtain posterior estimates of the quantities of interest. Two algorithms, Hamiltonian Monte Carlo (HMC) and Automatic Differentiation Variational Inference (ADVI), are applied and compared, first by using simulated data and then with a real data set. The modelling workflow can be applied for different cancer screening programmes and datasets which typically have similar challenges.},
  archive      = {J_AOAS},
  author       = {Aapeli Nevala and Sirpa Heinävaara and Tytti Sarkeala and Sangita Kulathinal},
  doi          = {10.1214/24-AOAS1922},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3050-3070},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian hidden markov model for natural history of colorectal cancer: Handling misclassified observations, varying observation schemes and unobserved data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modelling correlation matrices in multivariate data, with
application to reciprocity and complementarity of child-parent exchanges
of support. <em>AOAS</em>, <em>18</em>(4), 3024–3049. (<a
href="https://doi.org/10.1214/24-AOAS1921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define a model for the joint distribution of multiple continuous la- tent variables, which includes a model for how their correlations depend on explanatory variables. This is motivated by and applied to social scientific re- search questions in the analysis of intergenerational help and support within families, where the correlations describe reciprocity of help between genera- tions and complementarity of different kinds of help. We propose an MCMC procedure for estimating the model which maintains the positive definiteness of the implied correlation matrices and describe theoretical results which jus- tify this approach and facilitate efficient implementation of it. The model is applied to data from the UK Household Longitudinal Study to analyse ex- changes of practical and financial support between adult individuals and their noncoresident parents.},
  archive      = {J_AOAS},
  author       = {Siliang Zhang and Jouni Kuha and Fiona Steele},
  doi          = {10.1214/24-AOAS1921},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3024-3049},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modelling correlation matrices in multivariate data, with application to reciprocity and complementarity of child-parent exchanges of support},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Communication network dynamics in a large organizational
hierarchy. <em>AOAS</em>, <em>18</em>(4), 3007–3023. (<a
href="https://doi.org/10.1214/24-AOAS1919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most businesses impose a supervisory hierarchy on employees to facilitate management, decision-making, and collaboration, yet routine inter-employee communication patterns within workplaces tend to emerge more naturally as a consequence of both supervisory relationships and the needs of the organization. What then is the relationship between a formal organizational structure and the emergent communications between its employees? Understanding the nature of this relationship is critical for the successful management of an organization. While scholars of organizational management have proposed theories relating organizational trees to communication dynamics, and separately, network scientists have studied the topological structure of communication patterns in different types of organizations; existing empirical analyses are both lacking in representativeness and limited in size. In fact, much of the methodology used to study the relationship between organizational hierarchy and communication patterns (and much of what is known about this relationship) comes from analyses of the Enron email corpus, reflecting a uniquely dysfunctional corporate environment. In this paper we develop new methodology for assessing the relationship between organizational hierarchy and communication dynamics and apply it to Microsoft Corporation, currently the highest valued company in the world, consisting of approximately 200,000 employees divided into 88 teams, organizational trees rooted at the senior leadership level. This reveals distinct communication network structures within and between teams. We then characterize the relationship of routine employee communication patterns to these team supervisory hierarchies, while empirically evaluating several theories of organizational management and performance. To do so, we propose new measures of communication reciprocity and new shortest-path distances for trees to track the frequency of messages passed up, down, and across the organizational hierarchy. By describing how communication clusters around the formal organization, we reveal the emergent communication dynamics between employees and the crucial role of position in the hierarchy.},
  archive      = {J_AOAS},
  author       = {Nathaniel Josephs and Sida Peng and Forrest W. Crawford},
  doi          = {10.1214/24-AOAS1919},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {3007-3023},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Communication network dynamics in a large organizational hierarchy},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Statistical curve models for inferring 3D chromatin
architecture. <em>AOAS</em>, <em>18</em>(4), 2979–3006. (<a
href="https://doi.org/10.1214/24-AOAS1917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing three-dimensional (3D) chromatin structure from conformation capture assays (such as Hi-C) is a critical task in computational biology, since chromatin spatial architecture plays a vital role in numerous cellular processes and direct imaging is challenging. Most existing algorithms that operate on Hi-C contact matrices produce reconstructed 3D configurations in the form of a polygonal chain. However, none of the methods exploit the fact that the target solution is a (smooth) curve in 3D: this contiguity attribute is either ignored or indirectly addressed by imposing spatial constraints that are challenging to formulate. In this paper we develop both B-spline and smoothing spline techniques for directly capturing this potentially complex 1D curve. We subsequently combine these techniques with a Poisson model for contact counts and compare their performance on a real data example. In addition, motivated by the sparsity of Hi-C contact data, especially when obtained from single-cell assays, we appreciably extend the class of distributions used to model contact counts. We build a general distribution-based metric scaling (DBMS) framework from which we develop zero-inflated and Hurdle Poisson models as well as negative binomial applications. Illustrative applications make recourse to bulk Hi-C data from IMR90 cells and single-cell Hi-C data from mouse embryonic stem cells.},
  archive      = {J_AOAS},
  author       = {Elena Tuzhilina and Trevor Hastie and Mark Segal},
  doi          = {10.1214/24-AOAS1917},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2979-3006},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Statistical curve models for inferring 3D chromatin architecture},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Background modeling for double higgs boson production:
Density ratios and optimal transport. <em>AOAS</em>, <em>18</em>(4),
2950–2978. (<a href="https://doi.org/10.1214/24-AOAS1916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is, therefore, a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events, where signal is unlikely to appear and where the background distribution is, therefore, identifiable. The background distribution can be estimated in these regions and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.},
  archive      = {J_AOAS},
  author       = {Tudor Manole and Patrick Bryant and John Alison and Mikael Kuusela and Larry Wasserman},
  doi          = {10.1214/24-AOAS1916},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2950-2978},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Background modeling for double higgs boson production: Density ratios and optimal transport},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multisite disease analytics with applications to estimating
COVID-19 undetected cases in canada. <em>AOAS</em>, <em>18</em>(4),
2928–2949. (<a href="https://doi.org/10.1214/24-AOAS1915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Even with daily case counts, the true scope of the COVID-19 pandemic in Canada is unknown due to undetected cases. We develop a novel multisite disease analytics model which estimates undetected cases using discrete-valued multivariate time series in the framework of Bayesian hidden Markov modelling techniques. We apply our multisite model to estimate the pandemic scope using publicly available disease count data including detected cases, recoveries among detected cases, and total deaths. These counts are used to estimate the case detection probability, the infection fatality rate through time, the probability of recovery, and several important population parameters including the rate of spread and importation of external cases. We estimate the total number of active COVID-19 cases per region of Canada for each reporting interval. We applied this multisite model Canada-wide to all provinces and territories, providing an estimate of the total COVID-19 burden for the 90 weeks from 23 April 2020 to 10 February 2022. We also applied this model to the five health authority regions of British Columbia, Canada, describing the pandemic in B.C. over the 31 weeks from 2 April 2020 to 30 October 2020.},
  archive      = {J_AOAS},
  author       = {Matthew R. P. Parker and Jiguo Cao and Laura L. E. Cowen and Lloyd T. Elliott and Junling Ma},
  doi          = {10.1214/24-AOAS1915},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2928-2949},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multisite disease analytics with applications to estimating COVID-19 undetected cases in canada},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Models with observation error and temporary emigration for
count data. <em>AOAS</em>, <em>18</em>(4), 2909–2927. (<a
href="https://doi.org/10.1214/24-AOAS1911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data at surveyed sites are an important monitoring tool for several species around the world. However, the raw count data are an underestimate of the size of the monitored population at any one time, as individuals can temporarily leave the site (temporary emigration, TE) and because the probability of detection of individuals, even when using the site, is typically much lower than one (observation error). In this paper we develop a novel modelling framework for estimating population size, from count data, while accounting for both TE and observation error. Our framework builds on the popular class of N-mixture models but extends them in a number of ways. Specifically, we introduce two model classes for TE, a parametric, which relies on temporal models, and a nonparametric, which relies on Dirichlet process mixture models. Both model classes give rise to interesting ecological interpretations of the TE pattern while being parsimonious in terms of the number of parameters required to model the pattern. When accounting for observation error, we use mixed-effects models and implement an efficient Bayesian variable selection algorithm for identifying important predictors for the probability of detection. We demonstrate our new modelling framework using an extensive simulation study, which highlights the importance of using mixed-effects models for the probability of detection and illustrates the performance of the model when estimating population size and underlying TE patterns. We also assess the ability of the corresponding variable selection algorithm to identify important predictors under different scenarios for observation error and its corresponding model. When fitted to two motivating data sets of parrots counted at their roosts, our results provide new insights into how each species uses the roost throughout the year, on changes in population size between and within years, and on observation error.},
  archive      = {J_AOAS},
  author       = {Fabian R. Ketwaroo and Eleni Matechou and Rebecca Biddle and Simon Tollington and Maria L. Da Silva},
  doi          = {10.1214/24-AOAS1911},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2909-2927},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Models with observation error and temporary emigration for count data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Early effects of 2014 u.s. Medicaid expansions on mortality:
Design-based inference for impacts on small subgroups despite small-cell
suppression. <em>AOAS</em>, <em>18</em>(4), 2887–2908. (<a
href="https://doi.org/10.1214/24-AOAS1910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since 2014, states in the U.S. can choose whether to adopt Medicaid expansion as part of the Affordable Care Act (ACA), relaxing eligibility requirements. This heterogeneity in policy adoption between states raises the question—would there be a difference in health outcomes for states that have not expanded insurance access if they did expand Medicaid eligibility? In this study we estimate the effect of ACA Medicaid expansion on county-level all-cause mortality in the U.S. in 2014 overall and for subgroups relevant to the racial politics surrounding the ACA. We bring a causal approach to this challenge which emphasizes observational study design, including prespecifying all analyses, matching counties on pretreatment covariates, and employing design-based inference. A challenge facing analyses like this one is gaining access to mortality outcomes, as statistical agencies in the U.S. and elsewhere suppress cell counts of 10 or fewer in public use data. We develop a rank-sum test statistic accommodating outcomes that are coarsened in this way and that lends itself to design-based inference with county-aggregated data. As applied to impact analysis of the ACA’s Medicaid expansion, the proposed method’s inferences from coarsened, publicly available data are substantively the same as those that would be drawn from the complete, restricted-access data.},
  archive      = {J_AOAS},
  author       = {Charlotte Z. Mann and Ben B. Hansen and Lauren Gaydosh},
  doi          = {10.1214/24-AOAS1910},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2887-2908},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Early effects of 2014 U.S. medicaid expansions on mortality: Design-based inference for impacts on small subgroups despite small-cell suppression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Incorporating auxiliary information for improved statistical
inference and its extensions to distributed algorithms with an
application to personal credit. <em>AOAS</em>, <em>18</em>(4),
2863–2886. (<a href="https://doi.org/10.1214/24-AOAS1909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personal credits have always been a hot topic in the society. Among all of them, the evaluation of default risk is particularly concerned since robust estimation, based on personal information, can both help needy individuals to get loans and financial institutions to avoid losses. So far, there have been no good solutions due to limited data, especially default information. With the advent of the era of big data, it is possible to improve the effectiveness of estimates by using auxiliary information from external studies or public domains. However, the individual-level data can not be gained directly because of the emphasis on data privacy; that is, only some summarized statistics with auxiliary information are allowed to be shared. To effectively utilize external integrated auxiliary information to improve the accuracy of default risk estimation, this paper introduces a unified auxiliary information framework, which is referred as enhanced GEE method, to effectively incorporate various external summary results by employing the generalized estimating equations (GEE) approach and augmenting a weighted logarithm of confidence density on GEE function. We establish asymptotic properties for the new method and prove that it can achieve the gain of statistical efficiency compared to the study-specific estimator without any auxiliary information. Besides, a low-cost Map-Reduce procedure for the distributed statistical inference of enhanced GEE method in big data is developed that can achieve the same efficiency as the oracle enhanced GEE approach under mild condition. This method is demonstrated by an application to predict the loan default risk of bank customers in Shanghai and shown to be more effective and reliable compared with the method based on the own data only. Furthermore, the superiorities of our approach, especially the construction of the tighter confidence intervals, are also illustrated with extensive simulation studies and a real personal default risk case.},
  archive      = {J_AOAS},
  author       = {Miaomiao Yu and Zhongfeng Jiang and Jiaxuan Li and Yong Zhou},
  doi          = {10.1214/24-AOAS1909},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2863-2886},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Incorporating auxiliary information for improved statistical inference and its extensions to distributed algorithms with an application to personal credit},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Implicit generative prior for bayesian neural networks.
<em>AOAS</em>, <em>18</em>(4), 2840–2862. (<a
href="https://doi.org/10.1214/24-AOAS1908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive uncertainty quantification is crucial for reliable decision-making in various applied domains. Bayesian neural networks offer a powerful framework for this task. However, defining meaningful priors and ensuring computational efficiency remain significant challenges, especially for complex real-world applications. This paper addresses these challenges by proposing a novel neural adaptive empirical Bayes (NA-EB) framework. NA-EB leverages a class of implicit generative priors derived from low-dimensional distributions. This allows for efficient handling of complex data structures and effective capture of underlying relationships in real-world datasets. The proposed NA-EB framework combines variational inference with a gradient ascent algorithm. This enables simultaneous hyperparameter selection and approximation of the posterior distribution, leading to improved computational efficiency. We establish the theoretical foundation of the framework through posterior and classification consistency. We demonstrate the practical applications of our framework through extensive evaluations on a variety of tasks, including the two-spiral problem, regression, 10 UCI datasets, and image classification tasks on both MNIST and CIFAR-10 datasets. The results of our experiments highlight the superiority of our proposed framework over existing methods, such as sparse variational Bayesian and generative models, in terms of prediction accuracy and uncertainty quantification.},
  archive      = {J_AOAS},
  author       = {Yijia Liu and Xiao Wang},
  doi          = {10.1214/24-AOAS1908},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2840-2862},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Implicit generative prior for bayesian neural networks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Neural networks for extreme quantile regression with an
application to forecasting of flood risk. <em>AOAS</em>, <em>18</em>(4),
2818–2839. (<a href="https://doi.org/10.1214/24-AOAS1907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.},
  archive      = {J_AOAS},
  author       = {Olivier C. Pasche and Sebastian Engelke},
  doi          = {10.1214/24-AOAS1907},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2818-2839},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Neural networks for extreme quantile regression with an application to forecasting of flood risk},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Individual dynamic prediction for cure and survival based on
longitudinal biomarkers. <em>AOAS</em>, <em>18</em>(4), 2796–2817. (<a
href="https://doi.org/10.1214/24-AOAS1906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To optimize personalized treatment strategies and extend patients’ survival times, it is critical to accurately predict patients’ prognoses at all stages, from disease diagnosis to follow-up visits. The longitudinal biomarker measurements during visits are essential for this prediction purpose. Patients’ ultimate concerns are cure and survival. However, in many situations there is no clear biomarker indicator for cure. We propose a comprehensive joint model of longitudinal and survival data and a landmark cure model, incorporating proportions of potentially cured patients. The survival distributions in the joint and landmark models are specified through flexible hazard functions with the proportional hazards as a special case, allowing other patterns such as crossing hazard and survival functions. Formulas are provided for predicting each individual’s probabilities of future cure and survival at any time point based on his or her current biomarker history. Simulations show that, with these comprehensive and flexible properties, the proposed cure models outperform standard cure models in terms of predictive performance, measured by the time-dependent area under the curve of receiver operating characteristic, Brier score, and integrated Brier score. The use and advantages of the proposed models are illustrated by their application to a study of patients with chronic myeloid leukemia.},
  archive      = {J_AOAS},
  author       = {Can Xie and Xuelin Huang and Ruosha Li and Alexander Tsodikov and Kapil Bhalla},
  doi          = {10.1214/24-AOAS1906},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2796-2817},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Individual dynamic prediction for cure and survival based on longitudinal biomarkers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A new multiple-mediator model maximally uncovering the
mediation pathway: Evaluating the role of neuroimaging measures in
age-related cognitive decline. <em>AOAS</em>, <em>18</em>(4), 2775–2795.
(<a href="https://doi.org/10.1214/24-AOAS1905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aging changes brain functions and structures in a downward trajectory and consequently leads to a decline in neurocognitive performance. Our research is motivated by understanding whether and to what extent the age-effect on cognitive decline can be explained by neuroimaging measures. We consider a new mediation model with age as an independent variable, while treating neuroimaging data and cognitive function as the multiple mediators and outcome, respectively. Given that the brain is the primary organ responsible for cognitive function, it is neurobiologically intuitive that the age-related decline in cognition is largely mediated through neuroimaging measures. Additionally, cognitive function is localized to certain regions of the brain rather than being a function of the entire brain. Taking these factors into account, we propose a novel mediation model with multiple mediators that aims to maximally uncover the mediation pathway while simultaneously identifying active neuroimaging mediators by imposing an ℓ1 penalty and ℓ2 constraint. We develop a computationally efficient algorithm to handle the nonconvex optimization problem of penalized mediation proportion maximization. We apply our method to a data example of 37,441 participants of UK Biobank with cortical gray-matter thickness and white-matter integrity measures and cognitive performance scores. Our results show that the mediation effect of brain-imaging variables can explain 97% of age-related cognitive decline.},
  archive      = {J_AOAS},
  author       = {Hwiyoung Lee and Chixiang Chen and Peter Kochunov and L. Elliot Hong and Shuo Chen},
  doi          = {10.1214/24-AOAS1905},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2775-2795},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A new multiple-mediator model maximally uncovering the mediation pathway: Evaluating the role of neuroimaging measures in age-related cognitive decline},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Causal health impacts of power plant emission controls under
modeled and uncertain physical process interference. <em>AOAS</em>,
<em>18</em>(4), 2753–2774. (<a
href="https://doi.org/10.1214/24-AOAS1904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference with spatial environmental data is often challenging due to the presence of interference: outcomes for observational units depend on some combination of local and nonlocal treatment. This is especially relevant when estimating the effect of power plant emissions controls on population health, as pollution exposure is dictated by: (i) the location of point-source emissions as well as (ii) the transport of pollutants across space via dynamic physical-chemical processes. In this work we estimate the effectiveness of air quality interventions at coal-fired power plants in reducing two adverse health outcomes in Texas in 2016: pediatric asthma ED visits and Medicare all-cause mortality. We develop methods for causal inference with interference when the underlying network structure is not known with certainty and instead must be estimated from ancillary data. Notably, uncertainty in the interference structure is propagated to the resulting causal effect estimates. We offer a Bayesian, spatial mechanistic model for the interference mapping, which we combine with a flexible nonparametric outcome model to marginalize estimates of causal effects over uncertainty in the structure of interference. Our analysis finds some evidence that emissions controls at upwind power plants reduce asthma ED visits and all-cause mortality; however, accounting for uncertainty in the interference renders the results largely inconclusive.},
  archive      = {J_AOAS},
  author       = {Nathan B. Wikle and Corwin M. Zigler},
  doi          = {10.1214/24-AOAS1904},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2753-2774},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Causal health impacts of power plant emission controls under modeled and uncertain physical process interference},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Regularized scalar-on-function regression analysis to assess
functional association of critical physical activity window with
biological age. <em>AOAS</em>, <em>18</em>(4), 2730–2752. (<a
href="https://doi.org/10.1214/24-AOAS1903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerometry data enables scientists to extract personal digital features useful in precision health decision making. Existing analytic methods often begin with discretizing physical activity (PA) counts into activity categories via fixed cutoffs; however, the cutoffs are validated under restricted settings and cannot be generalized across studies. Here we develop a data-driven approach to overcome this bottleneck in the analysis of PA data in which we holistically summarize an individual’s PA profile using occupation-time curves that describe the percentage of time spent at or above a continuum of activity levels. The resulting functional curve is informative to capture time-course individual variability of PA. We investigate functional analytics under an L0 regularization approach, which handles highly correlated micro-activity windows that serve as predictors in a scalar-on-function regression model. We develop a new one-step method that simultaneously conducts fusion via change-point detection and parameter estimation through a new L0 constraint formulation, which is evaluated via simulation experiments and a data analysis assessing the influence of PA on biological aging.},
  archive      = {J_AOAS},
  author       = {Margaret Banker and Leyao Zhang and Peter X. K. Song},
  doi          = {10.1214/24-AOAS1903},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2730-2752},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Regularized scalar-on-function regression analysis to assess functional association of critical physical activity window with biological age},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Model-robust bayesian design through generalised additive
models for monitoring submerged shoals. <em>AOAS</em>, <em>18</em>(4),
2705–2729. (<a href="https://doi.org/10.1214/24-AOAS1898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal sampling strategies are critical for surveys of deeper coral reef and shoal systems due to the significant cost of accessing and field sampling these remote and poorly understood ecosystems. Additionally, well-established standard diver-based sampling techniques used in shallow reef systems are not feasible at greater depths. In this study, we develop a Bayesian design strategy to optimise sampling for a shoal deep reef system using three years of pilot data. Bayesian designs are typically found by maximising the expectation of a utility function with respect to the joint distribution of the parameters and the response conditional on an assumed statistical model. Unfortunately, specifying such a model a priori is difficult, as knowledge of the data-generating process is typically incomplete. To overcome this, our approach focuses on finding Bayesian designs that are robust to unknown model uncertainty. We achieve this by couching the specified model within a generalised additive modelling framework and formulating prior information that allows the additive component to capture discrepancies between what is assumed and the underlying data-generating process. The motivation for this is to enable Bayesian designs to be found under epistemic model uncertainty; a highly desirable property of Bayesian designs. Initially, we demonstrate our approach with an exemplar design problem, deriving a theoretical result to explore the properties of optimal designs. We then apply this approach to design future monitoring of submerged shoals off the north-west coast of Australia to improve current monitoring practices.},
  archive      = {J_AOAS},
  author       = {Dilishiya De Silva and Rebecca Fisher and Ben Radford and Helen Thompson and James McGree},
  doi          = {10.1214/24-AOAS1898},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2705-2729},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Model-robust bayesian design through generalised additive models for monitoring submerged shoals},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A copula model for marked point process with a terminal
event: An application in dynamic prediction of insurance claims.
<em>AOAS</em>, <em>18</em>(4), 2679–2704. (<a
href="https://doi.org/10.1214/24-AOAS1902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of an insurer’s outstanding liabilities is crucial for maintaining the financial health of the insurance sector. We aim to develop a statistical model for insurers to dynamically forecast unpaid losses by leveraging the granular transaction data on individual claims. The liability cash flow from a single insurance claim is determined by an event process that describes the recurrences of payments, a payment process that generates a sequence of payment amounts, and a settlement process that terminates both the event and payment processes. More importantly, the three components are dependent on one another, which enables the dynamic prediction of an insurer’s outstanding liability. We introduce a copula-based point process framework to model the recurrent events of payment transactions from an insurance claim, where the longitudinal payment amounts and the time-to-settlement outcome are formulated as the marks and the terminal event of the counting process, respectively. The dependencies among the three components are characterized using the method of pair copula constructions. We further develop a stagewise strategy for parameter estimation and illustrate its desirable properties with numerical experiments. In the application we consider a portfolio of property insurance claims for building and contents coverage obtained from a commercial property insurance provider, where we find intriguing dependence patterns among the three components. The superior dynamic prediction performance of the proposed joint model enhances the insurer’s decision-making in claims reserving and risk financing operations.},
  archive      = {J_AOAS},
  author       = {Lu Yang and Peng Shi and Shimeng Huang},
  doi          = {10.1214/24-AOAS1902},
  journal      = {The Annals of Applied Statistics},
  month        = {12},
  number       = {4},
  pages        = {2679-2704},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A copula model for marked point process with a terminal event: An application in dynamic prediction of insurance claims},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Integrating mendelian randomization with causal mediation
analyses for characterizing direct and indirect exposure-to-outcome
effects. <em>AOAS</em>, <em>18</em>(3), 2656–2677. (<a
href="https://doi.org/10.1214/24-AOAS1901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) assesses the total effect of exposure on outcome. With the rapidly increasing availability of summary statistics from genome-wide association studies (GWASs), MR leverages existing summary statistics and is widely used to study the causal effects among complex traits and diseases. The total effect in the population is a sum of indirect and direct effects. For complex disease outcomes with complicated etiologies and/or for modifiable exposure traits, there may exist more than one pathway between exposure and outcome. The direct effect and the indirect effect via a mediator of interest could be in opposite directions, and the total effect estimates may not be informative for treatment and prevention decision-making or may even be misleading for different subgroups of patients. Causal mediation analysis delineates the indirect effect of exposure on outcome operating through the mediator and the direct effect transmitted through other mechanisms. However, causal mediation analysis often requires individual-level data measured on exposure, outcome, mediator and confounding variables, and the power of the mediation analysis is restricted by sample size. In this work, motivated by a study of the effects of atrial fibrillation (AF) on Alzheimer’s dementia, we propose a framework for Integrative Mendelian randomization and Mediation Analysis (IMMA). The proposed method integrates the total effect estimates from MR analyses based on large-scale GWASs with the direct and indirect effect estimates from mediation analysis based on individual-level data of a limited sample size. We introduce a series of IMMA models under the scenarios with or without exposure-mediator interaction and/or study heterogeneity. The proposed IMMA models improve the estimation and the power of inference on the direct and indirect effects in the population. Our analyses showed a significant positive direct effect of AF on Alzheimer’s dementia risk not through the use of the oral anticoagulant treatment and a significant indirect effect of AF-induced anticoagulant treatment in reducing Alzheimer’s dementia risk. The results suggested potential Alzheimer’s dementia risk prediction and prevention strategies for AF patients and paved the way for future reevaluation of anticoagulant treatment guidelines for AF patients. A sensitivity analysis was conducted to assess the sensitivity of the conclusions to a key assumption of the IMMA approach.},
  archive      = {J_AOAS},
  author       = {Fan Yang and Lin S. Chen and Shahram Oveisgharan and Dawood Darbar and David A. Bennett},
  doi          = {10.1214/24-AOAS1901},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2656-2677},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Integrating mendelian randomization with causal mediation analyses for characterizing direct and indirect exposure-to-outcome effects},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prediction intervals for economic fixed-event forecasts.
<em>AOAS</em>, <em>18</em>(3), 2635–2655. (<a
href="https://doi.org/10.1214/24-AOAS1900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fixed-event forecasting setup is common in economic policy. It involves a sequence of forecasts of the same (“fixed”) predictand so that the difficulty of the forecasting problem decreases over time. Fixed-event point forecasts are typically published without a quantitative measure of uncertainty. To construct such a measure, we consider forecast postprocessing techniques tailored to the fixed-event case. We develop regression methods that impose constraints motivated by the problem at hand and use these methods to construct prediction intervals for gross domestic product (GDP) growth in Germany and the U.S.},
  archive      = {J_AOAS},
  author       = {Fabian Krüger and Hendrik Plett},
  doi          = {10.1214/24-AOAS1900},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2635-2655},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Prediction intervals for economic fixed-event forecasts},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Are made and missed different? An analysis of field goal
attempts of professional basketball players via depth based testing
procedure. <em>AOAS</em>, <em>18</em>(3), 2615–2634. (<a
href="https://doi.org/10.1214/24-AOAS1899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we develop a novel depth-based testing procedure on spatial point processes to examine the difference in made and missed field goal attempts for NBA players. Specifically, our testing procedure can statistically detect the differences between made and missed field goal attempts for NBA players. We first obtain the depths of two processes under the polar coordinate system. A two-dimensional Kolmogorov–Smirnov test is then performed to test the difference between the depths of the two processes. Throughout extensive simulation studies, we show our testing procedure with good frequentist properties under both null hypothesis and alternative hypothesis. A comparison against the competing methods shows that our proposed procedure has better testing reliability and testing power. Application to the shot chart data of 191 NBA players in the 2017–2018 regular season offers interesting insights about these players’ made and missed shot patterns.},
  archive      = {J_AOAS},
  author       = {Kai Qi and Guanyu Hu and Wei Wu},
  doi          = {10.1214/24-AOAS1899},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2615-2634},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Are made and missed different? an analysis of field goal attempts of professional basketball players via depth based testing procedure},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bivariate functional patterns of lifetime medicare costs
among ESRD patients. <em>AOAS</em>, <em>18</em>(3), 2596–2614. (<a
href="https://doi.org/10.1214/24-AOAS1897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we study the lifetime Medicare spending patterns of patients with end-stage renal disease (ESRD). We extract the information of patients who started their ESRD services in 2007–2011 from the United States Renal Data System (USRDS). Patients are partitioned into three groups based on their kidney transplant status: 1-unwaitlisted and never transplanted, 2-waitlisted but never transplanted, and 3-waitlisted and then transplanted. To study their Medicare cost trajectories, we use a semiparametric regression model with both fixed and bivariate time-varying coefficients to compare groups 1 and 2 as well as a bivariate time-varying coefficient model with different starting times (time since the first ESRD service and time since the kidney transplant) to compare groups 2 and 3. In addition to demographics and other medical conditions, these regression models are conditional on the survival time, which ideally depict the lifetime Medicare spending patterns. For estimation we extend the profile weighted least squares (PWLS) estimator to longitudinal data for the first comparison and propose a two-stage estimating method for the second comparison. We use sandwich variance estimators to construct confidence intervals and validate inference procedures through simulations. Our analysis of the Medicare claims data reveals that waitlisting is associated with a lower daily medical cost at the beginning of ESRD service among waitlisted patients, which gradually increases over time. Averaging over lifespan, however, there is no difference between waitlisted and unwaitlisted groups. A kidney transplant, on the other hand, reduces the medical cost significantly after an initial spike.},
  archive      = {J_AOAS},
  author       = {Yue Wang and Bin Nan and John D. Kalbfleisch},
  doi          = {10.1214/24-AOAS1897},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2596-2614},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bivariate functional patterns of lifetime medicare costs among ESRD patients},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weighted biomarker variability in joint analysis of
longitudinal and time-to-event data. <em>AOAS</em>, <em>18</em>(3),
2576–2595. (<a href="https://doi.org/10.1214/24-AOAS1896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the clinical evidence that the biomarker variability may have prognostic value for a related disease, we extend the standard joint model for longitudinal and time-to-event outcomes to incorporate the weighted cumulative effects of both biomarker level and variability on the survival hazard. A mixed-effects model is specified for biomarker observations wherein the subject-specific trajectories are modelled by spline functions with random coefficients. Borrowing ideas from smoothing splines, we propose a new variability measure which characterizes the roughness of the subject-specific biomarker trajectory by the integrated amount of its second derivatives over time. The inclusion of weight functions in cumulative quantities permits the importance of biomarker history to vary with time. To reduce computational complexity, we confine the weight functions to a particular parametric family with scale parameters to be estimated. Asymptotic properties of maximum likelihood estimators are established with a discussion on the identification issue of the scale parameters. We use EM algorithm in estimation with initial values obtained from a two-stage method. Simulation studies have been conducted under different settings. Finally, we apply our model to investigate the weighted cumulative effects of systolic blood pressure level and variability on cardiovascular events in the Medical Research Council trial.},
  archive      = {J_AOAS},
  author       = {Chunyu Wang and Jiaming Shen and Christiana Charalambous and Jianxin Pan},
  doi          = {10.1214/24-AOAS1896},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2576-2595},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Weighted biomarker variability in joint analysis of longitudinal and time-to-event data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A flexible model for correlated count data, with application
to multicondition differential expression analyses of single-cell RNA
sequencing data. <em>AOAS</em>, <em>18</em>(3), 2551–2575. (<a
href="https://doi.org/10.1214/24-AOAS1894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting differences in gene expression is an important part of single-cell RNA sequencing experiments, and many statistical methods have been developed for this aim. Most differential expression analyses focus on comparing expression between two groups (e.g., treatment vs. control). But there is increasing interest in multicondition differential expression analyses in which expression is measured in many conditions and the aim is to accurately detect and estimate expression differences in all conditions. We show that directly modeling single-cell RNA-seq counts in all conditions simultaneously, while also inferring how expression differences are shared across conditions, leads to greatly improved performance for detecting and estimating expression differences compared to existing methods. We illustrate the potential of this new approach by analyzing data from a single-cell experiment studying the effects of cytokine stimulation on gene expression. We call our new method “Poisson multivariate adaptive shrinkage,” and it is implemented in an R package available at https://github.com/stephenslab/poisson.mash.alpha.},
  archive      = {J_AOAS},
  author       = {Yusha Liu and Peter Carbonetto and Michihiro Takahama and Adam Gruenbaum and Dongyue Xie and Nicolas Chevrier and Matthew Stephens},
  doi          = {10.1214/24-AOAS1894},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2551-2575},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A flexible model for correlated count data, with application to multicondition differential expression analyses of single-cell RNA sequencing data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Mixture conditional regression with ultrahigh dimensional
text data for estimating extralegal factor effects. <em>AOAS</em>,
<em>18</em>(3), 2532–2550. (<a
href="https://doi.org/10.1214/24-AOAS1893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing judicial impartiality is a problem of fundamental importance in empirical legal studies for which standard regression methods have been popularly used to estimate the extralegal factor effects. However, those methods cannot handle control variables with ultrahigh dimensionality, such as those found in judgment documents recorded in text format. To solve this problem, we develop a novel mixture conditional regression (MCR) approach, assuming that the whole sample can be classified into a number of latent classes. Within each latent class, a standard linear regression model can be used to model the relationship between the response and a key feature vector, which is assumed to be of a fixed dimension. Meanwhile, ultrahigh dimensional control variables are then used to determine the latent class membership, where a naïve Bayes type model is used to describe the relationship. Hence, the dimension of control variables is allowed to be arbitrarily high. A novel expectation-maximization algorithm is developed for model estimation. Therefore, we are able to estimate the key parameters of interest as efficiently as if the true class membership were known in advance. Simulation studies are presented to demonstrate the proposed MCR method. A real dataset of Chinese burglary offenses is analyzed for illustration purposes.},
  archive      = {J_AOAS},
  author       = {Jiaxin Shi and Fang Wang and Yuan Gao and Xiaojun Song and Hansheng Wang},
  doi          = {10.1214/24-AOAS1893},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2532-2550},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Mixture conditional regression with ultrahigh dimensional text data for estimating extralegal factor effects},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian sparse vector autoregressive switching models with
application to human gesture phase segmentation. <em>AOAS</em>,
<em>18</em>(3), 2511–2531. (<a
href="https://doi.org/10.1214/24-AOAS1892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sparse vector autoregressive (VAR) hidden semi-Markov model (HSMM) for modeling temporal and contemporaneous (e.g., spatial) dependencies in multivariate nonstationary time series. The HSMM’s generic state distribution is embedded in a special transition matrix structure, facilitating efficient likelihood evaluations and arbitrary approximation accuracy. To promote sparsity of the VAR coefficients, we deploy an l1-ball projection prior, which combines differentiability with a positive probability of obtaining exact zeros, achieving variable selection within each switching state. This also facilitate posterior estimation via HMC. We further place nonlocal priors on the parameters of the HSMM dwell distribution improving the ability of Bayesian model selection to distinguish whether the data is better supported by the simpler hidden Markov model (HMM) or more flexible HSMM. Our proposed methodology is illustrated via an application to human gesture phase segmentation based on sensor data, where we successfully identify and characterize the periods of rest and active gesturing as well as the dynamical patterns involved in the gesture movements associated with each of these states.},
  archive      = {J_AOAS},
  author       = {Beniamino Hadj-Amar and Jack Jewson and Marina Vannucci},
  doi          = {10.1214/24-AOAS1892},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2511-2531},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian sparse vector autoregressive switching models with application to human gesture phase segmentation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A forensic statistical analysis of fraud in the federal food
stamp program. <em>AOAS</em>, <em>18</em>(3), 2486–2510. (<a
href="https://doi.org/10.1214/24-AOAS1891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study develops methods to detect anomalous transactions linked with fraud in food stamp purchases through order statistics methods. The methods detect clusters in the order statistics of the transaction amounts that merit further scrutiny. Our techniques use scan statistics to determine when an excessive number of transactions occur (cluster), which is historically linked to fraud. A scoring paradigm is constructed that ranks the degree in which detected clusters and individual transactions are anomalous among approximately 250 million total transactions.},
  archive      = {J_AOAS},
  author       = {Jonathan Woody and Zhicong Zhao and Robert Lund and Tung-Lung Wu},
  doi          = {10.1214/24-AOAS1891},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2486-2510},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A forensic statistical analysis of fraud in the federal food stamp program},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent level correlation modeling of multivariate
discrete-valued financial time series. <em>AOAS</em>, <em>18</em>(3),
2462–2485. (<a href="https://doi.org/10.1214/24-AOAS1890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-frequency financial data, dynamic patterns of transaction counts in regular time intervals provide crucial insights into market microstructure, such as short-term trading activities and intermittent intensities of price oscillation. In this paper we propose a Bayesian hierarchical framework that incorporates correlated latent level and temporal effects to model multivariate count data during intraday transaction intervals. Built on the INLA method for implementation, our framework proves to be competitive with the traditional MCMC approach in terms of model inference and computational cost. We demonstrate the efficacy of our methodology by applying it to assets from three Global Industry Classification Standard (GICS) sectors, namely, healthcare, energy, and industrials. The analysis uncovers various microstructures of financial count data using our framework. Specifically, our model featuring a correlated latent effect structure adeptly captures the pattern of the empirical correlations within the count data patterns with additional statistical inference, such as assessing different associations between short-term averaged trading size as well as trading duration, the counts at different risk levels, and uncovering differential levels of uncertainty resulted from market temporal behavior and unobservable latent effects across the three sectors. We also discuss some potential applications of our framework in real-world scenarios.},
  archive      = {J_AOAS},
  author       = {Yanzhao Wang and Haitao Liu and Jian Zou and Nalini Ravishanker},
  doi          = {10.1214/24-AOAS1890},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2462-2485},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Latent level correlation modeling of multivariate discrete-valued financial time series},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint modeling of multistate and nonparametric multivariate
longitudinal data. <em>AOAS</em>, <em>18</em>(3), 2444–2461. (<a
href="https://doi.org/10.1214/24-AOAS1889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is oftentimes the case in studies of disease progression that subjects can move into one of several disease states of interest. Multistate models are an indispensable tool to analyze data from such studies. The Environmental Determinants of Diabetes in the Young (TEDDY) is an observational study of at-risk children from birth to onset of type-1 diabetes (T1D) up through the age of 15. A joint model for simultaneous inference of multistate and multivariate nonparametric longitudinal data is proposed to analyze data and answer the research questions brought up in the study. The proposed method allows us to make statistical inferences, test hypotheses, and make predictions about future state occupation in the TEDDY study. The performance of the proposed method is evaluated by simulation studies. The proposed method is applied to the motivating example to demonstrate the capabilities of the method.},
  archive      = {J_AOAS},
  author       = {Lu You and Falastin Salami and Carina Törn and Åke Lernmark and Roy Tamura},
  doi          = {10.1214/24-AOAS1889},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2444-2461},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint modeling of multistate and nonparametric multivariate longitudinal data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Site occupancy and abundance models for analyzing
multiple-visit detection/nondetection data. <em>AOAS</em>,
<em>18</em>(3), 2424–2443. (<a
href="https://doi.org/10.1214/24-AOAS1888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an enhanced site occupancy model for analyzing ecological detection/nondetection data obtained from multiple visits. The model distinguishes between abundance, occupancy, and detection probabilities. We allow for transient individuals through a community parameter, c, that characterizes the proportion of individuals fixed across visits. This parameter seamlessly transitions from the standard occupancy model (c=0) to the N-mixture model (c=1), enabling a more accurate analysis of site occupancy data. Through theoretical developments and simulation studies, we demonstrate how this model effectively addresses biases inherent in conventional approaches, particularly for c is not at 0 or 1. We apply the model to various datasets of mammal and bird species and compare it to current approaches.},
  archive      = {J_AOAS},
  author       = {Huu-Dinh Huynh and Matthew Schofield and Wen-Han Hwang},
  doi          = {10.1214/24-AOAS1888},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2424-2443},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Site occupancy and abundance models for analyzing multiple-visit detection/nondetection data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Evaluation of transplant benefits with the u.s. Scientific
registry of transplant recipients by semiparametric regression of mean
residual life. <em>AOAS</em>, <em>18</em>(3), 2403–2423. (<a
href="https://doi.org/10.1214/24-AOAS1887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney transplantation is the most effective renal replacement therapy for end stage renal disease patients. With the severe shortage of kidney supplies and for the clinical effectiveness of transplantation, patient’s life expectancy posttransplantation is used to prioritize patients for transplantation; however, severe comorbidity conditions and old age are the most dominant factors that negatively impact posttransplantation life expectancy, effectively precluding sick or old patients from receiving transplants. It would be crucial to design objective measures to quantify the transplantation benefit by comparing the mean residual life with and without a transplant, after adjusting for comorbidity and demographic conditions. To address this urgent need, we propose a new class of semiparametric covariate-dependent mean residual life models. Our method estimates covariate effects semiparametrically efficiently and the mean residual life function nonparametrically, enabling us to predict the residual life increment potential for any given patient. Our method potentially leads to a more fair system that prioritizes patients who would have the largest residual life gains. Our analysis of the kidney transplant data from the U.S. Scientific Registry of Transplant Recipients also suggests that a single index of covariates summarize well the impacts of multiple covariates, which may facilitate interpretations of each covariate’s effect. Our subgroup analysis further disclosed inequalities in survival gains across groups defined by race, gender and insurance type (reflecting socioeconomic status).},
  archive      = {J_AOAS},
  author       = {Ge Zhao and Yanyuan Ma and Huazhen Lin and Yi Li},
  doi          = {10.1214/24-AOAS1887},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2403-2423},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Evaluation of transplant benefits with the U.S. scientific registry of transplant recipients by semiparametric regression of mean residual life},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sparse clustering for customer segmentation with
high-dimensional mixed-type data. <em>AOAS</em>, <em>18</em>(3),
2382–2402. (<a href="https://doi.org/10.1214/24-AOAS1886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Customer segmentation has wide applications in business activities, such as personalized marketing and targeted product development. To realize customer segmentation, clustering methods are commonly used. However, modern customer segmentation encounters challenges characterized by high-dimensionality and mixed-type variables (i.e., the mixture of continuous variables and categorical variables). It brings great challenges to customer segmentation, because most existing clustering methods are only designed for data with one single type of variables. Furthermore, the existence of noise variables highlights the necessity of simultaneous variable selection and data clustering. Motivated by these issues, we develop a Davies–Bouldin index based sparse clustering (DBI-SC) method for customer segmentation with high-dimensional mixed-type data. In this method we define dissimilarity measures for continuous variables and categorical variables separately. Then an adjusted DBI criterion is designed to measure the contribution of each variable to clustering. For variable selection we apply the sparse clustering framework and introduce different penalty parameters for the mixed-type variables. The screening consistency property of the DBI-SC method is also investigated. Extensive simulation studies demonstrate the satisfactory performance of the DBI-SC method in both clustering and variable selection. Finally, a designated driving service dataset is analyzed for customer segmentation using the proposed method.},
  archive      = {J_AOAS},
  author       = {Feifei Wang and Shaodong Xu and Yichen Qin and Ye Shen and Yang Li},
  doi          = {10.1214/24-AOAS1886},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2382-2402},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sparse clustering for customer segmentation with high-dimensional mixed-type data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent variable approach for modeling relational data with
multiple receivers. <em>AOAS</em>, <em>18</em>(3), 2359–2381. (<a
href="https://doi.org/10.1214/24-AOAS1885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directional relational event data, such as email data, often contain unicast messages (i.e., messages of one sender toward one receiver) and multicast messages (i.e., messages of one sender toward multiple receivers). The Enron email data that is the focus in this paper consists of 31% multicast messages. Multicast messages contain important information about the roles of actors in the network, which is needed for better understanding social interaction dynamics. In this paper a multiplicative latent factor model is proposed to analyze such relational data. For a given message, all potential receiver actors are placed on a suitability scale, and the actors are included in the receiver set whose suitability score exceeds a threshold value. Unobserved heterogeneity in the social interaction behavior is captured using a multiplicative latent factor structure with latent variables for actors (which differ for actors as senders and receivers) and latent variables for individual messages. The model is referred to as the multicast additive and multiplicative effects network (mc-amen) model. A Bayesian computational algorithm, which relies on Gibbs sampling, is proposed for model fitting. Model assessment is done using posterior predictive checks. Numerical simulations show that the model is widely applicable for various scenarios involving multicast messages. Furthermore, a mc-amen model with a two-dimensional latent variable can accurately capture the empirical distribution of the cardinality of the receiver set and the composition of the receiver sets for commonly observed messages in the Enron email data. In the Enron network, actors have a comparable (but not identical) role as a sender and as a receiver in the network.},
  archive      = {J_AOAS},
  author       = {Joris Mulder and Peter D. Hoff},
  doi          = {10.1214/24-AOAS1885},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2359-2381},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A latent variable approach for modeling relational data with multiple receivers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing aquatic toxicity assessment via a clustered
variance model. <em>AOAS</em>, <em>18</em>(3), 2342–2358. (<a
href="https://doi.org/10.1214/24-AOAS1884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the need to assess consistency in the outcomes of aquatic toxicity tests conducted by different labs at different time points, we propose a clustering of variance method in linear mixed models. The proposed method, referred as CVM, is able to identify the cluster structure of the variances and estimate model parameters simultaneously. In our proposed method, a penalized approach based on pairwise penalties is proposed to identify the cluster structure. We construct an optimization problem and develop an algorithm based on the alternating direction method of multipliers. Simulation studies show that the proposed approach can identify the cluster structure well and outperforms traditional methods based on k-means. In the end, the proposed approach is applied to the aquatic toxicity assessment data, which gives a more reasonable cluster structure than the traditional methods.},
  archive      = {J_AOAS},
  author       = {Xin Wang and Jing Zhang},
  doi          = {10.1214/24-AOAS1884},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2342-2358},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing aquatic toxicity assessment via a clustered variance model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian joint modeling of high-dimensional discrete
multivariate longitudinal data using generalized linear mixed models.
<em>AOAS</em>, <em>18</em>(3), 2326–2341. (<a
href="https://doi.org/10.1214/24-AOAS1883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In routine cancer care, various patient- and clinician-reported symptoms are collected throughout treatment. This informs a crucial part of clinical research, particularly in studying the factors associated with symptom underascertainment. To jointly analyze such discrete, multivariate, and potentially high-dimensional repeated measures, we propose a Bayesian longitudinal generalized linear mixed model (BLGLMM). This model integrates three key methodologies: a low-rank matrix decomposition to approximate the high-dimensional regression coefficient matrix, a sparse factor model to capture the dependence among multiple outcomes, and random effects to account for the dependence among repeated responses. Posterior computation is performed using an efficient Markov chain Monte Carlo algorithm. We conduct simulations and provide an illustrative example examining the factors associated with symptom underascertainment in prostate cancer patients to demonstrate the efficacy and utility of our proposed method.},
  archive      = {J_AOAS},
  author       = {Paloma Hauser and Xianming Tan and Fang Chen and Ronald C. Chen and Joseph G. Ibrahim},
  doi          = {10.1214/24-AOAS1883},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2326-2341},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian joint modeling of high-dimensional discrete multivariate longitudinal data using generalized linear mixed models},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric modeling of SARS-CoV-2 transmission using
tests, cases, deaths, and seroprevalence data. <em>AOAS</em>,
<em>18</em>(3), 2307–2325. (<a
href="https://doi.org/10.1214/24-AOAS1882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mechanistic models fit to streaming surveillance data are critical for understanding the transmission dynamics of an outbreak as it unfolds in real-time. However, transmission model parameter estimation can be imprecise, sometimes even impossible, because surveillance data are noisy and not informative about all aspects of the mechanistic model. To partially overcome this obstacle, Bayesian models have been proposed to integrate multiple surveillance data streams. We devised a modeling framework for integrating SARS-CoV-2 diagnostics test and mortality time series data as well as seroprevalence data from cross-sectional studies and tested the importance of individual data streams for both inference and forecasting. Importantly, our model for incidence data accounts for changes in the total number of tests performed. We apply our Bayesian data integration method to COVID-19 surveillance data collected in Orange County, California, between March 2020 and February 2021 and find that 32–72% of the Orange County residents experienced SARS-CoV-2 infection by mid-January, 2021. Despite this high number of infections, our results suggest that the abrupt end of the winter surge in January 2021 was due to both behavioral changes and a high level of accumulated natural immunity.},
  archive      = {J_AOAS},
  author       = {Damon Bayer and Isaac H. Goldstein and Jonathan Fintzi and Keith Lumbard and Emily Ricotta and Sarah Warner and Jeffrey R Strich and Daniel S. Chertow and Lindsay M. Busch and Daniel M. Parker and Bernadette Boden-Albala and Richard Chhuon and Matthew Zahn and Nichole Quick and Alissa Dratch and Volodymyr M. Minin},
  doi          = {10.1214/24-AOAS1882},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2307-2325},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric modeling of SARS-CoV-2 transmission using tests, cases, deaths, and seroprevalence data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric linear regression with an interval-censored
covariate in the atherosclerosis risk in communities study.
<em>AOAS</em>, <em>18</em>(3), 2295–2306. (<a
href="https://doi.org/10.1214/24-AOAS1881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In longitudinal studies, investigators are often interested in understanding how the time since the occurrence of an intermediate event affects a future outcome. The intermediate event is often asymptomatic such that its occurrence is only known to lie in a time interval induced by periodic examinations. We propose a linear regression model that relates the time since the occurrence of the intermediate event to a continuous response at a future time point through a rectified linear unit activation function while formulating the distribution of the time to the occurrence of the intermediate event through the Cox proportional hazards model. We consider nonparametric maximum likelihood estimation with an arbitrary sequence of examination times for each subject. We present an EM algorithm that converges stably for arbitrary datasets. The resulting estimators of regression parameters are consistent, asymptotically normal, and asymptotically efficient. We assess the performance of the proposed methods through extensive simulation studies and provide an application to the Atherosclerosis Risk in Communities Study.},
  archive      = {J_AOAS},
  author       = {Richard Sizelove and Donglin Zeng and Dan-Yu Lin},
  doi          = {10.1214/24-AOAS1881},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2295-2306},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric linear regression with an interval-censored covariate in the atherosclerosis risk in communities study},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An integrative network-based mediation model (NMM) to
estimate multiple genetic effects on outcomes mediated by functional
connectivity. <em>AOAS</em>, <em>18</em>(3), 2277–2294. (<a
href="https://doi.org/10.1214/24-AOAS1880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional connectivity of the brain, characterized by interconnected neural circuits across functional networks, is a cutting-edge feature in neuroimaging. It has the potential to mediate the effect of genetic variants on behavioral outcomes or diseases. Existing mediation analysis methods can evaluate the impact of genetics and brain structure/function on cognitive behavior or disorders, but they tend to be limited to single genetic variants or univariate mediators, without considering cumulative genetic effects and the complex matrix and group and network structures of functional connectivity. To address this gap, the paper presents an integrative network-based mediation model (NMM) that estimates the effect of multiple genetic variants on behavioral outcomes or diseases mediated by functional connectivity. The model incorporates group information of inter-regions at broad network level and imposes low-rank and sparse assumptions to reflect the complex structures of functional connectivity and selecting network mediators simultaneously. We adopt block coordinate descent algorithm to implement a fast and efficient solution to our model. Simulation results indicate the efficacy of the model in selecting active mediators and reducing bias in effect estimation. With application to the Human Connectome Project Youth Adult (HCP-YA) study of 493 young adults, two genetic variants (rs769448 and rs769449) on the APOE4 gene are identified that lead to deficits in functional connectivity within visual networks and fluid intelligence.},
  archive      = {J_AOAS},
  author       = {Wei Dai and Heping Zhang},
  doi          = {10.1214/24-AOAS1880},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2277-2294},
  shortjournal = {Ann. Appl. Stat.},
  title        = {An integrative network-based mediation model (NMM) to estimate multiple genetic effects on outcomes mediated by functional connectivity},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Surrogate method for partial association between mixed data
with application to well-being survey analysis. <em>AOAS</em>,
<em>18</em>(3), 2254–2276. (<a
href="https://doi.org/10.1214/24-AOAS1879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by the analysis of a survey study focusing on college student well-being before and after the COVID-19 pandemic outbreak. A statistical challenge in well-being studies lies in the multidimensionality of outcome variables, recorded in various scales such as continuous, binary, or ordinal. The presence of mixed data complicates the examination of their relationships when adjusting for important covariates. To address this challenge, we propose a unifying framework for studying partial association between mixed data. We achieve this by defining a unified residual using the surrogate method. The idea is to map the residual randomness to a consistent continuous scale, regardless of the original scales of outcome variables. This framework applies to parametric or semiparametric models for covariate adjustments. We validate the use of such residuals for assessing partial association, introducing a measure that generalizes classical Kendall’s tau to capture both partial and marginal associations. Moreover, our development advances the theory of the surrogate method by demonstrating its applicability without requiring outcome variables to have a latent variable structure. In the analysis of the college student well-being survey, our proposed method unveils the contingency of relationships between multidimensional well-being measures and micro personal risk factors (e.g., physical health, loneliness, and accommodation) as well as the macro disruption caused by COVID-19.},
  archive      = {J_AOAS},
  author       = {Shaobo Li and Zhaohu Fan and Ivy Liu and Philip S. Morrison and Dungang Liu},
  doi          = {10.1214/24-AOAS1879},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2254-2276},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Surrogate method for partial association between mixed data with application to well-being survey analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling curves and derivatives as predictors for traffic
breakdown probabilities. <em>AOAS</em>, <em>18</em>(3), 2230–2253. (<a
href="https://doi.org/10.1214/24-AOAS1878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an interest in predicting the status of road traffic congestion within a short period, this paper presents a generalized functional linear regression model for predicting traffic breakdown probabilities. In this model, traffic congestion status is the response variable, and we utilize the observed traffic speed trajectories and their first two derivatives as functional predictors, representing different features of a random function. While the derivatives of a trajectory may contain useful information, they cannot be observed directly and so must be estimated. To address this challenge, we apply the Karhunen–Loève representation to individual functional predictors, including the trajectory and its derivatives. The regression model is reparameterized to represent both the integrated regression effect and the predictor-specific effects. The importance of these effects is indicated by the corresponding weight parameters. We also provide the consistency properties of the estimators relating to the derivative functional principal components and the regression parameter functions. In our simulation study, we find that the modeling approach is useful in its application to freeway traffic data; in particular, the use of speed trajectory derivatives as predictors for traffic status successfully enhances prediction accuracy.},
  archive      = {J_AOAS},
  author       = {Jeng-Min Chiou and Pai-Ling Li},
  doi          = {10.1214/24-AOAS1878},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2230-2253},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling curves and derivatives as predictors for traffic breakdown probabilities},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Probabilistic contrastive dimension reduction for
case-control study data. <em>AOAS</em>, <em>18</em>(3), 2207–2229. (<a
href="https://doi.org/10.1214/24-AOAS1877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-control experiments are essential to the scientific method, as they allow researchers to test biological hypotheses by looking for differences in outcome between cases and controls. It is then of interest to characterize variation that is enriched in a “foreground” (case) dataset relative to a “background” (control) dataset. For example, in a genomics context, the goal is to identify low-dimensional transcriptional structure unique to patients with certain disease (cases) vs. those without that disease (controls). In this work we propose probabilistic contrastive principal component analysis (PCPCA), a probabilistic dimension reduction method designed for case-control data. We describe inference in PCPCA through a contrastive likelihood and show that our model generalizes PCA, probabilistic PCA, and contrastive PCA. We discuss how to set the tuning parameter in theory and in practice, and we show several of PCPCA’s advantages in the analysis of case-control data over related methods, including greater interpretability, uncertainty quantification and principled inference, robustness to noise and missing data, and the ability to generate “foreground-enriched” data from the model. We demonstrate PCPCA’s performance on case-control data through a series of simulations, and we successfully identify variation specific to case data in genomic case-control experiments with data modalities, including gene expression, protein expression, and images.},
  archive      = {J_AOAS},
  author       = {Didong Li and Andrew Jones and Barbara Engelhardt},
  doi          = {10.1214/24-AOAS1877},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2207-2229},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Probabilistic contrastive dimension reduction for case-control study data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonconvex SVM for cancer diagnosis based on morphologic
features of tumor microenvironment. <em>AOAS</em>, <em>18</em>(3),
2187–2206. (<a href="https://doi.org/10.1214/24-AOAS1876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surroundings of a cancerous tumor impact how it grows and develops in humans. New data from early breast cancer patients contains information on the collagen fibers surrounding the tumorous tissue—offering hope of finding additional biomarkers for diagnosis and prognosis—but poses two challenges for typical analysis. Each image section contains information on hundreds of fibers, and each tissue has multiple image sections contributing to a single prediction of tumor vs. nontumor. This nested relationship of fibers within image spots within tissue samples requires a specialized analysis approach. We devise a novel support vector machine (SVM)-based predictive algorithm for this data structure. By treating the collection of fibers as a probability distribution, we can measure similarities between the collections through a flexible kernel approach. By assuming the relationship of tumor status between image sections and tissue samples, the constructed SVM problem is nonconvex, and traditional algorithms can not be applied. We propose two algorithms that exchange computational accuracy and efficiency to manage data of all sizes. The predictive performance of both algorithms is evaluated on the collagen fiber data set and additional simulation scenarios. We offer reproducible implementations of both algorithms of this approach in the R package mildsvm.},
  archive      = {J_AOAS},
  author       = {Sean Kent and Menggang Yu},
  doi          = {10.1214/24-AOAS1876},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2187-2206},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Nonconvex SVM for cancer diagnosis based on morphologic features of tumor microenvironment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Support vector machine for dynamic survival prediction with
time-dependent covariates. <em>AOAS</em>, <em>18</em>(3), 2166–2186. (<a
href="https://doi.org/10.1214/24-AOAS1875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting time-to-event outcomes using time-dependent covariates is a challenging problem. Many machine learning approaches, such as tree-based methods and support vector regression, predominantly utilize only baseline covariates. Only a few methods can incorporate time-dependent covariates, but they often lack theoretical justification. In this paper we present a new framework for event time prediction, leveraging the support vector machines to forecast the associated counting processes. Utilizing the kernel trick, we accommodate nonlinear functions in both time and covariate spaces. Subsequently, we use a chain algorithm to predict future events. Theoretical analysis proves that our method is equivalent to comparing time-varying hazard rates among at-risk subjects, and we obtain the convergence rate of the resulting prediction loss. Through simulation studies and a case study on Huntington’s disease, we demonstrate the superior performance of our approach compared to alternative methods based on machine learning, deep learning, and statistical models.},
  archive      = {J_AOAS},
  author       = {Wenyi Xie and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1214/24-AOAS1875},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2166-2186},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Support vector machine for dynamic survival prediction with time-dependent covariates},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Exposure effects on count outcomes with observational data,
with application to incarcerated women. <em>AOAS</em>, <em>18</em>(3),
2147–2165. (<a href="https://doi.org/10.1214/24-AOAS1874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference methods can be applied to estimate the effect of a point exposure or treatment on an outcome of interest using data from observational studies. For example, in the Women’s Interagency HIV Study, it is of interest to understand the effects of incarceration on the number of sexual partners and the number of cigarettes smoked after incarceration. In settings like this where the outcome is a count, the estimand is often the causal mean ratio, that is, the ratio of the counterfactual mean count under exposure to the counterfactual mean count under no exposure. This paper considers estimators of the causal mean ratio based on inverse probability of treatment weights, the parametric g-formula, and doubly robust estimation, each of which can account for overdispersion, zero-inflation, and heaping in the measured outcome. Methods are compared in simulations and are applied to data from the Women’s Interagency HIV Study.},
  archive      = {J_AOAS},
  author       = {Bonnie E. Shook-Sa and Michael G. Hudgens and Andrea K. Knittel and Andrew Edmonds and Catalina Ramirez and Stephen R. Cole and Mardge Cohen and Adebola Adedimeji and Tonya Taylor and Katherine G. Michel and Andrea Kovacs and Jennifer Cohen and Jessica Donohue and Antonina Foster and Margaret A. Fischl and Dustin Long and Adaora A. Adimora},
  doi          = {10.1214/24-AOAS1874},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2147-2165},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Exposure effects on count outcomes with observational data, with application to incarcerated women},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent process model for monitoring progress toward
hard-to-measure targets with applications to mental health and online
educational assessments. <em>AOAS</em>, <em>18</em>(3), 2123–2146. (<a
href="https://doi.org/10.1214/24-AOAS1873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent shift to remote learning and work has aggravated long-standing problems, such as the problem of monitoring the mental health of individuals and the progress of students toward learning targets. We introduce a novel latent process model with a view to monitoring the progress of individuals toward a hard-to-measure target of interest and measured by a set of variables. The latent process model is based on the idea of embedding both individuals and variables measuring progress toward the target of interest in a shared metric space, interpreted as an interaction map that captures interactions between individuals and variables. The fact that individuals are embedded in the same metric space as the target helps assess the progress of individuals toward the target. We demonstrate, with the help of simulations and applications, that the latent process model enables a novel look at mental health and online educational assessments in disadvantaged subpopulations.},
  archive      = {J_AOAS},
  author       = {Minjeong Jeon and Michael Schweinberger},
  doi          = {10.1214/24-AOAS1873},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2123-2146},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A latent process model for monitoring progress toward hard-to-measure targets with applications to mental health and online educational assessments},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Benefits and costs of matching prior to a difference in
differences analysis when parallel trends does not hold. <em>AOAS</em>,
<em>18</em>(3), 2096–2122. (<a
href="https://doi.org/10.1214/24-AOAS1872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consequence of a change in school leadership (e.g., principal turnover) on student achievement has important implications for education policy. The impact of such an event can be estimated via the popular difference in difference (DiD) estimator, where those schools with a turnover event are compared to a selected set of schools that did not have such an event. The strength of this comparison depends on the plausibility of the “parallel trends” assumption that the “treated group” of those schools which had leadership turnover, absent such turnover, would have changed “similarly” to those which did not. To bolster such a claim, one might generate a comparison group, via matching, that is similar to the treated group with respect to pretreatment outcomes and/or pretreatment covariates. Unfortunately, as has been previously pointed out, this intuitively appealing approach also has a cost in terms of bias. To assess the trade-offs of matching in our application, we first characterize the bias of matching prior to a DiD analysis under a linear structural model that allows for time-invariant observed and unobserved confounders with time-varying effects on the outcome. Given our framework, we verify that matching on baseline covariates generally reduces bias. We further show how additionally matching on pretreatment outcomes has both cost and benefit. First, matching on pretreatment outcomes partially balances unobserved confounders, which mitigates some bias. This reduction is proportional to the outcome’s reliability, a measure of how coupled the outcomes are with the latent covariates. Offsetting these gains, matching also injects bias into the final estimate by undermining the second difference in the DiD via a regression-to-the-mean effect. Consequently, we provide heuristic guidelines for determining to what degree the bias reduction of matching is likely to outweigh the bias cost. We illustrate our guidelines by reanalyzing a principal turnover study that used matching prior to a DiD analysis and find that matching on both the pretreatment outcomes and observed covariates makes the estimated treatment effect more credible.},
  archive      = {J_AOAS},
  author       = {Dae Woong Ham and Luke Miratrix},
  doi          = {10.1214/24-AOAS1872},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2096-2122},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Benefits and costs of matching prior to a difference in differences analysis when parallel trends does not hold},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonparametric mixed-effects mixture model for patterns of
clinical measurements associated with COVID-19. <em>AOAS</em>,
<em>18</em>(3), 2080–2095. (<a
href="https://doi.org/10.1214/23-AOAS1871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some patients with COVID-19 show changes in signs and symptoms, such as temperature and oxygen saturation days before being positively tested for SARS-CoV-2, while others remain asymptomatic. It is important to identify these subgroups and to understand what biological and clinical predictors are related to these subgroups. This information will provide insights into how the immune system may respond differently to infection and can further be used to identify infected individuals. We propose a flexible nonparametric mixed-effects mixture model that identifies risk factors and classifies patients with biological changes. We model the latent probability of biological changes using a logistic regression model and trajectories in the latent groups using smoothing splines. We developed an EM algorithm to maximize the penalized likelihood for estimating all parameters and mean functions. We evaluate our methods by simulations and apply the proposed model to investigate changes in temperature in a cohort of COVID-19-infected hemodialysis patients.},
  archive      = {J_AOAS},
  author       = {Xiaoran Ma and Wensheng Guo and Mengyang Gu and Len Usvyat and Peter Kotanko and Yuedong Wang},
  doi          = {10.1214/23-AOAS1871},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2080-2095},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A nonparametric mixed-effects mixture model for patterns of clinical measurements associated with COVID-19},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric estimation for dynamic networks with shifted
connecting intensities. <em>AOAS</em>, <em>18</em>(3), 2062–2079. (<a
href="https://doi.org/10.1214/23-AOAS1870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural circuits are of paramount importance in the nervous system, as they are the essential infrastructure in guiding animal behavior. However, modeling the development of neural circuits poses significant challenges due to inherent properties of the development process. First, the neural circuit development process is transient, where the course of development can only be observed once. Second, despite potentially sharing similar underlying mechanisms for development, neural circuits from different subjects possess distinct sets of neurons, which limits the sharing of information across subjects. Third, neurons have diverse, unobserved activation times, which may obscure the analysis of neural activities. In light of these challenges, this study presents a novel approach aimed at clustering neurons based on their connecting behaviors while accommodating disparities at the neuron level. To this end, we propose a dynamic stochastic block model that accommodates unknown time shifts. We establish the conditions that guarantee the identifiability of cluster memberships of nodes and representative connecting intensities across clusters. Using methods for shape invariant models, we propose computationally efficient semiparametric estimation procedures to simultaneously estimate time shifts, cluster memberships, and connecting intensities. We illustrate the performance of the proposed procedures via extensive simulation experiments. We further apply the proposed method on a motor circuit development data from zebrafish to reveal distinct roles of neurons and identify representative connecting behaviors.},
  archive      = {J_AOAS},
  author       = {Zitong Zhang and Shizhe Chen},
  doi          = {10.1214/23-AOAS1870},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2062-2079},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric estimation for dynamic networks with shifted connecting intensities},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hidden markov models for latent variable labeling
assignments in conflict research: Application to the role ceasefires
play in conflict dynamics. <em>AOAS</em>, <em>18</em>(3), 2034–2061. (<a
href="https://doi.org/10.1214/23-AOAS1869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A crucial challenge for solving problems in conflict research is in leveraging the semisupervised nature of the data that arise. Observed response data, such as counts of battle deaths over time, indicate latent processes of interest, such as intensity and duration of conflicts, but defining and labeling instances of these unobserved processes requires nuance and imprecision. The availability of such labels, however, would make it possible to study the effect of intervention-related predictors—such as ceasefires—directly on conflict dynamics (e.g., latent intensity) rather than through an intermediate proxy, like observed counts of battle deaths. Motivated by this problem and the new availability of the ETH-PRIO Civil Conflict Ceasefires data set, we propose a Bayesian autoregressive (AR) hidden Markov model (HMM) framework as a sufficiently flexible machine learning approach for semisupervised regime labeling with uncertainty quantification. We motivate our approach by illustrating the way it can be used to study the role that ceasefires play in shaping conflict dynamics. This ceasefires data set is the first systematic and globally comprehensive data on ceasefires, and our work is the first to analyze this new data and to explore the effect of ceasefires on conflict dynamics in a comprehensive and cross-country manner.},
  archive      = {J_AOAS},
  author       = {Jonathan P. Williams and Gudmund H. Hermansen and Håvard Strand and Govinda Clayton and Håvard Mokleiv Nygård},
  doi          = {10.1214/23-AOAS1869},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2034-2061},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian hidden markov models for latent variable labeling assignments in conflict research: Application to the role ceasefires play in conflict dynamics},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantile regression decomposition analysis of disparity
research using complex survey data: Application to disparities in BMI
and telomere length between u.s. Minority and white population groups.
<em>AOAS</em>, <em>18</em>(3), 2012–2033. (<a
href="https://doi.org/10.1214/23-AOAS1868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a quantile regression decomposition (QRD) method for analyzing observed disparities (OD) between population groups in socioeconomic and health-related outcomes for complex survey data. The conventional decomposition approaches use the conditional mean regression to decompose the disparity into two parts, the part explained by the difference arising from the different distributions in the explanatory covariates and the remaining part, which is unexplained by the covariates. Many socioeconomic and health outcomes exhibit heteroscedastic distributions, where the magnitude of observed disparities varies across different quantiles of these outcomes. Thus, differences in the explanatory covariates may account for varying differences in the OD across the quantiles of the outcome. The QRD can identify where there are greater differences in the outcome distribution, for example, 90th quantile, and how important the covariates are in explaining those differences. Much socioeconomic and health research relies on complex surveys, such as the National Health and Nutrition Examination Survey (NHANES), that oversample individuals from disadvantaged/minority population groups in order to provide improved precision. QRD has not been extended to the complex survey setting. We improve the QRD approach proposed in Machado and Mata (2005) to yield more reliable estimates at the quantiles, where the data are sparse, and extend it to the complex survey setting. We also propose a perturbation-based variance estimation method. Simulation studies indicate that the estimates of the unexplained portions of the OD across quantiles are unbiased and the coverage of the confidence intervals are close to nominal value. This methodology is used to study disparities in body mass index (BMI) and telomere length between race/ethnic groups estimated from the NHANES data.},
  archive      = {J_AOAS},
  author       = {Hyokyoung G. Hong and Barry I. Graubard and Joseph L. Gastwirth and Mi-Ok Kim},
  doi          = {10.1214/23-AOAS1868},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {2012-2033},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Quantile regression decomposition analysis of disparity research using complex survey data: Application to disparities in BMI and telomere length between U.S. minority and white population groups},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent conjunctive bayesian network: Unify attribute
hierarchy and bayesian network for cognitive diagnosis. <em>AOAS</em>,
<em>18</em>(3), 1988–2011. (<a
href="https://doi.org/10.1214/23-AOAS1867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive diagnostic assessment aims to measure specific knowledge structures in students. To model data arising from such assessments, cognitive diagnostic models with discrete latent variables have gained popularity in educational and behavioral sciences. In a learning context, the latent variables often denote sequentially acquired skill attributes, which is often modeled by the so-called attribute hierarchy method. One drawback of the traditional attribute hierarchy method is that its parameter complexity varies substantially with the hierarchy’s graph structure, lacking statistical parsimony. Additionally, arrows among the attributes do not carry an interpretation of statistical dependence. Motivated by these, we propose a new family of latent conjunctive Bayesian networks (LCBNs), which rigorously unify the attribute hierarchy method for sequential skill mastery and the Bayesian network model in statistical machine learning. In an LCBN the latent graph not only retains the hard constraints on skill prerequisites as an attribute hierarchy but also encodes nice conditional independence interpretation as a Bayesian network. LCBNs are identifiable, interpretable, and parsimonious statistical tools to diagnose students’ cognitive abilities from assessment data. We propose an efficient two-step EM algorithm for structure learning and parameter estimation in LCBNs and establish the consistency of this procedure. Application of our method to an international educational assessment dataset gives interpretable findings of cognitive diagnosis.},
  archive      = {J_AOAS},
  author       = {Seunghyun Lee and Yuqi Gu},
  doi          = {10.1214/23-AOAS1867},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1988-2011},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Latent conjunctive bayesian network: Unify attribute hierarchy and bayesian network for cognitive diagnosis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning and forecasting of age-specific period mortality
via b-spline processes with locally-adaptive dynamic coefficients.
<em>AOAS</em>, <em>18</em>(3), 1965–1987. (<a
href="https://doi.org/10.1214/23-AOAS1866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the analysis of human mortality has a well-established history, the attempt to accurately forecast future death-rate patterns for different age groups and time horizons still attracts active research. Such a predictive focus has motivated an increasing shift toward more flexible representations of age-specific period mortality trajectories at the cost of reduced interpretability. Although this perspective has led to successful predictive strategies, the inclusion of interpretable structures in modeling of human mortality can be, in fact, beneficial for improving forecasts. We pursue this direction via a novel b-spline process with locally-adaptive dynamic coefficients. Such a process outperforms state-of-the-art forecasting strategies by explicitly incorporating the core structures of period mortality within an interpretable formulation which enables inference on age-specific mortality trends and the corresponding rates of change across time. This is obtained by modeling the age-specific death counts via a Poisson log-normal model parameterized through a linear combination of b-spline bases with dynamic coefficients that characterize time changes in mortality rates via suitably defined stochastic differential equations. While flexible, the resulting formulation can be accurately approximated by a Gaussian state-space model that facilitates closed-form Kalman filtering, smoothing and forecasting, for both the trends of the spline coefficients and the corresponding first derivatives, which measure rates of change in mortality for different age groups. As illustrated in applications to mortality data from different countries, the proposed model outperforms state-of-the-art methods, both in point forecasts and in calibration of predictive intervals. Moreover, it unveils substantial differences in mortality patterns across countries and ages, both in the past decades and during the covid-19 pandemic.},
  archive      = {J_AOAS},
  author       = {Federico Pavone and Sirio Legramanti and Daniele Durante},
  doi          = {10.1214/23-AOAS1866},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1965-1987},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning and forecasting of age-specific period mortality via B-spline processes with locally-adaptive dynamic coefficients},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Outcome-guided disease subtyping by generative model and
weighted joint likelihood in transcriptomic applications. <em>AOAS</em>,
<em>18</em>(3), 1947–1964. (<a
href="https://doi.org/10.1214/23-AOAS1865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With advances in high-throughput technology, molecular disease subtyping by high-dimensional omics data has been recognized as an effective approach for identifying subtypes of complex diseases with distinct disease mechanisms and prognoses. Conventional cluster analysis takes omics data as input and generates patient clusters with similar gene expression pattern. The omics data, however, usually contain multifaceted cluster structures that can be defined by different sets of genes. If the gene set associated with irrelevant clinical variables (e.g., sex or age) dominates the clustering process, the resulting clusters may not capture clinically meaningful disease subtypes. This motivates the development of a clustering framework with guidance from a prespecified disease outcome, such as lung function measurement or survival, in this paper. We propose two disease subtyping methods by omics data with outcome guidance using a generative model or a weighted joint likelihood. Both methods connect an outcome association model and a disease subtyping model by a latent variable of cluster labels. Compared to the generative model, weighted joint likelihood contains a data-driven weight parameter to balance the likelihood contributions from outcome association and gene cluster separation, which improves generalizability in independent validation but requires heavier computing. Extensive simulations and two real applications in lung disease and triple-negative breast cancer demonstrate superior disease subtyping performance of the outcome-guided clustering methods in terms of disease subtyping accuracy, gene selection and outcome association. Unlike existing clustering methods, the outcome-guided disease subtyping framework creates a new precision medicine paradigm to directly identify patient subgroups with clinical association.},
  archive      = {J_AOAS},
  author       = {Yujia Li and Peng Liu and Wenjia Wang and Wei Zong and Yusi Fang and Zhao Ren and Lu Tang and Juan C. Celedón and Steffi Oesterreich and George C. Tseng},
  doi          = {10.1214/23-AOAS1865},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1947-1964},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Outcome-guided disease subtyping by generative model and weighted joint likelihood in transcriptomic applications},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Joint mixed membership modeling of multivariate longitudinal
and survival data for learning the individualized disease progression.
<em>AOAS</em>, <em>18</em>(3), 1924–1946. (<a
href="https://doi.org/10.1214/23-AOAS1864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients with Alzheimer’s disease (AD) often exhibit substantial heterogeneity in disease progression due to multiple genetic causes for such a complex disease. Investigating diverse subtypes of neurodegeneration and individualized disease progression is essential for early diagnosis and precision medicine. In this article we present a novel joint mixed membership model for multivariate longitudinal AD-related biomarkers and time of AD diagnosis. Unlike conventional finite mixture models that assign each subject a single subgroup membership, the proposed model assigns partial membership across subgroups, allowing subjects to lie between two or more subgroups. This flexible structure enables individualized disease progression and facilitates the identification of clinically meaningful neurological statuses often elusive in current mixed effects models. We employ a spline-based trajectory model to characterize complex and possibly nonlinear patterns of multiple longitudinal clinical markers. A Cox model is then used to examine the effects of time-variant risk factors on the hazard of developing AD. We develop a Bayesian method coupled with efficient Markov chain Monte Carlo sampling schemes to perform statistical inference. The proposed approach is assessed through extensive simulation studies and an application to the Alzheimer’s Disease Neuroimaging Initiative study, showing a better performance in AD diagnosis than existing joint models.},
  archive      = {J_AOAS},
  author       = {Yuyang He and Xinyuan Song and Kai Kang},
  doi          = {10.1214/23-AOAS1864},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1924-1946},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Joint mixed membership modeling of multivariate longitudinal and survival data for learning the individualized disease progression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable multiple network inference with the joint graphical
horseshoe. <em>AOAS</em>, <em>18</em>(3), 1899–1923. (<a
href="https://doi.org/10.1214/23-AOAS1863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network models are useful tools for modelling complex associations. In statistical omics such models are increasingly popular for identifying and assessing functional relationships and pathways. If a Gaussian graphical model is assumed, conditional independence is determined by the nonzero entries of the inverse covariance (precision) matrix of the data. The Bayesian graphical horseshoe estimator provides a robust and flexible framework for precision matrix inference, as it introduces local, edge-specific parameters which prevent over-shrinkage of nonzero off-diagonal elements. However, its applicability is currently limited in statistical omics settings, which often involve high-dimensional data from multiple conditions that might share common structures. We propose: (i) a scalable expectation conditional maximisation (ECM) algorithm for the original graphical horseshoe and (ii) a novel joint graphical horseshoe estimator, which borrows information across multiple related networks to improve estimation. We show numerically that our single-network ECM approach is more scalable than the existing graphical horseshoe Gibbs implementation, while achieving the same level of accuracy. We also show that our joint-network proposal successfully leverages shared edge-specific information between networks while still retaining differences, outperforming state-of-the-art methods at any level of network similarity. Finally, we leverage our approach to clarify gene regulation activity within and across immune stimulation conditions in monocytes, and formulate hypotheses on the pathogenesis of immune-mediated diseases.},
  archive      = {J_AOAS},
  author       = {Camilla Lingjærde and Benjamin P. Fairfax and Sylvia Richardson and Hélène Ruffieux},
  doi          = {10.1214/23-AOAS1863},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1899-1923},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Scalable multiple network inference with the joint graphical horseshoe},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel bayesian model for assessing intratumor
heterogeneity of tumor infiltrating leukocytes with multiregion gene
expression sequencing. <em>AOAS</em>, <em>18</em>(3), 1879–1898. (<a
href="https://doi.org/10.1214/23-AOAS1862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intratumor heterogeneity (ITH) of tumor-infiltrated leukocytes (TILs) is an important phenomenon of cancer biology with potentially profound clinical impacts. Multiregion gene expression sequencing data provide a promising opportunity that allows for explorations of TILs and their intratumor heterogeneity for each subject. Although several existing methods are available to infer the proportions of TILs, considerable methodological gaps exist for evaluating intratumor heterogeneity of TILs with multiregion gene expression data. Here we develop ICeITH, immune cell estimation reveals intratumor heterogeneity, a Bayesian hierarchical model that borrows cell-type profiles as prior knowledge to decompose mixed bulk data while accounting for the within-subject correlations among tumor samples. ICeITH quantifies intratumor heterogeneity by the variability of targeted cellular compositions. Through extensive simulation studies, we demonstrate that ICeITH is more accurate in measuring relative cellular abundance and evaluating intratumor heterogeneity compared with existing methods. We also assess the ability of ICeITH to stratify patients by their intratumor heterogeneity score and associate the estimations with the survival outcomes. Finally, we apply ICeITH to two multiregion gene expression datasets from lung cancer studies to classify patients into different risk groups according to the ITH estimations of targeted TILs that shape either pro- or antitumor processes. In conclusion, ICeITH is a useful tool to evaluate intratumor heterogeneity of TILs from multiregion gene expression data.},
  archive      = {J_AOAS},
  author       = {Peng Yang and Shawna M. Hubert and P. Andrew Futreal and Xingzhi Song and Jianhua Zhang and J. Jack Lee and Ignacio Wistuba and Ying Yuan and Jianjun Zhang and Ziyi Li},
  doi          = {10.1214/23-AOAS1862},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1879-1898},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A novel bayesian model for assessing intratumor heterogeneity of tumor infiltrating leukocytes with multiregion gene expression sequencing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Patient recruitment using electronic health records under
selection bias: A two-phase sampling framework. <em>AOAS</em>,
<em>18</em>(3), 1858–1878. (<a
href="https://doi.org/10.1214/23-AOAS1860">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHRs) are increasingly recognized as a cost-effective resource for patient recruitment in clinical research. However, how to optimally select a cohort from millions of individuals to answer a scientific question of interest remains unclear. Consider a study to estimate the mean or mean difference of an expensive outcome. Inexpensive auxiliary covariates predictive of the outcome may often be available in patients’ health records, presenting an opportunity to recruit patients selectively, which may improve efficiency in downstream analyses. In this paper we propose a two-phase sampling design that leverages available information on auxiliary covariates in EHR data. A key challenge in using EHR data for multiphase sampling is the potential selection bias, because EHR data are not necessarily representative of the target population. Extending existing literature on two-phase sampling design, we derive an optimal two-phase sampling method that improves efficiency over random sampling while accounting for the potential selection bias in EHR data. We demonstrate the efficiency gain from our sampling design via simulation studies and an application evaluating the prevalence of hypertension among U.S. adults leveraging data from the Michigan Genomics Initiative, a longitudinal biorepository in Michigan Medicine.},
  archive      = {J_AOAS},
  author       = {Guanghao Zhang and Lauren J. Beesley and Bhramar Mukherjee and Xu Shi},
  doi          = {10.1214/23-AOAS1860},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1858-1878},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Patient recruitment using electronic health records under selection bias: A two-phase sampling framework},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bootstrap model comparison test for identifying genes with
context-specific patterns of genetic regulation. <em>AOAS</em>,
<em>18</em>(3), 1840–1857. (<a
href="https://doi.org/10.1214/23-AOAS1859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding how genetic variation affects gene expression is essential for a complete picture of the functional pathways that give rise to complex traits. Although numerous studies have established that many genes are differentially expressed in distinct human tissues and cell types, no tools exist for identifying the genes whose expression is differentially regulated. Here we introduce DRAB (differential regulation analysis by bootstrapping), a gene-based method for testing whether patterns of genetic regulation are significantly different between tissues or other biological contexts. DRAB first leverages the elastic net to learn context-specific models of local genetic regulation and then applies a novel bootstrap-based model comparison test to check their equivalency. Unlike previous model comparison tests, our proposed approach can determine whether population-level models have equal predictive performance by accounting for the variability of feature selection and model training. We validated DRAB on mRNA expression data from a variety of human tissues in the Genotype-Tissue Expression (GTEx) Project. DRAB yielded biologically reasonable results and had sufficient power to detect genes with tissue-specific regulatory profiles while effectively controlling false positives. By providing a framework that facilitates the prioritization of differentially regulated genes, our study enables future discoveries on the genetic architecture of molecular phenotypes.},
  archive      = {J_AOAS},
  author       = {Mykhaylo M. Malakhov and Ben Dai and Xiaotong T. Shen and Wei Pan},
  doi          = {10.1214/23-AOAS1859},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1840-1857},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bootstrap model comparison test for identifying genes with context-specific patterns of genetic regulation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Continuous and atlas-free analysis of brain structural
connectivity. <em>AOAS</em>, <em>18</em>(3), 1815–1839. (<a
href="https://doi.org/10.1214/23-AOAS1858">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain structural networks are often represented as discrete adjacency matrices with elements summarizing the connectivity between pairs of regions of interest (ROIs). These ROIs are typically determined a priori using a brain atlas. The choice of atlas is often arbitrary and can lead to a loss of important connectivity information at the sub-ROI level. This work introduces an atlas-free framework that overcomes these issues by modeling brain connectivity using smooth random functions. In particular, we assume that the observed pattern of white matter fiber tract endpoints is driven by a latent random function defined over a product manifold domain. To facilitate statistical analysis of these high-dimensional functional data objects, we develop a novel algorithm to construct a data-driven reduced-rank function space that offers a desirable trade-off between computational complexity and flexibility. Using real data from the Human Connectome Project, we show that our method outperforms state-of-the-art approaches that use the traditional atlas-based structural connectivity representation on a variety of connectivity analysis tasks. We further demonstrate how our method can be used to detect localized regions and connectivity patterns associated with group differences.},
  archive      = {J_AOAS},
  author       = {William Consagra and Martin Cole and Xing Qiu and Zhengwu Zhang},
  doi          = {10.1214/23-AOAS1858},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1815-1839},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Continuous and atlas-free analysis of brain structural connectivity},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic modeling and online monitoring of tensor data
streams with application to passenger flow surveillance. <em>AOAS</em>,
<em>18</em>(3), 1789–1814. (<a
href="https://doi.org/10.1214/23-AOAS1845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passenger flow surveillance in urban transport systems has emerged as a major global issue for smart city management. Governments are taking proper measures to monitor passenger flow in order to maintain social stability and to prevent unexpected group events. It is critical to develop a passenger flow surveillance system that continuously monitors the passenger flow over time and triggers a signal as soon as the passenger flow begins to deteriorate so that timely government intervention can be implemented. In this paper passenger flow surveillance is novelly formulated as dynamic modeling and online monitoring of tensor data streams. Existing tensor monitoring methods either rely heavily on the assumption that the tensor coefficients exhibit a low-rank structure or are inapplicable to general-order tensors. We propose a unified monitoring framework based on the tensor normal distribution to overcome these challenges. We begin by developing a tensor model selection procedure that ensures that the chosen tensor structure strikes a balance between model complexity and estimation accuracy. Then we propose an online estimation procedure to dynamically estimate the tensor parameters on which sequential change-detection procedures, using the generalized likelihood ratio test, are proposed. Extensive simulations and an analysis of real passenger flow data in Hong Kong demonstrate the efficacy of our approach.},
  archive      = {J_AOAS},
  author       = {Yifan Li and Chunjie Wu and Wendong Li and Fugee Tsung and Jianhua Guo},
  doi          = {10.1214/23-AOAS1845},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1789-1814},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dynamic modeling and online monitoring of tensor data streams with application to passenger flow surveillance},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Multiscale poisson process approaches for detecting and
estimating differences from high-throughput sequencing assays.
<em>AOAS</em>, <em>18</em>(3), 1773–1788. (<a
href="https://doi.org/10.1214/23-AOAS1828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating and testing for differences in molecular phenotypes (e.g., gene expression, chromatin accessibility, transcription factor binding) across conditions is an important part of understanding the molecular basis of gene regulation. These phenotypes are commonly measured using high-throughput sequencing assays (e.g., RNA-seq, ATAC-seq, ChIP-seq), which provide high-resolution count data that reflect how the phenotypes vary along the genome. Multiple methods have been proposed to help exploit these high-resolution measurements for differential expression analysis. However, they ignore the count nature of the data, instead using normal distributions that work well only for data with large sample sizes or high counts. Here we develop count-based methods to address this problem. We model the data for each sample using an inhomogeneous Poisson process with spatially structured underlying intensity function and then, building on multiscale models for the Poisson process, estimate and test for differences in the underlying intensity function across samples (or groups of samples). Using both simulation and real ATAC-seq data, we show that our method outperforms previous normal-based methods, especially in situations with small sample sizes or low counts.},
  archive      = {J_AOAS},
  author       = {Heejung Shim and Zhengrong Xing and Ester Pantaleo and Francesca Luca and Roger Pique-Regi and Matthew Stephens},
  doi          = {10.1214/23-AOAS1828},
  journal      = {The Annals of Applied Statistics},
  month        = {9},
  number       = {3},
  pages        = {1773-1788},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Multiscale poisson process approaches for detecting and estimating differences from high-throughput sequencing assays},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Investigating swimming technical skills by a double
partition clustering of multivariate functional data allowing for
dimension selection. <em>AOAS</em>, <em>18</em>(2), 1750–1772. (<a
href="https://doi.org/10.1214/23-AOAS1857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Investigating technical skills of swimmers is a challenge for performance improvement that can be achieved by analyzing multivariate functional data recorded by inertial measurement units (IMU). To investigate technical levels of front-crawl swimmers, a new model-based approach is introduced to obtain two complementary partitions reflecting, for each swimmer, its swimming pattern and its ability to reproduce it. Contrary to the usual approaches for functional data clustering, the proposed approach also considers the information of the error terms resulting from the functional basis decomposition. Indeed, after decomposing into functional basis with finite number of elements both the original signal (measuring the swimming pattern) and the signal of squared error terms (measuring the ability to reproduce the swimming pattern), the method fits the joint distribution of the coefficients related to both decompositions by considering dependency between both partitions. Modeling this dependency is mandatory since the difficulty of reproducing a swimming pattern depends on its shape. Moreover, a sparse decomposition of the distribution within components that permits a selection of the relevant dimensions during clustering is proposed. The partitions obtained on the IMU data aggregate the kinematical stroke variability linked to swimming technical skills and allow relevant biomechanical strategy for front-crawl sprint performance to be identified.},
  archive      = {J_AOAS},
  author       = {Antoine Bouvet and Salima El Kolei and Matthieu Marbac},
  doi          = {10.1214/23-AOAS1857},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1750-1772},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Investigating swimming technical skills by a double partition clustering of multivariate functional data allowing for dimension selection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selecting invalid instruments to improve mendelian
randomization with two-sample summary data. <em>AOAS</em>,
<em>18</em>(2), 1729–1749. (<a
href="https://doi.org/10.1214/23-AOAS1856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a widely-used method to estimate the causal relationship between a risk factor and disease. A fundamental part of any MR analysis is to choose appropriate genetic variants as instrumental variables. Genome-wide association studies often reveal that hundreds of genetic variants may be robustly associated with a risk factor, but in some situations investigators may have greater confidence in the instrument validity of only a smaller subset of variants. Nevertheless, the use of additional instruments may be optimal from the perspective of mean squared error, even if they are slightly invalid; a small bias in estimation may be a price worth paying for a larger reduction in variance. For this purpose we consider a method for “focused” instrument selection whereby genetic variants are selected to minimise the estimated asymptotic mean squared error of causal effect estimates. In a setting of many weak and locally invalid instruments, we propose a novel strategy to construct confidence intervals for postselection focused estimators that guards against the worst case loss in asymptotic coverage. In empirical applications to: (i) validate lipid drug targets and (ii) investigate vitamin D effects on a wide range of outcomes, our findings suggest that the optimal selection of instruments does not involve only a small number of biologically-justified instruments but also many potentially invalid instruments.},
  archive      = {J_AOAS},
  author       = {Ashish Patel and Francis J. DiTraglia and Verena Zuber and Stephen Burgess},
  doi          = {10.1214/23-AOAS1856},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1729-1749},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Selecting invalid instruments to improve mendelian randomization with two-sample summary data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A hierarchical spline model for correcting and hindcasting
temperature data. <em>AOAS</em>, <em>18</em>(2), 1709–1728. (<a
href="https://doi.org/10.1214/23-AOAS1855">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weather observations are important for a wide range of applications although they do pose statistical challenges, such as missing values, errors, flawed outliers and poor spatial and temporal coverage to name a few. A Bayesian hierarchical spline framework is presented here to deal with such challenges in temperature time series. Motivated by a real-life problem, the approach uses penalised splines, constructed hierarchically, to pool the data, along with a discrete mixture distribution to deal with outliers and publicly available global reanalysis data sets (climate model data) to integrate physically constrained information. Efficient Bayesian implementation is achieved using conditional conjugacy, which allows thorough model checking and uncertainty quantification. Fitting the model to daily maximum temperature illustrates its flexibility in capturing temporal structures, in pooling of the information and in outlier detection. The model is used to hindcast the time series 50 years into the past while maintaining uncertainty at reasonable levels.},
  archive      = {J_AOAS},
  author       = {Theodoros Economou and Catrina Johnson and Elizabeth Dyson},
  doi          = {10.1214/23-AOAS1855},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1709-1728},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A hierarchical spline model for correcting and hindcasting temperature data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How are PreLaunch online movie reviews related to box office
revenues? <em>AOAS</em>, <em>18</em>(2), 1686–1708. (<a
href="https://doi.org/10.1214/23-AOAS1854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the dynamic patterns of the prelaunch online movie reviews, or movie electronic word-of-mouth (eWOM), over time and investigates their relations to the subsequent box office revenues. The volume and valence of prelaunch eWOM have been shown to be early indicators of strong or weak box office. The time patterns of prelaunch eWOM evolution, which are essentially functional data, on the other hand, tend to be overlooked. We apply the functional principal component analysis, a dimension reduction technique in functional data analysis, to analyze the dynamic patterns of various quantile trajectories of the movie eWOM, instead of directly studying the whole eWOM functional data. The functional principal component (FPC) scores of quantile trajectories at various quantile levels are used to predict the box office revenues. We use the sparse group lasso method to select the quantile levels and individual FPC scores that make significant contributions to the prediction of box office revenues. The results show that compared with other measures, such as valence and variance, the top-end quantiles would be a better measure in capturing the relations between the prelaunch product ratings time pattern and launch sales.},
  archive      = {J_AOAS},
  author       = {Tianyu Guan and Jason Ho and Robert Krider and Jiguo Cao and Andrew Fogg},
  doi          = {10.1214/23-AOAS1854},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1686-1708},
  shortjournal = {Ann. Appl. Stat.},
  title        = {How are PreLaunch online movie reviews related to box office revenues?},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional concurrent regression with compositional
covariates and its application to the time-varying effect of causes of
death on human longevity. <em>AOAS</em>, <em>18</em>(2), 1668–1685. (<a
href="https://doi.org/10.1214/23-AOAS1853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate functional data that are cross-sectionally compositional data are attracting increasing interest in the statistical modeling literature, a major example being trajectories over time of compositions derived from cause-specific mortality rates. In this work we develop a novel functional concurrent regression model in which independent variables are functional compositions. This allows us to investigate the relationship over time between life expectancy at birth and compositions derived from cause-specific mortality rates of four distinct age classes, namely, zero to four, five to 39, 40–64 and 65+ in 25 countries. A penalized approach is developed to estimate the regression coefficients and select the relevant variables. Then an efficient computational strategy, based on an augmented Lagrangian algorithm, is derived to solve the resulting optimization problem. The good performances of the model in predicting the response function and estimating the unknown functional coefficients are shown in a simulation study. The results on real data confirm the important role of neoplasms and cardiovascular diseases in determining life expectancy emerged in other studies and reveal several other contributions not yet observed.},
  archive      = {J_AOAS},
  author       = {Emanuele Giovanni Depaoli and Marco Stefanucci and Stefano Mazzuco},
  doi          = {10.1214/23-AOAS1853},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1668-1685},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Functional concurrent regression with compositional covariates and its application to the time-varying effect of causes of death on human longevity},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Variance as a predictor of health outcomes: Subject-level
trajectories and variability of sex hormones to predict body fat changes
in peri- and postmenopausal women. <em>AOAS</em>, <em>18</em>(2),
1642–1667. (<a href="https://doi.org/10.1214/23-AOAS1852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Longitudinal biomarker data and cross-sectional outcomes are routinely collected in modern epidemiology studies, often with the goal of informing tailored early intervention decisions. For example, hormones, such as estradiol (E2) and follicle-stimulating hormone (FSH), may predict changes in womens’ health during the midlife. Most existing methods focus on constructing predictors from mean marker trajectories. However, subject-level biomarker variability may also provide critical information about disease risks and health outcomes. Current literature does not provide statistical models to investigate such relationships with valid uncertainty quantification. In this paper we develop a fully Bayesian joint model that estimates subject-level means, variances, and covariances of multiple longitudinal biomarkers and uses these as predictors to evaluate their respective associations with a cross-sectional health outcome. Simulations demonstrate excellent recovery of true model parameters. The proposed method provides less biased and more efficient estimates, relative to alternative approaches that either ignore subject-level differences in variances or perform two-stage estimation where estimated marker variances are treated as observed. Empowered by the model, analyses of women’s health data reveal, for the first time, that larger variability of E2 was associated with slower increases in waist circumference across the menopausal transition.},
  archive      = {J_AOAS},
  author       = {Irena Chen and Zhenke Wu and Siobán D. Harlow and Carrie A. Karvonen-Gutierrez and Michelle M. Hood and Michael R. Elliott},
  doi          = {10.1214/23-AOAS1852},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1642-1667},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Variance as a predictor of health outcomes: Subject-level trajectories and variability of sex hormones to predict body fat changes in peri- and postmenopausal women},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A framework for analysing longitudinal data involving
time-varying covariates. <em>AOAS</em>, <em>18</em>(2), 1618–1641. (<a
href="https://doi.org/10.1214/23-AOAS1851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard models for longitudinal data ignore the stochastic nature of time-varying covariates and their stochastic evolution over time by treating them as fixed variables. There have been recent methods for modelling time-varying covariates; however, those methods cannot be applied to analyse longitudinal data when the longitudinal response and the time-varying covariates for each subject are measured at different time points. Moreover, it is difficult to study the temporal effects of a time-varying covariate on the longitudinal response and the temporal correlation between them. Motivated by data from an AIDS cohort study conducted over 26 years at the University Hospitals Leuven in which the measurements on the CD4 cell count and viral load for patients are not taken at the same time point, we present a framework to address those challenges by using joint multivariate mixed models to jointly model time-varying covariates and a longitudinal response, instead of including time-varying covariates in the response model. This approach also has the advantage that one can study the association between the covariate at any time point and the response at any other time point without having to explicitly model the conditional distribution of the response given the covariate. We use penalised spline functions of time to capture the evolutions of both the response and time-varying covariates over time.},
  archive      = {J_AOAS},
  author       = {Reza Drikvandi and Geert Verbeke and Geert Molenberghs},
  doi          = {10.1214/23-AOAS1851},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1618-1641},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A framework for analysing longitudinal data involving time-varying covariates},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spatial predictions on physically constrained domains:
Applications to arctic sea salinity data. <em>AOAS</em>, <em>18</em>(2),
1596–1617. (<a href="https://doi.org/10.1214/23-AOAS1850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we predict sea surface salinity (SSS) in the Arctic Ocean based on satellite measurements. SSS is a crucial indicator for ongoing changes in the Arctic Ocean and can offer important insights about climate change. We particularly focus on areas of water mistakenly flagged as ice by satellite algorithms. To remove bias in the retrieval of salinity near sea ice, the algorithms use conservative ice masks, which result in considerable loss of data. We aim to produce realistic SSS values for such regions to obtain more complete understanding about the SSS surface over the Arctic Ocean and benefit future applications that may require SSS measurements near edges of sea ice or coasts. We propose a class of scalable nonstationary processes that can handle large data from satellite products and complex geometries of the Arctic Ocean. Barrier overlap-removal acyclic directed graph GP (BORA-GP) constructs sparse directed acyclic graphs (DAGs) with neighbors conforming to barriers and boundaries, enabling characterization of dependence in constrained domains. The BORA-GP models produce more sensible SSS values in regions without satellite measurements and show improved performance in various constrained domains in simulation studies compared to state-of-the-art alternatives. An R package is available at https://github.com/jinbora0720/boraGP.},
  archive      = {J_AOAS},
  author       = {Bora Jin and Amy H. Herring and David Dunson},
  doi          = {10.1214/23-AOAS1850},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1596-1617},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Spatial predictions on physically constrained domains: Applications to arctic sea salinity data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian hierarchical small area population model
accounting for data source specific methodologies from american
community survey, population estimates program, and decennial census
data. <em>AOAS</em>, <em>18</em>(2), 1565–1595. (<a
href="https://doi.org/10.1214/23-AOAS1849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small area population counts are necessary for many epidemiological studies, yet their quality and accuracy are often not assessed. In the United States, small area population counts are published by the United States Census Bureau (USCB) in the form of the decennial census counts, intercensal population projections (PEP), and American Community Survey (ACS) estimates. Although there are significant relationships between these three data sources, there are important contrasts in data collection, data availability, and processing methodologies such that each set of reported population counts may be subject to different sources and magnitudes of error. Additionally, these data sources do not report identical small area population counts due to post-survey adjustments specific to each data source. Consequently, in public health studies, small area disease/mortality rates may differ depending on which data source is used for denominator data. To accurately estimate annual small area population counts and their associated uncertainties, we present a Bayesian population (BPop) model, which fuses information from all three USCB sources, accounting for data source specific methodologies and associated errors. We produce comprehensive small area race-stratified estimates of the true population, and associated uncertainties, given the observed trends in all three USCB population estimates. The main features of our framework are: (1) a single model integrating multiple data sources, (2) accounting for data source specific data generating mechanisms and specifically accounting for data source specific errors, and (3) prediction of population counts for years without USCB reported data. We focus our study on the Black and White only populations for 159 counties of Georgia and produce estimates for years 2006–2023. We compare BPop population estimates to decennial census counts, PEP annual counts, and ACS multi-year estimates. Additionally, we illustrate and explain the different types of data source specific errors. Lastly, we compare model performance using simulations and validation exercises. Our Bayesian population model can be extended to other applications at smaller spatial granularity and for demographic subpopulations defined further by race, age, and sex, and/or for other geographical regions.},
  archive      = {J_AOAS},
  author       = {Emily N. Peterson and Rachel C. Nethery and Tullia Padellini and Jarvis T. Chen and Brent A. Coull and Frédéric B. Piel and Jon Wakefield and Marta Blangiardo and Lance A. Waller},
  doi          = {10.1214/23-AOAS1849},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1565-1595},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian hierarchical small area population model accounting for data source specific methodologies from american community survey, population estimates program, and decennial census data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Assessing screening efficacy in the presence of cancer
overdiagnosis. <em>AOAS</em>, <em>18</em>(2), 1543–1564. (<a
href="https://doi.org/10.1214/23-AOAS1848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer screening facilitates the early detection of cancer at a stage when treatment is often most effective. However, it also brings the risk of overdiagnosis, where a diagnosis made through screening would not have led to symptoms or death during the patient’s lifetime. In this paper we tackle a significant unresolved issue in the evaluation of screening efficacy: selecting primary endpoints and inferential procedures that efficiently consider potential overdiagnosis in screening trials. This is motivated by the necessity to design and analyze a phase IV Early Detection Initiative (EDI) trial for evaluating a pancreatic cancer screening strategy. We introduce two novel approaches for assessing screening efficacy, grounded on cancer stage shift. These methods address potential overdiagnosis by: (i) borrowing information about clinical diagnosis from the control arm that hasn’t undergone screening (the BR approach) and (ii) performing sensitivity analysis, contingent upon a conservative bound of the overdiagnosis magnitude (the SEN-T approach). Analytical methods and extensive simulation studies underscore the superiority of our proposed methods, demonstrating enhanced efficiency in estimating and testing screening efficacy compared to existing methods. The latter either overlook overdiagnosis or adhere to a valid, yet conservative, cumulative incidence endpoint. We illustrate the practical application of these approaches using ovarian cancer data from the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial. The results affirm that our methods bolster an efficient and robust study design for cancer screening trials.},
  archive      = {J_AOAS},
  author       = {Ying Huang and Ziding Feng},
  doi          = {10.1214/23-AOAS1848},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1543-1564},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Assessing screening efficacy in the presence of cancer overdiagnosis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling extremal streamflow using deep learning
approximations and a flexible spatial process. <em>AOAS</em>,
<em>18</em>(2), 1519–1542. (<a
href="https://doi.org/10.1214/23-AOAS1847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantifying changes in the probability and magnitude of extreme flooding events is key to mitigating their impacts. While hydrodynamic data are inherently spatially dependent, traditional spatial models, such as Gaussian processes, are poorly suited for modeling extreme events. Spatial extreme value models with more realistic tail dependence characteristics are under active development. They are theoretically justified but give intractable likelihoods, making computation challenging for small datasets and prohibitive for continental-scale studies. We propose a process mixture model (PMM) which specifies spatial dependence in extreme values as a convex combination of a Gaussian process and a max-stable process, yielding desirable tail dependence properties but intractable likelihoods. To address this, we employ a unique computational strategy where a feed-forward neural network is embedded in a density regression model to approximate the conditional distribution at one spatial location, given a set of neighbors. We then use this univariate density function to approximate the joint likelihood for all locations by way of a Vecchia approximation. The PMM is used to analyze changes in annual maximum streamflow within the U.S. over the last 50 years and is able to detect areas which show increases in extreme streamflow over time.},
  archive      = {J_AOAS},
  author       = {Reetam Majumder and Brian J. Reich and Benjamin A. Shaby},
  doi          = {10.1214/23-AOAS1847},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1519-1542},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling extremal streamflow using deep learning approximations and a flexible spatial process},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). As treated analyses of cluster randomized trials.
<em>AOAS</em>, <em>18</em>(2), 1506–1518. (<a
href="https://doi.org/10.1214/23-AOAS1846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-negative designs have rapidly become an appealing approach to assess disease interventions when randomization is not feasible and specifically used to measure the effectiveness of vaccines in the field (Vaccine 31 (2013) 2165–2168). An innovative extension of the test-negative design was recently used to assess the impact of a mosquito intervention where the intervention was applied at a cluster level with cluster assignment chosen at random, the AWED (applying Wolbachia to eliminate dengue) trial. The primary analysis reported was intention-to-treat (ITT) (Trials 19 (2018) 302; N. Engl. J. Med. 384 (2021) 2177–2186). However, the level of uptake of the intervention on mosquitoes was routinely captured in all clusters over time, and, furthermore, participants’ mobility across clusters was measured in the time immediately preceding the onset of symptoms (whether test-positive or test-negative). Combinations of these measurements provide proxies for the true exposure to the intervention, thereby permitting an “as treated” assessment. We consider the use of marginal generalized estimating equations (GEE) and conditional generalized inear mixed models (GLMM) to estimate as treated efficacy, contrasting both with the ITT. We illustrate the strengths and challenges of these methods in the context of the AWED trial, highlighting several ways that common approaches to analysis of clustered data can yield incorrect results that can in turn be obscured and compounded by limitations in routine software. In addition, we estimate a greater level of intervention efficacy than shown in the ITT analysis.},
  archive      = {J_AOAS},
  author       = {Ari I. F. Fogelson and Kirsten E. Landsiedel and Suzanne M. Dufault and Nicholas P. Jewell},
  doi          = {10.1214/23-AOAS1846},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1506-1518},
  shortjournal = {Ann. Appl. Stat.},
  title        = {As treated analyses of cluster randomized trials},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Penalized joint models of high-dimensional longitudinal
biomarkers and a survival outcome. <em>AOAS</em>, <em>18</em>(2),
1490–1505. (<a href="https://doi.org/10.1214/23-AOAS1844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional biomarkers, such as gene expression profiles, are often collected longitudinally to monitor disease progression in clinical studies, where the primary endpoint of interest is often a survival outcome. It is of great interest to study the associations between high-dimensional longitudinal biomarkers and the survival outcome as well as to identify biomarkers related to the survival outcome. Joint models, which have been extensively studied in the past decades, are commonly used to study the associations between longitudinal biomarkers and the survival outcome. However, existing joint models only consider one or a few longitudinal biomarkers and cannot deal with high-dimensional longitudinal biomarkers. In this paper we propose a novel penalized joint model that can handle high-dimensional longitudinal biomarkers. Specifically, we impose an adaptive lasso penalty on the parameters for the effects of the longitudinal biomarkers on the survival outcome, which allows for variable selection. We also develop a computationally efficient algorithm for model estimation based on the Gaussian variational approximation method, which can be implemented using the HDJM package in R. Furthermore, based on the penalized joint model, we propose a two-stage selection procedure that can reduce the estimation bias, due to the penalization, and allows for inference. We conduct extensive simulation studies to evaluate the performance of our proposed method. The performance of our proposed method is further demonstrated on a longitudinal gene expression dataset of patients with idiopathic pulmonary fibrosis.},
  archive      = {J_AOAS},
  author       = {Jiehuan Sun and Sanjib Basu},
  doi          = {10.1214/23-AOAS1844},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1490-1505},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Penalized joint models of high-dimensional longitudinal biomarkers and a survival outcome},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible instrumental variable models with bayesian additive
regression trees. <em>AOAS</em>, <em>18</em>(2), 1471–1489. (<a
href="https://doi.org/10.1214/23-AOAS1843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods utilizing instrumental variables have been a fundamental statistical approach to causal estimation in the presence of unmeasured confounding, usually occurring in nonrandomized observational data common to fields such as economics and public health. However, such methods traditionally make constricting linearity and additivity assumptions that are inapplicable to the complex modeling challenges of today. The growing body of observational data being collected may benefit from flexible regression modeling while also retaining the ability to control for confounding using instrumental variables. Therefore, this article presents a flexible instrumental variable regression model based on Bayesian regression tree ensembles to estimate the causal exposure-outcome relationship, including interactions with covariates, in the presence of confounding. One exciting application of this method is to use genetic variants as instruments, known as Mendelian randomization. We present our flexible Bayesian instrumental variable regression tree method with an example from the UK Biobank where body mass index is related to blood pressure using genetic variants as the instruments. Body mass index is one factor that is hypothesized to have a nonlinear relationship with cardiovascular risk factors, such as blood pressure, while interacting with age. Heterogeneity in patient characteristics, such as age, could be clinically interesting from a precision medicine perspective where individualized treatment is emphasized.},
  archive      = {J_AOAS},
  author       = {Charles Spanbauer and Wei Pan},
  doi          = {10.1214/23-AOAS1843},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1471-1489},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Flexible instrumental variable models with bayesian additive regression trees},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Analyzing cross-talk between superimposed signals: Vector
norm dependent hidden markov models and applications to ion channels.
<em>AOAS</em>, <em>18</em>(2), 1445–1470. (<a
href="https://doi.org/10.1214/23-AOAS1842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and investigate a hidden Markov model (HMM) for the analysis of dependent, aggregated, superimposed two-state signal recordings. A major motivation for this work is that often these signals cannot be observed individually but only their superposition. Among others, such models are in high demand for the understanding of cross-talk between ion channels, where each single channel cannot be measured separately. As an essential building block, we introduce a parameterized vector norm dependent Markov chain model and characterize it in terms of permutation invariance as well as conditional independence. This building block leads to a hidden Markov chain sum process which can be used for analyzing the dependence structure of superimposed two-state signal observations within an HMM. Notably, the model parameters of the vector norm dependent Markov chain are uniquely determined by the parameters of the sum process and are, therefore, identifiable. We provide algorithms to estimate the parameters, discuss model selection and apply our methodology to real-world ion channel data from the heart muscle, where we show competitive gating.},
  archive      = {J_AOAS},
  author       = {Laura Jula Vanegas and Benjamin Eltzner and Daniel Rudolf and Miroslav Dura and Stephan E. Lehnart and Axel Munk},
  doi          = {10.1214/23-AOAS1842},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1445-1470},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Analyzing cross-talk between superimposed signals: Vector norm dependent hidden markov models and applications to ion channels},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Forecasting u.s. Inflation using bayesian nonparametric
models. <em>AOAS</em>, <em>18</em>(2), 1421–1444. (<a
href="https://doi.org/10.1214/23-AOAS1841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relationship between inflation and predictors, such as unemployment, is potentially nonlinear with a strength that varies over time, and prediction errors may be subject to large, asymmetric shocks. Inspired by these concerns, we develop a model for inflation forecasting that is nonparametric both in the conditional mean and in the error using Gaussian and Dirichlet processes, respectively. We discuss how both these features may be important in producing accurate forecasts of inflation. In a forecasting exercise involving CPI inflation, we find that our approach has substantial benefits, both overall and in the left tail, with nonparametric modeling of the conditional mean being of particular importance.},
  archive      = {J_AOAS},
  author       = {Todd E. Clark and Florian Huber and Gary Koop and Massimiliano Marcellino},
  doi          = {10.1214/23-AOAS1841},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1421-1444},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Forecasting U.S. inflation using bayesian nonparametric models},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hierarchical dependence modeling for the analysis of large
insurance claims data. <em>AOAS</em>, <em>18</em>(2), 1404–1420. (<a
href="https://doi.org/10.1214/23-AOAS1840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme weather events associated with climate change have caused significant damages. In particular, hail storms damage millions of properties in the U.S. and result in billion-dollar insured losses each year in the recent decade. To facilitate the insurance claims management operations in insurance companies, we construct a hierarchical dependence model, which accommodates the complex dependence within and between the outcomes of interests including the propensity of filing a claim, time to report a claim, and the claim amount. The storm-specific and property-specific characteristics are incorporated through marginal models, such as generalized linear models and survival analysis models. The dependence within the hail event is captured by spatial factor copula, while the dependence between different outcomes is captured by bivariate copula. For parameter estimation we develop a two-step procedure that first maximizes the marginal likelihood function and then maximizes the pairwise likelihood, which ensures computational feasibility for big data. We apply this modeling framework to analyze a large dataset involving hail storms in Colorado from 2011 to 2015 impacting hundreds of thousands of insured properties and demonstrate that the predictive performance can be improved by our proposed methodology.},
  archive      = {J_AOAS},
  author       = {Ting Fung Ma and Yizhou Cai and Peng Shi and Jun Zhu},
  doi          = {10.1214/23-AOAS1840},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1404-1420},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Hierarchical dependence modeling for the analysis of large insurance claims data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Flexible multivariate spatiotemporal hawkes process models
of terrorism. <em>AOAS</em>, <em>18</em>(2), 1378–1403. (<a
href="https://doi.org/10.1214/23-AOAS1839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop flexible multivariate spatiotemporal Hawkes process models to analyze patterns of terrorism. Previous applications of point process methods to political violence data mainly utilize temporal Hawkes process models, neglecting spatial variation in these attack patterns. This limits what can be learned from these models, as any effective counter-terrorism strategy requires knowledge on both when and where attacks are likely to occur. Even the existing work on spatiotemporal Hawkes processes imposes restrictions on the triggering function that are not well-suited for terrorism data. Therefore, we generalize the structure of the spatiotemporal triggering function considerably, allowing for nonseparability, nonstationarity, and cross-triggering (across multiple terror groups). To demonstrate the utility of our models, we analyze two samples of real-world terrorism data: Afghanistan (2002–2013) as a univariate analysis and Nigeria (2009–2017) as a bivariate analysis. Jointly, these two studies demonstrate that our generalized models outperform standard Hawkes process models, besting widely-used alternatives in overall model fit and revealing spatiotemporal patterns that are, by construction, masked in these models (e.g., increasing dispersion in cross-triggering over time).},
  archive      = {J_AOAS},
  author       = {Mikyoung Jun and Scott Cook},
  doi          = {10.1214/23-AOAS1839},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1378-1403},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Flexible multivariate spatiotemporal hawkes process models of terrorism},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). MASH: Mediation analysis of survival outcome and
high-dimensional omics mediators with application to complex diseases.
<em>AOAS</em>, <em>18</em>(2), 1360–1377. (<a
href="https://doi.org/10.1214/23-AOAS1838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental exposures, such as cigarette smoking, influence health outcomes through intermediate molecular phenotypes, such as the methylome, transcriptome, and metabolome. Mediation analysis is a useful tool for investigating the role of potentially high-dimensional intermediate phenotypes in the relationship between environmental exposures and health outcomes. However, little work has been done on mediation analysis when the mediators are high-dimensional and the outcome is a survival endpoint, and none of it has provided a robust measure of total mediation effect. To this end, we propose an estimation procedure for Mediation Analysis of Survival outcome and High-dimensional omics mediators (MASH), based on a second-moment-based measure of total mediation effect for survival data analogous to the R2 measure in a linear model. In addition, we propose a three-step mediator selection procedure to mitigate potential bias induced by nonmediators. Extensive simulations showed good performance of MASH in estimating the total mediation effect and identifying true mediators. By applying MASH to the metabolomics data of 1919 subjects in the Framingham Heart Study, we identified five metabolites as mediators of the effect of cigarette smoking on coronary heart disease risk (total mediation effect, 51.1%) and two metabolites as mediators between smoking and risk of cancer (total mediation effect, 50.7%). Application of MASH to a diffuse large B-cell lymphoma genomics data set identified copy-number variations for eight genes as mediators between the baseline International Prognostic Index score and overall survival.},
  archive      = {J_AOAS},
  author       = {Sunyi Chi and Christopher R. Flowers and Ziyi Li and Xuelin Huang and Peng Wei},
  doi          = {10.1214/23-AOAS1838},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1360-1377},
  shortjournal = {Ann. Appl. Stat.},
  title        = {MASH: Mediation analysis of survival outcome and high-dimensional omics mediators with application to complex diseases},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Privacy-preserving, communication-efficient, and
target-flexible hospital quality measurement. <em>AOAS</em>,
<em>18</em>(2), 1337–1359. (<a
href="https://doi.org/10.1214/23-AOAS1837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate hospital performance measurement is important to both patients and providers but is challenging due to case-mix heterogeneity, differences in treatment guidelines, and data privacy regulations that preclude the sharing of individual patient data. Motivated to overcome these issues in the setting of hospital quality measurement, we develop a federated causal inference framework. We devise a doubly robust estimator of the mean potential outcome in a target population and show that it is consistent even when some models are misspecified. To enable real-world use, our proposed algorithm is privacy-preserving (requiring only summary statistics to be shared between hospitals) and communication-efficient (requiring only one round of communication between hospitals). We show that our estimator has good finite sample properties in simulation studies. We investigate the quality of hospital care provided by a diverse set of 51 candidate Cardiac Centers of Excellence, as measured by 30-day mortality and length of stay for acute myocardial infarction (AMI) patients. We find that our proposed federated global estimator improves the precision of treatment effect estimates by 34% to 86%, compared to using data from the target hospital alone. This precision gain results in qualitatively different conclusions about the estimated effect of percutaneous coronary intervention (PCI), compared to medical management (MM) in 43% (22 of 51) of hospitals.},
  archive      = {J_AOAS},
  author       = {Larry Han and Yige Li and Bijan Niknam and José R. Zubizarreta},
  doi          = {10.1214/23-AOAS1837},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1337-1359},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Privacy-preserving, communication-efficient, and target-flexible hospital quality measurement},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Risk-aware restricted outcome learning for individualized
treatment regimes of schizophrenia. <em>AOAS</em>, <em>18</em>(2),
1319–1336. (<a href="https://doi.org/10.1214/23-AOAS1836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia is a severe mental disorder that distorts patients’ perception of reality, and its treatment with antipsychotics can lead to significant side effects. Despite the heterogeneity in patient responses to treatments, most existing studies on individualized treatment regimes only focus on optimizing treatment efficacy, disregarding potential negative effects. To fill this gap, we propose a restricted outcome weighted learning method that optimizes efficacy outcomes while adhering to individual-level negative effect constraints. Our method is developed for multistage treatment decision problems that include single-stage decision as a special case. We propose an efficient learning algorithm that utilizes the difference-of-convex algorithm and the Lagrange multiplier to solve nonconvex optimization with nonconvex risk constraints. We also establish theoretical properties, including Fisher consistency and strong duality results, for the proposed method. We apply our method to a clinical study to design effective schizophrenia treatment [Stroup et al. (Schizophr. Bull. 29 (2003) 15–31)] and find that our approach reduces side-effect risk by at least 22.5% and improves efficacy by at least 26.3% compared to competing methods. In addition, we discover that certain covariates, such as the PANSS score, clinician global impressions severity score, and BMI, have a significant impact on controlling side effects and determining optimal treatment recommendations. These results are valuable in identifying subgroups of patients who need special attention when prescribing more aggressive treatment plans.},
  archive      = {J_AOAS},
  author       = {Shuying Zhu and Weining Shen and Haoda Fu and Annie Qu},
  doi          = {10.1214/23-AOAS1836},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1319-1336},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Risk-aware restricted outcome learning for individualized treatment regimes of schizophrenia},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor quantile regression with low-rank tensor train
estimation. <em>AOAS</em>, <em>18</em>(2), 1294–1318. (<a
href="https://doi.org/10.1214/23-AOAS1835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroimaging studies often involve predicting a scalar outcome from an array of images collectively called tensor. The use of magnetic resonance imaging (MRI) provides a unique opportunity to investigate the structures of the brain. To learn the association between MRI images and human intelligence, we formulate a scalar-on-image quantile regression framework. However, the high dimensionality of the tensor makes estimating the coefficients for all elements computationally challenging. To address this, we propose a low-rank coefficient array estimation algorithm, based on tensor train (TT) decomposition, which we demonstrate can effectively reduce the dimensionality of the coefficient tensor to a feasible level while ensuring adequacy to the data. Our method is more stable and efficient compared to the commonly used canonic polyadic rank approximation-based method. We also propose a generalized lasso penalty on the coefficient tensor to take advantage of the spatial structure of the tensor, further reduce the dimensionality of the coefficient tensor, and improve the interpretability of the model. The consistency and asymptotic normality of the TT estimator are established under some mild conditions on the covariates and random errors in quantile regression models. The rate of convergence is obtained with regularization under the total variation penalty. Extensive numerical studies, including both synthetic and real MRI imaging data, are conducted to examine the empirical performance of the proposed method and its competitors.},
  archive      = {J_AOAS},
  author       = {Zihuan Liu and Cheuk Yin Lee and Heping Zhang},
  doi          = {10.1214/23-AOAS1835},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1294-1318},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tensor quantile regression with low-rank tensor train estimation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semiparametric bivariate hierarchical state space model with
application to hormone circadian relationship. <em>AOAS</em>,
<em>18</em>(2), 1275–1293. (<a
href="https://doi.org/10.1214/23-AOAS1834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adrenocorticotropic hormone and cortisol play critical roles in stress regulation and the sleep-wake cycle. Most research has been focused on how the two hormones regulate each other in terms of short-term pulses. Few studies have been conducted on the circadian relationship between the two hormones and how it differs between normal and abnormal groups. The circadian patterns are difficult to model as parametric functions. Directly extending univariate functional mixed effects models would result in a large dimensional problem and a challenging nonparametric inference. In this article we propose a semiparametric bivariate hierarchical state space model in which each hormone profile is modeled by a hierarchical state space model with nonparametric population-average and subject-specific components. The bivariate relationship is constructed by concatenating two latent independent subject-specific random functions specified by a design matrix, leading to a parametric inference on the correlation. We propose a computationally efficient state-space EM algorithm for estimation and inference. We apply the proposed method to a study of chronic fatigue syndrome and fibromyalgia and discover an erratic regulation pattern in the patient group in contrast to a circadian regulation pattern conforming to the day–night cycle in the control group.},
  archive      = {J_AOAS},
  author       = {Mengying You and Wensheng Guo},
  doi          = {10.1214/23-AOAS1834},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1275-1293},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Semiparametric bivariate hierarchical state space model with application to hormone circadian relationship},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating the likelihood of arrest from police records in
presence of unreported crimes. <em>AOAS</em>, <em>18</em>(2), 1253–1274.
(<a href="https://doi.org/10.1214/23-AOAS1833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many important policy decisions concerning policing hinge on our understanding of how likely various criminal offenses are to result in arrests. Since many crimes are never reported to law enforcement, estimates based on police records alone must be adjusted to account for the likelihood that each crime would have been reported to the police. In this paper we present a methodological framework for estimating the likelihood of arrest from police data that incorporates estimates of crime reporting rates computed from a victimization survey. We propose a parametric regression-based two-step estimator that: (i) estimates the likelihood of crime reporting using logistic regression with survey weights and then (ii) applies a second regression step to model the likelihood of arrest. Our empirical analysis focuses on racial disparities in arrests for violent crimes (sex offenses, robbery, aggravated and simple assaults) from 2006–2015 police records from the National Incident Based Reporting System (NIBRS), with estimates of crime reporting obtained using 2003–2020 data from the National Crime Victimization Survey (NCVS). We find that, after adjusting for unreported crimes, the likelihood of arrest computed from police records decreases significantly. We also find that, while incidents with white offenders, on average, result in arrests more often than those with black offenders, the disparities tend to be small after accounting for crime characteristics and unreported crimes.},
  archive      = {J_AOAS},
  author       = {Riccardo Fogliato and Arun Kumar Kuchibhotla and Zachary Lipton and Daniel Nagin and Alice Xiang and Alexandra Chouldechova},
  doi          = {10.1214/23-AOAS1833},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1253-1274},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating the likelihood of arrest from police records in presence of unreported crimes},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Athlete rating in multicompetitor games with scored outcomes
via monotone transformations. <em>AOAS</em>, <em>18</em>(2), 1236–1252.
(<a href="https://doi.org/10.1214/23-AOAS1832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports organizations often want to estimate athlete strengths. For games with scored outcomes, a common approach is to assume observed game scores follow a normal distribution conditional on athletes’ latent abilities, which may change over time. In many games, however, this assumption of conditional normality does not hold. To estimate athletes’ time-varying latent abilities using nonnormal game score data, we propose a Bayesian dynamic linear model with flexible monotone response transformations. Our model learns nonlinear monotone transformations to address nonnormality in athlete scores and can be easily fit using standard regression and optimization routines, which we implement in the dlmt package in R. We demonstrate our method on data from several Olympic sports, including biathlon, diving, rugby, and fencing.},
  archive      = {J_AOAS},
  author       = {Jonathan Che and Mark Glickman},
  doi          = {10.1214/23-AOAS1832},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1236-1252},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Athlete rating in multicompetitor games with scored outcomes via monotone transformations},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning common structures in a collection of networks. An
application to food webs. <em>AOAS</em>, <em>18</em>(2), 1213–1235. (<a
href="https://doi.org/10.1214/23-AOAS1831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let a collection of networks represent interactions within several (social or ecological) systems. We pursue two objectives: identifying similarities in the topological structures that are held in common between the networks and clustering the collection into subcollections of structurally homogeneous networks. We tackle these two questions with a probabilistic model-based approach. We propose an extension of the stochastic block model (SBM) adapted to the joint modeling of a collection of networks. The networks in the collection are assumed to be independent realizations of SBMs. The common connectivity structure is imposed through the equality of some parameters. The model parameters are estimated with a variational expectation-maximization (EM) algorithm. We derive an ad hoc penalized likelihood criterion to select the number of blocks and to assess the adequacy of the consensus found between the structures of the different networks. This same criterion can also be used to cluster networks on the basis of their connectivity structure. It thus provides a partition of the collection into subsets of structurally homogeneous networks. The relevance of our proposition is assessed on two collections of ecological networks. First, an application to three stream food webs reveals the homogeneity of their structures and the correspondence between groups of species in different ecosystems playing equivalent ecological roles. Moreover, the joint analysis allows a finer analysis of the structure of smaller networks. Second, we cluster 67 food webs according to their connectivity structures and demonstrate that five mesoscale structures are sufficient to describe this collection.},
  archive      = {J_AOAS},
  author       = {Saint-Clair Chabert-Liddell and Pierre Barbillon and Sophie Donnet},
  doi          = {10.1214/23-AOAS1831},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1213-1235},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning common structures in a collection of networks. an application to food webs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor regression for incomplete observations with
application to longitudinal studies. <em>AOAS</em>, <em>18</em>(2),
1195–1212. (<a href="https://doi.org/10.1214/23-AOAS1830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate longitudinal data are frequently encountered in practice such as in our motivating longitudinal microbiome study. It is of general interest to associate such high-dimensional, longitudinal measures with some univariate continuous outcome. However, incomplete observations are common in a regular study design, as not all samples are measured at every time point, giving rise to the so-called blockwise missing values. Such missing structure imposes significant challenges for association analysis and defies many existing methods that require complete samples. In this paper we propose to represent multivariate longitudinal data as a three-way tensor array (i.e., sample-by-feature-by-time) and exploit a parsimonious scalar-on-tensor regression model for association analysis. We develop a regularized covariance-based estimation procedure that effectively leverages all available observations without imputation. The method achieves variable selection and smooth estimation of time-varying effects. The application to the motivating microbiome study reveals interesting links between the preterm infant’s gut microbiome dynamics and their neurodevelopment. Additional numerical studies on synthetic data and a longitudinal aging study further demonstrate the efficacy of the proposed method.},
  archive      = {J_AOAS},
  author       = {Tianchen Xu and Kun Chen and Gen Li},
  doi          = {10.1214/23-AOAS1830},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1195-1212},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tensor regression for incomplete observations with application to longitudinal studies},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Accurate estimation of rare cell-type fractions from tissue
omics data via hierarchical deconvolution. <em>AOAS</em>,
<em>18</em>(2), 1178–1194. (<a
href="https://doi.org/10.1214/23-AOAS1829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bulk transcriptomics in tissue samples reflects the average expression levels across different cell types and is highly influenced by cellular fractions. As such, it is critical to estimate cellular fractions to both deconfound differential expression analyses and infer cell type-specific differential expression. Since experimentally counting cells is infeasible in most tissues and studies, in silico cellular deconvolution methods have been developed as an alternative. However, existing methods are designed for tissues consisting of clearly distinguishable cell types and have difficulties estimating highly correlated or rare cell types. To address this challenge, we propose hierarchical deconvolution (HiDecon) that uses single-cell RNA sequencing references and a hierarchical cell-type tree, which models the similarities among cell types and cell differentiation relationships, to estimate cellular fractions in bulk data. By coordinating cell fractions across layers of the hierarchical tree, cellular fraction information is passed up and down the tree, which helps correct estimation biases by pooling information across related cell types. The flexible hierarchical tree structure also enables estimating rare cell fractions by splitting the tree to higher resolutions. Through simulations and real data applications with the ground truth of measured cellular fractions, we demonstrate that HiDecon outperforms existing methods and accurately estimates cellular fractions. Finally, we show the utility of HiDecon estimates in identifying the associations between cellular fractions and Alzheimer’s disease.},
  archive      = {J_AOAS},
  author       = {Penghui Huang and Manqi Cai and Xinghua Lu and Chris McKennan and Jiebiao Wang},
  doi          = {10.1214/23-AOAS1829},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1178-1194},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Accurate estimation of rare cell-type fractions from tissue omics data via hierarchical deconvolution},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Filtrated common functional principal component analysis of
multigroup functional data. <em>AOAS</em>, <em>18</em>(2), 1160–1177.
(<a href="https://doi.org/10.1214/23-AOAS1827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local field potentials (LFPs) are signals that measure electrical activities in localized cortical regions and are collected from multiple tetrodes implanted across a patch on the surface of cortex. Hence, they can be treated as multigroup functional data, where the trajectories collected across temporal epochs from one tetrode are viewed as a group of functions. In many cases multitetrode LFP trajectories contain both global variation patterns (which are shared by most groups, due to signal synchrony) and idiosyncratic variation patterns (common only to a small subset of groups), and such structure is very informative to the data mechanism. Therefore, one goal in this paper is to develop an efficient algorithm that is able to capture and quantify both global and idiosyncratic features. We develop the novel filtrated common functional principal components (filt-fPCA) method, which is a novel forest-structured fPCA for multigroup functional data. A major advantage of the proposed filt-fPCA method is its ability to extract the common components in a flexible “multiresolution” manner. The proposed approach is highly data-driven, and no prior knowledge of “ground-truth” data structure is needed, making it suitable for analyzing complex multigroup functional data. In addition, the filt-fPCA method is able to produce parsimonious, interpretable, and efficient functional reconstruction (low reconstruction error) for multigroup functional data with orthonormal basis functions. Here the proposed filt-fPCA method is employed to study the impact of a shock (induced stroke) on the synchrony structure of rat brain. The proposed filt-fPCA is general and inclusive that can be readily applied to analyze any multigroup functional data, such as multivariate functional data, spatial-temporal data, and longitudinal functional data.},
  archive      = {J_AOAS},
  author       = {Shuhao Jiao and Ron Frostig and Hernando Ombao},
  doi          = {10.1214/23-AOAS1827},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1160-1177},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Filtrated common functional principal component analysis of multigroup functional data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian nested latent class models for cause-of-death
assignment using verbal autopsies across multiple domains.
<em>AOAS</em>, <em>18</em>(2), 1137–1159. (<a
href="https://doi.org/10.1214/23-AOAS1826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding cause-specific mortality rates is crucial for monitoring population health and designing public health interventions. Worldwide, two-thirds of deaths do not have a cause assigned. Verbal autopsy (VA) is a well-established tool to collect information describing deaths outside of hospitals by conducting surveys to caregivers of a deceased person. It is routinely implemented in many low- and middle-income countries. Statistical algorithms to assign cause of death using VAs are typically vulnerable to the distribution shift between the data used to train the model and the target population. This presents a major challenge for analyzing VAs, as labeled data are usually unavailable in the target population. This article proposes a latent class model framework for VA data (LCVA) that jointly models VAs collected over multiple heterogeneous domains, assigns causes of death for out-of-domain observations and estimates cause-specific mortality fractions for a new domain. We introduce a parsimonious representation of the joint distribution of the collected symptoms using nested latent class models and develop a computationally efficient algorithm for posterior inference. We demonstrate that LCVA outperforms existing methods in predictive performance and scalability. Supplementary Material and reproducible analysis codes are available online. The R package LCVA implementing the method is available on GitHub (https://github.com/richardli/LCVA).},
  archive      = {J_AOAS},
  author       = {Zehang Richard Li and Zhenke Wu and Irena Chen and Samuel J. Clark},
  doi          = {10.1214/23-AOAS1826},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1137-1159},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian nested latent class models for cause-of-death assignment using verbal autopsies across multiple domains},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A population-aware retrospective regression to detect
genome-wide variants with sex difference in allele frequency.
<em>AOAS</em>, <em>18</em>(2), 1113–1136. (<a
href="https://doi.org/10.1214/23-AOAS1825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sex difference in allele frequency is an emerging topic that is crucial to our understanding of data quality and features, particularly when it comes to the largely overlooked X chromosome. To detect sex differences in allele frequency for both X chromosomal and autosomal variants, the existing method is conservative when applied to samples from multiple ancestral populations. Additionally, it remains unexplored whether the sex difference in allele frequency varies between populations, which is important for transancestral genetic studies. To answer these questions, we thus developed a novel, retrospective regression-based testing framework that led to interpretable and easy-to-implement solutions. We then applied the proposed methods to the high-coverage whole genome sequence data of the 1000 Genomes Project, robustly analyzing all samples available from the five super-populations. We had 97 novel findings by recognizing and modelling ancestral differences. Finally, we replicated the specific findings and overall conclusion using the gnomAD v3.1.2 data.},
  archive      = {J_AOAS},
  author       = {Zhong Wang and Andrew D. Paterson and Lei Sun},
  doi          = {10.1214/23-AOAS1825},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1113-1136},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A population-aware retrospective regression to detect genome-wide variants with sex difference in allele frequency},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network method for voxel-pair-level brain connectivity
analysis under spatial-contiguity constraints. <em>AOAS</em>,
<em>18</em>(2), 1090–1112. (<a
href="https://doi.org/10.1214/23-AOAS1824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain connectome analysis commonly compresses high-resolution brain scans (typically composed of millions of voxels) down to only hundreds of regions of interest (ROIs) by averaging within-ROI signals. This significant dimension reduction improves computational speed and the morphological properties of anatomical structures; however, it comes at the cost of substantial losses in spatial specificity and sensitivity, especially when the signals exhibit high within-ROI heterogeneity. Oftentimes, abnormally expressed functional connectivity (FC) between a pair of ROIs, caused by a brain disease, is primarily driven by only small subsets of voxel pairs within the ROI pair. This article proposes a new network method for the detection of voxel-pair-level neural dysconnectivity with spatial constraints. Specifically, focusing on an ROI pair, our model aims to extract dense subareas that contain aberrant voxel-pair connections while ensuring that the involved voxels are spatially contiguous. In addition, we develop subcommunity-detection algorithms to realize the model, and we justify the consistency of these algorithms. Comprehensive simulation studies demonstrate our method’s effectiveness in reducing the false-positive rate while increasing statistical power, detection replicability, and spatial specificity. We apply our approach to reveal: (i) disrupted voxelwise FC patterns related to nicotine addiction between the basal ganglia, hippocampus, and insular gyrus in 3269 participants using UK Biobank data; (ii) voxelwise schizophrenia-altered FC patterns within the salience and temporal-thalamic network in 330 participants in a schizophrenia study. The detected results align with previous medical findings but include improved localized information.},
  archive      = {J_AOAS},
  author       = {Tong Lu and Yuan Zhang and Peter Kochunov and Elliot Hong and Shuo Chen},
  doi          = {10.1214/23-AOAS1824},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1090-1112},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Network method for voxel-pair-level brain connectivity analysis under spatial-contiguity constraints},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficient and effective calibration of numerical model
outputs using hierarchical dynamic models. <em>AOAS</em>,
<em>18</em>(2), 1064–1089. (<a
href="https://doi.org/10.1214/23-AOAS1823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical air quality models, such as the Community Multiscale Air Quality (CMAQ) system, play a critical role in characterizing pollution levels at fine spatial and temporal scales. The model outputs, however, tend to systematically over- or underestimate the real pollutant concentrations. In this study we propose a Bayesian hierarchical dynamic model to calibrate large-scale grid-level CMAQ model outputs using data from other sources, especially point-level observations from sparsely located monitoring stations. In our model a stochastic integro-differential equation (IDE) is implemented to account for space-time interactions of air pollutants. To better approximate the spatial pattern of pollutants, we employ nonregular meshes to discretize IDEs. A spatial partitioning procedure is embedded to improve the scalability of the approach for very large meshes. An algorithm based on variational Bayes and ensemble Kalman smoother is developed to accelerate the parameter estimation and calibration procedure. We apply the proposed approach to calibrate CMAQ outputs for China’s Beijing–Tianjin–Hebei region. In contrast to existing methods, the proposed approach captures space-time interactions, produces more accurate calibration results, and operates at a higher computational efficiency. A reanalysis dataset is also adopted to demonstrate the effectiveness and efficiency of our approach to large spatial data.},
  archive      = {J_AOAS},
  author       = {Yewen Chen and Xiaohui Chang and Bohai Zhang and Hui Huang},
  doi          = {10.1214/23-AOAS1823},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1064-1089},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Efficient and effective calibration of numerical model outputs using hierarchical dynamic models},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Functional partial least squares with censored outcomes:
Prediction of breast cancer risk with mammogram images. <em>AOAS</em>,
<em>18</em>(2), 1051–1063. (<a
href="https://doi.org/10.1214/23-AOAS1822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of predicting breast cancer risk using mammogram imaging data where the dimension of pixels greatly exceed the number of individuals in the cohort. The functional partial least squares (FPLS) is a popular dimensional reduction method in constructing latent explanatory components using linear combinations of the original predictor variables. While FPLS with scalar responses has been studied in the literature, the presence of right censoring under the survival framework poses challenges in modeling and estimation. Given several different representations for PLS with Cox regression in the literature, we unify and extend three formulations to deal with right censoring, that is, reweighing, mean imputation, and deviance residuals to the functional setting in this paper. We empirically investigate and compare the performance of the three proposed FPLS frameworks in the context of imaging predictor via intensive simulation studies. The proposed methods are applied to the Joanne Knight Breast Health Cohort where we show increased model discriminatory performance under the FPLS framework compared to competing models.},
  archive      = {J_AOAS},
  author       = {Shu Jiang and Jiguo Cao and Graham A. Colditz},
  doi          = {10.1214/23-AOAS1822},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1051-1063},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Functional partial least squares with censored outcomes: Prediction of breast cancer risk with mammogram images},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Information-incorporated clustering analysis of disease
prevalence trends. <em>AOAS</em>, <em>18</em>(2), 1035–1050. (<a
href="https://doi.org/10.1214/23-AOAS1821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical research the analysis of disease prevalence is of critical importance. While most of the existing prevalence studies focus on individual diseases, there has been increasing effort that jointly examines the prevalence values and their trends of multiple diseases. Such joint analysis can provide valuable insights not shared by individual-disease analysis. A critical limitation of the existing analysis is that there is a lack of attention to existing information, which has been accumulated through a large number of studies and can be valuable especially when there are a large number of diseases but the number of prevalence values for a specific disease is limited. In this study we conduct the functional clustering analysis of prevalence trends for a large number of diseases. A novel approach based on the penalized fusion technique is developed to incorporate information mined from published articles. It is innovatively designed to take into account that such information may not be fully relevant or correct. Another significant development is that statistical properties are rigorously established. Simulation is conducted and demonstrates its competitive performance. In the analysis of data from Taiwan NHIRD (National Health Insurance Research Database), new and interesting findings that differ from the existing ones are made.},
  archive      = {J_AOAS},
  author       = {Chenjin Ma and Cunjie Lin and Yuan Xue and Sanguo Zhang and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1214/23-AOAS1821},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1035-1050},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Information-incorporated clustering analysis of disease prevalence trends},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Readability prediction: How many features are necessary?
<em>AOAS</em>, <em>18</em>(2), 1010–1034. (<a
href="https://doi.org/10.1214/23-AOAS1820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, readability prediction has relied on readability formulas, which are based on shallow text characteristics such as average word and sentence length. With recent advances in text mining and natural language processing, more complex text properties can be incorporated into readability prediction models, with papers in the literature suggesting to use up to 200 features for predicting text readability. However, many of the features generated using natural language processing tools are highly correlated and can be thought to measure similar latent text properties. When dealing with a high-dimensional space of correlated features, removing the redundant variables has two advantages: (1) improving interpretability and (2) increasing the predictive power of the model. In this paper we propose an ordinal version of the averaged lasso, which combines hierarchical clustering with the lasso, in order to identify relevant features for readability prediction. We illustrate the approach on two corpora and show improved prediction accuracy when benchmarking against a set of competing models. The annotated corpora as well as the steps necessary for feature creation are freely available as R packages, thus allowing the obtained results to be directly incorporated into a readability estimation pipeline.},
  archive      = {J_AOAS},
  author       = {Florian Schwendinger and Laura Vana and Kurt Hornik},
  doi          = {10.1214/23-AOAS1820},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {1010-1034},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Readability prediction: How many features are necessary?},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Identification of influencing factors on self-reported count
data with multiple potential inflated values. <em>AOAS</em>,
<em>18</em>(2), 991–1009. (<a
href="https://doi.org/10.1214/23-AOAS1819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The online chauffeured service demand (OCSD) research is an exploratory market study of designated driver services in China. Researchers are interested in the influencing factors of chauffeured service adoption and usage and have collected relevant data using a self-reported questionnaire. As self-reported count measure data is typically inflated, there exist challenges to its validity, which may bias estimation and increase error in empirical research. Motivated by the analysis of self-reported data with multiple inflated values, we propose a novel approach to simultaneously achieve data-driven inflated value selection and identification of important influencing factors. In particular, the regularization technique is applied to the mixing proportions of inflated values and the regression parameters to obtain shrinkage estimates. We analyze the OCSD data with the proposed approach, deriving insights into the determinants impacting service demand. The proper interpretations and implications contribute to service promotion and related policy optimization. Extensive simulation studies and consistent asymptotic properties further establish the effectiveness of the proposed approach.},
  archive      = {J_AOAS},
  author       = {Yang Li and Mingcong Wu and Mengyun Wu and Shuangge Ma},
  doi          = {10.1214/23-AOAS1819},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {991-1009},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Identification of influencing factors on self-reported count data with multiple potential inflated values},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A latent mixture model for heterogeneous causal mechanisms
in mendelian randomization. <em>AOAS</em>, <em>18</em>(2), 966–990. (<a
href="https://doi.org/10.1214/23-AOAS1816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a popular method in epidemiology and genetics that uses genetic variation as instrumental variables for causal inference. Existing MR methods usually assume most genetic variants are valid instrumental variables that identify a common causal effect. There is a general lack of awareness that this effect homogeneity assumption can be violated when there are multiple causal pathways involved, even if all the instrumental variables are valid. In this article we introduce a latent mixture model MR-Path that groups instruments that yield similar causal effect estimates together. We develop a Monte Carlo EM algorithm to fit this mixture model, derive approximate confidence intervals for uncertainty quantification, and adopt a modified Bayesian Information Criterion (BIC) for model selection. We verify the efficacy of the Monte Carlo EM algorithm, confidence intervals, and model selection criterion using numerical simulations. We identify potential mechanistic heterogeneity when applying our method to estimate the effect of high-density lipoprotein cholesterol on coronary heart disease and the effect of adiposity on type II diabetes.},
  archive      = {J_AOAS},
  author       = {Daniel Iong and Qingyuan Zhao and Yang Chen},
  doi          = {10.1214/23-AOAS1816},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {966-990},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A latent mixture model for heterogeneous causal mechanisms in mendelian randomization},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian hierarchical modelling of sparse count processes in
retail analytics. <em>AOAS</em>, <em>18</em>(2), 946–965. (<a
href="https://doi.org/10.1214/23-AOAS1811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of retail analytics has been transformed by the availability of rich data, which can be used to perform tasks such as demand forecasting and inventory management. However, one task which has proved more challenging is the forecasting of demand for products which exhibit very few sales. The sparsity of the resulting data limits the degree to which traditional analytics can be deployed. To combat this, we represent sales data as a structured sparse multivariate point process, which allows for features such as autocorrelation, cross-correlation, and temporal clustering, known to be present in sparse sales data. We introduce a Bayesian point process model to capture these phenomena, which includes a hurdle component to cope with sparsity and an exciting component to cope with temporal clustering within and across products. We then cast this model within a Bayesian hierarchical framework, to allow the borrowing of information across different products, which is key in addressing the data sparsity per product. We conduct a detailed analysis, using real sales data, to show that this model outperforms existing methods in terms of predictive power, and we discuss the interpretation of the inference.},
  archive      = {J_AOAS},
  author       = {James Pitkin and Ioanna Manolopoulou and Gordon Ross},
  doi          = {10.1214/23-AOAS1811},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {946-965},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian hierarchical modelling of sparse count processes in retail analytics},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A high-dimensional approach to measure connectivity in the
financial sector. <em>AOAS</em>, <em>18</em>(2), 922–945. (<a
href="https://doi.org/10.1214/22-AOAS1702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven network models to measure systemic risk in the financial sector and identify “too-connected-to-fail” institutions are becoming increasingly common in financial applications. Existing statistical methods for building such networks either take a pairwise approach of fitting many bivariate models or a system-wide approach of fitting penalized regression models. The former strategy is prone to large false positive selection, while the latter suffers from shrinkage bias and lack of formal inference machinery. These issues are accentuated in small sample, low signal-to-noise settings common in financial data. Building up on recent advances in high-dimensional inference, we propose debiased lasso Penalized Vector Autoregression (DLVAR), a method for building financial networks that addresses these limitations. Our empirical analysis highlights the importance of debiasing in a way that increases power of the algorithm in finite samples. We also provide formal inference guarantees of Granger causality tests in high-dimension to justify our method. We apply DLVAR to the stock returns of U.S. large financial institutions covering the period 1990–2021 and illustrate its usefulness in detecting systemically risky periods and institutions, especially during the Great Financial Crisis of 2008–2009 and the most recent Covid-19 related market shock.},
  archive      = {J_AOAS},
  author       = {Sumanta Basu and Sreyoshi Das and George Michailidis and Amiyatosh Purnanandam},
  doi          = {10.1214/22-AOAS1702},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {922-945},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A high-dimensional approach to measure connectivity in the financial sector},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A marginal structural model for partial compliance in
SMARTs. <em>AOAS</em>, <em>18</em>(2), 905–921. (<a
href="https://doi.org/10.1214/21-AOAS1586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cyclical and heterogeneous nature of many substance use disorders highlights the need to adapt the type and/or the dose of treatment to accommodate the specific and changing needs of individuals. The Adaptive Treatment for Alcohol and Cocaine Dependence study (ENGAGE) is a sequential multiple assignment randomized trial (SMART) that provided longitudinal data for constructing dynamic treatment regimes (DTRs) to improve patients’ engagement in therapy. However, the high rate of noncompliance and lack of analytic tools to account for noncompliance has impeded researchers from using the data to achieve the main goal of the trial; namely, construction of individually tailored DTRs. We address this by defining our target parameter as the mean outcome under different DTRs for potential compliance strata and propose a marginal structural model with principal stratification to estimate this quantity. We model the principal strata using a Bayesian semiparametric approach. An important feature of our work is that we consider partial rather than binary compliance strata, which is more relevant in longitudinal studies. We assess the performance of our method through simulation. We illustrate its application on ENGAGE and demonstrate the optimal DTRs depend on compliance strata compared with ignoring compliance information as in intention-to-treat analyses.},
  archive      = {J_AOAS},
  author       = {William J. Artman and Indrabati Bhattacharya and Ashkan Ertefaie and Kevin G. Lynch and James R. McKay and Brent A. Johnson},
  doi          = {10.1214/21-AOAS1586},
  journal      = {The Annals of Applied Statistics},
  month        = {6},
  number       = {2},
  pages        = {905-921},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A marginal structural model for partial compliance in SMARTs},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning healthcare delivery network with longitudinal
electronic health records data. <em>AOAS</em>, <em>18</em>(1), 882–898.
(<a href="https://doi.org/10.1214/23-AOAS1818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge networks, such as the healthcare delivery network (HDN), describing relationships among different medical encounters, are useful summaries of state-of-art medical knowledge. The increasing availability of longitudinal electronic health records (EHR) data promises a rich data source for learning HDN. Most existing methods for inferring knowledge networks are based on cooccurrence patterns that do not account for temporal effects or patient-level heterogeneity. In this article, building upon the multivariate Hawkes process (mvHP), we propose a flexible covariate-adjusted random effects (CARE) mvHP modeling strategy for HDN construction. Our model allows for patient-specific time-varying background intensity functions via random effects, which can also adjust for effects of important covariates. We adopt a penalized approach to select fixed effects, yielding a sparse network structure, and to remove unnecessary random effects from the model. Through extensive simulation studies, we show that our proposed method performs well in recovering the network structure and that it is essential to account for patient heterogeneities. We further illustrate our CARE mvHP method in an EHR study of type 2 diabetes patients to learn an HDN for these patients and demonstrate that our results are consistent with current clinical practice in healthcare systems.},
  archive      = {J_AOAS},
  author       = {Jiehuan Sun and Katherine P. Liao and Tianxi Cai},
  doi          = {10.1214/23-AOAS1818},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {882-898},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Learning healthcare delivery network with longitudinal electronic health records data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A simple and flexible test of sample exchangeability with
applications to statistical genomics. <em>AOAS</em>, <em>18</em>(1),
858–881. (<a href="https://doi.org/10.1214/23-AOAS1817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scientific studies involving analyses of multivariate data, basic but important questions often arise for the researcher: Is the sample exchangeable, meaning that the joint distribution of the sample is invariant to the ordering of the units? Are the features independent of one another, or perhaps the features can be grouped so that the groups are mutually independent? In statistical genomics these considerations are fundamental to downstream tasks such as demographic inference and the construction of polygenic risk scores. We propose a nonparametric approach, which we call the V test, to address these two questions, namely, a test of sample exchangeability given dependency structure of features and a test of feature independence given sample exchangeability. Our test is conceptually simple, yet fast and flexible. It controls the Type I error across realistic scenarios and handles data of arbitrary dimensions by leveraging large-sample asymptotics. Through extensive simulations and a comparison against unsupervised tests of stratification based on random matrix theory, we find that our test compares favorably in various scenarios of interest. We apply the test to data from the 1000 Genomes Project, demonstrating how it can be employed to assess exchangeability of the genetic sample or find optimal linkage disequilibrium (LD) splits for downstream analysis. For exchangeability assessment we find that removing rare variants can substantially increase the p-value of the test statistic. For optimal LD splitting, the V test reports different optimal splits than previous approaches not relying on hypothesis testing. Software for our methods is available in R (CRAN: flintyR) and Python (PyPI: flintyPy).},
  archive      = {J_AOAS},
  author       = {Alan J. Aw and Jeffrey P. Spence and Yun S. Song},
  doi          = {10.1214/23-AOAS1817},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {858-881},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A simple and flexible test of sample exchangeability with applications to statistical genomics},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Weak signal inclusion under dependence and applications in
genome-wide association study. <em>AOAS</em>, <em>18</em>(1), 841–857.
(<a href="https://doi.org/10.1214/23-AOAS1815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study we present a data-driven method called false negative control (FNC) screening to address the challenge of detecting weak signals in underpowered genome-wide association studies (GWASs), where true signals are often obscured by a large amount of noise. Our approach focuses on controlling false negatives and efficiently regulates the proportion of false negatives at a user-specified level in realistic settings with arbitrary covariance dependence between variables. We calibrate overall dependence using a parameter that aligns with the existing phase diagram in high-dimensional sparse inference, allowing us to asymptotically explicate the joint effect of covariance dependence, signal sparsity, and signal intensity on the proposed method. Our new phase diagram shows that FNC screening can efficiently select a set of candidate variables to retain a high proportion of signals, even when the signals are not individually separable from noise. We compare the performance of FNC screening to several existing methods in simulation studies, and the proposed method outperforms the others in adapting to a user-specified false negative control level. Moreover, we apply FNC screening to 145 GWAS datasets, obtained from the UK Biobank, and demonstrate a substantial increase in power to retain true signals for downstream analyses.},
  archive      = {J_AOAS},
  author       = {X. Jessie Jeng and Yifei Hu and Quan Sun and Yun Li},
  doi          = {10.1214/23-AOAS1815},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {841-857},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Weak signal inclusion under dependence and applications in genome-wide association study},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Testing for the causal mediation effects of multiple
mediators using the kernel machine difference method in genome-wide
epigenetic studies. <em>AOAS</em>, <em>18</em>(1), 819–840. (<a
href="https://doi.org/10.1214/23-AOAS1814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of high-throughput genomic and epigenomic data enables exploration of biological mechanisms underlying diseases causing processes beyond traditional association studies. Using the causal mediation analysis framework, we develop the kernel machine difference (KMD) method, which provides a testing procedure for detecting the mediation effects of a set of mediators, for example, the DNA methylation probes within a region or a gene. Our method extends the difference method in single mediator analysis to jointly model the mediatory role of the methylation of multiple neighboring probes, as they often work together in a collaborative fashion. Kernel machine regression is employed to accommodate flexible parametric and nonparametric effects of multiple mediators on the outcome and to allow for robust testing for the joint natural indirect effect (NIE). The proposed testing procedure does not require explicit modeling of the dependence of multiple mediators on exposure and confounders and the correlation among multiple mediators. It hence provides a robust and computationally efficient tool, especially for genomic regions with moderate to high-dimensional probes. We evaluate the performance of the proposed test using extensive simulations and demonstrate its gain in robustness and power when the effects are nonlinear. We apply the proposed test to the analysis of the epigenome-wide Normative Aging Study (NAS) to investigate the mediatory role of DNA methylation in the causal pathway between smoking behavior and lung function.},
  archive      = {J_AOAS},
  author       = {Jincheng Shen and Joel Schwartz and Andrea A. Baccarelli and Xihong Lin},
  doi          = {10.1214/23-AOAS1814},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {819-840},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Testing for the causal mediation effects of multiple mediators using the kernel machine difference method in genome-wide epigenetic studies},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Land-use filtering for nonstationary spatial prediction of
collective efficacy in an urban environment. <em>AOAS</em>,
<em>18</em>(1), 794–818. (<a
href="https://doi.org/10.1214/23-AOAS1813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collective efficacy—the capacity of communities to exert social control toward the realization of their shared goals—is a foundational concept in the urban sociology and neighborhood effects literature. Traditionally, empirical studies of collective efficacy use large sample surveys to estimate collective efficacy of different neighborhoods within an urban setting. Such studies have demonstrated an association between collective efficacy and local variation in community violence, educational achievement, and health. Unlike traditional collective efficacy measurement strategies, the Adolescent Health and Development in Context (AHDC) Study implemented a new approach, obtaining spatially-referenced, place-based ratings of collective efficacy from a representative sample of individuals residing in Columbus, OH. In this paper we introduce a novel nonstationary spatial model for interpolation of the AHDC collective efficacy ratings across the study area, which leverages administrative data on land use. Our constructive model specification strategy involves dimension expansion of a latent spatial process and the use of a filter defined by the land-use partition of the study region to connect the latent multivariate spatial process to the observed ordinal ratings of collective efficacy. Careful consideration is given to the issues of parameter identifiability, computational efficiency of an MCMC algorithm for model fitting, and fine-scale spatial prediction of collective efficacy.},
  archive      = {J_AOAS},
  author       = {J. Brandon Carter and Christopher R. Browning and Bethany Boettner and Nicolo Pinchak and Catherine A. Calder},
  doi          = {10.1214/23-AOAS1813},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {794-818},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Land-use filtering for nonstationary spatial prediction of collective efficacy in an urban environment},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Density-based matching rule: Optimality, estimation, and
application in forensic problems. <em>AOAS</em>, <em>18</em>(1),
770–793. (<a href="https://doi.org/10.1214/23-AOAS1812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider matching problems where the goal is to determine whether two observations randomly drawn from a population with multiple (sub)groups are from the same (sub)group. This is a key question in forensic science, where items with unidentified origins from suspects and crime scenes are compared to objects from a known set of sources to see if they originated from the same source. We derive the optimal matching rule under known density functions of data that minimizes the decision error probabilities. Empirically, the proposed matching rule is computed by plugging parametrically estimated density functions using training data into the formula of the optimal matching rule. The connections between the optimal matching rule and existing methods in forensic science are explained. In particular, we contrast the optimal matching rule to classification and also compare it to a score-based approach that relies on similarity features extracted from paired items. Numerical simulations are conducted to evaluate the proposed method and show that it outperforms the existing methods in terms of a higher ROC curve and higher power to identify matched pairs of items. We also demonstrate the utility of the proposed method by applying it to a real forensic data analysis of glass fragments.},
  archive      = {J_AOAS},
  author       = {Hana Lee and Yumou Qiu and Alicia Carriquiry and Danica Ommen},
  doi          = {10.1214/23-AOAS1812},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {770-793},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Density-based matching rule: Optimality, estimation, and application in forensic problems},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A continuous multiple hypothesis testing framework for
optimal exoplanet detection. <em>AOAS</em>, <em>18</em>(1), 749–769. (<a
href="https://doi.org/10.1214/23-AOAS1810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When searching for exoplanets, one wants to count how many planets orbit a given star, and to determine what their characteristics are. If the estimated planet characteristics are too far from those of a planet truly present, this should be considered as a false detection. This setting is a particular instance of a general one: aiming to retrieve parametric components in a dataset corrupted by nuisance signals, with a certain accuracy on their parameters. We exhibit a detection criterion minimizing false and missed detections, either as a function of their relative cost or when the expected number of false detections is bounded. If the components can be separated in a technical sense discussed in detail, the optimal detection criterion is a posterior probability obtained as a by-product of Bayesian evidence calculations. Optimality is guaranteed within a model, and we introduce model criticism methods to ensure that the criterion is robust to model errors. We show on two simulations emulating exoplanet searches that the optimal criterion can significantly outperform other criteria. Finally, we show that our framework offers solutions for the identification of components of mixture models and Bayesian false discovery rate control when hypotheses are not discrete.},
  archive      = {J_AOAS},
  author       = {Nathan C. Hara and Thibault de Poyferré and Jean-Baptiste Delisle and Marc Hoffmann},
  doi          = {10.1214/23-AOAS1810},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {749-769},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A continuous multiple hypothesis testing framework for optimal exoplanet detection},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Composite scores for transplant center evaluation: A new
individualized empirical null method. <em>AOAS</em>, <em>18</em>(1),
729–748. (<a href="https://doi.org/10.1214/23-AOAS1809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk-adjusted quality measures are used to evaluate healthcare providers with respect to national norms while controlling for factors beyond their control. Existing healthcare provider profiling approaches typically assume that the between-provider variation in these measures is entirely due to meaningful differences in quality of care. However, in practice, much of the between-provider variation will be due to trivial fluctuations in healthcare quality or unobservable confounding risk factors. If these additional sources of variation are not accounted for, conventional methods will disproportionately identify larger providers as outliers, even though their departures from the national norms may not be “extreme” or clinically meaningful. Motivated by efforts to evaluate the quality of care provided by transplant centers, we develop a composite evaluation score based on a novel individualized empirical null method, which robustly accounts for overdispersion due to unobserved risk factors, models the marginal variance of standardized scores as a function of the effective sample size, and only requires the use of publicly-available center-level statistics. The evaluations of United States kidney transplant centers based on the proposed composite score are substantially different from those based on conventional methods. Simulations show that the proposed empirical null approach more accurately classifies centers in terms of quality of care, compared to existing methods.},
  archive      = {J_AOAS},
  author       = {Nicholas Hartman and Joseph M. Messana and Jian Kang and Abhijit S. Naik and Tempie H. Shearon and Kevin He},
  doi          = {10.1214/23-AOAS1809},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {729-748},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Composite scores for transplant center evaluation: A new individualized empirical null method},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A partially functional linear regression framework for
integrating genetic, imaging, and clinical data. <em>AOAS</em>,
<em>18</em>(1), 704–728. (<a
href="https://doi.org/10.1214/23-AOAS1808">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by the joint analysis of genetic, imaging, and clinical (GIC) data collected in the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study. We propose a partially functional linear regression (PFLR) framework to map high-dimensional GIC-related pathways for Alzheimer’s disease (AD). We develop a joint model selection and estimation procedure by embedding imaging data in the reproducing kernel Hilbert space and imposing the ℓ0 penalty for the coefficients of genetic variables. We apply the proposed method to the ADNI dataset to identify important features from tens of thousands of genetic polymorphisms (reduced from millions using a preprocessing step) and study the effects of a certain set of informative genetic variants and the baseline hippocampus surface on 13 future cognitive scores. We also explore the shared and distinct heritability patterns of these cognitive scores. Analysis results suggest that both the hippocampal and genetic data have heterogeneous effects on different scores, with the trend that the value of both hippocampi are negatively associated with the severity of cognition deficits. Polygenic effects are observed for all the thirteen cognitive scores. The well-known APOE4 genotype only explains a small part of the cognitive function. Shared genetic etiology exists; however, greater genetic heterogeneity exists within disease classifications after accounting for the baseline diagnosis status. These analyses are useful in further investigation of functional mechanisms for AD progression.},
  archive      = {J_AOAS},
  author       = {Ting Li and Yang Yu and J. S. Marron and Hongtu Zhu},
  doi          = {10.1214/23-AOAS1808},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {704-728},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A partially functional linear regression framework for integrating genetic, imaging, and clinical data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Modeling the visibility distribution for respondent-driven
sampling with application to population size estimation. <em>AOAS</em>,
<em>18</em>(1), 683–703. (<a
href="https://doi.org/10.1214/23-AOAS1807">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respondent-driven sampling (RDS) is used throughout the world to estimate prevalence and population size for hidden populations. Although RDS is an effective method for enrolling people from key populations in studies, it relies on a partially unknown sampling mechanism, and thus each individual’s inclusion probability is unknown. Current estimators for population prevalence, population size, and other outcomes rely on a participant’s network size (degree) to approximate their inclusion probability in the sample from the networked population. However, in most RDS studies, a participant’s network size is attained via a self-report and is subject to many types of misreporting and bias. Because design-based inclusion probabilities cannot be exactly computed, we instead use the term visibility to describe how likely a person is to be selected to participate in the study. The commonly used successive sampling population size estimation (SS-PSE) framework to estimate population sizes from RDS data relies on self-reported network sizes in the model for the sampling mechanism. We propose an enhancement of the SS-PSE framework that adds a measurement error model for visibility used in place of the self-reported network size and a model for the number of recruits an individual can enroll. Inferred visibilities are a way to smooth the degree distribution and bring in outliers as well as a mechanism to deal with missing and invalid network sizes. We demonstrate the performance of visibility SS-PSE on three populations from Kosovo sampled in 2014 using RDS. We also discuss how the visibility modeling framework could be extended to prevalence estimation.},
  archive      = {J_AOAS},
  author       = {Katherine R. McLaughlin and Lisa G. Johnston and Xhevat Jakupi and Dafina Gexha-Bunjaku and Edona Deva and Mark S. Handcock},
  doi          = {10.1214/23-AOAS1807},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {683-703},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Modeling the visibility distribution for respondent-driven sampling with application to population size estimation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quantifying replicability of multiple studies in a
meta-analysis. <em>AOAS</em>, <em>18</em>(1), 664–682. (<a
href="https://doi.org/10.1214/23-AOAS1806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For valid scientific discoveries, it is fundamental to evaluate whether research findings are replicable across different settings. While large-scale replication projects across broad research topics are not feasible, systematic reviews and meta-analyses (SRMAs) offer viable alternatives to assess replicability. Due to subjective inclusion and exclusion of studies, SRMAs may contain nonreplicable study findings. However, there is no consensus on rigorous methods to assess the replicability of SRMAs or to explore sources of nonreplicability. Nonreplicability is often misconceived as high heterogeneity. This article introduces a new measure, the externally standardized residuals from a leave-m-studies-out procedure, to quantify replicability. It not only measures the impact of nonreplicability from unknown sources on the conclusion of an SRMA but also differentiates nonreplicability from heterogeneity. A new test statistic for replicability is derived. We explore its asymptotic properties and use extensive simulations and real data to illustrate this measure’s performance. We conclude that replicability should be routinely assessed for all SRMAs and recommend sensitivity analyses, once nonreplicable study results are identified in an SRMA.},
  archive      = {J_AOAS},
  author       = {Mengli Xiao and Haitao Chu and James S. Hodges and Lifeng Lin},
  doi          = {10.1214/23-AOAS1806},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {664-682},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Quantifying replicability of multiple studies in a meta-analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian adaptive and interpretable functional regression
for exposure profiles. <em>AOAS</em>, <em>18</em>(1), 642–663. (<a
href="https://doi.org/10.1214/23-AOAS1805">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pollutant exposure during gestation is a known and adverse factor for birth and health outcomes. However, the links between prenatal air pollution exposures and educational outcomes are less clear, in particular, the critical windows of susceptibility during pregnancy. Using a large cohort of students in North Carolina, we study the link between prenatal daily PM2.5 exposure and fourth end-of-grade reading scores. We develop and apply a locally adaptive and highly scalable Bayesian regression model for scalar responses with functional and scalar predictors. The proposed model pairs a B-spline basis expansion with dynamic shrinkage priors to capture both smooth and rapidly-changing features in the regression surface. The model is accompanied by a new decision analysis approach for functional regression that extracts the critical windows of susceptibility and guides the model interpretations. These tools help to identify and address broad limitations with the interpretability of functional regression models. Simulation studies demonstrate more accurate point estimation, more precise uncertainty quantification, and far superior window selection than existing approaches. Leveraging the proposed modeling, computational, and decision analysis framework, we conclude that prenatal PM2.5 exposure during early and late pregnancy is most adverse for fourth end-of-grade reading scores.},
  archive      = {J_AOAS},
  author       = {Yunan Gao and Daniel R. Kowal},
  doi          = {10.1214/23-AOAS1805},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {642-663},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian adaptive and interpretable functional regression for exposure profiles},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Tensor mixture discriminant analysis with applications to
sensor array data analysis. <em>AOAS</em>, <em>18</em>(1), 626–641. (<a
href="https://doi.org/10.1214/23-AOAS1804">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensor arrays are often used to identify chemicals by measuring properly chosen chemical interactions. Machine learning techniques are of vital importance to accurately recognize a chemical based on the sensor array measurements. However, sensor array data often take the form of matrices (i.e, two-way tensors), and the concentration levels may have a complex impact on the measurements. Hence, existing linear and/or vector classification methods may be inadequate for sensor array data. In this article we propose a novel tensor mixture discriminant analysis (TMDA) model carefully tailored for the classification of sensor array data. We model the distribution of each chemical by a mixture of tensor normal distributions. TMDA leverages the tensor structure for better estimation and prediction, while the mixed tensor normal component accounts for the possibly varying concentration levels. The TMDA model can also be viewed as an approximation of the potentially nonnormal measurements. An efficient expectation-maximization algorithm is developed to fit the TMDA model. The application of TMDA on two sensor array datasets demonstrates its superior performance to many popular competitors.},
  archive      = {J_AOAS},
  author       = {Xuesong Hou and Qing Mai and Hui Zou},
  doi          = {10.1214/23-AOAS1804},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {626-641},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Tensor mixture discriminant analysis with applications to sensor array data analysis},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online monitoring of air quality using PCA-based sequential
learning. <em>AOAS</em>, <em>18</em>(1), 600–625. (<a
href="https://doi.org/10.1214/23-AOAS1803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution surveillance is critically important for public health. One air pollutant, ozone, is extremely challenging to analyze properly, as it is a secondary pollutant caused by complex chemical reactions in the air and does not emit directly into the atmosphere. Numerous environmental studies confirm that ozone concentration levels are associated with meteorological conditions, and long-term exposure to high ozone concentration levels is associated with the incidence of many diseases, including asthma, respiratory, and cardiovascular diseases. Thus, it is important to develop an air pollution surveillance system to collect both air pollution and meteorological data and monitor the data continuously over time. To this end, statistical process control (SPC) charts provide a major statistical tool. But most existing SPC charts are designed for cases when the in-control (IC) process observations at different times are assumed to be independent and identically distributed. The air pollution and meteorological data would not satisfy these conditions due to serial data correlation, high dimensionality, seasonality, and other complex data structure. Motivated by an application to monitor the ground ozone concentration levels in the Houston–Galveston–Brazoria (HGB) area, we developed a new process monitoring method using principal component analysis and sequential learning. The new method can accommodate high dimensionality, time-varying IC process distribution, serial data correlation, and nonparametric data distribution. It is shown to be a reliable analytic tool for online monitoring of air quality.},
  archive      = {J_AOAS},
  author       = {Xiulin Xie and Nicole Qian and Peihua Qiu},
  doi          = {10.1214/23-AOAS1803},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {600-625},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Online monitoring of air quality using PCA-based sequential learning},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A novel estimator of earth’s curvature (allowing for
inference as well). <em>AOAS</em>, <em>18</em>(1), 585–599. (<a
href="https://doi.org/10.1214/23-AOAS1802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper estimates the curvature of the Earth, defined as one over its radius, without relying on physical measurements. The orthodox model states that the Earth is (nearly) spherical with a curvature of π/20,000km. By contrast, the heterodox flat-Earth model stipulates a curvature of zero. Abstracting from the well-worn arguments for and against both models, rebuttals and counter-rebuttals ad infinitum, we propose a novel statistical methodology based on verifiable flight times along regularly scheduled commercial airline routes; this methodology allows for both estimating and making inference for Earth’s curvature. In particular, a formal hypothesis test resolutely rejects the flat-Earth model, whereas it does not reject the orthodox spherical-Earth model.},
  archive      = {J_AOAS},
  author       = {David R. Bell and Olivier Ledoit and Michael Wolf},
  doi          = {10.1214/23-AOAS1802},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {585-599},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A novel estimator of earth’s curvature (Allowing for inference as well)},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Change point detection in dynamic gaussian graphical models:
The impact of COVID-19 pandemic on the u.s. Stock market. <em>AOAS</em>,
<em>18</em>(1), 555–584. (<a
href="https://doi.org/10.1214/23-AOAS1801">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable estimates of volatility and correlation are fundamental in economics and finance for understanding the impact of macroeconomics events on the market and guiding future investments and policies. Dependence across financial returns is likely to be subject to sudden structural changes, especially in correspondence with major global events, such as the COVID-19 pandemic. In this work we are interested in capturing abrupt changes over time in the conditional dependence across U.S. industry stock portfolios, over a time horizon that covers the COVID-19 pandemic. The selected stocks give a comprehensive picture of the U.S. stock market. To this end, we develop a Bayesian multivariate stochastic volatility model based on a time-varying sequence of graphs capturing the evolution of the dependence structure. The model builds on the Gaussian graphical models and the random change points literature. In particular, we treat the number, the position of change points, and the graphs as object of posterior inference, allowing for sparsity in graph recovery and change point detection. The high dimension of the parameter space poses complex computational challenges. However, the model admits a hidden Markov model formulation. This leads to the development of an efficient computational strategy, based on a combination of sequential Monte-Carlo and Markov chain Monte-Carlo techniques. Model and computational development are widely applicable, beyond the scope of the application of interest in this work.},
  archive      = {J_AOAS},
  author       = {Beatrice Franzolini and Alexandros Beskos and Maria De Iorio and Warrick Poklewski Koziell and Karolina Grzeszkiewicz},
  doi          = {10.1214/23-AOAS1801},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {555-584},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Change point detection in dynamic gaussian graphical models: The impact of COVID-19 pandemic on the U.S. stock market},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A nonseparable first-order spatiotemporal intensity for
events on linear networks: An application to ambulance interventions.
<em>AOAS</em>, <em>18</em>(1), 529–554. (<a
href="https://doi.org/10.1214/23-AOAS1800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The algorithms used for the optimal management of an ambulance fleet require an accurate description of the spatiotemporal evolution of the emergency events. In the last years, several authors have proposed sophisticated statistical approaches to forecast ambulance dispatches, typically modelling the data as a point pattern occurring on a planar region. Nevertheless, ambulance interventions can be more appropriately modelled as a realisation of a point process occurring on a linear network. The constrained spatial domain raises specific challenges and unique methodological problems that cannot be ignored when developing a proper statistical approach. Hence, this paper proposes a spatiotemporal model to analyse ambulance dispatches focusing on the interventions that occurred in the road network of Milan (Italy) from 2015 to 2017. We adopt a nonseparable first-order intensity function with spatial and temporal terms. The temporal dimension is estimated semiparametrically using a Poisson regression model, while the spatial dimension is estimated nonparametrically using a network kernel function. A set of weights is included in the spatial term to capture space-time interactions, inducing nonseparability in the intensity function. A series of tests show that our approach successfully models the ambulance interventions and captures the space-time patterns more accurately than planar or separable point process models.},
  archive      = {J_AOAS},
  author       = {Andrea Gilardi and Riccardo Borgoni and Jorge Mateu},
  doi          = {10.1214/23-AOAS1800},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {529-554},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A nonseparable first-order spatiotemporal intensity for events on linear networks: An application to ambulance interventions},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). What makes forest-based heterogeneous treatment effect
estimators work? <em>AOAS</em>, <em>18</em>(1), 506–528. (<a
href="https://doi.org/10.1214/23-AOAS1799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of heterogeneous treatment effects (HTE) is of prime importance in many disciplines, from personalized medicine to economics among many others. Random forests have been shown to be a flexible and powerful approach to HTE estimation in both randomized trials and observational studies. In particular “causal forests” introduced by Athey, Tibshirani and Wager (Ann. Statist. 47 (2019) 1148–1178), along with the R implementation in package grf were rapidly adopted. A related approach, called “model-based forests” that is geared toward randomized trials and simultaneously captures effects of both prognostic and predictive variables, was introduced by Seibold, Zeileis and Hothorn (Stat. Methods Med. Res. 27 (2018) 3104–3125) along with a modular implementation in the R package model4you. Neither procedure is directly applicable to the estimation of individualized predictions of excess postpartum blood loss caused by a cesarean section in comparison to vaginal delivery. Clearly, randomization is hardly possible in this setup, and thus model-based forests lack clinical trial data to address this question. On the other hand, the skewed and interval-censored postpartum blood loss observations violate assumptions made by causal forests. Here we present a tailored model-based forest for skewed and interval-censored data to infer possible predictive prepartum characteristics and their impact on excess postpartum blood loss caused by a cesarean section. As a methodological basis, we propose a unifying view on causal and model-based forests that goes beyond the theoretical motivations and investigates which computational elements make causal forests so successful and how these can be blended with the strengths of model-based forests. To do so, we show that both methods can be understood in terms of the same parameters and model assumptions for an additive model under L2 loss. This theoretical insight allows us to implement several flavors of “model-based causal forests” and dissect their different elements in silico. The original causal forests and model-based forests are compared with the new blended versions in a benchmark study exploring both randomized trials and observational settings. In the randomized setting, both approaches performed akin. If confounding was present in the data-generating process, we found local centering of the treatment indicator with the corresponding propensities to be the main driver for good performance. Local centering of the outcome was less important and might be replaced or enhanced by simultaneous split selection with respect to both prognostic and predictive effects. This lays the foundation for future research combining random forests for HTE estimation with other types of models.},
  archive      = {J_AOAS},
  author       = {Susanne Dandl and Christian Haslinger and Torsten Hothorn and Heidi Seibold and Erik Sverdrup and Stefan Wager and Achim Zeileis},
  doi          = {10.1214/23-AOAS1799},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {506-528},
  shortjournal = {Ann. Appl. Stat.},
  title        = {What makes forest-based heterogeneous treatment effect estimators work?},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Retrospective varying coefficient association analysis of
longitudinal binary traits: Application to the identification of genetic
loci associated with hypertension. <em>AOAS</em>, <em>18</em>(1),
487–505. (<a href="https://doi.org/10.1214/23-AOAS1798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many genetic studies contain rich information on longitudinal phenotypes that require powerful analytical tools for optimal analysis. Genetic analysis of longitudinal data that incorporates temporal variation is important for understanding the genetic architecture and biological variation of complex diseases. Most of the existing methods assume that the contribution of genetic variants is constant over time and fail to capture the dynamic pattern of disease progression. However, the relative influence of genetic variants on complex traits fluctuates over time. In this study, we propose a retrospective varying coefficient mixed model association test, RVMMAT, to detect time-varying genetic effect on longitudinal binary traits. We model dynamic genetic effect using smoothing splines, estimate model parameters by maximizing a double penalized quasi-likelihood function, design a joint test using a Cauchy combination method, and evaluate statistical significance via a retrospective approach to achieve robustness to model misspecification. Through simulations we illustrated that the retrospective varying-coefficient test was robust to model misspecification under different ascertainment schemes and gained power over the association methods assuming constant genetic effect. We applied RVMMAT to a genome-wide association analysis of longitudinal measure of hypertension in the Multi-Ethnic Study of Atherosclerosis. Pathway analysis identified two important pathways related to G-protein signaling and DNA damage. Our results demonstrated that RVMMAT could detect biologically relevant loci and pathways in a genome scan and provided insight into the genetic architecture of hypertension.},
  archive      = {J_AOAS},
  author       = {Gang Xu and Amei Amei and Weimiao Wu and Yunqing Liu and Linchuan Shen and Edwin C. Oh and Zuoheng Wang},
  doi          = {10.1214/23-AOAS1798},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {487-505},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Retrospective varying coefficient association analysis of longitudinal binary traits: Application to the identification of genetic loci associated with hypertension},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Latent subgroup identification in image-on-scalar
regression. <em>AOAS</em>, <em>18</em>(1), 468–486. (<a
href="https://doi.org/10.1214/23-AOAS1797">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-on-scalar regression has been a popular approach to modeling the association between brain activities and scalar characteristics in neuroimaging research. The associations could be heterogeneous across individuals in the population, as indicated by recent large-scale neuroimaging studies, for example, the Adolescent Brain Cognitive Development (ABCD) Study. The ABCD data can inform our understanding of heterogeneous associations and how to leverage the heterogeneity and tailor interventions to increase the number of youths who benefit. It is of great interest to identify subgroups of individuals from the population such that: (1) within each subgroup the brain activities have homogeneous associations with the clinical measures; (2) across subgroups the associations are heterogeneous, and (3) the group allocation depends on individual characteristics. Existing image-on-scalar regression methods and clustering methods cannot directly achieve this goal. We propose a latent subgroup image-on-scalar regression model (LASIR) to analyze large-scale, multisite neuroimaging data with diverse sociodemographics. LASIR introduces the latent subgroup for each individual and group-specific, spatially varying effects, with an efficient stochastic expectation maximization algorithm for inferences. We demonstrate that LASIR outperforms existing alternatives for subgroup identification of brain activation patterns with functional magnetic resonance imaging data via comprehensive simulations and applications to the ABCD study. We have released our reproducible codes for public use with the software package available on Github.},
  archive      = {J_AOAS},
  author       = {Zikai Lin and Yajuan Si and Jian Kang},
  doi          = {10.1214/23-AOAS1797},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {468-486},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Latent subgroup identification in image-on-scalar regression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Selective inference for sparse multitask regression with
applications in neuroimaging. <em>AOAS</em>, <em>18</em>(1), 445–467.
(<a href="https://doi.org/10.1214/23-AOAS1796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask learning is frequently used to model a set of related response variables from the same set of features, improving predictive performance and modeling accuracy relative to methods that handle each response variable separately. Despite the potential of multitask learning to yield more powerful inference than single-task alternatives, prior work in this area has largely omitted uncertainty quantification. Our focus in this paper is a common multitask problem in neuroimaging, where the goal is to understand the relationship between multiple cognitive task scores (or other subject-level assessments) and brain connectome data collected from imaging. We propose a framework for selective inference to address this problem, with the flexibility to: (i) jointly identify the relevant predictors for each task through a sparsity-inducing penalty and (ii) conduct valid inference in a model based on the estimated sparsity structure. Our framework offers a new conditional procedure for inference, based on a refinement of the selection event that yields a tractable selection-adjusted likelihood. This gives an approximate system of estimating equations for maximum likelihood inference, solvable via a single convex optimization problem, and enables us to efficiently form confidence intervals with approximately the correct coverage. Applied to both simulated data and data from the Adolescent Brain Cognitive Development (ABCD) study, our selective inference methods yield tighter confidence intervals than commonly used alternatives, such as data splitting. We also demonstrate through simulations that multitask learning with selective inference can more accurately recover true signals than single-task methods.},
  archive      = {J_AOAS},
  author       = {Snigdha Panigrahi and Natasha Stewart and Chandra Sripada and Elizaveta Levina},
  doi          = {10.1214/23-AOAS1796},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {445-467},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Selective inference for sparse multitask regression with applications in neuroimaging},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network-level traffic flow prediction: Functional time
series vs. Functional neural network approach. <em>AOAS</em>,
<em>18</em>(1), 424–444. (<a
href="https://doi.org/10.1214/23-AOAS1795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic state prediction is an essential component and an underlying backbone of intelligent transportation systems, especially in the context of smart city framework. Its significance is mainly twofold in modern transportation systems: supporting advanced traffic operations and management for highways and urban road networks to mitigate traffic congestion and enabling individual drivers with connected vehicles in the traffic system to dynamically optimize their routes to improve travel time. Traffic state prediction with interval-based pointwise methods at 15-minute or hourly intervals is common in traffic literature. However, because traffic dynamics are a continuous process over time, the discrete-time pointwise methods for traffic prediction at a fixed time interval hardly meet the advanced demands of continuous prediction in modern transportation systems. To close the gap, we propose functional approaches to intraday and day-by-day continuous-time prediction for traffic volume. This research focuses on network-level traffic flow predictions concurrently for all locations of interest. Two functional approaches are introduced, namely, the network-integrated functional time-series model and the functional neural network model. With functional approaches a 24-hour intraday traffic profile is modeled as a functional curve over time, and sequences of historical traffic curves are used to predict traffic curves for near future days in a row and multiple locations of interest. We also include the functional varying coefficient model, Sparse VAR and traditional AR models in the comparative study; empirical results show that the network-integrated functional time-series model outperforms other approaches in terms of the accuracy of predictions at network-scale.},
  archive      = {J_AOAS},
  author       = {Tao Ma and Fang Yao and Zhou Zhou},
  doi          = {10.1214/23-AOAS1795},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {424-444},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Network-level traffic flow prediction: Functional time series vs. functional neural network approach},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian spatio-temporal level set dynamic model and
application to fire front propagation. <em>AOAS</em>, <em>18</em>(1),
404–423. (<a href="https://doi.org/10.1214/23-AOAS1794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intense wildfires impact nature, humans, and society, causing catastrophic damage to property and the ecosystem as well as the loss of life. Forecasting wildfire front propagation and understanding the behavior of wildfire propagation within a formal uncertainty quantification framework are essential in order to support fire fighting efforts and plan evacuations. The level set method has been widely used to analyze the change in surfaces, shapes, and boundaries. In particular, a signed distance function used in level set methods can readily be interpreted to represent complicated boundaries and their changes in time. While there is substantial literature on the level set method in wildfire applications, these implementations have relied on a heavily-parameterized formula for the rate of spread. These implementations have not typically considered uncertainty quantification, incorporated data-driven learning, nor summarized the effect of the environmental covariates. Here we present a Bayesian spatio-temporal dynamic model, based on level sets, which can be utilized for inference and forecasting the boundary of interest in the presence of uncertain data and lack of knowledge about the boundary velocity. The methodology relies on both a mechanistically-motivated dynamic model for level sets and a stochastic spatio-temporal dynamic model for the front velocity. We show the effectiveness of our method via simulation and with forecasting the fire front boundary evolution of two classic California megafires—the 2017–2018 Thomas fire and the 2017 Haypress fire.},
  archive      = {J_AOAS},
  author       = {Myungsoo Yoo and Christopher K. Wikle},
  doi          = {10.1214/23-AOAS1794},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {404-423},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian spatio-temporal level set dynamic model and application to fire front propagation},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian projections of total fertility rate conditional on
the united nations sustainable development goals. <em>AOAS</em>,
<em>18</em>(1), 375–403. (<a
href="https://doi.org/10.1214/23-AOAS1793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Women’s educational attainment and contraceptive prevalence are two mechanisms identified as having an accelerating effect on fertility decline and that can be directly impacted by policy. Quantifying the potential accelerating effect of education and family planning policies on fertility decline in a probabilistic way is of interest to policymakers, particularly in high-fertility countries. We propose a conditional Bayesian hierarchical model for projecting fertility, given education and family planning policy interventions. To illustrate the effect policy changes could have on future fertility, we create probabilistic projections of fertility that condition on scenarios such as achieving the sustainable development goals (SDGs) for universal secondary education and universal access to family planning by 2030.},
  archive      = {J_AOAS},
  author       = {Daphne H. Liu and Adrian E. Raftery},
  doi          = {10.1214/23-AOAS1793},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {375-403},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian projections of total fertility rate conditional on the united nations sustainable development goals},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A bayesian machine learning approach for estimating
heterogeneous survivor causal effects: Applications to a critical care
trial. <em>AOAS</em>, <em>18</em>(1), 350–374. (<a
href="https://doi.org/10.1214/23-AOAS1792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assessing heterogeneity in the effects of treatments has become increasingly popular in the field of causal inference and carries important implications for clinical decision-making. While extensive literature exists for studying treatment effect heterogeneity when outcomes are fully observed, there has been limited development in tools for estimating heterogeneous causal effects when patient-centered outcomes are truncated by a terminal event, such as death. Due to mortality occurring during study follow-up, the outcomes of interest are unobservable, undefined, or not fully observed for many participants in which case principal stratification is an appealing framework to draw valid causal conclusions. Motivated by the Acute Respiratory Distress Syndrome Network (ARDSNetwork) ARDS respiratory management (ARMA) trial, we developed a flexible Bayesian machine learning approach to estimate the average causal effect and heterogeneous causal effects among the always-survivors stratum when clinical outcomes are subject to truncation. We adopted Bayesian additive regression trees (BART) to flexibly specify separate mean models for the potential outcomes and latent stratum membership. In the analysis of the ARMA trial, we found that the low tidal volume treatment had an overall benefit for participants sustaining acute lung injuries on the outcome of time to returning home but substantial heterogeneity in treatment effects among the always-survivors, driven most strongly by biologic sex and the alveolar-arterial oxygen gradient at baseline (a physiologic measure of lung function and degree of hypoxemia). These findings illustrate how the proposed methodology could guide the prognostic enrichment of future trials in the field.},
  archive      = {J_AOAS},
  author       = {Xinyuan Chen and Michael O. Harhay and Guangyu Tong and Fan Li},
  doi          = {10.1214/23-AOAS1792},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {350-374},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A bayesian machine learning approach for estimating heterogeneous survivor causal effects: Applications to a critical care trial},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ANOPOW for replicated nonstationary time series in
experiments. <em>AOAS</em>, <em>18</em>(1), 328–349. (<a
href="https://doi.org/10.1214/23-AOAS1791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel analysis of power (ANOPOW) model for analyzing replicated nonstationary time series commonly encountered in experimental studies. Based on a locally stationary ANOPOW Cramér spectral representation, the proposed model can be used to compare the second-order time-varying frequency patterns among different groups of time series and to estimate group effects as functions of both time and frequency. Formulated in a Bayesian framework, independent two-dimensional second-order random walk (RW2D) priors are assumed on each of the time-varying functional effects for flexible and adaptive smoothing. A piecewise stationary approximation of the nonstationary time series is used to obtain localized estimates of time-varying spectra. Posterior distributions of the time-varying functional group effects are then obtained via integrated nested Laplace approximations (INLA) at a low computational cost. The large-sample distribution of local periodograms can be appropriately utilized to improve estimation accuracy since INLA allows modeling of data with various types of distributions. The usefulness of the proposed model is illustrated through two real-data applications: analyses of seismic signals and pupil diameter time series in children with attention deficit hyperactivity disorder. Simulation studies, Supplementary Material (Li, Yue and Bruce (2024a)), and R code (Li, Yue and Bruce (2024b)) for this article are also available.},
  archive      = {J_AOAS},
  author       = {Zeda Li and Yu (Ryan) Yue and Scott A. Bruce},
  doi          = {10.1214/23-AOAS1791},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {328-349},
  shortjournal = {Ann. Appl. Stat.},
  title        = {ANOPOW for replicated nonstationary time series in experiments},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Inferring changes to the global carbon cycle with WOMBAT
v2.0, a hierarchical flux-inversion framework. <em>AOAS</em>,
<em>18</em>(1), 303–327. (<a
href="https://doi.org/10.1214/23-AOAS1790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The natural cycles of the surface-to-atmosphere fluxes of carbon dioxide (CO2) and other important greenhouse gases are changing in response to human influences. These changes need to be quantified to understand climate change and its impacts, but this is difficult to do because natural fluxes occur over large spatial and temporal scales and cannot be directly observed. Flux inversion is a technique that estimates the spatiotemporal distribution of a gas’ fluxes using observations of the gas’ mole fraction and a chemical transport model. To infer trends in fluxes and identify phase shifts and amplitude changes in flux seasonal cycles, we construct a flux-inversion system that uses a novel spatially-varying time-series decomposition of the fluxes. We incorporate this decomposition into the Wollongong Methodology for Bayesian Assimilation of Trace-gases (WOMBAT, Zammit-Mangion et al., Geosci. Model Dev., 15, 2022), a Bayesian hierarchical flux-inversion framework that yields posterior distributions for all unknowns in the underlying model. We also extend WOMBAT to accommodate physical constraints on the fluxes and to take direct in situ and flask measurements of trace-gas mole fractions as observations. We apply the new method, which we call WOMBAT v2.0, to a mix of satellite observations of CO2 mole fraction from the Orbiting Carbon Observatory-2 (OCO-2) satellite and direct measurements of CO2 mole fraction from a variety of sources. We estimate the changes in the natural cycles of CO2 fluxes that occurred from January 2015 to December 2020, and compare our posterior estimates to those from an alternative method based on a bottom-up understanding of the physical processes involved. We find substantial trends in the fluxes, including that tropical ecosystems trended from being a net source to a net sink of CO2 over the study period. We also find that the amplitude of the global seasonal cycle of ecosystem CO2 fluxes increased over the study period by 0.11 PgC/month (an increase of 8%) and that the seasonal cycle of ecosystem CO2 fluxes in the northern temperate and northern boreal regions shifted earlier in the year by 0.4–0.7 and 0.4–0.9 days, respectively (2.5th to 97.5th posterior percentiles), consistent with expectations for the carbon cycle under a warming climate.},
  archive      = {J_AOAS},
  author       = {Michael Bertolacci and Andrew Zammit-Mangion and Andrew Schuh and Beata Bukosa and Jenny A. Fisher and Yi Cao and Aleya Kaushik and Noel Cressie},
  doi          = {10.1214/23-AOAS1790},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {303-327},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Inferring changes to the global carbon cycle with WOMBAT v2.0, a hierarchical flux-inversion framework},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian model-based clustering for populations of network
data. <em>AOAS</em>, <em>18</em>(1), 266–302. (<a
href="https://doi.org/10.1214/23-AOAS1789">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is increasing appetite for analysing populations of network data due to the fast-growing body of applications demanding such methods. While methods exist to provide readily interpretable summaries of heterogeneous network populations, these are often descriptive or ad hoc, lacking any formal justification. In contrast, principled analysis methods often provide results difficult to relate back to the applied problem of interest. Motivated by two complementary applied examples, we develop a Bayesian framework to appropriately model complex heterogeneous network populations, while also allowing analysts to gain insights from the data and make inferences most relevant to their needs. The first application involves a study in computer science measuring human movements across a university. The second analyses data from neuroscience investigating relationships between different regions of the brain. While both applications entail analysis of a heterogeneous population of networks, network sizes vary considerably. We focus on the problem of clustering the elements of a network population, where each cluster is characterised by a network representative. We take advantage of the Bayesian machinery to simultaneously infer the cluster membership, the representatives, and the community structure of the representatives, thus allowing intuitive inferences to be made. The implementation of our method on the human movement study reveals interesting movement patterns of individuals in clusters, readily characterised by their network representative. For the brain networks application, our model reveals a cluster of individuals with different network properties of particular interest in neuroscience. The performance of our method is additionally validated in extensive simulation studies.},
  archive      = {J_AOAS},
  author       = {Anastasia Mantziou and Simón Lunagómez and Robin Mitra},
  doi          = {10.1214/23-AOAS1789},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {266-302},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian model-based clustering for populations of network data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Dynamic risk prediction for cervical precancer screening
with continuous and binary longitudinal biomarkers. <em>AOAS</em>,
<em>18</em>(1), 246–265. (<a
href="https://doi.org/10.1214/23-AOAS1788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic risk prediction that incorporates longitudinal measurements of biomarkers is useful in identifying high-risk patients for better clinical management. Our work is motivated by the prediction of cervical precancers. Currently, Pap cytology is used to identify HPV-positive (HPV+) women at high-risk of cervical precancer, but cytology lacks accuracy and reproducibility. Molecular markers, like HPV DNA methylation, that are closely linked to the carcinogenic process show promise of improved risk stratification. We are interested in developing a dynamic risk model that uses all longitudinal biomarker information to improve precancer risk estimation. We propose a joint model to link both the continuous methylation biomarker and a binary cytology biomarker to the time to precancer outcome using shared random effects. The model uses a discretization of the time scale to allow for closed-form likelihood expressions, thereby avoiding potential high dimensional integration of the random effects. The method handles an interval-censored time-to-event outcome, due to intermittent clinical visits, incorporates sampling weights to deal with stratified sampling data and can provide immediate and five-year risk estimates that may inform clinical decision-making. Applying the method to longitudinally measured HPV methylation data improves risk stratification for triage of HPV+ women.},
  archive      = {J_AOAS},
  author       = {Siddharth Roy and Anindya Roy and Megan A. Clarke and Ana Gradissimo and Robert D. Burk and Nicolas Wentzensen and Paul S. Albert and Danping Liu},
  doi          = {10.1214/23-AOAS1788},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {246-265},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Dynamic risk prediction for cervical precancer screening with continuous and binary longitudinal biomarkers},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonnegative tensor completion for dynamic counterfactual
prediction on COVID-19 pandemic. <em>AOAS</em>, <em>18</em>(1), 224–245.
(<a href="https://doi.org/10.1214/23-AOAS1787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has been a worldwide health crisis for the past three years, casting unprecedented challenges for policymakers in different countries and regions. While one country or region can only implement one social mobility restriction policy at a given time, it is of great interest for policy makers to decide whether to elevate or deelevate the restriction policy from time to time. This article proposes a novel nonnegative tensor completion method to predict the potential counterfactual outcomes of multifaceted social mobility restriction policies over time. The proposed method builds upon a low-rank tensor decomposition of the pandemic data, which also explicitly characterizes the ordinal nature of the mobility restriction strength and the smooth trend of the pandemic evolution over time. Its application to the COVID-19 pandemic data reveals some interesting facts regarding the impact of social mobility restriction policy on the spread of the virus. The effectiveness of the proposed method is also supported by its asymptotic estimation consistency and extensive numerical experiments on the synthetic datasets.},
  archive      = {J_AOAS},
  author       = {Yaoming Zhen and Junhui Wang},
  doi          = {10.1214/23-AOAS1787},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {224-245},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Nonnegative tensor completion for dynamic counterfactual prediction on COVID-19 pandemic},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A behavioral approach to repeated bayesian security games.
<em>AOAS</em>, <em>18</em>(1), 199–223. (<a
href="https://doi.org/10.1214/23-AOAS1786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of security threats to organizational defense demands models that support real-world policymaking. Security games are a potent tool in this regard; however, although canonical models effectively allocate limited resources, they generally do not consider adaptive, boundedly rational adversaries. Empirical findings suggest this characterization describes real-world human behavior, so the development of decision-support frameworks against such adversaries is a critical need. We examine a family of policies applicable to repeated games in which a boundedly rational adversary is modeled using a behavioral-economic theory of learning, that is, experience-weighted attraction learning. These policies take into account realistic uncertainty about the competition by adopting the perspective of adversarial risk analysis. Using Bayesian reasoning, these repeated games are decomposed into multiarm bandit problems. A collection of cost-function approximation policies are given to solve these problems. The efficacy of our approach is shown via extensive computational testing on a defense-related case study.},
  archive      = {J_AOAS},
  author       = {William Caballero and Jake Cooley and David Banks and Phillip Jenkins},
  doi          = {10.1214/23-AOAS1786},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {199-223},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A behavioral approach to repeated bayesian security games},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applied regression analysis of correlations for correlated
data. <em>AOAS</em>, <em>18</em>(1), 184–198. (<a
href="https://doi.org/10.1214/23-AOAS1785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correlated data are ubiquitous in today’s data-driven society. While regression models for analyzing means and variances of responses of interest are relatively well developed, the development of these models for analyzing the correlations is largely confined to longitudinal data, a special form of sequentially correlated data. This paper proposes a new method for the analysis of correlations to fully exploit the use of covariates for general correlated data. In a renewed analysis of the classroom data, a highly unbalanced multilevel clustered data with within-class and within-school correlations, our method reveals informative insights on these structures not previously known. In another analysis of the malaria immune response data in Benin, a longitudinal study with time-dependent covariates where the exact times of the observations are not available, our approach again provides promising new results. At the heart of our approach is a new generalized z-transformation that converts correlation matrices, constrained to be positive definite, to vectors with unrestricted support and is order-invariant. These two properties enable us to develop regression analysis incorporating covariates for the modelling of correlations via the use of maximum likelihood.},
  archive      = {J_AOAS},
  author       = {Jie Hu and Yu Chen and Chenlei Leng and Cheng Yong Tang},
  doi          = {10.1214/23-AOAS1785},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {184-198},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Applied regression analysis of correlations for correlated data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Generative machine learning methods for multivariate
ensemble postprocessing. <em>AOAS</em>, <em>18</em>(1), 159–183. (<a
href="https://doi.org/10.1214/23-AOAS1784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require postprocessing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate postprocessing have been proposed where ensemble predictions are first postprocessed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular, the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate postprocessing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper scoring rule, which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. Our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. In two case studies on multivariate temperature and wind speed forecasting at weather stations over Germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.},
  archive      = {J_AOAS},
  author       = {Jieyu Chen and Tim Janke and Florian Steinke and Sebastian Lerch},
  doi          = {10.1214/23-AOAS1784},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {159-183},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Generative machine learning methods for multivariate ensemble postprocessing},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A quantitative linguistic analysis of a cancer online health
community with a smooth latent space model. <em>AOAS</em>,
<em>18</em>(1), 144–158. (<a
href="https://doi.org/10.1214/23-AOAS1783">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online health communities (OHCs) provide free, open, and well-resourced platforms for patients, family members, and others to discuss illnesses, express feelings, and connect with others. Linguistic analysis of OHC posts can assist in better understanding disease conditions as well as monitoring the emotional and mental status of patients and those who are closely related. Many existing OHC linguistic analyses are limited by focusing on individual words. There are a handful of cooccurrence network analyses, which have multiple methodological limitations. In this article we analyze posts that are publicly available at the LUNGevity Foundation’s Lung Cancer Support Community (LCSC). The analyzed data contains 21,028 posts published between April 2018 and February 2022. For word cooccurrence network analysis, we develop a two-part latent space model, which advances from the existing ones by accommodating network weights. Further, we consider the scenario where there are change points in time, networks remain the same between two change points but differ on the two sides of a change point, and the number and locations of change points are unknown. A penalized fusion approach is developed to data-dependently determine change points and estimate networks. In data analysis multiple change points are identified, which reflect significant changes in lung cancer patients’ and their close affiliates’ emotional/mental status and mostly align with the changes in COVID-19. The obtained network structures and other findings are also sensible.},
  archive      = {J_AOAS},
  author       = {Mengque Liu and Xinyan Fan and Shuangge Ma},
  doi          = {10.1214/23-AOAS1783},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {144-158},
  shortjournal = {Ann. Appl. Stat.},
  title        = {A quantitative linguistic analysis of a cancer online health community with a smooth latent space model},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Using simultaneous regression calibration to study the
effect of multiple error-prone exposures on disease risk utilizing
biomarkers developed from a controlled feeding study. <em>AOAS</em>,
<em>18</em>(1), 125–143. (<a
href="https://doi.org/10.1214/23-AOAS1782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systematic measurement error in self-reported data creates important challenges in association studies between dietary intakes and chronic disease risks, especially when multiple dietary components are studied jointly. The joint regression calibration method has been developed for measurement error correction when objectively measured biomarkers are available for all dietary components of interest. Unfortunately, objectively measured biomarkers are only available for very few dietary components, which limits the application of the joint regression calibration method. Recently, for single dietary components, controlled feeding studies have been performed to develop new biomarkers for many more dietary components. However, it is unclear whether the biomarkers separately developed for single dietary components are valid for joint calibration. In this paper we show that biomarkers developed for single dietary components cannot be used for joint regression calibration. We propose new methods to utilize controlled feeding studies to develop valid biomarkers for joint regression calibration to estimate the association between multiple dietary components simultaneously with the disease of interest. Asymptotic distribution theory for the proposed estimators is derived. Extensive simulations are performed to study the finite sample performance of the proposed estimators. We apply our methods to examine the joint effects of sodium and potassium intakes on cardiovascular disease incidence using the Women’s Health Initiative cohort data. We identify positive associations between sodium intake and cardiovascular diseases as well as negative associations between potassium intake and cardiovascular disease.},
  archive      = {J_AOAS},
  author       = {Yiwen Zhang and Ran Dai and Ying Huang and Ross Prentice and Cheng Zheng},
  doi          = {10.1214/23-AOAS1782},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {125-143},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Using simultaneous regression calibration to study the effect of multiple error-prone exposures on disease risk utilizing biomarkers developed from a controlled feeding study},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Estimating fiber orientation distribution with application
to study brain lateralization using HCP d-MRI data. <em>AOAS</em>,
<em>18</em>(1), 100–124. (<a
href="https://doi.org/10.1214/23-AOAS1781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion-weighted magnetic resonance imaging (D-MRI) is an in vivo and noninvasive imaging technology for characterizing tissue microstructure in biological samples. A major application of D-MRI is for white matter fiber tract reconstruction in brains. It begins by estimating the water molecule movements (serving as proxies for fiber directions) in the brain voxels and then combines the results to form fiber tracts. The voxel-level fiber direction information can be modeled by a fiber orientation distribution (FOD) function, and in this paper, we propose a computationally scalable FOD estimator, the blockwise James–Stein (BJS) estimator. We then apply BJS to the D-MRI data from the Human Connectome Project (HCP) to study brain lateralization, an important topic in neuroscience. Specifically, we focus on the association between lateralization of the superior longitudinal fasciculus (SLF)—a major association tract and handedness. For each subject from the HCP data, we extract voxel-level directional information by BJS and then reconstruct the SLF in each brain hemisphere through a tractography algorithm. Finally, we derive a lateralization score that quantifies hemispheric asymmetry of the reconstructed SLF. We then relate this lateralization score to gender and handedness through an ANOVA model, where significant handedness effects are found. The results indicate that the SLF lateralization is likely to be different in right-handed and left-handed individuals. Codes and example scripts for both synthetic experiments and HCP data application can be found at https://github.com/vic-dragon/BJS.},
  archive      = {J_AOAS},
  author       = {Seungyong Hwang and Thomas C. M. Lee and Debashis Paul and Jie Peng},
  doi          = {10.1214/23-AOAS1781},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {100-124},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Estimating fiber orientation distribution with application to study brain lateralization using HCP D-MRI data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Bayesian multiple instance classification based on
hierarchical probit regression. <em>AOAS</em>, <em>18</em>(1), 80–99.
(<a href="https://doi.org/10.1214/23-AOAS1780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multiple instance learning (MIL), the response variable is predicted by features (or covariates) of one or more instances, which are collectively denoted as a bag. Learning the relationship between bags and instances is challenging because of the unknown and possibly complicated data generating mechanism regarding how instances contribute to the bag label. MIL has been applied to solve a variety of real-world problems, which have been mostly focused on supervised tasks, such as molecule activity prediction, protein binding affinities prediction, object detection, and computer-aided diagnosis. However, to date, the majority of the off-the-shelf MIL methods are developed in the computer science domain, and they focus on improving the prediction performance while spending little effort on explainability of the algorithm. In this article a Bayesian multiple instance learning model, based on probit regression (MICProB), is proposed, which contributes a significant portion to the suite of statistical methodologies for MIL. MICProB is composed of two nested probit regression models, where the inner model is estimated for predicting primary instances, which are considered as the “important” ones that determine the bag label, and the outer model is for predicting bag-level responses based on the primary instances estimated by the inner model. The posterior distribution of MICProB can be conveniently approximated using a Gibbs sampler, and the prediction for new bags can be performed in a fully integrated Bayesian way. We evaluate the performance of MICProB against 15 benchmark methods and demonstrate its competitiveness in simulation and real-data examples. In addition to its capability of identifying primary instances, as compared to existing optimization-based approaches, MICProB also enjoys great advantages in providing a transparent model structure, straightforward statistical inference of quantities related to model parameters, and favorable interpretability of covariate effects on the bag-level response.},
  archive      = {J_AOAS},
  author       = {Danyi Xiong and Seongoh Park and Johan Lim and Tao Wang and Xinlei Wang},
  doi          = {10.1214/23-AOAS1780},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {80-99},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Bayesian multiple instance classification based on hierarchical probit regression},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Distributed proportional likelihood ratio model with
application to data integration across clinical sites. <em>AOAS</em>,
<em>18</em>(1), 63–79. (<a
href="https://doi.org/10.1214/23-AOAS1779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world evidence synthesis through integration of data from distributed research networks has gained increasing attention in recent years. Due to privacy concerns and restrictions of sharing patient-level data, distributed algorithms that do not require sharing patient level information are in great need for facilitating multisite collaborations. On the other hand, data collected at multiple sites often come from diverse populations, and there exists a substantial amount of heterogeneity across sites in patient characteristics. Most of the existing distributed algorithms have ignored such between-site heterogeneity. In this paper we aim to fill this methodological gap by proposing a general distributed algorithm. We develop our distributed algorithm based on a general semiparametric model, namely, the proportional likelihood ratio model (Biometrika 99 (2012) 211–222), which is a semiparametric extension of generalized linear model. We devise the proportional likelihood ratio model with site-specific baseline function, to account for between-site heterogeneity, and shared regression parameters to borrow information across sites. Under this flexible formulation, our distributed algorithm is designed to be privacy-preserving and communication-efficient (i.e., only one round of communication across sites is needed). We validate our method via simulation studies and demonstrate the utility of our method via a multisite study of pediatric avoidable hospitalization based on electronic health record data from a total of 354,672 patients across 26 different clinical sites within the Children’s Hospital of Philadelphia health system.},
  archive      = {J_AOAS},
  author       = {Chongliang Luo and Rui Duan and Mackenzie Edmondson and Jiasheng Shi and Mitchell Maltenfort and Jeffrey S. Morris and Christopher B. Forrest and Rebecca Hubbard and Yong Chen},
  doi          = {10.1214/23-AOAS1779},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {63-79},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Distributed proportional likelihood ratio model with application to data integration across clinical sites},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the fair comparison of optimization algorithms in
different machines. <em>AOAS</em>, <em>18</em>(1), 42–62. (<a
href="https://doi.org/10.1214/23-AOAS1778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An experimental comparison of two or more optimization algorithms requires the same computational resources to be assigned to each algorithm. When a maximum runtime is set as the stopping criterion, all algorithms need to be executed in the same machine if they are to use the same resources. Unfortunately, the implementation code of the algorithms is not always available, which means that running the algorithms to be compared in the same machine is not always possible. And even if they are available, some optimization algorithms might be costly to run, such as training large neural-networks in the cloud. In this paper we consider the following problem: how do we compare the performance of a new optimization algorithm B with a known algorithm A in the literature if we only have the results (the objective values) and the runtime in each instance of algorithm A? Particularly, we present a methodology that enables a statistical analysis of the performance of algorithms executed in different machines. The proposed methodology has two parts. First, we propose a model that, given the runtime of an algorithm in a machine, estimates the runtime of the same algorithm in another machine. This model can be adjusted so that the probability of estimating a runtime longer than what it should be is arbitrarily low. Second, we introduce an adaptation of the one-sided sign test that uses a modified p-value and takes into account that probability. Such adaptation avoids increasing the probability of type I error associated with executing algorithms A and B in different machines.},
  archive      = {J_AOAS},
  author       = {Etor Arza and Josu Ceberio and Ekhiñe Irurozki and Aritz Pérez},
  doi          = {10.1214/23-AOAS1778},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {42-62},
  shortjournal = {Ann. Appl. Stat.},
  title        = {On the fair comparison of optimization algorithms in different machines},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sensitivity analysis of wind energy resources with bayesian
non-gaussian and nonstationary functional ANOVA. <em>AOAS</em>,
<em>18</em>(1), 23–41. (<a
href="https://doi.org/10.1214/23-AOAS1770">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transition from nonrenewable to renewable energies represents a global societal challenge, and developing a sustainable energy portfolio is an especially daunting task for developing countries where little to no information is available regarding the abundance of renewable resources such as wind. Weather model simulations are key to obtain such information when observational data are scarce and sparse over a country as large and geographically diverse as Saudi Arabia. However, output from such models is uncertain, as it depends on inputs such as the parametrization of the physical processes and the spatial resolution of the simulated domain. In such situations a sensitivity analysis must be performed, and the input may have a spatially heterogeneous influence of wind. In this work we propose a latent Gaussian functional analysis of variance (ANOVA) model that relies on a nonstationary Gaussian Markov random field approximation of a continuous latent process. The proposed approach is able to capture the local sensitivity of Gaussian and non-Gaussian wind characteristics such as speed and threshold exceedances over a large simulation domain, and a continuous underlying process also allows us to assess the effect of different spatial resolutions. Our results indicate that: (1) the nonlocal planetary boundary layer scheme and high spatial resolution are both instrumental in capturing wind speed and energy (especially over complex mountainous terrain), and (2) the impact of planetary boundary layer scheme and resolution on Saudi Arabia’s planned wind farms is small (at most 1.4%). Thus, our results lend support for the construction of these wind farms in the next decade.},
  archive      = {J_AOAS},
  author       = {Jiachen Zhang and Paola Crippa and Marc G. Genton and Stefano Castruccio},
  doi          = {10.1214/23-AOAS1770},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {23-41},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Sensitivity analysis of wind energy resources with bayesian non-gaussian and nonstationary functional ANOVA},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). RZiMM-scRNA: A regularized zero-inflated mixture model
framework for single-cell RNA-seq data. <em>AOAS</em>, <em>18</em>(1),
1–22. (<a href="https://doi.org/10.1214/23-AOAS1761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of single-cell RNA sequencing in various biomedical research areas have been blooming. This new technology provides unprecedented opportunities to study disease heterogeneity at the cellular level. However, unique characteristics of scRNA-seq data, including large dimensionality, high dropout rates, and possibly batch effects, bring great difficulty into the analysis of such data. Not appropriately addressing these issues obstructs true scientific discovery. Herein we propose a unified Regularized Zero-inflated Mixture Model framework, designed for scRNA-seq data (RZiMM-scRNA), to simultaneously detect cell subgroups and identify gene differential expression based on a developed importance score, accounting for both dropouts and batch effects. We conduct extensive simulation studies in which we evaluate the performance of RZiMM-scRNA and compare it with several popular methods, including Seurat, SC3, K-means, and hierarchical clustering. Simulation results show that RZiMM-scRNA demonstrates superior clustering performance and enhanced biomarker detection accuracy, compared to alternative methods, especially when cell subgroups are less distinct, verifying the robustness of our method. Our empirical investigations focus on two brain tumor studies dealing with astrocytoma of various grades, including the most malignant of all brain tumors, glioblastoma multiforme (GBM). Our goal is to delineate cell heterogeneity and identify driving biomarkers associated with these tumors. Notably, RZiMM-scNRA successfully identifies a small group of oligodendrocyte cells, which has drawn much attention in biomedical literature on brain cancers. In addition, our method discovers several new biomarkers which are not discussed in the original studies, including PLP1, BCAN, and PTPRZ1—all associated with the development and malignant growth of glioma—as well as CAMK2B, which is downregulated in glioma and GBM and implicated in neurodevelopment, brain function, learning and memory processes.},
  archive      = {J_AOAS},
  author       = {Xinlei Mi and William Bekerman and Anil K. Rustgi and Peter A. Sims and Peter D. Canoll and Jianhua Hu},
  doi          = {10.1214/23-AOAS1761},
  journal      = {The Annals of Applied Statistics},
  month        = {3},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Ann. Appl. Stat.},
  title        = {RZiMM-scRNA: A regularized zero-inflated mixture model framework for single-cell RNA-seq data},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Correction to: Spatiotemporal wildfire modeling through
point processes with moderate and extreme marks. <em>AOAS</em>,
<em>18</em>(1), 899–903. (<a
href="https://doi.org/10.1214/23-AOAS1861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AOAS},
  author       = {Jonathan Koh and François Pimont and Jean-Luc Dupuy and Thomas Opitz},
  doi          = {10.1214/23-AOAS1861},
  journal      = {The Annals of Applied Statistics},
  month        = {2},
  number       = {1},
  pages        = {899-903},
  shortjournal = {Ann. Appl. Stat.},
  title        = {Correction to: Spatiotemporal wildfire modeling through point processes with moderate and extreme marks},
  volume       = {18},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
