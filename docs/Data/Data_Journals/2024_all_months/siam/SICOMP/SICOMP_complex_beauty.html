<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SICOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sicomp---57">SICOMP - 57</h2>
<ul>
<li><details>
<summary>
(2024). Counting small induced subgraphs satisfying monotone
properties. <em>SICOMP</em>, <em>53</em>(6), FOCS20-139-174. (<a
href="https://doi.org/10.1137/20M1365624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a graph property , the problem asks, on input of a graph and a positive integer , to compute the number of induced subgraphs of size in that satisfy . The search for explicit criteria on ensuring that is hard was initiated by Jerrum and Meeks [J. Comput. System Sci., 81 (2015), pp. 702–716] and is part of the major line of research on counting small patterns in graphs. However, apart from an implicit result due to Curticapean, Dell, and Marx [STOC, ACM, New York, pp. 151–158] proving that a full classification into “easy” and “hard” properties is possible and some partial results on edge-monotone properties due to Meeks [Discrete Appl. Math., 198 (2016), pp. 170–194] and Dörfler et al. [MFCS, LIPIcs Leibniz Int. Proc. Inform. 138, Wadern Germany, 2019, 26], not much is known. In this work, we fully answer and explicitly classify the case of monotone, that is, subgraph-closed, properties: We show that for any nontrivial monotone property , the problem cannot be solved in time for any function , unless the exponential time hypothesis fails. By this, we establish that any significant improvement over the brute-force approach is unlikely; in the language of parameterized complexity, we also obtain a -completeness result. The methods we develop for the above problem also allow us to prove a conjecture by Jerrum and Meeks [ACM Trans. Comput. Theory, 7 (2015), 11; Combinatorica 37 (2017), pp. 965–990]: is -complete if is a nontrivial graph property only depending on the number of edges of the graph.},
  archive      = {J_SICOMP},
  author       = {Marc Roth and Johannes Schmitt and Philip Wellnitz},
  doi          = {10.1137/20M1365624},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-139-174},
  shortjournal = {SIAM J. Comput.},
  title        = {Counting small induced subgraphs satisfying monotone properties},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Low-density parity-check codes achieve list-decoding
capacity. <em>SICOMP</em>, <em>53</em>(6), FOCS20-38-73. (<a
href="https://doi.org/10.1137/20M1365934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that Gallager&#39;s ensemble of low-density parity-check (LDPC) codes achieves list-decoding capacity with high probability. These are the first graph-based codes shown to have this property. This result opens up a potential avenue toward truly linear-time list-decodable codes that achieve list-decoding capacity. Our result on list-decoding follows from a much more general result: any local property satisfied with high probability by a random linear code is also satisfied with high probability by a random LDPC code from Gallager&#39;s distribution. Local properties are properties characterized by the exclusion of small sets of codewords and include list-decodability, list-recoverability, and average-radius list-decodability. In order to prove our results on LDPC codes, we establish sharp thresholds for when local properties are satisfied by a random linear code. More precisely, we show that for any local property $\mathcal{P}$, there is some $R^*$ so that random linear codes of rate slightly less than $R^*$ satisfy $\mathcal{P}$ with high probability, while random linear codes of rate slightly more than $R^*$, with high probability, do not. We also give a characterization of the threshold rate $R^*$.},
  archive      = {J_SICOMP},
  author       = {Jonathan Mosheiff and Nicolas Resch and Noga Ron-Zewi and Shashwat Silas and Mary Wootters},
  doi          = {10.1137/20M1365934},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-38-73},
  shortjournal = {SIAM J. Comput.},
  title        = {Low-density parity-check codes achieve list-decoding capacity},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral independence in high-dimensional expanders and
applications to the hardcore model. <em>SICOMP</em>, <em>53</em>(6),
FOCS20-1-37. (<a href="https://doi.org/10.1137/20M1367696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We say a probability distribution $\mu$ is spectrally independent if an associated pairwise influence matrix has a bounded largest eigenvalue for the distribution and all of its conditional distributions. We prove that if $\mu$ is spectrally independent, then the corresponding high-dimensional simplicial complex is a local spectral expander. Using a line of recent works on mixing time of high-dimensional walks on simplicial complexes [T. Kaufman and D. Mass, Proceedings of ITCS, 2017, pp. 4:1–4:27; I. Dinur and T. Kaufman, Proceedings of the IEEE 58th Annual Symposium on Foundations of Computer Science, 2017, pp. 974–985; T. Kaufman and I. Oppenheim, Proceedings of APPROX/RANDOM, 2018, pp. 47:1–47:17; V. L. Alev and L. C. Lau, Proceedings of the 52nd Annual ACM Symposium on Theory of Computing, 2020], this implies that the corresponding Glauber dynamics mixes rapidly and generates (approximate) samples from $\mu$. As an application, we show that natural Glauber dynamics mixes rapidly (in polynomial time) to generate a random independent set from the hardcore model up to the uniqueness threshold. This improves the quasi-polynomial running time of Weitz&#39;s deterministic correlation decay algorithm [D. Weitz, Proceedings of the 38th Annual ACM Symposium on Theory of Computing, 2006, pp. 140–149] for estimating the hardcore partition function, also answering a long-standing open problem of mixing time of Glauber dynamics [M. Luby and E. Vigoda, Proceedings of the 29th Annual ACM Symposium on Theory of Computing, 1997, pp. 682–687; M. Luby and E. Vigoda, Random Structures Algorithms, 15 (1999), pp. 229–241; M. Dyer and C. Greenhill, J. Algorithms, 35 (2000), pp. 17–49; E. Vigoda, Electron. J. Combin., 8 (2001); C. Efthymiou et al., Proceedings of FOCS, 2016, pp. 704–713].},
  archive      = {J_SICOMP},
  author       = {Nima Anari and Kuikui Liu and Shayan Oveis Gharan},
  doi          = {10.1137/20M1367696},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-1-37},
  shortjournal = {SIAM J. Comput.},
  title        = {Spectral independence in high-dimensional expanders and applications to the hardcore model},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Polynomial data structure lower bounds in the group model.
<em>SICOMP</em>, <em>53</em>(6), FOCS20-74-101. (<a
href="https://doi.org/10.1137/20M1381988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proving superlogarithmic data structure lower bounds in the static group model has been a fundamental challenge in computational geometry since the early &#39;80s. We prove a polynomial ($n^{\Omega(1)}$) lower bound for an explicit range counting problem of $n^3$ convex polygons in $\mathbb{R}^2$ (each with $n^{\tilde{O}(1)}$ facets/semialgebraic complexity), against linear storage arithmetic data structures in the group model. Our construction and analysis are based on a combination of techniques in Diophantine approximation, pseudorandomness, and compressed sensing—in particular, on the existence and partial derandomization of optimal binary compressed sensing matrices in the polynomial sparsity regime ($k = n^{1-\delta}$). As a byproduct, this establishes a (logarithmic) separation between compressed sensing matrices and the stronger RIP property.},
  archive      = {J_SICOMP},
  author       = {Alexander Golovnev and Gleb Posobin and Oded Regev and Omri Weinstein},
  doi          = {10.1137/20M1381988},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-74-101},
  shortjournal = {SIAM J. Comput.},
  title        = {Polynomial data structure lower bounds in the group model},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An <span
class="math inline"><em>O</em>(log log <em>m</em>)</span> prophet
inequality for subadditive combinatorial auctions. <em>SICOMP</em>,
<em>53</em>(6), FOCS20-239-275. (<a
href="https://doi.org/10.1137/20M1382799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prophet inequalities compare the expected performance of an online algorithm for a stochastic optimization problem to the expected optimal solution in hindsight. They are a major alternative to classic worst-case competitive analysis, of particular importance in the design and analysis of simple (posted-price) incentive compatible mechanisms with provable approximation guarantees. A central open problem in this area concerns subadditive combinatorial auctions. Here $n$ agents with subadditive valuation functions compete for the assignment of $m$ items. The goal is to find an allocation of the items that maximizes the total value of the assignment. The question is whether there exists a prophet inequality for this problem that significantly beats the best known approximation factor of $O(\log m)$. We make major progress on this question by providing an $O(\log \log m)$ prophet inequality. Our proof goes through a novel primal-dual approach. It is also constructive, resulting in an online policy that takes the form of static and anonymous item prices that can be computed in polynomial time given appropriate query access to the valuations. As an application of our approach, we construct a simple and incentive compatible mechanism based on posted prices that achieves an $O(\log \log m)$ approximation to the optimal revenue for subadditive valuations under an item-independence assumption.},
  archive      = {J_SICOMP},
  author       = {Paul Dütting and Thomas Kesselheim and Brendan Lucier},
  doi          = {10.1137/20M1382799},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-239-275},
  shortjournal = {SIAM J. Comput.},
  title        = {An $O(\log \log m)$ prophet inequality for subadditive combinatorial auctions},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A parameterized approximation scheme for min <span
class="math inline"><em>k</em></span>-cut. <em>SICOMP</em>,
<em>53</em>(6), FOCS20-205-238. (<a
href="https://doi.org/10.1137/20M1383197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Min $k$-Cut problem, the input consists of an edge weighted graph $G$ and an integer $k$, and the task is to partition the vertex set into $k$ nonempty sets, such that the total weight of the edges with endpoints in different parts is minimized. When $k$ is part of the input, the problem is NP-complete and hard to approximate within any factor less than 2. Recently, the problem has received significant attention from the perspective of parameterized approximation. Gupta, Lee, and Li [Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms, A. Czumaj, ed., SIAM, Philadelphia, 2018, pp. 2821–2837] initiated the study of FPT-approximation for the Min $k$-Cut problem and gave a 1.9997-approximation algorithm running in time $2^{\mathcal{O}(k^6)}n^{\mathcal{O}(1)}$. Later, the same set of authors [Proceedings of the 59th IEEE Annual Symposium on Foundations of Computer Science, M. Thorup, ed., 2018, pp. 113–123] designed a $(1 +\epsilon)$-approximation algorithm that runs in time $(k/\epsilon)^{\mathcal{O}(k)}n^{k+\mathcal{O}(1)}$ and a 1.81-approximation algorithm running in time $2^{\mathcal{O}(k^2)}n^{\mathcal{O}(1)}$. More, recently, Kawarabayashi and Lin [Proceedings of the 31st ACM-SIAM Symposium on Discrete Algorithms, S. Chawla, ed., SIAM, Philadelphia, 2020, pp. 990–999] gave a $(5/3 + \epsilon)$-approximation for Min $k$-Cut running in time $2^{\mathcal{O}(k^2 \log k)}n^{\mathcal{O}(1)}$. In this paper, we give a parameterized approximation algorithm with best possible approximation guarantee and best possible running time dependence on said guarantee (up to the exponential time hypothesis and constants in the exponent). In particular, for every $\epsilon &gt; 0$, the algorithm obtains a $(1 +\epsilon)$-approximate solution in time $(k/\epsilon)^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$. The main ingredients of our algorithm are a simple sparsification procedure, a new polynomial time algorithm for decomposing a graph into highly connected parts, and a new exact algorithm with running time $s^{\mathcal{O}(k)}n^{\mathcal{O}(1)}$ on unweighted (multi-) graphs. Here, $s$ denotes the number of edges in a minimum $k$-cut. The latter two are of independent interest.},
  archive      = {J_SICOMP},
  author       = {Daniel Lokshtanov and Saket Saurabh and Vaishali Surianarayanan},
  doi          = {10.1137/20M1383197},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-205-238},
  shortjournal = {SIAM J. Comput.},
  title        = {A parameterized approximation scheme for min $k$-cut},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Unit capacity maxflow in almost <span
class="math inline"><em>m</em><sup>4/3</sup></span> time.
<em>SICOMP</em>, <em>53</em>(6), FOCS20-175-204. (<a
href="https://doi.org/10.1137/20M1383525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an algorithm which given any $m$-edge directed graph with positive integer capacities at most $U$, vertices $a$ and $b$, and an approximation parameter $\epsilon \in (0, 1)$ computes an additive $\epsilon mU$-approximate $a$-$b$ maximum flow in time $m^{1+o(1)}/\sqrt{\epsilon}$. By applying the algorithm for $\epsilon = (mU)^{-2/3}$, rounding to an integral flow, and using augmenting paths, we obtain an algorithm which computes an exact $a$-$b$ maximum flow in time $m^{4/3+o(1)}U^{1/3}$ and an algorithm which given an $m$-edge bipartite graph computes an exact maximum cardinality matching in time $m^{4/3+o(1)}$.},
  archive      = {J_SICOMP},
  author       = {Tarun Kathuria and Yang P. Liu and Aaron Sidford},
  doi          = {10.1137/20M1383525},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-175-204},
  shortjournal = {SIAM J. Comput.},
  title        = {Unit capacity maxflow in almost $m^{4/3}$ time},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Constant depth formula and partial function versions of MCSP
are hard. <em>SICOMP</em>, <em>53</em>(6), FOCS20-317-367. (<a
href="https://doi.org/10.1137/20M1383562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attempts to prove the intractability of the Minimum Circuit Size Problem ($\mathsf{MCSP}$) date as far back as the 1950s and are well motivated by connections to cryptography, learning theory, and average-case complexity. In this work, we make progress, on two fronts, towards showing $\mathsf{MCSP}$ is intractable under worst-case assumptions. While Masek showed in the late 1970s that the version of $\mathsf{MCSP}$ for $\mathsf{DNF}$ formulas is $\mathsf{NP}$-hard, extending this result to the case of depth-3 AND/OR formulas was open. We show that determining the minimum size of a depth-$d$ formula computing a given Boolean function is $\mathsf{NP}$-hard under quasipolynomial-time randomized reductions for all constant $d \geq 2$. Our approach is based on a method to “lift” depth-$d$ formula lower bounds to depth-$(d+1)$. This method also implies the existence of a function with a $2^{\Omega_d(n)}$ additive gap between its depth-$d$ and depth-$(d+1)$ formula complexity. We also make progress in the case of general, unrestricted circuits. We show that the version of $\mathsf{MCSP}$ where the input is a partial function (represented by a string in $\{0,1,\star\}^*$) is not in $\mathsf{P}$ under the Exponential Time Hypothesis (ETH). Intriguingly, we formulate a notion of lower bound statements being $(\mathsf{P/poly})$-recognizable that is closely related to Razborov and Rudich&#39;s definition of being $(\mathsf{P/poly})$-constructive. We show that unless there are subexponential-sized circuits computing $\mathsf{SAT}$, the collection of lower bound statements used to prove the correctness of our reductions cannot be $(\mathsf{P/poly})$-recognizable.},
  archive      = {J_SICOMP},
  author       = {Rahul Ilango},
  doi          = {10.1137/20M1383562},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-317-367},
  shortjournal = {SIAM J. Comput.},
  title        = {Constant depth formula and partial function versions of MCSP are hard},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decodable quantum LDPC codes beyond the <span
class="math inline">$\sqrt{n}$</span> distance barrier using
high-dimensional expanders. <em>SICOMP</em>, <em>53</em>(6),
FOCS20-276-316. (<a href="https://doi.org/10.1137/20M1383689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing quantum low-density parity-check (LDPC) codes with a minimum distance that grows faster than a square root of the length has been a major challenge of the field. With this challenge in mind, we investigate constructions that come from high-dimensional expanders, in particular Ramanujan complexes. These naturally give rise to very unbalanced quantum error correcting codes that have a large $X$-distance but a much smaller $Z$-distance. However, together with a classical expander LDPC code and a tensoring method that generalizes a construction of Hastings and also the Tillich–Zémor construction of quantum codes, we obtain quantum LDPC codes whose minimum distance exceeds the square root of the code length and whose dimension comes close to a square root of the code length. When the ingredient is a 2-dimensional Ramanujan complex, or the 2-skeleton of a 3-dimensional Ramanujan complex, we obtain a quantum LDPC code of minimum distance $n^{1/2}\log^{1/2}n$. We then exploit the expansion properties of the complex to devise the first polynomial-time algorithm that decodes above the square root barrier for quantum LDPC codes. Using a 3-dimensional Ramanujan complex, we also obtain an overall quantum code of minimum distance $n^{1/2}\log n$, which sets a new record for quantum LDPC codes.},
  archive      = {J_SICOMP},
  author       = {Shai Evra and Tali Kaufman and Gilles Zémor},
  doi          = {10.1137/20M1383689},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-276-316},
  shortjournal = {SIAM J. Comput.},
  title        = {Decodable quantum LDPC codes beyond the $\sqrt{n}$ distance barrier using high-dimensional expanders},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Smoothing the gap between NP and ER. <em>SICOMP</em>,
<em>53</em>(6), FOCS20-102-138. (<a
href="https://doi.org/10.1137/20M1385287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study algorithmic problems that belong to the complexity class of the existential theory of the reals . A problem is -complete if it is as hard as the problem existential theory of the reals (ETR) and if it can be written as an ETR formula. Traditionally, these problems are studied in the real random access machine (RAM), a model of computation that assumes that the storage and comparison of real-valued numbers can be done in constant space and time, with infinite precision. The complexity class is often called a real RAM analogue of NP, since the problem ETR can be viewed as the real-valued variant of SAT. The real RAM assumption that we can represent and in which we can compare arbitrary irrational values in constant space and time is not very realistic. Yet this assumption is vital, since some -complete problems have an “exponential bit phenomenon,” where there exists an input for the problem, such that the witness of the solution requires geometric coordinates which need exponential word size when represented in binary. The problems that exhibit this phenomenon are NP-hard (since ETR is NP-hard) but it is unknown if they lie in NP. NP membership is often showed by using the famous Cook–Levin theorem, which states that the existence of a polynomial-time verification algorithm for the problem witness is equivalent to NP membership. The exponential bit phenomenon prohibits a straightforward application of the Cook–Levin theorem. In this paper we first present a result which we believe to be of independent interest: we prove a real RAM analogue to the Cook–Levin theorem which shows that membership is equivalent to having a verification algorithm that runs in polynomial-time on a real RAM. This gives an easy proof of -membership, as verification algorithms on a real RAM are much more versatile than ETR formulas. We use this result to construct a framework to study -complete problems under smoothed analysis. We show that for a wide class of -complete problems, its witness can be represented with logarithmic input-precision by using smoothed analysis on its real RAM verification algorithm. This shows in a formal way that the boundary between NP and (formed by inputs whose solution witness needs high input-precision) consists of contrived input. We apply our framework to well-studied -complete recognition problems which have the exponential bit phenomenon such as the recognition of realizable order types or the Steinitz problem in fixed dimension. Interestingly our techniques also generalize to problems with a natural notion of resource augmentation (geometric packing, the art gallery problem).},
  archive      = {J_SICOMP},
  author       = {Jeff Erickson and Ivor van der Hoog and Tillmann Miltzow},
  doi          = {10.1137/20M1385287},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-102-138},
  shortjournal = {SIAM J. Comput.},
  title        = {Smoothing the gap between NP and ER},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Symmetries, graph properties, and quantum speedups.
<em>SICOMP</em>, <em>53</em>(6), FOCS20-368-415. (<a
href="https://doi.org/10.1137/23M1573975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Aaronson and Ambainis [Theory Comput., 10 (2014), pp. 133–166] and Chailloux [Proceedings of the 10th Innovations in Theoretical Computer Science Conference, 2018, pp. 19:1–19:7] showed that fully symmetric (partial) functions do not admit exponential quantum query speedups. This raises a natural question: how symmetric must a function be before it cannot exhibit a large quantum speedup? In this work, we prove that hypergraph symmetries in the adjacency matrix model allow at most a polynomial separation between randomized and quantum query complexities. We also show that, remarkably, permutation groups constructed out of these symmetries are essentially the only permutation groups that prevent superpolynomial quantum speedups. We prove this by fully characterizing the primitive permutation groups that allow superpolynomial quantum speedups. In contrast, in the adjacency list model for bounded-degree graphs—where graph symmetry is manifested differently—we exhibit a property testing problem that shows an exponential quantum speedup. These results resolve open questions posed by Ambainis, Childs, and Liu [Lecture Notes in Comput. Sci. 6845, Springer, 2011, pp. 365–376] and Montanaro and de Wolf [Theory Comput., 7 (2016)].},
  archive      = {J_SICOMP},
  author       = {Shalev Ben-David and Andrew M. Childs and András Gilyén and William Kretschmer and Supartha Podder and Daochen Wang},
  doi          = {10.1137/23M1573975},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-368-415},
  shortjournal = {SIAM J. Comput.},
  title        = {Symmetries, graph properties, and quantum speedups},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Special section on the sixty-first annual IEEE symposium on
foundations of computer science (2020). <em>SICOMP</em>, <em>53</em>(6),
FOCS20–i. (<a href="https://doi.org/10.1137/24M1694859">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SICOMP},
  author       = {Yuval Filmus and Elena Grigorescu and Sungjin Im and Yi Li},
  doi          = {10.1137/24M1694859},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {FOCS20-i},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the sixty-first annual IEEE symposium on foundations of computer science (2020)},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Collapsing the bounded width hierarchy for infinite-domain
constraint satisfaction problems: When symmetries are enough.
<em>SICOMP</em>, <em>53</em>(6), 1709–1745. (<a
href="https://doi.org/10.1137/22M1538934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove that relational structures admitting specific polymorphisms (namely, canonical pseudo-WNU operations of all arities ) have low relational width. This implies a collapse of the bounded width hierarchy for numerous classes of infinite-domain constraint satisfaction problems (CSPs) studied in the literature. Moreover, we obtain a characterization of bounded width for first-order reducts of unary structures and a characterization of Monotone Monadic SNP (MMSNP) sentences that are equivalent to a Datalog program, answering a question posed by Bienvenu et al. In particular, the bounded width hierarchy collapses in those cases as well. Our results extend the scope of theorems of Barto and Kozik characterizing bounded width for finite structures and show the applicability of infinite-domain CSPs to other fields.},
  archive      = {J_SICOMP},
  author       = {Antoine Mottet and Tomáš Nagy and Michael Pinsker and Michał Wrona},
  doi          = {10.1137/22M1538934},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {1709-1745},
  shortjournal = {SIAM J. Comput.},
  title        = {Collapsing the bounded width hierarchy for infinite-domain constraint satisfaction problems: When symmetries are enough},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decidability of membership problems for flat rational
subsets of <span
class="math inline">GL<strong>(</strong><strong>2</strong><strong>,</strong> ℚ<strong>)</strong></span>
and singular matrices. <em>SICOMP</em>, <em>53</em>(6), 1663–1708. (<a
href="https://doi.org/10.1137/22M1512612">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider membership problems for rational subsets of the semigroup of matrices over . For a semigroup , the rational subsets are defined as the sets accepted by nondeterministic finite automatons whose transitions are labeled by elements of . In general, it is undecidable on inputs and whether belongs to . Therefore, we restrict our attention to the family of flat rational subsets of over , where is a subsemigroup of . It consists of finite unions of the form , where and . Assuming that the membership for is decidable, we prove various results when the membership for is decidable. If is a subgroup of a group , then we provide a rather general condition when is an (effective) relative Boolean algebra. This leads to one of our main results that the emptiness problem for Boolean combinations of sets in is decidable. It is possible that such a strong decidability result cannot be pushed any further for groups sitting between and . To support this possibility, we prove the following dichotomy: If is a finitely generated group such that , then either or contains an extension of the Baumslag–Solitar group of infinite index. It is open whether the membership for rational subsets is decidable in the latter case. For singular matrices, we will show that the membership problem for is decidable in doubly exponential time, where is the monoid generated by .},
  archive      = {J_SICOMP},
  author       = {Volker Diekert and Igor Potapov and Pavel Semukhin},
  doi          = {10.1137/22M1512612},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {1663-1708},
  shortjournal = {SIAM J. Comput.},
  title        = {Decidability of membership problems for flat rational subsets of \(\boldsymbol{{\textrm{GL}}(2,\boldsymbol{{\mathbb{Q}}})}\) and singular matrices},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Prophet secretary for combinatorial auctions and matroids.
<em>SICOMP</em>, <em>53</em>(6), 1641–1662. (<a
href="https://doi.org/10.1137/19M1264047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The secretary and the prophet inequality problems are central to the field of stopping theory. Recently, there has been a lot of work in generalizing these models to multiple items because of their applications in mechanism design. The most important of these generalizations are to matroids and to combinatorial auctions. Kleinberg and Weinberg and Feldman, Gravin, and Lucier show that for adversarial arrival order of random variables the optimal prophet inequalities give a -approximation. For many settings, however, it is conceivable that the arrival order is chosen uniformly at random, akin to the secretary problem. For such a random arrival model, we improve upon the -approximation and obtain -approximation prophet inequalities for both matroids and combinatorial auctions. This also gives improvements to the results of Yan and of Esfandiari and colleagues who worked in the special cases where either we can fully control the arrival order or there is only a single item. Our techniques are threshold based. We convert our discrete problem into a continuous setting and then give a generic template on how to dynamically adjust these thresholds to lower bound the expected total welfare.},
  archive      = {J_SICOMP},
  author       = {Soheil Ehsani and Mohammadtaghi Hajiaghayi and Thomas Kesselheim and Sahil Singla},
  doi          = {10.1137/19M1264047},
  journal      = {SIAM Journal on Computing},
  month        = {12},
  number       = {6},
  pages        = {1641-1662},
  shortjournal = {SIAM J. Comput.},
  title        = {Prophet secretary for combinatorial auctions and matroids},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Twin-width III: Max independent set, min dominating set, and
coloring. <em>SICOMP</em>, <em>53</em>(5), 1602–1640. (<a
href="https://doi.org/10.1137/21M142188X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We recently introduced the notion of twin-width, a novel graph invariant, and showed that first-order model checking can be solved in time for -vertex graphs given with a witness that the twin-width is at most , called -contraction sequence or -sequence, and formulas of size [Bonnet et al., JACM ’22]. The inevitable price to pay for such a general result is that is a tower of exponentials of height roughly . In this paper, we show that algorithms based on twin-width need not be impractical. We present -time algorithms for -independent set, -scattered set, -clique, and -dominating set when an -sequence of the graph is given in input. We further show how to solve the weighted version of -independent set, subgraph isomorphism, and induced subgraph isomorphism in the slightly worse running time . Up to logarithmic factors in the exponent, all these running times are optimal unless the exponential time hypothesis fails. Like our first-order model checking algorithm, these new algorithms are based on a dynamic programming scheme following the sequence of contractions forward. We then show a second algorithmic use of the contraction sequence by starting at its end and rewinding it. As an example of such a reverse scheme, we present a polynomial-time algorithm that properly colors the vertices of a graph with relatively few colors, thereby establishing that bounded twin-width classes are -bounded. This significantly extends the -boundedness of bounded rank-width classes and does so with a very concise proof. It readily yields a constant approximation for max independent set on -free graphs of bounded twin-width and a -approximation for min coloring on bounded twin-width graphs. We further observe that a constant approximation for max independent set on bounded twin-width graphs (but arbitrarily large clique number) would actually imply a polynomial-time approximation scheme. The third algorithmic use of twin-width builds on the second one. Playing the contraction sequence backward, we show that bounded twin-width graphs can be edge-partitioned into a linear number of bicliques such that both sides of the bicliques are on consecutive vertices in a fixed vertex ordering. This property is trivially shared with graphs of bounded average degree. Given that biclique edge-partition, we show how to solve the unweighted single-source shortest paths, and hence all-pairs shortest paths, in time and time , respectively. In sharp contrast, even diameter does not admit a truly subquadratic algorithm on bounded twin-width graphs unless the strong exponential time hypothesis fails. The fourth algorithmic use of twin-width builds on the so-called versatile tree of contractions [Bonnet et al., Comb. Theory ’22], a branching and more robust witness of low twin-width. We present constant-approximation algorithms for min dominating set and related problems on bounded twin-width graphs by showing that the integrality gap is constant. This is done by going down the versatile tree and stopping according to a problem-dependent criterion. At the reached node, a greedy approach yields the desired approximation.},
  archive      = {J_SICOMP},
  author       = {Édouard Bonnet and Colin Geniet and Eun Jung Kim and Stéphan Thomassé and Rémi Watrigant},
  doi          = {10.1137/21M142188X},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1602-1640},
  shortjournal = {SIAM J. Comput.},
  title        = {Twin-width III: Max independent set, min dominating set, and coloring},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Small but unwieldy: A lower bound on adjacency labels for
small classes. <em>SICOMP</em>, <em>53</em>(5), 1578–1601. (<a
href="https://doi.org/10.1137/23M1618661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We show that for any natural number , there is a constant and a subgraph-closed class having, for any natural , at most graphs on vertices up to isomorphism, but no adjacency labeling scheme with labels of size at most . In other words, for every , there is a small—even tiny—monotone class without universal graphs of size . Prior to this result, it was not excluded that every small class has an almost linear universal graph, or equivalently a labeling scheme with labels of size . The existence of such a labeling scheme, a scaled-down version of the recently disproved Implicit Graph Conjecture, was repeatedly raised [Gavoille and Labourel, Proceedings of the 15th Annual European Symposium on Algorithms, Lecture Notes in Comput. Sci. 4698, Springer, 2007, pp. 582–593; Dujmović et al., J. ACM, 68 (2021), pp. 1–33; Bonamy, Gavoille, and Pilipczuk, SIAM J. Discrete Math., 36 (2022), pp. 2082–2099; Bonnet et al., Comb. Theory, 2 (2022)]. Furthermore, our small monotone classes have unbounded twin-width and thus simultaneously disprove the already-refuted Small conjecture, but this time with a self-contained proof, not relying on elaborate group-theoretic constructions. As our main ingredient, we show that with high probability an Erdős–Rényi random graph with has, for every , at most subgraphs on vertices, up to isomorphism. As a barrier to our general method of producing even more complex tiny classes, we show that when , the latter no longer holds. More concretely, we provide an explicit lower bound on the number of unlabeled -vertex induced subgraphs of when . We thereby obtain a threshold for the property of having exponentially many unlabeled induced subgraphs: if with , then with high probability even the number of all unlabeled (not necessarily induced) subgraphs is , whereas if for sufficiently large , then with high probability the number of unlabeled induced subgraphs is . This result supplements the study of counting unlabeled induced subgraphs that was initiated by Erdős and Rényi with a question on the number of unlabeled induced subgraphs of Ramsey graphs, eventually answered by Shelah.},
  archive      = {J_SICOMP},
  author       = {Édouard Bonnet and Julien Duron and John Sylvester and Viktor Zamaraev and Maksim Zhukovskii},
  doi          = {10.1137/23M1618661},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1578-1601},
  shortjournal = {SIAM J. Comput.},
  title        = {Small but unwieldy: A lower bound on adjacency labels for small classes},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Internal pattern matching queries in a text and
applications. <em>SICOMP</em>, <em>53</em>(5), 1524–1577. (<a
href="https://doi.org/10.1137/23M1567618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider several types of internal queries, that is, questions about fragments of a given text specified in constant space by their locations in . Our main result is an optimal data structure for internal pattern matching (IPM) queries, which, given two fragments and , ask for a representation of all fragments contained in and matching exactly. This problem can be viewed as an internal version of the fundamental exact pattern matching problem: We are looking for exact occurrences of one substring of within another substring of . Our data structure answers IPM queries in time proportional to the quotient of the fragments’ lengths, which is required due to the worst-case information content of the output. If is a text of length over an integer alphabet of size , then our data structure occupies machine words (that is, bits) and admits an -time construction algorithm. We also show how to use IPM queries for answering internal queries corresponding to other classic string processing problems. Among others, we derive optimal data structures reporting the periods of a fragment and testing the cyclic equivalence of two fragments. Since the publication of the conference version of this paper [Kociumaka et al., Internal pattern matching queries in a text and applications, SODA 2015], IPM queries have found numerous further applications, following the path paved by the classic longest common extension (LCE) queries of Landau and Vishkin [J. Comput. System Sci., 37 (1988), pp. 63–78]. In particular, IPM queries have been implemented in grammar-compressed and dynamic settings and, along with LCE queries, constitute elementary operations of the model, developed by Charalampopoulos, Kociumaka, and Wellnitz [Faster approximate pattern matching: A unified approach, FOCS 2020] to design approximate pattern matching algorithms that work in multiple settings. All our algorithms are deterministic, whereas the data structure in the conference version of the paper only admits a randomized construction in expected time. To achieve this, we provide a novel construction of string synchronizing sets of Kempa and Kociumaka [String synchronizing sets: Sublinear-time BWT construction and optimal LCE data structure, STOC 2019]. Our method, based on a new restricted version of the recompression technique of Jeż [J. ACM, 63 (2016), pp. 4:1–4:51], yields a hierarchy of string synchronizing sets covering the whole spectrum of the fragments’ lengths.},
  archive      = {J_SICOMP},
  author       = {Tomasz Kociumaka and Jakub Radoszewski and Wojciech Rytter and Tomasz Waleń},
  doi          = {10.1137/23M1567618},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1524-1577},
  shortjournal = {SIAM J. Comput.},
  title        = {Internal pattern matching queries in a text and applications},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). An improved upper bound for the universal TSP on the grid.
<em>SICOMP</em>, <em>53</em>(5), 1476–1523. (<a
href="https://doi.org/10.1137/17M1154849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the universal traveling salesman problem in an grid with the shortest path metric. The goal is to define a (universal) total ordering over the vertices of the grid, in a way that for any input (subset of vertices), the tour, which visits the points in this ordering, is a good approximation of the optimal tour, i.e., has low competitive ratio. This problem was first studied by Platzman and Bartholdi [J. Assoc. Comput. Mach., 36 (1989), pp. 719–737]. They proposed a heuristic, which was based on the Sierpinski space-filling curve, in order to define a universal ordering of the unit square under the Euclidean metric. Their heuristic visits the points of the unit square in the order of their appearance along the space-filling curve. They provided a logarithmic upper bound which was shown to be tight up to a constant by Bertsimas and Grigni [Oper. Res. Lett. 8 (1989), pp. 241–244]. Bertsimas and Grigni further showed logarithmic lower bounds for other space-filling curves, and they conjectured that any universal ordering has a logarithmic lower bound for the grid. In this work, we disprove this conjecture by showing that there exists a universal ordering of the grid with competitive ratio of . The heuristic we propose defines a universal ordering of the vertices of the grid based on a generalization of the Lebesgue space-filling curve. In order to analyze the competitive ratio of our heuristic, we employ techniques from the theory of geometric spanners in Euclidean spaces. We finally show that our analysis is tight up to a constant factor.},
  archive      = {J_SICOMP},
  author       = {Giorgos Christodoulou and Alkmini Sgouritsa},
  doi          = {10.1137/17M1154849},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1476-1523},
  shortjournal = {SIAM J. Comput.},
  title        = {An improved upper bound for the universal TSP on the grid},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameterized inapproximability of the minimum distance
problem over all fields and the shortest vector problem in all <span
class="math inline"><em>ℓ</em><sub><em>p</em></sub></span> norms.
<em>SICOMP</em>, <em>53</em>(5), 1439–1475. (<a
href="https://doi.org/10.1137/23M1573021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove that the minimum distance problem on linear codes over any fixed finite field and parameterized by the input distance bound is -hard to approximate within any constant factor. We also prove analogous results for the parameterized shortest vector problem on integer lattices. Specifically, we prove that the in the norm is -hard to approximate within any constant factor for any fixed and -hard to approximate within a factor approaching 2 for . (We show hardness under randomized reductions in each case.) These results answer the main questions left open (and explicitly posed) by Bhattacharyya et al. [J. ACM, 68 (2021), 16] on the complexity of the parameterized and . For the , they established similar hardness for binary linear codes and left the case of general fields open. For the in norms with , they showed inapproximability within some constant factor (depending on ) and left open showing such hardness for arbitrary constant factors. They also left open showing -hardness even of the exact SVP in the norm.},
  archive      = {J_SICOMP},
  author       = {Huck Bennett and Mahdi Cheraghchi and Venkatesan Guruswami and João Ribeiro},
  doi          = {10.1137/23M1573021},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1439-1475},
  shortjournal = {SIAM J. Comput.},
  title        = {Parameterized inapproximability of the minimum distance problem over all fields and the shortest vector problem in all \({\ell_{{p}}}\) norms},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counting subgraphs in somewhere dense graphs.
<em>SICOMP</em>, <em>53</em>(5), 1409–1438. (<a
href="https://doi.org/10.1137/22M1535668">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problems of counting copies and induced copies of a small pattern graph in a large host graph . Recent work fully classified the complexity of those problems according to structural restrictions on the patterns . In this work, we address the more challenging task of analyzing the complexity for restricted patterns and restricted hosts. Specifically, we ask which families of allowed patterns and hosts imply fixed-parameter tractability, i.e., the existence of an algorithm running in time for some computable function . Our main results present exhaustive and explicit complexity classifications for families that satisfy natural closure properties. Among others, we identify the problems of counting small matchings and independent sets in subgraph-closed graph classes as our central objects of study and establish the following crisp dichotomies as consequences of the exponential time hypothesis: (1) Counting -matchings in a graph is fixed-parameter tractable if and only if is nowhere dense. (2) Counting -independent sets in a graph is fixed-parameter tractable if and only if is nowhere dense. Moreover, we obtain almost tight conditional lower bounds if is somewhere dense, i.e., not nowhere dense. These base cases of our classifications subsume a wide variety of previous results on the matching and independent set problem, such as counting -matchings in bipartite graphs (Curticapean, Marx; FOCS 14), in -colorable graphs (Roth, Wellnitz; SODA 20), and in degenerate graphs (Bressan, Roth; FOCS 21), as well as counting -independent sets in bipartite graphs (Curticapean et al.; Algorithmica 19). At the same time, our proofs are much simpler: using structural characterizations of somewhere dense graphs, we show that a colorful version of a recent breakthrough technique for analyzing pattern counting problems (Curticapean, Dell, Marx; STOC 17) applies to any subgraph-closed somewhere dense class of graphs, yielding a unified view of our current understanding of the complexity of subgraph counting.},
  archive      = {J_SICOMP},
  author       = {Marco Bressan and Leslie Ann Goldberg and Kitty Meeks and Marc Roth},
  doi          = {10.1137/22M1535668},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1409-1438},
  shortjournal = {SIAM J. Comput.},
  title        = {Counting subgraphs in somewhere dense graphs},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complete characterization of fairness in secure two-party
computation of boolean functions. <em>SICOMP</em>, <em>53</em>(5),
1381–1408. (<a href="https://doi.org/10.1137/18M1232656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Fairness is a desirable property in secure computation; informally it means that if one party gets the output of the function, then all parties get the output. Alas, an implication of Cleve’s result [18th ACM Symposium on the Theory of Computing, 1986] is that when there is no honest majority, in particular in the important case of the two-party setting, there exist functions that cannot be computed with fairness. In a surprising result, Gordon et al. [18th ACM Symposium on the Theory of Computing, 2008; J. ACM, 58 (2011), 24] showed that some interesting functions can be computed with fairness in the two-party setting and reopened the question of understanding which Boolean functions can be computed with fairness, and which cannot. Our main result in this work is a complete characterization of the (symmetric) Boolean functions that can be computed with fairness in the two-party setting; this settles an open problem of Gordon et al. The statement of the characterization is quite simple: A function can be computed with fairness if and only if the all-one vector or the all-zero vector are in the affine span of either the rows or the columns of the matrix describing the function. This is true for both deterministic and randomized functions. To prove the possibility result, we modify the protocol of Gordon et al.; the resulting protocol computes with full security (and in particular with fairness) all functions that are computable with fairness. Complementing this result, we also show that any function that does not satisfy the aforementioned condition can be reduced to a fair sampling protocol, which, by Agrawal and Prabhakaran [Advances in Cryptology – CRYPTO 2013, 2013], cannot be computed with fairness.},
  archive      = {J_SICOMP},
  author       = {Gilad Asharov and Amos Beimel and Nikolaos Makriyannis and Eran Omri},
  doi          = {10.1137/18M1232656},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1381-1408},
  shortjournal = {SIAM J. Comput.},
  title        = {Complete characterization of fairness in secure two-party computation of boolean functions},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal resizable arrays. <em>SICOMP</em>, <em>53</em>(5),
1354–1380. (<a href="https://doi.org/10.1137/23M1575792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A resizable array is an array that can grow and shrink by the addition or removal of items from its end, or both its ends, while still supporting constant-time access to each item stored in the array given its index. Since the size of an array, i.e., the number of items in it, varies over time, space-efficient maintenance of a resizable array requires dynamic memory management. A standard doubling technique allows the maintenance of an array of size using only space, with amortized time, or even worst-case time, per operation. Sitarski, and (apparently independently) Brodnik, Carlsson, Demaine, Munro, and Sedgewick describe much better solutions that maintain a resizable array of size using only space, still with time per operation. Brodnik et al. give a simple proof that this is best possible. We distinguish between the space needed for storing a resizable array, and accessing its items, and the temporary space that may be needed while growing or shrinking the array. For every integer , we show that space is sufficient for storing and accessing an array of size , if space can be used briefly during grow and shrink operations. Accessing an item by index takes worst-case time, while grow and shrink operations take amortized time. Using an exact analysis of a growth game, we show that for any data structure from a wide class of data structures that uses only space to store the array, the amortized cost of grow is , even if only grow and access operations are allowed. The time for grow and shrink operations cannot be made worst-case unless .},
  archive      = {J_SICOMP},
  author       = {Robert E. Tarjan and Uri Zwick},
  doi          = {10.1137/23M1575792},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1354-1380},
  shortjournal = {SIAM J. Comput.},
  title        = {Optimal resizable arrays},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Complexity classification transfer for CSPs via algebraic
products. <em>SICOMP</em>, <em>53</em>(5), 1293–1353. (<a
href="https://doi.org/10.1137/22M1534304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the complexity of infinite-domain constraint satisfaction problems (CSPs): our basic setting is that a complexity classification for the CSPs of first-order expansions of a structure can be transferred to a classification of the CSPs of first-order expansions of another structure . We exploit a product of structures (the algebraic product) that corresponds to the product of the respective polymorphism clones and present a complete complexity classification of the CSPs for first-order expansions of the -fold algebraic power of . This is proved by various algebraic and logical methods in combination with knowledge of the polymorphisms of the tractable first-order expansions of and explicit descriptions of the expressible relations in terms of syntactically restricted first-order formulas. By combining our classification result with general classification transfer techniques, we obtain surprisingly strong new classification results for highly relevant formalisms such as Allen’s Interval Algebra, the -dimensional Block Algebra, and the Cardinal Direction Calculus, even if higher-arity relations are allowed. Our results confirm the infinite-domain tractability conjecture for classes of structures that have been difficult to analyze with older methods. For the special case of structures with binary signatures, the results can be substantially strengthened and tightly connected to Ord-Horn formulas; this solves several longstanding open problems from the artificial intelligence (AI) literature.},
  archive      = {J_SICOMP},
  author       = {Manuel Bodirsky and Peter Jonsson and Barnaby Martin and Antoine Mottet and Žaneta Semanišinová},
  doi          = {10.1137/22M1534304},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1293-1353},
  shortjournal = {SIAM J. Comput.},
  title        = {Complexity classification transfer for CSPs via algebraic products},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Traversing combinatorial 0/1-polytopes via optimization.
<em>SICOMP</em>, <em>53</em>(5), 1257–1292. (<a
href="https://doi.org/10.1137/23M1612019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we present a new framework that exploits combinatorial optimization for efficiently generating a large variety of combinatorial objects based on graphs, matroids, posets, and polytopes. Our method is based on a simple and versatile algorithm for computing a Hamilton path on the skeleton of a 0/1-polytope , where . The algorithm uses as a black box any algorithm that solves a variant of the classical linear optimization problem , and the resulting delay, i.e., the running time per visited vertex on the Hamilton path, is larger than the running time of the optimization algorithm only by a factor of . When encodes a particular class of combinatorial objects, then traversing the skeleton of the polytope along a Hamilton path corresponds to listing the combinatorial objects by local change operations; i.e., we obtain Gray code listings. As concrete results of our general framework, we obtain efficient algorithms for generating all (-optimal) bases and independent sets in a matroid; (-optimal) spanning trees, forests, matchings, maximum matchings, and -optimal matchings in a graph; vertex covers, minimum vertex covers, -optimal vertex covers, stable sets, maximum stable sets, and -optimal stable sets in a bipartite graph; as well as antichains, maximum antichains, -optimal antichains, and -optimal ideals of a poset. Specifically, the delay and space required by these algorithms are polynomial in the size of the matroid ground set, graph, or poset, respectively. Furthermore, all of these listings correspond to Hamilton paths on the corresponding combinatorial polytopes, namely the base polytope, matching polytope, vertex cover polytope, stable set polytope, chain polytope, and order polytope, respectively. As another corollary from our framework, we obtain an delay algorithm for the vertex enumeration problem on 0/1-polytopes , where and , and is the time needed to solve the linear program . This improves upon the 25-year-old delay algorithm due to Bussieck and Lübbecke.},
  archive      = {J_SICOMP},
  author       = {Arturo Merino and Torsten Mütze},
  doi          = {10.1137/23M1612019},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1257-1292},
  shortjournal = {SIAM J. Comput.},
  title        = {Traversing combinatorial 0/1-polytopes via optimization},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online primal dual meets online matching with stochastic
rewards: Configuration LP to the rescue. <em>SICOMP</em>,
<em>53</em>(5), 1217–1256. (<a
href="https://doi.org/10.1137/21M1454705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Mehta and Panigrahi (FOCS 2012, IEEE, Piscataway, NJ, 2012, pp. 728–737) introduce the problem of online matching with stochastic rewards, where edges are associated with success probabilities and a match succeeds with the probability of the corresponding edge. It is one of the few online matching problems that have defied the randomized online primal dual framework by Devanur, Jain, and Kleinberg (SODA 2013, SIAM, Philadelphia, 2013, pp. 101–107) thus far. This paper unlocks the power of randomized online primal dual in online matching with stochastic rewards by employing the configuration linear program rather than the standard matching linear program used in previous works. Our main result is a 0.572 competitive algorithm for the case of vanishing and unequal probabilities, improving the best previous bound of 0.534 by Mehta, Waggoner, and Zadimoghaddam (SODA 2015, SIAM, Philadelphia, 2015, pp. 1388–1404) and, in fact, is even better than the best previous bound of 0.567 by Mehta and Panigrahi (FOCS 2012, IEEE, Piscataway, NJ, 2012, pp. 728–737) for the more restricted case of vanishing and equal probabilities. For vanishing and equal probabilities, we get a better competitive ratio of 0.576. Our results further generalize to the vertex-weighted case due to the intrinsic robustness of the randomized online primal dual analysis.},
  archive      = {J_SICOMP},
  author       = {Zhiyi Huang and Qiankun Zhang},
  doi          = {10.1137/21M1454705},
  journal      = {SIAM Journal on Computing},
  month        = {10},
  number       = {5},
  pages        = {1217-1256},
  shortjournal = {SIAM J. Comput.},
  title        = {Online primal dual meets online matching with stochastic rewards: Configuration LP to the rescue},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). PTAS for minimum cost MultiCovering with disks.
<em>SICOMP</em>, <em>53</em>(4), 1181–1215. (<a
href="https://doi.org/10.1137/22M1523352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we study the following Minimum Cost Multicovering (MCMC) problem: Given a set of client points and a set of server points in a fixed dimensional space, determine a set of disks centered at these server points so that each client point is covered by at least disks and the total cost of these disks is minimized, where is a function that maps every client point to some nonnegative integer no more than and the cost of each disk is measured by the th power of its radius for some constant . MCMC is a fundamental optimization problem with applications in many areas such as wireless/sensor networking. Despite extensive research on this problem for about two decades, only constant approximations were known for general . It has been a long standing open problem to determine whether a PTAS is possible. In this paper, we give an affirmative answer to this question by presenting the first PTAS for it. Our approach is based on a number of novel techniques, such as balanced recursive realization and bubble charging, and new counterintuitive insights to the problem. Particularly, we approximate each disk with a set of sub-boxes and optimize them at the subdisk level. This allows us to first compute an approximate disk cover through dynamic programming, and then obtain the desired disk cover through a balanced recursive realization procedure.},
  archive      = {J_SICOMP},
  author       = {Ziyun Huang and Qilong Feng and Jianxin Wang and Jinhui Xu},
  doi          = {10.1137/22M1523352},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1181-1215},
  shortjournal = {SIAM J. Comput.},
  title        = {PTAS for minimum cost MultiCovering with disks},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate gomory–hu tree is faster than <span
class="math inline"><strong>n</strong> <strong>−</strong> <strong>1</strong></span>
maximum flows. <em>SICOMP</em>, <em>53</em>(4), 1162–1180. (<a
href="https://doi.org/10.1137/21M1463379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Gomory–Hu tree or cut tree [R. E. Gomory and T. C. Hu, J. Soc. Indust. Appl. Math., 9 (1961), pp. 551–570] is a classic data structure for reporting -mincuts (and by duality, the values of -maxflows) for all-pairs of vertices and in an undirected graph. Gomory and Hu showed that it can be computed using exact maxflow computations. Surprisingly, this remains the best algorithm for Gomory–Hu trees more than 50 years later, even for approximate mincuts. In this paper, we break this longstanding barrier and give an algorithm for computing a -approximate Gomory–Hu tree using maxflow computations. Specifically, we obtain the running time bounds we describe below. We obtain a randomized (Monte Carlo) algorithm for undirected, weighted graphs that runs in time and returns a -approximate Gomory–Hu tree with high probability (w.h.p.). Previously, the best running time known was , which is obtained by running Gomory and Hu’s original algorithm on a cut sparsifier of the graph. Next, we obtain a randomized (Monte Carlo) algorithm for undirected, unweighted graphs that runs in time and returns a -approximate Gomory–Hu tree w.h.p. This improves on our first result for sparse graphs, namely . Previously, the best running time known for unweighted graphs was for an exact Gomory–Hu tree [A. Bhalgat et al., Proceedings of the 39th Annual ACM Symposium on Theory of Computing, San Diego, CA, 2007, pp. 605–614]; no better result is known if approximations are allowed. As a consequence of our Gomory–Hu tree algorithms, we also solve the -approximate all-pairs mincut (APMC) and single-source mincut (SSMC) problems in the same time bounds. (These problems are simpler in that the goal is to only return the -mincut values, and not the mincuts.) This improves on the recent algorithm for these problems in time due to Abboud, Krauthgamer, and Trabelsi [2020 IEEE 61st Annual Symposium on Foundations of Computer Science, IEEE Computer Society, 2020, pp. 105–118].},
  archive      = {J_SICOMP},
  author       = {Jason Li and Debmalya Panigrahi},
  doi          = {10.1137/21M1463379},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1162-1180},
  shortjournal = {SIAM J. Comput.},
  title        = {Approximate Gomory–Hu tree is faster than \(\boldsymbol{n}\,\boldsymbol{-\, 1}\) maximum flows},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Algorithms for subpath convex hull queries and ray-shooting
among segments. <em>SICOMP</em>, <em>53</em>(4), 1132–1161. (<a
href="https://doi.org/10.1137/21M145118X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this paper, we first consider the subpath convex hull query problem: Given a simple path of vertices, preprocess it so that the convex hull of any query subpath of can be quickly obtained. Previously, Guibas, Hershberger, and Snoeyink [Int. J. Comput. Geom. Appl., 1 (1991), pp. 1–22; first appeared in SODA 1990] proposed a data structure of space and query time; they also reduced the query time to by increasing the space to . We present an improved result that uses space while achieving query time. Like the previous work, our query algorithm returns a compact interval tree representing the convex hull so that standard binary-search-based queries on the hull can be performed in time each. The preprocessing time of our data structure is after the vertices of are sorted by -coordinate. As the subpath convex hull query problem has many applications, our new result leads to improvements for several other problems. In particular, with the help of the above result, along with other techniques, we present new algorithms for the ray-shooting problem among segments. Given a set of (possibly intersecting) line segments in the plane, preprocess it so that the first segment hit by a query ray can be quickly found. We give a data structure of space that can answer each query in time. If the segments are nonintersecting or if the segments are lines, then the space can be reduced to . As a by-product, given a set of (possibly intersecting) segments in the plane, we build a data structure of space that can determine whether a query line intersects a segment in time. The preprocessing time is for all four problems, which can be reduced to time by a randomized algorithm so that the query time is bounded by with high probability. All these are classical problems that have been studied extensively. Previously data structures of query time were known in the early 1990s (the notation suppresses a polylogarithmic factor); nearly no progress has been made for more than two decades. For all these problems, our new results provide improvements by reducing the space of the data structures by at least a logarithmic factor while the preprocessing and query times are the same as before or even better.},
  archive      = {J_SICOMP},
  author       = {Haitao Wang},
  doi          = {10.1137/21M145118X},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1132-1161},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithms for subpath convex hull queries and ray-shooting among segments},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast FPT-approximation of branchwidth. <em>SICOMP</em>,
<em>53</em>(4), 1085–1131. (<a
href="https://doi.org/10.1137/22M153937X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Branchwidth determines how graphs and, more generally, arbitrary connectivity (symmetric and submodular) functions can be decomposed into a tree-like structure by specific cuts. We develop a general framework for designing fixed-parameter tractable 2-approximation algorithms for branchwidth of connectivity functions. The first ingredient of our framework is combinatorial. We prove a structural theorem establishing that either a sequence of particular refinement operations can decrease the width of a branch decomposition or the width of the decomposition is already within a factor of 2 from the optimum. The second ingredient is an efficient implementation of the refinement operations for branch decompositions that support efficient dynamic programming. We present two concrete applications of our general framework. The first is an algorithm that, for a given -vertex graph and integer , in time either constructs a rank decomposition of of width at most or concludes that the rankwidth of is more than . It also yields a -approximation algorithm for cliquewidth within the same time complexity, which in turn improves to the running times of various algorithms on graphs of cliquewidth . Breaking the “cubic barrier” for rankwidth and cliquewidth was an open problem in the area. The second application is an algorithm that, for a given -vertex graph and integer , in time either constructs a branch decomposition of of width at most or concludes that the branchwidth of is more than . This improves over the 3-approximation that follows from the recent treewidth 2-approximation of Korhonen [FOCS 2021].},
  archive      = {J_SICOMP},
  author       = {Fedor V. Fomin and Tuukka Korhonen},
  doi          = {10.1137/22M153937X},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1085-1131},
  shortjournal = {SIAM J. Comput.},
  title        = {Fast FPT-approximation of branchwidth},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Revisionist simulations: A new approach to proving space
lower bounds. <em>SICOMP</em>, <em>53</em>(4), 1039–1084. (<a
href="https://doi.org/10.1137/20M1322923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Determining the number of registers required for solving obstruction-free (or randomized wait-free) -set agreement is an open problem that highlights important gaps in our understanding of the space complexity of synchronization. The best known upper bound on the number of registers needed to solve this problem among processes is registers. No general lower bound better than 2 was known. We prove that any obstruction-free protocol solving -set agreement among processes must use at least registers. In particular, we get a tight lower bound of exactly registers for solving obstruction-free and randomized wait-free consensus. Our main tool is a simulation that serves as a reduction from the impossibility of deterministic wait-free -set agreement. In particular, we show that if an obstruction-free protocol for -set agreement uses fewer registers, then it is possible for processes to simulate the protocol and deterministically solve -set agreement in a wait-free manner, which is impossible. An important aspect of the simulation is the ability of simulating processes to revise the past of simulated processes. We introduce an augmented snapshot object, which facilitates this. More generally, our simulation applies to the broad class of colorless tasks. We can use it to prove, for example, a lower bound on the number of registers needed to solve obstruction-free -approximate agreement, which matches the best known upper bound to within a factor of 2 when is sufficiently small. No general lower bound for this problem was known. Finally, we prove that any lower bound on the number of registers used by obstruction-free protocols applies to protocols that satisfy nondeterministic solo-termination. Hence, our lower bounds for obstruction-free protocols also hold for randomized wait-free protocols.},
  archive      = {J_SICOMP},
  author       = {Faith Ellen and Rati Gelashvili and Leqi Zhu},
  doi          = {10.1137/20M1322923},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1039-1084},
  shortjournal = {SIAM J. Comput.},
  title        = {Revisionist simulations: A new approach to proving space lower bounds},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Economical convex coverings and applications.
<em>SICOMP</em>, <em>53</em>(4), 1002–1038. (<a
href="https://doi.org/10.1137/23M1568351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Coverings of convex bodies have emerged as a central component in the design of efficient solutions to approximation problems involving convex bodies. Intuitively, given a convex body and , a covering is a collection of convex bodies whose union covers such that a constant factor expansion of each body lies within an expansion of . Coverings have been employed in many applications, such as approximations for diameter, width, and -kernels of point sets, approximate nearest neighbor searching, polytope approximations with low combinatorial complexity, and approximations to the closest vector problem (CVP). It is known how to construct coverings of size for general convex bodies in . In special cases, such as when the convex body is the unit ball, this bound has been improved to . This raises the question of whether such a bound generally holds. In this paper we answer the question in the affirmative. We demonstrate the power and versatility of our coverings by applying them to the problem of approximating a convex body by a polytope, where the error is measured through the Banach–Mazur metric. Given a well-centered convex body and an approximation parameter , we show that there exists a polytope consisting of vertices (facets) such that . This bound is optimal in the worst case up to factors of . (This bound has been established recently using different techniques, but our approach is arguably simpler and more elegant.) As an additional consequence, we obtain the fastest -approximate CVP algorithm that works in any norm, with a running time of up to polynomial factors in the input size, and we obtain the fastest -approximation algorithm for integer programming. We also present a framework for constructing coverings of optimal size for any convex body (up to factors of ).},
  archive      = {J_SICOMP},
  author       = {Sunil Arya and Guilherme D. da Fonseca and David M. Mount},
  doi          = {10.1137/23M1568351},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {1002-1038},
  shortjournal = {SIAM J. Comput.},
  title        = {Economical convex coverings and applications},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On the privacy of noisy stochastic gradient descent for
convex optimization. <em>SICOMP</em>, <em>53</em>(4), 969–1001. (<a
href="https://doi.org/10.1137/23M1556538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A central issue in machine learning is how to train models on sensitive user data. Industry has widely adopted a simple algorithm: Stochastic Gradient Descent (SGD) with noise (a.k.a. Stochastic Gradient Langevin Dynamics). However, foundational theoretical questions about this algorithm’s privacy loss remain open—even in the seemingly simple setting of smooth convex losses over a bounded domain. Our main result resolves these questions: for a large range of parameters, we characterize the differential privacy up to a constant factor. This result reveals that all previous analyses for this setting have the wrong qualitative behavior. Specifically, while previous privacy analyses increase ad infinitum in the number of iterations, we show that after a small burn-in period, running SGD longer leaks no further privacy. Our analysis departs from previous approaches based on fast mixing, instead using techniques based on optimal transport (namely, Privacy Amplification by Iteration) and the Sampled Gaussian Mechanism (namely, Privacy Amplification by Sampling). Our techniques readily extend to other settings, e.g., strongly convex losses, nonuniform stepsizes, arbitrary batch sizes, and random or cyclic choice of batches.},
  archive      = {J_SICOMP},
  author       = {Jason M. Altschuler and Jinho Bok and Kunal Talwar},
  doi          = {10.1137/23M1556538},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {969-1001},
  shortjournal = {SIAM J. Comput.},
  title        = {On the privacy of noisy stochastic gradient descent for convex optimization},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Two variable logic with ultimately periodic counting.
<em>SICOMP</em>, <em>53</em>(4), 884–968. (<a
href="https://doi.org/10.1137/22M1504792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the extension of with quantifiers that state that the number of elements where a formula holds should belong to a given ultimately periodic set. We show that both satisfiability and finite satisfiability of the logic are decidable. We also show that the spectrum of any sentence, i.e., the set of the sizes of its finite models, is definable in Presburger arithmetic. In the process we present several refinements to the “biregular graph method.” In this method, decidability issues concerning two-variable logics are reduced to questions about Presburger definability of integer vectors associated with partitioned graphs, where nodes in a partition satisfy certain constraints on their in- and out-degrees.},
  archive      = {J_SICOMP},
  author       = {Michael Benedikt and Egor V. Kostylev and Tony Tan},
  doi          = {10.1137/22M1504792},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {884-968},
  shortjournal = {SIAM J. Comput.},
  title        = {Two variable logic with ultimately periodic counting},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On testability of first-order properties in bounded-degree
graphs and connections to proximity-oblivious testing. <em>SICOMP</em>,
<em>53</em>(4), 825–883. (<a
href="https://doi.org/10.1137/23M1556253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study property testing of properties that are definable in first-order logic (FO) in the bounded-degree graph and relational structure models. We show that any FO property that is defined by a formula with quantifier prefix is testable (i.e., testable with constant query complexity), while there exists an FO property that is expressible by a formula with quantifier prefix that is not testable. In the dense graph model, a similar picture has long been known [N. Alon, E. Fischer, M. Krivelevich, and M. Szegedy, Combinatorica, 20 (2000), pp. 451–476] despite the very different nature of the two models. In particular, we obtain our lower bound by an FO formula that defines a class of bounded-degree expanders, based on zig-zag products of graphs. We expect this to be of independent interest. We then use our class of FO definable bounded-degree expanders to answer a long-standing open problem for proximity-oblivious testers (POTs). POTs are a class of particularly simple testing algorithms, where a basic test is performed a number of times that may depend on the proximity parameter, but the basic test itself is independent of the proximity parameter. In their seminal work, Goldreich and Ron [STOC 2009; SIAM J. Comput., 40 (2011), pp. 534–566] show that the graph properties that are constant-query proximity-oblivious testable in the bounded-degree model are precisely the properties that can be expressed as a generalized subgraph freeness (GSF) property that satisfies the non-propagation condition. It is left open whether the non-propagation condition is necessary. Indeed, calling properties expressible as a generalized subgraph freeness property GSF-local properties, they ask whether all GSF-local properties are non-propagating. We give a negative answer by showing that our FO definable property is GSF-local and propagating. Hence, in particular, our property does not admit a POT, despite being GSF-local. For this result we establish a new connection between FO properties and GSF-local properties via neighborhood profiles.},
  archive      = {J_SICOMP},
  author       = {Isolde Adler and Noleen Köhler and Pan Peng},
  doi          = {10.1137/23M1556253},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {825-883},
  shortjournal = {SIAM J. Comput.},
  title        = {On testability of first-order properties in bounded-degree graphs and connections to proximity-oblivious testing},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). How to trap a gradient flow. <em>SICOMP</em>,
<em>53</em>(4), 803–824. (<a
href="https://doi.org/10.1137/21M1397854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of finding an -approximate stationary point of a smooth function on a compact domain of . In contrast with dimension-free approaches such as gradient descent, we focus here on the case where is finite, and potentially small. This viewpoint was explored in 1993 by Vavasis, who proposed an algorithm which, for any fixed finite dimension , improves upon the oracle complexity of gradient descent. For example for , Vavasis’s approach obtains the complexity . Moreover, for he also proved a lower bound of for deterministic algorithms (we extend this result to randomized algorithms). Our main contribution is an algorithm, which we call gradient flow trapping (GFT), and the analysis of its oracle complexity. In dimension , GFT closes the gap with Vavasis’s lower bound (up to a logarithmic factor), as we show that it has complexity . In dimension , we show a complexity of , improving upon Vavasis’s . In higher dimensions, GFT has the remarkable property of being a logarithmic parallel depth strategy, in stark contrast with the polynomial depth of gradient descent or Vavasis’s algorithm. We augment this result with another algorithm, named cut and flow (CF), which improves upon Vavasis’s algorithm in any fixed dimension.},
  archive      = {J_SICOMP},
  author       = {Sébastien Bubeck and Dan Mikulincer},
  doi          = {10.1137/21M1397854},
  journal      = {SIAM Journal on Computing},
  month        = {8},
  number       = {4},
  pages        = {803-824},
  shortjournal = {SIAM J. Comput.},
  title        = {How to trap a gradient flow},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Proof complexity and the binary encoding of combinatorial
principles. <em>SICOMP</em>, <em>53</em>(3), 764–802. (<a
href="https://doi.org/10.1137/20M134784X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider proof complexity in light of the unusual binary encoding of certain combinatorial principles. We contrast this proof complexity with the normal unary encoding in several refutation systems, based on Resolution and Sherali–Adams. We first consider , which is an extension of Resolution working on -DNFs (Disjunctive Normal Form formulas). We prove an exponential lower bound of for the size of refutations of the binary version of the -Clique Principle in , where and is a doubly exponential function. Our result improves that of Lauria et al., who proved a similar lower bound for , i.e., Resolution. For the -Clique and other principles we study, we show how lower bounds in Resolution for the unary version follow from lower bounds in for the binary version, so we start a systematic study of the complexity of proofs in Resolution-based systems for families of contradictions given in the binary encoding. We go on to consider the binary version of the (weak) Pigeonhole Principle . We prove that for any , requires refutations of size in for . Our lower bound cannot be improved substantially with the same method since for we can prove there are size refutations of in . This is a consequence of the same upper bound for the unary weak Pigeonhole Principle of Buss and Pitassi. We contrast unary versus binary encoding in the Sherali–Adams (SA) refutation system where we prove lower bounds for both rank and size. For the unary encoding of the Pigeonhole Principle and the Ordering Principle, it is known that linear rank is required for refutations in SA, although both admit refutations of polynomial size. We prove that the binary encoding of the (weak) Pigeonhole Principle requires exponentially sized (in ) SA refutations, whereas the binary encoding of the Ordering Principle admits logarithmic rank, polynomially sized SA refutations. We continue by considering a natural refutation system we call “SA+Squares,” which is intermediate between SA and Lasserre (Sum-of-Squares). This has been studied under the name static- by Grigoriev et al. In this system, the unary encoding of the Linear Ordering Principle requires rank while the unary encoding of the Pigeonhole Principle becomes constant rank. Since Potechin has shown that the rank of in Lasserre is , we uncover an almost quadratic separation between SA+Squares and Lasserre in terms of rank. Grigoriev et al. noted that the unary Pigeonhole Principle has rank 2 in SA+Squares and therefore polynomial size. Since we show the same applies to the binary , we deduce an exponential separation for size between SA and SA+Squares.},
  archive      = {J_SICOMP},
  author       = {Stefan Dantchev and Nicola Galesi and Abdul Ghani and Barnaby Martin},
  doi          = {10.1137/20M134784X},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {764-802},
  shortjournal = {SIAM J. Comput.},
  title        = {Proof complexity and the binary encoding of combinatorial principles},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). AdWords in a panorama. <em>SICOMP</em>, <em>53</em>(3),
701–763. (<a href="https://doi.org/10.1137/22M1478896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Three decades ago, Karp, Vazirani, and Vazirani [Proceedings of the 22nd Annual ACM Symposium on Theory of Computing, 1990, pp. 352–358] defined the online matching problem and gave an optimal -competitive algorithm. Fifteen years later, Mehta et al. [J. ACM, 54 (2007), pp. 22:1–22:19] introduced the first generalization called AdWords driven by online advertising and obtained the optimal competitive ratio in the special case of small bids. It has been open ever since whether there is an algorithm for general bids better than the 0.5-competitive greedy algorithm. This paper presents a 0.5016-competitive algorithm for AdWords, answering this open question on the positive end. The algorithm builds on several ingredients, including a combination of the online primal dual framework and the configuration linear program of matching problems recently explored by Huang and Zhang [Proceedings of the 52nd ACM Symposium on Theory of Computing, 2020], a novel formulation of AdWords which we call the panorama view, and a generalization of the online correlated selection by Fahrbach et al. [Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science, 2020], which we call the panoramic online correlated selection.},
  archive      = {J_SICOMP},
  author       = {Zhiyi Huang and Qiankun Zhang and Yuhao Zhang},
  doi          = {10.1137/22M1478896},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {701-763},
  shortjournal = {SIAM J. Comput.},
  title        = {AdWords in a panorama},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Semialgebraic proofs, IPS lower bounds, and the <span
class="math inline"><strong>τ</strong></span>-conjecture: Can a natural
number be negative? <em>SICOMP</em>, <em>53</em>(3), 648–700. (<a
href="https://doi.org/10.1137/20M1374523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce the binary value principle, which is a simple subset-sum instance expressing that a natural number written in binary cannot be negative, relating it to central problems in proof and algebraic complexity. We prove conditional superpolynomial lower bounds on the Ideal Proof System (IPS) refutation size of this instance, based on a well-known hypothesis by Shub and Smale about the hardness of computing factorials, where IPS is the strong algebraic proof system introduced by Grochow and Pitassi [J. ACM, 65 (2018), 37]. Conversely, we show that short IPS refutations of this instance bridge the gap between sufficiently strong algebraic and semialgebraic proof systems. Our results extend to unrestricted IPS the paradigm introduced by Forbes, Shpilka, Tzameret, and Wigderson [Theory Comput., 17 (2021), pp. 1–88], whereby lower bounds against subsystems of IPS were obtained using restricted algebraic circuit lower bounds, and demonstrate that the binary value principle captures the advantage of semialgebraic over algebraic reasoning, for sufficiently strong systems. Specifically, we show the following. (1) Conditional IPS lower bounds: The Shub–Smale hypothesis [Duke Math. J., 81 (1995), pp. 47–54] implies a superpolynomial lower bound on the size of IPS refutations of the binary value principle over the rationals defined as the unsatisfiable linear equation for Boolean ’s. Further, the related and more widely known -conjecture [Duke Math. J., 81 (1995), pp. 47–54] implies a superpolynomial lower bound on the size of IPS refutations of a variant of the binary value principle over the ring of rational functions. No prior conditional lower bounds were known for IPS or apparently weaker propositional proof systems such as Frege systems (though our lower bounds do not translate to Frege lower bounds since the hard instances are not Boolean formulas). (2) Algebraic versus semialgebraic proofs: Admitting short refutations of the binary value principle is necessary for any algebraic proof system to fully simulate any known semialgebraic proof system, and for strong enough algebraic proof systems it is also sufficient. In particular, we introduce a very strong proof system that simulates all known semialgebraic proof systems (and most other known concrete propositional proof systems), under the name Cone Proof System (CPS), as a semialgebraic analogue of the IPS: CPS establishes the unsatisfiability of collections of polynomial equalities and inequalities over the reals, by representing sum-of-squares proofs (and extensions) as algebraic circuits. We prove that IPS polynomially simulates CPS iff IPS admits polynomial-size refutations of the binary value principle (for the language of systems of equations that have no 0/1-solutions), over both and .},
  archive      = {J_SICOMP},
  author       = {Yaroslav Alekseev and Dima Grigoriev and Edward A. Hirsch and Iddo Tzameret},
  doi          = {10.1137/20M1374523},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {648-700},
  shortjournal = {SIAM J. Comput.},
  title        = {Semialgebraic proofs, IPS lower bounds, and the \(\boldsymbol{\tau}\)-conjecture: Can a natural number be negative?},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Induced subgraphs of bounded treewidth and the container
method. <em>SICOMP</em>, <em>53</em>(3), 624–647. (<a
href="https://doi.org/10.1137/20M1383732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A hole in a graph is an induced cycle of length at least 4. A hole is long if its length is at least 5. By , we denote a path on vertices. In this paper, we give polynomial-time algorithms for the following problems: the maximum weight independent set problem in long-hole–free graphs and the feedback vertex set problem in -free graphs. Each of the above results resolves a corresponding long-standing open problem. An extended is a five-vertex hole with an additional vertex adjacent to one or two consecutive vertices of the hole. Let be the class of graphs excluding an extended and holes of length at least 6 as induced subgraphs; contains all long-hole–free graphs and all -free graphs. We show that, given an -vertex graph with vertex weights and an integer , one can, in time, find a maximum-weight induced subgraph of of treewidth less than . This implies both aforementioned results. To achieve this goal, we extend the framework of potential maximal cliques (PMCs) to containers. Developed by Bouchitté and Todinca [SIAM J. Comput., 31 (2001), pp. 212–232] and extended by Fomin, Todinca, and Villanger [SIAM J. Comput., 44 (2015), pp. 54–87], this framework allows us to solve a wide variety of tasks, including finding a maximum-weight induced subgraph of treewidth less than for fixed , in time polynomial in the size of the graph and the number of potential maximal cliques. Further developments, tailored to solve the maximum weight independent set problem within this framework (e.g., for -free [Lokshtanov, Vatshelle, and Villanger, SODA 2014, pp. 570–581] or -free graphs [Grzesik, Klimošová, Pilipczuk, and Pilipczuk, ACM Trans. Algorithms, 18 (2022), pp. 4:1–4:57]), enumerate only a specifically chosen subset of all PMCs of a graph. In all aforementioned works, the final step is an involved dynamic programming algorithm whose state space is based on the considered list of PMCs. Here, we modify the dynamic programming algorithm and show that it is sufficient to consider only a container for each PMC: a superset of the maximal clique that intersects the sought solution only in the vertices of the PMC. This strengthening of the framework not only allows us to obtain our main result but also leads to significant simplifications of the reasoning in previous papers.},
  archive      = {J_SICOMP},
  author       = {Tara Abrishami and Maria Chudnovsky and Marcin Pilipczuk and Paweł Rzążewski and Paul Seymour},
  doi          = {10.1137/20M1383732},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {624-647},
  shortjournal = {SIAM J. Comput.},
  title        = {Induced subgraphs of bounded treewidth and the container method},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Cluster before you hallucinate: Node-capacitated network
design and energy efficient routing. <em>SICOMP</em>, <em>53</em>(3),
588–623. (<a href="https://doi.org/10.1137/20M1360645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the following node-capacitated network design problem. The input is an undirected graph, a set of demands, uniform node capacity, and arbitrary node costs. The goal is to find a minimum node-cost subgraph that supports all demands concurrently subject to the node capacities. We consider both single- and multicommodity demands and provide the first polylogarithmic approximation guarantees. For single-commodity demands (i.e., all request pairs have the same sink node), we obtain an approximation to the cost with an factor violation in node capacities. For multicommodity demands, we obtain an approximation to the cost with an factor violation in node capacities. We use a variety of techniques, including single-sink confluent flows, low-load set cover, random sampling, and cut-sparsification. We also develop new techniques for clustering multicommodity demands into (nearly) node-disjoint clusters, which may be of independent interest. Moreover, this network design problem has applications to energy-efficient virtual circuit routing. In this setting, there is a network of routers that are speed scalable and that may be shut down when idle. We assume the standard model for power: the power consumed by a router with load (speed) is , where is the static power and the exponent . We obtain the first polylogarithmic approximation algorithms for this problem when speed-scaling occurs on nodes of a network.},
  archive      = {J_SICOMP},
  author       = {Ravishankar Krishnaswamy and Viswanath Nagarajan and Kirk Pruhs and Clifford Stein},
  doi          = {10.1137/20M1360645},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {588-623},
  shortjournal = {SIAM J. Comput.},
  title        = {Cluster before you hallucinate: Node-capacitated network design and energy efficient routing},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Further collapses in <span class="math inline">TFNP</span>.
<em>SICOMP</em>, <em>53</em>(3), 573–587. (<a
href="https://doi.org/10.1137/22M1498346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We show . Here the class consists of all total search problems that reduce to the End-of-Potential-Line problem, which was introduced in the works by Hubáček and Yogev (SICOMP 2020) and Fearnley et al. (JCSS 2020). In particular, our result yields a new simpler proof of the breakthrough collapse by Fearnley et al. (STOC 2021). We also prove a companion result , where is the class associated with the Sink-of-Potential-Line problem.},
  archive      = {J_SICOMP},
  author       = {Mika Göös and Alexandros Hollender and Siddhartha Jain and Gilbert Maystre and William Pires and Robert Robere and Ran Tao},
  doi          = {10.1137/22M1498346},
  journal      = {SIAM Journal on Computing},
  month        = {6},
  number       = {3},
  pages        = {573-587},
  shortjournal = {SIAM J. Comput.},
  title        = {Further collapses in \(\boldsymbol{\mathsf{TFNP}}\)},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sublinear time approximation of the cost of a metric <span
class="math inline"><em>k</em></span>-nearest neighbor graph.
<em>SICOMP</em>, <em>53</em>(2), 524–571. (<a
href="https://doi.org/10.1137/22M1544105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Let be an -point metric space. We assume that is given in the distance oracle model, that is, and for every pair of points from we can query their distance in constant time. A -nearest neighbor (-NN) graph for is a directed graph that has an edge to each of ’s nearest neighbors. We use to denote the sum of edge weights of . In this paper, we study the problem of approximating in sublinear time when we are given oracle access to the metric space that defines . Our goal is to develop an algorithm that solves this problem faster than the time required to compute . We first present an algorithm that in time with probability at least approximates to within a factor of . Next, we present a more elaborate sublinear algorithm that in time computes an estimate of that satisfies with probability at least , where denotes the cost of the minimum spanning tree of . Further, we complement these results with near matching lower bounds. We show that any algorithm that for a given metric space of size , with probability at least , estimates to within a factor requires time. Similarly, any algorithm that with probability at least estimates to within an additive error term requires time.},
  archive      = {J_SICOMP},
  author       = {Artur Czumaj and Christian Sohler},
  doi          = {10.1137/22M1544105},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {524-571},
  shortjournal = {SIAM J. Comput.},
  title        = {Sublinear time approximation of the cost of a metric \({k}\)-nearest neighbor graph},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rigid matrices from rectangular PCPs. <em>SICOMP</em>,
<em>53</em>(2), 480–523. (<a
href="https://doi.org/10.1137/22M1495597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a variant of Probabilistically Checkable Proofs (PCPs) that we refer to as rectangular PCPs, wherein proofs are thought of as square matrices, and the random coins used by the verifier can be partitioned into two disjoint sets, one determining the row of each query and the other determining the column. We construct PCPs that are efficient, short, smooth, and (almost) rectangular. As a key application, we show that proofs for hard languages in NTIME, when viewed as matrices, are rigid infinitely often. This strengthens and simplifies a recent result of Alman and Chen [FOCS, 2019] constructing explicit rigid matrices in FNP. Namely, we prove the following theorem: There is a constant such that there is an FNP-machine that, for infinitely many , on input outputs matrices with entries in that are -far (in Hamming distance) from matrices of rank at most . Our construction of rectangular PCPs starts with an analysis of how randomness yields queries in the Reed–Muller-based outer PCP of Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan [SIAM J. Comput., 36 (2006), pp. 889–974; CCC, 2005]. We then show how to preserve rectangularity under PCP composition and a smoothness-inducing transformation. This warrants refined and stronger notions of rectangularity, which we prove for the outer PCP and its transforms.},
  archive      = {J_SICOMP},
  author       = {Amey Bhangale and Prahladh Harsha and Orr Paradise and Avishay Tal},
  doi          = {10.1137/22M1495597},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {480-523},
  shortjournal = {SIAM J. Comput.},
  title        = {Rigid matrices from rectangular PCPs},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Parameterized complexity of untangling knots.
<em>SICOMP</em>, <em>53</em>(2), 431–479. (<a
href="https://doi.org/10.1137/22M1501969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Deciding whether a diagram of a knot can be untangled with a given number of moves (as a part of the input) is known to be NP-complete. In this paper we determine the parameterized complexity of this problem with respect to a natural parameter called defect. Roughly speaking, it measures the efficiency of the moves used in the shortest untangling sequence of Reidemeister moves. We show that in a shortest untangling sequence the moves, that is, the moves removing two adjacent crossings, can be essentially performed greedily. Using that, we show that this problem belongs to W[P] when parameterized by the defect. We also show that this problem is W[P]-hard by a reduction from Minimum axiom set.},
  archive      = {J_SICOMP},
  author       = {Clément Legrand-Duchesne and Ashutosh Rai and Martin Tancer},
  doi          = {10.1137/22M1501969},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {431-479},
  shortjournal = {SIAM J. Comput.},
  title        = {Parameterized complexity of untangling knots},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improved list-decodability and list-recoverability of
reed–solomon codes via tree packings. <em>SICOMP</em>, <em>53</em>(2),
389–430. (<a href="https://doi.org/10.1137/21M1463707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper shows that there exist Reed–Solomon (RS) codes, over exponentially large finite fields in the code length, that are combinatorially list-decodable well beyond the Johnson radius, in fact almost achieving the list-decoding capacity. In particular, we show that for any there exist RS codes with rate that are list-decodable from radius of . We generalize this result to list-recovery, showing that there exist -list-recoverable RS codes with rate . Along the way we use our techniques to give a new proof of a result of Blackburn on optimal linear perfect hash matrices, and strengthen it to obtain a construction of strongly perfect hash matrices. To derive the results in this paper we show a surprising connection of the above problems to graph theory, and in particular to the tree packing theorem of Nash-Williams and Tutte. We also state a new conjecture that generalizes the tree packing theorem to hypergraphs and show that if this conjecture holds, then there would exist RS codes that are optimally (nonasymptotically) list-decodable.},
  archive      = {J_SICOMP},
  author       = {Zeyu Guo and Ray Li and Chong Shangguan and Itzhak Tamo and Mary Wootters},
  doi          = {10.1137/21M1463707},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {389-430},
  shortjournal = {SIAM J. Comput.},
  title        = {Improved list-decodability and list-recoverability of Reed–Solomon codes via tree packings},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharp thresholds in random simple temporal graphs.
<em>SICOMP</em>, <em>53</em>(2), 346–388. (<a
href="https://doi.org/10.1137/22M1511916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A graph whose edges only appear at certain points in time is called a temporal graph (among other names). Such a graph is temporally connected if each ordered pair of vertices is connected by a path which traverses edges in chronological order (i.e., a temporal path). In this paper, we consider a simple model of random temporal graph, obtained from an Erdős–Rényi random graph, by considering a random permutation of the edges and interpreting the ranks in as presence times. We give a thorough study of the temporal connectivity of such graphs and derive implications for the existence of several kinds of sparse spanners. It turns out that temporal reachability in this model exhibits a surprisingly regular sequence of thresholds. In particular, we show that at , any fixed pair of vertices can asymptotically almost surely (a.a.s.) reach each other; at , at least one vertex (and, in fact, any fixed vertex) can a.a.s. reach all others; and at , all the vertices can a.a.s. reach each other; i.e., the graph is temporally connected. Furthermore, the graph admits a temporal spanner of size as soon as it becomes temporally connected, which is nearly optimal, as is a lower bound. This result is quite significant because temporal graphs do not admit spanners of size in general [Kempe, Kleinberg, and Kumar, J. Comput. System Sci., 64 (2002), pp. 820–842]. In fact, they do not even always admit spanners of size [Axiotis and Fotakis, On the size and the approximability of minimum temporally connected subgraphs, 2016, pp. 149:1–149:14]. Thus, our result implies that the obstructions found in these works—and more generally any non-negligible obstruction—are statistically insignificant: nearly optimal spanners always exist in random temporal graphs. All the above thresholds are sharp. Carrying the study of temporal spanners a step further, we show that pivotal spanners—i.e., spanners of size composed of two spanning trees glued at a single vertex (one descending in time, the other ascending subsequently)—exist a.a.s. at , this threshold being also sharp. Finally, we show that optimal spanners (of size ) also exist a.a.s. at . Whether this value is a sharp threshold is open; we conjecture that it is. For completeness, we compare the above results to existing results in related areas, including edge-ordered graphs, gossip theory, and population protocols, showing that our results can be interpreted in these settings as well and that in some cases they improve known results therein. Finally, we discuss an intriguing connection between our results and Janson’s celebrated results on percolation in weighted graphs.},
  archive      = {J_SICOMP},
  author       = {Arnaud Casteigts and Michael Raskin and Malte Renken and Viktor Zamaraev},
  doi          = {10.1137/22M1511916},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {346-388},
  shortjournal = {SIAM J. Comput.},
  title        = {Sharp thresholds in random simple temporal graphs},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast metric embedding into the hamming cube.
<em>SICOMP</em>, <em>53</em>(2), 315–345. (<a
href="https://doi.org/10.1137/22M1520220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of embedding a subset of into a low-dimensional Hamming cube in an almost isometric way. We construct a simple, data-oblivious, and computationally efficient map that achieves this task with high probability; we first apply a specific structured random matrix, which we call the double circulant matrix; using that a matrix requires linear storage and matrix-vector multiplication that can be performed in near-linear time. We then binarize each vector by comparing each of its entries to a random threshold, selected uniformly at random from a well-chosen interval. We estimate the number of bits required for this encoding scheme in terms of two natural geometric complexity parameters of the set: its Euclidean covering numbers and its localized Gaussian complexity. The estimate we derive turns out to be the best that one can hope for, up to logarithmic terms. The key to the proof is a phenomenon of independent interest: we show that the double circulant matrix mimics the behavior of the Gaussian matrix in two important ways. First, it maps an arbitrary set in into a set of well-spread vectors. Second, it yields a fast near-isometric embedding of any finite subset of into . This embedding achieves the same dimension reduction as the Gaussian matrix in near-linear time, under an optimal condition—up to logarithmic factors—on the number of points to be embedded. This improves a well-known construction due to Ailon and Chazelle.},
  archive      = {J_SICOMP},
  author       = {Sjoerd Dirksen and Shahar Mendelson and Alexander Stollenwerk},
  doi          = {10.1137/22M1520220},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {315-345},
  shortjournal = {SIAM J. Comput.},
  title        = {Fast metric embedding into the hamming cube},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fixed-parameter algorithms for the kneser and schrijver
problems. <em>SICOMP</em>, <em>53</em>(2), 287–314. (<a
href="https://doi.org/10.1137/23M1557076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Kneser graph is defined for integers and with as the graph whose vertices are all the -subsets of where two such sets are adjacent if they are disjoint. The Schrijver graph is defined as the subgraph of induced by the collection of all -subsets of that do not include two consecutive elements modulo . It is known that the chromatic number of both and is . In the computational Kneser and Schrijver problems, we are given access to a coloring with colors of the vertices of and , respectively, and the goal is to find a monochromatic edge. We prove that the problems admit randomized algorithms with running time , hence they are fixed-parameter tractable with respect to the parameter . The analysis involves structural results on intersecting families and on induced subgraphs of Kneser and Schrijver graphs. We also study the Agreeable-Set problem of assigning a small subset of a set of items to a group of agents, so that all agents value the subset at least as much as its complement. As an application of our algorithm for the Kneser problem, we obtain a randomized polynomial-time algorithm for the Agreeable-Set problem for instances with . We further show that the Agreeable-Set problem is at least as hard as a variant of the Kneser problem with extended access to the input coloring.},
  archive      = {J_SICOMP},
  author       = {Ishay Haviv},
  doi          = {10.1137/23M1557076},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {287-314},
  shortjournal = {SIAM J. Comput.},
  title        = {Fixed-parameter algorithms for the kneser and schrijver problems},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Decentralized low-stretch trees via low diameter graph
decompositions. <em>SICOMP</em>, <em>53</em>(2), 247–286. (<a
href="https://doi.org/10.1137/22M1489034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the problem of approximating the distances in an undirected weighted graph by the distances in trees based on the notion of stretch. Focusing on decentralized models of computation such as the , and semi-streaming models, our main results are as follows: (1) We develop a simple randomized algorithm that constructs a spanning tree such that the expected stretch of every edge is , where is the number of nodes in . If is unweighted, then this algorithm can be implemented to run in rounds in the model, where is the hop-diameter of ; thus our algorithm is asymptotically optimal in this case. In the weighted case, the run-time of the algorithm matches the currently best known bound for exact single source shortest path (SSSP) computations, which despite recent progress is still separated from the lower bound of by polynomial factors. A naive attempt to replace exact SSSP computations with approximate ones in order to improve the complexity in the weighted case encounters a fundamental challenge, as the underlying decomposition technique fails to work under distance approximation. (2) We overcome this obstacle by developing a technique termed blurry ball growing. This technique, in combination with a clever algorithmic idea of Miller, Peng, and Xu (SPAA 2013), allows us to obtain low diameter graph decompositions with small edge cutting probabilities based solely on approximate SSSP computations. (3) Using these decompositions, we in turn obtain metric tree embedding algorithms in the vein of the celebrated work of Bartal (FOCS 1996), whose computational complexity is optimal up to polylogarithmic factors not only in the model but also in the and semi-streaming models. Our embeddings have the additional useful property that the tree can be mapped back to the original graph such that each edge is “used” only logarithmically many times. This property is of interest for capacitated problems and for simulating algorithms on the tree into which the graph is embedded.},
  archive      = {J_SICOMP},
  author       = {Ruben Becker and Yuval Emek and Mohsen Ghaffari and Christoph Lenzen},
  doi          = {10.1137/22M1489034},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {247-286},
  shortjournal = {SIAM J. Comput.},
  title        = {Decentralized low-stretch trees via low diameter graph decompositions},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reachability preservers: New extremal bounds and
approximation algorithms. <em>SICOMP</em>, <em>53</em>(2), 221–246. (<a
href="https://doi.org/10.1137/21M1442176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We define and study reachability preservers, a graph-theoretic primitive that has been implicit in prior work on network design. Given a directed graph and a set of demand pairs , a reachability preserver is a sparse subgraph that preserves reachability between all demand pairs Our first contribution is a series of extremal bounds on the size of reachability preservers. Our main result states that, for an -node graph and demand pairs of the form for a small node subset , there is always a reachability preserver on edges. We additionally give a lower bound construction demonstrating that this upper bound characterizes the settings in which size reachability preservers are generally possible, in a large range of parameters. The second contribution of this paper is a new connection between extremal graph sparsification results and classical Steiner Network Design problems. Surprisingly, prior to this work, the osmosis of techniques between these two fields had been superficial. This allows us to improve the state of the art approximation algorithms for the most basic Steiner-type problem in directed graphs from the of Chlamtáč et al. [Approximating spanners and directed steiner forest: Upper and lower bounds, in Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, Philadelphia, 2017, pp. 534–553] to .},
  archive      = {J_SICOMP},
  author       = {Amir Abboud and Greg Bodwin},
  doi          = {10.1137/21M1442176},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {221-246},
  shortjournal = {SIAM J. Comput.},
  title        = {Reachability preservers: New extremal bounds and approximation algorithms},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Counting small induced subgraphs with hereditary properties.
<em>SICOMP</em>, <em>53</em>(2), 189–220. (<a
href="https://doi.org/10.1137/22M1512211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the computational complexity of the problem of counting -vertex induced subgraphs of a graph that satisfy a graph property . Our main result establishes an exhaustive and explicit classification for all hereditary properties, including tight conditional lower bounds under the Exponential Time Hypothesis (ETH): If a hereditary property is true for all graphs, or if it is true only for finitely many graphs, then is solvable in polynomial time. Otherwise, is -complete when parameterized by , and, assuming ETH, it cannot be solved in time for any function . This classification features a wide range of properties for which the corresponding detection problem (as classified by Khot and Raman [Theoret. Comput. Sci., 289 (2002), pp. 997–1008]) is tractable but counting is hard. Moreover, even for properties which are already intractable in their decision version, our results yield significantly stronger lower bounds for the counting problem. As an additional result, we also present an exhaustive and explicit parameterized complexity classification for all properties that are invariant under homomorphic equivalence. By covering one of the most natural and general notions of closure, namely, closure under vertex-deletion (hereditary), we generalize some of the earlier results on this problem. For instance, our results fully subsume and strengthen the existing classification of for monotone (subgraph-closed) properties due to Roth, Schmitt, and Wellnitz [SIAM J. Comput., (2022), pp. FOCS20-139–FOCS20-174].},
  archive      = {J_SICOMP},
  author       = {Jacob Focke and Marc Roth},
  doi          = {10.1137/22M1512211},
  journal      = {SIAM Journal on Computing},
  month        = {4},
  number       = {2},
  pages        = {189-220},
  shortjournal = {SIAM J. Comput.},
  title        = {Counting small induced subgraphs with hereditary properties},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024a). Four-coloring <span
class="math inline"><strong>P</strong><sub><strong>6</strong></sub></span>-free
graphs. II. Finding an excellent precoloring. <em>SICOMP</em>,
<em>53</em>(1), 146–187. (<a
href="https://doi.org/10.1137/18M1234849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This is the second paper in a series of two. The goal of the series is to give a polynomial-time algorithm for the 4-coloring problem and the 4-precoloring extension problem restricted to the class of graphs with no induced six-vertex path, thus proving a conjecture of Huang. Combined with previously known results, this completes the classification of the complexity of the 4-coloring problem for graphs with a connected forbidden induced subgraph. In this paper we give a polynomial time-algorithm that starts with a 4-precoloring of a graph with no induced six-vertex path and outputs a polynomial-sized collection of so-called excellent precolorings. Excellent precolorings are easier to handle than general ones, and, in addition, in order to determine whether the initial precoloring can be extended to the whole graph, it is enough to answer the same question for each of the excellent precolorings in the collection. The first paper in the series deals with excellent precolorings, thus providing a complete solution to the problem.},
  archive      = {J_SICOMP},
  author       = {Maria Chudnovsky and Sophie Spirkl and Mingxian Zhong},
  doi          = {10.1137/18M1234849},
  journal      = {SIAM Journal on Computing},
  month        = {2},
  number       = {1},
  pages        = {146-187},
  shortjournal = {SIAM J. Comput.},
  title        = {Four-coloring \(\boldsymbol{P_6}\)-free graphs. II. finding an excellent precoloring},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024b). Four-coloring <span
class="math inline"><em>P</em><sub>6</sub></span>-free graphs. I.
Extending an excellent precoloring. <em>SICOMP</em>, <em>53</em>(1),
111–145. (<a href="https://doi.org/10.1137/18M1234837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This is the first paper in a series whose goal is to give a polynomial-time algorithm for the 4-coloring problem and the 4-precoloring extension problem restricted to the class of graphs with no induced six-vertex path, thus proving a conjecture of Huang. Combined with previously known results, this completes the classification of the complexity of the 4-coloring problem for graphs with a connected forbidden induced subgraph. In this paper we give a polynomial-time algorithm that determines if a special kind of precoloring of a -free graph has a precoloring extension, and constructs such an extension if one exists. Combined with the main result of the second paper of the series, this gives a complete solution to the problem.},
  archive      = {J_SICOMP},
  author       = {Maria Chudnovsky and Sophie Spirkl and Mingxian Zhong},
  doi          = {10.1137/18M1234837},
  journal      = {SIAM Journal on Computing},
  month        = {2},
  number       = {1},
  pages        = {111-145},
  shortjournal = {SIAM J. Comput.},
  title        = {Four-coloring \(P_6\)-free graphs. i. extending an excellent precoloring},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online edge coloring via tree recurrences and correlation
decay. <em>SICOMP</em>, <em>53</em>(1), 87–110. (<a
href="https://doi.org/10.1137/22M152431X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We give an online algorithm that with high probability computes a edge coloring on a graph with maximum degree under online edge arrivals against oblivious adversaries, making first progress on the conjecture of Bar-Noy, Motwani, and Naor in this general setting. Our algorithm is based on reducing to a matching problem on locally treelike graphs, and then applying a tree recurrence based approach for arguing correlation decay.},
  archive      = {J_SICOMP},
  author       = {Janardhan Kulkarni and Yang P. Liu and Ashwin Sah and Mehtaab S. Sawhney and Jakub Tarnawski},
  doi          = {10.1137/22M152431X},
  journal      = {SIAM Journal on Computing},
  month        = {2},
  number       = {1},
  pages        = {87-110},
  shortjournal = {SIAM J. Comput.},
  title        = {Online edge coloring via tree recurrences and correlation decay},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Quasi-polynomial time approximation schemes for the maximum
weight independent set problem in <span
class="math inline"><strong>H</strong></span>-free graphs.
<em>SICOMP</em>, <em>53</em>(1), 47–86. (<a
href="https://doi.org/10.1137/20M1333778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the Maximum Independent Set problem we are asked to find a set of pairwise nonadjacent vertices in a given graph with the maximum possible cardinality. In general graphs, this classical problem is known to be NP-hard and hard to approximate within a factor of for any . Due to this, investigating the complexity of Maximum Independent Set in various graph classes in hope of finding better tractability results is an active research direction. In -free graphs, that is, graphs not containing a fixed graph as an induced subgraph, the problem is known to remain NP-hard and APX-hard whenever contains a cycle, a vertex of degree at least four, or two vertices of degree at least three in one connected component. For the remaining cases, where every component of is a path or a subdivided claw, the complexity of Maximum Independent Set remains widely open, with only a handful of polynomial-time solvability results for small graphs such as , the claw, or the fork. We prove that for every such “possibly tractable” graph there exists an algorithm that, given an -free graph and an accuracy parameter , finds an independent set in of cardinality within a factor of of the optimum in time exponential in a polynomial of and . Furthermore, an independent set of maximum size can be found in subexponential time . That is, we show that for every graph for which Maximum Independent Set is not known to be APX-hard and SUBEXP-hard in -free graphs, the problem admits a quasi-polynomial time approximation scheme and a subexponential-time exact algorithm in this graph class. Our algorithms also work in the more general weighted setting, where the input graph is supplied with a weight function on vertices and we are maximizing the total weight of an independent set.},
  archive      = {J_SICOMP},
  author       = {Maria Chudnovsky and Marcin Pilipczuk and Michał Pilipczuk and Stéphan Thomassé},
  doi          = {10.1137/20M1333778},
  journal      = {SIAM Journal on Computing},
  month        = {2},
  number       = {1},
  pages        = {47-86},
  shortjournal = {SIAM J. Comput.},
  title        = {Quasi-polynomial time approximation schemes for the maximum weight independent set problem in \(\boldsymbol{H}\)-free graphs},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Hardness of random optimization problems for boolean
circuits, low-degree polynomials, and langevin dynamics.
<em>SICOMP</em>, <em>53</em>(1), 1–46. (<a
href="https://doi.org/10.1137/22M150263X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the problem of finding nearly optimal solutions of optimization problems with random objective functions. Such problems arise widely in the theory of random graphs, theoretical computer science, and statistical physics. Two concrete problems we consider are (a) optimizing the Hamiltonian of a spherical or Ising -spin glass model and (b) finding a large independent set in a sparse Erdős–Rényi graph. The following families of algorithms are considered: (a) low-degree polynomials of the input—a general framework that captures many prior algorithms; (b) low-depth Boolean circuits; (c) the Langevin dynamics algorithm, a canonical Monte Carlo analogue of the gradient descent algorithm. We show that these families of algorithms cannot have high success probability. For the case of Boolean circuits, our results improve the state-of-the-art bounds known in circuit complexity theory (although we consider the search problem as opposed to the decision problem). Our proof uses the fact that these models are known to exhibit a variant of the overlap gap property (OGP) of near-optimal solutions. Specifically, for both models, every two solutions whose objectives are above a certain threshold are either close to or far from each other. The crux of our proof is that the classes of algorithms we consider exhibit a form of stability (noise-insensitivity): a small perturbation of the input induces a small perturbation of the output. We show by an interpolation argument that stable algorithms cannot overcome the OGP barrier. The stability of Langevin dynamics is an immediate consequence of the well-posedness of stochastic differential equations. The stability of low-degree polynomials and Boolean circuits is established using tools from Gaussian and Boolean analysis—namely hypercontractivity and total influence, as well as a novel lower bound for random walks avoiding certain subsets, which we expect to be of independent interest. In the case of Boolean circuits, the result also makes use of Linial–Mansour–Nisan’s classical theorem. Our techniques apply more broadly to low influence functions, and we expect that they may apply more generally.},
  archive      = {J_SICOMP},
  author       = {David Gamarnik and Aukosh Jagannath and Alexander S. Wein},
  doi          = {10.1137/22M150263X},
  journal      = {SIAM Journal on Computing},
  month        = {2},
  number       = {1},
  pages        = {1-46},
  shortjournal = {SIAM J. Comput.},
  title        = {Hardness of random optimization problems for boolean circuits, low-degree polynomials, and langevin dynamics},
  volume       = {53},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
