<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---47">SIMODS - 47</h2>
<ul>
<li><details>
<summary>
(2024). Lipschitz-regularized gradient flows and generative particle
algorithms for high-dimensional scarce data. <em>SIMODS</em>,
<em>6</em>(4), 1205–1235. (<a
href="https://doi.org/10.1137/23M1587841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We have developed a new class of generative algorithms capable of efficiently learning arbitrary target distributions from possibly scarce, high-dimensional data and subsequently generating new samples. These particle-based generative algorithms are constructed as gradient flows of Lipschitz-regularized Kullback–Leibler or other -divergences. In this framework, data from a source distribution can be stably transported as particles towards the vicinity of the target distribution. As a notable result in data integration, we demonstrate that the proposed algorithms accurately transport gene expression data points with dimensions exceeding 54K, even though the sample size is typically only in the hundreds.},
  archive      = {J_SIMODS},
  author       = {Hyemin Gu and Panagiota Birmpa and Yannis Pantazis and Luc Rey-Bellet and Markos A. Katsoulakis},
  doi          = {10.1137/23M1587841},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1205-1235},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Lipschitz-regularized gradient flows and generative particle algorithms for high-dimensional scarce data},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). SketchySGD: Reliable stochastic optimization via randomized
curvature estimates. <em>SIMODS</em>, <em>6</em>(4), 1173–1204. (<a
href="https://doi.org/10.1137/23M1575330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce SketchySGD, a stochastic second-order method that uses sketching to approximate the curvature of the loss function. SketchySGD improves on existing stochastic gradient methods in machine learning by using randomized low-rank approximations to the subsampled Hessian and by introducing an automated step size that works well across a wide range of convex machine learning problems. We show theoretically that SketchySGD with a fixed step size converges linearly to a small ball around the minimum. Further, in the ill-conditioned setting, we show that SketchySGD converges at a faster rate than stochastic gradient descent for least-squares problems. We validate this improvement empirically with ridge regression experiments on real data. Numerical experiments on both ridge and logistic regression problems with dense and sparse data show that SketchySGD equipped with its default hyperparameters can achieve comparable or better results than popular stochastic gradient methods and preconditioned conjugate gradients, even when they have been tuned to yield their best performance. In particular, SketchySGD is able to solve an ill-conditioned logistic regression problem with a data matrix that takes more than 840 GB of RAM to store, while its competitors, even when tuned, are unable to make any progress. SketchySGD’s ability to work out of the box with its default hyperparameters and excel on ill-conditioned problems is an advantage over other stochastic gradient methods, most of which require careful hyperparameter tuning (especially of the learning rate) to obtain good performance and degrade in the presence of ill-conditioning.},
  archive      = {J_SIMODS},
  author       = {Zachary Frangella and Pratik Rathore and Shipu Zhao and Madeleine Udell},
  doi          = {10.1137/23M1575330},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1173-1204},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {SketchySGD: Reliable stochastic optimization via randomized curvature estimates},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Federated primal dual fixed point algorithm.
<em>SIMODS</em>, <em>6</em>(4), 1138–1172. (<a
href="https://doi.org/10.1137/23M1565528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Federated learning is a distributed learning paradigm that allows several clients to learn a global model without sharing their private data. In this paper, we generalize a primal dual fixed point (PDFP) method [P. Chen, J. Huang, and X. Zhang, Inverse Problems, 29 (2013), 025011] to the federated learning setting and propose an algorithm called federated PDFP (FPDFP) for solving composite optimization problems. In addition, a quantization scheme is applied to reduce the communication overhead during the learning process. An convergence rate (where is the communication round) of the proposed FPDFP is provided. Numerical experiments, including graph-guided logistic regression and three-dimensional computed tomography reconstruction, are considered to evaluate the proposed algorithm.},
  archive      = {J_SIMODS},
  author       = {Ya-Nan Zhu and Jingwei Liang and Xiaoqun Zhang},
  doi          = {10.1137/23M1565528},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1138-1172},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Federated primal dual fixed point algorithm},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fast kernel summation in high dimensions via slicing and
fourier transforms. <em>SIMODS</em>, <em>6</em>(4), 1109–1137. (<a
href="https://doi.org/10.1137/24M1632085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Kernel-based methods are heavily used in machine learning. However, they suffer from complexity in the number of considered data points. In this paper, we propose an approximation procedure, which reduces this complexity to . Our approach is based on two ideas. First, we prove that any radial kernel with an analytic basis function can be represented as sliced version of some one-dimensional kernel and derive an analytic formula for the one-dimensional counterpart. It turns out that the relation between one- and -dimensional kernels is given by a generalized Riemann–Liouville fractional integral. Hence, we can reduce the -dimensional kernel summation to a one-dimensional setting. Second, for solving these one-dimensional problems efficiently, we apply fast Fourier summations on nonequispaced data, a sorting algorithm, or a combination of both. Due to its practical importance we pay special attention to the Gaussian kernel, where we show a dimension-independent error bound and represent its one-dimensional counterpart via a closed-form Fourier transform. We provide a runtime comparison and error estimate of our fast kernel summations.},
  archive      = {J_SIMODS},
  author       = {Johannes Hertrich},
  doi          = {10.1137/24M1632085},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1109-1137},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Fast kernel summation in high dimensions via slicing and fourier transforms},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Fredholm integral equations for function approximation and
the training of neural networks. <em>SIMODS</em>, <em>6</em>(4),
1078–1108. (<a href="https://doi.org/10.1137/23M156642X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a novel and mathematically transparent approach to function approximation and the training of large, high-dimensional neural networks, based on the approximate least-squares solution of associated Fredholm integral equations of the first kind by Ritz–Galerkin discretization, Tikhonov regularization, and tensor train methods. Practical application to supervised learning problems of regression and classification type confirm that the resulting algorithms are competitive with state-of-the-art neural network–based methods.},
  archive      = {J_SIMODS},
  author       = {Patrick Gelß and Aizhan Issagali and Ralf Kornhuber},
  doi          = {10.1137/23M156642X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1078-1108},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Fredholm integral equations for function approximation and the training of neural networks},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Energy-based sequential sampling for low-rank PSD-matrix
approximation. <em>SIMODS</em>, <em>6</em>(4), 1055–1077. (<a
href="https://doi.org/10.1137/23M162449X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce a pseudoconvex differentiable relaxation of the column-sampling problem for the Nyström approximation of positive-semidefinite (PSD) matrices. The relaxation is based on the interpretation of PSD matrices as integral operators and relies on the supports of measures to characterize samples of columns. We describe a class of gradient-based sequential sampling strategies which leverages the properties of the considered framework, and we demonstrate its ability to produce accurate Nyström approximations. The time-complexity of the stochastic variants of the discussed strategies is linear in the order of the considered matrices, and the underlying computations can be easily parallelized.},
  archive      = {J_SIMODS},
  author       = {Matthew Hutchings and Bertrand Gauthier},
  doi          = {10.1137/23M162449X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1055-1077},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Energy-based sequential sampling for low-rank PSD-matrix approximation},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Approximate message passing with rigorous guarantees for
pooled data and quantitative group testing. <em>SIMODS</em>,
<em>6</em>(4), 1027–1054. (<a
href="https://doi.org/10.1137/23M1604928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In the pooled data problem, the goal is to identify the categories associated with a large collection of items via a sequence of pooled tests. Each pooled test reveals the number of items of each category within the pool. We study an approximate message passing (AMP) algorithm for estimating the categories and rigorously characterize its performance, in both the noiseless and noisy settings. For the noiseless setting, we show that the AMP algorithm is equivalent to one recently proposed by El Alaoui et al. Our results provide a rigorous version of their performance guarantees, previously obtained via nonrigorous techniques. For the case of pooled data with two categories, known as quantitative group testing (QGT), we use the AMP guarantees to compute precise limiting values of the false positive rate and the false negative rate. Though the pooled data problem and QGT are both instances of estimation in a linear model, existing AMP theory cannot be directly applied since the design matrices are binary valued. The key technical ingredient in our analysis is a rigorous asymptotic characterization of AMP for generalized linear models defined via generalized white noise design matrices. This result, established using a recent universality result of Wang, Zhong, and Fan, is of independent interest. Our theoretical results are validated by numerical simulations. For comparison, we propose estimators based on convex relaxation and iterative thresholding, without providing theoretical guarantees. The simulations indicate that AMP outperforms the convex estimator for noiseless pooled data and QGT, but the convex estimator performs slightly better for noisy pooled data with three categories when the number of observations is small.},
  archive      = {J_SIMODS},
  author       = {Nelvin Tan and Pablo Pascual Cobo and Jonathan Scarlett and Ramji Venkataramanan},
  doi          = {10.1137/23M1604928},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1027-1054},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Approximate message passing with rigorous guarantees for pooled data and quantitative group testing},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Computing wasserstein barycenters via operator splitting:
The method of averaged marginals. <em>SIMODS</em>, <em>6</em>(4),
1000–1026. (<a href="https://doi.org/10.1137/23M1584228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The Wasserstein barycenter (WB) is an important tool for summarizing sets of probability measures. It finds applications in applied probability, clustering, image processing, etc. When the measures’ supports are finite, computing a (balanced) WB can be done by solving a linear optimization problem whose dimensions generally exceed standard solvers’ capabilities. In the more general setting where measures have different total masses, we propose a convex nonsmooth optimization formulation for the so-called unbalanced WB problem. Due to their colossal dimensions, we introduce a decomposition scheme based on the Douglas–Rachford splitting method that can be applied to both balanced and unbalanced WB problem variants. Our algorithm, which has the interesting interpretation of being built upon averaging marginals, operates a series of simple (and exact) projections that can be parallelized and even randomized, making it suitable for large-scale datasets. Numerical comparisons against state-of-the-art methods on several data sets from the literature illustrate the method’s performance.},
  archive      = {J_SIMODS},
  author       = {Daniel W. Mimouni and Paul Malisani and Jiamin Zhu and Welington de Oliveira},
  doi          = {10.1137/23M1584228},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {1000-1026},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Computing wasserstein barycenters via operator splitting: The method of averaged marginals},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal neural network approximation of wasserstein gradient
direction via convex optimization. <em>SIMODS</em>, <em>6</em>(4),
978–999. (<a href="https://doi.org/10.1137/23M1573173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The calculation of the direction of the Wasserstein gradient is vital for addressing problems related to posterior sampling and scientific computing. To approximate the Wasserstein gradient using finite samples, it is necessary to solve a variation problem. Our study focuses on the variation problem within the framework of two-layer networks with squared ReLU activations. We present a semidefinite program (SDP) relaxation as a solution, which can be viewed as an approximation of the Wasserstein gradient for a broader range of functions, including two-layer networks. By solving the convex SDP, we achieve the best approximation of the Wasserstein gradient direction in this function class. We also provide conditions to ensure the relaxation is tight. Additionally, we propose methods for practical implementation, such as subsampling and dimension reduction. The effectiveness and efficiency of our proposed method are demonstrated through numerical experiments, including Bayesian inference with PDE constraints and parameter estimation in COVID-19 modeling.},
  archive      = {J_SIMODS},
  author       = {Yifei Wang and Peng Chen and Mert Pilanci and Wuchen Li},
  doi          = {10.1137/23M1573173},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {978-999},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimal neural network approximation of wasserstein gradient direction via convex optimization},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High probability bounds for stochastic subgradient schemes
with heavy tailed noise. <em>SIMODS</em>, <em>6</em>(4), 953–977. (<a
href="https://doi.org/10.1137/22M1536558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work we study high-probability bounds for stochastic subgradient methods under heavy tailed noise in Hilbert spaces. In this setting the noise is only assumed to have finite variance as opposed to a sub-Gaussian distribution for which it is known that standard subgradient methods enjoy high-probability bounds. We analyzed a clipped version of the projected stochastic subgradient method, where subgradient estimates are truncated whenever they have large norms. We show that this clipping strategy leads both to optimal anytime and finite horizon bounds for general averaging schemes of the iterates. We also show an application of our proposal to the case of kernel methods which gives an efficient and fully implementable algorithm for statistical supervised learning problems. Preliminary experiments are shown to support the validity of the method.},
  archive      = {J_SIMODS},
  author       = {Daniela Angela Parletta and Andrea Paudice and Massimiliano Pontil and Saverio Salzo},
  doi          = {10.1137/22M1536558},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {953-977},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {High probability bounds for stochastic subgradient schemes with heavy tailed noise},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Stochastic variance-reduced majorization-minimization
algorithms. <em>SIMODS</em>, <em>6</em>(4), 926–952. (<a
href="https://doi.org/10.1137/23M1571836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study a class of nonconvex nonsmooth optimization problems in which the objective is a sum of two functions; one function is the average of a large number of differentiable functions, while the other function is proper, lower semicontinuous. Such problems arise in machine learning and regularized empirical risk minimization applications. However, nonconvexity and the large-sum structure are challenging for the design of new algorithms. Consequently, effective algorithms for such scenarios are scarce. We introduce and study three stochastic variance-reduced majorization-minimization (MM) algorithms, combining the general MM principle with new variance-reduced techniques. We provide almost surely subsequential convergence of the generated sequence to a stationary point. We further show that our algorithms possess the best-known complexity bounds in terms of gradient evaluations. We demonstrate the effectiveness of our algorithms on sparse binary classification problems, sparse multiclass logistic regressions, and neural networks by employing several widely used and publicly available data sets.},
  archive      = {J_SIMODS},
  author       = {Duy Nhat Phan and Sedi Bartz and Nilabja Guha and Hung M. Phan},
  doi          = {10.1137/23M1571836},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {926-952},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic variance-reduced majorization-minimization algorithms},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Subgradient langevin methods for sampling from nonsmooth
potentials. <em>SIMODS</em>, <em>6</em>(4), 897–925. (<a
href="https://doi.org/10.1137/23M1591451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is concerned with sampling from probability distributions on admitting a density of the form , where , with being a linear operator and being nondifferentiable. Two different methods are proposed, both employing a subgradient step with respect to but, depending on the regularity of , either an explicit or an implicit gradient step with respect to . For both methods, nonasymptotic convergence proofs are provided, with improved convergence results for more regular . Further, numerical experiments are conducted for simple 2D examples, illustrating the convergence rates, and for examples of Bayesian imaging, showing the practical feasibility of the proposed methods for high-dimensional data.},
  archive      = {J_SIMODS},
  author       = {Andreas Habring and Martin Holler and Thomas Pock},
  doi          = {10.1137/23M1591451},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {897-925},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Subgradient langevin methods for sampling from nonsmooth potentials},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Finite-time analysis of natural actor-critic for POMDPs.
<em>SIMODS</em>, <em>6</em>(4), 869–896. (<a
href="https://doi.org/10.1137/23M1587683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the reinforcement learning problem for partially observed Markov decision processes (POMDPs) with large state spaces. We consider a natural actor-critic method that employs an internal memory state for policy parameterization to address partial observability, function approximation in both actor and critic to address the curse of dimensionality, and a multistep temporal difference learning algorithm for policy evaluation. We establish nonasymptotic error bounds for actor-critic methods for partially observed systems under function approximation. In particular, in addition to the function approximation and statistical errors that also arise in MDPs, we explicitly characterize the error due to the use of finite-state controllers. This additional error is stated in terms of the total variation distance between the belief state in POMDPs and the posterior distribution of the hidden state when using a finite-state controller. Further, in the specific case of sliding-window controllers, we show that this inference error can be made arbitrarily small by using larger window sizes under certain ergodicity conditions.},
  archive      = {J_SIMODS},
  author       = {Semih Cayci and Niao He and R. Srikant},
  doi          = {10.1137/23M1587683},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {869-896},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Finite-time analysis of natural actor-critic for POMDPs},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Nonasymptotic bounds for adversarial excess risk under
misspecified models. <em>SIMODS</em>, <em>6</em>(4), 847–868. (<a
href="https://doi.org/10.1137/23M1598210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We propose a general approach to evaluating the performance of robust estimators based on adversarial losses under misspecified models. We first show that adversarial risk is equivalent to the risk induced by a distributional adversarial attack under certain smoothness conditions. This ensures that the adversarial training procedure is well-defined. To evaluate the generalization performance of the adversarial estimator, we study the adversarial excess risk. Our proposed analysis method includes investigations on both generalization error and approximation error. We then establish nonasymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions. In addition, we apply our general results to adversarial training for classification and regression problems. For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over that for a general loss.},
  archive      = {J_SIMODS},
  author       = {Changyu Liu and Yuling Jiao and Junhui Wang and Jian Huang},
  doi          = {10.1137/23M1598210},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {12},
  number       = {4},
  pages        = {847-868},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Nonasymptotic bounds for adversarial excess risk under misspecified models},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Corrigendum: Post-training quantization for neural networks
with provable guarantees. <em>SIMODS</em>, <em>6</em>(3), 842–846. (<a
href="https://doi.org/10.1137/24M1635582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We correct an error in Lemma A.5 of [J. Zhang, Y. Zhou, and R. Saab, SIAM J. Math. Data Sci., 5 (2023), pp. 373–399]; this lemma propagates to the proofs of Theorem 3.1, Theorem 3.2, and Theorem C.2. We restate these theorems and give their corrected proofs. The main results in the original paper still hold, with minor modifications.},
  archive      = {J_SIMODS},
  author       = {Jinjie Zhang and Yixuan Zhou and Rayan Saab},
  doi          = {10.1137/24M1635582},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {842-846},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Corrigendum: Post-training quantization for neural networks with provable guarantees},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Topological fingerprints for audio identification.
<em>SIMODS</em>, <em>6</em>(3), 815–841. (<a
href="https://doi.org/10.1137/23M1605090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We present a topological audio fingerprinting approach for robustly identifying duplicate audio tracks. Our method applies persistent homology on local spectral decompositions of audio signals, using filtered cubical complexes computed from mel spectrograms. By encoding the audio content in terms of local Betti curves, our topological audio fingerprints enable accurate detection of time-aligned audio matchings. Experimental results demonstrate the accuracy of our algorithm in the detection of tracks with the same audio content, even when subjected to various obfuscations. Our approach outperforms existing methods in scenarios involving topological distortions, such as time stretching and pitch shifting.},
  archive      = {J_SIMODS},
  author       = {Wojciech Reise and Ximena Fernández and Maria Dominguez and Heather A. Harrington and Mariano Beguerisse-Díaz},
  doi          = {10.1137/23M1605090},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {815-841},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Topological fingerprints for audio identification},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Improving the accuracy-robustness trade-off of classifiers
via adaptive smoothing. <em>SIMODS</em>, <em>6</em>(3), 788–814. (<a
href="https://doi.org/10.1137/23M1564560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. While prior research has proposed a plethora of methods that build neural classifiers robust against adversarial robustness, practitioners are still reluctant to adopt them due to their unacceptably severe clean accuracy penalties. Real-world services based on neural networks are thus still unsafe. This paper significantly alleviates the accuracy-robustness trade-off by mixing the output probabilities of a standard classifier and a robust classifier, where the standard network is optimized for clean accuracy and is not robust in general. We show that the robust base classifier’s confidence difference for correct and incorrect examples is the key to this improvement. In addition to providing empirical evidence, we theoretically certify the robustness of the mixed classifier under realistic assumptions. We then adapt an adversarial input detector into a mixing network that adaptively adjusts the mixture of the two base models, further reducing the accuracy penalty of achieving robustness. The proposed flexible mixture-of-experts framework, termed “adaptive smoothing,” works in conjunction with existing or even future methods that improve clean accuracy, robustness, or adversary detection. We use strong attack methods, including AutoAttack and adaptive attacks, to evaluate our models’ robustness. On the CIFAR-100 dataset, we achieve an clean accuracy while maintaining a -AutoAttacked accuracy, becoming the second most robust method on the RobustBench benchmark as of submission, while improving the clean accuracy by 10 percentage points over all listed models. Code implementation is available at https://github.com/Bai-YT/AdaptiveSmoothing.},
  archive      = {J_SIMODS},
  author       = {Yatong Bai and Brendon G. Anderson and Aerin Kim and Somayeh Sojoudi},
  doi          = {10.1137/23M1564560},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {788-814},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Improving the accuracy-robustness trade-off of classifiers via adaptive smoothing},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). New equivalences between interpolation and SVMs: Kernels and
structured features. <em>SIMODS</em>, <em>6</em>(3), 761–787. (<a
href="https://doi.org/10.1137/23M1568764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. The support vector machine (SVM) is a supervised learning algorithm that finds a maximum-margin linear classifier, often after mapping the data to a high-dimensional feature space via the kernel trick. Recent work has demonstrated that in certain sufficiently overparameterized settings, the SVM decision function coincides exactly with the minimum-norm label interpolant. This phenomenon of support vector proliferation (SVP) is especially interesting because it allows us to understand SVM performance by leveraging recent analyses of harmless interpolation in linear and kernel models. However, previous work on SVP has made restrictive assumptions on the data/feature distribution and spectrum. In this paper, we present a new and flexible analysis framework for proving SVP in an arbitrary reproducing kernel Hilbert space with a flexible class of generative models for the labels. We present conditions for SVP for features in the families of general bounded orthonormal systems (e.g., Fourier features) and independent sub-Gaussian features. In both cases, we show that SVP occurs in many interesting settings not covered by prior work, and we leverage these results to prove novel generalization results for kernel SVM classification.},
  archive      = {J_SIMODS},
  author       = {Chiraag Kaushik and Andrew D. McRae and Mark Davenport and Vidya Muthukumar},
  doi          = {10.1137/23M1568764},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {761-787},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {New equivalences between interpolation and SVMs: Kernels and structured features},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal dorfman group testing for symmetric distributions.
<em>SIMODS</em>, <em>6</em>(3), 731–760. (<a
href="https://doi.org/10.1137/23M1595138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study Dorfman’s classical group testing protocol in a novel setting where individual specimen statuses are modeled as exchangeable random variables. We are motivated by infectious disease screening. In that case, specimens which arrive together for testing often originate from the same community and so their statuses may exhibit positive correlation. Dorfman’s protocol screens a population of specimens for a binary trait by partitioning it into nonoverlapping groups, testing these, and only individually retesting the specimens of each positive group. The partition is chosen to minimize the expected number of tests under a probabilistic model of specimen statuses. We relax the typical assumption that these are independent and identically distributed and instead model them as exchangeable random variables. In this case, their joint distribution is symmetric in the sense that it is invariant under permutations. We give a characterization of such distributions in terms of a function where is the marginal probability that any group of size tests negative. We use this interpretable representation to show that the set partitioning problem arising in Dorfman’s protocol can be reduced to an integer partitioning problem and efficiently solved. We apply these tools to an empirical dataset from the COVID-19 pandemic. The methodology helps explain the unexpectedly high empirical efficiency reported by the original investigators.},
  archive      = {J_SIMODS},
  author       = {Nicholas C. Landolfi and Sanjay Lall},
  doi          = {10.1137/23M1595138},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {731-760},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimal dorfman group testing for symmetric distributions},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Spectral triadic decompositions of real-world networks.
<em>SIMODS</em>, <em>6</em>(3), 703–730. (<a
href="https://doi.org/10.1137/23M1586926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A fundamental problem in mathematics and network analysis is to find conditions under which a graph can be partitioned into smaller pieces. A ubiquitous tool for this partitioning is the Fiedler vector or discrete Cheeger inequality. These results relate the graph spectrum (eigenvalues of the normalized adjacency matrix) to the ability to break a graph into two pieces, with few edge deletions. An entire subfield of mathematics, called spectral graph theory, has emerged from these results. Yet these results do not say anything about the rich community structure exhibited by real-world networks, which typically have a significant fraction of edges contained in numerous densely clustered blocks. Inspired by the properties of real-world networks, we discover a new spectral condition that relates eigenvalue powers to a network decomposition into densely clustered blocks. We call this the spectral triadic decomposition. Our relationship exactly predicts the existence of community structure, as commonly seen in real networked data. Our proof provides an efficient algorithm to produce the spectral triadic decomposition. We observe on numerous social, coauthorship, and citation network datasets that these decompositions have significant correlation with semantically meaningful communities.},
  archive      = {J_SIMODS},
  author       = {Sabyasachi Basu and Suman Kalyan Bera and C. Seshadhri},
  doi          = {10.1137/23M1586926},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {703-730},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Spectral triadic decompositions of real-world networks},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Memory capacity of two layer neural networks with smooth
activations. <em>SIMODS</em>, <em>6</em>(3), 679–702. (<a
href="https://doi.org/10.1137/23M1599355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Determining the memory capacity of two layer neural networks with hidden neurons and input dimension (i.e., total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine learning question. For activations that are real analytic at a point and, if restricting to a polynomial there, have sufficiently high degree, we establish a lower bound of and optimality up to a factor of approximately 2. All practical activations, such as sigmoids, Heaviside, and the rectified linear unit (ReLU), are real analytic at a point. Furthermore, the degree condition is mild, requiring, for example, that if the activation is . Analogous prior results were limited to Heaviside and ReLU activations—our result covers almost everything else. In order to analyze general activations, we derive the precise generic rank of the network’s Jacobian, which can be written in terms of Hadamard powers and the Khatri–Rao product. Our analysis extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from prior works on memory capacity and holds promise for extending to deeper models and other architectures.},
  archive      = {J_SIMODS},
  author       = {Liam Madden and Christos Thrampoulidis},
  doi          = {10.1137/23M1599355},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {679-702},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Memory capacity of two layer neural networks with smooth activations},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). ABBA neural networks: Coping with positivity, expressivity,
and robustness. <em>SIMODS</em>, <em>6</em>(3), 649–678. (<a
href="https://doi.org/10.1137/23M1589591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We introduce ABBA networks, a novel class of (almost) nonnegative neural networks, which are shown to possess a series of appealing properties. In particular, we demonstrate that these networks are universal approximators while enjoying the advantages of nonnegative weighted networks. We derive tight Lipschitz bounds in both the fully connected and convolutional cases. We propose a strategy for designing ABBA nets that are robust against adversarial attacks, by finely controlling the Lipschitz constant of the network during the training phase. We show that our method outperforms other state-of-the-art defenses against adversarial white-box attackers. Experiments are performed on image classification tasks on four benchmark datasets.},
  archive      = {J_SIMODS},
  author       = {Ana Neacşu and Jean-Christophe Pesquet and Vlad Vasilescu and Corneliu Burileanu},
  doi          = {10.1137/23M1589591},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {649-678},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {ABBA neural networks: Coping with positivity, expressivity, and robustness},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Three-operator splitting for learning to predict equilibria
in convex games. <em>SIMODS</em>, <em>6</em>(3), 627–648. (<a
href="https://doi.org/10.1137/22M1544531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Systems of competing agents can often be modeled as games. Assuming rationality, the most likely outcomes are given by an equilibrium, e.g., a Nash equilibrium. In many practical settings, games are influenced by context, i.e., additional data beyond the control of any agent (e.g., weather for traffic and fiscal policy for market economies). Often, the exact game mechanics are unknown, yet vast amounts of historical data consisting of (context, equilibrium) pairs are available, raising the possibility of learning a solver that predicts the equilibria given only the context. We introduce Nash fixed-point networks (N-FPNs), a class of neural networks that naturally output equilibria. Crucially, N-FPNs employ a constraint decoupling scheme to handle complicated agent action sets while avoiding expensive projections. Empirically, we find that N-FPNs are compatible with the recently developed Jacobian-free backpropagation technique for training implicit networks, making them significantly faster and easier to train than prior models. Our experiments show that N-FPNs are capable of scaling to problems orders of magnitude larger than existing learned game solvers. All code is available online.},
  archive      = {J_SIMODS},
  author       = {D. McKenzie and H. Heaton and Q. Li and S. Wu Fung and S. Osher and W. Yin},
  doi          = {10.1137/22M1544531},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {627-648},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Three-operator splitting for learning to predict equilibria in convex games},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Gradient descent in the absence of global lipschitz
continuity of the gradients. <em>SIMODS</em>, <em>6</em>(3), 602–626.
(<a href="https://doi.org/10.1137/22M1527210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Gradient descent (GD) is a collection of continuous optimization methods that have achieved immeasurable success in practice. Owing to data science applications, GD with diminishing step sizes has become a prominent variant. While this variant of GD has been well studied in the literature for objectives with globally Lipschitz continuous gradients or by requiring bounded iterates, objectives from data science problems do not satisfy such assumptions. Thus, in this work, we provide a novel global convergence analysis of GD with diminishing step sizes for differentiable nonconvex functions whose gradients are only locally Lipschitz continuous. Through our analysis, we generalize what is known about gradient descent with diminishing step sizes, including interesting topological facts, and we elucidate the varied behaviors that can occur in the previously overlooked divergence regime. Thus, we provide a general global convergence analysis of GD with diminishing step sizes under realistic conditions for data science problems.},
  archive      = {J_SIMODS},
  author       = {Vivak Patel and Albert S. Berahas},
  doi          = {10.1137/22M1527210},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {602-626},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Gradient descent in the absence of global lipschitz continuity of the gradients},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Equivariant neural networks for indirect measurements.
<em>SIMODS</em>, <em>6</em>(3), 579–601. (<a
href="https://doi.org/10.1137/23M1582862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In recent years, deep learning techniques have shown great success in various tasks related to inverse problems, where a target quantity of interest can only be observed through indirect measurements by a forward operator. Common approaches apply deep neural networks in a postprocessing step to the reconstructions obtained by classical reconstruction methods. However, the latter methods can be computationally expensive and introduce artifacts that are not present in the measured data and, in turn, can deteriorate the performance on the given task. To overcome these limitations, we propose a class of equivariant neural networks that can be directly applied to the measurements to solve the desired task. To this end, we build appropriate network structures by developing layers that are equivariant with respect to data transformations induced by well-known symmetries in the domain of the forward operator. We rigorously analyze the relation between the measurement operator and the resulting group representations and prove a representer theorem that characterizes the class of linear operators that translate between a given pair of group actions. Based on this theory, we extend the existing concepts of Lie group equivariant deep learning to inverse problems and introduce new representations that result from the involved measurement operations. This allows us to efficiently solve classification, regression, or even reconstruction tasks based on indirect measurements also for very sparse data problems, where a classical reconstruction-based approach may be hard or even impossible. We illustrate the effectiveness of our approach in numerical experiments and compare with existing methods.},
  archive      = {J_SIMODS},
  author       = {Matthias Beckmann and Nick Heilenkötter},
  doi          = {10.1137/23M1582862},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {9},
  number       = {3},
  pages        = {579-601},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Equivariant neural networks for indirect measurements},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). A diffusion process perspective on posterior contraction
rates for parameters. <em>SIMODS</em>, <em>6</em>(2), 553–577. (<a
href="https://doi.org/10.1137/22M1516038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We analyze the posterior contraction rates of parameters in Bayesian models via the Langevin diffusion process, in particular by controlling moments of the stochastic process and taking limits. Analogous to the nonasymptotic analysis of statistical -estimators and stochastic optimization algorithms, our contraction rates depend on the structure of the population log-likelihood function, and stochastic perturbation bounds between the population and sample log-likelihood functions. Convergence rates are determined by a nonlinear equation that relates the population-level structure to stochastic perturbation terms, along with a term characterizing the diffusive behavior. Based on this technique, we also prove nonasymptotic versions of a Bernstein–von Mises guarantee for the posterior. We illustrate this general theory by deriving posterior convergence rates for various concrete examples.},
  archive      = {J_SIMODS},
  author       = {Wenlong Mou and Nhat Ho and Martin Wainwright and Peter Bartlett and Michael Jordan},
  doi          = {10.1137/22M1516038},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {553-577},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A diffusion process perspective on posterior contraction rates for parameters},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Max-affine regression via first-order methods.
<em>SIMODS</em>, <em>6</em>(2), 534–552. (<a
href="https://doi.org/10.1137/23M1594662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a nonasymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anticoncentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alternating minimization and GD in the noiseless scenario but also outperforms them in low-sample scenarios with noise.},
  archive      = {J_SIMODS},
  author       = {Seonho Kim and Kiryung Lee},
  doi          = {10.1137/23M1594662},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {534-552},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Max-affine regression via first-order methods},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The geometric median and applications to robust mean
estimation. <em>SIMODS</em>, <em>6</em>(2), 504–533. (<a
href="https://doi.org/10.1137/23M1592420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper is devoted to the statistical and numerical properties of the geometric median and its applications to the problem of robust mean estimation via the median of means principle. Our main theoretical results include (a) an upper bound for the distance between the mean and the median for general absolutely continuous distributions in , and examples of specific classes of distributions for which these bounds do not depend on the ambient dimension ; (b) exponential deviation inequalities for the distance between the sample and the population versions of the geometric median, which again depend only on the trace-type quantities and not on the ambient dimension. As a corollary, we deduce improved bounds for the (geometric) median of means estimator that hold for large classes of heavy-tailed distributions. Finally, we address the error of numerical approximation, which is an important practical aspect of any statistical estimation procedure. We demonstrate that the objective function minimized by the geometric median satisfies a “local quadratic growth” condition that allows one to translate suboptimality bounds for the objective function to the corresponding bounds for the numerical approximation to the median itself and propose a simple stopping rule applicable to any optimization method which yields explicit error guarantees. We conclude with the numerical experiments, including the application to estimation of mean values of log-returns for S&amp;P 500 data.},
  archive      = {J_SIMODS},
  author       = {Stanislav Minsker and Nate Strawn},
  doi          = {10.1137/23M1592420},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {504-533},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The geometric median and applications to robust mean estimation},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Scalable tensor methods for nonuniform hypergraphs.
<em>SIMODS</em>, <em>6</em>(2), 481–503. (<a
href="https://doi.org/10.1137/23M1584472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. While multilinear algebra appears natural for studying the multiway interactions modeled by hypergraphs, tensor methods for general hypergraphs have been stymied by theoretical and practical barriers. A recently proposed adjacency tensor is applicable to nonuniform hypergraphs but is prohibitively costly to form and analyze in practice. We develop tensor times same vector (TTSV) algorithms for this tensor, which improve complexity from to a low-degree polynomial in , where is the number of vertices and is the maximum hyperedge size. Our algorithms are implicit, avoiding formation of the order adjacency tensor. We demonstrate the flexibility and utility of our approach in practice by developing tensor-based hypergraph centrality and clustering algorithms. We also show that these tensor measures offer complementary information to analogous graph-reduction approaches on data and are also able to detect higher-order structure that many existing matrix-based approaches provably cannot.},
  archive      = {J_SIMODS},
  author       = {Sinan G. Aksoy and Ilya Amburg and Stephen J. Young},
  doi          = {10.1137/23M1584472},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {481-503},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Scalable tensor methods for nonuniform hypergraphs},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The common intuition to transfer learning can win or lose:
Case studies for linear regression. <em>SIMODS</em>, <em>6</em>(2),
454–480. (<a href="https://doi.org/10.1137/23M1563062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study a fundamental transfer learning process from source to target linear regression tasks, including overparameterized settings where there are more learned parameters than data samples. The target task learning is addressed by using its training data together with the parameters previously computed for the source task. We define a transfer learning approach to the target task as a linear regression optimization with a regularization on the distance between the to-be-learned target parameters and the already learned source parameters. We analytically characterize the generalization performance of our transfer learning approach and demonstrate its ability to resolve the peak in generalization errors in double descent phenomena of the minimum -norm solution to linear regression. Moreover, we show that for sufficiently related tasks, the optimally tuned transfer learning approach can outperform the optimally tuned ridge regression method, even when the true parameter vector conforms to an isotropic Gaussian prior distribution. Namely, we demonstrate that transfer learning can beat the minimum mean square error (MMSE) solution of the independent target task. Our results emphasize the ability of transfer learning to extend the solution space to the target task and, by that, to have an improved MMSE solution. We formulate the linear MMSE solution to our transfer learning setting and point out its key differences from the common design philosophy to transfer learning.},
  archive      = {J_SIMODS},
  author       = {Yehuda Dar and Daniel LeJeune and Richard G. Baraniuk},
  doi          = {10.1137/23M1563062},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {454-480},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The common intuition to transfer learning can win or lose: Case studies for linear regression},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Robust and tuning-free sparse linear regression via
square-root slope. <em>SIMODS</em>, <em>6</em>(2), 428–453. (<a
href="https://doi.org/10.1137/23M1608690">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider the high-dimensional linear regression model and assume that a fraction of the measurements are altered by an adversary with complete knowledge of the data and the underlying distribution. We are interested in a scenario where dense additive noise is heavy-tailed, while the measurement vectors follow a sub-Gaussian distribution. Within this framework, we establish minimax lower bounds for the performance of an arbitrary estimator that depend on the fraction of corrupted observations as well as the tail behavior of the additive noise. Moreover, we design a modification of the so-called square-root Slope estimator with several desirable features: (a) It is provably robust to adversarial contamination and satisfies performance guarantees in the form of sub-Gaussian deviation inequalities that match the lower error bounds, up to logarithmic factors; (b) it is fully adaptive with respect to the unknown sparsity level and the variance of the additive noise; and (c) it is computationally tractable as a solution of a convex optimization problem. To analyze performance of the proposed estimator, we prove several properties of matrices with sub-Gaussian rows that may be of independent interest.},
  archive      = {J_SIMODS},
  author       = {Stanislav Minsker and Mohamed Ndaoud and Lang Wang},
  doi          = {10.1137/23M1608690},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {428-453},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Robust and tuning-free sparse linear regression via square-root slope},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Rigorous dynamical mean-field theory for stochastic gradient
descent methods. <em>SIMODS</em>, <em>6</em>(2), 400–427. (<a
href="https://doi.org/10.1137/23M1594388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We prove closed-form equations for the exact high-dimensional asymptotics of a family of first-order gradient-based methods, learning an estimator (e.g., M-estimator, shallow neural network) from observations on Gaussian data with empirical risk minimization. This includes widely used algorithms such as stochastic gradient descent (SGD) or Nesterov acceleration. The obtained equations match those resulting from the discretization of dynamical mean-field theory equations from statistical physics when applied to the corresponding gradient flow. Our proof method allows us to give an explicit description of how memory kernels build up in the effective dynamics and to include nonseparable update functions, allowing datasets with nonidentity covariance matrices. Finally, we provide numerical implementations of the equations for SGD with generic extensive batch size and constant learning rates.},
  archive      = {J_SIMODS},
  author       = {Cédric Gerbelot and Emanuele Troiani and Francesca Mignacco and Florent Krzakala and Lenka Zdeborová},
  doi          = {10.1137/23M1594388},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {400-427},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Rigorous dynamical mean-field theory for stochastic gradient descent methods},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Structural balance and random walks on complex networks with
complex weights. <em>SIMODS</em>, <em>6</em>(2), 372–399. (<a
href="https://doi.org/10.1137/23M1584265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Complex numbers define the relationship between entities in many situations. A canonical example would be the off-diagonal terms in a Hamiltonian matrix in quantum physics. Recent years have seen an increasing interest to extend the tools of network science when the weight of edges are complex numbers. Here, we focus on the case when the weight matrix is Hermitian, a reasonable assumption in many applications, and investigate both structural and dynamical properties of the networks with complex weights. Building on concepts from signed graphs, we introduce a classification of complex-weighted networks based on the notion of structural balance and illustrate the shared spectral properties within each type. We then apply the results to characterize the dynamics of random walks on complex-weighted networks, where local consensus can be achieved asymptotically when the graph is structurally balanced, while global consensus will be obtained when it is strictly unbalanced. Finally, we explore potential applications of our findings by generalizing the notion of cut and propose an associated spectral clustering algorithm. We also provide further characteristics of the magnetic Laplacian, associating directed networks to complex-weighted ones. The performance of the algorithm is verified on both synthetic and real networks.},
  archive      = {J_SIMODS},
  author       = {Yu Tian and Renaud Lambiotte},
  doi          = {10.1137/23M1584265},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {372-399},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Structural balance and random walks on complex networks with complex weights},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Learning functions varying along a central subspace.
<em>SIMODS</em>, <em>6</em>(2), 343–371. (<a
href="https://doi.org/10.1137/23M1557751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Many functions of interest are in a high-dimensional space but exhibit low-dimensional structures. This paper studies regression of an -Hölder function in which varies along a central subspace of dimension while . A direct approximation of in with an accuracy requires the number of samples in the order of . In this paper, we analyze the generalized contour regression (GCR) algorithm for the estimation of the central subspace and use piecewise polynomials for function approximation. GCR is among the best estimators for the central subspace, but its sample complexity is an open question. In this paper, we partially answer this questions by proving that if a variance quantity is exactly known, GCR leads to a mean squared estimation error of for the central subspace. The estimation error of this variance quantity is also given in this paper. The mean squared regression error of is proved to be in the order of , where the exponent depends on the dimension of the central subspace instead of the ambient space . This result demonstrates that GCR is effective in learning the low-dimensional central subspace. We also propose a modified GCR with improved efficiency. The convergence rate is validated through several numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Hao Liu and Wenjing Liao},
  doi          = {10.1137/23M1557751},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {343-371},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Learning functions varying along a central subspace},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimal estimation of smooth transport maps with kernel SoS.
<em>SIMODS</em>, <em>6</em>(2), 311–342. (<a
href="https://doi.org/10.1137/22M1528847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Under smoothness conditions, it was recently shown by Vacher et al. [Proceedings of the 34th Conference on Learning Theory, Proc. Mach. Learn. Res. 134, 2021] that the squared Wasserstein distance between two distributions could be approximately computed in polynomial time with appealing statistical error bounds. In this paper, we propose to extend their result to the problem of estimating in distance the transport map between two distributions. Also building upon the kernelized sum-of-squares approach, a way to model smooth positive functions, we derive a computationally tractable estimator of the transport map. Contrary to the aforementioned work, the dual problem that we solve is closer to the so-called semidual formulation of optimal transport that is known to gain convexity with respect to the linear dual formulation. After deriving a new stability result on the semidual and using localization-like techniques through Gagliardo–Nirenberg inequalities, we manage to prove under the same assumptions as in Vacher et al. that our estimator is minimax optimal up to polylog factors. Then we prove that this estimator can be computed in the worst case in time, where is the number of samples, and show how to improve its practical computation with a Nyström approximation scheme, a classical tool in kernel methods. Finally, we showcase several numerical simulations in medium dimension, where we compute our estimator on simple examples.},
  archive      = {J_SIMODS},
  author       = {Adrien Vacher and Boris Muzellec and Francis Bach and François-Xavier Vialard and Alessandro Rudi},
  doi          = {10.1137/22M1528847},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {311-342},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimal estimation of smooth transport maps with kernel SoS},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Reversible gromov–monge sampler for simulation-based
inference. <em>SIMODS</em>, <em>6</em>(2), 283–310. (<a
href="https://doi.org/10.1137/23M1550384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper introduces a new simulation-based inference procedure to model and sample from multidimensional probability distributions given access to independent and identically distributed samples, circumventing the usual approaches of explicitly modeling the density function or designing Markov chain Monte Carlo. Motivated by the seminal work on distance and isomorphism between metric measure spaces, we develop a new transform sampler to perform simulation-based inference, which estimates a notion of optimal alignments between two heterogeneous metric measure spaces and from empirical data sets, with estimated maps that approximately push forward one measure to the other , and vice versa. We introduce a new notion called the reversible Gromov–Monge (RGM) distance, providing mathematical formalism behind the new sampler. We study the statistical rate of convergence of the new transform sampler, along with several analytic properties of the RGM distance and operator viewpoints of transform sampling. Synthetic and real-world examples showcasing the effectiveness of the new sampler are also demonstrated.},
  archive      = {J_SIMODS},
  author       = {YoonHaeng Hur and Wenxuan Guo and Tengyuan Liang},
  doi          = {10.1137/23M1550384},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {283-310},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Reversible Gromov–Monge sampler for simulation-based inference},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). The sample complexity of sparse multireference alignment and
single-particle cryo-electron microscopy. <em>SIMODS</em>,
<em>6</em>(2), 254–282. (<a
href="https://doi.org/10.1137/23M155685X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Multireference alignment (MRA) is the problem of recovering a signal from its multiple noisy copies, each acted upon by a random group element. MRA is mainly motivated by single-particle cryo–electron microscopy (cryo-EM) that has recently joined X-ray crystallography as one of the two leading technologies to reconstruct biological molecular structures. Previous papers have shown that, in the high-noise regime, the sample complexity of MRA and cryo-EM is , where is the number of observations, is the variance of the noise, and is the lowest-order moment of the observations that uniquely determines the signal. In particular, it was shown that, in many cases, for generic signals, and thus, the sample complexity is . In this paper, we analyze the second moment of the MRA and cryo-EM models. First, we show that, in both models, the second moment determines the signal up to a set of unitary matrices whose dimension is governed by the decomposition of the space of signals into irreducible representations of the group. Second, we derive sparsity conditions under which a signal can be recovered from the second moment, implying sample complexity of . Notably, we show that the sample complexity of cryo-EM is if at most one-third of the coefficients representing the molecular structure are nonzero; this bound is near-optimal. The analysis is based on tools from representation theory and algebraic geometry. We also derive bounds on recovering a sparse signal from its power spectrum, which is the main computational problem of X-ray crystallography.},
  archive      = {J_SIMODS},
  author       = {Tamir Bendory and Dan Edidin},
  doi          = {10.1137/23M155685X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {254-282},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {The sample complexity of sparse multireference alignment and single-particle cryo-electron microscopy},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Efficiency of ETA prediction. <em>SIMODS</em>,
<em>6</em>(2), 227–253. (<a
href="https://doi.org/10.1137/23M155699X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Modern mobile applications such as navigation services and ride-sharing platforms rely heavily on geospatial technologies, most critically predictions of the time required for a vehicle to traverse a particular route, or the so-called estimated time of arrival (ETA). There are various methods used in practice, which differ in terms of the geographic granularity at which the predictive model is trained—e.g., segment-based methods predict travel time at the level of road segments (or a combination of several adjacent road segments) and then aggregate across the route, whereas route-based methods use generic information about the trip, such as origin and destination, to predict travel time. Though various forms of these methods have been developed, there has been no rigorous theoretical comparison regarding their accuracies, and empirical studies have, in many cases, drawn opposite conclusions. We provide the first theoretical analysis of the predictive accuracy of various ETA prediction methods and argue that maintaining a segment-level architecture in predicting travel time is often of first-order importance. Our work highlights that the accuracy of ETA prediction is driven not just by the sophistication of the model but also by the spatial granularity at which those methods are applied.},
  archive      = {J_SIMODS},
  author       = {Chiwei Yan and James Johndrow and Dawn Woodard and Yanwei Sun},
  doi          = {10.1137/23M155699X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {6},
  number       = {2},
  pages        = {227-253},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Efficiency of ETA prediction},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Asymptotics of the sketched pseudoinverse. <em>SIMODS</em>,
<em>6</em>(1), 199–225. (<a
href="https://doi.org/10.1137/22M1530264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We take a random matrix theory approach to random sketching and show an asymptotic first-order equivalence of the regularized sketched pseudoinverse of a positive semidefinite matrix to a certain evaluation of the resolvent of the same matrix. We focus on real-valued regularization and extend previous results on an asymptotic equivalence of random matrices to the real setting, providing a precise characterization of the equivalence even under negative regularization, including a precise characterization of the smallest nonzero eigenvalue of the sketched matrix, which may be of independent interest. We then further characterize the second-order equivalence of the sketched pseudoinverse. We also apply our results to the analysis of the sketch-and-project method and to sketched ridge regression. Last, we prove that these results generalize to asymptotically free sketching matrices, obtaining the resulting equivalence for orthogonal sketching matrices and comparing our results to several common sketches used in practice.},
  archive      = {J_SIMODS},
  author       = {Daniel LeJeune and Pratik Patil and Hamid Javadi and Richard G. Baraniuk and Ryan J. Tibshirani},
  doi          = {10.1137/22M1530264},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {199-225},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Asymptotics of the sketched pseudoinverse},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Network online change point localization. <em>SIMODS</em>,
<em>6</em>(1), 176–198. (<a
href="https://doi.org/10.1137/22M1529816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We study the following online network change point detection settings: A time series of independent, possibly sparse Bernoulli networks whose distributions might change at an unknown time are observed in a sequential manner, and at each time point, a determination has to be made on whether a change has taken place in the near past. The goal is to detect the change point event (if any has occurred) as quickly as possible, subject to prespecified constraints on the probability or number of false alarms. We propose a CUSUM-based procedure and derive two high-probability upper bounds on its detection delay, i.e., detection delay ; , under a low-rank assumption; , under a block-constancy assumption, where , and are the normalized jump size, network size, entrywise sparsity, rank sparsity, and overall type I error upper bound. All the model parameters are allowed to vary as , the unknown change point, diverges. We further establish a minimax lower bound on the detection delay. Under the low-rank assumption and when the rank is of constant order or under the block-constancy assumption when the number of blocks , we obtain minimax rates. The above upper bounds are achieved by novel procedures proposed in this paper, designed for quick detection under two different forms of type I error control. The first is based on controlling the overall probability of a false alarm when there are no change points, and the second is based on specifying a lower bound on the expected time of the first false alarm. Extensive experiments show that under different scenarios and the aforementioned forms of type I error control, our proposed approaches well outperform state-of-the-art methods.},
  archive      = {J_SIMODS},
  author       = {Yi Yu and Oscar H. Madrid Padilla and Daren Wang and Alessandro Rinaldo},
  doi          = {10.1137/22M1529816},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {176-198},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Network online change point localization},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Faster rates for compressed federated learning with
client-variance reduction. <em>SIMODS</em>, <em>6</em>(1), 154–175. (<a
href="https://doi.org/10.1137/23M1553820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Due to the communication bottleneck in distributed and federated learning applications, algorithms using communication compression have attracted significant attention and are widely used in practice. Moreover, the huge number, high heterogeneity, and limited availability of clients result in high client-variance. This paper addresses these two issues together by proposing compressed and client-variance reduced methods COFIG and FRECON. We prove an bound on the number of communication rounds of COFIG in the nonconvex setting, where is the total number of clients, is the number of clients participating in each round, is the convergence error, and is the variance parameter associated with the compression operator. In case of FRECON, we prove an bound on the number of communication rounds. In the convex setting, COFIG converges within communication rounds, which, to the best of our knowledge, is also the first convergence result for compression schemes that do not communicate with all the clients in each round. We stress that neither COFIG nor FRECON needs to communicate with all the clients, and they enjoy the first or faster convergence results for convex and nonconvex federated learning in the regimes considered. Experimental results point to an empirical superiority of COFIG and FRECON over existing baselines.},
  archive      = {J_SIMODS},
  author       = {Haoyu Zhao and Konstantin Burlachenko and Zhize Li and Peter Richtárik},
  doi          = {10.1137/23M1553820},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {154-175},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Faster rates for compressed federated learning with client-variance reduction},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Sharp analysis of sketch-and-project methods via a
connection to randomized singular value decomposition. <em>SIMODS</em>,
<em>6</em>(1), 127–153. (<a
href="https://doi.org/10.1137/23M1545537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. Sketch-and-project is a framework which unifies many known iterative methods for solving linear systems and their variants, as well as further extensions to nonlinear optimization problems. It includes popular methods such as randomized Kaczmarz, coordinate descent, variants of the Newton method in convex optimization, and others. In this paper, we develop a theoretical framework for obtaining sharp guarantees on the convergence rate of sketch-and-project methods. Our approach is the first to (1) show that the convergence rate improves at least linearly with the sketch size, and even faster when the data matrix exhibits certain spectral decays and (2) allow for sparse sketching matrices, which are more efficient than dense sketches and more robust than subsampling methods. In particular, our results explain an observed phenomenon that a radical sparsification of the sketching matrix does not affect the per iteration convergence rate of sketch-and-project. To obtain our results, we develop new nonasymptotic spectral bounds for the expected sketched projection matrix, which are of independent interest; and we establish a connection between the convergence rates of iterative sketch-and-project solvers and the approximation error of randomized singular value decomposition, which is a widely used one-shot sketching algorithm for low-rank approximation. Our experiments support the theory and demonstrate that even extremely sparse sketches exhibit the convergence properties predicted by our framework.},
  archive      = {J_SIMODS},
  author       = {Michał Dereziński and Elizaveta Rebrova},
  doi          = {10.1137/23M1545537},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {127-153},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Sharp analysis of sketch-and-project methods via a connection to randomized singular value decomposition},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Applications of no-collision transportation maps in manifold
learning. <em>SIMODS</em>, <em>6</em>(1), 97–126. (<a
href="https://doi.org/10.1137/23M1567771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. In this work, we investigate applications of no-collision transportation maps introduced by Nurbekyan et al. in 2020 in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [L. Nurbekyan, A. Iannantuono, and A. M. Oberman, J. Sci. Comput., 82 (2020), 45] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively, dilations) of a single probability measure and the translation (respectively, dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotations. The numerical experiments confirm our theoretical findings and show that no-collision distances achieve similar or better performance on several manifold learning tasks compared to other OT and Euclidean-based methods at a fraction of the computational cost.},
  archive      = {J_SIMODS},
  author       = {Elisa Negrini and Levon Nurbekyan},
  doi          = {10.1137/23M1567771},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {97-126},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Applications of no-collision transportation maps in manifold learning},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). On design of polyhedral estimates in linear inverse
problems. <em>SIMODS</em>, <em>6</em>(1), 76–96. (<a
href="https://doi.org/10.1137/22M1543331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A polyhedral estimate [A. Juditsky and A. Nemirovski, Electron. J. Stat., 14 (2020), pp. 458–502; Statistical Inference via Convex Optimization, Princeton University Press, Princeton, NJ, 2020] is a generic efficiently computable nonlinear in observations routine for recovering an unknown signal belonging to a given convex compact set from noisy observation of the signal’s linear image. Risk analysis and optimal design of polyhedral estimates may be addressed through efficient bounding of optimal values of optimization problems. Such problems are typically hard, yet it was shown in [A. Juditsky and A. Nemirovski, Electron. J. Stat., 14 (2020), pp. 458–502] that nearly minimax optimal (“up to logarithmic factors”) estimates can be efficiently constructed when the signal set is an ellitope—a member of a wide family of convex and compact sets of special geometry (see, e.g., [A. Juditsky and A. Nemirovski, Math. Stat. Learn., 1 (2018), pp. 171–225]). The subject of this paper is a new risk analysis for a polyhedral estimate in the situation where the signal set is an intersection of an ellitope and an arbitrary polytope allowing for improved polyhedral estimate design in this situation.},
  archive      = {J_SIMODS},
  author       = {Anatoli Juditsky and Arkadi Nemirovski},
  doi          = {10.1137/22M1543331},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {76-96},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On design of polyhedral estimates in linear inverse problems},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Online MCMC thinning with kernelized stein discrepancy.
<em>SIMODS</em>, <em>6</em>(1), 51–75. (<a
href="https://doi.org/10.1137/22M1510108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. A fundamental challenge in Bayesian inference is efficient representation of a target distribution. Many nonparametric approaches do so by sampling a large number of points using variants of Markov chain Monte Carlo (MCMC). We propose an MCMC variant that retains only those posterior samples which exceed a kernelized Stein discrepancy (KSD) threshold, which we call KSD thinning. We establish the convergence and complexity trade-offs for several settings of KSD thinning as a function of the KSD threshold parameter, sample size, and other problem parameters. We provide experimental comparisons against other online nonparametric Bayesian methods that generate low-complexity posterior representations. We observe superior consistency/complexity trade-offs across a range of settings including MCMC sampling on two Bayesian inference problems from the biological sciences, and inference speedup and storage reduction for Bayesian neural networks with no loss of accuracy and no increase in training time. Our code is available at https://github.com/colehawkins/KSD-Thinning.},
  archive      = {J_SIMODS},
  author       = {Alec Koppel and Joe Eappen and Sujay Bhatt and Cole Hawkins and Sumitra Ganesh},
  doi          = {10.1137/22M1510108},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {51-75},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Online MCMC thinning with kernelized stein discrepancy},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). High-dimensional analysis of double descent for linear
regression with random projections. <em>SIMODS</em>, <em>6</em>(1),
26–50. (<a href="https://doi.org/10.1137/23M1558781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. We consider linear regression problems with a varying number of random projections, where we provably exhibit a double descent curve for a fixed prediction problem, with a high-dimensional analysis based on random matrix theory. We first consider the ridge regression estimator and review earlier results using classical notions from nonparametric statistics, namely, degrees of freedom, also known as effective dimensionality. We then compute asymptotic equivalents of the generalization performance (in terms of squared bias and variance) of the minimum norm least-squares fit with random projections, providing simple expressions for the double descent phenomenon.},
  archive      = {J_SIMODS},
  author       = {Francis Bach},
  doi          = {10.1137/23M1558781},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {26-50},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {High-dimensional analysis of double descent for linear regression with random projections},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
<li><details>
<summary>
(2024). Optimization on manifolds via graph gaussian processes.
<em>SIMODS</em>, <em>6</em>(1), 1–25. (<a
href="https://doi.org/10.1137/22M1529907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract. This paper integrates manifold learning techniques within a Gaussian process upper confidence bound algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.},
  archive      = {J_SIMODS},
  author       = {Hwanwoo Kim and Daniel Sanz-Alonso and Ruiyi Yang},
  doi          = {10.1137/22M1529907},
  journal      = {SIAM Journal on Mathematics of Data Science},
  month        = {3},
  number       = {1},
  pages        = {1-25},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Optimization on manifolds via graph gaussian processes},
  volume       = {6},
  year         = {2024},
}
</textarea>
</details></li>
</ul>

</body>
</html>
